URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10124v1,Modeling beam chromaticity for high-resolution CMB analyses,"We investigate the impact of beam chromaticity, i.e., the frequency dependence of the beam window function, on cosmological and astrophysical parameter constraints from CMB power spectrum observations. We show that for future high-resolution CMB measurements it is necessary to include a color-corrected beam for each sky component with a distinct spectral energy distribution. We introduce a formalism able to easily implement the beam chromaticity in CMB power spectrum likelihood analyses and run a case study using a Simons Observatory (SO) Large Aperture Telescope-like experimental setup and within the public SO software stack. To quantify the impact, we assume that beam chromaticity is present in simulated spectra but omitted in the likelihood analysis. We find that, for passbands of fractional width \Delta\nu/\nu\sim 0.2, neglecting this effect leads to significant biases, with astrophysical foreground parameters shifting by more than 2\sigma and cosmological parameters by significant fractions of the error.","The increase in precision in Cosmic Microwave Background (CMB) observations which we have witnessed over the last three decades [see e.g., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] has demanded a large effort in developing data analysis pipelines able to account for precise characterization of the instruments and of the sky emission. This work is essential to make sure that the very tight constraints set on cosmological models are robust and unbiased. For example, the subsequent releases of the Planck mission data (from the initial early survey results to the latest NPIPE products) have seen the deployment of many techniques to reduce systematics arising from e.g., uncertain instrument performance or the scanning strategy, and contamination from Galactic emission [11, 12, 13, 14, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]. While the Planck satellite focused on getting the most robust large scale modes (\ell\lesssim 1500-2000), from the ground, experiments like the Atacama Cosmology Telescope (ACT) and the South Pole Telescope (SPT) have refined intermediate and small angular scales (1000\lesssim\ell\lesssim 10000). The requirement for those scales is to tackle in particular the astrophysical foreground emission from extra-galactic sources and other unresolved signals [25, 6, 26, 27], and the frequency-dependent systematics effects which couple with them. If ignored, these systematics can lead to incorrect estimates of foreground parameters – preventing their astrophysical and cosmological interpretation [28, 29, 30, 31, 32, 33, 34]– and potentially introduce biases in the estimation of cosmological parameters. Next-generation ground-based experiments like the Simons Observatory (SO) [35] and CMB-S4 [36] are actively developing these aspects of the analysis pipeline [37, 38, 39, 40, 41]. A crucial ingredient for a correct estimate of CMB and foreground emission is the accurate knowledge of the beam, which represents the optical response of the instrument. Its width encodes the resolution of the instrument and its azimuthal and polar profile depends on the entire optical chain of the instrument. It is customary, in particular for high-resolution analyses, to use the assumption of azimuthally-symmetric beams [42], which holds well for the main beam component and simplifies the beam to its radial profile b(\theta), function of the polar angle \theta. This works in the case of a redundant scanning strategy, where each pixel is observed from many angles and the averaged beam gets symmetrized. Usually beam profiles are estimated employing observations of planets or known sources [43, 44] and, for broad passbands, the resulting beams can strongly depend on source spectral type. Thus, the effective beam profile may vary for different sources of emissions. However, in many previous cosmological analyses this effect has often been neglected [22, 45] or not included in the baseline analysis because too small to significantly impact cosmological parameter inference111The ACT DR4 analysis computed an approximate color-correction to adjust the beam estimated from Uranus observations to the beam appropriate for the CMB and used those in the baseline likelihood. Color-corrections for the other sky components were neglected. Tests assessing the impact of beam chromaticity were done as a robustness test of the cosmology results [6]; finding negligible impact on cosmological parameters and O(\lesssim 1\sigma) in astrophysical parameters, the extended beam modeling was not used in the baseline likelihood. [5, 6], particularly given the sensitivity or the multipole range of the experiment. Only some recent component separation studies [46, 38] included this effect for the first time in the baseline analysis settings. As we show in this paper, with the increasing sensitivity of upcoming experiments, it will be essential to accurately measure and incorporate the frequency dependence of beam profiles into the analysis pipeline. This will allow proper modeling of the observed power spectra and ensure an unbiased recovery of both cosmological and foreground parameters. In this work, we lay out the formalism for the integration of beam chromaticity in the power spectrum and likelihood analysis of a CMB experiment, and show the potential impact of this effect on the recovery of parameters from an SO Large Aperture Telescope (LAT)-like experiment. We derive the mathematics needed for the integration of this term in the calculation of the foreground Spectral Energy Distributions (SEDs), and we implement the modeling of the chromatic beams in the public SO power spectrum likelihood code LAT_MFLike222LAT_MFLike, version 1.0.0 and its foreground spectrum library fgspectra333fgspectra, version 1.3.0. The paper is organized as follows. We describe our formalism in Section II, then present its implementation and results on how the chromatic beam effect can bias cosmological and foreground parameters in Section III. We then draw conclusions in Section IV."
https://arxiv.org/html/2411.10076v1,Primordial blackhole formation: Exploring chaotic potential with a sharp step via the GLMS perspective,"A sharp step on a chaotic potential can enhance primordial curvature fluctuations on smaller scales to the \mathcal{O}(10^{-2}) to form primordial black holes (PBHs). The present study discusses an inflationary potential with a sharp step that results in the formation of PBHs in four distinct mass ranges. Also this inflationary model allows the separate consideration of observable parameters n_{s} and r on the CMB scale from the physics at small scales, where PBHs formation occur. In this work we computed the fractional abundance of PBHs (f_{PBH}) using the GLMS approximation of peak theory and also the Press-Schechter (PS) formalism. In the two typical mass windows, 10^{-13}M_{\odot} and 10^{-11}M_{\odot}, f_{PBH} calculated using the GLMS approximation is nearly equal to 1 and that calculated via PS is of 10^{-3}. In the other two mass windows 1M_{\odot} and 6M_{\odot}, f_{PBH} obtained using GLMS approximation is 0.01 and 0.001 respectively, while f_{PBH} calculated via PS formalism yields 10^{-5} and 10^{-6}. The results obtained via GLMS approximation are found to be consistent with observational constraints. A comparative analysis of f_{PBH} obtained using the GLMS perspective and the PS formalism is also included.","The detection of gravitational waves (GWs) [2] from the merger of binary black holes marked a significant milestone in the field of astrophysics and marked the beginning of the era of multimessenger astronomy [3]. GWs are ripples in space-time that are produced by accelerating massive objects, such as binary black hole systems. These GWs [4] were first predicted by Albert Einstein in his general theory of relativity in 1915 [5]. However, it took decades of technological advancements to detect these subtle distortions in space-time. The advent of gravitational wave (GW) detectors like the Laser-Interferometer GW Observatory (LIGO) and Virgo paved a new way to observe these events. The first detection of GWs occurred on September 14, 2015, by the LIGO observatories. This signal was generated by the merger of two binary black holes located around 1.3 billion light years away [3]. These GWs travel freely through the universe once generated and act as a powerful tool to explore the early universe. Various potential sources for the origin of GWs are investigated, including reheating after inflation [6, 7, 8], phase transitions [9, 10], topological defects [11, 12], etc. Moreover, GWs originating from various distinct sources are uncorrelated, leading to the generation of a stochastic GW background. Consequently, the analysis of GW signals observed by various pulsar timing array (PTA) experiments, such as NANOGrav [13], EPTA [14], InPTA, PPTA and CPTA [15] in the nHz frequency range pointed towards alternative cosmological explanations like GWs generated by cosmic strings or PBHs [16]. These signals, believed to have originated during the inflationary phase [17, 18, 19] of the universe, require further clarification through future PTA observations [20]. Additionally, incorporating data from GW standard sirens into existing cosmic models enhances precision in determining the interaction strength between dark matter and dark energy [21]. In the context of inflationary cosmology, if the scalar perturbations are large enough on small scales, this can result in the production of abundant PBHs. This situation could arise if the inflationary perturbation spectrum showed a non-Gaussianity and substantial blue tilt [22], or alternatively if the inflaton field experienced a slower roll for a particular duration of time that was much shorter than the entire inflationary phase [23, 24, 25, 26]. Due to Hawking radiation, PBHs with a mass smaller than approximately 5\times 10^{-19}M_{\odot} have already undergone evaporation. However, PBHs with a mass greater than this threshold can remain in stable existence to the present day [27]. The PBHs formed during the early epoch of our universe could have significant implications [28, 29], as they might seed the formation of supermassive black holes in galactic nuclei and AGN’s [30, 31], influence the ionization history of the universe [32, 33] and contribute to the overall density of dark matter [34, 35]. The abundances of PBHs denoted as f_{PBH} is characterised by its fraction within the current dark matter content. When f_{PBH}\approx 0.1, PBHs become a plausible candidate for dark matter [36, 37]. If f_{PBH}<<10^{-3} their potential as a dark matter candidate within the specified mass range can be excluded. Furthermore, recent studies have shown that, considering the quantum effects such as the memory burden effect, semiclassical evaporation constraints on PBHs are altered and this effect slows down the PBHs evaporation, thereby allowing those with masses below 10^{9}g to survive until the present day and significantly contribute to dark matter [38, 39]. Thus the ultralight PBHs with masses below 10^{9}g can have significant implications for dark matter content and GW phenomology. PBHs with masses 10^{9}g evaporate before big-bang nucleosynthesis (BBN) and can temporarily dominate the energy density of universe, leading to significant small scale density fluctuations and can induce stochastic gravitational wave background [40, 41]. Also the memory burden effect, where Hawking evaporation slows after a PBH has lost about half its mass, extends the PBH’s lifetime and enhances the GW signal [42, 43]. Within the framework of single field inflationary models, the formation of PBHs is plausible when the potential exhibits characteristics like a nearby inflection point or a saddle-like region. This feature slows the motion of the inflaton field, resulting in an intensified peak within the perturbation spectrum [44, 45, 46, 47]. In the present work, we consider a small step-like feature in the base inflationary potential V_{b}(\phi). This step effectively acts like a speed breaker by locally slowing the scalar field motion. Consequently, this leads to a sharp increase in the power spectrum at least to the \mathcal{O}(10^{-2}) and inducing stochastic GW background. Thus, the inherent localised nature of the speed breaker mechanism allows the generation of PBHs across a broad spectrum of masses spanning from the extremely light weight, 10^{-17}M_{\odot} to the immensely massive 10^{2}M_{\odot}. Note also that this inflationary model with sharp step can produce ultralight PBHs which have significant contribution to the totality of dark matter and are associated with a rich GW phenomology. Remarkably, this inflationary mechanism allows for a wide range of PBH masses without significantly changing n_{s} and r on the cosmic microwave background (CMB) scales. We calculate the f_{PBH} using an approximate method of peak theory (GLMS approximation) [48] and Press-Scheter (PS) theory [49] and then compare the results. The structure of our article is as follows. In Section 2 we consider the chaotic inflationary model featuring a step in its potential, which has a brief period of ultra-slow roll inflation. The potential parameters are selected such that the feature enhances the curvature perturbations at small scale, which is essential for PBHs formation, with out affecting the key observational inflationary parameters like scalar spectral index n_{s} and tensor to scalar ratio r on CMB scales. The production of PBH and its mass are discussed in Section 3. Section 4 covers the fractional abundance of PBHs due to this chaotic inflationary model with sharp step, for four distinct mass windows of PBHs using the GLMS approximation of the peak theory formalism and also in the PS formalism. In Section 5, the summary and conclusions are discussed."
https://arxiv.org/html/2411.09735v1,"Gas thermodynamics meets galaxy kinematics:
Joint mass measurements for eROSITA galaxy clusters","The mass of galaxy clusters is a critical quantity for probing cluster cosmology and testing theories of gravity, but its measurement could be biased given assumptions are inevitable in order to make use of any approach. In this paper, we employ and compare two mass proxies for galaxy clusters: thermodynamics of the intracluster medium and kinematics of member galaxies. We select 22 galaxy clusters from the cluster catalog in the first SRG/eROSITA All-Sky Survey (eRASS1) that have sufficient optical and near-infrared observations. We generate multi-band images in the energy range of (0.3, 7) keV for each cluster, and derive their temperature profiles, gas mass profiles and hydrostatic mass profiles using a parametric approach that does not assume dark matter halo models. With spectroscopically confirmed member galaxies collected from multiple surveys, we numerically solve the spherical Jeans equation for their dynamical mass profiles. Our results quantify the correlation between dynamical mass and line-of-sight velocity dispersion, \log M_{\rm dyn}=(1.296\pm 0.001)\log(\sigma_{\rm los}^{2}r_{\rm proj}/G)-(3.8% 7\pm 0.23), with an rms scatter of 0.14 dex. We find the two mass proxies lead to roughly the same total mass, with no observed systematic bias. As such, the \sigma_{8} tension is not specific to hydrostatic mass or weak lensing shears, but also appears with galaxy kinematics. Interestingly, the hydrostatic-to-dynamical mass ratios decrease slightly toward large radii, which could possibly be evidence for accreting galaxies in the outskirts. We also compare our hydrostatic masses with the latest weak lensing masses inferred with scaling relations. The comparison shows the weak lensing mass is significantly higher than our hydrostatic mass by \sim110%. This might explain the significantly larger value of \sigma_{8} from the latest measurement using eRASS1 clusters than almost all previous estimates in the literature. Finally, we test the radial acceleration relation (RAR) established in disk galaxies. We confirm the missing baryon problem in the inner region of galaxy clusters using three independent mass proxies for the first time. As ongoing and planned surveys are providing deeper X-ray observations and more galaxy spectra for cluster members, we expect to extend the study to cluster outskirts in the near future.","Studying the dynamics of galaxy clusters provides a useful way to constrain the nature of dark matter, as the cold dark matter model predicts that dark matter halos are in a universal profile (Navarro et al., 1996). It also helps test the universality of scaling relations that are mostly established in galaxies, such as the Baryonic Tully-Fisher relation (McGaugh et al., 2000) and the radial acceleration relation (RAR, McGaugh et al., 2016; Lelli et al., 2017a; Li et al., 2018). Both scientific objectives require robust measurements of cluster mass profiles, which is non-trivial, as it often suffers from both technical and observational difficulties. Table 1: Properties of the 22 selected clusters. 1eRASS Cluster zspec {\rm RA_{BCG}} {\rm Dec_{BCG}} {\rm RA_{X-ray}} {\rm Dec_{X-ray}} N_{\rm spec} N_{\rm eff} R_{\rm 500,wl} M_{\rm 500,wl} M_{\rm 500,hydro} (deg.) (deg.) (deg.) (deg.) (kpc) (10^{13}M_{\odot}) (10^{13}M_{\odot}) J004049.8-440743 0.3493 10.2082 -44.1307 10.2077 -44.1298 51 43.1 1250{}^{+40}_{-35} 79.9{}^{+8.0}_{-6.6} 26.9{}^{+38.9}_{-12.8} J004207.1-283154 0.1075 10.5370 -28.5357 10.5328 -28.5334 45 39.4 1258{}^{+37}_{-28} 62.6{}^{+5.7}_{-4.1} 28.6{}^{+18.9}_{-9.0} J024339.2-483339 0.4983 40.9121 -48.5608 40.9115 -48.5612 33 26.6 1248{}^{+38}_{-28} 94.7{}^{+8.9}_{-6.4} 31.8{}^{+37.1}_{-14.3} J034656.2-543854 0.5286 56.7311 -54.6486 56.7329 -54.6475 45 37.3 1048{}^{+37}_{-35} 58.1{}^{+6.4}_{-5.7} 24.5{}^{+30.5}_{-12.3} J043817.8-541917 0.4218 69.5737 -54.3223 69.5736 -54.3218 55 48.8 1292{}^{+38}_{-28} 95.9{}^{+8.9}_{-6.0} 85.6{}^{+51.5}_{-32.2} J052806.2-525951 0.7681 82.0222 -52.9981 82.0220 -52.9973 33 25.0 752{}^{+50}_{-61} 28.6{}^{+6.0}_{-6.4} 16.1{}^{+35.0}_{-9.7} J055942.9-524851 0.6087 89.9301 -52.8242 89.9285 -52.8224 42 33.6 984{}^{+52}_{-68} 52.8{}^{+8.9}_{-10.2} 30.1{}^{+44.1}_{-17.6} J073220.0+313748 0.1705 113.0846 31.6335 113.0828 31.6301 78 62.4 1358{}^{+37}_{-32} 84.2{}^{+7.1}_{-5.8} 15.3{}^{+8.3}_{-3.7} J073721.1+351739 0.2094 114.3372 35.2949 114.3365 35.2947 56 47.5 1297{}^{+44}_{-37} 76.5{}^{+8.0}_{-6.4} 33.4{}^{+39.6}_{-16.0} J080056.9+360324 0.2866 120.2367 36.0565 120.2375 36.0577 97 80.4 1360{}^{+39}_{-33} 96.0{}^{+8.3}_{-6.8} 62.9{}^{+70.9}_{-33.6} J082317.9+155700 0.1529 125.8304 15.9627 125.8297 15.9578 66 57.9 991{}^{+43}_{-38} 32.2{}^{+4.4}_{-3.6} 15.2{}^{+30.3}_{-9.4} J084257.5+362208 0.2818 130.7399 36.3665 130.7412 36.3663 125 107.6 1481{}^{+36}_{-31} 123.1{}^{+9.4}_{-7.6} 107.3{}^{+69.7}_{-47.1} J090131.5+030055 0.1936 135.3795 3.0157 135.3786 3.0149 51 41.8 1091{}^{+39}_{-38} 44.8{}^{+4.9}_{-4.5} 30.9{}^{+20.5}_{-9.7} J101703.2+390250 0.2048 154.2652 39.0471 154.2645 39.0487 77 63.2 1373{}^{+30}_{-38} 90.2{}^{+6.1}_{-7.3} 15.5{}^{+5.8}_{-3.9} J115518.0+232422 0.1422 178.8250 23.4049 178.8257 23.4057 59 54.6 1422{}^{+33}_{-28} 93.9{}^{+6.7}_{-5.4} 48.7{}^{+35.1}_{-17.7} J121741.6+033931 0.0773 184.4214 3.6559 184.4221 3.6578 61 56.8 1252{}^{+33}_{-22} 60.1{}^{+4.9}_{-3.0} 37.5{}^{+15.9}_{-8.9} J125922.4-041138 0.0843 194.8438 -4.1961 194.8441 -4.1944 61 55.5 1323{}^{+31}_{-32} 71.4{}^{+5.2}_{-5.1} 49.5{}^{+27.7}_{-15.7} J130252.8-023059 0.0838 195.7191 -2.5164 195.7180 -2.5165 58 51.2 1014{}^{+23}_{-26} 32.1{}^{+2.3}_{-2.3} 14.1{}^{+24.9}_{-5.1} J213056.8-645842 0.3163 322.7342 -64.9779 322.7398 -64.9789 37 31.3 1079{}^{+56}_{-39} 49.5{}^{+8.2}_{-5.1} 32.2{}^{+42.3}_{-18.4} J213536.8-572622 0.4268 323.9060 -57.4419 323.9046 -57.4404 29 27.5 1021{}^{+66}_{-65} 47.7{}^{+9.8}_{-8.6} 15.9{}^{+33.9}_{-8.9} J213800.9-600758 0.3188 324.5035 -60.1317 324.5031 -60.1333 28 26.5 1320{}^{+34}_{-48} 91.0{}^{+7.2}_{-9.6} 34.3{}^{+55.7}_{-17.8} J235137.0-545253 0.3838 357.9086 -54.8817 357.9078 -54.8830 29 26.5 1032{}^{+66}_{-57} 46.8{}^{+9.5}_{-7.4} 20.5{}^{+40.9}_{-12.1} 111The sample is selected from Bulbul et al. (2024). The weighted spectroscopic redshift z_{\rm spec}, the coordinates of BCGs ({\rm RA_{BCG}}, {\rm Dec_{BCG}}), and best-fit X-ray centers ({\rm RA_{X-ray}}, {\rm Dec_{X-ray}}) are from Kluge et al. (2024) and Bulbul et al. (2024), respectively. Nspec is the number of available spectroscopic member galaxies, while N_{\rm eff} is the corresponding effective number of galaxy spectra, taking into account membership probabilities. The cluster radius R_{\rm 500,wl} and total mass M_{\rm 500,wl} are derived from scaling relations, as presented in Ghirardini et al. (2024). M_{\rm 500,hydro} is the hydrostatic mass derived in this paper. The commonly employed dynamical tracer to measure cluster mass profiles is the hot cluster gas via the assumption of hydrostatic equilibrium. Hot cluster gas emits X-ray photons, so they can be measured with X-ray observations. The first all-sky X-ray telescope ROSAT led to the construction of the Northern (NORAS) catalog of 495 clusters (Böhringer et al., 2000) and the Southern (REFLEX) catalog of 447 clusters (Böhringer et al., 2004). There are smaller sky-coverage but deeper observations with the XMM-Newton telescope, such as the XMM-Newton Cluster Survey (XCS, Mehrtens et al., 2012) for 503 galaxy clusters and the XMM-Newton Cluster Archive Super Survey (X-CLASS Koulouridis et al., 2021) for 1646 galaxy clusters. Deep follow-up programme are limited to tens to hundreds of galaxy clusters, such as the X-COP project (Eckert et al., 2017) and the CHEX-MATE project (CHEX-MATE Collaboration et al., 2021). A big milestone is made by the latest all-sky survey telescope, eROSITA (Predehl et al., 2021), which has observed more than 1.2 million X-ray sources (Merloni et al., 2024), and identified more than 26 thousand extended sources (Bulbul et al., 2024) in the first SRG/eROSITA All-Sky Survey (eRASS1). More than 12 thousand extended sources have been identified by optical observations (Kluge et al., 2024), leading to the largest catalog of galaxy clusters with X-ray data available. The significant increase in the number of X-ray selected clusters suggests that studying cluster dynamics in a statistical way is becoming more and more promising. Cluster galaxies have also been used as dynamical tracers assuming they are in dynamical equilibrium. The total cluster mass can be estimated by building a scaling relation between the total velocity dispersion of cluster galaxies and the virial mass via simulations (Biviano et al., 2006; Munari et al., 2013; Saro et al., 2013). To derive dynamical mass profiles, one needs to solves the Jeans equation (e.g. Binney & Tremaine, 2008). However, this approach suffers two technical difficulties. First, dynamical mass is related to radial velocity dispersion and 3-D galaxy distribution through spherical Jeans equation (Binney & Tremaine, 2008), but observationally only projected spatial distributions and line-of-sight velocity dispersion of member galaxies are measurable. Second, there is a serious degeneracy between dynamical mass and velocity anisotropy. Degeneracy is a general problem in astronomy, which can lead to unrealistic estimations of parameters and even result in absurd conclusion (e.g. see Li et al., 2021). Mamon et al. (2013); Biviano et al. (2013) proposed to tackle these problems by parameterizing mass profiles, tracer distribution functions, and velocity anisotropy profiles using 3D functions. These functions are determined by matching their projected forms to observed phase-space information. Li et al. (2023a) also used parameterized profiles but with more flexible functions, and addressed the mass-velocity anisotropy degeneracy by introducing additional constraints that involve two virial shape parameters, namely the fourth order of velocity dispersion (Merrifield & Kent, 1990). As a result, the dynamical mass profiles can be well derived (see Appendix B in Li et al., 2023a). Both approaches assume galaxy clusters are relaxed, which cannot be independently, robustly verified. An interesting fact is that the two approaches assume different relaxations, one on hot intracluster gas via thermodynamics, and the other on member galaxies through kinematic motions. It is therefore interesting to compare these two mass estimators in a statistical sense. This is particularly interesting because of the \sigma_{8} tension (Planck Collaboration et al., 2014), since the cluster mass measurements using hot gas are thought to be biased lower due to non-thermal flows (e.g. Lau et al., 2009; Vazza et al., 2009; Battaglia et al., 2012; Nelson et al., 2014; Shi et al., 2015; Biffi et al., 2016) and thereby lead to lower matter fluctuation compared with that from cosmic microwave background fluctuations (Planck Collaboration et al., 2016b). In this paper, we select a subsample of galaxy clusters from the cluster catalog in the first all-sky survey of eROSITA (Bulbul et al., 2024) based on their optical properties (Kluge et al., 2024). We measure their cluster mass profiles using both hot intracluster gas and cluster galaxies as dynamical tracers and make a systematic comparison between these two mass proxies. The paper is organized as follows: Section 2 describes our sample selection criteria; Section 3 & 4 present the results of X-ray analysis and kinematics analysis, respectively; Section 5 compares the mass profiles measured with different estimators; Section 6 shows a test for the universality of the RAR; Section 7 discusses the results and concludes the paper."
https://arxiv.org/html/2411.10366v1,Numerical gravitational backreaction on cosmic string loops from simulation,"We report on the results of performing computational gravitational backreaction on cosmic string loops taken from a network simulation. The principal effect of backreaction is to smooth out small-scale structure on loops, which we demonstrate by various measures including the average loop power spectrum and the distribution of kink angles on the loops. Backreaction does lead to self-intersections in most cases, but these are typically small. An important effect discussed in prior work is the rounding off of kinks to form cusps, but we find that the cusps produced by that process are very weak and do not significantly contribute to the total gravitational-wave radiation of the loop. We comment briefly on extrapolating our results to loops as they would be found in nature.","Cosmic strings are one-dimensional topological defects formed in the early universe by a spontaneously-broken symmetry [1] with a non-simply-connected vacuum manifold. (See [2] for a review.) They are a generic prediction of grand unified models of particle physics [3], and can also arise from the collision of D-branes in a string-theoretic model [4, 5]. The symmetry-breaking endows the strings with a mass; depending on the details of the breaking, they might also carry currents [6] or be part of a hybrid network with other defects [7, 8, 9]. In addition, strings are called global or local (a.k.a. gauge) based on the kind of symmetry which is broken. Regardless of these details, it is generally accepted that the symmetry breaking produces a network of strings which fills the universe. While there has not yet been a detection of cosmic strings, predictions of detectable signals (and constraints based on non-observations) rely principally on the characteristics of this network and the loops within it. The detection of gravitational waves is one of the most promising channels for finding cosmic strings, particularly now that we are in the era of gravitational-wave astronomy, and many gravitational-wave observatories are searching for strings [10, 11, 12, 13, 14, 15, 16, 17]. In addition to sourcing gravitational waves that we might observe, gravitational effects of the string act on the loop itself in a process termed gravitational backreaction. This self-interaction changes the shape of the loop [18], in turn changing the pattern of gravitational waves emitted. Thus, an understanding of gravitational backreaction’s effects on loops is critical for making precise predictions about potentially-observable signals from cosmic strings. However, solving this problem analytically is intractable. Even for simple models of cosmic string shapes, exact solutions are known only for a few cases [19, 20, 21, 22, 23]. In a realistic network, the typical loop’s shape is complex [24] and not easily described by simple mathematical functions. Any large-scale study of how loops in such a network evolve must be done computationally. This of course brings its own problems, namely computational time complexity, which require the development of specialized methods and codes [22, 25]. In this paper, we present the results of numerically evolving loops taken from large network simulations, under gravitational backreaction. We focus on gauge strings which are coupled only to gravity. In Sec. II, we review some useful properties of strings, discuss how we represent strings in our code, and introduce the corpus of loops we study in the remainder of the work. In Sec. III, we review how the numerical evolution is done, discuss the computational effort involved, and summarize the evolution of the realistic loops we studied. In Sec. IV, we discuss how we compute the gravitational radiation power from our loops. In Sec. V, we discuss backreaction’s effects on self-intersections (V.1), loop power spectra (V.2), the formation of cusps (V.3), and smoothing (V.4); in addition, we discuss how to extrapolate our results to loops as they might be found in nature (V.5). We conclude in Sec. VI. We work in units where the speed of light and \hbar are taken to be 1."
https://arxiv.org/html/2411.10216v1,On the dark matter origin of an LDMX signal,"Fixed target experiments where beam electrons are focused upon a thin target have shown great potential for probing new physics, including the sub-GeV dark matter (DM) paradigm. However, a signal in future experiments such as the light dark matter experiment (LDMX) would require an independent validation to assert its DM origin. To this end, we propose to combine LDMX and next generation DM direct detection (DD) data in a four-step analysis strategy, which we here illustrate with Monte Carlo simulations. In the first step, the hypothetical LDMX signal (i.e. an excess in the final state electron energy and transverse momentum distributions) is recorded. In the second step, a DM DD experiment operates with increasing exposure to test the DM origin of the LDMX signal. Here, LDMX and DD data are simulated. In the third step, a posterior probability density function (pdf) for the DM model parameters is extracted from the DD data, and used to predict the electron recoil energy and transverse momentum distributions at LDMX. In the last step, predicted and recorded electron recoil energy and transverse momentum distributions are compared in a chi-square test. We present the results of this comparison in terms of a threshold exposure that a DD experiment has to operate with to assert whether predicted and recorded distributions can be statistically dependent. We find that this threshold exposure grows with the DM particle mass, m_{\chi}. It varies from 0.012 kg-year for a DM mass of m_{\chi}=4 MeV to 1 kg-year for m_{\chi}=25 MeV, which is or will soon be within reach.","There is mounting interest in dark matter (DM) models where the DM particle is lighter than a GeV, and lies in the same mass range of the known constituents of matter, such as electrons, protons and neutrons [1, 2, 3]. The reason for this increased interest is twofold. On the one hand, a DM candidate in this mass range would carry a kinetic energy smaller than \sim 10^{-6}m_{\chi} in our galaxy, where m_{\chi} is the DM mass, and would thus “by construction” evade the increasingly strong constraints from direct DM searches in nuclear recoil experiments [4]. On the other hand, a DM candidate in this mass range could also be produced in the early universe via the canonical freeze-out mechanism [5], as long as the relevant number-changing processes involve the exchange of a new particle mediator, so that the Lee-Weinberg bound [6] (that applies to weak-scale processes) can be circumvented. This latter observation in particular implies that the search for sub-GeV DM is tightly related to the search for new mediator particles. New particle mediators can be searched for at fixed target experiments, where a beam of particles (electrons or protons) collides with a target at rest, producing the new mediator in, e.g.: 1) “dark” bremsstrahlung processes, where the new particle replaces the ordinary final state photon, or 2) meson decays, where the new mediator is produced in \pi^{0} or \eta decays [7]. Once produced, the new mediator particle can decay visibly into Standard Model particles, or invisibly into DM particles, which illustrates the aforementioned interplay between mediator and DM particle searches in the sub-GeV DM context. In the next generation fixed target experiment LDMX [8], 4 – 8 GeV beam electrons are focused upon a thin tungsten target, and the hypothetically produced new particle mediator is searched for by full reconstruction of the final state electron recoil energy and transverse momentum distributions. The prospects for new mediator particle or DM production at LDMX are reviewed in [9] (and references therein), and a recent extension of the LDMX analysis framework to spin-1 DM is discussed in [10]. Importantly, if an excess above the expected inclusive single electron background is found at LDMX in the recorded electron recoil energy and transverse momentum distributions, there is no guarantee that this excess is related to DM. Indeed, from this LDMX signal one could only infer that an invisible particle carrying a significant fraction of the initial beam energy has been produced. Whether or not this new particle is associated with DM, and plays a key role in the DM cosmological production is a question that would have to be addressed separately. In this work, we propose a strategy that can be used to test the DM origin of a future LDMX signal. Besides the LDMX signal itself, our proposal relies on information that can be extracted from next generation DM direct detection experiments, and consists of four steps. In the first step, the hypothetical LDMX signal is recorded (or, to illustrate our approach, simulated). In the second step a DM direct detection experiment operates with increasing exposure, and parameter inference is performed on the recorded data to extract posterior probability density function (pdf) and associated credible regions for the DM mass and coupling constants. Here, we perform Monte Carlo simulations of electron/hole pair production in a silicon target to emulate the operation of a next generation, DM direct detection experiment. In the third step, the posterior pdf obtained from DM direct detection data is used to predict the electron recoil energy and transverse momentum distributions that are expected at LDMX based on the outcome of the DM direct detection experiment in question. Obviously, the predicted distributions will depend on the assumed experimental exposure. In the last step, predicted and observed electron recoil energy and transverse momentum distributions are compared in a chi-square test to determine whether the two distributions have been sampled from different underlying models. This last step allows us to find the exposure that is required to assert the compatibility of the hypothetical LDMX signal with a DM origin. This strategy highlights the strong complementarity of mediator particle searches at fixed target experiments and DM searches at direct detection experiments. Our work is organised as follows. In Sec. 2 we review the particle physics model used to describe DM in our analysis strategy (the four steps of which were outlined above). In Sec. 3 and Sec. 4 we introduce the simulation and statistical frameworks used to implement our strategy. The numerical results are presented in Sec. 5, while we summarise and conclude in Sec. 6."
https://arxiv.org/html/2411.09765v1,The Neutrino Mass Bound from Leptogenesis Revisited,"We revisit unflavoured leptogenesis in the seesaw model applying recent improvements in the computation of CP-conserving and CP-violating equilibration rates. These are relevant for the relativistic regime of the sterile Majorana fermions and the dynamics of the Standard Model particles acting as spectator processes. In order to probe the regime of large ({\cal O}(10^{2})) washout parameters, we add \Delta L=2 washout processes, which we derive in the CTP-formalism. We then perform a parameter scan of the final baryon asymmetry and find a constraint m_{\text{lightest}}\lesssim$0.15\text{\,}\mathrm{e}\mathrm{V}$ on the absolute neutrino mass scale, which is slightly less stringent than previously reported bounds obtained without the aforementioned improvements. The relaxation of the bounds is mainly due to partially equilibrated spectator fields, which protect part of the asymmetry from washout and lead to larger final asymmetries.","Leptogenesis is a framework that connects two of the long-standing problems of the Standard Model: the origin of neutrino masses and of the baryon asymmetry of the Universe (BAU). If neutrino masses are produced through the coupling to a Majorana fermion, its out-of-equilibrium decay could produce a lepton asymmetry in the early Universe, which would then be converted into a baryon asymmetry via sphaleron processes. One of the first and most compelling proposals to explain the neutrino masses is the seesaw mechanism, in which active neutrinos couple to heavy Majorana fermions via the Higgs boson. One can then find that large Majorana masses naturally explain the smallness of neutrino masses. The simplest scenario of leptogenesis in the seesaw model is the case of strongly hierarchical Majorana fermions M_{2},M_{3}\gg M_{1} without flavour effects. In this setup, an upper bound on the neutrino masses, parametrized by the lightest neutrino mass m_{\text{lightest}}\lesssim$0.12\text{\,}\mathrm{e}\mathrm{V}$ was found [1, 2, 3]. This bound is in agreement with cosmological bounds on neutrino masses, with the constraint \sum m_{\nu}<$0.12\text{\,}\mathrm{e}\mathrm{V}$ (95% C.L.) from Planck [4], corresponding to m_{\text{lightest}}<0.03(0.016)\,$\text{\,}\mathrm{e}\mathrm{V}$ in normal (inverted) hierarchy, while DESI [5] further tightened this constraint to \sum m_{\nu}<$0.072\text{\,}\mathrm{e}\mathrm{V}$ (95% C.L.), corresponding to m_{\text{lightest}}<$0.0086\text{\,}\mathrm{e}\mathrm{V}$ in normal hierarchy and below the threshold for inverted hierarchy. However, given the many tensions in cosmological data and between cosmological and terrestrial constraints, the robustness of these bounds is yet to be confirmed [6, 7, 8, 9]. In view of this, the best model-independent constraint is given by the KATRIN experiment, which placed an upper bound on the effective electron antineutrino mass m_{\text{lightest}}\approx m_{e}=\sqrt{\sum_{i}|U_{ei}|^{2}m_{i}^{2}}<$0.8% \text{\,}\mathrm{e}\mathrm{V}$ (90% C.L.) [10]. Additionally, in the absence of cancellations due to new physics effects, KamLAND-Zen also places a constraint m_{\text{lightest}}<0.18-$0.48\text{\,}\mathrm{e}\mathrm{V}$ (90% C.L.) from neutrinoless double beta decay assuming Majorana masses [11]. While KamLAND-Zen has since obtained a stronger contraint on the effective Majorana mass \braket{m_{\beta\beta}} [12], its translation into a bound on m_{\text{lightest}} depends on the mass hierarchy. Given recent improvements on the computation of the fluid equations for leptogenesis [13], it is interesting to investigate whether this allows us to accomodate larger neutrino masses in leptogenesis. As far as the dynamics of leptogenesis is concerned, a value of m_{\text{lightest}}\approx$0.14\text{\,}\mathrm{e}\mathrm{V}$ that we find in our analysis pushes M_{1}\gtrsim$5\text{\times}{10}^{12}\text{\,}\mathrm{e}\mathrm{V}$. At the corresponding temperatures, tau-Yukawa couplings are out of"
https://arxiv.org/html/2411.09747v1,Thermo-Coupled Early Dark Energy,"Early dark energy solutions to the Hubble tension introduce an additional scalar field which is frozen at early times but becomes dynamical around matter-radiation equality. In order to alleviate the tension, the scalar’s share of the total energy density must rapidly shrink from \sim 10\% at the onset of matter domination to \ll 1\% by recombination. This typically requires a steep potential that is imposed ad hoc rather than emerging from a concrete particle physics model. Here, we point out an alternative possibility: a homogeneous scalar field coupled quadratically to a cosmological background of light thermal relics (such as the Standard Model neutrino) will acquire an effective potential which can reproduce the dynamics necessary to alleviate the tension. We identify the relevant parameter space for this “thermo-coupled” scenario and study its unique phenomenology at the background level, including the back-reaction on the neutrino mass. Follow-up numerical work is necessary to determine the constraints placed on the model by early-time measurements.","Since the emergence of precision cosmology over the past few decades, the \LambdaCDM model has had considerable success at codifying the structure and evolution of the Universe in concordance with large and robust data sets, demonstrating a sophisticated understanding of systematics. Even so, a central parameter of this model, the total expansion rate of the Universe H_{0}, has a value (presently 67-68\text{ km s}^{-1}\text{ Mpc}^{-1} [1, 2, 3]) that is discrepant with direct late-time observations (which instead suggest 70-75\text{ km s}^{-1}\text{ Mpc}^{-1} [4, 5, 6]). This tension has persisted for over a decade, leading many to consider modifications to \LambdaCDM that may rectify the situation. None so far is definitive [7, 8, 9, 10, 11, 12, 13]. One promising candidate to address the H_{0} tension is early dark energy (EDE) [14, 15, 16]. This is an additional component of the Universe that behaves like a cosmological constant while the Universe is radiation dominated. Upon peaking at \sim 10\% of the total energy density, it redshifts faster than radiation, becoming unobservable past the epoch of recombination. While there are many concrete realizations of this scenario, the net effect is to increase the value of the Hubble parameter that is inferred from measurements of the cosmic microwave background (CMB) and large-scale structure (LSS). Embedding EDE into a well-defined particle physics model poses a significant challenge [17]. It can be emulated by a scalar field that is frozen at some initial value, becoming dynamical and thereby damping its energy density once this energy comprises a substantial fraction of the total. In order to redshift away sufficiently rapidly, the scalar must evolve in an unusually steep potential, often of exotic origin, over the brief window between matter-radiation equality (MRE) and recombination. In this work, we explore how such behavior may instead emerge from a simple quadratic coupling to a cosmological background of fermions, focusing on the case of Standard Model (SM) neutrinos. Such a model is one possible realization of “thermo-coupled” EDE (TCEDE), where the scalar potential is dominated by a thermal contribution from the fermionic sector that sharply enhances the redshifting of the net energy density as required. A secondary effect is that the vacuum expectation value of the scalar back-reacts on the neutrino mass, resulting in a distinctive phenomenology for both components as the scalar time-evolves. These features are qualitatively similar to those found in quintessence models involving mass-varying neutrinos (MaVaNs) [18, 19, 20, 21], albeit with the dynamical behavior taking place around MRE rather than at late times. There is also a superficial resemblance to “neutrino-assisted” EDE [22, 23, 24], which involves scalar-neutrino interactions deriving from a conformal coupling to the metric. Such models effectively add a linear term to the scalar potential, aiming to explain why the energy density only becomes significant at MRE (sometimes considered a coincidence problem). However, they rely on a separate (usually quartic) bare potential to damp the scalar once it becomes active. By contrast, in this work the apparent coincidence is simply an initial condition, which may arise from misalignment or other UV physics that remains unspecified. The quadratic coupling alone suffices to reproduce the desired dynamics, and the corresponding impacts on the neutrino sector differ substantially. The paper is organized as follows. In Sec. II, we introduce our model, consisting of a new scalar and a fermion that is cosmologically abundant as a thermal relic. Restricting our attention to the SM neutrino, we then study the cosmological evolution of the system in Sec. III. We discuss the observational consequences on H_{0} and the neutrino sector in Sec. IV, and conclude in Sec. V."
https://arxiv.org/html/2411.09733v1,Fuzzy Gasoline: Cosmological hydrodynamical simulations of dwarf galaxy formation with Fuzzy Dark Matter,"We present the first set of high-resolution, hydrodynamical cosmological simulations of galaxy formation in a Fuzzy Dark Matter (FDM) framework. These simulations were performed with a new version of the gasoline2 code, known as fuzzy-gasoline, which can simulate quantum FDM effects alongside a comprehensive baryonic model that includes metal cooling, star formation, supernova feedback, and black hole physics, previously used in the NIHAO simulation suite. Using thirty zoom-in simulations of galaxies with halo masses in the range 10^{9}\lesssim M_{\text{halo}}/M_{\odot}\lesssim 10^{11}, we explore how the interplay between FDM’s quantum potential and baryonic processes influences dark matter distributions and observable galaxy properties. Our findings indicate that both baryons and low-mass FDM contribute to core formation within dark matter profiles, though through distinct mechanisms: FDM-induced cores emerge in all haloes, particularly within low-mass systems at high redshift, while baryon-driven cores form within a specific mass range and at low redshift. Despite these significant differences in dark matter structure, key stellar observables such as star formation histories and velocity dispersion profiles remain remarkably similar to predictions from the Cold Dark Matter (CDM) model, making it challenging to distinguish between CDM and FDM solely through stellar observations.","The Cold Dark Matter (CDM) model, characterized by its cold, dark, and collisionless nature, has been considered the leading framework for explaining the dark matter component in cosmic structure formation over the past few decades (see e.g. Mo et al., 2010, for a comprehensive review on the subject). Nonetheless, unresolved tensions at small scales, combined with the ongoing failure to detect Weakly Interacting Massive Particles (WIMPs) — the leading particle candidate of the CDM model — have continued to raise doubts about the model’s viability. Motivated by the elusiveness of WIMPs in predominant direct and indirect detection methods, several alternative dark matter models have come to the forefront, investigating the lower mass regimes for dark matter particles (Jungman et al., 1996). Moving away from the GeV/c2 mass range associated with WIMPs, these efforts explored and proposed several lighter dark matter particle candidates, one being the axion particle, which is theorized to arise from the CP-symmetry breaking in quantum chromo-dynamics (QCD) theories (Peccei & Quinn, 1977). In a cosmological context, a pseudo-scalar bosonic particle can be generalized from the QCD axion model, motivating a comprehensive class of axion-like particles (ALPs) acting as dark matter candidates. These ALPs span a broad range of masses, encompassing over 24 orders of magnitude from 10^{-24} to 1 eV/c2 (see e.g. Hui et al., 2017; Ferreira, 2021, for reviews on FDM models). Dark matter models related to ALP particle masses in the mass range (10^{-24} to 10^{-19} eV/c2) are known as Fuzzy Dark Matter (FDM) models, whose identifying boson mass m_{\chi} is typically represented in terms of m_{22}=m_{\chi}/(10^{-22}\text{ eV}/c^{2}). The mass range of FDM corresponds to de Broglie wavelengths on scales of \mathcal{O}(1\text{ kpc}), exhibiting wave-like behavior at sub-galactic scales (Hu et al., 2000). The quantum wave-like nature of FDM results in a net repulsive force that, on one hand, modifies the matter power spectrum of cold dark matter (CDM) during matter-radiation equality and smooths out density perturbations at small scales, ultimately leading to fewer collapsed structures (Hu et al., 2000; Marsh & Silk, 2014). On the other hand, it induces a resistance to gravitational collapse resulting in decreased dark matter (DM) distribution in the central region of FDM haloes. This effectively translates to FDM haloes featuring cored inner DM density profiles (\rho(r)\sim constant) contrasted with CDM’s cuspy inner DM density profiles (\rho(r)\sim r^{-1}) for dwarf galaxy systems (Hu et al., 2000). While the CDM model has been successful in modeling large-scale cosmological structures (Springel et al., 2005; Tegmark et al., 2006; Alam et al., 2017), several challenges have arisen on smaller, non-linear scales. These include well-known issues such as the cusp-core problem (Flores & Primack, 1994; Moore, 1994) and the missing satellites problem (Klypin et al., 1999; Moore et al., 1999) [see Bullock & Boylan-Kolchin (2017) for a detailed review]. Verifying the model’s validity at these non-linear scales has proven to be particularly challenging. In response, numerous studies have defended the CDM model, pointing out that earlier works overstated the severity of these problems due to theoretical and observational limitations. These studies emphasize the growing importance of baryonic physics in structure formation on smaller scales (Brooks & Zolotov, 2014; Macciò et al., 2020; Waterval et al., 2022), as well as the inefficiency of star formation in dwarf galaxies, which complicates their observational detection (Yang et al., 2003; Fitts et al., 2017; Frings et al., 2017). Previous studies investigating the role of baryonic feedback processes in dwarf galaxies have found that baryons are able to produce significant cores (\sim 1 kpc) in their dark matter distribution (Governato et al., 2010; Macciò et al., 2011; Benítez-Llambay et al., 2019). The most-widely accepted mechanism explaining this phenomenon is the sub-dynamical time-scale changes in the central (\sim\bigO(\text{kpc})) potential of the halo. These rapid changes in the central potential, caused by stellar and black hole feedback, are tied to strong gas outflows that irreversibly alter the central potential by transferring energy to collisionless DM particles (Pontzen & Governato, 2012). However, these baryonic effects help alleviate these small scale tensions only up to a certain mass scale (M_{halo} \sim 10{}^{10}M_{\odot}). Since these mechanisms are out of play in lowest mass, gas-deficient dark-matter dominated dwarf galaxies, the central DM distribution of the halo reverts back to the cuspy profiles (e.g. Tollet et al., 2016). The addition of FDM interaction to baryonic effects might help alleviate these tensions at lower halo masses while maintaining CDM large scale features. Numerical simulations of structure formation within FDM models have been initially performed by means of highly numerically intensive Adaptive Mesh Refinement (AMR) algorithms able to solve the Schrödinger-Poisson equations over a grid (see e.g. Schive et al., 2010, 2018; Mocz et al., 2017), leading to impressive and very detailed results on the properties of individual FDM collapsed objects (see e.g. Woo & Chiueh, 2009; Schive et al., 2014; Veltmaat et al., 2018). However, the computational cost of such approach hindered the possibility to extend the investigation of late time structure formation to large cosmological volumes. To address this issue, N-Body codes were employed, initially only including the (linear) suppression in the initial conditions but neglecting the integrated effect of the FDM interaction during the subsequent dynamical evolution (see e.g. Schive et al., 2016; Iršič et al., 2017; Armengaud et al., 2017) – i.e. basically treating FDM as standard dark matter with a suppressed primordial power spectrum. The inclusion of the typical FDM interaction in N-body codes was achieved with ax-gadget Nori & Baldi (2018), a modified version of the cosmological hydrodynamical code p-gadget3 that implemented the general scheme suggested by Mocz & Succi (2015) and Marsh (2015). The code ax-gadget allowed the investigation of FDM in larger cosmological volumes with a vast number of systems (Nori et al., 2019) as well as a variety of complex galactic systems with many evolving substructures (Nori & Baldi, 2020; Nori et al., 2023; Elgamal et al., 2024) hardly obtainable with other simulation strategies. Nonetheless, previous studies on FDM cosmologies with ax-gadget have all relied on dark-matter-only (DMO) simulations. To further investigate FDM models in a proper physical context and examine their impact on galaxy formation, this work expands on what has been done with ax-gadget since Nori & Baldi (2018) by incorporating baryonic effects in a cosmological hydrodynamical N-body code, which are essential for a correct description of structure formation. While effective in modeling FDM behavior, ax-gadget is limited in simulating baryonic processes like gas cooling, star formation, and black hole feedback. Conversely, gasoline2– another cosmological hydrodynamical code with a compatible N-body structure – has been constantly developed and integrated with new routines related to baryonic process, and has been shown to be very effective in these areas in the past years (e.g. Stinson et al., 2006; Brooks et al., 2013; Wang et al., 2015). By integrating the FDM routines from ax-gadget into gasoline2 (Wadsley et al., 2017), we have developed a new version of the gasoline2 code, fuzzy-gasoline, capable of running hydrodynamic FDM simulations with baryons through to the present day (z=0) of large and complex systems at a reasonable computational cost. To the authors knowledge, this is the first code of its kind capable to do so. In this work, we leverage the fuzzy-gasoline code to create novel hydrodynamical simulations of dwarf galaxy systems with halo masses in the range of 10^{9}\lesssim M_{\text{halo}}/M_{\odot}\lesssim 10^{11}. We detail their properties, including dark matter, gas and star density and velocity profiles, as well as star formation histories, and compare them with those of their cold dark matter (CDM) NIHAO counterparts. Our goal is to explore two key aspects: first, what is the combined effect of baryons and FDM on galactic properties, and second, whether it is possible to disentangle the degeneracy of the two individual contributions. The remainder of this paper is organized as follows: in Sec. 2 we provide an overview of the theoretical background of FDM models; in Sec. 3 we detail the numerical methodology implemented in this work, specifically related to FDM dynamics and simulations; in Sec. 4 we present the main results, focusing on DM density profiles, differentiating between its two driving factors: FDM’s quantum pressure and baryonic feedback processes, and their impact on the observable properties of the explored systems; finally, we summarize our findings in Sec. 5."
https://arxiv.org/html/2411.09571v1,The Redshift-Space Momentum Power Spectrum III: measuring the growth rate from the SDSSv survey using auto- and cross- power spectrum of the galaxy density and momentum fields,"The large-scale structure of the Universe and its evolution over time contains an abundance of cosmological information. One way to unlock this is by measuring the density and momentum power spectrum from the positions and peculiar velocities of galaxies, and fitting the cosmological parameters from these power spectrum. In this paper, we will explore the cross power spectrum between the density and momentum fields of galaxies. We derive the estimator of the density-momentum cross power spectrum multipoles. The growth rate of the large-scale-structure, f\sigma_{8} is measured from fitting the combined density monopole, momentum monopole and cross dipole power spectrum. The estimators and models of power spectrum as well as our fitting method have been tested using mock catalogues, and we find that they perform well in recovering the fiducial values of the cosmological parameters of the simulations, and we also find that the errors of the parameters can be largely reduced by including the cross-power spectrum in the fit. We measure the auto-density, auto-momentum and cross power spectrum using the Sloan Digital Sky Survey Data Release 14 peculiar velocity catalogue. The fit result of the growth rate f\sigma_{8} is f\sigma_{8}=0.413^{+0.050}_{-0.058} at effective redshift z_{\mathrm{eff}}=0.073, and our measurement is consistent with the prediction of the \Lambda Cold Dark Matter cosmological model assuming General Relativity. The code of the power spectrum measurements, power spectrum models and window function convolution is in here: https://github.com/FeiQin-cosmologist/PowerSpectrumMultipoles. The Sloan Digital Sky Survey Data Release 14 peculiar velocity catalog (and mocks) used in this paper can be downloaded from: https://zenodo.org/record/6640513.","Deciphering the formation and evolution of large-scale-structures in our Universe is paramount in cosmology. In recent years, several cosmological theoretical models have been developed to understand the evolution of our Universe (Peebles 1980, 1993; Peacock 1999; Strauss & Willick 1995 and references therein). The peculiar velocities of galaxies, arising from the fluctuation of the mass density field of the Universe, have been widely utilized to test these cosmological models. There are several substantial efforts that have been invested in this realm, including directly mapping the cosmic flow field, then comparing to the predictions from the cosmological models( e.g. Nusser & Davis 1995; Qin et al. 2019b; Whitford et al. 2023 and references therein), or alternatively, measuring the cosmological parameters and then comparing to the predictions of the cosmological models. The cosmological parameters that we seek to measure are the growth rate of structure f\equiv d\ln\,D(a)/d\ln\,a which is the derivative of the linear growth factor D(a) with respect to the scale factor a, and the galaxy bias parameter b\equiv\sqrt{P_{g}({\bf k})/P_{\mathrm{DM}}({\bf k})}, which is the ratio between the galaxies density and dark matter density, where P_{\mathrm{DM}} and P_{g} are the dark matter and galaxies density power spectrum, respectively. Moreover, the growth rate and galaxy bias parameter can be normalized as f\sigma_{8} and b\sigma_{8}, where \sigma_{8} is defined as the root mean square of the mass density fluctuation within spheres of 8 Mpc h^{-1}. Both f\sigma_{8} and b\sigma_{8} are measurable from galaxies. To achieve this, there have been many techniques developed in previous research. One technique is estimating the growth rate f\sigma_{8} and galaxy bias parameter b\sigma_{8} from the galaxy and/or velocity correlation functions (Gorski et al., 1989; Howlett et al., 2015c; Adams & Blake, 2017; Dupuy et al., 2019; Wang et al., 2018, 2021; Turner et al., 2021, 2023). However, this method faces two main challenges: the non-Gaussianity of velocity correlation, and the time-consuming nature of measuring the correlation function for large numbers of data-points or simulations. These parameters can also estimated via the comparison of measured/reconstructed velocity and density fields (Pike & Hudson, 2005; Erdoǧdu et al., 2006; Ma et al., 2012; Springob et al., 2014; Carrick et al., 2015; Said et al., 2020; Qin et al., 2023; Lilow et al., 2024). This method has been shown to return smaller uncertainties on the growth rate, however, the systematic errors of the reconstruction methods are difficult to estimate and some recent work has demonstrated a potential for bias or underestimated error to exist in these previous measurements (Turner & Blake, 2023). A third ""maximum likelihood fields"" method, has been developed to estimate these parameters in recent years which also models the density and velocity fields directly, but remains computationally demanding. (Adams & Blake, 2020; Lai et al., 2023; Carreres et al., 2023). In particular Lai et al. (2023) used this technique to obtain constraints on the Sloan Digital Sky Survey Data Release 14 peculiar velocity catalogue (SDSSv, Howlett et al. 2022). An alternative opportunity lies in measuring these parameters from the power spectrum of galaxies (Feldman et al., 1994; Park, 2000; Park & Park, 2006; Yamamoto et al., 2006; Blake et al., 2010; Johnson et al., 2014; Howlett et al., 2017; Zhang et al., 2017; Howlett, 2019; Qin et al., 2019a; Appleby et al., 2023; Shi et al., 2024). This has the benefit of being faster to measure than the correlation function, and more natural to model from theory, but at the cost of introducing potential effects from complex survey geometries and shot-noise. The estimator for density power spectrum measurement was first developed in Feldman et al. (1994), while the momentum power spectrum was firstly investigated in Park (2000) and Park & Park (2006). Following these works, Howlett (2019) (here after Paper I) modified the momentum power spectrum method so that it can be implemented in the same way as the typical galaxy density power spectrum (Yamamoto et al., 2006; Bianchi et al., 2015; Scoccimarro, 2015). Further more, Qin et al. (2019a) (here after Paper II) then applied this technique to successfully measure the cosmological parameters using the combined density and momentum power spectrum from the combined 2MASS Tully-Fisher (2MTF, Hong et al. 2019) and 6dFGS peculiar-velocity surveys (6dFGSv, Campbell et al. 2014). This paper is the third of the series. In this paper, we extend our method to measure and model the density and momentum cross power spectrum, completing the full set of 3\times 2-point functions that can be measured from the two field. We use this to extract the cosmological parameters from the combined auto-density, auto-momentum and cross-power spectrum of the SDSSv catalogue, improving on the constraints from Lai et al. (2023). This paper is structured as follows: in Section 2, we introduce the SDSSv data and mock catalogues that reproduce the survey characteristics and are used to validate our analysis methods. In Section 3, we present the power spectrum estimators we apply to this data. In Section 4, we introduce our power spectrum models and discuss how we account for the survey window function. We test and verify our method in Section 5 on the mocks. Lastly, Section 6 presents our main results applying our analysis pipeline to the SDSSv data to estimate f\sigma_{8} and b\sigma_{8}. We also discuss the consistency of these constraints with the predictions from GR and other works therein, before summarising in Section 7. Where necessary, we adopt a flat \Lambda Cold Dark Matter (\LambdaCDM) fiducial cosmological model with parameters n_{s}=0.9653, \Omega_{m}=0.3121, \Omega_{b}=0.0491, \sigma_{8}=0.8150 and H_{0}=100h km s-1 Mpc-1, where h=0.6751. The corresponding fiducial value of the growth rate is f\sigma_{8}=0.432."
https://arxiv.org/html/2411.09412v1,Rapid identification of lensed type Ia supernovae with color-magnitude selection,"Strongly lensed type Ia supernovae (SNe Ia) provide a unique cosmological probe to address the Hubble tension problem in cosmology. In addition to the sensitivity of the time delays to the value of the Hubble constant, the transient and standard candle nature of SNe Ia also enable valuable joint constraints on the model of the lens and the cosmological parameters. The upcoming Legacy Survey of Space and Time (LSST) with the Vera C. Rubin Observatory is expected to increase the number of observed SNe Ia by an order of magnitude in ten years of its lifetime. However, finding such systems in the LSST data is a challenge. In this work, we revisit the color-magnitude (CM) diagram used previously as a means to identify lensed SNe Ia and extend the work further as follows. We simulate LSST-like photometric data (rizy bands) of lensed SNe Ia and analyze it in the CM parameter space. We find that a subset of lensed SNe Ia are redder compared to unlensed SNe Ia at a given magnitude, both in the rising and falling phases of their light curves and for SNe up to z=3. We propose a modified selection criterion based on these new results. We show that the contamination coming from the unlensed core-collapse (CC) SNe is negligible, whereas a small fraction of lensed CC SNe types Ib and Ic may get selected by this criterion as potential lensed SNe. Finally, we demonstrate that our criterion works well on a wide sample of observed unlensed SNe Ia, a handful of known multiply-imaged lensed SNe systems, and a representative sample of observed super-luminous supernovae.","Gravitational lensing, deflection in the path of light from a source in the presence of deep gravitational potential, can lead to the formation of multiple images of the same source. The multiple images appear in the sky separated by a time delay. The time delays between multiple images in strong lensing yield a direct measure of the cosmological distances involved in the lens system and thus constrain cosmological parameters such as the Hubble constant (H_{0}; Refsdal, 1964). This method of constraining H_{0} - time delay cosmography - has so far primarily been accomplished with lensed quasars, (e.g., Suyu et al., 2017; Wong et al., 2017; Chen et al., 2019; Birrer et al., 2020; Shajib et al., 2020; Wong et al., 2020). Wong et al. (2020) inferred H_{0} using 6 lensed quasar systems and gave a joint constraint of 73.3{}^{+1.7}_{-1.8} km s-1 Mpc-1. However, there are challenges involved in the time delay cosmography with lensed quasars. The light curves of quasars are stochastic and heterogeneous, thus typically requiring years of monitoring of lensed quasar systems to calculate precise time delays (Liao et al., 2015). Since quasars typically outshine the host and the lens galaxy light, reconstructing the lensing potential becomes challenging. In contrast, lensed SNe can be more useful for time delay cosmography compared to quasars due to their standardizable and transient nature. SNe light curves are relatively well-studied and predictable, which makes it easier to constrain their time delays with shorter observing campaigns (Goldstein et al., 2018). The transient nature of SNe enables better follow-up imaging of the host and lens galaxy without contamination from the SNe light as it fades away subsequently. These measurements can help to constrain the lens mass model. SNe Ia, specifically, have remarkably homogeneous light curves among all SNe types with almost identical luminosity within its class. This standardizable nature of SNe Ia enables constraints on the absolute magnification factors of individual images, which can be valuable in minimizing the mass-sheet degeneracy and resulting in improved H_{0} measurements (Oguri & Kawano, 2003). Alternatively, it also allows joint constraints on the lens model and the cosmological parameters (e.g., Oguri & Kawano, 2003; Linder, 2004, 2011). For further discussion on the suitability of lensed SNe for time delay cosmography and various methods to identify such systems in large-scale surveys, see recent reviews (e.g., Oguri, 2019; Liao et al., 2022; Suyu et al., 2024). The sample of strongly lensed SNe has grown over the last decade to include PS1-10afx (Quimby et al., 2014), SN Refsdal (Kelly et al., 2015), iPTF2016geu (Goobar et al., 2017), SN Requiem (Rodney et al., 2021), C22 (Chen et al., 2022), SN Zwicky (Goobar et al., 2023), SN H0pe (Frye et al., 2024), and SN Encore (Pierel et al., 2024). Out of the above, PS1-10afx, SN Zwicky, SN Requiem, iPTF2016geu, SN H0pe, and SN Encore either show strong evidence of SNe Ia nature or are spectroscopically confirmed to be of type Ia (Quimby et al., 2014; Goobar et al., 2017; Rodney et al., 2021; Goobar et al., 2023; Frye et al., 2024; Pierel et al., 2024)). Inferring H_{0} from these systems has been primarily limited owing to insufficient follow-up data, too short time delays between multiple images, and systematics in lens mass modeling. Until now, SN Refsdal was able to provide H_{0} constraints at 66.6{}^{+4.1}_{-3.3} km s-1 Mpc-1 (Kelly et al., 2023) and SN H0pe provided constraints at 75.4{}^{+8.1}_{-5.5} km s-1 Mpc-1 (Pascale et al., 2024). Legacy Survey of Space and Time (LSST) to be conducted at the Rubin Observatory (Ivezic et al., 2011; Ivezić et al., 2019) will be a powerful SN factory, discovering \mathcal{O}(10^{6}) of SNe Ia in 10 years of its survey life (Abell et al., 2009; Ivezić et al., 2019). It is also expected to find several hundreds of lensed SNe Ia in its lifetime (e.g., Oguri & Marshall, 2010; Quimby et al., 2014; Goldstein & Nugent, 2016; Wojtak et al., 2019; Arendse et al., 2023). Arendse et al. (2023) predict that lensed SNe Ia detected within about three years of LSST operations will lead to a 1.5% precision in the measurement of H_{0}. The prospect of precise, independent measurement of H_{0} is particularly important given the Hubble Tension - the 4~{}\sigma-6~{}\sigma disagreement in the value of H_{0} as measured using early-time and late-time probes (e.g., Freedman et al., 2001; Aghanim et al., 2020; Wong et al., 2020; Riess et al., 2021). Due to the cosmological importance and rarity of occurrence of lensed SNe Ia, it is paramount to devise effective methods to identify such systems from the LSST data. 1.1 Color-magnitude selection criterion As the observed lensed SNe population will primarily come from higher redshifts compared to unlensed SNe, the peak of the spectral energy distribution for SNe Ia will shift out of the observer frame bluer bands, giving redder observed colors to lensed SNe Ia. Additionally, for lensed and unlensed SNe Ia at the same redshifts, the lensed SNe Ia will highly likely appear brighter because of the lensing magnification. Owing to this, Quimby et al. (2014) proposed the idea of exploring the color-magnitude (CM) space of SNe to identify promising strongly lensed SNe candidates. They proposed a “red limit"", a limit on the color of unlensed SNe for a given apparent magnitude (the bold black curve in figure 4 of Quimby et al., 2014), as a promising criterion to select only lensed SNe. This criterion selects the SNe that lie above the red limit as potential lensed SNe Ia. The original red limit was proposed to select unresolved lensed SNe on the rising edges of their light curves. In the current work, we extend this study by systematically investigating photometric data of unlensed and lensed SNe Ia in the CM parameter space for SNe. We propose modified red limits that select lensed SNe Ia candidates, both unresolved and resolved, on both the rising and falling phases of the light curve. The CM selection criterion successfully selects lensed SNe Ia till the redshift of 3 in our simulations. To verify the applicability of the proposed selection criterion on the observed SNe data, we select real unlensed SNe Ia from a variety of archival surveys and show that the proposed selection criterion successfully eliminates them. Finally, since the focus of this study is to identify the lensed SNe type Ia, we consider other types of (un-)lensed SNe as contaminants. We investigate the contamination in the lensed SNe Ia CM parameter space arising from a population of simulated CC SNe and find the subtypes that primarily contaminate this parameter space. Along with this, we also check for contamination by a representative sample of observed superluminous supernovae (SLSNe). We outline our (un-)lensed SNe Ia and CC SNe simulation procedure in Sec. 2 and describe the details of the observed SNe Ia and SLSNe data used from various published surveys in Sec. 3. We present the results from the investigation of the CM diagram for various SNe samples in Sec. 4, and describe the conclusions along with some discussions in Sec. 5. We use AB magnitudes throughout this work."
https://arxiv.org/html/2411.09163v1,Impact of Large-Scale Anisotropies on Galaxy Clustering and Cosmological Constraints,"We critically assess the impact of significant dipole and large-scale anisotropies on galaxy clustering signals, with a focus on radio continuum surveys. Our study reveals that these anisotropies—resulting from intrinsic cosmological effects and/or observational systematics—profoundly influence the two-point correlation function (2PCF) and angular power spectrum (C_{\ell}). Notably, large-scale anisotropies can obscure or simulate non-Gaussianity signals, complicating the extraction of precise cosmological information. The results emphasize that it is crucial to address systematics and rigorously mask the dipole and its surrounding multipoles to obtain accurate cosmological constraints. This approach is essential for extracting cosmological results from clustering signals, particularly for future surveys such as SKA, DESI, and LSST, to ensure the precision and reliability of cosmological analyses.","Galaxy clustering is a key tool for probing cosmological parameters, offering critical insights into the matter distribution and evolution of the Universe (Peebles, 1980). By comparing observed clustering patterns with theoretical models, we impose constraints on fundamental parameters such as matter density, fluctuation amplitude, and structure growth. This process refines our understanding of the Universe’s composition and the mechanisms behind galaxy and large-scale structure formation. In the era of precision cosmology, where achieving exceptional accuracy in parameter measurements is crucial, we rely on advanced observational techniques and data analysis methods. Next-generation surveys, such as the Square Kilometre Array (SKA; Wilkinson 1991; Bacon et al. 2020), the Dark Energy Spectroscopic Instrument (DESI; DESI Collaboration et al. 2016, 2022), and the Large Synoptic Survey Telescope (LSST; Ivezić et al. 2019), are set to dramatically enhance our ability to map the Universe and explore phenomena like dark energy, modified gravity (Clifton et al., 2012), and primordial non-Gaussianities (Komatsu & Spergel, 2001). These advancements will critically evaluate the current cosmological model and explore alternative cosmological models. However, these analyses face challenges due to large-scale anisotropies in galaxy distributions. Whether arising from intrinsic cosmological effects or observational systematics, these anisotropies can distort clustering signals and complicate accurate cosmological inference. A prime example is the observed dipole in radio and infrared galaxy counts, first identified in the NRAO VLA Sky Survey (NVSS) (Blake & Wall, 2002; Singal, 2011). This dipole has generated debate over its origin-whether it reflects our motion relative to the cosmic rest frame, an intrinsic large-scale structure feature, or uncorrected systematics (Gibelyou & Huterer, 2012; Rubart & Schwarz, 2013; Tiwari et al., 2015; Tiwari & Nusser, 2016; Colin et al., 2017; Dolfi et al., 2019; Tiwari et al., 2019; Secrest et al., 2022). The impact of such anisotropies is profound. The dipole and other large-scale modes can obscure the isotropic Gaussian assumption commonly used in standard analyses, complicating the detection of subtle effects such as primordial non-Gaussianity or scale-dependent biases from inflationary models (Matarrese et al., 2000; Dalal et al., 2008; Bruni et al., 2012). Most leading cosmological clustering studies have thus far overlooked these effects, operating under the assumption of large-scale isotropy and homogeneity—commonly known as the Cosmological Principle, a fundamental basis for standard cosmological analyses. Yet recent observations, especially from radio and infrared surveys (Tiwari et al., 2015; Tiwari & Nusser, 2016; Colin et al., 2017; Secrest et al., 2021, 2022; Singal, 2024; von Hausegger, 2024), reveal a prominent dipole signal in galaxy distributions, making it crucial to consider this signal and underscoring the urgency of addressing this issue. In this letter, we investigate the effect of large-scale anisotropies on galaxy clustering measurements, focusing on the two-point correlation function (2PCF) and the angular power spectrum (C_{\ell}). Using NVSS data as a case study, we assess implications for upcoming surveys like SKA, DESI, and LSST. We highlight the necessity of aggressive masking of large-scale anisotropies and rigorous systematic treatment to mitigate the dipole and adjacent multipoles, which can otherwise introduce significant biases in cosmological parameter estimates. We demonstrate how anisotropies can mimic or obscure key cosmological signals, such as non-Gaussianities and scale-dependent biases. We need for robust data processing techniques and careful management of large-scale systematics to fully exploit the capabilities of next-generation surveys. We conclude that effectively addressing large-scale anisotropies is essential for achieving precise cosmological inferences."
https://arxiv.org/html/2411.09042v1,Excess Radiation from Axion-Photon Conversion,"Two notable anomalies in radio observations — the excess radiation in the Rayleigh-Jeans tail of the cosmic microwave background, revealed by ARCADE2, and the twice-deeper absorption trough of the global 21cm line, identified by EDGES — remain unresolved. These phenomena may have a shared origin, as the enhancement of the 21cm absorption trough could arise from excess heating. We investigate this scenario through the framework of axion-like particles (ALPs), showing that the resonant conversion of ALPs into photons can produce a photon abundance sufficient to resolve both anomalies simultaneously. Our model naturally explains the observed radio excess between 0.4 and 10GHz while also enhances the 21cm absorption feature at 78MHz. Furthermore, it predicts a novel power-law scaling of the radio spectrum above 0.5GHz and an additional absorption trough below 30MHz, which could be verified through cross-detection in upcoming experiments.","Appendix A In this appendix we derive the main formula of our paper. We consider an ALP-photon mixing system propagating along the l-direction in an perturbative magnetic background field \bm{B}(l)=(B_{x},B_{y},B_{z}). For highly relativistic axions m_{a}\ll\omega, the short-wavelength approximation can be applied and the ALP-photon mixing in the expanding universe reduces to a linearized system (Mirizzi et al., 2007) \displaystyle\left(\omega-i(1+z)\frac{\textrm{d}}{\textrm{d}l}+M\right)\left(% \begin{array}[]{c}A_{x}\\ A_{y}\\ a\end{array}\right) \displaystyle= \displaystyle 0,\qquad M=\left(\begin{array}[]{ccc}\Delta_{xx}&\Delta_{xy}&% \frac{1}{2}g_{a\gamma}B_{x}\\ \Delta_{xy}&\Delta_{yy}&\frac{1}{2}g_{a\gamma}B_{y}\\ \frac{1}{2}g_{a\gamma}B_{x}&\frac{1}{2}g_{a\gamma}B_{y}&\Delta_{a}\end{array}\right) (14) with the scale factor a and the spatial coordinate l in the comoving framework. Here the mixing matrix element reads \displaystyle\Delta_{xx} \displaystyle= \displaystyle\Delta_{pl}+\Delta_{CM},\quad\Delta_{yy}=\Delta_{pl}+\Delta_{CM},% \quad\Delta_{xy}=\Delta_{yx}=\Delta_{R},\quad\Delta_{a}=-m_{a}^{2}/2\omega,% \quad\Delta_{pl}=-\omega_{pl}^{2}/2\omega (15) where m_{a} is the ALP mass and g_{a\gamma} the ALP-photon coupling strength. The plasma effect arsing from photon refraction in the medium is characterized by the plasma frequency \omega_{pl}=\sqrt{e^{2}n_{e}/m_{e}} with free electron’s charge e, mass m_{e} and the number density n_{e}. In addition, the term \Delta_{CM} describe the CottonMouton effect, which originate from QED corrections for photon in a transverse magnetic field. As shown in Ref. (Ejlli, 2018), this effect is quadratic in magnetic strength and hence negligible in our analysis since we focus on the linear order of perturbative B field. The Faraday rotation term \Delta_{R} couples theA_{x} and A_{y} modes, which is relevant for analyzing polarized photon sources but irrelevant to this study. Thus, Eq. 14 simplifies as \displaystyle\partial_{l}\left(\begin{array}[]{c}A_{x}(\omega,l)\\ A_{y}(\omega,l)\\ a(\omega,l)\end{array}\right) \displaystyle= \displaystyle i\mathcal{K}\left(\begin{array}[]{c}A_{x}(\omega,l)\\ A_{y}(\omega,l)\\ a(\omega,l)\end{array}\right),\,\,\,\,\hfill\mathcal{K}=\frac{1}{1+z}\left(% \begin{array}[]{ccc}\omega+\Delta_{pl}&0&\frac{1}{2}g_{a\gamma}B_{x}(l)\\ 0&\omega+\Delta_{pl}&\frac{1}{2}g_{a\gamma}B_{y}(l)\\ \frac{1}{2}g_{a\gamma}B_{x}(l)&\frac{1}{2}g_{a\gamma}B_{y}(l)\hfill&\omega+% \Delta_{a}\end{array}\right). (25) We apply the perturbative approach by decomposing matrix \mathcal{K} into \displaystyle\mathcal{K} \displaystyle= \displaystyle\mathcal{K}_{0}+\delta\mathcal{K}=\frac{1}{1+z}\left(\begin{array% }[]{ccc}\omega+\Delta_{pl}&0&0\\ 0&\omega+\Delta_{pl}&0\\ 0&0&\omega+\Delta_{a}\end{array}\right)+\frac{1}{1+z}\left(\begin{array}[]{ccc% }0&0&\frac{1}{2}g_{a\gamma}B_{x}(l)\\ 0&0&\frac{1}{2}g_{a\gamma}B_{y}(l)\\ \frac{1}{2}g_{a\gamma}B_{x}(l)&\frac{1}{2}g_{a\gamma}B_{y}(l)&0\end{array}% \right). (32) Physically speaking, the evolution of ALP and photon free of external field is described by principle part \mathcal{K}_{0}, and their mixing through the magnetic background is encoded in perturbation matrix \delta\mathcal{K}. Note that \mathcal{K}_{0} implicitly depends on l from relation dl=dz/H in expanding universe. We arrange Eq. 25 into \displaystyle\partial_{l}\left(e^{-i\int_{l_{0}}^{l}dl_{1}\mathcal{K}_{0}\left% (l_{1}\right)}\mathcal{U}\left(l,l_{0}\right)\right) \displaystyle= \displaystyle ie^{-i\int_{l_{0}}^{l}dl_{1}\mathcal{K}_{0}\left(l_{1}\right)}% \delta\mathcal{K}(l)\mathcal{U}\left(l,l_{0}\right) (33) by introducing a conversion matrix \mathcal{U} defined by \left(A_{x}(l),A_{y}(l),a(l)\right)^{T}=\mathcal{U}\left(l,l_{0}\right)\left(A% _{x}(l_{0}),A_{y}(l_{0}),a(l_{0})\right)^{T}. One can iteratively solve Eq. 33 up to the first order \displaystyle\mathcal{U}\left(l,l_{0}\right) \displaystyle= \displaystyle e^{i\int_{l_{0}}^{l}dl_{1}\mathcal{K}_{0}(l_{1})}+ie^{i\int_{l_{% 0}}^{l}dl^{\prime\prime}\mathcal{K}_{0}(l^{\prime\prime})}\int_{l_{0}}^{l}dl^{% \prime}e^{-i\int_{l_{0}}^{l^{\prime}}d\ell^{\prime\prime}\mathcal{K}_{0}(l^{% \prime\prime})}\delta\mathcal{K}\left(l^{\prime}\right)e^{i\int_{l_{0}}^{l^{% \prime}}dl^{\prime\prime}\mathcal{K}_{0}(l^{\prime\prime})}+\mathcal{O}\left(% \left(\delta\mathcal{K}\right)^{2}\right). (34) Each component of \mathcal{U}\left(l,l_{0}\right) reflects the mode transitions btween ALP and photon states. For instance, photon generation at distance l can be read from the conversion matrix with A_{x}(l)=\mathcal{U}_{13}a(l_{0}) and A_{y}(l)=\mathcal{U}_{23}a(l_{0}), leading to the conversion probability \displaystyle\mathcal{P}(l) \displaystyle= \displaystyle\frac{|A_{x}(l)|^{2}+|A_{y}(l)|^{2}}{\left|a(l_{0})\right|^{2}}=|% \mathcal{U}_{13}(l,l_{0})|^{2}+|\mathcal{U}_{23}(l,l_{0})|^{2}. (35) From Eq. 34 one can obtain \displaystyle\left|\mathcal{U}_{13}\left(l,l_{0}\right)\right|^{2} \displaystyle= \displaystyle\frac{1}{4}g_{a\gamma}^{2}\int_{l_{0}}^{l}dl_{1}\int_{l_{0}}^{l}% dl_{2}\left(\frac{1}{1+z}B_{x}(l_{1})\right)\left(\frac{1}{1+z}B_{x}(l_{2})% \right)e^{i\int_{l_{1}}^{l_{2}}dl_{3}\frac{1}{1+z}(\Delta_{xx}(l_{3})-\Delta_{% a}(l_{3}))}, \displaystyle\left|\mathcal{U}_{23}\left(l,l_{0}\right)\right|^{2} \displaystyle= \displaystyle\frac{1}{4}g_{a\gamma}^{2}\int_{l_{0}}^{l}dl_{1}\int_{l_{0}}^{l}% dl_{2}\left(\frac{1}{1+z}B_{y}(l_{1})\right)\left(\frac{1}{1+z}B_{y}(l_{2})% \right)e^{i\int_{l_{1}}^{l_{2}}dl_{3}\frac{1}{1+z}(\Delta_{xx}(l_{3})-\Delta_{% a}(l_{3}))}. (36) Regarding the magnetic field, we assume it to be a statistically isotropic Gaussian distributed random field with correlation function defined by \displaystyle\left\langle\mathbf{B}_{i}(\mathbf{x_{1}})\mathbf{B}_{j}\left(% \mathbf{x}_{2}\right)\right\rangle=\frac{1}{(2\pi)^{3}}\int_{k_{IR}}^{k_{UV}}d% ^{3}ke^{i\mathbf{k}\cdot\left(\mathbf{x}_{1}-\mathbf{x_{2}}\right)}\left[\left% (\delta_{ij}-\hat{\mathbf{k}}_{i}\hat{\mathbf{k}}_{j}\right)P_{B}(k)-i\epsilon% _{ijm}\hat{\mathbf{k}}_{m}P_{aB}(k)\right], (37) where \hat{\mathbf{k}}=\mathbf{k}/k and \epsilon_{ijm} is the antisymmetric symbol. The P_{B}(k) and P_{aB}(k) are respectively the symmetric and antisymmetric components of the spectrum, whereas the latter is absent from the expression \displaystyle\left\langle\mathbf{B}_{x}(l_{1})\mathbf{B}_{x}\left(l_{2}\right)% +\mathbf{B}_{y}(l_{1})\mathbf{B}_{y}\left(l_{2}\right)\right\rangle=\frac{1}{(% 2\pi)^{3}}\int d^{3}ke^{ik\cos\theta\left(l_{1}-l_{2}\right)}\left(1+\cos^{2}% \theta\right)P_{B}(k). (38) Here \theta denotes the angle between propagation direction l and the wave-vector \bm{k} of the magnetic field. Combining Eqs. 35, 36 and 38, we obtain \displaystyle\mathcal{P} \displaystyle= \displaystyle\frac{3}{8(2\pi)^{3}}\left(1+z\right)^{2}g_{a\gamma}^{2}\int d^{3% }ke^{ik\cos\theta\left(l_{1}-l_{2}\right)}P_{B}(k)\int_{l_{0}}^{l}dl_{1}\int_{% l_{0}}^{l}dl_{2}\exp\left(i\int_{l_{1}}^{l_{2}}dl_{3}\frac{1}{1+z}(\Delta_{pl}% (l_{3})-\Delta_{a}(l_{3}))\right) (39) with approximate 1+\cos^{2}\theta\simeq 3/2 to simplify the calculation. In fact, we have checked that inclusion of exact term 1+\cos^{2}\theta yields a minor correction ( \sim 1.1 correction factor in overall amplitude) to this approximation. To perform the integral, we use a semi-steady approximation by discritizing the universe expansion into a decreasing sequence of redshifts [z_{1},z_{2},...z_{N}] with z_{1}\simeq 1100 marking the end of Recombination. A proper discretization scheme is given by the sequence z_{i+1}=z_{i}(1-\epsilon) , where \epsilon is a small parameter. Physically, for a proper small value of \epsilon, this scheme implies that the ALP-photon mixing propagation distance l\simeq\epsilon\mathcal{H}^{-1} remains well within the comoving Hubble radius \mathcal{H}^{-1}. Throughout the paper we set \epsilon=0.1. In each interval z\in[z_{i},z_{i+1}], we approximate \Delta(z) by a linear expansion centered at z_{c}=(z_{i}+z_{i+1})/2 as \displaystyle\Delta(z)\simeq\Delta(z_{c})+\Delta^{\prime}(z_{c}) \displaystyle(z-z_{c}), (40) where \Delta(z)=\Delta_{pl}(z)-\Delta_{a}(z), \Delta^{\prime}(z)=\Delta^{\prime}_{pl}(z)-\Delta^{\prime}_{a}(z) and prime is derivative with respect to the redshift. Then we substitute dl=dz/H and approximate Eq. 39 as \displaystyle\mathcal{P}(z_{i},z_{i+1}) \displaystyle\simeq \displaystyle\frac{3}{8(2\pi)^{2}}\frac{(1+z_{c})^{2}g_{a\gamma}^{2}}{H_{c}^{2% }}\int k^{2}P_{B}dk\int_{-1}^{+1}d(\cos\theta)\int_{z_{i}}^{z_{i+1}}dz_{1}\int% _{z_{i}}^{z_{i+1}}dz_{2} (41) \displaystyle\times\exp\left(ik\cos\theta\frac{1}{H_{c}}\left(z_{1}-z_{2}% \right)+i\int_{z_{1}}^{z_{2}}\frac{1}{\left(1+z_{c}\right)H_{c}}dz_{3}(\Delta_% {pl}(z_{3})-\Delta_{a}(z_{3}))\right) \displaystyle\simeq \displaystyle\frac{3}{8(2\pi)^{2}}\frac{(1+z_{c})^{2}g_{a\gamma}^{2}}{H_{c}^{2% }}\int k^{2}P_{B}dk\int_{-1}^{+1}d(\cos\theta)\int_{z_{i}}^{z_{i+1}}dz_{1}\int% _{z_{i}}^{z_{i+1}}dz_{2} \displaystyle\times\exp\left(i(z_{2}-z_{1})\left(\frac{\Delta^{\prime}(z_{c})}% {2\left(1+z_{c}\right)H_{c}}(z_{2}+z_{1}-2z_{c})+\frac{k\cos\theta}{H_{c}}% \right)\right) \displaystyle= \displaystyle\frac{3\pi}{16(2\pi)^{2}}\frac{\left(1+z_{c}\right)^{3}g_{a\gamma% }^{2}}{H_{c}\left|\triangle^{\prime}(z_{c})\right|}\int k^{2}P_{B}dk\int_{-1}^% {+1}d(\cos\theta) \displaystyle\times\left\{\textrm{Erf}\left(\left(-1\right)^{1/4}\sqrt{\frac{% \left|\triangle^{\prime}(z_{c})\right|}{2\left(1+z_{c}\right)H_{c}}}\left(z_{c% }-\frac{\left(1+z_{c}\right)k\cos\theta}{\triangle^{\prime}(z_{c})}-z_{i+1}% \right)\right)\right. \displaystyle\left.-\textrm{Erf}\left(\left(-1\right)^{1/4}\sqrt{\frac{\left|% \triangle^{\prime}(z_{c})\right|}{2\left(1+z_{c}\right)H_{c}}}\left(z_{c}-% \frac{\left(1+z_{c}\right)k\cos\theta}{\triangle^{\prime}(z_{c})}-z_{i}\right)% \right)\right\} \displaystyle\times\left\{-\textrm{Erf}\left(\left(-1\right)^{3/4}\sqrt{\frac{% \left|\triangle^{\prime}(z_{c})\right|}{2\left(1+z_{c}\right)H_{c}}}\left(z_{c% }-\frac{\left(1+z_{c}\right)k\cos\theta}{\triangle^{\prime}(z_{c})}-z_{i+1}% \right)\right)\right. \displaystyle\left.+\textrm{Erf}\left(\left(-1\right)^{3/4}\sqrt{\frac{\left|% \triangle^{\prime}(z_{c})\right|}{2\left(1+z_{c}\right)H_{c}}}\left(z_{c}-% \frac{\left(1+z_{c}\right)k\cos\theta}{\triangle^{\prime}(z_{c})}-z_{i}\right)% \right)\right\}, where H_{c}=H(z_{c}) and error function \textrm{Erf}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^{2}}dt. Thanks to the step-like shape of the error function, one can infer the integral over \cos\theta from its geometric meaning and find \displaystyle\mathcal{P}(z_{i},z_{i+1}) \displaystyle= \displaystyle\frac{3\pi}{4(2\pi)^{2}}\frac{(1+z_{c})^{3}g_{a\gamma}^{2}}{H_{c}% |\Delta^{\prime}(z_{c})|}\int dkk^{2}P_{B}(k)W(t_{1},t_{2}) (42) with \displaystyle t_{1} \displaystyle= \displaystyle\frac{\left(z_{c}-z_{f}\right)\Delta^{\prime}(z_{c})-\triangle(z_% {c})}{\left(1+z_{c}\right)k},\,\,\,\,\,\hfill t_{2}=\frac{\left(z_{c}-z_{i}% \right)\Delta^{\prime}(z_{c})-\triangle(z_{c})}{\left(1+z_{c}\right)k}, (43) and \displaystyle W(t_{1},t_{2}) \displaystyle= \displaystyle\begin{cases}|t_{1}-t_{2}|,&|t_{1}|<1\quad|t_{2}|<1\\ |t_{1}+1|,&|t_{1}|<1\quad t_{2}<-1\\ |t_{1}-1|,&|t_{1}|<1\quad t_{2}>1\\ |t_{2}+1|,&t_{1}<-1\quad|t_{2}|<1\\ |t_{2}-1|,&t_{1}>1\quad|t_{2}|<1\\ 2,&t_{1}<-1\quad t_{2}>1\quad or\quad t_{1}>1\quad t_{2}<-1\\ \simeq 0,&\textrm{others}\end{cases}. (44) Note that Eq. 42 holds under the linear expansion of \Delta-term (see Eq. 40) in [z_{i},z_{i+1}], which is valid given our choice of the discritization parmeter \epsilon=0.1 and parameter space considered in this paper. Let us apply our main formula Eq. 42 to analyze two limiting cases. The first case considers a mass-equal resonance in presence of the stochastic magnetic field. This resonance arises at a redshift z_{res} when the mass-equal condition \triangle(z_{res})=m_{\gamma}(z_{res})-m_{a}(z_{res})=0 is satisfied, producing a peak in the oscillation length profile (see Fig. 1). Assuming a monochromatic spectrum P_{B}(k)=\pi^{2}B_{0}^{2}\delta\left(k-k_{B}\right)/k_{B}^{2} and a constant re-ionization fraction X_{e}=10^{-4}, from Eq. 42 we derive the conversion probability over [z_{i}=z_{res}(1+\epsilon/2),z_{i+1}=z_{res}(1-\epsilon/2)] as \displaystyle\mathcal{P} \displaystyle\simeq \displaystyle 8.6*10^{-4}\left(\frac{g_{a\gamma}}{10^{-12}\textrm{GeV}^{-1}}% \right)^{2}\left(\frac{B_{0}}{\textrm{nGs}}\right)^{2}\left(\frac{\omega_{0}}{% \textrm{GHz}}\right)\left(\frac{m_{a}}{10^{-12}\textrm{eV}}\right)^{1/3}% \textrm{Min}\left(\epsilon z_{res}\left(\frac{\textrm{GHz}}{\omega_{0}}\right)% \left(\frac{\lambda_{B}}{\textrm{kpc}}\right),1\right). (45) One can see that the front factor is of similar magnitude and scaling on each variable as those obtained in Landau-Zener approximation (Marsh et al., 2022; Mirizzi et al., 2009; Choi et al., 2020; Mondino et al., 2024; Tashiro et al., 2013) or the same spirit but so called saddle-point approach (Chen and Suyama, 2013). Interestingly, our results introduce a Minimum function that highlight a novel suppression effect, which arises from the stochastic nature of the magnetic background. In fact, the \omega_{0}^{-2} scaling of the brightness temperature shown in Fig. 2 transits to \omega_{0}^{-3} at the higher frequency \omega_{0}\gtrsim 100GHz. Such a suppression is absent in commonly used domain-like model in literature. In domain-like model, the magnetic field is treated as constant over a domain patch in size of correlation length \lambda_{B}. Thus the Landau-Zener approximation is only valid when domain size exceeds the resonance width W_{res}, namely \lambda_{B}\gtrsim W_{res}. Our approach, however, is out of this limit and can extend to the case with small \lambda_{B}, where a suppression is revealed as a consequence of stochastic magnetic field and narrowness of the mass-equal resonance. The second case corresponds to a rather slowly varying oscillation length scale (in physical frame) with \Delta^{\prime}<<1. Again, we assume a monochromatic spectrum for stochastic magnetic field. When the condition -1<t_{1}\simeq t_{2}=\frac{-\triangle(z_{c})}{(1+z_{c})k_{B}}<1 is met, we find W(t_{1},t_{2})=\epsilon|\Delta^{\prime}(z_{c})|/k_{B} and hence the conversion probability reads \displaystyle\mathcal{P} \displaystyle\simeq \displaystyle\frac{3}{32}z_{c}^{2}g_{a\gamma}^{2}B_{0}^{2}\lambda_{B}\triangle l, (46) where \triangle l\simeq\epsilon z_{c}/H_{c} is approximately the comoving distance during [z_{i},z_{i+1}]. Under the condition \left|\frac{\triangle(z_{c})}{(1+z_{c})k_{B}}\right|<1, the conversion probability shows a linear growth with distance. This condition can also be expressed as \pi\lambda_{B}<l_{osc} given the definition of comoving oscillation length l_{osc}(z)=2\frac{1+z}{\triangle(z)}. Such a resonant condition associated with the stochastic magnetic field has also been identified in Ref. (Addazi et al., 2024). In this paper, we study two representative cases with m_{a}=3\times 10^{-14}\textrm{eV} and m_{a}=3\times 10^{-13}\textrm{eV}. As shown in Fig. 1, the oscillation curve consists of the peak at mass-equal resonance and relatively slow-varying region.We employ the semi-steady approximation across the entire post recombination epoch with redshift sequence [z_{1},z_{2},...,z_{N}] and have verified that the linear expansion condition holds as mentioned above. The total conversion probability from recombination up to z_{i} is simply the sum \mathcal{P}^{tot}(z_{i})=\sum_{j=1}^{j=i}\mathcal{P}(z_{j}). By interpolating \mathcal{P}^{tot}(z_{i}) (i=1,2,...) over the redshift sequence, one can obtain the total probability \mathcal{P}^{tot}(z) from recombination to any value of z."
https://arxiv.org/html/2411.08957v1,Learning Optimal and Interpretable Summary Statistics of Galaxy Catalogs with SBI,"How much cosmological information can we reliably extract from existing and upcoming large-scale structure observations? Many summary statistics fall short in describing the non-Gaussian nature of the late-time Universe in comparison to existing and upcoming measurements. In this article we demonstrate that we can identify optimal summary statistics and that we can link them with existing summary statistics. Using simulation based inference (SBI) with automatic data-compression, we learn summary statistics for galaxy catalogs in the context of cosmological parameter estimation. By construction these summary statistics do not require the ability to write down an explicit likelihood. We demonstrate that they can be used for efficient parameter inference. These summary statistics offer a new avenue for analyzing different simulation models for baryonic physics with respect to their relevance for the resulting cosmological features. The learned summary statistics are low-dimensional, feature the underlying simulation parameters, and are similar across different network architectures. To link our models, we identify the relevant scales associated to our summary statistics (e.g. in the range of modes between k=5-30h/\mathrm{Mpc}) and we are able to match the summary statistics to underlying simulation parameters across various simulation models.","The field of Cosmology has firmly entered the data rich era. Current [1, 2, 3, 4, 5, 6, 7, 8, 9] and future surveys [10, 11, 12, 13, 14, 15] will reach immense statistical precision. Due to this development, systematics become the dominant source of error for cosmological inference in these surveys. This brings up a key question: Can we reliably use the entirety of the cosmological information captured by future surveys? One issue in answering this question is given by baryonic feedback [16]. Baryonic physics in active galactic nuclei (AGN) and stars can produce strong outflows of energetic gas, which changes the matter distribution on scales up to a few Mpc [17]. As these processes originate on scales much smaller than what is usually considered in cosmological simulations, these effects prove to be hard to model and the resulting suppression of the matter power spectrum is uncertain [18, 19]. Hydrodynamic simulations which attempt to implement these effects via stochastic so-called subgrid physics models, therefore vary widely in the effect that baryonic physics has on the matter distribution [17]. In light of this, many efforts have been led to identify observables in hydrodynamic simulations, which are indicative of feedback strength. Promising probes include the (unobservable) baryon fraction in halos [18], the thermal and kinetic Sunyaev-Zel’dovich effect [20, 21, 22, 23, 24, 25, 26], X-ray observations of clusters [27], and recently the dispersion measure of fast radio bursts [28, 29]. While these directions of research seem very promising, it is not clear that these observables (or some linear combination) will be the optimal choice for constraining feedback in the setting of cosmological inference. It is more likely that an optimal observable would be created from a non-linear combination of different parts of the data vector. Likewise, for the extraction of cosmological information, human-selected statistics capturing some non-Gaussian features are also not guaranteed to be optimal (see Refs. [30, 31, 32, 33, 34, 35] for examples). In the case of galaxy clustering, Ref. [36] have recently shown that lower order n-point functions fail to capture a significant amount of cosmological information, which calls for the search for highly informative summaries. The question of the ideal representation of some data can be asymptotically solved by means of an optimization problem: Given a set of observables, for instance a galaxy catalog, we can pose an inference problem to automatically identify important features in the data relevant for cosmological inference. This optimization task would be naturally solvable with the help of neural networks. Furthermore, this can be set up in such a way to guarantee maximum information extraction which results in the tightest constraints on the parameters of interest. We explore this approach in this paper. On the machine learning side, advances over the last few years, especially in the field of simulation-based inference (SBI) [37, 38, 39] have laid the groundwork for such a task. In this framework inference is possible solely with the help of a simulator and a probability density estimator (e.g. to estimate the posterior of cosmological parameters). The likelihood is implicitly shifted into the simulation and need not be specified in the inference process. This is of utmost importance for machine-learned summary statistics, for which we cannot model the likelihood. Even in the case of hand-crafted summaries, the inability to write down a likelihood severely limits cosmological inference tasks. The detail of the employed density estimator may vary. Early implementations of SBI made use of traditional density estimators such as approximate bayesian computation (ABC) [40, 41, 42]. However, ABC is lacking in computational efficiency and is especially struggling with higher dimensional targets. As a result, these older methods have largely been replaced by neural network accelerated density estimators such as normalizing flows [43]. The SBI framework has been successfully applied to astrophysical [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67] and cosmological [68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96] inference problems to great detail showing great overall performance and efficiency. In this work we utilize SBI to address cosmological inference on galaxy catalogs where standard perturbative EFT approaches can no longer be applied. Beyond increasing the amount of information which can be used for cosmological inference, our approach also has advantages with respect to hydrodynamical modeling itself. Our approach provides a novel evaluation tool for simulations as it enables us to compare the differences among different simulation models. This is particularly relevant with respect to selecting relevant simulation parameters, reducing the number of simulations necessary for cosmological inference. In practice, this method provides a quantitative handle on priors of baryonic feedback and galaxy formation models. It can also be used for calibration of simulations using observations as discussed in Ref. [90]. In order to perform inference within this framework, the data is compressed to a summary vector [97]. As proposed by Ref. [98], feed forward neural network have proven to be a powerful way of achieving compression for SBI. A popular way to perform this compression consists of training a neural network regressor with a mean square error loss function to predict the input parameters of a simulation as in Refs. [80, 94]. The approach is based on previous work by Ref. [99] in the context of linear regression which was extended to a neural network based approach by Ref. [100]. This prediction of the input simulation parameters would then be considered a “summary statistic” with which SBI can be performed. This way of compression has some immediate disadvantages. The resulting data vector is not necessarily information optimal which restricts the statistical power of the subsequent inference process. Secondly, the dimensionality of the summary vector is fixed to the dimensionality of the input parameter vector, hindering the network from finding a potentially simpler higher dimensional embedding space. Furthermore, the mean square error is only the maximum likelihood estimate of normally distributed variables [101]. This attenuates the power of SBI, which lies in its ability to perform inference for explicitly unknown likelihoods. Ref. [102] have proposed information maximizing neural networks, a way of compression in which the Gaussian approximation of the Fisher information is maximized at one or multiple fiducial points in parameter space. This is a non-linear extension of the MOPED algorithm [103]. While this approach does maximize the information contained in the summary vector by construction, it is computationally expensive if trained at multiple fiducial points. This is necessary as Ref. [104] have explicitly shown that if trained at only a single point, the information content of the learned summary varies widely over the entire prior volume. Another drawback to this compression scheme is that it requires covariance estimates and finite differences to compute the Fisher information. In order to enable these computations additional simulations have to be run. Recently, Ref. [105] have applied such a scheme to find information maximizing summary statistics complementary to the power spectrum from weak lensing convergence maps, and found a significant improvement in constraining power. In comparison to their work we attempt to discover summaries which are not only optimal at a single point in parameter space, but for the entire simulation range. Another alternative is proposed by Ref. [106], who maximize the variational mutual information between the summary and the cosmological parameters. Effectively, this follows the previously published framework by Ref. [107]. The approach has been applied recently by Refs. [108, 109] who found this method to be very performative, even yielding sufficient summaries in idealized settings. Ref. [110] have also used this approach to find summaries complementary to human-made statistics. In all these works, the two-dimensional input data is given to a convolutional neural network which performs the compression. The compressed data vector is then directly fed into a simulation-based inference pipeline which computes the variational mutual information by means of the expected Kullback-Leibler divergence. In this work we extend on these efforts and combine a graph neural network for compression with the traditional SBI framework. Such a graph perspective is natural in the context of survey catalogs which are straight-forwardly translated into a graph where each catalog entry denotes a node and the relative position is encoded in the edges. A key advantage of this approach is that we gain access to the compressed data vector resulting from the graph, i.e. a machine-learned summary statistic. We investigate the inference process itself by examining this summary. In doing so we identify not only scales that the machine identifies as important, but also assess which subgrid physics processes the pipeline has learned automatically. In contrast to other works that have investigated the importance of feedback parameters in these simulations, we can make statements not only in regard to any given human-made summary statistic, but in regard to the total cosmological information. To facilitate data efficiency, following the geometric deep learning paradigm [111], it is strongly advised to reflect important symmetries also in the machine learning model. Mathematical graphs provide for a natural way of achieving this. They consist of nodes connected by edges and can be constructed in such a way that translation and rotation symmetries of the underlying data are captured. Graph neural networks (GNNs) [112, 111] operate on graphs and have already been explored for their capabilities in cosmological inference [113, 114, 115, 116, 117, 118, 119, 120, 121]. An important feature of this setup is that it enables the input of variably sized data into the GNN as long as it can be structured into a graph. This architecture has been mostly explored with the moment network loss function [122], which predicts the first two moments of the marginal parameter posterior. Refs. [117] and [118] in particular have shown that using positional information and velocity information in one dimension enables robust inference across feedback models. We build upon these works by coupling this graph architecture with a full SBI framework based on masked autoregressive flows [39]. The resulting architecture is similar to that of Ref. [123] who have used such an approach to recover the dark matter profile of dwarf galaxies and to that of Ref. [124]. Apart from a different use case here, our architecture also differs as we treat the number of learned summaries as an optimizable hyperparameter. Once the architecture is trained, we are not only interested in the constraining power we achieve, but also in the Physics contained in the machine-learned summary. Concerning our interpretability efforts, we would like to mention similar works on interpretability for machine learning in the field of Cosmology by Refs. [125, 126, 127, 128]. In these works additional constraints are set on the respective machine learning architectures to either disentangle the latent space or link the resulting latent variables with theoretical quantities a priori. We do not follow such an approach as to not prohibit the inference network from maximum performance. Instead, we investigate how much insight into the summaries we can get from purely analyzing them a posteriori. We leave the exploration of a priori more easily interpretable machine learning architectures as are used in these works up to further study. This paper is structured as follows: In Section 2 we describe the simulations we use in our analysis. Section 3 describes our inference pipeline and how we train and validate our models. We present our results in Section 4, before concluding in Section 5."
https://arxiv.org/html/2411.08945v1,Evaluating cosmological simulations of galaxy formation with spectral variance in the optical window,"Cosmological hydrodynamical simulations provide valuable insights on galaxy evolution when coupled with observational data. Comparisons with real galaxies are typically performed via scaling relations of model fitting. Here we follow an alternative approach based on the spectral variance in a model-independent way. We build upon the work presented in Sharbaf et al. (2023) that studied the covariance of high quality SDSS continuum-subtracted spectra in a relatively narrow range of velocity dispersion (\sigma\in[100,150] km s-1). Here the same analysis is applied to synthetic data from the EAGLE and Illustris TNG100 simulations, to assess the ability of these runs to mimic real galaxies. The real and simulated spectra are consistent regarding spectral variance, although with subtle differences that can inform the implementation of subgrid physics. Spectral fitting done a posteriori on stacks segregated with respect to latent space reveals that the first principal component (PC1) is predominantly influenced by the stellar age distribution, with an underlying age-metallicity degeneracy. Good agreement is found regarding star formation prescriptions but there is disagreement with AGN feedback, that also affects the subset of quiescent galaxies. We show a substantial difference in the implementation of the AGN subgrid, regarding central black hole seeding, that could lead to the mismatch. Differences are manifest between these two simulations in the star formation histories stacked with respect to latent space. We emphasise that this methodology only relies on the spectral variance to assess whether simulations provide a true representation of galaxy formation.","Galaxy formation and evolution represent one of the most significant frontiers of astrophysics over the past decade. An exploration of the formation history of galaxies provides insights into the various physical processes involved in creating the stellar and gaseous components that we can observe through telescopes or investigate through simulations. Cosmological hydrodynamical simulations, such as EAGLE (Schaye et al., 2015) and IllustrisTNG (Pillepich et al., 2018), offer valuable insights when coupled with high-quality survey data most notably the Sloan Digital Sky Survey (York et al., 2000), enhancing our understanding of galaxy evolution. There is a complementary role for both observation and simulation data. This is because observations are used to constrain various parameters in simulations, while simulations are used to interpret the observations with fundamental properties of galaxies. Galaxies form and evolve as a result of the interaction between diverse physical processes that, in addition to gravity, influence baryonic matter. Given the inherent resolution limit of simulations that rely on a finite set of particles, or gridpoints, sub-grid physics is employed for the modeling of baryonic processes below the galactic scale, such as the formation of Black Holes (BHs), their growth, and feedback. Incorporating these processes into simulations represents a significant challenge because these are complex physical processes and it is difficult to develop numerical algorithms that can accurately model their effects in a computationally efficient manner (see, e.g, Somerville & Davé, 2015; Naab & Ostriker, 2017; Crain & van de Voort, 2023). A variety of sub-grid models are employed in the simulations, including different locations and masses for seeding BHs, different models for computing accretion on BHs, various efficiency factors and modeling techniques to inject energy released by AGNs, AGN feedback channels are explicitly determined by the BH mass in some models, while uniform feedback is assumed in others. Feedback from star formation is another important subgrid process that is expected to affect in a fundamental way the observed distribution of galaxies(e.g., Schaye & Dalla Vecchia, 2008). From a theoretical standpoint, state-of-the-art hydrodynamical simulations such as EAGLE (Schaye et al., 2015) and IllustrisTNG (Pillepich et al., 2018) reproduce the general fundamental properties of galaxies i.e. the evolution of the galaxy mass function (Furlong et al., 2015; Kaviraj et al., 2017; Pillepich et al., 2018), AGN luminosity (Rosas-Guevara et al., 2016; Volonteri et al., 2016; McAlpine et al., 2017) as well as the bimodality of galaxy color (Trayford et al., 2015; Trayford et al., 2016; Nelson et al., 2018), and the SFR and UVJ-based quenched fraction at z\lesssim 2-3 (Donnari et al., 2019, 2021). Despite the good agreement between observational constraints and simulations, and the recent tremendous progress that has been made in these areas, there are still challenges to overcome. Non-trivial subgrid physics is therefore the major source of uncertainty in cosmological simulations, and adjusting these parameters can significantly alter results (Okamoto et al., 2005; Schaye et al., 2010; Scannapieco et al., 2012; Haas et al., 2013a, b; Le Brun et al., 2014; Torrey et al., 2014; Negri & Volonteri, 2017). By comparing simulations and observations, sub-grid physics can be tested. Simulations and observations are frequently compared in papers (e.g., Nelson et al., 2015; Pillepich et al., 2018; Vogelsberger et al., 2014; Habouzit et al., 2021), but the comparison with observational constraints can be challenging since these constraints often require physical modeling or assumptions. Using variance111In the strictest sense, we refer here to covariance, as we are dealing with multivariate analysis, but we use both terms with a similar meaning and prefer to use the term “spectral variance”. analysis, it is possible to obtain information from galaxy spectra in a model-independent manner, and without imposing physical constraints on the model (e.g., Ferreras et al., 2006; Rogers et al., 2007, 2010b; Sharbaf et al., 2023). Galaxy spectra encode the kinematics, age, and chemical composition of the stellar populations underlying them, thus representing one of the most reliable sources of information about galaxies. The spectral variance can be combined with stellar population synthesis models (e.g., Bruzual & Charlot, 2003; Vazdekis et al., 2016) to retrieve information from galaxy spectra. Following the methodology of Rogers et al. (2007), we use a multivariate analysis method to explore the retrieval of information from galaxy spectra on a model-independent basis. This method has been applied to a general sample of SDSS galaxies in Sharbaf et al. (2023), hereafter referenced as PCA-SDSS. We performed principal component analysis on three separate groups of galaxy spectra: Star Forming (SF), AGN, and Quiescent (Q), based on the nebular emission properties. We emphasize that the variance of the input data in this work only relates to the absorption lines in the photospheres of stellar populations. The PCA-SDSS study analyses SDSS optical spectra using PCA to determine what physical phenomena contribute to the spectral variance, and suggested that galaxy structure may be controlled by a single (or a few) parameters, since only one component demonstrates a correlation with age, and plays a primary role as an evolutionary trend. In this study, we evaluate how the variance in the synthetic spectra created from the EAGLE (Schaye et al., 2015) and IllustrisTNG (Pillepich et al., 2018) simulations behaves in comparison with the optical spectra (York et al., 2000), and evaluate the subgrid physics. A comparison of the different properties that are successfully reproduced and those that are not is made. We are interested in understanding how different sub-grid models can produce different spectral variance. The structure of the paper is as follows: the sample and the simulations are presented in section 2, followed by data pre-processing and an explanation of the restrictions in section 3. In section 4 we show how the synthetic spectra are produced from the simulations. The decomposition of optical spectra into principal components and projection of the synthetic and optical spectra to those principal components are explained in section 5, and the projections are explored in section 6, along with models of population synthesis. We discuss the results and present our conclusions in section 7."
https://arxiv.org/html/2411.09606v1,Carl Wirtz’ article from 1924 in Astronomische Nachrichten on the radial motions of spiral nebulae,"In the year 1924, a paper by Carl Wirtz appeared in ”Astronomische Nachrichten"", entitled ""De Sitter’s cosmology and the radial motion of spiral galaxies"". This paper and its author remained largely unnoticed by the community, but it seems to be the first cosmological interpretation of the redshift of galaxies as a time dilation effect and the expansion of the Universe. Edwin Hubble knew Wirtz’ publications quite well. The modern reader would find Wirtz’ own understanding diffuse and contradictory in some aspects, but that reflected the early literature on nebulae, to which he himself made important contributions. The 100th anniversary provides a good opportunity to present an English transcription to the community, which can be found in the appendix. This anniversary also provokes to ask for the present status of cosmology which many authors see in a crisis. From an observational viewpoint it shall be illustrated that until today there is no consistent/convincing understanding of how the Universe evolved.","1 Carl Wirtz and the expansion of the Universe, the beginning of modern observational cosmology 1.1 Carl Wirtz is not prominent The discovery of the expansion of the Universe marked the beginning of modern observational cosmology. How that happened was rather a process than a moment (e.g. van den Bergh \APACyear2011; Grøn \APACyear2018; Cervantes-Cota \BOthers. \APACyear2023) and the prominent names are known to every astronomer and with Einstein even outside the astronomical community. Relativistic cosmology has been founded by Einstein himself and de Sitter, but de Sitter was also a key author for making Einstein’s theory quickly known to the English speaking community (de Sitter, \APACyear1916\APACexlab\BCnt1, \APACyear1916\APACexlab\BCnt2) (de Sitter wrote German articles as well). The same can be said for Eddington (\APACyear1923) and Weyl (\APACyear1922). Friedmann’s fundamental papers on world models as general solutions of Einstein’s field equations (Friedmann, \APACyear1922, \APACyear1924) were written in German and were more or less ignored by the community for about a decade (although Einstein knew them well) until Robertson (\APACyear1933) presented his review of relativistic cosmology. Friedmann already introduced the beginning of he Universe and described pulsating Universes with a period of the order 10^{10} years. On the observational side, the question who discovered the expansion normally leads to the names of Lemaitre and Hubble. However, in the recent review article of Cervantes-Cota \BOthers. (\APACyear2023), a paragraph is also devoted to Carl Wirtz, a name that is not known to every astronomer. The low prominence the work of Carl Wirtz enjoys in comparison with Lemaitre and Hubble, is inappropriate. There are a few modern references to Wirtz in the English literature, most notably Duerbeck (\APACyear1989), Seitter \BBA Duerbeck (\APACyear1999) and Duerbeck (\APACyear2002), now also Cervantes-Cota \BOthers. (\APACyear2023). There are a few more in German in amateur journals, we cite Priester \BBA Schaaf (\APACyear1987), Duerbeck \BBA Seitter (\APACyear1990\APACexlab\BCnt1), Duerbeck \BBA Seitter (\APACyear1990\APACexlab\BCnt2), and Appenzeller (\APACyear2009). In the year 1924, Wirtz published in ""Astronomische Nachrichten"" a paper (Wirtz, \APACyear1924) which has the characteristics to count as the starting point of modern observational cosmology (at the time of writing: 22 citations). We find its 100th anniversary a nice opportunity to present that short paper in an English translation to the community in the same journal. The translated article is presented in the Appendix. Basic biographical dates: Carl Wirtz was born 1876 in Krefeld and died 1938 in Hamburg. He got his PhD 1899 at Bonn University, worked as assistant in Vienna until 1901, then moved to Strasbourg, where he got a professorship in 1909 and stayed there until 1919. Then he moved to Kiel as professor and was forced into early retirement 1937. In the following, the work of Carl Wirtz shall be briefly introduced in the context of cosmological quest in the early 20th century. 1.2 Earlier work of Carl Wirtz in the context of cosmology from 1915 -1923 It was Schwarzschild who introduced the concept of curvature of space in cosmology (Schwarzschild, \APACyear1900; Stewart \BOthers., \APACyear1998). He estimated the minimal curvature radius of the Universe to be approximately 10^{8} astronomical units by consideration of the maximal inhomogeneity of the distribution of stars. When Einstein (\APACyear1915) published his famous equation connecting gravity with the geometry of space-time, the Universe was still commonly understood as consisting mainly of a stellar system populating a large volume, as Schwarzschild did. The distinction between Galactic and extragalactic objects had no reason. The distances and nature of the spiral nebulae were unknown. de Sitter (\APACyear1917) published his world models as cosmological solutions to Einstein’s equations. De Sitter’s Universe (his model B) was finite with a certain curvature radius R, static and empty. Time delation with distance r resulted through the metric tensor element g_{44} which in the four-dimensional line element ds^{2}=...+cos^{2}(r/R)c^{2}dt^{2} effectively gives a non-linear relation between redshift and distance. de Sitter himself tried to identify cosmological redshifts among various types of stars and only had the Andromeda nebula, NGC 1068, and NGC 4696 as extragalactic objects. But Andromeda is approaching and the other two galaxies show quite similar velocities about 1000 km/s, so the effect of increasing time dilation with distance which he hoped to see, was not visible. At about the same time Wirtz (\APACyear1918) wrote a paper on proper motions of 378 spiral nebulae. That was a topic that he had addressed earlier in several contributions. He used the (spurious) proper motions to derive an apex of the solar motion quite similar to that from stellar streams. The mean proper motion for the spirals resulted to 0.027 ""/year. However, this time he also compiled 15 radial velocities of spiral galaxies and noted the higher velocities with respect to the stellar system, but also that a common velocity and a vertex direction is not a good representation. The introduction of a constant K independent from the coordinates at the sphere led to a much better representation through the equation v=V_{x}*cos\alpha~{}cos\delta+V_{y}*sin\alpha~{}cos\delta+V_{z}*sin\delta+K (1) v being the velocity of an individual object at spherical coordinates \alpha and \delta, and V_{x},V_{y},V_{z} the velocity components of the (negative) apex motion of the observer. He got the best fit of the above relation for V_{x}=-145 km/s, V_{y}=-268 km/s, V_{z}=-762 km/s (i.e. V=-820 km/s), and K=+656 km/s, and interpreted V as the radial correspondence to the proper motion. Then it is straightforward to derive a mean distance for the spiral nebulae. His result was 6.66 kpc and he concluded that the spiral nebulae are located outside the Milky Way system. He then tried to interpret the ""strange"" constant K. In his own words: If we give this value a literal interpretation, then it means that the system of spiral nebulae drives apart with respect to the actual position of the solar system as the centre with a velocity of +656 km/s. The paper concludes: Also in the case of the nebulae one expects that we hold single threads of a mesh in hands whose complete pattern we still cannot disentangle. Four years later Wirtz published a paper entitled On the statistics of the radial motions of spiral nebulae and globular clusters (Wirtz, \APACyear1922), again in ""Astronomische Nachrichten"". Strikingly, proper motions of the nebulae that had accompanied his scientific career for a long time, were not mentioned at all. Apparently this chapter had been closed with the previous paper. Now Wirtz lists only radial velocities of 29 spiral nebulae and 10 globular clusters. For the spiral nebulae, he finds a somewhat higher K-term of +887 km/s, meaning that the entire system is moving away with this common radial velocity. He now considers the residuals to this common movements which he calls ""errors"" and understands them as the peculiar velocities of the spiral galaxies. For these velocities, he finds a slight dependence on Galactic latitude in the sense that the polar nebulae have higher velocities and on apparent magnitude, where the brightest nebulae are approaching and the fainter ones receding. This dependence also appears with the angular diameters. Wirtz: Of course, one finds the analog relation also with the diameter of the nebulae, where small diameters belong to a positive change of the distance, large diameters to a negative change. These statistical phenomena occur on top of the most striking and principal instance that can be described as if the system of spiral nebulae was drifting apart relative to our position… and the dependence on magnitude indicates that the nearest or the most massive spiral nebulae show a slower outward motion than the more distant or the less massive ones. Here appears for the first time the notion of a radial velocity dependent on distance, but disguised as a second order effect that is still covered by the imagination of a system of spiral galaxies moving away with a constant velocity. In modern language, Wirtz had to deal with the fact that many of his galaxies are not in the Hubble-flow. Wirtz’ main observational work in Strasbourg was the photometry of nebulae, including globular clusters. This resulted in a catalogue of surface brightnesses, total brightnesses, and angular diameters for 566 objects (Wirtz, \APACyear1923). Presumably, Wirtz saw that apparent magnitudes and angular diameters are not well correlated and therefore did not use magnitudes as distance indicators. 1.3 The role of Friedmann Two publications of the mathematician, meteorologist, geophysicist, pilot and adventurer dominated the cosmological field for (at least!) 100 years. In a static Universe the old mystification of circular orbits may have survived. Planetary orbits must(!) be circular, elliptical orbits are ugly. And the Universe cannot be variable! In this analogy, Friedmann’s role is the closest to Kepler’s. In an expanding Friedmann universe, time dilation results naturally as proportional to 1+z and a linear velocity-distance relation is necessary to maintain homogeneity. In 1922 Friedmann (\APACyear1922) already introduced ""non-stationary"" world models (in German). There are no indications that Wirtz was aware of Friedmann’s work, but even Einstein needed some time to see that Friedmann’s solutions were indeed correct. The most famous physicist of the epoch gives in a brief note his blessings to the concept of an expanding Universe and is ignored himself (Einstein, \APACyear1923)! Perhaps the German language again was an obstacle for the international community. Also Friedmann’s last paper (Friedmann, \APACyear1924) did not express interest in observational evidence for his models. 1.4 The work of Silberstein, Oepik, Lundmark and Strömberg In the same year 1924 appeared the work of Silberstein (\APACyear1924). Silberstein understood de Sitter’s time dilation as a relativistic Doppler effect and derived a relation between redshift and curvature radius of the Universe. To determine this radius R, he used the radial velocities of 7 globular clusters and got R=1.1\times 10^{7} pc. Silberstein (\APACyear1929) still argued strongly for a value of 1.67\times 10^{6}pc, meaning that in his de Sitter model, the largest possible distance in the Universe would be 1.3\times 10^{6} pc, contradicting strongly the by then established extragalactic distances. The extragalactic nature of the Andromeda nebula was by 1922 quite clear. Oepik (\APACyear1922), by using the rotational velocity and applying an early version of the Tully-Fisher relation, got a distance of 500000 pc, three years before E\BPBIP. Hubble (\APACyear1925) published in a much more famous paper the Cepheid distance of NGC 6822 of 214 000 pc, in Hubble’s words the first object definitely assigned to a region outside the galactic system. However, it is Lundmark (\APACyear1924\APACexlab\BCnt1) who deserved the credit to have shown the first ""Hubble-diagram"", although a velocity-distance relation is not recognisable in his graph. As for Silberstein, his main objective was to measure the curvature radius of a de Sitter Universe, and not the nature of spiral galaxies. For that he needed absolute distances for the spiral nebulae. He used the Andromeda nebulae as the reference distance of 200 000 pc which he got from comparing Galactic novae with novae in Andromeda. He explicitly did not exclude a distance of 500 000 pc, as favoured by Oepik (\APACyear1922). Lundmark adopted Silberstein’s formula and interpretation, but with a much enlarged data set, ranging from nearby stars over Cepheids and novae to the sample of spiral galaxies which had been used also by Wirtz (he apparently got the spiral galaxy data personally from Slipher, Lundmark \APACyear1924\APACexlab\BCnt2). Lundmark determined individual distances for his spiral galaxies through the comparison with Andromeda regarding brightnesses and angular diameters. Without an established distance scale, the individual errors are of course large, particularly the three most distant galaxies (on Lundmark’s scale) NGC 278, NGC 1700, and NGC 2681 show moderate velocities around 700 km/s. And many nebulae are, in modern terms, not in the Hubble flow, unfortunately also Andromeda which is the reference. Lundmark also gives ""mean distances"" for his sample by the comparison of the K-term with (spurious) proper motions. Although he labels the proper motion values as ""illusory"" and ""only of use for giving a upper limit to the motion"", the mere fact that there is a common proper motion of spiral nebulae, clashed with the concept of expansion. A third author who used Slipher’s radial velocities in a cosmological context was Gustaf Strömberg (Stromberg, \APACyear1925). He performed his analysis without the knowledge of Lundmarks results, but expressed in the published paper his satisfaction about the agreement with the main result, namely that there is no relation of redshift with distance. Strömberg calculated the distances for spiral nebulae as 10^{0.2m} where the m are apparent magnitudes as given by Wirtz (\APACyear1923)! He only gives correlation coefficients. The strongest correlation has been found with the angular distance from the sun’s apex. As one sees already from Lundmarks sample, the individual distances based on magnitudes are simply too uncertain to uncover any velocity-distance relation. One may find some irony in the fact that Wirtz’ own measurements are used against his findings. Strömberg moreover understood the redshifts as caused by de Sitter’s time dilation not as indicating a real motion, which he considers to be ""ficticious"". 1.5 Wirtz’ influence on Hubble To what degree Hubble was influenced by the reading of Wirtz’ publications will remain elusive in detail (see Way \APACyear2013 for more comments on Hubble’s work) , but there is no doubt that Hubble knew Wirtz’ 1924 paper (and others) quite well. The German language apparently was no obstacle to him. He cited regularly German papers, but firstly not Wirtz, particularly not in E. Hubble (\APACyear1929) nor in E. Hubble \BBA Humason (\APACyear1931). However, Wirtz occupies a quite prominent place in Hubble’s book ""The realm of the nebulae"" from 1936. In the chapter ""The velocity-distance relation"" Hubble describes the content of Wirtz (\APACyear1918) and Wirtz (\APACyear1922) in some detail (for the latter paper, he cites the year as 1921, not 1922). But he praises Wirtz mainly as the discoverer of the ""K-term"", the mean residual of radial velocities of the nebulae after correction for solar motion. Hubble calls Wirtz even ""the leader in the field"". He qualifies the results of the 1924 paper as ""suggestive rather than definitive"". In conjunction with the discussion of the work of Lundmark (\APACyear1924\APACexlab\BCnt2) and Strömberg (\APACyear1925), Hubble concluded that by 1925, ""the data did not establish a relation"" (between distance and radial velocity) and so repeated Lundmark’s and Strömberg’s statements, probably without looking further into the details of their distance determinations. Nevertheless, Hubble adds a footnote ""Wirtz later published a stimulating popular presentation of the investigation and the implications of the results (Scientia, 38, 303, 1925) in which he assumes that de Sitter’s prediction has been verified."" The 100 years after Wirtz’ paper have mainly seen an acceptance of Friedmann’s understanding of the expansion of the Universe, culminating in the emergence of the ""standard cosmology"" with Dark Matter and Dark Energy as its pillars, simultaneously its threats."
https://arxiv.org/html/2411.09099v1,Detection of an orthogonal alignment between parsec scale AGN jets and their host galaxies,"The relationship between galaxies and their supermassive black holes (SMBHs) is an area of active research. One way to investigate this is to compare parsec-scale jets formed by SMBHs with the projected shape of their kiloparsec-scale host galaxies. We analyse Very Long Baseline Interferometry (VLBI) images of Active Galactic Nuclei (AGN) and optical images of their host galaxies. We compare the inner-jet position angle in VLBI-detected radio sources with the optical shapes of galaxies as measured by several large optical surveys. In total 6273 galaxy-AGN pairs were found. We carefully account for the systematics of the cross-matched sources and find that Dark Energy Spectroscopic Instrument Legacy Imaging Surveys data (DESI LS) is significantly less affected by them. Using DESI LS, with which 5853 galaxy-AGN pairs were cross-matched, we find a weak but significant alignment signal (with a p-value \lesssim 0.01) between the parsec-scale AGN jet and the kpc projected minor axis of the optical host galaxy in sources with measured spectroscopic redshifts. Our results show that the observed source properties are connected over 3 orders of magnitude in scale. This points towards an intimate connection between the SMBH, their host galaxies and their subsequent evolution.","Conclusion We find good evidence that the pc-scale VLBI jet is connected to the projected kpc-scale optical host galaxy morphology. The direction of the pc-scale jet (which may be a proxy for the SMBH-accretion disk system) appears to be oriented perpendicularly to the projected optical shape of the host galaxy with a p-value < 0.05. The interpretation of these results is complicated but may have far-reaching implications. Ultimately these results will need to be followed up with higher source counts and higher-resolution optical observations (e.g. HST, JWST) and with radio-quiet VLBI observations in order to determine how universal such a connection may be. In the near future there will be large surveys such will be performed using the Vera C. Rubin Observatory and Square Kilometer Array (SKA) VLBI and therefore the potential to vastly increase our sample sizes."
https://arxiv.org/html/2411.09081v1,Hawking Radiation from non-evaporating primordial black holes cannot enable the formation of direct collapse black holes,"The formation of supermassive black holes (SMBHs) in the early Universe is a subject of significant debate. In this study, we examine whether non-evaporating primordial black holes (PBHs) can offer a solution. We establish initial constraints on the range of PBH masses that correspond to Hawking radiation (HR) effective temperatures in the range needed to avoid the fragmentation of primordial gas into smaller, stellar-mass black holes. We also investigate the specific intensity of the HR from non-evaporating PBHs and compare it with the critical radiation needed for direct collapse black holes (DCBHs). We show that HR from non-evaporating PBHs cannot serve as the heating mechanism to facilitate the formation of the seeds for the SMBHs we observe in the high-redshift Universe unless, perhaps, the PBHs within the relevant mass range comprise a significant fraction of dark matter and are significantly clustered towards the center of the primordial halo.","In the standard model of cosmology, our Universe is expanding from a hot and dense state [e.g. 1]. Astrophysical structures begin to form when initial fluctuations collapse to form the first gas clouds (also known as primordial gas clouds). These are the birthplaces of the first galaxies [e.g. 2, 3, 4, 5]. Many, if not all, of these galaxies are believed to harbor a massive black hole (BH) in their center. Observations of quasars formed in the early Universe indicate that a number of these BHs have masses of more than a billion solar masses, and were formed before the Universe was a billion years old111Such as the quasar ULAS J1342+0928, whose SMBH has a mass of 800 million solar masses [6]. See also [7, 8, 9, 10].. The formation of these high-mass black holes, aptly called supermassive black holes (SMBHs), is a matter of considerable debate [e.g. 11, 10]. Three of the most prevalent proposed scenarios, outlined in [11], include: 1) the collapse of massive Pop III stars and their growth due to accretion [12, 13, 14, 15], 2) runaway mergers in dense star clusters [16], and 3) the collapse of a primordial, metal-free gas cloud [17, 18, 19, 20, 21, 22, 23, 24, 25]. Alternative scenarios, among others, involve the growth of primordial black holes (PBHs) [26, 27], the increase of the mass of protostellar collapse in the presence of magnetic fields [28, 29] or gas dynamical processes [30, 31]. For a more detailed coverage of these and other scenarios, see [32, 33, 34, 35, 10, 36, 5]. Even the most common proposals are not without problems. For example, a newly-born Pop III star in the range of \sim\rm 100\>M_{\odot} would need to accrete continuously at a rate close to the Eddington limit for at least a Gyr to reach masses close to \rm 10^{9}\>M_{\odot} at redshifts z=6-7 [37, 33, 35], which poses difficulties for the first scenario. Dense star clusters would need to be able to retain their members after the mergers (i.e. to overpower the gravitational kicks that lead to the ejection of the merged object from the cluster [38, 39]), posing difficulties for the second scenario. This paper focuses on the third scenario: the direct collapse of metal-free gas clouds, a ‘heavy seeds’ scenario. In this case, the masses of the initial BHs are on the order of \rm 10^{5}-10^{6}\>M_{\odot} [e.g. 40, 22]. Therefore, they could grow through more viable rates of accretion into the SMBHs we observe by z\approx 6-7. One of the key prerequisites of this scenario is to avoid fragmentation of the collapsing gas cloud222The presence of angular momentum is also an obstacle to massive BH collapse that needs to be overcome. Here, we assume that the gas clouds in question have overcome this issue by assuming they have either formed with negligible angular momentum or that they have lost their initial angular momentum due to interactions with their environment [41].. To avoid fragmentation, this family of models usually assumes the presence of an ‘atomic cooling halo’–a halo without significant metals or molecules that could cool efficiently and lead to fragmentation of the original cloud into smaller clumps–with an equilibrium temperature on the order of T_{\rm crit}\sim 10^{4} K [32, 23] (for some alternative direct-collapse scenarios see e.g. [42, 31]). To achieve such high temperatures, a number of mechanisms have been invoked [10, 43, 44, 45]. In this paper, we consider the presence of strong UV radiation in the Lyman-Werner (LW) range (11.2-13.6 eV) that is able to suppress the formation of molecular hydrogen H2 [12, 46, 47, 48, 49] through radiation from PBHs. For the heating mechanism, we investigate whether the Hawking radiation (HR) [50] emitted from early-Universe PBHs [51, 52] can create the necessary conditions for the formation of large BH seeds via the direct collapse of primordial gas clouds. In this proposed mechanism, the required energy is supplied by the HR of an evaporating PBH that is still far from “exploding”. We model this radiation as a blackbody spectrum, focusing only on primary photon emission. The role of PBHs in the formation of the first stars and galaxies has been investigated before [53, 54, 55], but remains an important unanswered question in the scientific community. Most recently, [55] studied the heating effects of secondary HR spectra from “exploding” PBHs in primordial gas clouds. They find that in the presence of significant clustering of the PBHs in the clouds, PBHs of masses on the order of 10^{14} g can lead to a successful direct collapse of the cloud. On the other hand, [53, 54] used cosmological simulations and semi-analytical models to explore the effects of stellar-mass (\sim 10-100\ \rm M_{\odot}) PBHs in the acceleration of structure formation and gas heating by accretion feedback on the BHs. In the former case, they find that structures are similar to the ones found in \LambdaCDM simulations, but in latter case, they show that LW photons from accretion onto stellar-mass PBHs in the halo can indeed promote the formation of massive seeds through the direct-collapse scenario. In this work, we describe a simplified and easily-modifiable analysis framework, more in line with [55], in hopes that the scientific community can continue to build on this work to answer this open question. For all cosmological calculations in this paper, we assume a flat \LambdaCDM cosmology with H_{0}=70 km/s/Mpc and \Omega_{m}=0.3. This paper is organized as follows: Section 2 reviews the main consequences of Hawking radiation. Section 3 introduces some of the relevant features of primordial black holes. Section 4 sets constraints on the primordial black holes that could act as sources of the Hawking radiation needed to produce ‘heavy seeds’, and compares these constraints with current observational limits. Finally, Section 5 provides a comprehensive discussion of the assumptions and caveats of this work and proposes extensions to this simplified model."
https://arxiv.org/html/2411.08956v1,Reconciling concentration to virial mass relations,"Context. The concentration–virial mass (c-M) relation is a fundamental scaling relation within the standard cold dark matter (\LambdaCDM) framework well established in numerical simulations. However, observational constraints of this relation are hampered by the difficulty of characterising the properties of dark matter haloes. Recent comparisons between simulations and observations have suggested a systematic difference of the c-M relation, with higher concentrations in the latter.Aims. In this work, we undertake detailed comparisons between simulated galaxies and observations of a sample of strong-lensing galaxies.Methods. We explore several factors of the comparison with strong gravitational lensing constraints, including the choice of the generic dark matter density profile, the effect of radial resolution, the reconstruction limits of observed versus simulated mass profiles, and the role of the initial mass function in the derivation of the dark matter parameters. Furthermore, we show the dependence of the c-M relation on reconstruction and model errors through a detailed comparison of real and simulated gravitational lensing systems.Results. An effective reconciliation of simulated and observed c-M relations can be achieved if one considers less strict assumptions on the dark matter profile, for example, by changing the slope of a generic NFW profile or focusing on rather extreme combinations of stellar-to-dark matter distributions. A minor effect is inherent to the applied method: fits to the NFW profile on a less well-constrained inner mass profile yield slightly higher concentrations and lower virial masses.","The standard paradigm of galaxy formation rests on the presence of two main ingredients: dark matter and baryons. Both are subject to gravitational forces, but only the latter can be detected with photons. In this framework, dark matter represents a fundamental component, as it provides the backbone structure over large scales (cosmic web) and within the scales in which galaxies are found (haloes). The properties of dark matter haloes are thus essential to understanding galaxy formation, but these properties are limited to indirect constraints involving observations of the baryonic matter. Gravitational lensing provides a special method of detection by using the distortions exerted on the photons emitted by a background source as they pass through a gravitating structure. By ‘removing’ the contribution of the baryons from the lensing signal, it is thus possible to study dark matter haloes. One of the main correlations found in haloes is the concentration versus mass relation (hereafter c-M, Navarro et al. 1997), by which more massive haloes tend to have progressively lower concentrations, albeit with a large scatter. This relation has been readily found in simulations (e.g. Bullock et al., 2001; Macciò et al., 2007; Dutton & Macciò, 2014; Diemer & Joyce, 2019; Ishiyama et al., 2021), and its scatter could unravel the details concerning the growth of haloes (e.g. Wang et al., 2020). As a first approximation, this trend is a direct consequence of the bottom-up scenario of structure formation, as lower mass haloes formed statistically at earlier cosmic times, when the density in the expanding Universe was higher (see, e.g. Mo et al., 2010). The concentration of dark matter haloes depends on the adopted cosmology (Macciò et al., 2008). However, this result is expected from a simple spherical collapse scenario, whereas a more realistic depiction would involve non-spherical structures and extended mass assembly histories, making the interpretation of concentration more complicated. Virialised haloes, in principle, should have higher concentrations (Neto et al., 2007). If the subsequent growth after the formation of the ”first” structure is slow, we can expect a gradual increase of the concentration with total mass, whereas faster growth can keep the concentration unchanged (Correa et al., 2015). Therefore, variations among haloes on the c-M plane depend on their mass assembly history. The large scatter of the c-M relation found in simulations is thus an indicator of the diverse formation histories of structures. While virial mass is a good guess for a first-order parameter, more information is needed to describe the details of individual haloes (e.g. assembly bias; Wechsler et al. 2006). For instance, at a fixed halo mass, one would expect ‘older’ haloes to populate the high concentration envelope of the c-M relation. The dynamical state of a halo – shape, spin, virialisation – also affects its location on the c-M plot. Observational constraints are harder to come by and mostly rely on the X-ray emission from the hot gas surrounding the most massive structures (e.g. Buote et al., 2007). Gravitational lensing offers a complementary method to determine the properties of dark matter haloes (e.g. Comerford & Natarajan, 2007; Mandelbaum et al., 2008; Merten et al., 2015). Moreover, when restricting the lenses to galaxy scales, it is possible to constrain dark matter haloes for masses below \lesssim 10^{13}M⊙ (e.g. Leier et al., 2011). However, all observational methods are based on an indirect detection of the dark matter via the gravitational potential. Simulations such as EAGLE (Schaye et al., 2015) or Illustris (Vogelsberger et al., 2014) do not suffer from this disadvantage, as the dark matter distribution is directly accessible from the simulation outputs. However, even the simulations rely on the parameterisation of mass and concentration, which require the adoption of a specific density profile function, such as those proposed by Navarro et al. (1997) or Einasto (1965). The goal of this work is to assess the concentrations derived from strong lensing analysis over galaxy scales, which appear inconsistently high with respect to the predictions from numerical simulations (e.g. Leier et al., 2012). We show how the extracted c-M values (see Table 1) depend on resolution, modelling uncertainties, and assumptions of the underlying analytic functions, such as the dark matter profile and the initial mass function (IMF). We build upon a recent analysis of the c-M relation (Leier et al., 2022) in a sample of strong-lensing galaxies versus simulated galaxies from EAGLE using a Monte-Carlo type combination of pixelised lens models (Saha & Williams, 2003) and stellar population synthesis maps, as used in Ferreras et al. (2005). In Sect. 2 we describe the data, and in Sect. 3, the method used in this study is presented. Table 1: Overview of the c-\rm M_{\rm vir} power-law parameters. Sample \alpha c_{13} (1) (2) (3) lit, B07 -0.199\pm 0.026 14.42\pm 0.91 lit, L12 -0.401\pm 0.064 17.70\pm 3.89 lens,all -0.65\pm 0.10 14.56^{+5.54}_{-4.02} lens,<50 -0.41\pm 0.07 16.73^{+2.98}_{-3.62} EAGLE,all -0.13\pm 0.03 11.02^{+0.46}_{-0.49} EAGLE,{K13} -0.19\pm 0.05 8.18^{+0.57}_{-0.60} 111In col. 1, B07 refers to the results of Buote et al. (2007) and L12 to Leier et al. (2012). The samples “lens” and “EAGLE” give the best fits from Leier et al. (2022). “lens<50” denotes fits to our lens sample with a root mean square deviation better than the median value. Col. 2 is the slope of the scaling relation and col. 3 is the concentration of the best fit at virial mass 10^{13}M⊙. We provide errors from bootstrap. Our results in Sect. 4 show how fitted c-M relations change if the reconstructed lens profile is less well resolved in the inner part – in other words how the c-M relation changes as a function of the minimum radius. Furthermore, we demonstrate how a transformation that keeps the total enclosed mass at the Einstein radius constant but changes the steepness of the inner profile affects the c-M relation as well as the goodness-of-fit. In addition, we compare our findings with non-parametric concentrations derived from the inner radial region of lenses and simulated haloes alike. In Sect. 4.4, we show how dark matter distributions based on parametric functions other than the standard NFW profile (named after Navarro et al., 1997), change the c-M relation. Subsequently, in Sect. 4.5, we study how the adoption of different choices of the stellar IMF affects the c-M relation. Finally, in Sect. 5, we discuss whether or not all of these factors can reconcile the observed and simulated c-M relations."
https://arxiv.org/html/2411.08951v1,Perturbative gravitational wave predictions for the real-scalar extended Standard Model,"We perform a state-of-the-art study of the cosmological phase transitions of the real-scalar extended Standard Model. We carry out a broad scan of the parameter space of this model at next-to-next-to-leading order in powers of couplings. We use effective field theory to account for the necessary higher-order resummations, and to construct consistent real and gauge-invariant gravitational wave predictions. Our results provide a comprehensive account of the convergence of perturbative predictions for the gravitational wave signals in this model. For the majority of the parameter points in our study, we observe apparent convergence. While leading and next-to-leading order predictions of the gravitational wave amplitude typically suffer from relative errors between 10 and 10^{4}, at next-to-next-to-leading order the typical relative errors are reduced to between 0.5 and 50. Nevertheless, for those parameter points predicting the largest signals, potentially observable by future gravitational wave observatories, the validity of the perturbative expansion is in doubt.","Gravitational wave observatories offer the exciting prospect to be able to probe the very early universe, through sensitivity to the gravitational waves produced by a cosmological first-order phase transition. However, recent works have cast doubt on the reliability of current gravitational wave predictions within perturbation theory, finding uncertainties in the expected amplitude of several orders of magnitude Croon:2020cgk . The scalar singlet extension of the Standard Model has been at the centre of this debate Carena:2019une ; Gould:2021oba ; Athron:2022jyi ; Lewicki:2024xan ; Ramsey-Musolf:2024ykk . In the \mathrm{Z}_{2} symmetric version of this model, some groups have found phase transitions to be observable by planned detectors, and others not, with the differences apparently due to loop corrections. For the Standard Model (SM), it has long been clear that perturbation theory fails to describe the electroweak phase transition even qualitatively. A loop expansion of the Higgs effective potential predicts a second-order transition at one-loop (unresummed), but a first-order transition at higher loops, all in contradiction with lattice simulations, which find a crossover Kajantie:1995kf ; Kajantie:1996mn . This is despite the electroweak sector being weakly coupled at zero temperature. At high temperatures near the electroweak phase transition, the long-wavelength modes of the Higgs and W and Z bosons become strongly coupled Linde:1980ts , and it is these modes that determine the nature of the phase transition Kajantie:1995dw . Nevertheless, a perturbative expansion can be constructed for an electroweak-like phase transition. One finds that the effective expansion parameter is \sim 20\lambda/g^{2}, where \lambda is the Higgs self-coupling, g^{2} is the weak gauge coupling and the approximate value 20 follows from the coefficients of the expansion Ekstedt:2022zro ; Ekstedt:2024etx . This expansion parameter is greater than one for Standard Model couplings, indicating that the perturbative expansion does not converge. For small enough values of the Higgs self-coupling such that the expansion is convergent, the transition is strongly first order. In this case, lattice and perturbative predictions are in good agreement Gould:2022ran ; Ekstedt:2022zro ; Ekstedt:2024etx . Thus, one may ask: can perturbation theory describe the gravitational waves produced by strongly first-order phase transitions in extensions of the Standard Model? The present paper aims to address this question in the context of a scan of the parameter space of a concrete model, building on and complementing refs. Gould:2021oba ; Gould:2023jbz ; Lewicki:2024xan ; Ramsey-Musolf:2024ykk . The Lagrangian for the real singlet scalar extension of the Standard Model (xSM) is \displaystyle\mathscr{L} \displaystyle=\mathscr{L}_{\text{SM}}+\mathscr{L}_{s}+\mathscr{L}_{\text{% portal}}, (1) \displaystyle\mathscr{L}_{s}= \displaystyle-\frac{1}{2}\partial_{\mu}s\partial^{\mu}s-b_{1}s-\frac{1}{2}m_{s% }^{2}s^{2}-\frac{1}{3}b_{3}s^{3}-\frac{1}{4}b_{4}s^{4}, (2) \displaystyle\mathscr{L}_{\text{portal}} \displaystyle=-\frac{1}{2}a_{1}s\phi^{\dagger}\phi-\frac{1}{2}a_{2}s^{2}\phi^{% \dagger}\phi, (3) where \mathscr{L}_{\text{SM}} is the Standard Model Lagrangian, \phi is the Higgs field and s is the new scalar. This is the most general renormalisable Lagrangian for this field content. Our notation largely follows ref. Niemi:2021qvp . The xSM is an ideal playground for the study of cosmological phase transitions, as it can yield a wide variety of different cosmological phase histories including strong first-order phase transitions. It is also motivated as a minimal extension of the Standard Model that provides a dark matter candidate Silveira:1985rk ; GAMBIT:2017gge , and may provide for baryogenesis together with higher-dimension CP-violating operators Cline:2012hg ; Cline:2021iff ; Harigaya:2022ptp ; Ellis:2022lft . If the new scalar has a mass much below the electroweak scale, it can also be relevant in astrophysical and cosmological contexts Burrage:2018dvt ; Brax:2021rwk . We study the cosmological thermal history of this model, focusing on temperatures in the vicinity of the electroweak phase transition. In the Standard Model the pseudocritical temperature of the crossover transition is approximately 160~{}\mathrm{GeV} DOnofrio:2015gop . Our aim is to study the reliability and convergence of perturbation theory to describe possible phase transitions in this model and their gravitational wave signals. Ref. Carena:2019une studied phase transitions in the Z_{2} symmetric xSM (b_{1}=b_{3}=a_{1}=0), and demonstrated that at leading, one-loop order, the model predicts observable gravitational wave signals. However, given the importance of the one-loop terms, their effects should be resummed. At leading order, doing so corresponds to including the infinite set of daisy diagrams Arnold:1992rz . Carrying out this resummation, ref. Carena:2019une found that the predicted gravitational wave signals were reduced in amplitude by 10 orders of magnitude, rendering them unobservable by planned detectors. As thermal effects, and consequently phase transitions, arise first at one-loop order, a study of convergence requires reaching at least (resummed) two-loop order. The first two-loop study of phase transitions and gravitational wave signals in the xSM was carried out in ref. Gould:2021oba , building on refs. Niemi:2021qvp ; Schicho:2021gca . The predicted gravitational wave amplitudes were found to be observable, but at leading order suffered from up to 11 orders of magnitude uncertainty. At two-loop order, these uncertainties were significantly reduced, to between 1 and 4 orders of magnitude, and the overall amplitude of the signal was increased. However, this study was limited to just two parameter points, so it was unclear to what extent the conclusions were special to the parameter points chosen. A number of recent works have extended and consolidated this line of study, by carrying out two-loop order analyses of phase transitions in the xSM for scans of the model parameter space. Refs. Niemi:2024vzw ; Niemi:2024axp have done so for the equilibrium phase transition properties, and refs. Lewicki:2024xan ; Ramsey-Musolf:2024ykk for the gravitational wave signals. Ref. Niemi:2024axp also carried out lattice Monte-Carlo simulations, to definitively test the validity of perturbative predictions. Our work extends and complements these studies. In this paper, we analyse the convergence of perturbation theory for gravitational wave signals of phase transitions in the xSM. We carry out our analysis at a whole model level, by performing a comprehensive scan of the generic (non-Z_{2}-symmetric) parameter space at three successive perturbative orders. At each order, we use renormalisation scale dependence as a proxy for the theoretical uncertainty of the prediction, and we quantify the reduction of uncertainty at higher orders. We analyse how the distribution of predictions, as well as the distribution of uncertainties, changes between orders in a given parameter scan."
https://arxiv.org/html/2411.08943v1,The  project: tracking the expansion and merger histories of ionized bubbles during the Epoch of Reionization,"The growth of ionized hydrogen bubbles in the intergalactic medium around early luminous objects is a fundamental process during the Epoch of Reionization (EoR). Observations using Lyman-\alpha emission from high-redshift galaxies and forthcoming 21 cm maps are beginning to constrain the sizes of these ionized regions. In this study, we analyze bubble sizes and their evolution using the state-of-the-art thesan radiation-hydrodynamics simulation suite, which self-consistently models radiation transport and realistic galaxy formation throughout a large (95.5\,\text{cMpc})^{3} volume of the Universe. Analogous to the accretion and merger tree histories employed in galaxy formation simulations, we characterize the growth and merger rates of ionized bubbles by focusing on the spatially-resolved redshift of reionization. By tracing the chronological expansion of bubbles, we partition the simulation volume and construct a natural ionization history. We identify three distinct stages of ionized bubble growth: (1) initial slow expansion around the earliest ionizing sources seeding formation sites, (2) accelerated growth through percolation as bubbles begin to merge, and (3) rapid expansion dominated by the largest bubble. Notably, we find that the largest bubble emerges by z\approx 9\!-\!10, well before the midpoint of reionization. This bubble becomes dominant during the second growth stage, and defines the third stage by rapidly expanding to eventually encompass the remainder of the simulation volume and becoming one of the few bubbles actively growing. Additionally, we observe a sharp decline in the number of bubbles with radii around \sim 10 cMpc compared to smaller sizes, indicating a characteristic scale in the final segmented bubble size distribution. Overall, these chronologically sequenced spatial reconstructions offer crucial insights into the physical mechanisms driving ionized bubble growth during the EoR and provide a framework for interpreting the structure and evolution of reionization itself.","As the first luminous objects formed and emitted hydrogen ionizing radiation billions of years ago, they initiated the Epoch of Reionization (EoR), spanning redshifts from z\approx 5\!-\!20 (EoR; Shapiro & Giroux, 1987; Barkana & Loeb, 2001; Furlanetto et al., 2006; Wise, 2019). The radiation from these early sources locally and inhomogeneously ionized the surrounding neutral hydrogen gas, creating ionized regions or “bubbles” in the intergalactic medium (IGM). Over time, these ionized bubbles grew, at first in isolation but eventually different bubbles began to overlap with each other, leading to percolation and a rapid increase in bubble sizes as they merged (Furlanetto & Oh, 2016). This process ultimately resulted in the nearly fully ionized Universe observed today. The EoR presents several astrophysical and cosmological frontiers, including understanding the physical mechanisms driving ionized bubble growth. Observing galaxies, active galactic nuclei, and the ambient IGM during the EoR is challenging due to the extremely high redshifts. However, characterizing and interpreting observational signatures is of crucial importance, especially as recent and forthcoming James Webb Space Telescope (JWST) surveys and 21 cm radio telescopes offer promising avenues to probe high-redshift galaxies and the properties of ionized bubbles (Robertson, 2022). To fully understand the EoR, studying both the IGM and galaxy sources is paramount, motivating investigations into the dynamics and morphologies of ionized bubbles and their connection to the local and large-scale environments (Gnedin & Madau, 2022). The 21 cm radio interferometers, including the Low Frequency Array (LOFAR; van Haarlem et al., 2013), Hydrogen Epoch of Reionization Array (HERA; DeBoer et al., 2017; HERA Collaboration et al., 2023), Square Kilometer Array (SKA; Mellema et al., 2013), and others are beginning to map the distribution of neutral hydrogen in the Universe. The 21 cm line, arising from the forbidden spin-flip hyperfine transition of neutral hydrogen, directly probes the IGM. By detecting the redshifted 21 cm signal from the EoR, these instruments can reveal the global evolution, statistical spatial correlations, and even a tomographic picture of neutral and ionized gas. These measurements will be critical for studying ionized bubbles during the EoR as well as constraining cosmological parameters (McQuinn et al., 2006; Mesinger et al., 2011; Liu & Parsons, 2016; Park et al., 2019; Kannan et al., 2022b). In anticipation of the forthcoming 21 cm data, numerous theoretical studies have sought to characterize the properties of ionized bubbles and their connections to the dominant processes in cosmic reionization. Different reionization scenarios and source models can lead to varying bubble morphologies; thus, the distribution of bubble sizes has emerged as a crucial diagnostic for distinguishing between reionization models (McQuinn et al., 2007a, b; Majumdar et al., 2016). Several methods have been developed to detect and analyze ionized bubble sizes within hydrodynamical simulations and semi-analytic models. Prominent among these are: • Mean-Free Path (MFP) Method: Calculates effective bubble sizes by tracing rays from ionized cells to the nearest neutral cell in all directions (Mesinger & Furlanetto, 2007). • Spherical Averaging (SPA): Determines the largest sphere over which the average ionization fraction exceeds a set ionization threshold (Zahn et al., 2007). • Friends-of-Friends (FOF) Algorithm: Links ionized cells within a given distance to form bubbles (Ivezić et al., 2014). • Granulometry: Uses a “sieving” process to count objects that fit within a given structural hierarchy (Kakiichi et al., 2017). • Watershed Method: Identifies bubbles by filling “catchment basins” from local minima until neighbouring basins intersect (Lin et al., 2016). Beyond this, topological analyses have been fruitful in characterizing further distinctive signatures of reionization (Friedrich et al., 2011; Busch et al., 2020; Giri et al., 2020; Elbers & van de Weygaert, 2023). Building upon these existing studies, we perform an analysis of ionized bubble sizes within the thesan simulation suite (Garaldi et al., 2022; Kannan et al., 2022a; Smith et al., 2022; Garaldi et al., 2024). thesan combines the galaxy formation model of IllustrisTNG (Weinberger et al., 2017; Pillepich et al., 2018a, b) with detailed on-the-fly modeling of radiative processes, including radiation transport (arepo-rt; Kannan et al., 2019), non-equilibrium heating and cooling, and realistic ionizing sources, within a large cosmological volume of 95.5 cMpc per box side. The thesan simulations have been employed to make a wide range of EoR predictions (e.g. Kannan et al., 2022b; Qin et al., 2022; Borrow et al., 2023; Kannan et al., 2023; Yeh et al., 2023; Xu et al., 2023; Shen et al., 2024b, a). Specifically, we examine the ionized bubbles by analyzing the reionization redshift (z_{\text{reion}}), which we define as the last time the ionized hydrogen fraction (H ii) crosses the threshold value of 0.5 from below (e.g. Thélie et al., 2022). z_{\text{reion}} serves as an effective tracer for bubble sizes, particularly at scales below approximately 1 cMpc (Neyer et al., 2024). A distinction of our study is that we employ a chronological time-ordering of z_{\text{reion}} to construct a natural segmentation of the simulation volume. The resulting expansion and merger histories provide a unique and intuitive perspective on the evolution of ionized bubbles during the EoR. The paper is organized as follows. In Section 2, we describe our methods, including a brief overview of the thesan simulation suite and the bubble tree algorithm used to analyze z_{\text{reion}}. In Section 3, we present our main findings, including the growth of ionized bubbles (3.1) and the distribution of bubble sizes (3.2) throughout the EoR. We explore the physical effects of different model parameters in Section 4. Finally, we synthesize our conclusions in Section 5. Supplementary discussion of grid resolution is found in Appendix A and the impact of corners being counted as neighbours in Appendix B. All further mentions of “reionization” refer specifically to hydrogen reionization, and any mention of “bubbles,” “bubble groups,” or “groups” refers to ionized bubbles."
https://arxiv.org/html/2411.08931v1,Does the fluid-static equilibrium of a self-gravitating isothermal sphere of van der Waals’ gas present multiple solutions?,"We take up the investigation we had to put in the future-work stack at the end of Sec. V B 2 of Ref. Giordano et al., 2024, in which we pointed out the obvious necessity to inquire about existence or absence of values of the characteristic numbers a and b in correspondence to which the perfect-gas model’s self gravitational effects, namely upper boundedness of the gravitational number, spiraling behavior of peripheral density, oscillating behavior of central density, and existence of multiple solutions corresponding to the same value of the gravitational number, appear also for the van der Waals’ model. The development of our investigation brings to the conversion of our M2 scheme based on a second-order differential equation into an equivalent system of two first-order differential equations that incorporates Milne’s homology invariant variables. The converted scheme 1oM2 turns out to be much more efficacious than the M2 scheme in terms of numerical calculations’ easiness and richness of results. We use the perfect-gas model as benchmark to test the 1oM2 scheme; we re-derive familiar results and put them in a more general and rational perspective that paves the way to deal with the van der Waals’ gas model. We introduce variable transformations that turn out to be the key to study (almost) analytically the monotonicity of the peripheral density with respect to variations of the gravitational number. The study brings to the proof that the gravitational number is not constrained by upper boundedness, the peripheral density does not spiral, and the central density does not oscillate for any couple of values assumed by the characteristic numbers a and b; however, multiple solutions corresponding to the same value of the gravitational number can exist but their genesis is completely different from that of the perfect-gas model’s multiple solutions. We provide the boundary in the \mbox{{\tengr a}},\mbox{{\tengr b}} plane between the two regions of solution’s uniqueness and multiplicity. Finally, by the application of the 1oM2 scheme, we resolve the mystery of the missing steep curve corresponding to Aronson and Hansen’s 60km caseAronson and Hansen (1972) that we unsuccessfully chased in Ref. Giordano et al., 2024 and we even detect another solution that had somehow escaped the attention of those authors.","Our series of studies regarding self-gravitating fluids began with Ref. Giordano et al., 2019 centered on the perfect gas model (PG-m) and continued with Ref. Giordano et al., 2024 focused on the van der Waals’ gas model (vdWG-m). Here, we take up the investigation we had to put in the future-work stack at the end of Sec. V B 2 of Ref. Giordano et al., 2024. For obvious reasons of consistency, we use same notation of and definitions introduced in the mentioned references. Also, we maintain the convention to subscript cross-references to Refs. Giordano et al., 2024, 2019 with the labels [Giordano et al., 2019] and [Giordano et al., 2024], respectively; for example, Eq. (3){}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2019% ejmb}{\@@citephrase{, }}{}}}]}$}} refers to Eq. (3) in Ref. Giordano et al., 2019 and Fig. 3{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}} refers to Fig. 3 in Ref. Giordano et al., 2024. We consider only the situation in which the fluid-sphere temperature is above the critical temperature [Eq. (51){}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}} top]; therefore, phase equilibria are excluded from consideration. In Sec. V B{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}}, we described and discussed results relative to a test case characterized by the couple of values \mbox{{\tengr a}}=0.1053,\,\mbox{{\tengr b}}=0.0595 of the characteristic numbers [Eqs. (31b){}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}} and (31c){}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}}] appearing in the nondimensional vdWG-m’s equation of state [Eq. (32){}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}}]; in the sequel, we will refer to this case with the label TC{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}}. We provided undeniable evidence in Figs. 10{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}} and 11{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}} that the physically questionable self-gravitational effects accompanying the PG-m, namely, upper boundedness (N\leq 2.5176) of the gravitational number defined in Eq. (31a){}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}}, spiraling behavior of peripheral density, and oscillating behavior of the central density, do not trouble the vdWG-m. That outcome led us to conjecture111Admittedly, we did not prove it. also the absence of the last uncomfortable effect featured by the PG-m: the existence of multiple equilibria in correspondence to a specified gravitational number, mainly evidenced by the spiraling curve in Fig. 10{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}} and explicitly exemplified in Figs. 7{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2019% ejmb}{\@@citephrase{, }}{}}}]}$}} and 8{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2019% ejmb}{\@@citephrase{, }}{}}}]}$}} for N=2.4. Hereinafter, for brevity, we convene to group the mentioned self-gravitational effects related to the PG-m under the label PG-sge. We concluded Sec. V B 2{}_{\scalebox{0.65}{$\mbox{[{\cite[citep]{\@@bibref{AuthorsPhrase1Year}{dg2024% pof}{\@@citephrase{, }}{}}}]}$}} with the admission that the results presented therein proved the disappearance of the PG-sge only for the assumed values of a and b and with the recognition that extrapolation of such an occurrence to arbitrary values may be hazardous. We also pointed out the obvious necessity of an investigation regarding existence or absence of values of a and b in correspondence to which the PG-sge appear also for the vdWG-m. The contents of this communication describe our efforts to go to the bottom of this matter."
https://arxiv.org/html/2411.08097v1,"Atomic Dark Matter, Interacting Dark Radiation, and the Hubble Tension","We present a new class of interacting dark sector models that can address the Hubble tension. Interacting dark radiation (DR) has previously been put forward as a solution to the problem, but this proposal is disfavored by the high-\ell cosmic microwave background (CMB) data. We modify this basic framework by introducing a subcomponent of dark matter (DM) that interacts strongly with the DR, so that together they constitute a tightly coupled fluid at early times. We show that if this subcomponent decouples from the interacting DR during the CMB epoch, the \ell modes of the CMB that entered the horizon before decoupling are impacted differently from those that entered after, allowing a solution to the problem. We present a model that realizes this framework, which we dub “New Atomic Dark Matter”, or nuADaM, in which the interacting dark matter (iDM) subcomponent is composed of dark atoms, and dark “neutrinos” with long-range interactions contribute to the DR, hence the name of the model. This iDM subcomponent is acoustic at early times but decouples from the DR following dark recombination. In contrast to conventional atomic dark matter (ADM) models, the dark photon is part of a richer DR sector, which ensures that it continues to be self-interacting even after recombination. We show that this model admits a fit to the available cosmological data that is significantly better than both \Lambda\mathrm{CDM} and conventional ADM.","There exists a large discrepancy between the value of H_{0}, the current expansion rate of the universe, inferred from the best fit of the parameters in the \Lambda\mathrm{CDM} model to cosmological data, and the value obtained from more direct measurements based on the distance ladder technique. This may be the first serious indication that we need to go beyond the standard \LambdaCDM cosmological model. When comparing two of the most precise measurements, the \Lambda\mathrm{CDM} fit to Planck cosmic microwave background (CMB) data Planck:2018vyg which gives H_{0}=67.36\pm 0.54 km/s/Mpc, and the supernovae measurements made by the SH0ES collaboration calibrated to Cepheid variable stars Riess:2021jrx which yield H_{0}=73.04\pm 1.04 km/s/Mpc, one observes a >5\sigma tension between them. While it is possible that this H_{0} tension could be due to some unknown systematic effects Kamionkowski:2022pkx ; Freedman:2021ahq , the distance ladder measurements have withstood significant scrutiny and the results have remained stable across different variations in the data and analysis methods employed.111 It is worth noting, however, that the Chicago-Carnegie Hubble Program (CCHP) collaboration reports a direct measurement of H_{0} that is not in significant tension with the \Lambda\mathrm{CDM} fit to Planck data. The most recent CCHP results can be found in Ref. Freedman:2024eph . A wide range of new models beyond \Lambda\mathrm{CDM} have been proposed to address this tension (see, e.g., Refs. Buen-Abad:2015ova ; Lesgourgues:2015wza ; Buen-Abad:2017gxg ; Zhao:2017cud ; DiValentino:2017gzb ; Poulin:2018cxd ; Smith:2019ihp ; Lin:2019qug ; Alexander:2019rsc ; Agrawal:2019lmo ; Escudero:2019gvw ; Berghaus:2019cls ; Ye:2020btb ; RoyChoudhury:2020dmd ; Brinckmann:2020bcn ; Krishnan:2020vaf ; Ye:2021iwa ; Niedermann:2021vgd ; Aloni:2021eaq ; Dainotti:2021pqg ; Odintsov:2022eqm ; Berghaus:2022cwf ; Schoneberg:2022grr ; Colgain:2022rxy ; Joseph:2022jsf ; Brinckmann:2022ajr ; Buen-Abad:2022kgf ; Wang:2022bmk ; Bansal:2022qbi ; Buen-Abad:2023uva ; Sandner:2023ptm ; Zu:2023rmc ; Niedermann:2023ssr ; Greene:2024qis ; Allali:2024anb ; Co:2024oek ; Cho:2024lhp ; Simon:2024jmu for a partial list of these proposals, and also the models reviewed in DiValentino:2021izs ; Schoneberg:2021qvd ; Abdalla:2022yfr ; Poulin:2023lkg ; Khalife:2023qbu ). Late-time cosmological data, such as measurements of the baryon acoustic oscillations (BAO) Beutler:2011hx ; Ross:2014qpa ; eBOSS:2020yzd ; eBOSS:2020qek ; eBOSS:2020fvk ; eBOSS:2020gbb ; eBOSS:2020uxp ; eBOSS:2020tmo and the apparent magnitudes of Type-Ia supernovae Scolnic:2021amr ; Brout:2022vxf , are well described by an expansion history governed by \Lambda\mathrm{CDM}. This motivates introducing modifications to the \Lambda\mathrm{CDM} paradigm only at early times, around recombination and matter-radiation equality, in such a way that they change the sound horizon scale Aylor:2018drw . In such an approach, even though new physics only directly affects redshifts z\gtrsim 10^{3}, this impacts the extraction of H_{0} because when the sound horizon scale is changed, one needs to adjust the cosmological parameters to keep the ratio of the sound horizon scale to the angular distance to recombination fixed, thereby changing H_{0}. In order to address the tension, one needs to decrease the sound horizon at recombination, which implies the need to increase the energy density (and therefore H) near recombination. There have been a wide range of proposals that introduce new dark sector states to provide the extra energy density needed around the time of recombination. The simplest scenarios involve new massless degrees of freedom constituting a dark radiation (DR) component (whose abundance is parametrized by \Delta N_{\mathrm{eff}}), which may be either free-streaming, similar to neutrinos, or having sizable self-interactions such that the new particles can be described as a perfect fluid. The biggest challenge for this class of solutions is that the change in the Hubble scale affects the sound horizon scale and the diffusion damping scales differently, which leads to an increase in Silk damping at small scales Blinov:2020hmc ; Hou:2011ec . This can be partially compensated for by an increase in the spectral tilt (n_{s}), but since n_{s} affects all scales whereas Silk damping primarily affects smaller scales, adjusting this parameter only provides a limited improvement in the overall fit. A natural extension of this framework incorporates a step-like increase in \Delta N_{\mathrm{eff}} occurring during the CMB epoch (“stepped DR”) Aloni:2021eaq . It has been shown that such a proposal leads to a significant improvement over the minimal DR scenario Aloni:2021eaq ; Schoneberg:2022grr . In this scenario, modes that enter the horizon prior to the step are impacted differently from those that enter afterward. Such a step can naturally arise from the freeze-out of a massive particle in the DR bath. Despite the reduction in the Hubble tension, these stepped DR models generally lead to an enhancement in the power spectrum at small scales (primarily due to an increase in n_{s} compared to \Lambda\mathrm{CDM}). This has the effect of exacerbating tensions with small scale measurements from weak lensing and Lyman-\alpha forest data Goldstein:2023gnw ; Rogers:2023upm . Further extending this basic scenario to include interactions between dark matter (DM) and DR Joseph:2022jsf ; Buen-Abad:2022kgf can offer a resolution to this problem Allali:2023zbi ; Buen-Abad:2023uva ; Schoneberg:2023rnx ; Bagherian:2024obh . In this work, we introduce a new class of interacting dark sector models designed to address the Hubble tension. Our approach extends the standard self-interacting DR (SIDR) scenario by incorporating a subcomponent of DM that interacts strongly with DR. This interacting dark matter (iDM) subcomponent and the DR form a tightly coupled fluid in the early universe. The rest of the DM is composed of standard cold dark matter (CDM). The crucial observation is that, if this subcomponent decouples from the DR during the CMB epoch, the modes entering the horizon before and after this decoupling takes place are affected differently, offering a potential resolution to the problem. We present a specific realization of this framework based on atomic dark matter (ADM) Kaplan:2009de ; Kaplan:2011yj ; Cyr-Racine:2013fsa ; Cyr-Racine:2012tfp ; Bansal:2021dfh ; Bansal:2022qbi that we refer to as “New Atomic Dark Matter”, or nuADaM. In this model, the DM subcomponent interacting with DR consists of dark atoms. Initially, these dark atoms are ionized, making this iDM subcomponent acoustic. Eventually, dark recombination takes place during the CMB epoch, and the now neutral dark atoms decouple from the DR. In contrast to standard ADM models, the dark photon in nuADaM is part of a broader DR sector that continues to exhibit fluid-like behavior even after the dark atoms have decoupled. We assume that the DR is populated only after Big Bang nucleosynthesis (BBN) concludes, which could naturally occur in scenarios where the dark sector equilibrates with Standard Model (SM) neutrinos at some point after BBN Aloni:2021eaq . Finally, we perform a cosmological data analysis and demonstrate that this model provides a notable improvement over both \Lambda\mathrm{CDM} and standard ADM models. We can obtain a qualitative understanding of the role played by the different ingredients in the model. The presence of an interacting DR component alters the sound horizon scale, allowing larger H_{0} values to be realized Baumann:2015rya ; Brust:2017nmv ; Blinov:2020hmc . In addition, the interactions between the DR and the iDM subcomponent have two important effects. Firstly, the pressure the DR imparts on the iDM decreases the power spectrum at small scales which, as we discuss in more detail below, may improve the fit to direct measurements of the matter power spectrum (MPS). Secondly, the decoupling of the DR from the iDM results in a difference between the evolution of CMB modes that entered the horizon prior to decoupling and those that entered later. This is qualitatively similar to the effect of stepped DR Aloni:2021eaq ; Buen-Abad:2022kgf , and, in combination with shifts in other cosmological parameters, allows for a significantly larger energy density in DR without degrading the fit to CMB data. Both these effects only affect smaller scales, since after dark recombination iDM behaves exactly like CDM, and therefore modes that enter the horizon after dark recombination evolve as in \Lambda\mathrm{CDM}. As we shall see, these ingredients allow the model to realize a much larger H_{0} value, while maintaining a good fit to CMB data and without worsening the fit (or even improving it) to small scale probes of the MPS. This paper is organized as follows. In Sec. 2, we introduce the model, and discuss the relevant interactions between its different parts, as well as their associated parameters. In Sec. 3, we study the cosmological evolution of the new components of the model and determine the parameter space in which the model reproduces the dynamics that is most relevant for addressing the cosmological tensions. In Sec. 4, we discuss the MCMC fits of our model to different combinations of data and compare results with updated fits to conventional ADM. We conclude in Sec. 5, and include our full numerical results in App. A."
https://arxiv.org/html/2411.08873v1,"A consistency relation for induced 
gravitational wave anisotropies","We show that the anisotropies in the spectrum of gravitational waves induced by scalar modes after the end of inflation in canonical, single-field models are completely determined by the tilt of the scalar and tensor power spectra. The latter contains information about anisotropies produced due to the propagation of the tensor modes in an inhomogeneous Universe, whereas the former represents the anisotropies generated at the time of production and arise only when non-Gaussian corrections to the angular power spectrum are considered. Our proof takes into account all scalar interactions in the cubic inflaton Lagrangian.","I INTRODUCTION Upon expanding Einstein’s equations to second order in perturbations, one finds that gravitational waves are sourced by terms quadratic in scalar modes [1, 2, 3, 4, 5]. The physics of these scalar-induced gravitational waves has been thoroughly explored in recent years, see e.g. [6] for a pedagogical review. This mechanism of gravitational wave production becomes particularly relevant in models which feature an enhancement in the scalar power spectrum. For instance, in the context of single-field inflation, an inflection point in the potential can lead to a phase of ultra-slow-roll in which the field slows down and the power spectrum develops a sharp feature, which then translates into a peak in the frequency spectrum of gravitational waves. This class of models is motivated by the fact that an enhanced scalar spectrum leads to large density fluctuations which later collapse into primordial black holes, compact objects that could account for the entirety of dark matter. Other cosmological sources (such as phase transitions or topological defects) can also produce a peak in the spectrum, however, and a key question that remains open is whether one can use other observables to distinguish between different signals. One possibility is to use anisotropies in the gravitational wave background. Astrophysical sources typically produce anisotropies that are much larger in amplitude than those due to cosmological sources, making them a key observable in determining the origin of the signal. Significant efforts have been devoted recently to search for these anisotropies in Pulsar Timing Arrays [7, 8], but measuring them is a difficult task 111For instance, even an isotropic background could produce large anisotropies due to interference between different sources [30].. Scalar-induced gravitational waves represent one of the main candidates for new physics in this context [10, 11] and thus understanding their anisotropies is of paramount importance for these analyses. Since gravitational waves are generated locally (once the large scalar fluctuations that source them re-enter the horizon after inflation), one would expect distant patches in the sky to be essentially uncorrelated, and therefore for the corresponding anisotropies to be volume-suppressed. Although generally speaking this argument is correct, non-Gaussianities provide a way out of this issue. For instance, anisotropies induced by non-Gaussianities of the local kind (that is, when the curvature perturbation \mathcal{R} can be written as \mathcal{R}=\mathcal{R}_{\rm G}+f_{\rm NL}\mathcal{R}_{\rm G}^{2} for some Gaussian variable \mathcal{R}_{\rm G}) were considered in [12, 13, 14, 15, 16, 17], where it was shown that, since the product \mathcal{R}_{\rm G}^{2} becomes a convolution in Fourier space, it is possible for two short-wavelength modes to conspire and create a long-wavelength one, effectively correlating distant patches 222The non-Gaussian correction to the gravitational wave frequency spectrum have also been considered in [31, 32, 33, 34, 35, 36, 37, 38, 39].. As shown in [19], the argument also holds when other types of non-Gaussianity (e.g. those that arise due to the self-interactions of the inflaton) are considered. The goal of this work is to show that, in canonical, single-field models of inflation, anisotropies are completely determined by the tilt of the scalar and tensor spectrum on small scales. Concretely, let us anticipate that we find the coefficients C_{\ell} for the angular power spectrum of the gravitational wave density contrast \delta_{\rm GW} are given by [13, 17] C_{\ell}(q)=\frac{2\pi\mathcal{P}^{\rm L}_{\mathcal{R}}}{\ell(\ell+1)}\bigg{\{% }\frac{\Omega_{\rm NG}(q)}{\Omega_{\rm GW}(q)}+\frac{3}{5}\bigg{[}4-\frac{% \partial\log\Omega_{\rm GW}(q)}{\partial\log q}\bigg{]}\bigg{\}}^{2}. (1) where \Omega_{\rm GW} is the induced gravitational wave energy density, \mathcal{P}^{\rm L}_{\mathcal{R}}\sim 10^{-9} is the long-wavelength scalar power spectrum on CMB scales, and the term \displaystyle\Omega_{\rm NG}(q)=-\frac{1}{12}\int\frac{d^{3}p}{(2\pi)^{3}}% \frac{1}{2\pi^{2}q}\Big{[}{\bm{p}}\cdot{\bm{e}}^{s}({\bm{q}})\cdot{\bm{p}}\Big% {]}^{2}\frac{(2\pi^{2})^{2}}{p^{3}|{\bm{q}}-{\bm{p}}|^{3}} \displaystyle\;\frac{q^{2}}{a^{2}H^{2}}\overline{I_{q}(p,|{\bm{q}}-{\bm{p}}|)^% {2}}\mathcal{P}_{\mathcal{R}}(|{\bm{q}}-{\bm{p}}|)\mathcal{P}_{\mathcal{R}}(p)% \frac{d\log\mathcal{P}_{\mathcal{R}}(p)}{d\log p} (2) is identical to the usual expression for the scalar-induced \Omega_{\rm GW} (see the following section for our notation and conventions), but where the integrand is modulated by the tilt of the scalar spectrum on small scales. The above equation is the main result of this paper and shows that, in single-field models of inflation, the non-Gaussian corrections to the angular power spectrum only appear in one specific combination, namely, the scalar bispectrum in the squeezed limit, which can then be related to the tilt of the scalar spectrum by using Maldacena’s consistency relation [20, 21]. Previous works studying non-Gaussianties of the local kind can therefore be thought of as particular cases of this equation (if we restrict our attention to single-field models, for which the consistency relation in [20, 21] holds). This relation was first discussed in [19], where non-Gaussianities induced by the inflaton self-interactions were considered. Here we show that this is not an accidental result, and in fact is a general relation which we obtain by considering the full single-field cubic Lagrangian in our proof."
https://arxiv.org/html/2411.08854v1,Stochastic inflation and non-perturbative power spectrum beyond slow roll,"Stochastic inflation, together with the \Delta N formalism, provides a powerful tool for estimating the large-scale behaviour of primordial fluctuations. We construct a numerical code to capture the non-perturbative statistics of such fluctuations and test our code to obtain the exponential non-Gaussian tail of the curvature perturbations. We provide a numerical algorithm to compute the non-perturbative curvature power spectrum and apply it to slow-roll (SR) and ultra-slow-roll (USR) single-field models of inflation. For the USR case, we successfully reproduce the peak in the power spectrum, which agrees to a certain accuracy with the perturbative power spectrum. We highlight some important differences between non-perturbative and perturbative approaches that may suggest the inconsistency of the \Delta N formalism at the transition stages between attractor and non-attractor regimes.","Despite uncountable criticism and lack of smoking gun proof, inflation stands as arguably the most established theory explaining the creation of primordial perturbations that gave rise to inhomogeneous structures in the present universe, even decades after its discovery [1, 2, 3, 2, 4]. A number of predictions have been successfully verified by either ground-based or space-based missions viz. the flat geometry of the spacetime at very large scales [5, 6], statistical homogeneity, isotropy and scale invariant adiabatic primordial density perturbations from observing the spatial temperature anisotropies of the Cosmic Microwave Background (CMB) [7, 8, 9], with forthcoming measurements in line to detect and measure the primordial gravitational waves from B-modes of the CMB [10, 11, 12, 13, 14]. Several inflationary models with a slow-roll (SR) attractor solution have been devised that give rise to scale-invariant Gaussian perturbations [15, 16, 17, 18]. On the other hand, a non-attractor regime in the inflaton potential can be featured by introducing a flat region in the potential, leading to an Ultra-Slow roll (USR) regime, which enhances the perturbations by many orders of magnitude at the scales yet unprobed by the present measurements. An important consequence of this is that such large fluctuations could undergo gravitational collapse after re-entering the horizon post-inflation resulting in Primordial Black Holes (PBHs) [19, 20, 21, 22, 23, 24, 25, 26, 27]. Since the detection of gravitational waves from binary black hole mergers by the LIGO-VIRGO collaboration [28, 29], many eyes have turned towards PBHs as potential candidates. The possibility that PBHs could contribute to a notable fraction of present dark matter density has still not been ruled out, given that there are still some unconstrained mass windows [30, 31, 32, 33, 34]. Moreover, such large scalar perturbations couple to tensor perturbations at second order in perturbation theory, resulting in the generation of Scalar Induced Gravitational Waves that might serve as a viable explanation for the recent detection of stochastic gravitational wave background by Pulsar Timing Array [35, 36, 37] from several collaborations of NANOGrav worldwide [38, 39, 40]. It is known that the large amplitude fluctuations resulting from USR models may not follow strict Gaussian statistics. The assumption of Gaussianity could have serious implications in computing the abundance of PBHs, which form from rare large fluctuations and hence, are susceptible to the tail of the probability distribution of the curvature perturbations [41, 42, 43, 44, 45]. Stochastic inflation was introduced to incorporate the non-linear effects resulting from the influence of the quantized small-scale modes of the field fluctuations on the large-scale dynamics of the expanding background, also called quantum diffusion [46, 47, 48, 49, 50]. The stochastic approach treats the overall volume in Fourier space as large-scale modes that have classicalized substantially and small-scale modes quantum modes that influence the large-scale modes every time they cross the threshold. Another complementary tool in the non-perturbative bucket is the separate universe approach, popularly known as the \Delta N formalism [51, 52, 53, 54], that tracks the evolution of the independent super-Hubble locally FLRW patches and relates their differences in expansion rate to the equivalent large scale curvature perturbation, leading us to the perturbations without solving the linearized perturbed Einstein equations [55, 56]. When the \Delta N formalism is applied after infusing the quantum diffusion effects onto the classical background evolution, the method is called stochastic \Delta N formalism. Lately, numerous works have studied non-perturbative statistics via stochastic inflation for both SR and USR models of inflation. It was shown in [57, 58, 45, 59, 60, 61] that the tail of the probability distribution of the curvature perturbation exponential non-Gaussianity, thus predicting a much higher PBH abundance when compared with the Gaussian counterpart. Moreover, [62, 63, 64] applied importance sampling technique to significantly speed up the computation of the tail of the distribution by cherry-picking more rare realizations. On the other hand, a non-perturbative framework of computing the curvature power spectrum has also been coined [65, 66, 67] and also for higher correlators [68]. It has been exhibited that quantum diffusion effects on the power spectrum of USR models lead to significant deviations from the linearized treatment in the beyond SR models [69, 70, 71, 72, 73]. At this stage, it is important to highlight two specific works due to their proximity to our results. First, Ref. [69] analyzes quantum diffusion effects during a non-attractor phase beyond slow-roll for several realistic inflationary models, and shows the impact of stochastic noise from sub-Hubble modes, computed using an approximation, on the curvature power spectrum at the end of inflation. For this purpose, they compute the Mukhanov-Sasaki equation numerically over a quantum-corrected background. Moreover, they set up a statistical distribution of the field velocity by running many realizations of stochastic inflation using the Kramers-Moyal equation, a generalized form of the Fokker-Planck equation, again using the simplistic approximation of the noise variance. On the other hand, Ref. [70] computes the correlators of the field perturbations via the time derivative of the solution to the Fokker-Planck equation and numerically solves for the system of equations. The power spectrum is obtained as a time derivative of the two-point correlators without simulating many stochastic realizations. Our work is different from both of them in regards that we treat our model via the Langevin equations of linear order, and numerically simulate the inflationary regime taking into account the stochastic background evolution. We repeat this for several realizations to catch the first passage time of the field through its value at the end of inflation in each of them. Finally, we make use of the \Delta N formalism to first study the probability distribution of the curvature perturbations and then the non-perturbative power spectrum using the algorithm explained in Section 5. To the best of our knowledge, such a numerical implementation of the non-perturbative scalar power spectrum, accounting for the numerical evolution of the stochastic noise, has not been done in the context of USR models. This paper is drafted as follows: In Section 2 we summarize the picture of a homogeneous FLRW universe with the first-order perturbations over it. In Section 3 we give a formal intuition of stochastic inflation, \Delta N formalism and collect the important equations. Section 4 is reserved for testing our numerical code to obtain the probability distribution of perturbations and check it against the known results. Our main results are presented in Section 5, where we elucidate the numerical approach to non-perturbatively getting the power spectrum, accompanied by its application on attractor and non-attractor models. We end this discussion with concluding remarks in Section 6. In Appendix A, we recall the computation of the perturbed Klein-Gordon equation to point out an inaccuracy in the equation of motion for sub-Hubble perturbation evolution in Ref. [57, 58]. For the more curious readers, we provide the visuals of the stochastic noise and the accuracy check of our code in Appendix B and Appendix C respectively."
https://arxiv.org/html/2411.08774v1,JWST observations constrain the time evolution of fine structure constants and dark energy-electromagnetic coupling,"It was hypothesized in the literature that some physical parameters may be time-evolving and the astrophysical data can serve as a probe. Recently, James Webb Space Telescope (JWST) have released its early observations. In this work, we select the JWST spectroscopic observations of the high redshift (z>7.1) galaxies with strong [O III] (\lambda=4959Å and 5007Å in the rest frame) emission lines to constraint the evolution of the fine structure constant (\alpha). With the spectra from two galaxies at redshifts of 7.19 and 8.47, the deviation of \alpha to its fiducial value is found to be as small as 0.44^{+8.4+1.7}_{-8.3-1.7}\times 10^{-4} and -10.0^{+18+1.5}_{-18-1.5}\times 10^{-4}, respectively (the first error is statistical and the latter is systematic). The combination of our results with the previous data reveals that \frac{1}{\alpha}\frac{d\alpha}{dt}=0.30^{+4.5}_{-4.5}\times 10^{-17}~{}{\rm yr% ^{-1}}. Clearly, there is no evidence for a cosmic evolution of \alpha. The prospect of further constraining the time evolution of \alpha is also discussed. The scalar field of dark energy is hypothesized to drive the acceleration of the universe’s expansion through an interaction with the electromagnetic field. By integrating the observational data of the fine-structure constant variation, \frac{\Delta\alpha}{\alpha}(z), we have established a stringent upper limit on the coupling strength between dark energy and electromagnetism. Our analysis yields \zeta\leq 3.92\times 10^{-7} at the 95% confidence level, representing the most stringent bound to date.","Fundamental physical constants are one facet of nature’s laws, but are they really non-revolving in the universe? Dirac (1937) proposed the famous large-number coincidence, suggesting an association between fundamental constants and the current status of the universe. About one decade after that, Teller (1948) argued that the time variation of gravitational constant (G) seems impossible due to the ecosystems on our Earth. Nevertheless, further probe is still necessary to check whether there is the cosmic evolution of the physical constants or not. One good target is the fine-structure constant which can be expressed as \alpha=\frac{e^{2}}{4\pi\varepsilon_{0}\hbar c}, where e is the electron charge, \varepsilon_{0} is the vacuum permittivity, \hbar is the reduced Planck constant, and c is the speed of light in the vacuum. The numerical value of this dimensionless constant is found to be \alpha^{-1}=137.035999206(11) (Morel et al. 2020). A reliable identification of a deviation of \alpha from such a standard value would suggest the presence of the new physics. In the middle of the 20^{th} century, Stanyukovich (1963) and Gamow (1967) introduced the idea of time-variation of \alpha in cosmology. Dyson (1967) deduced that the time variation of e is less than 1/1600 during the history of Earth from the terrestrial existence of the nuclei Re^{187} and Os^{187}. But Gamow (1967) did give suggestions on the detection of the time-varying \alpha through astronomical sources. Savedoff (1956) firstly analyzed the spectral fine-structure of the emission lines of [N II] and [Ne III] in the spectrum of the nearby Seyfert galaxy. Astronomers have since been clear that the upper bound of the relative variation \Delta\alpha/\alpha and the time variation \frac{1}{\alpha}\frac{d\alpha}{dt} in our local universe must be extremely small (Petrov et al. 2006; Rosenband et al. 2008; Wilczynska et al. 2015; Martins & Pinho 2017). Consequently, modern observations focus on high-redshift celestial bodies to explore the potential change of \alpha in the early universe (Alves et al. 2018; Wilczynska et al. 2020). Thanks to the successful launch and outstanding performance of the James Webb Space Telescope (JWST), astronomers are now able to catch a glimpse of the young generation of galaxies and stars at very high redshifts. Recently, Jiang et al. 2024a, b111After the initial submission of our manuscript for publication on May 4, 2024, Jiang et al. 2024b appeared in arXiv. In our analysis, just the high resolution spectral data have been taken into account, while Jiang et al. 2024b also analyzed the medium resolution data. constraint on high redshift evolution of fine-structure constant with galaxy spectrum, improved the high redshift research of electromagnetism force. With the JWST data, the possible cosmic evolution of \alpha and dark energy-electromagnetism coupling can be further explored, which is the main purpose of this work. In this paper we probe the cosmic variation of \alpha using the [O III] \lambda\lambda4959,5007 doublet emission-lines (hereafter [O III]) of the very high redshift (z>7) galaxies. These two lines do not suffer from serious absorption and are strong enough to be reliably measured in the infrared spectrum of the high redshift objects. We concentrate on two sources at the redshifts of z=8.47 and 7.19 from the JWST Advanced Deep Extragalactic Survey (JADES; Eisenstein et al. 2023; Bunker et al. 2023). The structure of this work is as follows. In Section 2, we introduce our sample, discuss the advantages of the [O III] doublet to probe the possible variations of \alpha, and outline our method for fitting the spectroscopic data. Section 3 covers the calculation of the variation of \alpha and the presentation of our key findings. We will wrap up and delve into the implications of our results in Section 4. In Section 4, we constraint the time evolution of the fine-structure constant and Dark Energy-electromagnetism coupling. Additionally, we address potential contamination issues associated with the [O III] method utilized in this study. Throughout this work, we adopt a \Lambda-CDM Model with H_{0}=67.4{\rm\,km\,s^{-1}\,Mpc^{-1}}, \Omega_{m}=0.3, and \Omega_{\Lambda}=0.7 (Planck Collaboration et al. 2020)."
https://arxiv.org/html/2411.08723v1,Asymmetry in the distribution of HSC galaxy spin directions: comment on arXiv: 2410.18884v1,"In the past decade, an asymmetry in the large-scale distribution of galaxy spin directions has been observed in data from all relevant digital sky surveys, all showing a higher number of galaxies rotating in the opposite direction relative to the Milky Way as observed from Earth. Additionally, JWST deep fields have shown that the asymmetry is clear and obvious, and can be sensed even by the naked human eye. These experiments were performed using two separate statistical methods: standard binomial distribution and simple \chi^{2} statistics. Stiskalek & Desmond (2024) suggested that the asymmetry in the distribution of galaxy spin directions is due to the use of binomial or \chi^{2} statistics. Instead, they developed a new complex ad-hoc statistical method that shows random distribution in galaxy spin directions, and specifically in data from HSC. Source code for the method was also made available. The primary downside of the new method is that it is not able to identify asymmetry in the distribution of galaxy spin directions. Even when the new method is provided with synthetic data with extreme and obvious asymmetry, it still reports a null-hypothesis Universe with random distribution. That shows empirically that the method cannot sense asymmetry in the distribution of the directions of rotation of galaxies. While this further concludes that the distribution of galaxy spin direction as observed from Earth is not symmetric, it is not necessarily an indication of an anomaly in the large-scale structure of the Universe. The excessive number of galaxies that rotate in the opposite direction relative to the Milky Way can also be driven by the internal structure of galaxies and the physics of galaxy rotation. The phenomenon can be related directly to other puzzling anomalies such the Ho tension. Data are publicly available, and code is not needed to reproduce the results since only conventional statistics is used.","The large-scale asymmetry in the distribution of galaxy spin directions has been suggested as early as the 1980’s (MacGillivray and Dodd,, 1985). Using the unprecedented data collection power of digital sky surveys, that asymmetry has been observed by all relevant sky surveys such as SDSS (Longo,, 2011; Shamir,, 2012; Shamir, 2020c, ; Mcadam et al.,, 2023), PanSTARRS (Shamir, 2020c, ), DES (Shamir, 2022b, ), HST (Shamir, 2020b, ), HCS (Shamir, 2024a, ), DECam (Shamir,, 2021), and DESI Legacy Survey (Shamir, 2022a, ). The largest experiment included \sim 1.3\cdot 10^{6} galaxies from the DESI Legacy Survey, allowing to perform binomial distribution analysis of the entire sky (Shamir, 2022a, ). The asymmetry exhibits itself in the form of a dipole axis that peaks at close proximity to the Galactic pole (Shamir, 2022a, ; McAdam and Shamir,, 2023; Shamir,, 2023) and becomes stronger as the redshift gets higher (Shamir, 2020c, ; Shamir, 2022c, ), leading to the possibility that a higher number of galaxies that rotate in the opposite direction relative to the Milky Way is observed from Earth (Shamir, 2022a, ; McAdam and Shamir,, 2023; Shamir,, 2023; Shamir, 2024a, ). Indeed, JWST has shown clearly that the number of galaxies that rotate in the opposite direction relative to the MW is far higher than the number of galaxies that rotate in the opposite direction relative to the MW (Shamir, 2024b, ). Recently, Stiskalek and Desmond, (2024) proposed a new statistical method that shows that the distribution of the galaxy spin directions is random, and use annotated HSC galaxies to make that claim Stiskalek and Desmond, (2024). Instead of using standard binomial statistics or simple \chi^{2} statistics, the new method is a complex ad-hoc method. This paper examines the new method empirically to test its ability to identify asymmetry in datasets of galaxies annotated by their direction of rotation."
https://arxiv.org/html/2411.08639v1,Revisiting holographic dark energy after DESI 2024,"New insights from the Dark Energy Spectroscopic Instrument (DESI) 2024 baryon acoustic oscillations (BAO) data, in conjunction with cosmic microwave background (CMB) and Type Ia supernova (SN) data, suggest that dark energy may not be a cosmological constant. In this work, we investigate the cosmological implications of holographic dark energy (HDE) and interacting holographic dark energy (IHDE) models, utilizing CMB, DESI BAO, and SN data. By considering the combined DESI BAO and SN data, we determine that in the IHDE model, the parameter c>1 and the dark-energy equation of state w does not cross -1 at the 1\sigma confidence level, whereas in the HDE model, it marginally falls below this threshold. Upon incorporating CMB data, we observe that in the HDE model, the parameter c<1 and w crosses -1 at a level beyond 10\sigma. Conversely, for the IHDE model, the likelihood of w crossing -1 is considerably diminished, implying that the introduction of interaction within the HDE model could potentially resolve or mitigate the cosmic big rip conundrum. Furthermore, our analysis reveals that the HDE and IHDE models are statistically as viable as the \LambdaCDM model when assessing Bayesian evidence with DESI BAO data combined with SN data. However, when CMB data are added, the HDE and IHDE models are significantly less favored compared to the \LambdaCDM model. Our findings advocate for further exploration of the HDE and IHDE models using forthcoming, more precise late-universe observations.","Since the initial discovery of the universe’s accelerating expansion Riess et al. (1998); Perlmutter et al. (1999), the nature of dark energy has persisted as an enduring enigma in contemporary cosmology and fundamental physics. Over the past two decades, a plethora of theoretical and phenomenological models have been crafted in response. Notably, the standard \Lambda cold dark matter (\LambdaCDM) cosmological model, with six basic parameters, has garnered significant acclaim for its exceptional fit to the majority of cosmological observations. However, as the precision of cosmological parameter measurements has increased, some perplexing discrepancies have emerged. Particularly, the Hubble constant H_{0} inferred from the \LambdaCDM model’s best fit to the Planck cosmic microwave background (CMB) data Aghanim et al. (2020a) is in significant tension, exceeding 5\sigma, with the direct measurements by the SH0ES team Riess et al. (2022) utilizing the distance ladder method. Lately, the Hubble tension has been a hot topic in current cosmology (see, e.g., Refs. Li et al. (2013); Zhang et al. (2015); Feng et al. (2017); Zhao et al. (2017); Verde et al. (2019); Riess (2019); Guo et al. (2019, 2020); Gao et al. (2021); Di Valentino et al. (2021); Cai et al. (2021); Kamionkowski and Riess (2023); Gao et al. (2024); Lynch et al. (2024); for relevant forecast analyses, see also, e.g., Refs. Zhao et al. (2011); Cai et al. (2018); Du et al. (2019); Zhang (2019); Zhang et al. (2019); Chen (2020); Chen et al. (2021); Bian et al. (2021); Jin et al. (2021); Wang et al. (2022a, b); Qi et al. (2022); Zhao et al. (2022); Jin et al. (2023a, b); Zhang et al. (2023); Song et al. (2024); Dong et al. (2024); Jin et al. (2024a, b); Xiao et al. (2024); Zhang et al. (2024); Pan et al. (2024); Fu and Zhang (2024)). On another front, the cosmological constant \Lambda in the \LambdaCDM model, which is equivalent to the vacuum energy density, also faces severe theoretical challenges, namely, the “fine-tuning” and “cosmic coincidence” problems Sahni and Starobinsky (2000); Bean et al. (2005). In light of the first-year data release from the Dark Energy Spectroscopic Instrument (DESI) collaboration, the \LambdaCDM model is encountering heightened scrutiny. The combination of DESI baryon acoustic oscillations (BAO) data with CMB anisotropy measurements from Planck and the Atacama Cosmology Telescope (ACT), along with Type Ia supernova (SN) data from the PantheonPlus, Union3, and DESY5 datasets, has revealed a statistically significant inclination towards a dynamical dark energy scenario, specifically for the w_{0}w_{a}CDM model, with confidence levels of 2.5\sigma, 3.5\sigma, and 3.9\sigma respectively. The dark-energy equation of state (EoS) in the w_{0}w_{a}CDM model is articulated by w(a)=w_{0}+w_{a}(1-a), (1) where a represents the cosmological scale factor, and w_{0} and w_{a} are constants. The DESI collaboration has reported w_{0}=-0.727\pm 0.067 and w_{a}=-1.05^{+0.31}_{-0.27} based on CMB+DESI+DESY5 datasets, signifying a pronounced preference for w_{0}>-1 and a substantial w_{a}<0. These notable deviations from the \LambdaCDM model, as indicated by DESI, have ignited extensive debates on the nature of dark energy. Numerous studies have endeavored to constrain cosmological parameters utilizing the novel DESI BAO data Calderon et al. (2024); Wang et al. (2024a); Escamilla-Rivera and Sandoval-Orozco (2024); Di Valentino et al. (2024); Yang et al. (2024); Wang and Piao (2024); Gomez-Valent and Sola Peracaula (2024); Allali et al. (2024); Qu et al. (2024a); Wang (2024a); Colgáin et al. (2024); Cortês and Liddle (2024); Wang (2024b); Jiang et al. (2024); Giarè et al. (2024a, b); Du et al. (2024); Toda et al. (2024); Pang et al. (2024); Rebouças et al. (2024); Sabogal et al. (2024); Escamilla et al. (2024); Li et al. (2024a); Specogna et al. (2024); Park et al. (2024a, b); Alestas et al. (2024); Wang et al. (2024b); Giarè (2024); Wu and Zhang (2024); Ye et al. (2024); Tyagi et al. (2024). While parameterization approaches such as the w_{0}w_{a}CDM model are valuable for data-driven description and serve as stringent consistency tests against the null hypothesis of the \LambdaCDM model, they remain empirical and lack a fundamental physical underpinning. Thus, it is imperative to delve into dark energy models that are anchored in solid theoretical frameworks and to evaluate whether the DESI BAO data can accommodate these models. More fundamentally, it is widely posited that the conundrums surrounding dark energy are inextricably tied to the realm of quantum gravity. Consequently, probing the nature of dark energy through the perspective of quantum gravity is of paramount interest. In the absence of a comprehensive quantum gravity theory, we are compelled to lean on the holographic principle within quantum gravity to construct an effective theory of dark energy. The holographic dark energy (HDE) model, rooted in the holographic principle of quantum gravity, has gained significant traction in the quest to unravel the enigma of dark energy. Within the framework of effective quantum field theory, Cohen et al. Cohen et al. (1999) articulated that, when gravity is taken into account, the total energy of a system with scale L should not surpass the mass of a black hole of equivalent size, that is, L^{3}\rho_{\rm de}\lesssim LM_{\rm pl}^{2}. This energy bound gives rise to the density of HDE, \rho_{\rm de}=3c^{2}M_{\rm pl}^{2}L^{-2}, (2) where c is a dimensionless parameter accounting for certain indeterminacies in the effective quantum field theory, M_{\rm pl} is the reduced Planck mass defined by M_{\rm pl}^{2}=(8\pi G)^{-1}, and L represents the infrared cutoff within the theory. Should L be regarded as the scale of the current universe, such as the Hubble radius H^{-1}, the derived dark energy density aligns well with observational data. However, Hsu (2004) noted that this approach yields an incorrect EoS for dark energy. Li (2004) then proposed that L should instead be equated to the scale of the future event horizon, L=a\int^{\infty}_{t}\frac{{\rm d}t}{a}=a\int^{\infty}_{a}\frac{{\rm d}a}{Ha^{2% }}, (3) with H(a) being the Hubble parameter as a function of the scale factor a. This selection not only provides a plausible value for the density of dark energy but also leads to an accelerating universe. Furthermore, the HDE model offers a theoretical explanation for the cosmic coincidence problem Li (2004). Investigations into HDE models have been pursued in the literature (see, e.g., Refs. Huang and Gong (2004); Wang et al. (2005); Nojiri and Odintsov (2006); Zhang and Wu (2005); Zhang (2007, 2006); Zhang and Wu (2007); Zhang et al. (2007, 2014); Landim (2016); Cui et al. (2015a); Wang et al. (2017); Wang and Li (2023)). The parameter c plays a pivotal role in dictating the HDE evolution. At c=1, the EoS of HDE approaches that of a cosmological constant, steering the universe toward a de Sitter phase in the distant future. When c>1, the EoS of HDE stays above -1, with HDE exhibiting quintessence-like dark energy behavior. Conversely, if c<1, the EoS of HDE will cross the phantom divide at w=-1, culminating in a phantom universe destined for a catastrophic big rip. Prior observational constraints on the HDE model have collectively suggested c<1, hinting that HDE could precipitate a phantom universe ending in a big rip. To circumvent this conundrum, researchers have contemplated the implications of extra dimensions Zhang (2010) or entertained the notion of an interaction between dark energy and dark matter within the HDE model Li et al. (2008). Moreover, the possibility of direct interactions between dark energy and dark matter might offer a solution or at least a mitigation to the so-called Hubble tension and cosmic coincidence problems. Interacting dark energy (IDE) models have garnered substantial attention Zhang (2005); Zhang et al. (2008); Szydłowski et al. (2015); Zhang et al. (2010); Li et al. (2011); Li and Zhang (2011); Fu et al. (2012); Cui et al. (2015b); Zhang (2017); Li et al. (2020); Di Valentino et al. (2020); Feng et al. (2020); Zhang et al. (2021); Wang et al. (2022c); Jin et al. (2022); Nunes et al. (2022); Zhao et al. (2023); Han et al. (2024); Forconi et al. (2024); Li et al. (2024b); Halder et al. (2024); Benisty et al. (2024); Nong and Liang (2024). The exploration of interacting models within the framework of holographic dark energy has been extensively studied (see, e.g., Refs. Li et al. (2009); Zhang et al. (2012); Feng and Zhang (2016); Li et al. (2017)). Given that the value of c cannot be ascertained from the theoretical framework of the HDE and IHDE models, it is essential to pin down the value of c through cosmological observations. In this study, we harness the latest observational data, encompassing the DESI BAO data, CMB data from Planck and ACT, and SN data from three distinct compilations — PantheonPlus, Union3, and DESY5 — to constrain both the HDE and IHDE models. Our primary objective is to ascertain the value of c within the context of the HDE and IHDE models using the latest cosmological observations. Furthermore, we employ Bayesian evidence to assess the extent to which the HDE and IHDE models are endorsed by both early and late-universe data. This work is organized as follows. In Sec. II, we briefly introduce the HDE and IHDE models, as well as the cosmological data used in this work. In Sec. III, we report the constraint results and make some relevant discussions. The conclusion is given in Sec. IV."
https://arxiv.org/html/2411.08617v1,Globally Stable Dark Energy in F(R) Gravity,"F(R) models for dark energy generally exhibit a weak curvature singularity, which can be cured by adding an R^{2} term. This correction allows for a unified description of primordial and late-time accelerated expansions. However, most existing models struggle to achieve this, as they become unstable over certain negative ranges of the Ricci scalar, where either the first or second derivative of F(R) turns negative. These instabilities may disrupt the post-inflationary evolution when the Ricci scalar oscillates about the vacuum state after the R^{2} inflation. In this work, we introduce a new model-building to guarantee global stability, i.e., the first and second derivatives are positive for all real Ricci scalars. By extending the idea from Appleby and Battye, we demonstrate that viable models can be constructed by imposing a positive, bounded first derivative of F(R) with a sigmoid shape. As examples, we first reformulate and generalize the original Appleby-Battye model. Then, we propose a new dark energy model, which successfully explains the acceleration of cosmic expansion and passes local gravity tests.","Since the discovery of the current acceleration of the cosmic expansion about a quarter-century ago Riess et al. (1998); Perlmutter et al. (1999), numerous attempts have been made to explain the underlying repulsive energy source, referred to as dark energy Li et al. (2013); Yoo and Watanabe (2012); Mortonson et al. (2013); Klimchitskaya and Mostepanenko (2024). In addition to the static cosmological constant Aghanim et al. (2020); Bull et al. (2016); Perivolaropoulos and Skara (2022), dark energy could also be attributed to a dynamical scalar field emerging, for instance, from F(R) gravity Sotiriou and Faraoni (2010); De Felice and Tsujikawa (2010); Nojiri et al. (2017). By replacing the Ricci scalar R with a general function F(R), F(R) gravity introduces an additional scalar degree of freedom, dubbed scalaron. Capozziello was the first to attempt achieving cosmic acceleration by introducing quintessence into F(R) gravity, called curvature quintessence Capozziello (2002). Subsequently, Carroll et al. proposed the first well-studied model Carroll et al. (2004), while Nojiri et al. proposed the first potential unified model to describe both primordial and late-time acceleration within a single F(R) Lagrangian Nojiri and Odintsov (2003). Such a light scalar field would mediate a long-range fifth force, which is tightly constrained by solar-system tests through the Parameterized Post-Newtonian (PPN) parameter. Fortunately, nonlinear effects can screen the propagation of the fifth force via the chameleon mechanism Khoury and Weltman (2004a, b); Brax et al. (2004). After a few years of exploration, several viable models have been successfully developed Hu and Sawicki (2007); Starobinsky (2007); Appleby and Battye (2007); Tsujikawa (2008); Cognola et al. (2008). However, it was soon recognized that these models typically lead to a weak curvature singularity, occurring in the past Starobinsky (2007); Tsujikawa (2008); Frolov (2008); Appleby and Battye (2008). As a result, the scalaron mass exceeds the Planck mass even during the matter-dominated era, signaling the breakdown of F(R) gravity as an effective field theory. It has been found that adding an R^{2} term can naturally remove the singularity, and the resulting R^{2}-corrected dark energy may provide a unified description of primordial and late-time acceleration of the cosmic expansion Kobayashi and Maeda (2009); Appleby et al. (2010); Lee et al. (2012). In such a unified scenario, the standard sequence of inflation, reheating, radiation-dominated, matter-dominated, and dark energy-dominated epochs must be preserved. In the original R^{2} inflationary model, post-inflationary reheating begins when the linear R term dominates over the nonlinear R^{2} term and oscillates around the vacuum state R=0. Adhering to this reheating mechanism suggests that not all viable dark energy models can fulfill these criteria, as many become unstable when R<0. The only viable model, to the best of my knowledge, is the Appleby-Battye model Appleby et al. (2010), which, unlike others, was specifically designed with the condition F_{R}>0 for all ranges of Ricci scalar Appleby and Battye (2007). For recent cosmological constraints, see Ribeiro et al. (2024). In this paper, we will generalize the idea from the Appleby-Battye model and demonstrate that the favored form of F_{R}(R) is a bounded function with a sigmoid shape, with its midpoint (or center of symmetry) determined by the critical curvature. In addition, the \LambdaCDM model (for positive R) can be regarded as an extreme case of these sigmoid functions, i.e., the Heaviside delta function of R. The Appleby-Battye model contains exponential terms, and as we will show, the first derivative is actually the logistic function. For comparison, we will also introduce a new model in which the first derivative follows a power-law form. Consequently, our model resembles the Hu-Sawicki model but achieves global stability. We demonstrate that this model can account for the current cosmic acceleration by explicitly solving the Friedmann equation. Furthermore, the effective Equation of State (EoS) parameter for the dark energy component exhibits phantom crossing behavior, a common feature of F(R) dark energy models Amendola and Tsujikawa (2008); Bamba et al. (2010, 2011). Next, We constrain the model parameters by confronting our model with local gravity tests. Finally, We discuss the unified description of dark energy and inflation within the framework of globally stable models. Throughout this paper, we adopt natural units with c=\hbar=1 and focus exclusively on the metric formalism, where the metric signature is (-,+,+,+). A prime ”\prime” and a subscript ”R” denote derivatives with respect to the function’s argument and Ricci scalar, respectively."
https://arxiv.org/html/2411.08565v1,The overconcentrated dark halo in the strong lens SDSS J0946+1006 is a subhalo: evidence for self interacting dark matter?,"The nature of dark matter is poorly constrained on subgalactic scales. Alternative models to cold dark matter, such as warm dark matter or self-interacting dark matter, could produce very different dark haloes on these scales. One of the few known dark haloes smaller than a galaxy was discovered in the triple source plane strong lens system J0946+1006. Previous studies have found that this structure is much more concentrated than expected in \LambdaCDM, but have assumed the dark halo is at the same redshift as the main deflector (z_{\rm main}=0.222). In this paper, we fit for the redshift of this dark halo. We reconstruct the first two sources in the system using a forward modelling approach, allowing for additional complexity from multipole perturbations. We find that the perturber redshift is z_{\rm halo}={0.207}^{+0.019}_{-0.019}, and lower bounds on the evidence strongly prefer a subhalo over a line-of-sight structure. Whilst modelling both background sources does not improve constraints on the redshift of the subhalo, it breaks important degeneracies affecting the reconstruction of multipole perturbations. We find that the subhalo is a more than 5\sigma outlier from the \LambdaCDM v_{\rm max}–r_{\rm max} relation and has a steep profile with an average slope of \gamma_{\rm 2D}={-1.81}^{+0.15}_{-0.11} for radii between 0.75-1.25 kpc. This steep slope might indicate dark matter self-interactions causing the subhalo to undergo gravothermal collapse; such collapsed haloes are expected to have \gamma_{\rm 2D}\approx-2.","While \LambdaCDM makes successful predictions about the large-scale structure of the Universe, no particle candidate for dark matter (DM) has so far been detected in the laboratory. Although it has been studied extensively, cold dark matter (CDM) is, therefore, merely a placeholder for a more fundamental theory of dark matter. It is crucial for our understanding of astrophysics and cosmology that we place stronger constraints on its nature. The standard \LambdaCDM model predicts the formation of dark matter haloes that closely follow NFW mass profiles (Navarro et al., 1997). State-of-the-art hydrodynamical simulations (e.g. Schaye et al., 2015; Nelson et al., 2019; Wang et al., 2020; Hernández-Aguayo et al., 2023) indicate clear relations among the properties of these haloes, e.g. between their concentration and mass (see e.g. Sorini et al., 2024) or between the maximum circular velocity and the corresponding radius at which this velocity is found (see e.g. Moliné et al., 2023). However, the properties of dark matter that affect galactic and subgalactic scales are still not fully constrained. Alternatives to CDM might provide a better description of structure formation on these scales. For example, Warm dark matter (WDM) would be preferred if there is a suppressed number of small mass dark matter haloes (see e.g. Bode et al., 2001; Lovell et al., 2014; Lovell, 2020). Models such as fuzzy dark matter (FDM, see e.g. Hu et al., 2000; Hui et al., 2017) or self-interacting dark matter (SIDM, see e.g. Vogelsberger et al., 2012; Vogelsberger et al., 2016; Cyr-Racine et al., 2016; Despali et al., 2019; Yang et al., 2024) could further explain observed cores within dwarf galaxies. SIDM is particularly interesting, as it can generate a variety of haloes with both cores and cusps. Recently, velocity-dependent SIDM became more popular since these models can create cores in small galaxies while allowing Milky way-mass haloes to remain non-spherical and therefore consistent with observations (Vogelsberger et al., 2012; Zavala et al., 2013). A SIDM halo is initially cored because the self-interaction redistributes energy and momentum mostly in the centre where most interactions occur. The core then expands as heat flows inwards from the outskirts of the halo until it becomes isothermal. This process leads to halo profiles that are less cuspy in their centre than their CDM counterparts (see e.g. Spergel & Steinhardt, 2000; Shah & Adhikari, 2024; Yang et al., 2024). The expansion continues until a strong enough negative energy gradient is established and the random motion of particles in the core is no longer sufficient to support its own gravity. This leads eventually to gravothermal collapse, resulting in haloes with more cuspy density profiles than their CDM cousins (see e.g. Turner et al., 2021; Yang et al., 2024). Core collapse behaves similar to the gravothermal catastrophe found in globular clusters (Lynden-Bell & Eggleton, 1980). While these alternative models make predictions that differ from CDM, the presence of baryons can also resculpt haloes without the need for exotic dark matter models (see e.g. Vegetti et al., 2023, who provide a discussion specifically in the context of gravitational lensing). Baryonic physics will further affect dark halo concentrations (see e.g. Heinze et al., 2024) and the expected luminosity of the galaxies that they host (see e.g. Despali et al., 2024). Strong gravitational lensing has been employed extensively to study WDM and sterile neutrinos (see e.g. Vegetti et al., 2018; Ritondale et al., 2019; Gilman et al., 2020; Enzi et al., 2021; Nadler et al., 2021), SIDM (see e.g. Despali et al., 2022; Gilman et al., 2023; Kong et al., 2024), and FDM (see e.g. Powell et al., 2023; Amruth et al., 2023). It can be used to detect and constrain the profiles of dark haloes from their localised lensing effect on multiple images, therefore allowing us to draw conclusions on dark matter microphysics and baryonic physics. However, so far only a small number of subhalos (i.e. haloes that are hosted by more massive haloes) has been detected this way (less than five, Vegetti et al., 2010, 2012; Nierenberg et al., 2014; Hezaveh et al., 2016). A particularly interesting dark perturber has been detected by Vegetti et al. (2010) in the lens system is J0946+1006 (which is part of the SLACS sample of lens systems, see Gavazzi et al., 2008). Previous studies of this dark halo have found it to be unusually overconcentrated (Minor et al., 2021). Recently, Despali et al. (2024, referred to as D24 from here onwards) has further shown that it might further be an outlier in terms of its luminosity. Minor (2024, referred to as M24 from here onwards) found that supersampling plays a significant role in the detection significance. In agreement with Ballard et al. (2024, referred to as B24 from here onwards), they also find that the inclusion of the second source breaks several degeneracies in the lens model. Nonetheless, the concentration parameter remains in tension with \LambdaCDM. The contributed lensing effect from line-of-sight haloes can be more important than the one from subhaloes, depending on the geometry of a lens system (see, e.g. Despali et al., 2018). If the dark halo found in J0946+1006 is a line-of-sight halo, we would expect that its redshift has important implications for the observed arcs since compound lensing can lead to qualitatively different images than single plane lensing (Collett & Bacon, 2016). Only some of these effects can be absorbed by other parameters in the lens model, e.g. the mass distribution of the main deflector or the source light distribution (see e.g. Li et al., 2017; Amorisco et al., 2022). We, therefore, expect to constrain the redshift of this dark halo, even though we can not directly observe its light. So far, the redshift of this dark matter halo has not been constrained rigorously through lens modelling (although approximations have been made in the past, see, e.g. Minor et al., 2021). In this paper, we aim to constrain its redshift, test the power of compound lensing to improve these constraints, and discuss whether or not a free redshift can alleviate the observed tension with \LambdaCDM. Furthermore, while previous studies have not yet done so, we include the first-order multipole perturbations (the ""lopsidedness"") in our mass model. We infer our posteriors using the open source lens modelling code Herculens111https://github.com/Herculens/herculens (Galan et al., 2022) in combination with stochastic variational inference (SVI) and a Hamiltonian-Montecarlo-within-Gibbs sampler (Krawczyk, 2024) using Numpyro (Phan et al., 2019; Bingham et al., 2019). The remainder of this paper is structured as follows. In Section 2, we describe the data that we consider for our analyses. In Section 3, we provide a brief summary on compound lensing and outline the parameteric and pixelated models we use in this paper. In Section 4, we give a description of the statistical methods that we apply in this work. In Section 5, we present our results, including constraints on the redshift of the halo, and discuss them in detail. Finally, we present our conclusions in Section 6."
https://arxiv.org/html/2411.08560v1,"Probing the Cosmological Principle
with weak lensing shear","The Cosmological Principle is a cornerstone of the standard model of cosmology and shapes how we view the Universe and our place within it. It is imperative, then, to devise multiple observational tests which can identify and quantify possible violations of this foundational principle. One possible method of probing large-scale anisotropies involves the use of weak gravitational lensing. We revisit this approach in order to analyse the imprint of late-time anisotropic expansion on cosmic shear. We show that the cross-correlation of shear E- and B-modes on large scales can be used to constrain the magnitude (and possibly direction) of anisotropic expansion. We estimate the signal to noise for multipoles 10\lesssim\ell\lesssim 100 that is achievable by a Euclid-like survey. Our findings suggest that such a survey could detect the E-B signal for reasonable values of the late-time anisotropy parameter.","A key assumption that underlies the standard model of cosmology is that, on sufficiently large scales, the universe is both homogeneous and isotropic – the Cosmological Principle. Because of its numerous and far-reaching implications, this crucial assumption has deservedly been subjected to a litany of observational tests (see e.g. [1, 2]). The cosmic microwave background (CMB) provides tight constraints of \sim 10^{-5} on anisotropy around the time of recombination. The rapid decay of un-sourced anisotropic expansion from recombination to today yields the shear constraint \sim 10^{-10}–10^{-9} [3, 4, 5, 6, 7]. Combining CMB data with measurements of galaxy baryon acoustic oscillations can improve this constraint by several orders of magnitude [8]. Additionally, a period of inflation rapidly isotropises the Universe and severely diminishes any primordial anisotropy [9, 10]. These considerations, along with observations of the relative abundances of light elements [11, 12, 13, 14], appear to disfavour a period of significant anisotropic expansion during the early evolution of the Universe. Nevertheless, these considerations do not completely rule out the possibility of a period of anisotropic expansion. In particular, late-time anisotropy driven by dark energy with intrinsic anisotropic pressure is quite compelling, since it has the potential to generate deviations from isotropy at late times when dark energy begins to dominate the Universe’s energy content. Anisotropic pressure appears in models of magnetised dark energy [15, 16] as well as dark energy possessing an anisotropic equation of state [17, 18, 19, 20, 21, 22, 23, 24]. Moreover, the presence of effective anisotropic stresses is a fairly general feature of modified theories of gravity [25, 26, 27, 28]. Measurements of late-time anisotropy and anisotropic expansion have primarily focused on probes of the Hubble diagram using type Ia supernovae (SN Ia) data [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]. Tests of isotropy have also been performed to check whether the rest-frame of matter coincides with that of the CMB, as required by the Cosmological Principle [52]. There are many claims of an inconsistency in radio continuum and quasar samples (see e.g. [1]), but a recent analysis of the eBOSS surveys finds consistency [53]. A lesser-known probe of late-time anisotropy is weak gravitational lensing [54]. The presence of B-modes on large scales in the weak-lensing signal is indicative of a violation of isotropy [55]. This contrasts starkly with standard weak-lensing wisdom which, being built on perturbed Friedmann\hypLemaître\hypRobertson\hypWalker (FLRW) spacetimes, predicts that B-mode shear should not be generated by large-scale structure at significant levels [56, 57, 58]. On the contrary, detection of these odd-parity shear patterns at significant levels is usually seen as a sign of non-linearities [59, 60, 61, 62], or even systematic effects that need to be removed [63, 57, 64, 65, 66]. On large scales where late-time anisotropy is probed, the non-linear effects will not be a problem, but systematics can arise on very large scales. Nevertheless, once all systematic sources of error and spurious signals are eliminated, the detection of large-scale B-modes at a particular magnitude would allow us to place bounds on large-scale anisotropic expansion in the late Universe. The fundamental object in gravitational lensing is the Jacobi matrix \mathbfcal{D} which maps from observed angular size \bm{\theta}_{O} to physical separation at source \bm{\xi}_{S} \bm{\xi}_{S}=\mathbfcal{D}\bm{\theta}_{O}. (1.1) The Jacobi matrix is usually decomposed into convergence \kappa, rotation \psi, and shear \bm{\gamma} as \mathbfcal{D}=d_{A}\left[\matrixquantity()+\matrixquantity()+\matrixquantity()% \right], (1.2) where d_{A} is the background FLRW angular diameter distance. One can show that the rotation angle \psi\sim\gamma^{2} can be safely ignored in the weak-lensing regime where \gamma\ll 1. Being a scalar object on the celestial sphere, the convergence can be expanded in terms of spherical harmonics \kappa(\mathbf{n})=\sum_{\ell,m}\kappa_{\ell m}Y_{\ell m}(\mathbf{n}), (1.3) where the observation direction \mathbf{n} specifies a point on the sphere. The shear, on the other hand, is a spin-2 tensor variable and thus needs to be expanded in terms of spin-weighted spherical harmonics \gamma_{1}(\mathbf{n})\pm i\gamma_{2}(\mathbf{n})=\sum_{\ell,m}(E_{\ell m}\pm iB% _{\ell m})Y^{\pm 2}_{\ell m}(\mathbf{n}). (1.4) This decomposition separates the shear multipoles into even (E) and odd (B) parity contributions. Importantly, however, linear scalar perturbations of FLRW spacetimes generate only E-modes. Within the FLRW context, only vector, tensor, or higher-order scalar perturbations give rise to B-modes [59, 67, 68]. Building on their previous work, [55] developed a two-parameter perturbation scheme which handles small deviations from isotropic expansion in a systematic way [69] and used it to calculate the B-mode shear generated by the coupling of large-scale anisotropic expansion and scalar metric perturbations. Subsequently, these results were used to quantify the magnitude and behaviour of observables for surveys that will be carried out using the Euclid satellite and the Square Kilometre Array (SKA) radio telescope [70]. However, due to the limited availability of cosmic shear data at the time, [70] was only able to make a very rough estimate of \sigma_{0}/\mathcal{H}_{0}\lesssim 0.4 for the strength of late-time anisotropic expansion using CFHTLenS data [71]. In addition, the estimate by [70] was made using the E-B cross-correlation at an angular scale of \ell\sim 2000, where additional B-mode signals are generated by non-linear dynamics. In this work, we make use of the two-fold perturbation framework of [55], giving details of a model for generating late-time anisotropy and carrying out a systematic analysis of the E-E, E-B and B-B signals. We confirm that the B-B signal is below the noise for reasonable values of the anisotropy parameter. Then we consider the larger E-B power, exploiting the tomography of a Euclid survey [72, 73]. We make a careful estimate of the signal to noise, using a conservative range of multipoles that avoids the very largest scales \ell\lesssim 10, where difficult systematics typically arise, while staying in the large-scale regime \ell\lesssim 100, in order to avoid non-linear contamination. We make use of Halofit [74, 75, 76] in order to account for (mild) non-linearities in observed signals and to determine a cut-off scale \ell_{\text{max}} which avoids any possible small-scale B-mode effects. We construct a simple statistical estimator (Equation 5.4) which contains information about the magnitude and direction of anisotropic expansion. We then estimate the signal-to-noise ratios of this observable for the Euclid tomographic photometric survey, concluding that the signal should be detectable for anisotropy parameters that are consistent with current constraints. Notation: We work in units where c=1 and the metric has signature -+++. Spacetime indices run from 0 to 3 and are represented by lower-case Greek letters, while lower-case Latin letters i,j,\ldots denote spatial indices. These indices are also used to label the ten Euclid tomographic redshift bins, but there should be no confusion, since the two types of index are never used in the same expressions simultaneously. Upper-case Latin letters index the two angular directions associated with screen space."
https://arxiv.org/html/2411.08498v1,A dark energy parameterization independent constraint of the spatial curvature \Omega_{K},"Determining the spatial curvature \Omega_{K} of the Universe has long been crucial in cosmology. In practice, this effort is often entangled with assumptions of dark energy. A combination of distance (D_{\rm M}, D_{\rm L}) and expansion rate (H(z)) measurements can break this degeneracy. However, fitting against discrete data points requires parameterizations of distance and expansion rate as functions of redshifts, which often induces cosmological model dependence. In this work, we propose a new dark energy model-independent parameterization of the cosmological comoving radial distance \chi. Fitting data combining distance (D_{\rm M}, D_{\rm L}) and Hubble parameter (or equivalently D_{H}) measurements, we are then able to obtain \Omega_{K} in a dark energy model-independent manner. We test this parameterization and the associated fitting scheme with mock data generated with a wide range of fiducial dark energy equations of state (-1.3<w<1.3), finding that the best-fit \Omega_{K} is always unbiased. Then we combine SDSS Baryon Acoustic Oscillation (BAO), Pantheon+ sample of Type Ia Supernovae (SNe Ia), and Observational Hubble Data (OHD) to constrain \Omega_{K}. We find a flat universe with \Omega_{K}=-0.01\pm 0.09. Most constraining power is contributed by SDSS BAO, with the BAO-alone constraint \Omega_{K}=-0.03\pm 0.10. When replacing SDSS BAO with DESI year-one BAO measurement, we obtain \Omega_{K}=0.06\pm 0.08. With the full DESI BAO data alone, we forecast \sigma(\Omega_{K})\sim 0.03. Our result verifies the flatness of the universe free of dark energy modeling, and the proposed parameterization would be useful for future investigation of \Omega_{K} and other parameters of interest, such as the horizon radius.","Table 1: Sloan Digital Sky Survey Baryon Acoustic Oscillation (SDSS BAO). z_{\mathrm{eff}} 0.38 0.51 0.70 1.48 2.33 2.33 D_{\rm M}/r_{\rm d} 10.23\pm 0.17 13.36\pm 0.21 17.86\pm 0.33 30.69\pm 0.80 37.6\pm 1.9 37.3\pm 1.7 D_{H}/r_{\rm d} 25.00\pm 0.76 22.33\pm 0.58 19.33\pm 0.53 13.26\pm 0.55 8.93\pm 0.28 9.08\pm 0.34 refer. (Alam et al., 2017) (Gil-Marín et al., 2020) (Neveux et al., 2020) (du Mas des Bourboux et al., 2020) Table 2: Dark Energy Spectroscopic Instrument year-one Baryon Acoustic Oscillation (DESI year-one BAO). z_{\mathrm{eff}} 0.510 0.706 0.930 1.317 2.330 D_{\rm M}/r_{\rm d} 13.62\pm 0.25 16.85\pm 0.32 21.71\pm 0.28 27.79\pm 0.69 39.71\pm 0.94 D_{H}/r_{\rm d} 20.98\pm 0.61 20.08\pm 0.60 17.88\pm 0.35 13.82\pm 0.42 8.52\pm 0.17 refer. (DESI Collaboration et al., 2024a) (DESI Collaboration et al., 2024b) The spatial curvature \Omega_{K} not only influences the fate of our universe but also encodes information on the origin mechanism of the universe (e.g. Guth (1981); Linde (1982)), stimulating us to conduct an accurate constraint of it. So far, most observations indicate that our universe is remarkably flat. Cosmic microwave background (CMB) experiments such as BOOMERanG (Netterfield et al., 2002) and MAXIMA (Hanany et al., 2000) provided the first observational proof of a flat universe (de Bernardis et al., 2000; Balbi et al., 2001; Jaffe et al., 2001). The most precise constraint on the spatial curvature \Omega_{K}=-0.0001\pm 0.0018 was given by combining baryon acoustic oscillation (BAO) and CMB data (Alam et al., 2021; Planck Collaboration et al., 2020), which showed great consistency with a flat universe. However, the primary CMB suffers from a well-known geometry degeneracy (Bond et al., 1997; Zaldarriaga et al., 1997). Without CMB lensing or BAO, Planck Collaboration et al. (2016) and Planck Collaboration et al. (2020) represented a preference for closed universes with a significance slightly above 95% confidence (the ""curvature tension"", Handley (2021)). This geometry degeneracy can also be broken by independent H_{0} measurements (Planck Collaboration et al., 2016). However, given the issue of Hubble tension (e.g. Riess et al. (2019, 2022)), it would be useful to constrain \Omega_{K} independent of CMB data and local H_{0} measurements. Distance measurements, such as D_{\rm L} from Type Ia supernovae (SNe Ia) and D_{\rm M} from BAO, suffer from a degeneracy between \Omega_{K} and a general dark energy model. So are H(z) measurements from BAO and OHD (observational Hubble data). Combining distance measurements and H(z) measurement breaks this degeneracy, allowing for dark energy model independent constraint on \Omega_{K} through Eq.5 & 2 (Clarkson et al., 2008). Nevertheless, given the discrete data points and the required integration/differentiation operation, a smooth functional form of D_{\rm M}(z) and H(z) is required. This can be done by assuming a cosmological model such as \LambdaCDM with a free \Omega_{K} or wCDM. This will induce model dependence in \Omega_{K} constraint and may affect the constraint on other parameters (Dossett & Ishak, 2012). A widely adopted alternative is to parameterize the distance in a cosmological model-independent manner. The most direct idea was to expand the cosmology distance as a polynomial function. However, this expansion faces a serious divergence issue in the high-redshift domain (Cattoën & Visser, 2007). Many new methods were proposed to overcome this problem (Cattoën & Visser, 2007; Shafieloo, 2012; Aviles et al., 2013; Capozziello et al., 2020; Li et al., 2020; Zhang et al., 2023). One of these approaches is utilizing the Padé polynomials, which successfully leads to convergence (Li et al., 2020; Zhang et al., 2023). Inspired by this method and the asymptotic behavior of the proper distance \chi(z) at z\rightarrow 0 and z\rightarrow\infty, We propose a new parameterization of \chi(z). This 3-parameter parameterization has several advantages. (1) It is well behaved over all z, so is its derivative d\chi/dz=c/H(z). (2) It provides excellent description to wCDM over a wide range of dark energy equation of state w. So we believe that it is applicable in general cases. (3) All constraining power on \Omega_{K} then solely come from the \chi-D_{\rm L,M} relation. Namely, the \Omega_{K} constraint depends on no cosmological models other than the FRW metric. We then apply this parameterization to the latest observations, including BAO measurements, SNe Ia, and observational Hubble data (OHD) to constrain the spatial curvature \Omega_{K}. We use the 12 data points of D_{\rm M}/r_{\rm d} and D_{H}/r_{\rm d} collected in Alam et al. (2021), obtained from four generations of Sloan Digital Sky Survey (SDSS). This data set covers the redshift range from z=0.38 to z=2.33. For the SNe Ia data, we utilize the newest Pantheon+ sample, including 1701 light curves of 1550 unique and spectroscopically confirmed SNe Ia (Scolnic et al., 2022). As an additional data set, we assemble 33 data points of OHD (Jimenez et al., 2003; Simon et al., 2005; Stern et al., 2010; Moresco et al., 2012; Zhang et al., 2014; Moresco, 2015; Moresco et al., 2016; Ratsimbazafy et al., 2017; Borghi et al., 2022; Jiao et al., 2023), which provide the immediate constraint on the Hubble constant H_{0}. By applying parameter fitting to these three data sets with our model, we ought to obtain a new reliable constraint on the spatial curvature \Omega_{K}. In addition, we also apply to the BAO measurements of DESI data release 1 (DESI Collaboration et al., 2024a, b, c). Our analysis confirms the flatness of the universe, independent of dark energy parameterization and CMB data. This paper is organized as follows. We first introduce the method of constraining curvature in §2, and then present the data in §3. The results are shown and analyzed in §4. We finally present our conclusions and discussions in §5. We also include an appendix explaining the mock data test and providing further details."
https://arxiv.org/html/2411.08240v1,Constraints on local primordial non-Gaussianity with 3d Velocity Reconstruction from the Kinetic Sunyaev-Zeldovich Effect,"The cosmic velocity field is an unbiased probe of the total matter distribution but is challenging to measure directly at intermediate and high redshifts. The large-scale velocity field imprints a signal in the cosmic microwave background (CMB) through the kinetic Sunyaev-Zeldovich (kSZ) effect. We perform the first 3d reconstruction of the large-scale velocity field from the kSZ effect by applying a quadratic estimator to CMB temperature maps and the 3d positions of galaxies. We do so by combining CMB data from the fifth data release of the Atacama Cosmology Telescope (in combination with Planck) and a spectroscopic galaxy sample from the Sloan Digital Sky Survey. We then measure the galaxy-velocity cross-power spectrum and detect the presence of the kSZ signal at a signal-to-noise ratio of 7.2\sigma. Using this galaxy-velocity cross-correlation alone, we constrain the amplitude of local primordial non-Gaussianity finding f_{\rm NL}=-90^{+210}_{-350}. This pathfinder measurement sets the stage for joint galaxy-CMB kSZ constraints to significantly enhance the f_{\rm NL} information obtained from galaxy surveys through sample variance cancellation.","Unraveling the initial conditions that seeded structure formation is one of the main objectives of modern cosmology. From a diverse set of observational datasets, we have determined that the early density field is well described by a Gaussian random field [1]. However, small deviations from pure Gaussianity, referred to as primordial non-Gaussianities (PNG), could yield crucial information about the physics of inflation (see e.g. [2]). For this Letter, we will focus on local-type PNG quantified by the parameter f_{\rm NL} through the relation [3] \displaystyle\Phi=\phi+f_{\rm NL}\left(\phi^{2}-\langle\phi^{2}\rangle\right), (1) where \Phi is the total gravitational potential and \phi is the potential for a Gaussian field. The strongest constraints on local PNG have been obtained from the CMB from Planck with f_{\rm NL}=0.8\pm 5.0 [1] while large-scale structure probes have found f_{\rm NL}=7\pm 31 using the galaxy power spectrum and bispectrum [4], -4<f_{\rm NL}<27 using the eBOSS quasar samples [5], and -87<f_{\rm NL}<19 from cross-correlations between large-scale structure (LSS) tracers and CMB lensing [6]. The former CMB constraints probe non-Gaussianity through three-point functions in the temperature and polarization maps, while the latter LSS probes exploit the fact that a non-zero f_{\rm NL} imprints a scale-dependent bias to dark matter tracers such as galaxies such that [7] \displaystyle b_{g}^{\rm NL}(k)=b_{g}+f_{\rm NL}\frac{3H_{0}^{2}\Omega_{m}% \delta_{c}(b_{g}-1)}{k^{2}T(k)D(z)}, (2) where k is the magnitude of the comoving Fourier wavenumber, H_{0} is the Hubble constant, \Omega_{m} is the relic matter density, b_{g} is the galaxy bias, T(k) is the matter transfer function, \delta_{c}=1.42 is the critical overdensity, and D(z) is the linear growth factor at redshift z. Due to the 1/k^{2} scaling of the non-Gaussian bias, most of the information about f_{\rm NL} is located on the largest scales. However, due to cosmic variance, few independent realizations of the large-scale modes exist. To overcome this sample variance, it is helpful to complement the galaxy power spectrum measurement (proportional to \left(b_{g}^{\rm NL}\right)^{2}) with information from a different tracer of the matter density [8]. As first suggested in [9], the kinetic Sunyaev-Zeldovich (kSZ) effect in CMB maps can be used to reconstruct the velocity field, providing the necessary tracer that can dramatically improve constraints on f_{\rm NL} through sample variance cancellation. Recently, the kSZ effect was used to set bounds on f_{\rm NL} using a two-dimensional analog of the approach we demonstrate here [10], albeit without a detected kSZ signal. A similar 2d kSZ estimator was also used with the sample of luminous red galaxies from the Dark Energy Spectroscopic Instrument (DESI) reaching a detection significance of 3.8\sigma [11], but in a manner insensitive to scale-dependent bias. In this Letter, we constrain f_{\rm NL} through its effect on scale-dependent bias using the full 3d information in the velocity field and detecting the kSZ signal at >7\sigma. Figure 1: Sketch diagram of a two-dimensional Gaussian overdensity (1+\delta) and corresponding gravitational potential (\Phi), velocity magnitude (|\mathbf{v}|), and line-of-sight velocity (v_{\parallel}). We note the dipolar pattern which emerges when considering the parallel component of the velocity (the y-axis is taken as the line-of-sight). On large scales, the galaxy velocity field is an unbiased tracer of the total matter density, and the radial velocity field (along the line-of-sight) can be obtained from the continuity equation \displaystyle v_{r}\left(\mathbf{k}\right)=faH\frac{ik_{r}}{k^{2}}\delta_{m}% \left(\mathbf{k}\right), (3) where f is the logarithmic growth factor, a=1/(1+z) is the scale factor, H is the Hubble expansion rate, and \mathbf{k},\;k_{r},\;k are the comoving wavenumber, its radial component and its magnitude, respectively. The cosmic velocity field is thus a perfect complement to the galaxy density field if one hopes to get past the cosmic variance limit on large scales. The challenge in this case, however, is to measure said velocities. In one instance, we can use standard candles located within galaxies to make peculiar velocity measurements [12]. This gives precise measurements but is limited to low redshifts of about z\lesssim 0.15 [13]. This reduces the volume of the survey and the range of scales we can probe, thus limiting the available information to constrain (local) primordial non-Gaussianity. We can also reconstruct the velocity field from the galaxy density [14, 15, 16, 17]. However, this requires us to assume the form of the galaxy bias and cannot be used to probe its scale-dependence. This type of velocity reconstruction is also quite sensitive to non-linear growth of structure and, therefore, is unsuitable to extract more information about the cosmic velocity field. The kSZ effect is a CMB secondary which results from the motion of electrons relative to the rest frame of the CMB. As CMB photons travel through ionized gas, they inverse Compton scatter with free electrons and gain (or lose) energy proportional to the electron line-of-sight velocity (v_{e,\;r}) \displaystyle\frac{\Delta T_{\rm kSZ}}{T_{\rm CMB}}=\int dz\frac{d\chi}{dz}K(z% )(1+\delta_{e})v_{e,\;r}, (4) where \chi is the comoving distance, \delta_{e} is the electron density contrast. The kSZ prefactor reads \displaystyle K(z)\equiv\sigma_{T}n_{e,0}(1+z)^{2}x_{e}(z)e^{-\tau(z)}, (5) where \sigma_{T} is the Compton cross-section, n_{e,0} is the present-day electron number density, x_{e} is the ionization fraction, and \tau is the optical depth. The kSZ effect induces a squeezed bispectrum between long-wavelength (k_{L}\sim 0.01\;\mathrm{Mpc}^{-1}) radial velocities, short-wavelength (k_{S}\sim 1\;\mathrm{Mpc}^{-1}) galaxy overdensities, and CMB secondary temperature fluctuations [18]. From this bispectrum, one can construct a 3-dimensional quadratic estimator for the radial momentum field [18, 9, 19] \displaystyle\hat{v}_{r}\left(\mathbf{k}_{L}\right)=N_{v_{r}}^{(0)}\left(% \mathbf{k}_{L}\right)\frac{K_{\rm eff}}{\chi^{2}_{\rm eff}}\int\frac{d^{3}k_{S% }}{(2\pi)^{3}}\frac{d^{2}\ell}{(2\pi)^{2}}\frac{P_{ge}(k_{S})}{C_{\ell}^{\rm tot% }P_{gg}^{\rm tot}(k_{S})}\delta_{g}^{*}(\mathbf{k}_{S})T_{\rm CMB}^{*}(\bm{% \ell})(2\pi)^{3}\delta_{D}^{3}\left(\mathbf{k}_{L}+\mathbf{k}_{S}+\frac{\bm{% \ell}}{\chi_{\mathrm{eff}}}\right), (6) where K_{\rm eff},\;\chi_{\rm eff} are evaluated at z_{\rm eff} and \delta_{D}^{3} denotes the three-dimensional Dirac delta. The P_{gg}, P_{ge}, and C_{\ell}^{\rm tot} denote the 3d galaxy-galaxy, 3d galaxy-electron, and CMB temperature map angular power spectra, respectively. Here, T_{\rm CMB}(\bm{\ell}) is the 2d Fourier transform of the beam-deconvolved CMB temperature map and \delta_{g}^{*}(\mathbf{k}_{S}) is the 3d Fourier transform of the galaxy overdensity field. We can simplify the above by directly applying a filter to the CMB temperature map, giving us an approximated kSZ temperature map through \displaystyle\hat{T}_{\rm kSZ}(\bm{\theta})=\int\frac{d^{2}\ell}{(2\pi)^{2}}e^% {i\bm{\theta}\cdot\bm{\ell}}\frac{P_{ge}(\ell/\chi_{\rm eff})}{C_{\ell}^{\rm tot% }P_{gg}^{\rm tot}(\ell/\chi_{\rm eff})}T_{\rm CMB}(\bm{\ell}). (7) The galaxy auto-spectrum and the galaxy-electron cross-spectrum are obtained using the halo model prescription implemented in the hmvec code 111https://github.com/simonsobs/hmvec[18]. The reconstruction in Eq. 6 produces a “box” of the 3d momentum field, whose noise per mode on large scales tends to a constant \displaystyle N_{v_{r}}^{(0)}=\frac{\chi^{2}_{\rm eff}}{K^{2}_{\rm eff}}\left(% \int\frac{k_{S}dk_{S}}{2\pi}\;\frac{\left[P_{ge}\left(k_{S}\right)\right]^{2}}% {P_{gg}^{\rm tot}\left(k_{S}\right)C_{\ell}^{\rm tot}}\right)^{-1}, (8) where the CMB power spectrum is evaluated at \ell=k_{S}/\chi_{\rm eff}. With the noise being constant, we can calculate a temperature-based velocity weight by rescaling the approximated kSZ map through \displaystyle\hat{T}_{v_{r}}(\bm{\theta},z)=N_{v_{r}}^{(0)}\frac{K_{\rm eff}}{% \chi^{2}_{\rm eff}}\hat{T}_{\rm kSZ}(\bm{\theta}). (9) Since the exact distribution of electrons on cosmological scales is unknown, our choice of model for the galaxy-electron cross-spectrum introduces a bias in the reconstructed velocity field such that [18] \displaystyle\langle\hat{v}_{r}(\mathbf{k})\rangle=b_{v}(\mathbf{k})v_{r}(% \mathbf{k}) (10) where b_{v} is the velocity reconstruction bias. It has been shown to be constant on large scales [19]. This bias stems from the optical depth degeneracy of kSZ measurements and the uncertainties of baryonic physics. If our fiducial P_{ge} captures the true galaxy-electron correlation, we expect this bias to converge to unity. Since the true P_{ge} is not known a priori, we focus on large scales and marginalize over this parameter in our analysis. Since the reconstruction bias is perfectly degenerate with the combination f\sigma_{8}, our estimator cannot measure the growth function as with peculiar velocities surveys [20]. This degeneracy can be broken in a sufficiently high SNR measurements by modeling the galaxy-velocity octupole in combination with the dipole. Figure 2: (Left panel) Galaxy density in the CMASS NGC sample used in the reconstruction process. Darker shades correspond to higher density regions. (Right panel) Smoothed map of the reconstructed line-of-sight momentum using the quadratic estimator on the CMASS NGC sample. The angular direction denotes right ascension and the field are summed along the declination axis. The bluer regions denote motion towards the observer in the velocity panel. I.1 Data To measure the kSZ signal, we use CMB data from the Atacama Cosmology Telescope (ACT) and the Planck satellite together with galaxy positions and spectroscopic redshifts from the Sloan Digital Sky Survey Baryon Oscillation Spectroscopic Survey twelfth data release (SDSS BOSS DR12; [21, 22]). The CMB maps we use are ACT+Planck co-adds [23] at central frequencies of roughly 90 and 150 GHz (labeled f090 and f150). We describe the processing and filtering of these maps in Appendix A. For our galaxy sample, we limit our analysis to the North Galactic Cap (NGC) of the CMASS survey with 0.43<z<0.7 and z_{\rm eff}=0.55, since it it is the largest contiguous area from BOSS that has overlap with ACT data. We also do not include the LOWZ sample as its low effective redshift does not allow us to reconstruct sufficiently large scales."
https://arxiv.org/html/2411.08152v1,Assessing the growth of structure over cosmic time with CMB lensing,"The standard \LambdaCDM cosmological model informed by cosmic microwave background (CMB) anisotropies makes a precise prediction for the growth of matter density fluctuations over cosmic time on linear scales. A variety of cosmological observables offer independent and complementary ways of testing this prediction, but results have been mixed, with many constraints on the amplitude of structure S_{8} being 2-3\sigma lower than the expectation from Planck primary CMB anisotropies. It is currently unclear whether these discrepancies are due to observational systematics, non-linearities and baryonic effects or new physics. We review how gravitational lensing of the CMB has and will continue to provide insights into this problem, including through tomographic cross-correlations with galaxy surveys over cosmic time.","The cosmic microwave background (CMB) is a key prediction of the hot big bang and consists of photons that mostly last scattered during the epoch of recombination at around t=380,000 years. Measurements of CMB anisotropies by the COBE satellite [1] provided our first view of fluctuations in the early universe, providing a handle on physics from well before the epoch of structure formation. Subsequent measurements by the WMAP satellite [2, 3] (together with complementary balloon and ground-based experiments [4, 5, 6]) ushered in an era of precision cosmology, cementing the \Lambda-Cold Dark Matter (\LambdaCDM) model. With state-of-the-art primary CMB measurements from the Planck satellite [7, 8, 9, 10, 11, 12], the community now has a precise benchmark model to compare other observations to. Increasingly precise measurements of the CMB damping tail and CMB polarization by high-resolution ground-based experiments like the South Pole Telescope (SPT; e.g [13]) and the Atacama Cosmology Telescope (ACT; e.g. [14, 15]) are now on a path to enhancing the Planck legacy, especially for extensions to the \LambdaCDM model (e.g. [16]). At the same time, multiple distinct probes of the late-time universe continue to offer a test of the standard model, including galaxy redshift space distortions (RSD) [17, 18, 19], the Lyman-\alpha forest [19, 20, 21], the abundance of galaxy clusters [22, 23] and galaxy weak lensing [24, 25, 26, 27, 28, 29]. Secondary anisotropies in the CMB itself have become an informative source of information on large-scale structure through the variety of ways in which CMB photons interact with matter since the recombination epoch. This includes measurements of gas pressure and density through the thermal and kinetic Sunyaev-Zeldovich (SZ) effects [30, 31, 32, 33, 34] respectively. Here, we review the status of weak gravitational lensing of the CMB[35] as a probe of the growth of structure. The CMB allows for a semi-model-independent test of the standard model. Providing a snapshot of the z\approx 1090 (t\sim 380,000 yr) universe that can be modeled precisely with linear physics, it allows for precise constraints on parameters of the standard \LambdaCDM model (with the Planck satellite providing the state-of-the-art constraints today). This fit model can then be extrapolated to late times, with the growth of density perturbations on large scales modeled precisely with linear theory. A useful proxy for describing growth is the number \sigma_{8}, which measures the root-mean-square (RMS) amplitude of fluctuations in the total matter density assuming linear theory. The amplitude of fluctuations increases with cosmic time (or decreasing redshift z) through \sigma_{8}(z)=D(z)/D(0)\sigma_{8} due to the growth of perturbations under gravity (elaborated more in Sec 2). Some observables are sensitive to the combination S_{8}\equiv\sigma_{8}({\Omega_{m}}/0.3)^{0.5}, where {\Omega_{m}} is the total matter density. As suggested in [36], with the CMB prediction in hand, the amplitude of structure can then be measured more directly with late-time growth probes, although in many cases, significant modeling of non-linear physics is required. Many probes of the late-time universe are in mild disagreement with the Planck prediction. Disagreements between the early universe prediction and the late universe may either be to systematics in either CMB or late-time measurements, inadequacies of modeling of non-linear physics or new physics. We will discuss in this review how CMB lensing has recently been used to distinguish between systematics and new physics by providing an accurate view of the matter power spectrum primarily at wave-numbers k<0.1{\rm Mpc}^{-1} and redshifts z=1-5. In Sec 2, we review the physics behind growth of structure in the standard model, while in Sec 3 we explore how CMB lensing serves as a probe of growth. We conclude placing recent CMB lensing measurements in the broader context of other large-scale structure probes in 4."
https://arxiv.org/html/2411.08134v1,The EDGES measurement  an excess radio background during the cosmic dawn,"In 2018 the EDGES experiment claimed the first detection of the global cosmic 21cm signal, which featured an absorption trough centered around z\sim 17 with a depth of approximately -500mK. This amplitude is deeper than the standard prediction (in which the radio background is determined by the cosmic microwave background) by a factor of two and potentially hints at the existence of a radio background excess. While this result was obtained by fitting the data with a phenomenological flattened-Gaussian shape for the cosmological signal, here we develop a physical model for the inhomogeneous radio background sourced by the first galaxies hosting population III stars. Star formation in these galaxies is quenched at lower redshifts due to various feedback mechanisms, so they serve as a natural candidate for the excess radio background hinted by EDGES, without violating present day measurements by ARCADE2. We forward-model the EDGES sky temperature data, jointly sampling our physical model for the cosmic signal, a foreground model, and residual calibration errors. We compare the Bayesian evidences obtained by varying the complexity and prior ranges for the systematics. We find that the data is best explained by a model with seven log-polynomial foreground terms, and that it requires calibration residuals. Interestingly, the presence of a cosmic 21cm signal with a non-standard depth is decisively disfavored. This is contrary to previous EDGES analysis in the context of extra radio background models, serving as a caution against using a “pseudo-likelihood” built on a model (flattened Gaussian) that is different from the one being used for inference. We make our simulation code and associated emulator publicly-available.","By mapping out the first half of our observable Universe, the cosmological 21cm signal from neutral hydrogen promises to revolutionize astrophysics and cosmology. Experimental efforts to detect this signal fall into two categories. Interferometers like the Square Kilometre Array (SKA; e.g. Mellema et al. 2013), Hydrogen Epoch of Reionization Array (HERA; Abdurashidova et al. 2022), Low Frequency Array (LOFAR; van Haarlem et al. 2013) and Murchison Widefield Array (MWA; Tingay et al. 2013) are dedicated to measuring the spatial fluctuations of the 21 cm brightness temperature T_{21}. Alternatively, global experiments including EDGES (Bowman et al. 2018b), SARAS (Singh et al. 2018; Bevins et al. 2022), REACH (de Lera Acedo et al. 2022), MIST (Monsalve et al. 2024) and LEDA (Greenhill & Bernardi 2012) aim to detect the spatially-averaged (global) 21 cm signal \bar{T}_{21}, The first detection of the cosmic 21 cm signal was claimed by the EDGES experiment in 2018 (Bowman et al. 2018b, hereafter B18), which measured a flattened \bar{T}_{21} absorption profile centered around z\sim 17 with an amplitude of 500^{+500}_{-200}mK (at 99% credible interval). A subsequent Bayesian reanalysis of the entire signal chain by Murray et al. (2022) confirmed the presence of such a signal in the data. This signal is about twice as deep as the maximum amplitude allowed by standard models in which baryons cool at most adiabatically and the radio background is dominated by the cosmic microwave background (CMB). As has been pointed out in many previous studies, a cosmological origin for the EDGES signal necessitates either extra gas cooling, e.g. via interaction between dark matter (DM) and baryons (Barkana 2018; Muñoz & Loeb 2018; Berlin et al. 2018), early gas-CMB decoupling in early dark energy models (Hill & Baxter 2018), or an excess radio background component in addition to the CMB (Feng & Holder 2018; Ewall-Wice et al. 2018; Mirocha & Furlanetto 2019; Ewall-Wice et al. 2020; Reis et al. 2020). It is very challenging to perform accurate inference from the global 21cm signal. Astrophysical and terrestrial foregrounds are 4 to 6 orders of magnitude brighter than the cosmological signal (e.g. Pritchard & Loeb 2012; Hibbard et al. 2023) and unlike interferometers, global 21cm experiments cannot clean the signal by comparing different pointings and sightlines (e.g. Nasirudin et al. 2020). In addition to astrophysical foregrounds, other systematics such as the Earth’s ionosphere (e.g. Datta et al. 2016; Shen et al. 2021), antenna beam effects (e.g. Mahesh et al. 2021; Sims et al. 2023), and signal-chain defects (e.g. Monsalve et al. 2017; Murray et al. 2022) can further obscure the signal and potentially lead to false reconstructions (Tauscher et al. 2020; Hibbard et al. 2023). Therefore, the interpretation of the EDGES results requires detailed characterisation of these contaminants together with the cosmic signal, with all of the resulting parameters being constrained through Bayesian inference. The 21cm signal reported in B18 was obtained by fitting the EDGES sky temperature data with a flat-Gaussian \bar{T}_{21} profile and a 5-term polynomial foreground model. Subsequent analysis in Hills et al. 2018; Singh & Subrahmanyan 2019 showed that the best-fit profile found in B18 requires an un-physical foreground, and that the EDGES data can also be fit with multiple \bar{T}_{21} shapes that are different from those used in B18. Sims & Pober 2020 (hereafter SP20) extended these analyses by considering a set of models with various combinations of the cosmic signal, residual calibration systematics and foregrounds. After comparing the Bayesian evidence for a total of 128 models, SP20 found that models that are not strongly disfavored included a flattened Gaussian cosmic signal with a non-standard depth and sinusoidal calibration residuals. Subsequent data from the SARAS3 experiment disfavored the presence of an EDGES-like cosmic signal (Singh et al. 2022). It is somewhat surprising that many analyses reached different conclusions about the existence and/or depth of the cosmic 21cm profile in EDGES data. Ideally, any interpretation should use self-consistent forward models with physically-informed priors. The vast majority of radio excess and gas cooling analysis (see e.g. Ewall-Wice et al. 2018; Barkana 2018; Muñoz & Loeb 2018; Mirocha & Furlanetto 2019; Ewall-Wice et al. 2020; Reis et al. 2020) instead compute a ”pseudo likelihood” using only a summary of the data, typically the center and (or) width of the B18 profile. This approach is intrinsically problematic since the B18 profile was computed assuming a phenomenological ”flattened Gaussian” \bar{T}_{21} and therefore cannot be interpreted directly with a different (e.g. physical) model for the \bar{T}_{21}.111A CMB analogy to this common approach would be if one took the best fit EoR history from Planck obtained assuming a phenomenological tanh shape, \hat{x}_{\rm HII,tanh}(z), and then performed inference using a physical, galaxy-driven model for the EoR history, x_{\rm HII,gal}(z), but constructing a Gaussian likelihood around \hat{x}_{\rm HII,tanh}(z)-x_{\rm HII,gal}(z). To our knowledge, this has not been done in CMB analyses. Here we directly forward model the EDGES sky temperature data, comparing the Bayesian evidence for a range of foreground models and residual calibration systematics. Unlike the analytic and phenomenological models used in B18, SP20 and Murray et al. 2022, we build on the cosmological 21cmFAST simulation code (e.g. Mesinger et al. 2011; Murray et al. 2020), including an inhomogeneous, excess radio background sourced from the first, molecular-cooling galaxies that are hosted by \sim 10^{5}-10^{8}M_{\odot} halos. These primordial galaxies are expected to be dominated by metal-free, so-called population III (Pop III) stars and their remnants, which could have very different properties compared to later generations (e.g. Woosley et al. 2002; Heger et al. 2003). For simplicity, we refer to molecular-cooling galaxies as ”Pop III galaxies” in this work. Later generations of galaxies form out of Pop III galaxy seeds, and mainly reside in more massive dark matter halos (>10^{8}M_{\odot}) for which atomic cooling is efficient. Analogously to Pop III galaxies, here we will refer to atomic cooling galaxies as Pop II galaxies, as most of their stellar population is formed out of pre-enriched gas (e.g. Bromm & Larson 2004; Trenti 2010; Salvaterra et al. 2011). Previously it was shown in Mirocha & Furlanetto 2019 and Reis et al. 2020 that in order for Pop II galaxies to reproduce the amplitude of the B18 feature, the corresponding radio background would exceed present-day measurements from the Absolute Radiometer for Cosmology, Astrophysics, and Diffuse Emission 2 (ARCADE2; Fixsen et al. 2011) by orders of magnitude. This tension was resolved in an ad-hoc manner by introducing a phenomenological redshift cut-off parameter, z_{\rm off} (see e.g. Mirocha & Furlanetto 2019; Reis et al. 2020; Sikder et al. 2023, 2024), below which galactic radio emission is quenched. In contrast, Pop III galaxies are susceptible to feedback from the Lyman-Werner (LW) background (e.g. Qin et al. 2020a; Muñoz et al. 2022) which eventually sterilizes star formation inside them. As a result, the radio emission from Pop III galaxies decays naturally once a LW background is established, making Pop III galaxies a natural candidate to explain EDGES while being consistent with ARCADE2 measurements. In this work, we determine whether the EDGES data prefers such a physical model for a high-redshift radio-background excess. We forward-model the global signal varying Pop III galaxy properties, foregrounds and systematics/calibration residuals. We vary the complexity of the foreground and systematics models, comparing their Bayesian evidences. We include complementary data from ARCADE2 and Planck. To speed up the inference, we train an emulator of summary observables in our model, providing it as a new option in the public 21cmEMU 222https://github.com/21cmfast/21cmEMU package (Breitman et al. 2023). The paper is organized as follows. Sec. 2 reviews the EDGES observation, while Sec. 3 details our model for Pop III radio galaxy. We build some physical intuition about our model with an illustrative example in Sec. 4 and then discuss our model for foreground emission and systematics in Sec. 5. Our likelihoods and inference methodology are detailed in Secs. 6 and 7, respectively. Sec. 8 shows our results. Finally we present discussions and caveats in Sec. 9 and conclude in Sec. 10. We assume \Lambda{\rm CDM} cosmology with the relevant parameters set by Planck 2018 results (Aghanim et al. 2020): H_{0}=67.66\ {\rm kms^{-1}Mpc^{-1}}, \Omega_{\Lambda}=0.6903, \Omega_{{\rm m}}=0.3096, \Omega_{{\rm c}}=0.2607, \Omega_{{\rm b}}=0.0489, \ln(10^{10}A_{\rm s})=3.047, n_{\rm s}=0.967."
https://arxiv.org/html/2411.08087v1,Fréchet Vectors as sensitive tools for blind tests of CMB anomalies,"Cosmological data collected on a sphere, such as CMB anisotropies, are typically represented by the spherical harmonic coefficients, denoted as a_{\ell m}. The angular power spectrum, or C_{\ell}, serves as the fundamental estimator of the variance in this data. Alternatively, spherical data and their variance can also be characterized using Multipole Vectors (MVs) and the Fréchet variance. The vectors that minimize this variance, known as Fréchet Vectors (FVs), define the center of mass of points on a compact space, making them highly sensitive to small displacements of these points. This sensitivity makes FVs excellent indicators of statistical correlations between different multipoles. We demonstrate this using both simulations and real data. Through simulations, we show that FVs enable a blind detection and reconstruction of the location associated with a mock Cold Spot anomaly introduced in an otherwise isotropic sky. Applying this to the 2018 Planck maps, we implement several improvements on previous model-independent tests of Gaussianity and statistical isotropy, down to arc-minute scales. When compared with simulated maps that incorporate masking and anisotropic noise, for 2\leq\ell\leq 1500, while Planck’s MVs appear consistent with these hypotheses, the corresponding FVs reject them with significances between 5.2 and 8.3\sigma, depending on the component separation method.","Ever since its discovery, cosmic microwave background (CMB) radiation has played a central role in the development of modern cosmology. Following its first detection by Penzias & Wilson, the three generations of space-based instruments, together with several ground-based telescopes and balloon experiments (see [1] for a brief review), have cemented our understanding of the early universe and the Big Bang paradigm. In particular, they were pivotal in establishing \LambdaCDM as our de-facto standard cosmological model. CMB anisotropies are arguably the cleanest and most direct observational window into the early universe that we have. This conclusion is supported by our precise understanding of the physics that produce the observed fluctuations across different scales, due in part to the fact that these fluctuations largely occur in a regime where the dynamics is essentially linear. If the universe is further assumed to have primordial fluctuations which are Gaussian, and to have translational and rotational symmetries, as dictated by the Cosmological Principle, the angular power spectrum, C_{\ell}, becomes a summary statistic that encapsulates all available information. Although the measured C_{\ell}s of the CMB temperature map are in excellent agreement with what one would expect from a Gaussian, and statistically isotropic (GSI) universe [2, 3], and no detection of primordial non-Gaussianity has yet been made [4], deviations from the GSI expectations are certainly present in the data. Some arise naturally from known second-order perturbations, such as CMB lensing [5], detected at over 40\sigma [6], and aberration and Doppler couplings due to the observer motion [7, 8], detected at around 6\sigma [9]. However, other so-called anomalies have been found, which have no consensual explanation [10, 11]. Some of these deviations may be due to different cosmological models, or they may be simpler flukes in the data. Thus, a central issue is to tell whether measured deviations are of a cosmological, astrophysical, or systematic nature. This prompts us to examine the data using alternative mathematical representations, summary statistics, or both. This particular approach to the analysis of CMB anisotropies is not new. In fact, CMB studies have long benefited from alternative representations of the usual harmonic analysis, as exemplified by the extensive use of wavelets [12, 13, 14, 15, 16, 17], Minkowski functionals [18, 19, 20, 21], and multipole vectors [22, 23, 24, 25, 26]. These are versatile tools, applicable to a wide range of topics in CMB data analysis, from component separation methods [27], to non-Gaussianities [19, 20, 16], and cosmic topologies [28, 24]. Alternatively, if a null test of the GSI hypotheses is desired, then one can keep the usual harmonic approach and dispose of anisotropic and model-independent implementations of two-point correlation functions and their estimators [29, 30, 31, 32, 33, 34]. Among these tools, Multipole Vectors (MVs) figure as the least explored by cosmologists. Originally introduced by Maxwell in the context of electrostatics, they are an alternative to the standard harmonic basis, summarized by the multipolar coefficients a_{\ell m}s, to describe CMB fluctuations [22]. In the multipole vector basis, the 2\ell+1 degrees of freedom characterizing a given CMB multipole \ell are split into \ell unit headless vectors plus a scalar which, in the case of GSI skies, is proportional to the (cosmology-dependent) C_{\ell}s. Thus, in the standard cosmological model, MVs carry all the C_{\ell}-independent data contained in the a_{\ell m}s. Moreover, they are naturally invariant under spatial rotations, unlike the a_{\ell m}s which mix ms in such cases. These properties make the MVs an interesting tool to probe deviations from Gaussianity and isotropy hypotheses in a model-independent way [26]. This motivation led to a recent development of the code polyMV, discussed below, which allows efficient computation of the MVs at all scales, which was previously not possible [26]. Although the MV set contains all the information of a given scalar map of the sky, often in cosmology we make use of summary statistics to analyze the data. In the context of MVs, summary statistics have been designed for the specific task of detecting CMB anomalies [22, 35] or, more specifically, possible alignments between low \ell multipoles, which is one of the observed anomalies of the CMB [36, 37, 38]. For this reason, these statistics invariably mix vectors from different multipoles, and thus cannot be used as a summary of the behavior of the CMB at each angular scale. It is therefore interesting to have a summary statistic for each multipole, that is still capable of detecting deviations from isotropy. One such statistic is the Fréchet vectors (FVs), originally proposed in [26]. Frechét vectors borrow from the mathematical definition of variance in metric spaces, also known as Fréchet variance, to build a geometrically motivated and model-independent summary statistic of the MVs which results in one unit and headless vector per CMB multipole. These vectors are geometrically defined as the position of the “center of mass” of the MVs on the unit sphere and have many interesting geometrical and statistical properties. First, being based on the MVs, they are C_{\ell}-independent and have an isotropic 1-point distribution function in the case of GSI skies [26]. Second, since they result from compressing \ell vectors into just one, they are less prone to cosmic variance. Finally, because the direction of each FV is defined by the collection of \ell multipole vectors, the FVs are more susceptible to very small angular variations of each MV, which can result in greater sensitivity to anisotropies in the data. Here we demonstrate, using simulations, that the FVs can be directly correlated with spatial anisotropies artificially introduced in the input maps. This allows in principle for a blind reconstruction of spatial anisotropies of real CMB maps. We illustrate this for the case of a mock Cold Spot anomaly [39, 40], and show that in the case of a spot of nearly the same aperture but twice as cold, FVs can detect its presence and pinpoint its axis blindly. We also analyze real Planck 2018 data with a straightforward chi-squared test of the null GSI hypotheses using 1-point statistics of both MVs and FVs. Overall, we find Planck’s MVs to be consistent with GSI simulations, including mask and instrumental anisotropic noise across all scales considered (2\leq\ell\leq 2000). In contrast, Planck’s FVs rule them out with varying statistical strengths. These depend on whether we probe scales with high (\ell<1500) or low (\ell\geq 1500) signal-to-noise ratios. Conservatively, we find NILC and SMICA to be inconsistent with GSI simulations at 3.5\sigma and 4.3\sigma, respectively, in the former region. We include many important improvements in the analysis compared to [26], such as a pixel-based implementation (as opposed to an independent analysis on both angular coordinates of these vectors), an improved evaluation of MVs and FVs covariance matrix, and the inclusion of the simulated anisotropic noise in our simulations, which allowed for an extension of our tests to the scales 1500\leq\ell\leq 2000, where CMB maps are known to contain residual anisotropies. We start Section 2 by recalling the formalism of the MVs. We then introduce the notion of statistical variance in Riemannian spaces, from which the FVs are defined. In Section 3 we analyze a mock Cold Spot model. In Section 4 we describe our pixel-based implementation of a null test of Gaussianity and isotropy, and in Section 5 we present the results of our tests on 2018 Planck maps. We conclude in Section 6."
https://arxiv.org/html/2411.08744v1,The impact of large-scale galaxy clustering on the variance of the Hellings-Downs correlation: numerical results,"Pulsar timing array experiments have recently found evidence for a stochastic gravitational wave (GW) background, which induces correlations among pulsar timing residuals described by the Hellings and Downs (HD) curve. Standard calculations of the HD correlation and its variance assume an isotropic background. However, for a background of astrophysical origin, we expect a higher GW spectral density in directions with higher galaxy number densities. In a companion paper, we have developed a theoretical formalism to account for the anisotropies arising from large-scale galaxy clustering, leading to a new contribution to the variance of the HD correlation. In this subsequent work, we provide numerical results for this novel effect. We consider a GW background resulting from mergers of supermassive black hole binaries, and relate the merger number density to the overdensity of galaxies. We find that anisotropies due to large-scale galaxy clustering lead to a standard deviation of the HD correlation at most at percent level, remaining well below the standard contributions to the HD variance. Hence, this kind of anisotropies in the GW source distribution does not represent a substantial contamination to the correlations of timing residuals in present and future PTA surveys. Suitable statistical methods to extract the galaxy clustering signal from PTA data will be investigated in the future.","Pulsar timing arrays (PTAs) were used to deliver the first evidence of a stochastic gravitational wave (GW) background in the nHz band NANOGrav:2023gor ; Reardon:2023gzh ; EPTA:2023sfo ; Xu:2023wog . The correlation of pulsar timing residuals due to the passing of GWs, as a function of the pulsar pair separation on the sky, is usually assumed to be described by the Hellings and Downs (HD) curve Hellings:1983fr .111We note that we use the notion HD curve to refer to the theoretical prediction by Hellings and Downs Hellings:1983fr , while HD correlation corresponds to the observed correlation that is in fact measured in our Universe and might differ from the idealized HD curve. However, it has been recently shown that the stochasticity of GW sources, and the fact that the number of pulsar pairs at a given separation is finite, cause a departure from this idealized curve Allen:2022bjz ; Allen:2022dzg ; Allen:2022ksj ; Romano:2023zhb ; Allen:2024rqk . In these studies, however, the GW sources are assumed to be isotropically distributed. In Ref. Grimm:2024lfj we presented a novel framework to account for the impact of the cosmological large-scale structure on the HD correlation.222Prior to our work in Ref. Grimm:2024lfj , the possibility of anisotropies in the distribution of GW sources in the PTA band has been pointed out in various studies, however either not in the context of the HD correlation Allen:1996gp ; Cusin:2017fwz ; Cusin:2017mjm ; Cusin:2018rsq ; Cusin:2018avf ; Cusin:2019jpv ; Cusin:2019jhg ; Pitrou:2019rjz ; Jenkins:2019nks ; Alonso:2020mva ; renzini2022 , or without targeting the impact on its variance Mingarelli:2013dsa ; Taylor:2013esa ; Gair:2014rwa ; Ali-Haimoud:2016mbv . The detectability of kinematic anisotropies in the SGWB with different PTA experiments has been recently studied in Ref. Cruz:2024svc . Indeed, since galaxies are not isotropically distributed on the sky but cluster to form the large-scale structure, we expect an intrinsic isotropy to be present as well for astrophysical GW sources. The timing residual measured for a given pulsar is linear in the total GW strain in a given direction, due to a superposition of GWs, emitting incoherently by a large number of sources. Such a signal is stochastic for two distinct reasons: first because we cannot predict the properties of the emitted waves (in particular the phases), and second because we cannot predict the distribution of sources. Usually, the second source of stochasticity is ignored: sources are assumed to be isotropically distributed, and there is therefore only one possible realization of the distribution of sources. The HD curve is then defined as the product of the timing residuals of two pulsars, averaged over all realizations of the GW emission. As we have shown in Ref. Grimm:2024lfj , to account for anisotropies in the distribution of sources due to large-scale galaxy clustering, one needs to add an additional average over all possible realizations of the distribution of sources. We considered an ensemble of GW sources in one realization of the universe (i.e. a fixed distribution of sources), and then analyzed how the HD correlation varies from realization to realization. The mean of the HD correlation itself is not affected by clustering, which means that anisotropies in the distribution of sources do not generate a systematic bias. However, the large-scale structure of the Universe induces a variance in the measurement of the HD correlation, quantifying to which amount the measurements in our specific realization of the Universe can differ from the mean.333In this work and in the companion paper Grimm:2024lfj we consider only the effect of clustering (i.e. fluctuations in the distribution of sources) on the variance of the HD correlation, neglecting relativistic effects (due to gravitational potentials and source velocities) that are expected to be subdominant on all scales, see e.g. the calculation in Ref. Cusin:2017fwz ; Pitrou:2019rjz in the context of ground-based detectors. In this work, we numerically evaluate the size of this variance. To do so, we assume that the observed background is given by the GW emission of supermassive black hole (SMBH) binaries in the inspiraling phase. For our case study we choose to work with the catalogs of Refs. BarausseCatalogue ; Klein:2015hvg , and we show results for the three population models considered therein: the popIII model based on light growth seeds for SMBHs; and the other two heavy-seed Q3 SMBH models, with (Q3-d) and without (Q3-nd) delays between galaxy merger and SMBH binary merger. We assume that the overdensity of black hole mergers is a biased tracer of the underlying galaxy overdensity, which itself is a biased tracer of the matter distribution. We find that anisotropies due to large-scale galaxy clustering lead to a departure from the HD curve at a sub-percent level, ranging from 0.54% to 2.85% depending on pulsar pair separation, i.e. well below the precision of current PTA surveys NANOGrav:2023gor ; Reardon:2023gzh ; EPTA:2023sfo ; Xu:2023wog . Moreover, it is well below the standard variance calculated in Ref. Allen:2022dzg under the assumption of an isotropic source distribution. This indicates that the clustering of sources can safely be neglected in current and future analyses. In addition to these results, which refer to a single pulsar pair, we also compute the variance related to an (infinite) continuous distribution of pulsar pairs, which we refer to as the irreducible case. Averaging over pairs of pulsars reduces the clustering variance by an order of magnitude, since anisotropies in the distribution of sources affect pulsar pairs at different sky locations differently and are therefore washed out by the average over an infinite number of pulsars. The irreducible and single pair cases provide, respectively, a lower and upper bound on the clustering variance in a real observation with a finite number of pulsar pairs. We compare our results with the approximate computations of the clustering variance derived in Ref. Allen:2024mtn for the pulsar-averaged, i.e. irreducible case. We find that the irreducible clustering variance is, with a relative amplitude of 0.10-0.14\%, an order of magnitude smaller than the upper bound of 1% claimed in Ref. Allen:2024mtn . This difference is due to the different modeling of the clustering correlation of GW sources. Ref. Allen:2024mtn assumes a constant value for the angular power spectrum of galaxy correlations (independent on angular separation) and does not take any redshift dependence into account. In contrast, we are using the clustering correlation predicted in a \LambdaCDM cosmological model, which properly accounts for the non-trivial dependence of clustering correlations on source angular separation and on redshift. Our formalism provides therefore a clear link between our cosmological and astrophysical modeling and the variance of the HD correlation. The remaining part of this paper is structured as follows. In Section II, we describe basic concepts concerning PTAs and galaxy clustering in the context of this work. We also give an overview over different types of variance contributions to the HD correlation introduced in this and previous works Grimm:2024lfj ; Allen:2022dzg , and summarize the analytical results of Ref. Grimm:2024lfj for the clustering variance. Then, in Section III, we present the modeling and catalog reconstruction of the quantity b_{\rm GW}, which relates galaxy density to GW spectral density and modulates the amplitude of the clustering variance. The fact that SMBH mergers are biased tracers of the underlying galaxy and matter fields are discussed as well. Section IV presents the numerical results for the clustering variance obtained in this work. We also describe our numerical methods and provide a comparison to Ref. Allen:2024mtn in this section. Finally, we conclude in Section V. Moreover, in Appendix A, we present the analytical results for the irreducible clustering variance in the case of infinitely many pulsar pairs."
https://arxiv.org/html/2411.08691v1,Chiral Gravitational Wave Background from Audible Axion via Nieh-Yan Term,"Axions and axion-like particles can be probed through gravitational waves indirectly, often referred to as “audible axions”. The usual concept of audible axion relies on the coupling between the axions and the gauge fields. Here we consider an axion-like mechanism with coupling to the Nieh-Yan term. This interaction leads to the direct and efficient production of gravitational waves during the radiation-dominated era, originating from the tachyonic instability of the gravitational perturbations with the Nieh-Yan term. We calculate the energy spectral density of the chiral gravitational wave background and the comoving energy density of axion-like fields. Based on the numerical results, we explore the parameter space of axion masses and decay constants for detectable gravitational wave signals, either in pulsar timing arrays or space-based gravitational wave detections.","In modern physics, understanding the nature of dark matter is crucial. One of the candidates for dark matter is the axion [1, 2, 3, 4, 5, 6, 7], in general, it also includes axion-like particles (ALPs) [8, 3, 4, 5, 9, 10, 11, 12, 13]. Axion was originally introduced to solve the strong CP problem [14, 15], and it also appeared extensively in other studies such as string theory [16]. Following the first detection of gravitational waves by LIGO [17], the detection of new particles in cosmology, such as axions or ALPs, by gravitational waves has become possible [18, 19, 20]. In recent years, many studies focusing on parity-violating gravity, with Chern-Simons modified gravity being one of the most notable examples [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]. In this work, we utilize another healthy parity-violating model known as Nieh-Yan modified teleparallel gravity (or Nieh-Yan gravity) [32, 33, 34, 35, 36, 37, 38, 39, 40]. The Nieh-Yan gravity is a theory that modifies the teleparallel equivalent of general relativity (TEGR) by the Nieh-Yan term [41, 42], which couples with the axion or axion-like field. We explore the different parameter regions built with axion masses and decay constants, and we numerically calculate the chiral gravitational wave background [43, 44, 45, 46, 47, 48] produced by the audible axion via Nieh-Yan term. We find that the gravitational wave spectral energy density varies with different helicities. Considering the evolution of gravitational waves, we identify detectable signals in pulsar timing arrays (PTA) [49, 50, 51, 52, 53] or future space-based gravitational wave detectors such as LISA [54], Taiji [55], and ASTROD-GW [56]. Furthermore, by calculating the axion comoving energy density within the axion dynamics which is affected by gravitational backreaction, we find that backreaction of the Nieh-Yan term can reduce the axion comoving energy density by several orders of magnitude. Thus, in some parameter spaces, the relic density of axion dark matter can be comparable to the observational results [57, 58], and the chiral gravitational wave radiation energy density is also within a reasonable range [59]. The organization of this paper is as follows. In section II, we briefly review the TEGR, introduce the Nieh-Yan modified teleparallel gravity, and discuss how the axion dynamics are affected by geometric backreaction. In section III, we study the production mechanism of chiral gravitational waves via Nieh-Yan term, by numerically calculating the energy spectral density of chiral gravitational waves. The process of numerical calculation is described in detail. In section IV, we explore the parameter region of axion mass and decay constants that could be detected by PTA and the space-based gravitational wave detectors. The time evolution of the comoving energy densities of the axion and the gravitational wave are calculated. In the last section V, we conclude with the discussions. Throughout this paper, we choose the signature difference as (-,+,+,+), and set c=\hbar=1."
https://arxiv.org/html/2411.08630v1,Black hole jets on the scale of the Cosmic Web,"Jets launched by supermassive black holes transport relativistic leptons, magnetic fields, and atomic nuclei from the centres of galaxies to their outskirts and beyond. These outflows embody the most energetic pathway by which galaxies respond to their Cosmic Web environment. Studying black hole feedback is an astrophysical frontier, providing insights on star formation, galaxy cluster stability, and the origin of cosmic rays, magnetism, and heavy elements throughout the Universe. This feedback’s cosmological importance is ultimately bounded by the reach of black hole jets, and could be sweeping if jets travel far at early epochs. Here we present the joint LOFAR–uGMRT–Keck discovery of a black hole jet pair extending over 7 megaparsecs — the largest galaxy-made structure ever found. The outflow, seen 7.5 gigayears into the past, spans two-thirds of a typical cosmic void radius, thus penetrating voids at {\sim}95\% probability. This system demonstrates that jets can avoid destruction by magnetohydrodynamical instabilities over cosmological distances, even at epochs when the Universe was 15–7 times denser than it is today. Whereas previous record-breaking outflows were powered by radiatively inefficient active galactic nuclei, this outflow is powered by a radiatively efficient active galactic nucleus, a type common at early epochs. If, as implied, a population of early void-penetrating outflows existed, then black hole jets could have overwritten the fields from primordial magnetogenesis. This outflow shows that energy transport from supermassive black holes operates on scales of the Cosmic Web and raises the possibility that cosmic rays and magnetism in the intergalactic medium have a non-local, cross-void origin.","1 Main text Nearly every galaxy harbours a spinning supermassive black hole (SMBH) in its centre. The episodic infall of dust, gas, and stars is believed to activate the Blandford–Znajek mechanism [5], in which electric and magnetic fields convert black hole spin to kinetic energy carried by electrons and positrons. These leptons form a pair of jets: collimated, relativistic flows along the spin axis that point away from the galactic centre. Supported by helical magnetic fields [e.g. 58], the most powerful jets avoid disruption by stellar winds [e.g. 54] and entrain wind-borne atomic nuclei [e.g. 77], while blasting off towards intergalactic space. These jet-driven outflows comprise the majority of bright sources in the known radio sky. To clarify the impact of black hole energy transport on the intergalactic medium (IGM), recent studies [e.g. 14, 47, 48, 45] searched for Mpc-scale outflows: Nature’s largest, and often most powerful, jet systems. The International LOFAR Telescope [ILT; 70] has emerged as the prime instrument for their discovery and characterisation. Our team systematically scanned the ILT’s ongoing northern sky survey at wavelength \lambda=2.08\ \mathrm{m} both with machine learning and by eye — the latter with significant contributions from citizen scientists [26]. This endeavour has increased the number of known Mpc-scale outflows from a few hundred to over eleven thousand [45]. Our largest find is the outflow shown in Fig. 1, which we name Porphyrion. Figure 1: Deep radio images of a 7 Mpc–long, black hole–driven outflow at central wavelengths \lambda=2.08\ \mathrm{m} (top) and \lambda=0.46\ \mathrm{m} (bottom). These images were taken with the ILT and uGMRT, respectively, and have resolutions of 6.2^{\prime\prime} and 4.3^{\prime\prime}. The top panel’s inset shows ILT VLBI imagery at \lambda=2.08\ \mathrm{m} and a resolution of 0.4^{\prime\prime}. The bottom panel’s inset shows Legacy Survey DR10 optical–infrared imagery. The larger images cover 15^{\prime}\times 15^{\prime} of sky area, whilst the insets cover 1^{\prime}\times 1^{\prime}. For scale, we show the stellar Milky Way disk (diameter: 50 kpc) and a ten times inflated version. The source, of angular length \phi=13.4^{\prime}\pm 0.1^{\prime}, is unusually thin. It consists of a northern lobe, a northern jet, a core, a southern jet with an inner hotspot, and a southern outer hotspot with a backflow. To investigate from which of two radio-emitting galaxies halfway along the jet axis the outflow originates, we processed ILT very-long-baseline interferometry (VLBI) data of the central 4^{\prime}\times 4^{\prime}. At a spatial resolution of 3\ \mathrm{kpc}, the image (Fig. 1’s top panel inset) shows lone, unresolved radio sources in these galaxies, in both cases implying active accretion onto an SMBH. Because the detection of jets near either black hole (and along the overarching NNE–SSW axis) would clarify Porphyrion’s origin, we performed deep follow-up observations with the Upgraded Giant Metrewave Radio Telescope (uGMRT) at \lambda=0.46\ \mathrm{m}. The resulting image and ancillary optical–infrared data (Fig. 1’s bottom panel) reveal that the outflow protrudes from a massive (M_{\star}=6.7\begin{subarray}{c}+1.4\\ -1.4\end{subarray}\cdot 10^{11}\ M_{\odot}) galaxy. We observed this galaxy with the Low Resolution Imaging Spectrometer [LRIS; 51, 41, 64, 61] on the W. M. Keck Observatory’s Keck I Telescope, measuring a spectroscopic redshift z=0.896\pm 0.001 (Fig. 2). Figure 2: Both rest-frame ultraviolet–optical spectroscopy (top) and radio–ultraviolet photometry (bottom) demonstrate that the outflow’s host galaxy harbours an RE AGN. Top: LRIS spectrum exhibiting hydrogen, carbon, oxygen, and neon emission. The forbidden lines from multiply ionised oxygen and neon (dark red) could not be generated by even the hottest stars, and instead stem from the narrow-line region of an RE AGN at a redshift z=0.896\pm 0.001. Bottom: Bayesian inference of the galaxy’s SED (Methods) favours the presence of an AGN accretion disk (dark blue) with an obscuring torus (purple), again indicating radiative efficiency. We witness Porphyrion at t_{\mathrm{BB}}=6.3\ \mathrm{Gyr} after the Big Bang. The outflow’s angular length and redshift entail a sky-projected length l_{\mathrm{p}}=6.43\pm 0.05\ \mathrm{Mpc}. This makes Porphyrion the projectively longest known structure generated by an astrophysical body. The outflow’s total length exceeds this projected length, but by how much depends on the unknown inclination of the jets with respect to the sky plane. Deprojection formulae [48] predict a total length l=6.8\begin{subarray}{c}+1.2\\ -0.3\end{subarray}\ \mathrm{Mpc}, with expectation \mathbb{E}[L\ |\ L_{\mathrm{p}}=l_{\mathrm{p}}]=7.28\pm 0.05\ \mathrm{Mpc} (Methods). We thus estimate Porphyrion to be {\sim}7\ \mathrm{Mpc} long in total. Spanning {\sim}66\% of the radius of a typical cosmic void at its redshift, the outflow is truly cosmological. The fact that outflows exceeding 4 Mpc have been known since the 1970s [76], whilst those exceeding 5 Mpc remained undiscovered half a century of technological progress later, hitherto suggested a physical limit to outflow growth near 5 Mpc. Our finding proves this suggestion false. Surprisingly, SMBH jets can remain collimated over several megaparsecs, despite the growth of (magneto)hydrodynamical (MHD) instabilities — chiefly Kelvin–Helmholtz instabilities — predicted theoretically and seen in simulations of shorter jets [e.g. 53]. No MHD simulations of Mpc-scale jets yet exist: the spatio-temporal grids required imply a numerical cost {\sim}10^{2} times higher than that of state-of-the-art runs. Outflows like Porphyrion thus offer a window into a jet physics regime that, at present, cannot be explored numerically. Active galactic nuclei (AGN) with accretion disks extending to the innermost stable circular orbits of their SMBHs efficiently convert the gravitational potential energy of infalling matter into radiation, and are thus called radiatively efficient (RE); all others are called radiatively inefficient (RI) [27, 23]. In RE AGN, the luminous accretion disk photo-ionises a circumnuclear region emitting narrow, and often forbidden, spectral lines. The Keck-observed prominence of forbidden ultraviolet–optical lines from oxygen and neon (chiefly that of the [O III]\lambda5007 line, which is 10.3\pm 0.2 times brighter than the H\beta line) therefore reveals the presence of an RE AGN [7]. By contrast, all previous record-length outflows, such as 3C 236 (l_{\mathrm{p}}=4.6\ \mathrm{Mpc}; [76]), J1420–0545 (l_{\mathrm{p}}=4.9\ \mathrm{Mpc}; [39]), and Alcyoneus (l_{\mathrm{p}}=5.0\ \mathrm{Mpc}; [47]), are fuelled by RI AGN in recent history (t_{\mathrm{BB}}=10.2–12.4\ \mathrm{Gyr}). Whereas RI AGN occur primarily in evolved, ‘red and dead’ ellipticals [27], RE AGN feature vigorous gas inflows and are thus generally found in star-forming galaxies. Indeed, in the first billions of years of cosmic time, RE AGN dominated the radio-bright AGN population [75]. The potential of Mpc-scale outflows to spread cosmic rays (CRs), heat, heavy atoms, and magnetic fields through the IGM is particularly high if large specimina could emerge from the type of AGN abundant at early epochs, when the Universe’s volume was smaller. The discovery of a 7\ \mathrm{Mpc}–long, RE AGN–fuelled outflow before cosmic half-time therefore highlights the hitherto understudied cosmological transport capabilities of Mpc-scale outflows. Figure 3: By superimposing Porphyrion’s total length and radio luminosity (green dot) on evolutionary tracks from dynamical modelling (red–white–blue curves), we infer the outflow’s two-sided jet power and age. We assume the host galaxy to reside in a galaxy group bordering voids, through which the jets eventually travel. The host galaxy likely inhabits a Cosmic Web filament. Vast voids, which make up the bulk ({\sim}80\%) of the Universe’s volume [20], surround such massive structures in most directions. Jets as long as Porphyrion’s encounter void-like densities and temperatures with high probability ({\sim}95\%; Methods). Indeed, the collimated nature of the jets favours scenarios in which they descend into voids, as jets gain resilience against Kelvin–Helmholtz instabilities when the ambient density declines [e.g. 53]. Dynamical modelling suggests a two-sided jet power Q=1.3\pm 0.1\cdot 10^{39}\ \mathrm{W} and an age T=1.9\begin{subarray}{c}+0.7\\ -0.2\end{subarray}\ \mathrm{Gyr} (Fig. 3; Methods). The outflow’s average expansion speed v=0.012\ c, comparable to Alcyoneus’ [47]. In voids and the warm–hot IGM, the speed of sound c_{\mathrm{s}}\sim 10^{0}–10^{1}\ \mathrm{km\ s^{-1}}: the jets grow hypersonically at Mach numbers \mathcal{M}\sim 10^{2}–10^{3} and drive strong shocks into voids. Porphyrion’s jets have carried an energy E=QT=8\begin{subarray}{c}+2\\ -1\end{subarray}\cdot 10^{55}\ \mathrm{J} into the IGM — an amount comparable to the energy released during galaxy cluster mergers [e.g. 71]. This suggests that the outflow is among the most energetic post–Big Bang events to have occurred in its Cosmic Web region. Even though the SMBH might have gained a significant fraction of its mass while powering the jets (\Delta M_{\bullet}>2\frac{E}{c^{2}}=9\begin{subarray}{c}+2\\ -1\end{subarray}\cdot 10^{8}\ M_{\odot}), it appears to have maintained a constant spin axis throughout gigayears of activity. Shocks running perpendicular to the jets dissipate enough heat into the filament to increase its temperature by \Delta T\sim 10^{7}\ \mathrm{K} and its radius by \Delta r\sim 10^{-1}–10^{0}\ \mathrm{Mpc} (Methods). Outflows like Porphyrion thus locally alter the Cosmic Web’s shape. Figure 3 illustrates that the radio luminosity — and, consequently, the radio surface brightness — of constant–jet power, Mpc-long outflows decreases over time. As Fig. 1 evinces, Porphyrion borders on the noise of leading current-day telescopes; all outflows further progressed on the same evolutionary track hitherto evade detection. Similar outflows at higher redshifts or at lower jet powers, and similar but less slenderly shaped outflows, are likewise undetectable. More generally, statistical modelling [48] suggests that the detectable population is just the tip of the iceberg: owing to their low radio surface brightnesses, most Mpc-scale outflows are still concealed by noise. These arguments imply the existence of a hidden population of outflows with sizes comparable to, and possibly larger than, Porphyrion’s. Figure 4: Leptons escaping from the lobes of void-penetrating Mpc-scale outflows diffuse rapidly in weakly magnetised voids. For a single void, and through cosmic time, we show the volumetric fraction filled by electrons and positrons (with 1\ \mathrm{GeV} of initial energy) originating from Porphyrion. Equally energetic protons diffuse faster, given their minimal inverse Compton losses to the CMB. We consider diffusion through turbulent magnetic fields with strengths B and coherence lengths \lambda_{\mathrm{c}}\sim 1\ \mathrm{Mpc} (spanning a factor two; see translucent bands). The hatched strip marks the time prior to Porphyrion’s observed state during which its lobes likely (with probability {>}80\%) penetrated voids. Mpc-scale outflows long enough to breach filaments, such as Porphyrion, transport large quantities of heavy atoms and CRs into voids [4]. In particular, Mpc-scale jets endow \mathrm{Mpc}^{3}-scale volumes in voids with metallicities Z\sim 10^{-3}–10^{-2}\ Z_{\odot} (Methods). Furthermore, we predict that — in voids — the jet- and buoyancy-dominated phases of outflow dynamics are followed by a diffusion phase. Figure 4 shows the time evolution of the volume-filling fraction of CRs escaping from a void-penetrating lobe. Many particles undergo this fate: the lobe leaks CR energy at a rate P\sim 10^{30}\ \mathrm{W}\sim 10^{3}\ L_{\odot} (Methods), equivalent to a flux of {\sim}10^{39} 1 GeV–particles per second.111However, in the context of lobe energetics, this loss channel is negligible. For example, as P\sim 10^{-9}\ Q, jet power fluctuations have a far greater effect [e.g. 74]. The weaker the magnetic fields in voids initially are (see annotations), the greater the mean free path of the diffusing CRs is, and thus the more rapidly they disperse. If these CRs spread an amount of magnetic energy comparable to their own energy, as suggested by equipartition at source, then a single void-penetrating lobe could fill its void with a magnetic field of strength B\sim 10^{-16}–10^{-15}\ \mathrm{G} within a Hubble time (Methods). Diffusion-driven magnetisation is self-regulating: as the magnetic field strength rises, the mean free path falls, slowing further diffusion. This mechanism for astrophysical magnetogenesis generates fields consistent with constraints from GeV gamma-ray searches around TeV blazars [e.g. 46, 10]. Porphyrion indicates that RE AGN may be at least as effective at generating Mpc-scale outflows as RI AGN are in the Local Universe. If the comoving number density of actively powered Mpc-scale outflows has remained roughly constant over time at {\sim}10^{1}\ (100\ \mathrm{Mpc})^{-3} [48, 45], and a comoving volume of (100\ \mathrm{Mpc})^{3} contains {\sim}10^{1} voids [13], then there would exist {\sim}1 actively powered Mpc-scale outflow near every void at every instant. As Mpc-scale outflows are powered for {\sim}10^{-2}–10^{0}\ \mathrm{Gyr} [e.g. 25, 47], {\sim}10^{2} Mpc-scale outflows may have been generated near every void throughout cosmic history. Only few ({\sim}0.5\%)222A single Mpc-scale outflow may penetrate two or more voids. would need to extend into voids to make CR diffusion from leaky lobes common enough to magnetise the Universe to the observed levels. Our work suggests that void magnetic fields only trace primordial fields if the latter were strong; otherwise, primordial signals are readily overwritten by void-penetrating Mpc-scale outflows. Rather than stemming from the Early Universe, magnetism in voids could thus trace the history of black hole energy transport on the scale of the Cosmic Web."
https://arxiv.org/html/2411.08577v1,"Constraining Axion-Like Particles from observations of 
AGN B2 2234+28A and 3C 454.3","Axion-photon oscillation effect provides a possible explanation for the presence of very-high-energy (VHE) \gamma-ray signals from distant sources. In this work, we propose a model-dependent method to select possible sources that may give sufficient constraints on the axion parameters. We investigate such effect in the spectra of active galactic nuclei (AGN) B2 2234+28A and 3C 454.3 based on data obtained from Fermi Large Area Telescope (Fermi-LAT) and MAGIC U.L. We utilize the Markov Chain Monte Carlo method to fit the axion parameters, yielding a result of g_{a\gamma}=3.05^{+0.51}_{-0.31}\times 10^{-11} GeV-1 for the axion-photon coupling strength and m_{a}=5.25^{+2.35}_{-2.65}\times 10^{-8} eV for the axion mass. We also perform 95% confidence level (CL) constraints to set an upper limit for g_{a\gamma}.","I INTRODUCTION The hypothetical pseudoscalar boson called axion originate from addressing the strong CP problem in quantum chromodynamics (QCD) [1, 2, 3, 4, 5, 6]. Existence of analogous particles known as axion-like particles (ALPs) are also predicted by theories beyond the Standard Model such as string theory[7, 8], and have been considered as potential constituents of dark matter[9, 10, 11, 12]. ALP’s coupling with high energy photons leads to ALP-photon oscillation under transverse external magnetic field [13, 14, 15]. Many laboratory experiments are currently seeking ALP’s via this effect [16, 17], such as CAST [18], OSQAR [19, 20] and PVLAS [21, 22]. The conversion effect also suggests the feasibility of probing ALPs via astrophysical methods, i.e., through modifications in \gamma-ray spectra of active galactic nuclei (AGN). Specifically, ALP-photon oscillation reduces the photon loss due to annihilation with extragalactic background light (EBL) [23, 24, 25, 26, 27], leading to high energy photons that have considerably small probabilities of arrival on Earth being observed [28, 29, 30, 31, 32, 33]. Many prior works have been conducted utilizing such effect to constrain ALP parameters based on spectra of very high energy (VHE) \gamma-ray emission of TeV scale from sources such as Gamma Ray Burst 221009A (GRB 221009A) [34, 35], the Crab Nebula [36], Markarian 421 (Mrk421) [37, 38, 39, 40], Mrk 501 and M87 [41]. Despite the robustness of AGN spectrum data, only those spectra that exhibit observable modifications under ALP theory can provide stringent constraints on ALP parameters. It should be noted that while higher energy photons are more likely to undergo ALP oscillation, the detection of such \gamma-ray does not necessarily indicate efficient ALP modification. The relatively short distance of low redshift sources may diminish the effect of EBL absorption, allowing the transmission of VHE photons without ALP effects. An example of this is M87, from which photons ranging from GeV to TeV orders are observed [42, 43, 44]. However, the source cannot provide stringent constraints[41] because the main mechanism that enables the detection of its VHE photons is related to its relatively short distance from Earth, rather than any potential ALP effect. Therefore, we develop a source selection method that takes into account both the ALP oscillations at high energy scale and the reduced EBL absorption at low redshifts. This approach allows us to identify potential sources whose photons would indeed experience ALP oscillations during their journey through space under ALP theories and subsequently alter the Spectral energy distribution (SED). Using such method, we target our constraints on \gamma-ray observations of Flat Spectrum Radio Quasars (FSRQs) B2 2234+28A (B2A, z=0.790) and 3C 454.3 (3C4, z=0.859). B2A exhibited significantly increased activity in the GeV energy band in the recent decade [45], while 3C4 was once one of the brightest \gamma-ray sources in the sky [46] and photons of TeV scale was recently detected. The amplified \gamma-ray signals of both sources were observed by Fermi-LAT [47, 48] and MAGIC [49, 50, 51], covering an energy range of 1 GeV to 10 TeV, sufficient to conduct stringent constraints given the high redshifts of the two sources. The rest of the paper is organized as follows. In Section II, we briefly introduce the interstellar propagation of photons considering both EBL absorption and ALP-photon oscillation. In Section III, we present our method to select potential sources whose spectrum would exhibit apparent strengthening in the flux of high-energy photons, based on the critical energy for ALP-photon oscillation and its influence on photon propagation probability. In section IV, we describe the statistical and programming method used to analyze the observational data. The derived constraints, complemented by a detailed analysis, are presented in Section V. Finally, we summarize our results and conclusions in Section VI."
https://arxiv.org/html/2411.08467v1,"Inflationary constraints on the moduli-dependent species scale
in modular invariant theories","We demonstrate that a broad class of modular inflation models predicts the emergence of new physics within an energy range of approximately 10^{15}\,\mathrm{GeV} to 10^{17}\,\mathrm{GeV}. This prediction arises by comparing the moduli-dependent species scale with observational constraints on inflation. Specifically, we illustrate this within the context of SL(2,\mathbb{Z})-modular inflation models by re-expressing inflationary observables in terms of the species scale. We further discuss the implications of this approach for generic Calabi-Yau threefolds, showing that this reformulation allows us to directly constrain the fundamental parameters related to the geometry of extra dimensions, specifically the second Chern numbers.","A large N number of light degrees of freedom in a quantum theory of gravity modifies the scale at which quantum effects of gravity become relevant. Such an effective ultraviolet cutoff in theories of quantum gravity is called the species scale Dvali:2007hz ; Dvali:2007wp ; Dvali:2008ec , which in four dimensions is given by \displaystyle\Lambda_{\rm sp}=\frac{M_{\rm Pl}}{N^{1/2}}. (1) Since a large number of species leads to a decrease in \Lambda_{\rm sp}, the quantum effects of gravity become relevant below the Planck scale. The species scale depends on the light modes \tau, which in string compactifications correspond to moduli fields exhibiting geometric symmetries of compact extra-dimensional spaces. It was shown in Refs. vandeHeisteeg:2022btw ; vandeHeisteeg:2023ubh ; Castellano:2023aum ; vandeHeisteeg:2023dlw that the moduli-dependent species scale is constrained to be an automorphic form of the duality symmetries, including the modular symmetries of the theory, as calculated in Refs. Green:1999pu ; Green:1999pv ; Green:2005ba ; Green:2010kv ; Green:2010wi . In type II compactifications on Calabi-Yau (CY) threefolds, the species scale can be identified with the genus-one topological free energy F_{1}, as proposed in Ref. vandeHeisteeg:2022btw , i.e., N\simeq F_{1}, which is described by a specific modular function, as explicitly demonstrated in the Enriques CY (K3\times T^{2})/\mathbb{Z}_{2} Ferrara:1995yx . The decrease of the species scale impacts particle phenomenology and cosmology. Indeed, the decay rate of the tower of states |\partial_{\phi}\Lambda_{\rm sp}/\Lambda_{\rm sp}| is found to have a lower bound vandeHeisteeg:2023ubh ; Calderon-Infante:2023ler ; Castellano:2023stg ; Castellano:2023jjt and an upper bound of order {\cal O}(1) vandeHeisteeg:2023ubh ; Calderon-Infante:2023ler ; vandeHeisteeg:2023dlw ; Lust:2023zql , which permits only a finite range for the light mode in gravitational effective field theories. By imposing this finite range on the inflaton field during an accelerated expansion of the universe, it has been shown that the inflaton field range is bounded by the tensor-to-scalar ratio Cribiori:2023sch ; Scalisi:2019gfv ; vandeHeisteeg:2023uxj ; Scalisi:2024jhq . Furthermore, the coefficient of the {\cal R}^{2} term becomes a duality-invariant form, and the Starobinsky inflation model has been revisited from the perspective of string theory Lust:2023zql . In this paper, we focus on the SL(2,\mathbb{Z}) modular symmetry, which appears in four-dimensional effective field theories (EFT) on toroidal compact spaces and in the asymptotic limits of CY moduli spaces Ishiguro:2024xph . By imposing SL(2,\mathbb{Z}) modular invariance on the theory, one can achieve a unique pattern of flavor structure Feruglio:2017spp and a successful inflation mechanism Kobayashi:2016mzg ; Schimmrigk:2016bde ; Abe:2023ylh ; Ding:2024neh ; King:2024ssx ; Casas:2024jbw ; Kallosh:2024ymt , where the scalar potential is expected to be stable against higher-order corrections, in contrast to the Starobinsky model. Remarkably, several cosmological observables, such as the tensor-to-scalar ratio, are determined by the modular-invariant species scale Casas:2024jbw , indicating that recent cosmological observations place a bound on the species scale itself as well as its decay rate. Unlike the decay rate of the species, a phenomenologically viable range of the species scale has not yet been fully explored. The purpose of this Letter is to evaluate the species scale directly in modular-invariant theories by utilizing recent cosmological observations. We find that the species scale is restricted to the range 10^{15}\,\text{GeV}\lesssim\Lambda_{\rm sp}\lesssim 10^{17}\,\text{GeV} when the spectral index and tensor-to-scalar ratio are required to match observational data."
https://arxiv.org/html/2411.08114v1,Black-hole evaporation for cosmological observers,"In the present work, evaporation of a black hole immersed in a de Sitter environment is considered. Vaidya-de Sitter spacetime is used to model the process in a scenario of accelerated expansion of the Universe. The role of observers is highlighted in the development and Hayward thermodynamics for non stationary geometries is employed in the description of the compact objects. The results of the proposed dynamical model are compared with the usual description based on stationary geometries, focusing on primordial black holes (PBHs). It is found how the timescale of evaporation depends on the choice of a cosmological observer. It may differ substantially from the treatment based on stationary models for black holes. In particular, the standard assertion that there is a fixed initial mass just below 10^{15}\,\text{g}\sim 10^{-18}M_{\odot} for the PBHs which are ending their evaporation process today is imprecise, even when possible quantum corrections at the late stages are not considered. Deviations from this prediction appear when the evaporation is measured with respect to the cosmological time.","The formation of compact objects in the primitive Universe has been studied extensively Ambartsumian ; Ambartsumyan-Saakyan ; Novikov1 ; Novikov2 ; Hawking:1971 , leading to the possible existence of primordial black holes (PBHs) carr-spectrum ; Novikov3 . Nowadays, black holes have turned into directly observable objects, as shown by the Event Horizon Telescope collaboration Akiyama:2019bqs , and a new observational window for the early Universe has opened. If PBHs exist, mergers of them would be a possible source of gravitational waves as it was suggested by Nakamura, Thorne et al. Nakamura_1997 or, most recently, by Raidal et al. Raidal_2017 and Sasaki et al. sasaki2018primordial . In addition, one of the possible scenarios considers that the LIGO detection GW150914 Abbott:2016blz corresponds to a merger of PBHs of a binary system Sasaki:2016 . In fact, the idea that PBHs may account for all or even a portion of the dark matter in the Universe is under debate, with new suggestions for PBH mass spectra being discussed carr-silk . More recently, it was also proposed that the excess of microlensing events in the 5-year Optical Gravitational Lensing Experiment data and the anomalous orbits of trans-Newtonian objects could be interpreted as a new population of dark objects (predicted to be PBHs) Scholtz:2019csj . Moreover, new observations of unusually massive galaxies in early epochs of the Universe (z\gtrsim 10) by the James Webb Space Telescope were made Yan:2022sxd ; Labbe:2022 ; Finkelstein:2022 ; Atek:2023 . They could be explained by PBHs liu2022accelerating ; Yuan:2023bvh , increasing the interest in black holes within cosmological settings. Fundamentally, black holes are dynamical objects, either due to the accretion of matter content that crosses their event horizon or because of the semiclassical phenomenon of Hawking radiation. Thus, black holes are actually never stationary, although they are frequently modeled by stationary geometries if their mass variation is small. In addition, the black-hole background can be dynamical. For instance, considering a PBH formed during the radiation dominated era, it will evolve during the expansion of the Universe, eventually reaching the period when the cosmological constant dominates saida2007black . Nevertheless, despite their dynamic evolution, the evaporation of PBHs is typically treated using models based on the Schwarzschild geometry. These static treatments predict a PBH lifetime of the order of carr2021constraints ; calmet2014quantum \frac{G^{2}M_{0}^{3}}{\hbar c^{4}}\sim 10^{64}\bigg{(}\frac{M_{0}}{M_{\odot}}% \bigg{)}^{3}\ \text{yr}\,, (1) where M_{0} is the initial mass of the PBH. More precisely, Eq. (1) refers to the proper time measured by a static observer at (asymptotically flat) infinity. This model results in the important prediction that PBHs originated with an initial mass smaller than 10^{15}g would have already evaporated completely by now. However, due to the accelerated rate of expansion of the spacetime, deviations from this prediction are expected. In the present work, we are interested in the Vaidya-de Sitter spacetime, derived by R. Mallet in 1985 mallett1985radiating . As the Vaidya spacetime generalizes Schwarzschild for a black hole with accretion or emission of radiation, the Vaidya-de Sitter spacetime is an analogous generalization of Schwarzschild-de Sitter. It describes a black hole emitting (or accreting) photons immersed in a universe dominated by a positive cosmological constant. We will employ the Vaidya-de Sitter solution to model Hawking radiation, following an approach used in hiscock1981models ; hiscock1981modelsII discussing Vaidya geometry in the context of black-hole evaporation. Underlying this proposal is the fact that the dynamics of a PBH should assume neither a static nor a pure de Sitter description of the Universe. These two scenarios are considered to be the extremes of a spectrum of possible PBH models. To see how the usual Eq. (1) might fail, we propose to explore the other side of the spectrum, where the expansion rate is maximized. Deviations between the models can be expected to become more prominent in the later future, when the de Sitter description should be more appropriate. The Vaidya-de Sitter geometry has the drawback of having its simplest form in Eddington-Finkelstein-like coordinates berezin2017vaidya . In an asymptotically flat spacetime, the advanced and retarded time coordinates may coincide with the proper time of an observer infinitely distant, for instance, in Schwarazschild and Vaidya. But, in a cosmological background, those time coordinates have no direct cosmological meaning. In order to get around this problem, we link Eddington-Finkelstein-like coordinates in Vaidya-de Sitter to the time measured by cosmological observers comoving with the natural expansion of the Universe. These will replace the usual static observers in the Schwarzschild model. Based on the Vaidya-de Sitter spacetime, an evaporation model for black holes in a de Sitter background is constructed and the evolution of those compact objects are analyzed from the point of view of cosmological observers. The developed tools are applied to the estimation of PBH lifetimes. In the present work, an important tool is Hayward’s proposal for a black-hole thermodynamics. This development started with generalized laws for black hole mechanics hayward1994general , which were derived considering trapping horizons as the boundary of black holes. Subsequently, the proposal generalized the Bekenstein-Hawking thermodynamics for spherically symmetric dynamic spacetimes hayward1996gravitational ; hayward1998unified ; hayward2009local . Within this framework, dynamical spacetimes, such as Vaidya-de Sitter geometry, can be thermodynamically treated. We employ Hayward’s thermodynamics in the physical characterization of black-hole evaporation, including an analysis of the generated Hawking atmosphere. The structure of this paper is presented as follows. In section II, some main features of Vaidya-de Sitter spacetime are reviewed, focusing on the points that are relevant to the present work. In section III, a link between the cosmological time and Eddington-Finkelstein-like coordinates is established. A relativistic model for the black-hole evaporation in Vaidya-de Sitter is constructed in section IV. The family of “cosmological observers” introduced in the present work is discussed in section V, and in section VI the evaporation model is explored and compared to the usual static model applied to PBHs. In particular, it is shown how PBH evaporation timescales depend on the cosmological observer. Final comments are presented in section VII. Details of a relevant coordinate transformation are presented in Appendix A, and in Appendix B a thermodynamic analysis of the region outside the black hole is conducted. In this paper, we use signature (-,+,+,+)."
https://arxiv.org/html/2411.08103v1,The thermodynamic structure and large-scale structure filament in MACS J0717.5+3745,"We present the results of Chandra and XMM-Newton X-ray imaging and spatially-resolved spectroscopy, as well as new MUSTANG2 90 GHz observations of the thermal Sunyaev-Zeldovich from MACS J0717.5+3745, an intermediate redshift (z=0.5458) and exceptionally massive (3.5\pm 0.6\times 10^{15} M⊙) Frontier Fields cluster experiencing multiple mergers and hosting an apparent X-ray bright large scale structure filament. Thermodynamical maps are produced from Chandra, XMM-Newton, and ROSAT data using a new method for modelling the astrophysical and instrumental backgrounds. The temperature peak of 24\pm 4 keV is also the pressure peak of the cluster and closely correlates spatially with the Sunyaev-Zeldovich peaks from the MUSTANG2 data. The cluster center hosts shock fronts to the north and south, for which we report lower limits for the shock Mach numbers of \mathcal{M}=1.6\pm 0.4 and \mathcal{M}=1.9\pm 0.3, respectively. Bayesian X-ray Analysis methods were used to disentangle different projected spectral signatures for the filament structure, with Akaike and Bayes criteria being used to select the most appropriate model to describe the various temperature components. We report an X-ray filament temperature of 2.9_{-0.3}^{+0.5} keV and a density (1.60\pm 0.05)\times 10^{-4}\,{\rm cm^{-3}}, corresponding to an overdensity of 150 relative to the critical density of the Universe. We estimate the hot gas mass of the filament to be \sim 4.4\times 10^{12}\leavevmode\nobreak\ \rm M_{\odot}, while its total projected weak lensing measured mass is \sim 6.8\pm 2.7\times 10^{13}\leavevmode\nobreak\ \rm M_{\odot}, indicating a hot baryon fraction of 4–10%.","Current cosmological models and numerical simulations predict that the majority of the missing baryons in our Universe sit in the faint galaxy cluster outskirts and the interconnecting filaments of the cosmic web (Cen & Ostriker, 1999; Davé et al., 2001). This warm-hot intergalactic medium (WHIM) contributes to the growth of galaxy clusters through a slow constant accretion, as the in-falling filamentary gas virializes within the gravitational potential wells of massive clusters. Only a small number of X-ray bridges and filaments have been studied so far. This is partially because the diffuse WHIM remains a complicated challenge to detect with today’s instruments. Numerical cosmological simulations predict WHIM temperatures of 10^{5} to 10^{7} K and densities of 10-7 to 10-4 cm-3 (Cen & Ostriker, 2006; Haider et al., 2016); in other words, features in the WHIM are inherently characterized by soft X-ray emission, low surface brightness, and poor signal to noise ratios due to dominant contributions to the signal from both astrophysical and instrumental backgrounds. Needless to say, much care needs to be taken when modelling the spectra of these complex regions to account for all of the different contributions from other emission signatures. Current observational information about filaments is derived from either absorption or emission. Observations in emission have so far been limited to a small number of systems, MACS J0717.5+3745 (Ebeling et al., 2004), Abell 399/401 (Sakelliou & Ponman, 2004), Abell 222/223 (Werner et al., 2008), Abell 2811 (+offset)/2804/2801 (Sato et al., 2010), Abell 3558/3556 (Mitsuishi et al., 2012), Abell 2744 (Eckert et al., 2015a), Abell 3391/3395 (Alvarez et al., 2018; Reiprich et al., 2021; Veronica et al., 2022, 2024), Abell 2029/2033 (Mirakhor et al., 2022), and Abell 3667/3651 (Dietl et al., 2024). Temperature measurements were obtained only for four of these systems. For two of the four, it has been reported that the filament emission is dominated by the emission of the Intracluster Medium (ICM), as evidenced by the excessively large temperatures. There is a debate about the origin of these X-ray filaments. While some of the detected bridges could indeed be classified as WHIM filaments, others are due to other physical processes, such as shock-related compression heating between the two merging atmospheres or emission of ram-pressure stripped tails from infalling haloes. One of the most extensively studied merging galaxy clusters in almost every available wavelength is MACS J0717.5+3745 (RA 07h17m32.1, DEC +37∘45’21”), an extremely massive, intermediate redshift (z=0.5458), Hubble Frontier Fields Cluster (Ebeling et al., 2001). MACS J0717.5+3745 is also one of the most complex galaxy clusters to date, having hosted at least four different sub-cluster collisions. The collective mass from the many ingested dark matter halos makes this a great target for gravitational lensing studies and additional exploration of local substructures via the Sunyaev-Zeldovich effect due to the high temperatures. The complex mergers have also resulted in a complex ICM morphology, characterized by perturbed plasma discontinuities, stripped atmospheres, shock fronts, and cold fronts, and results in many peculiar features in the X-ray morphology and radio surface brightness features. Besides the gravitational lensing study, Jauzac et al. (2018), also looked at many of these X-ray substructures in detail. Most intriguing is the prominent X-ray bridge located in the S-SE of the cluster, which has previously been studied by van Weeren et al. (2016, 2017) using Chandra data. This paper aims to investigate the X-ray bridge in detail, statistically disentangling the diverse distribution of complex emission signatures associated with the filament using Bayesian X-ray Analysis (BXA) methods and nested models, along with a new dynamically-adaptive method for the full instrumental background modelling of each region for both Chandra and XMM-Newton. This paper also presents new thermodynamical maps using a joint modelling method combining available data from Chandra, XMM-Newton, and ROSAT, as well as trend-modeled residual maps where the average cluster thermodynamical properties over radial distances are removed. This paper additionally presents previously unpublished data from 90 GHz MUSTANG2 observations of the Sunyaev-Zeldovich effect in the direction of MACS J0717.5+3745. In Section 2, we explain the methods used in the data reduction, followed by Sec. 3 that explains the instrumental and X-ray background model and analysis of both Chandra and XMM-Newton. Section 4 then describes the main results of the thermodynamic maps and the properties of the filament, while Section 5 discusses these results. Throughout the paper, we assume the standard \Lambda cold dark matter cosmology with \Omega_{m}=0.286, \Omega_{\Lambda}=0.714, and H_{0}=69.6. Consequently, at z=0.5458, 1 arcmin corresponds to 387.42 kpc. The wilms abundance table is adopted for all plasma emission and photoelectric absorption models in the discussed spectral models (Wilms et al., 2000). Unless stated otherwise, the error bars correspond to a 68% confidence interval."
https://arxiv.org/html/2411.08079v1,Application of Machine Learning Methods for Detecting Atypical Structures in Astronomical Maps,The paper explores the use of various machine learning methods to search for heterogeneous or atypical structures on astronomical maps. The study was conducted on the maps of the cosmic microwave background radiation from the Planck mission obtained at various frequencies. The algorithm used found a number of atypical anomalous structures in the actual maps of the Planck mission. This paper details the machine learning model used and the algorithm for detecting anomalous structures. A map of the position of such objects has been compiled. The results were compared with known astrophysical processes or objects. Future research involves expanding the dataset and applying various algorithms to improve the detection and classification of outliers.,"Significant progress in astronomy has been made in recent years due to the introduction of machine learning methods for analyzing observational data. The enormous amount of data generated by telescopes and other instruments complicates the manual search and identification of both interesting and unusual structures. Machine learning algorithms can effectively analyze large datasets in high-dimensional parameter spaces and identify patterns that would be difficult for humans to detect 1 . The Planck Space Observatory, launched by the European Space Agency and NASA in 2009, was designed to study the cosmic microwave background (CMB) radiation, the oldest light in the universe. CMB maps contain a wealth of valuable information about the early universe and its large-scale structure, making the mission extremely important for cosmology. The mission aimed to map the Big Bang’s CMB with improved sensitivity and resolution and to test theories about the universe’s origin and evolution. However, extracting and analyzing this information is an extremely difficult task. The task is to identify structures on the raw CMB maps that deviate from the expected pattern in the standard cosmological model 4 . It is necessary to develop and implement an approach that allows to identify various heterogeneous structures from space maps. We called them foreground outliers because these structures can be caused by various astrophysical phenomena, such as galaxy clusters, supernovae, or other celestial objects, whose intensity exceeds that of the CMB. Examples of such objects include galaxies or galaxy clusters, bright stars, supernovae, dust contaminations, and other celestial objects. The study of foreground outliers on CMB maps is challenging due to their volume and complexity, requiring the development of methods and technologies to automate and enhance their detection and subsequent analysis. This makes it difficult to find them all manually. To address this issue, it is proposed to use machine learning methods, particularly neural networks and clustering algorithms. Neural networks are intended to extract features from CMB maps. Feature extraction involves identifying and extracting important features or patterns in a dataset relevant to the problem. In this case, neural networks will be trained to identify patterns on CMB maps indicating the presence of foreground outliers. Clustering algorithms will be used to group similar structures in the feature space created by the neural network. These methods can enhance the efficiency and accuracy of foreground outliers detection on CMB maps. In machine learning, foreground outliers detection can be classified as the task of detecting anomalous images in a sample consisting of a set of images that belong to some general population. Anomaly detection in arbitrary data can be classified as a binary or multi-cluster clustering task, depending on the purpose (detection or classification). Anomalies are data that do not conform to the established concept of normal behavior. It is important to distinguish anomalies from noise in the data. So, we define foreground outliers as statistically rare patterns on the map regardless of their origin. This means that as a result, among foreground outliers we can observe both point-like objects of known or unknown astrophysical origin, and might be CMB regions that do not correspond to the Lambda-CDM model. Anomaly detection is characterized by unique features that do not allow typical clustering approaches to be applied without modification. These features depend on the task statement and the subject area, while the data structure also has an impact. An overview of the main methods for anomaly detection is provided in 6 . The main features of the anomaly detection task include: 1. Class imbalance, where anomalous objects constitute a small fraction of the total data (usually less than 1%). 2. The potential absence of anomalies in the training sample, while their appearance in real data necessitates effective model detection. 3. Difficulties in defining a universal measure of similarity between data samples. 4. The challenge of distinguishing noise from anomalies in data, leading to poor anomaly detection quality in noisy datasets. In this paper, the problem of detecting anomalies in images is considered. There are several methods for solving this problem. The work 7 provides an overview of the main anomaly detection methods. Here are some of them: 1. Statistical methods. One approach involves constructing a sample distribution function and identifying points that fall outside this distribution. 2. Feature extraction-based methods. Machine learning algorithms, such as neural networks, can identify patterns in data 8 , 9 . By analyzing these patterns, indicators of anomalies can be found. This approach is particularly effective for high-dimensional data. 3. Reconstruction-based methods. Autoencoders can be used to reconstruct images, and reconstruction errors can indicate anomalies. 4. Clustering-based methods. Clustering algorithms can group similar data, with any image that does not fit the clusters being considered an anomaly. The choice of method depends on the specific task and characteristics of the data, so first you need to analyze the available data."
https://arxiv.org/html/2411.07995v1,"Large Field Polynomial Inflation in Palatini f(R,\phi) Gravity","In this paper, we employ the Palatini formalism to investigate the dynamics of large-field inflation using a renormalizable polynomial inflaton potential in the context of f(R,\phi) gravity. Assuming instant reheating, we make a comparative analysis of large-field polynomial inflation (PI). We first consider the minimal and non-minimal coupling of inflaton in R gravity, and then we continue with the minimally and non-minimally coupled inflaton in f(R,\phi) gravity. We scan the parameter space for the inflationary predictions (n_{s} and r) consistent with the Planck and BICEP/Keck 2018 results as well as the sensitivity forecast of the future CMB-S4 and depict the compliant regions in the \phi_{0}-\beta plane where \phi_{0} and \beta are two parameters of polynomial inflation model which control the saddle point of the potential and the flatness in the vicinity of this point respectively. We find that a substantial portion of the parameter space aligns with the observational data.","Cosmic inflation posits that the universe underwent an extremely rapid exponential expansion in its very early stages, right after the Big Bang, during the first tiny fraction of a second [1, 2, 3, 4]. This period of inflation helps explain the uniformity of the Cosmic Microwave Background (CMB) radiation and the universe’s large-scale structure [5, 6, 7]. There is a significant body of literature on inflationary cosmology [8, 9]. Amongst the plethora of inflationary models, the single-field models are the simplest ones, where a single scalar field slowly rolls down a sufficiently flat potential profile [10]. Those profiles are generically monomials of the inflaton with a power of two or four, so they are bounded from below and renormalizable. The problem with these monomial potentials is they are sufficiently flat only at fairly large field values. This results in the overproduction of tensor modes and consequently in a large tensor-to-scalar ratio r which measures the strength of the tensor perturbations (gravitational waves) relative to the scalar perturbations (density fluctuations). Thus, those models are ruled out after the release of Planck and BICEP/Keck 2018 results [11, 12]. The next simplest model involves a single real scalar field again. However, it has a renormalizable potential: a fourth-degree polynomial instead of a monomial. This model is dubbed the polynomial inflation (PI) [13, 14, 15, 16, 17]. It is similar to the inflection point inflation scenario [18, 19, 20, 21, 22, 23, 24]. In standard inflection point inflation, the inflaton potential has an inflection point, a point where the second derivative of the potential changes sign, but the first derivative remains small, and near this point, the potential can be approximated by a cubic function. Similarly, in PI, the polynomial potential has a saddle point, where both the first and second derivatives of the potential vanish. In practice, the location of this (would-be) saddle point is used as a free parameter, and a second free parameter controls the slope of the potential profile to modulate the flatness of the inflaton excursion zone. PI has been the subject of many analyses since the ’90s because it is possible to encounter this type of potential in both minimal supersymmetric standard model (MSSM) and string theory [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]. PI can bear fruit in string theory in the sense that, in the limit where inflation occurs at field values that are very far away from the saddle point the potential approximates a purely quartic one. This in turn leads to eternal inflation which helps justify the weak anthropic principle and relax the initial conditions problem of inflation itself [36, 37, 38]. PI has been analyzed for both small [39] and large field values [40, 41] considering the latest measurements and utilizing the metric formalism. Unlike the analyses conducted in these papers, we work out polynomial inflation in Palatini formalism in different cosmological settings like minimal and non-minimal inflaton coupling to standard metric-Palatini R gravity and metric-Palatini f(R,\phi) gravity. The paper is structured as follows. Section 2 provides a brief overview of the difference between metric and Palatini formulation of f(R,\phi) concerning inflation. Section 3 discusses the fundamentals of polynomial inflation in four different cosmological settings: minimal and non-minimal coupling of inflaton in standard metric-Palatini R gravity (standard Einstein theory of gravity) or f(R,\phi) metric-Palatini gravity. Section 4 analyzes the inflationary dynamics and compares the model’s predictions with observational data. Section 5 concludes with a summary and discussion of the implications and potential future research directions."
https://arxiv.org/html/2411.07970v2,MUltiplexed Survey Telescope: Perspectives for Large-Scale Structure Cosmology in the Era of Stage-V Spectroscopic Survey,"The MUltiplexed Survey Telescope (MUST) is a 6.5-meter telescope under development. Dedicated to highly-multiplexed, wide-field spectroscopic surveys, MUST observes over 20,000 targets simultaneously using 6.2-mm pitch positioning robots within a \sim 5\,{\rm deg}^{2} field of view. MUST aims to carry out the first Stage-V spectroscopic survey in the 2030s to map the 3D Universe with over 100 million galaxies and quasars, spanning from the nearby Universe to redshift z\sim 5.5, corresponding to around 1 billion years after the Big Bang. To cover this extensive redshift range, we present an initial conceptual target selection algorithm for different types of galaxies, from local bright galaxies, luminous red galaxies, and emission line galaxies to high-redshift (2<z<5.5) Lyman-break galaxies. Using Fisher forecasts, we demonstrate that MUST can address fundamental questions in cosmology, including the nature of dark energy, test of gravity theories, and investigations into primordial physics. This is the first paper in the series of science white papers for MUST, with subsequent developments focusing on additional scientific cases such as galaxy and quasar evolution, Milky Way physics, and dynamic phenomena in the time-domain Universe.","Over the past four decades, beginning with the “Stick Man” from the CfA Redshift Survey in 1982 [1, 2], spectroscopic mapping of large-scale structures (LSS) has accumulated more than 30 million redshifts of nearby and distant galaxies (see Figure 2). This monumental achievement has contributed significantly to the establishment of the current cosmological model \LambdaCDM, along with other cosmological probes, i.e., cosmic microwave background (CMB; [3, 4]), Type-Ia supernovae [5, 6, 7], or measurements of weak lensing (e.g., [8, 9]). For two decades (2000–2020), major experiments such as the Sloan Digital Sky Survey (SDSS; [10]), followed by the ongoing (2021–2026) survey from the Dark Energy Spectroscopic Instrument111https://www.desi.lbl.gov/the-desi-survey/ (DESI; [11]), have mapped the 3D universe at low and intermediate redshift (z\lesssim 3). Clustering measurements from spectroscopic surveys of galaxies and quasars have become a key probe of cosmology. They provide precise measurements on the baryon acoustic oscillations (BAO) scale [12] and the linear growth rate of structure f\sigma_{8} through redshift space distortions (RSD) [13]. Recent results from the DESI collaboration [14] suggest a potential deviation from the cosmological constant to time-varying dark energy. By the end of this decade, we expect to have sub-percent-level constraints on dark energy and gravity from galaxy clustering up to redshift z\sim 2. When the DESI project wraps up, we have finished the spectroscopic survey component of the four stages envisioned by the Dark Energy Task Force (DETF) report [15]. Yet, many fundamental questions remain unanswered, calling for a new era of cosmological experiments in the 2030s. Going one step further, 3D maps of the universe at high redshift (z>2) will enable the observation of linear modes in the primordial universe, significantly enhancing our ability to constrain dark energy and inflation [16]. In the next decade, a series of ground- and space-based photometric surveys CSST [17], Euclid [18], Nancy Grace Roman Space Telescope [19], LSST [20] will provide deep and high-quality images for future galaxy spectroscopic surveys allowing to target galaxies such as Lyman Break Galaxy (LBG) or Lyman-\alpha emitter (LAE) [21, 22] as tracer of matter at high redshift 2<z<5. Large-volume high-redshift redshift surveys using these new tracers have the unprecedented potential to help us test primordial non-Gaussianity, probe dynamic dark energy, and reveal possible new features in the primordial power spectrum, uncovering tantalizing hints for new physics. At the same time, a high-density spectroscopic survey of low-redshift (z<1.5) can provide a high-fidelity 3D map of the cosmic web and trace the matter distributions into the non-linear regimes, opening doors to unexplored scientific opportunities. Such a multi-purpose dataset can also enhance the scientific performance of other cosmological probes, such as calibrating the photometric redshift and intrinsic alignment models for weak gravitational lensing surveys (e.g., [23]; [24, 25]), providing spectroscopic follow-up and host galaxy properties for supernova surveys (e.g., [26, 27]), or exploring new approaches to map the low-redshift large-scale structures (LSS) such as a peculiar velocities survey (e.g., [28, 29]). More importantly, it will help maximize the potential for synergies between spectroscopic surveys and other cosmological probes, such as CMB (e.g., [30]), weak lensing (e.g., [31, 32, 33, 34]), and intensity mapping (IM, e.g., [35]) experiments. Motivated by these two promising directions, the cosmological & high-energy physics community has recently coined the concept for a Stage-V spectroscopic experiment (e.g., [36, 37]) to fulfill these high expectations. By definition, a Stage-V spectroscopic facility should utilize a telescope with high etendue value (A\Omega; A and \Omega are the collecting area and the field-of-view of the telescope) to ensure a high survey speed. More importantly, the facility should have significantly improved multiplexed capability (number of targets that can be observed simultaneously) compared to the Stage-IV survey (e.g., 5,000 fibers for DESI). Conceptually, a Stage-V facility demands a minimum of 10,000 fibers that could be easily reconfigured to target different objects. This is the primary technical challenge now for such an ambitious vision. At the same time, a Stage-V facility should also have excellent optical performance, high-performance multi-object spectrographs that at least cover the whole optical wavelength range, and a site with good observing conditions. Building on these requirements, multiple ground-based concepts have been proposed, including the 6.5 m MegaMapper telescope [38, 39], the dual-6 m & dual-hemisphere Spec-S5 project222https://www.spec-s5.org/ [40], the 11 m Maunakea Spectroscopic Explorer333https://mse.cfht.hawaii.edu/ (MSE; [41]), the 12 m Wide-field Spectroscopic Telescope444https://www.wstelescope.com/ (WST; [42]), and the \sim12 m Extremely Large Spectroscopic Survey Telescope (ESST; [43]). The MUltiplexed Survey Telescope555https://must.astro.tsinghua.edu.cn/en (MUST) is a 6.5-meter telescope [44] under active development. MUST will be located at the 4358 m Peak A of Saishiteng Mountain in Qinghai, China. Equipped with over 20,000 fibers over a \sim 5~{}\rm{deg}^{2} field of view (FoV), it features three-channel spectrographs covering wavelengths from 370-960 nm, with spectral resolution between R=2,000 and 4,500. MUST is designed to conduct an ambitious Stage-V cosmological spectroscopic survey, aiming to precisely measure key cosmological parameters and improve our understanding of dark energy and cosmic evolution. With the first light scheduled for 2031, MUST expect to conduct the first Stage-V spectroscopic survey, targeting Lyman Break Galaxy (LBG) and Lyman-\alpha emitter (LAE) at high redshift across \sim 13,000 deg2 of the northern sky. Clustering analysis of these tracers will provide sub-percent precision measurements on BAO parameters – D_{A}(z)/r_{d} and H(z)r_{d} – and the linear growth rate of structure, f\sigma_{8}, at redshift 2<z<5, a region not yet covered by current spectroscopic galaxy surveys. Additionally, MUST will provide stringent constraints on primordial non-Gaussianities (PNG, local type) through the parameter f_{\mathrm{NL}}^{\mathrm{local}} with a precision of \sigma(f_{\mathrm{NL}}^{\mathrm{local}})\sim 1. This will enable stringent testing of a wide range of inflationary models. MUST is expected to provide sufficient precision (\sim 0.03\,{\rm eV} when combined with CMB data) on the sum of neutrino masses to constrain a nonzero neutrino mass with \sim 2\sigma significance, assuming normal hierarchy. The inverted mass hierarchy can be tested with a significance \sim 1.3\sigma. Finally, from power spectrum measurements of the Lyman-\alpha forest, MUST will yield the most precise constraint on warm dark matter mass to date m_{X}>10.5 keV at 95% confidence level (assuming 14,000 deg2 and k_{\rm max}=0.67 h Mpc-1). Besides the unique potential of the MUST project, synergies with other cosmological surveys, i.e., future imaging surveys (e.g. CSST, Euclid, LSST), CMB experiments (e.g., Simon Observatory [45], CMB-S4 [46], LiteBird [47]) or radio surveys (e.g., SKAO [48]) will enhance the constraining power of MUST and will result in a better understanding of our Universe. As a dedicated spectroscopic survey facility, MUST can also carry out spectroscopic surveys supporting a wide range of scientific topics outside of LSS cosmology, such as the study of galaxy evolution, super-massive black hole (SMBH), the structure of the Milky Way, and time-domain astrophysics. This paper describes the MUST instrument and the scientific objectives of the cosmological survey that will be conducted over 5 years of observation. Section 2 provides an overview of the MUST project, including the current status of the whole project, the design of the telescope (Section 2.1), the focal plane system & the spectrograph (Section 2.2), the site & observing condition (Section 2.3), and the overall scientific capabilities (Section 2.4). Section 3 describes the key scientific motivations of MUST for the Stage-V cosmological surveys. The primary scientific goals are covered in detail while briefly summarizing the potential for new probes and the potential synergies with other cosmological surveys. Section 4 presents the current target selection strategy and provides the redshift distribution and target density estimations for the cosmological forecast. We will also introduce the conceptual survey design for MUST. Finally, Section 5 describes the method to forecast the cosmological potential of MUST theoretically and summarizes the forecasted results on dark energy, structure growth, primordial non-Gaussianity, neutrino mass, and warm dark matter constraints. Discussions and main conclusions of this project and future directions are described in Section 6. Throughout this work, we adopt as fiducial baseline \LambdaCDM cosmology with parameters H_{0}=67.6 km s-1 Mpc-1, \Omega_{b}=0.046 and \Omega_{m}=0.31. All magnitudes in this work are defined in the AB magnitude system [49]. Figure 1: Overview of the MUST project. The top right panel shows the location of the MUST site currently selected in Qinghai province of China. The top left panel is a picture of the Saishiteng mountain near Lenghu. We highlight the peaks that have been or are being developed for astronomy. The highest peak – Peak A – with an altitude of 4358 m, was selected as the site for MUST. From left to right, the bottom panels illustrate the preliminary design of the dome of MUST and the telescope, a sketch of the conceptual design of the focal plane of MUST, fiber, and spectrograph systems, and the fiducial design of the modular focal plane of MUST. By current design, MUST will host 21,168 robotic fiber positioners using 336 triangular modules."
https://arxiv.org/html/2411.07735v1,Detecting ultralight axions from multifrequency observations of neutral hydrogen intensity mapping,"We investigate the effects of ultralight axions (ULAs) on the differential brightness temperature fluctuations of neutral hydrogen at the post-reionization stage. Unique structure suppression features under the influence of ULAs are studied from angular correlations of the neutral hydrogen signals observed at different frequencies. Moreover, ULAs can behave like dark energy or dark matter at different mass regimes, implying that the fraction of the ULA energy density would be correlated with the sum of neutrino masses and the parameters of the dark energy equation of state for dark matter and dark energy like ULAs, respectively. We have explored the parameter space for the ULA physics and possible degeneracies among dark energy, neutrinos, and ULAs using the theoretical angular correlation functions expected from future intensity mapping surveys.","Gravitational effects of dark matter (DM) have been detected from observations at different wavelengths but its microscale physics remains unknown. It is still unclear whether dark matter particles are made of cold dark matter (CDM), neutrinos, or other exotic particles, such as axions which are hypothetical particles introduced to solve the strong CP problem in particle physics [1, 2] and are believed to be so light that the de Broglie wavelength can be as long as the astrophysical scale. One interesting example is a particular mass around 10^{-22} eV, which leads to a halo scale wavelength and can form condensates to solve the dark matter halo cuspy problem. This is the so-called —“fuzzy dark matter (FDM)” scenario [3]. However, the large-scale structure is more influenced by the much longer wavelengths of the ultralight axions (ULAs) which have typical masses of approximately 10^{-33} eV (corresponding to the Hubble scale H_{0} today) to 10^{-22} eV(corresponding to the fuzzy dark matter regime today) [4]. On the other hand, neutrinos behave like CDM at large scales but cannot cluster within free streaming scales [5, 6]. Therefore, in neutrino scenarios, there is mass-dependent structure suppression of matter fluctuations due to massive neutrinos. Theoretical calculations show that neutrinos can introduce approximately 6% suppression on small scales [7]. The axions are expected to be frozen at early times, behaving like dark energy (DE) with a constant equation of state; at later times, they are expected to begin oscillations so their averaged equation of state is zero, behaving like dark matter [8]. Therefore, the axion mechanism can describe both the dark sectors [9], corresponding to axion masses at m_{a}>10^{-27} eV and m_{a}<10^{-27} eV, respectively [9]. Different from neutrinos, the free streaming scales of axions are extremely large, but the axion condensates have a finite sound speed that has a critical scale, beyond which there is also structure suppression, comparable with the neutrino free streaming scales [10]. The axions are not produced thermally and move at non-relativistic speeds despite the low masses. Therefore, the axions do not cluster below a certain length scale due to the wave effects. Because of these features, there might be interesting degeneracies among CDM, neutrino, dark energy, and axion signatures according to cosmological and astrophysical observations. A strong degeneracy between the dark energy equation of state and the sum of neutrino masses has been unveiled from observations [11]. Axion signatures exist at different levels of fluctuation correlation functions, mainly two-point correlation functions, or angular power spectra [12]. Other statistical quantities such as the peak counts [13] and higher-order correlations [14, 15] might also be nontrivially affected by ULAs. The ULA signatures were searched for from different observations including the cosmic microwave background (CMB), galaxy redshift surveys, and Lyman-\alpha. Different datasets such as the WMAP, SDSS, and Lyman-\alpha datasets were analyzed to place constraints on the ULA models [9, 16, 17, 18, 19]. Furthermore, the axion energy density was constrained to less than 1% of the critical energy density [16] within 10^{-32} eV to 10^{-26} eV using both the CMB temperature and polarization power spectrum measurements from Planck [20], Atacama cosmology telescope [21] and south pole telescope [22] in conjunction with the matter power spectrum from the WiggleZ Dark Energy Survey [23]. The CMB power spectra are mostly affected by modifications of the global evolution history caused by the entire energy density fraction contributed by ULAs. The matter power spectra are more sensitive to ULA masses and other microscopic properties. The CMB lensing power spectrum was first adopted to constrain the ULA models in the following analysis [24]. Although no evidence of ULA signatures has been detected, the CMB lensing power spectra from the next-generation will improve the ULA constraining power by a factor of a few, compared to the primary fluctuations. Recent analysis also demonstrated that ULA, as a subcomponent of DM, can offer a possible solution to alleviate the \sigma_{8} tension [15]. Axions also have profound small-scale implications and can leave unique suppression imprints on cosmic shear measurements of weak lensing in galaxies. In such a small-scale regime, the fuzzy dark matter models might be more dominant, and the FDM constraints were obtained from the cosmic shear measurements of the Dark Energy Survey [25]. Moreover, as perturbations, axions can not only affect large-scale structures but also collapse and form bound objects such axion halos [26], axion minclusters [27] and boson stars [28, 29]. Future high-resolution intensity mapping (IM) experiments may be able to resolve these sources, so we can study them with better sensitivity. The 21 cm line is an ideal tracer of large-scale structures in both spatial and temporal coordinates at much smaller scales than the CMB. Tomographic intensity mapping of the neutral hydrogen (HI) can thus yield interesting constraints on the ULA models, forming a sensitive probe for ULA physics. Recent forecasts have shown that the axion energy density fraction can be constrained to a sub-percent level by future IM surveys [30]. In this work, we study the ULA signatures of the HI power spectra measured from future IM surveys at different wavelengths and further investigate the possible origins of ULA-DE and ULA-neutrino degeneracies."
https://arxiv.org/html/2411.07703v1,Some Times in Standard Cosmology,"The standard cosmological model is sufficiently well constrained that precise estimates can be provided for the redshift of various physically defined times in the chronology of the Universe. For example, it is well known that matter-radiation equality, recombination and reionisation happen at redshifts of around 3000, 1000 and 10, respectively, and these can be specified more precisely by fitting to data. What is less well known are the times in years (and their uncertainties) for these and other epochs in the history of the Universe. Here we provide precise time determinations for six epochs in cosmological history within the standard model, using data from the Planck satellite. Our main results are illustrated in a figure.","The current standard cosmological model, \Lambda\mathrm{CDM}, is defined by seven parameters within a set of simplifying assumption (summarised in table 1) [1, 2]. Anisotropies in the cosmic microwave background (CMB) provide the tightest constraints on most of these parameters, through their angular power spectra. Likelihood functions derived from the power spectra, fitted through Markov chain Monte Carlo (MCMC) techniques, provide sets of samples whose posterior distributions give central values and uncertainties on parameters. The CMB temperature T_{0} is distinct from the others, being so well determined that it can be considered to be fixed [3]. This leaves us with a 6-dimensional parameter space to determine the best fit to the CMB power spectra. In addition to the usual six parameters, it is also possible to calculate the constraints on any combination of those parameters, or indeed any function of them (see e.g. Ref. [4] for more examples). Among the parameters typically discussed are the epochs at which specific physical things happen in the history of the Universe, usually given in terms of the scale factor a or redshift z. The average non-cosmologist is likely to expect to hear of epochs defined in actual time units, but such values do not occur in most parameter tables. Here we calculate the time in years for several key epochs in cosmic history. Before starting, it is worth remembering that “cosmic time” can be well defined as the time coordinate measured by an observer moving in the Hubble flow (i.e. with no peculiar velocity), assuming a homogeneous background. Hence we can talk meaningfully about time to many digits of precision. Any epoch can be defined through the use of the scale factor a(t), or equivalently a redshift z(t) defined through 1+z(t)=\frac{1}{a(t)}. (1.1) Within the standard cosmology, the time corresponding to a given scale factor can be found through an integral. If we start with t=\int_{0}^{t}\differential{t}=\int_{0}^{a}\frac{\differential{a}}{\dot{a}}=% \int_{0}^{a}\frac{\differential{a}}{aH(a)}, (1.2) then we can use the Friedmann equation in the form H^{2}(a)=H_{0}^{2}\left\{\Omega_{\Lambda}+\Omega_{\mathrm{m}}a^{-3}+\Omega_{% \mathrm{r}}a^{-4}\right\} (1.3) to perform the integral. Here H\equiv\dot{a}/a is the Hubble parameter, quantifying the rate of expansion, and H_{0} is its value today. For models with curvature, things are a little more complicated, but we will stick with spatially flat models here (i.e. the curvature density parameter is \Omega_{K}=0). We are also assuming that the dark energy is a pure cosmological constant \Lambda (i.e. the equation of state parameter is w=-1). The present-day radiation density parameter \Omega_{\mathrm{r,0}} for relativistic species can be computed from the energy densities \rho for photons \gamma and neutrinos \nu, together with the monopole temperature T_{0} of the CMB and the effective number of neutrinos N_{\mathrm{eff}}: \displaystyle\Omega_{\mathrm{r,0}} \displaystyle=\frac{\rho_{\mathrm{r,0}}}{\rho_{\mathrm{crit}}}=\frac{8\pi G}{3% H_{0}^{2}}\left(\rho_{\gamma,0}+\rho_{\nu,0}\right) \displaystyle\quad\text{with}\quad\rho_{\gamma,0} \displaystyle=\frac{a_{\mathrm{B}}}{c^{2}}T_{0}^{4}, (1.4) \displaystyle\quad\text{and}\quad\frac{\rho_{\nu,0}}{\rho_{\gamma,0}} \displaystyle=\frac{7}{8}N_{\mathrm{eff}}\left(\frac{4}{11}\right)^{4/3}, and where G is Newton’s gravitational constant and a_{\mathrm{B}} is the radiation constant of the Stefan–Boltzmann law.111Related to the Stefan–Boltzmann constant \sigma=c\,a_{\mathrm{B}}/4. Because neutrino decoupling is not instantaneous (as well as some other minor corrections) we set the effective number of neutrinos to N_{\mathrm{eff}}=3.0440 when there are 3 neutrino species [5].222Strictly speaking, only relativistic neutrinos should enter into eq. 1.4, which we neglect here. As such the radiation density today contributes about \Omega_{\mathrm{r,0}}h^{2}\simeq$4\text{\times}{10}^{-5}$. In general the time integral from eq. 1.2 has to be performed numerically, but there are analytic solutions for 2-fluid simplifications, which we will use in sections 3.1, 3.2 and 3.5. The full numerical integration is already part of so-called Boltzmann solvers that calculate the angular power spectra for perturbations, such as the codes CAMB [6, 7] and CLASS [8, 9, 10, 11]. Hence we can use modified output from such codes to calculate times corresponding to different redshifts. The differences between the codes are negligible compared to the statistical uncertainties [10]. Description Parameter Value Baryon density today \Omega_{\mathrm{b,0}}h^{2} 0.02225{}\pm{} 0.00013 Cold dark matter density today \Omega_{\mathrm{c,0}}h^{2} 0.1190{}\pm{} 0.0011 Acoustic angular scale at last scattering (approximation) 100\,\theta_{\mathrm{MC}} 1.04085{}\pm{} 0.00026 Thomson scattering optical depth due to reionisation \tau_{\mathrm{reio}} 0.059{}\pm{} 0.006 Log-amplitude of primordial density perturbations \ln(10^{10}A_{\mathrm{s}}) 3.045{}\pm{} 0.012 Scalar spectral index of primordial density perturbations n_{\mathrm{s}} 0.968{}\pm{} 0.004 CMB monopole temperature today T_{0} 2.7255\text{\,}\mathrm{K} Curvature density parameter today (assuming flatness) \Omega_{K} 0 Sum of neutrino masses assuming 1 massive neutrino \sum m_{\nu} 0.06\text{\,}\mathrm{eV} Effective number of massive and massless neutrinos N_{\mathrm{eff}} 3.0440 Equation of state parameter for dark energy w -1 Table 1: The six cosmological sampling parameters (upper block; showing sample mean and standard deviation), and some fixed parameters (lower block) from the MCMC run with the Planck likelihoods lowlTT+lowlEE+highlTTTEEE+lensing (see section 2 for technical details). In the rest of this paper we will consider several specific epochs in the history of the Universe within the context of the standard \LambdaCDM cosmology. We will specifically focus on the time of: matter-radiation equality; recombination; baryon-temperature decoupling; reionisation; the transition from deceleration to acceleration; and the present day."
https://arxiv.org/html/2411.07647v1,"Primordial black holes 
and induced gravitational waves from logarithmic non-Gaussianity","We investigate the formation of primordial black hole (PBH) based on numerical relativity simulations and peak theory as well as the corresponding scalar induced gravitational wave (SIGW) signals in the presence of logarithmic non-Gaussianities which has recently been confirmed in a wide class of inflation models. Through numerical calculations, we find certain parameter spaces of the critical thresholds for the type A PBH formation and reveal a maximum critical threshold value. We also find that there is a region where no PBH is produced from type II fluctuations contrary to a previous study. We then confirm that SIGW signals originated from the logarithmic non-Gaussianity are detectable in the Laser Interferometer Space Antenna if PBHs account for whole dark matter. Finally, we discuss the SIGW interpretation of the nHz stochastic gravitational wave background reported by the recent pulsar timing array observations. We find that PBH overproduction is a serious problem for most of the parameter space, while this tension might still be alleviated in the non-perturbative regime.","\Acp PBH are hypothetical compact objects that might have been formed in the early universe [1, 2]. Although various formation scenarios have been discussed (e.g., the formation in the matter-dominated universe [3], the formation from the isocurvature perturbation [4, 5], and the formation from a resonant instability of cosmological perturbation during a preheating [6, 7]), the widely discussed one is the gravitational collapse of the highly dense regions in the radiation-dominated universe. \AcpPBH can be formed in a wide range of masses unlike astrophysical black holes, and they can provide a sizable amount of dark matter (DM). The mass range between [10^{-15}M_{\odot}\text{--}10^{-11}M_{\odot}] is often called the PBH Dark Matter window because it can account for all the DM according to the current observational constraints [8, 9, 10]. \AcpPBH are not only promising DM candidates but can also potentially explain other cosmological and astrophysical phenomena. For instance, they could constitute the seeds for the supermassive black holes in galactic nuclei or galaxies [11], be relevant sources of gravitational wave (GW) events in ground-based detectors [12, 13, 14, 15] and in pulsar timing array (PTA) observations [16, 17, 18, 19], have a role on explaining baryogenesis [20, 21, 22], and even stand as the exotic object present in the solar system [23, 24]. In addition to these motivations, recently, more precise studies focusing on the relationship between GWs and PBHs have been actively discussed (see, e.g., Ref. [25]). The large primordial perturbations necessary for PBH formation can induce GWs through the second-order interactions between the scalar and tensor metric perturbations. The frequency of GWs is related to the PBH mass, as both of them depend on the size of the Hubble horizon at the reentry. In particular, our main interest relies on the mass band of the PBH mass window corresponding to the frequency band of space-borne interferometers, such as Laser Interferometer Space Antenna (LISA) [26], Decihertz Gravitational Wave Observatory (DECIGO) [27], Taiji [28], and TianQin [29]. In fact, it was pointed out that LISA can detect the SIGW signal when the PBHs in the mass range of the DM window occupy the entire DM [30, 31]. Interestingly, the detectability of milihertz SIGW is robust against non-Gaussianity, which has also been widely studied [30, 32, 33, 34, 35]. On the theoretical side, recently, the curvature perturbations \zeta having a non-Gaussian exponential tail in their probability density function have come to be discussed [36, 37, 38, 39]. One typical model that realizes such a non-Gaussian curvature perturbation is the ultra slow-roll inflation. The primordial curvature perturbation produced during the ultra slow-roll phase could have sufficiently large amplitudes for PBH formations and have an exponential-tail distribution which asymptotically follows P(\zeta)\propto\exp(-3\zeta) in the large \zeta limit. It is explained by the logarithmic relation \zeta=-(1/3)\ln{(1-3\zeta_{g})} between the curvature perturbation \zeta and a certain Gaussian random field \zeta_{g}. A more general form of a logarithmic type curvature perturbation can be expressed as \zeta=-(1/\gamma)\ln{(1-\gamma\zeta_{g})} with a constant parameter \gamma. We call this the logarithmic non-Gaussianity relation. This logarithmic non-Gaussianity can be found in a class of the constant roll inflation model which is the generalized model of the ultra slow-roll inflation [40, 41, 42, 43, 44], as well as in the curvaton scenario [45]. In the constant roll scenario, the primordial scalar power spectrum can be blue-tilted and it could realize the efficient PBH formation [46, 47, 48, 49, 50, 51, 52]. The probability density function of the logarithmic type curvature perturbations has the exponential tail P(\zeta)\propto\exp(-\gamma\zeta) for \gamma>0, and follows the Gumbel tail as P(\zeta)\propto\exp\left(-\frac{1}{2\gamma^{2}\sigma^{2}}\mathrm{e}^{-2\gamma% \zeta}\right) with the variance \sigma^{2}=\braket{\zeta^{2}_{g}} for -3/2<\gamma<0 [42, 44]. Although the influence of the exponential tail in the ultra slow-roll model (i.e., \gamma=3) on the PBH formation and the corresponding SIGW signals have been studied in Refs. [53, 34], the PBH formation and the related SIGW signals in general parameter space are unknown. Additionally, as PBHs may be overproduced when trying to explain the nHz SGWB observed by PTAs as induced by the curvature perturbation [16], a negative non-Gaussianity is implied [54]. This also motivates us to study the PBH formation in the logarithmic non-Gaussianity with \gamma<0. In this work, we investigate the PBH formation when there is logarithmic non-Gaussianity, based on the numerical relativity and peak theory. The corresponding SIGW spectrum is also calculated analytically. This paper is organized as follows. We describe the process of PBH formation including the considered peak profile, threshold estimation, and computation of abundances in Sec. 2. Next, in Sec. 3, we briefly review the perturbative formula for the SIGW with the primordial scalar non-Gaussianities based on Ref. [34] and provide the predicted signal in relation to current and future experiments. We investigate the detectability for the SIGW with the logarithmic non-Gaussianity in light of PBH DM scenario by the LISA. We also perform the parameter estimation for SIGW in light of the logarithmic non-Gaussianity based on the recent PTA results on the stochastic gravitational wave background, and discuss the compatibility of the abundance of PBHs. Finally, we provide our conclusions in Sec. 4."
https://arxiv.org/html/2411.07509v1,A halo model approach for mock catalogs of time-variable strong gravitational lenses,"Time delays in both galaxy- and cluster-scale strong gravitational lenses have recently attracted a lot of attention in the context of the Hubble tension. Future wide-field cadenced surveys, such as the Legacy Survey of Space and Time (LSST), are anticipated to discover strong lenses across various scales. We generate mock catalogs of strongly lensed quasars (QSOs) and supernovae (SNe) on galaxy-, group-, and cluster-scales based on a halo model that incorporates dark matter halos, galaxies, and subhalos. For the upcoming LSST survey, we predict that approximately 3500 lensed QSOs and 200 lensed SNe with resolved multiple images will be discovered. Among these, about 80 lensed QSOs and 10 lensed SNe will have maximum image separations larger than 10~{}\mathrm{arcsec}, which roughly correspond to cluster-scale strong lensing. We find that adopting the Chabrier stellar initial mass function (IMF) instead of the fiducial Salpeter IMF reduces the predicted number of strong lenses approximately by half, while the distributions of lens and source redshifts and image separations are not significantly changed. In addition to mock catalogs of multiple-image lens systems, we create mock catalogs of highly magnified systems, including both multiple-image and single-image systems. We find that such highly magnified systems are typically produced by massive galaxies, but non-negligible fraction of them are located in the outskirt of galaxy groups and clusters. Furthermore, we compare subsamples of our mock catalogs with lensed QSO samples constructed from the Sloan Digital Sky Survey and Gaia to find that our mock catalogs with the fiducial Salpeter IMF reproduce the observation quite well. In contrast, our mock catalogs with the Chabrier IMF predict a significantly smaller number of lensed QSOs compared with observations, which adds evidence that the stellar IMF of massive galaxies is Salpeter-like. Our python code to generate the mock catalogs, Strong Lensing Halo model-based mock catalogs (SL-Hammocks), as well as mock catalogs of lensed QSOs sn SNe are made available online.","In recent years, the field of cosmology has been confronted with a significant challenge known as the Hubble tension problem (see, e.g., Di Valentino et al., 2021, for a review). The Hubble tension is an empirical problem in which two types of observations have conflicting measurements of the Hubble constant H_{0}. In the first approach, H_{0} is estimated from the early-universe measurements, in particular cosmic microwave background (CMB), assuming the so-called standard cosmology, \Lambda-dominated cold dark matter (\LambdaCDM) model. For example, Planck Collaboration (2020) provides a tight constraint of H_{0}=67.4\pm 0.5\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1} assuming the flat \LambdaCDM model. The other approach measures H_{0} more directly from observations of the local Universe. Recently, Supernovae and H_{0} for the Equation of State of dark energy (SH0ES) collaboration has obtained H_{0}=73.04\pm 1.04\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1} by type Ia supernovae (SNe Ia) calibrated via the distance ladder in the local Universe (Riess et al., 2022). The two types of observations exhibit inconsistency with significant 5\sigma tension of the value of H_{0}, which may pose a problem for the validity of the \LambdaCDM model. One of the important avenues for resolving this problem is the estimation of the Hubble constant with other independent methodologies. The Tip of the Red Giant Branch (TRGB), a waypoint in the evolutionary state for giant stars, offers another way to determine H_{0} as a local standard candle. Some work (see e.g., Freedman et al., 2020) shows a consistent result with both to 1.5\sigma or less of H_{0}\sim 70~{}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}, while others (see e.g., Anand et al., 2020; Blakeslee et al., 2021; Jones et al., 2022) yield somewhat higher value at H_{0}\sim 71.5-73~{}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}. Recently, Scolnic et al. (2023) measure H_{0}=73.22\pm 2.06\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1} using TRGB, which further clarifies the tension, by analyzing in conjunction with the Pantheon+ SN Ia sample while considering a correction for differences in SN surveys and local flows. Various other methods have also been used to measure H_{0}, including Megamasers (Kuo et al., 2013; Reid et al., 2013; Kuo et al., 2015; Reid et al., 2019), gravitational waves (Abbott et al., 2017; Mooley et al., 2018; Hotokezaka et al., 2019), fast radio bursts (Hagstotz et al., 2022; Wu et al., 2022; James et al., 2022; Liu et al., 2023a; Zhao et al., 2022), baryon acoustic oscillations (Addison et al., 2018; Benisty and Staicova, 2021), Type II SNe (de Jaeger et al., 2022), ages of old astrophysical objects (Vagnozzi et al., 2022; Wei and Melia, 2022), and others (Moresco et al., 2022). However, a satisfactory answer to this tension has not been found yet. Another independent approach for H_{0} measurements is to focus on time delays between multiple images of strong gravitational lenses. It was pointed out in the 1960s that observations of time delays between multiple images of gravitationally lensed SNe can be used to estimate H_{0} independently of the distance ladder (Refsdal, 1964). However, it is only recently that we have been able to measure the Hubble constant accurately enough to use it to potentially resolve the Hubble tension. Due to the scarcity of lensed SNe, lensed quasars (QSOs) have been mostly used to constrain the Hubble constant. For example, in Wong et al. (2020), the H_{0} Lenses in COSMOGRAIL’s Wellspring (H0LiCOW) collaboration estimated H_{0} as H_{0}=73.3_{-1.8}^{+1.7}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1} based on analysis of the time delays of six lensed QSOs with the simple mass distribution of lensing galaxies such as an elliptical power-law or stars plus standard dark matter halos. In Birrer et al. (2020), the time-delay cosmography (TDCOSMO) team obtained a constraint of H_{0}=74.5_{-6.1}^{+5.6}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}, by considering a more flexible radial mass distribution and breaking the degeneracy in mass models with stellar kinematics. While galaxy-scale lensed QSOs are used in the H0LiCOW and TDCOSMO analyses, recently, in Kelly et al. (2023), SN Refsdal, the first multiply-lensed SN with time delay measurements, is finally used to obtain the constraint on the Hubble constant of H_{0}=64.8_{-4.3}^{+4.4}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1} (see also Liu and Oguri, 2024, for the detailed analysis of the mass model dependence). In addition to the difference in the source population, SN Refsdal was strongly lensed by a massive cluster of galaxies rather than a single galaxy and, hence, is highly complementary to the constraints from H0LiCOW and TDCOSMO. Besides, another measurement of H_{0} from the gravitationally lensed SN H0pe due to a massive cluster of galaxies, H_{0}=75.4_{-5.5}^{+8.1}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}, was reported very recently (Pascale et al., 2024). The measurement of H_{0} using time-delay lensings will advance significantly in the coming decade thanks to several large-scale galaxy survey projects, including Euclid and Rubin Observatory Legacy Survey of Space and Time (LSST). Oguri and Marshall (2010, OM10 hereafter) was among the first to make a comprehensive prediction of event rates of galaxy-scale lensed SNe and QSOs, such as those used in the H0LiCOW and TDCOSMOS analysis, in various wide-field cadenced imaging surveys including LSST. According to OM10, the LSST should find more than 8000 galaxy-scale lensed QSOs, some 3000 of which will have well-measured time delays. The LSST should also find some 130 lensed SNe during the 10-year survey duration. OM10 also computed the available precision on the Hubble constant from the LSST survey and obtained \sigma(h)=0.017 as the predicted marginalized 68 percent confidence intervals, where h\equiv H_{0}/100~{}\mathrm{km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}. We note that there are some minor updates in the calculation of OM10, as presented in Oguri (2018); Lemon et al. (2023); Shen et al. (2023). Henceforth, we refer to the updated OM10 calculation as OM10+. Strong gravitational lenses are predominantly produced by single massive galaxies. This is one of the reasons why the analysis of TDCOSMO and H0LiCOW, as well as OM10, have focused on galaxy-scale strong lenses. However, several cluster-scale lensed QSO systems have already been found, which recently attracts increasing attention due to the Hubble tension problem. Denzel et al. (2021) used the cluster-scale lensed QSO SDSS J1004+4112 (Inada et al., 2003; Oguri et al., 2004; Inada et al., 2005; Fohlmeister et al., 2008) together with 7 galaxy-scale lensed QSOs to obtain H_{0}=71.8_{-3.3}^{+3.9}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}. Napier et al. (2023) focused more specifically on cluster-scale lensed QSOs by studying three cluster-lensed QSOs SDSS J1004+4112, SDSS J1029+2623 (Inada et al., 2006; Oguri et al., 2008a; Fohlmeister et al., 2013), and SDSS J2222+2745 (Dahle et al., 2013, 2015), to produce a combined measurement of the Hubble constant as H_{0}=71.5\pm 6.1\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}. Very recently, SDSS J1004+4112 has been reanalyzed by Martínez-Arrizabalaga et al. (2023) using a free-form lens model and by Liu et al. (2023b) using 16 different lens models, with results of H_{0}=74_{-13}^{+9}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1} and H_{0}=67.5_{-8.9}^{+14.5}\mathrm{~{}km}\mathrm{~{}s}^{-1}\mathrm{Mpc}^{-1}, respectively. In this work, we present detailed predictions of the numbers of gravitationally lensed QSOs and SNe produced for a wide mass range of lens objects, including both galaxy- and cluster-scale lenses, with a particular focus on the LSST. Specifically, we adopt the so-called halo model (see e.g., Asgari et al., 2023, for a review) approach adopting a compound lens model that consists of the dark matter halo and galaxy (stellar mass) components. This approach differs from the one in OM10 and OM10+ that focuses only on galaxy-scale strong lenses. They assume the singular isothermal ellipsoid (SIE) model and calculate the lensing properties from the line-of-sight projected stellar velocity dispersion of lens galaxies obtained from the distributions of the velocity dispersions, the so-called velocity function, empirically derived in the local universe by the Sloan Digital Sky Survey (SDSS). While the SIMCT pipeline presented in More et al. (2016) can also generate mock lenses at galaxy- as well as group- and cluster-scales, their approach differs from the halo model approach in this paper in that they use a hybrid approach of injecting mock lensed sources in foreground deflectors selected from a real or mock survey. In this work, equipped with the halo model, we generate mock catalogs of lensed QSOs and SNe, especially focusing on time delay and highly magnified systems, which would be observed in LSST’s 10-year long observing run. Thanks to the halo model approach, our mock catalogs cover a wide image separation range. We then discuss the properties of lens objects and images for the mocks while comparing the mock catalogs with those of OM10+. Our mock catalog generation code, Strong Lensing Halo model-based mock catalogs (SL-Hammocks), will be made publicly available on GitHub. Although we here only focus on the LSST survey, the SL-Hammocks code can be applied to arbitrary ongoing and future time-domain optical imaging surveys. Our work and the SL-Hammocks code will be helpful for future astrophysical and cosmological applications of lensed QSO and SN systems that have been/will be discovered. In addition to mock catalogs of multiply lens systems as those presented in OM10+, we construct mock catalogs of highly magnified systems, for which both single and multiple image systems are included. Since the contribution from group-scale halos is expected to be important for predicting the abundance of highly magnified systems (Keeton et al., 2005), we expect that our halo model approach enables a reliable prediction on the abundance and properties of such systems. The mock catalogs of highly magnified systems will be useful for e.g., assessing the magnification bias on observations of high-redshift QSOs and SNe in the LSST survey. This paper is organized as follows. We describe our halo model-based deflector model in Sec. 2. We check the validity of our model to populate galaxies in halos by comparing various observables predicted by our model with observational data in Sec. 3. We then generate mock catalogs for lensed QSOs and SNe with our deflector model. The results of our mock catalogs are presented and analyzed in Sec. 4. We compare our mock catalog with known lensed QSO samples in Sec. 5. We summarize the main conclusion in Sec. 6. Throughout this paper, we assume a flat \LambdaCDM cosmology and fix the cosmological parameters to the Planck 2018 best-fits (Planck Collaboration, 2020), specifically h=0.677, \Omega_{\mathrm{b}}h^{2}=0.0224,\omega_{\mathrm{cdm}}=0.120,\ln\left(10^{10}% \mathcal{A}_{\zeta}\right)=3.05, and n_{\mathrm{s}}=0.967."
https://arxiv.org/html/2411.07469v1,Primordial Black Hole Reformation in the Early Universe,"Primordial black holes (PBH) can arise in a wide range of scenarios, from inflation to first-order phase transitions. Light PBHs, such as those produced during preheating or at the GUT scale, could induce an early matter-dominated phase given a moderate initial abundance. During the early matter-domination, the growth of initial PBH density perturbations can trigger collapse on horizon scales, producing much heavier PBHs. While the remaining original PBHs evaporate and reheat the Universe, these massive reformed PBHs survive for an extended period of time, producing potentially observable signatures at the present. We study this PBH reformation scenario and show that those reformed PBHs can emit significant quantities of gamma rays detectable by the next generation of experiments. The rapid reheating after matter domination generates a coincident stochastic gravitational wave background, which could be within range of the upcoming CMB-S4 experiment. The PBH reformation scenario provides an intriguing possibility of decoupling the current PBH population and the initial formation mechanism from early Universe physics, while providing opportunities for observation through multi-messenger astronomy.","Primordial black holes (PBHs) can be produced in a variety of new physics scenarios, ranging from the collapse of perturbations seeded by inflationary fluctuations Carr and Hawking (1974); Carr (1975); Garcia-Bellido et al. (1996); Yokoyama (1997); Garcia-Bellido and Ruiz Morales (2017); Musco et al. (2021) to first-order phase transitions Hawking et al. (1982); Moss (1994); Jung and Okui (2021); Hong et al. (2020); Kawana and Xie (2022); Lu et al. (2022); Kawana et al. (2022); Lu et al. (2023a, b); Marfatia et al. (2024); Liu et al. (2022); Kawana et al. (2023) and many others Hawking (1989); Cotner et al. (2018); Conzinu et al. (2020); Helfer et al. (2017); Ruffini and Bonazzola (1969). The conceivable mass range of PBHs span more than 40 orders of magnitude, from the Planck mass to supermassive black holes. Of particular interest are light PBHs with masses between 5\times 10^{14}g<M_{\rm PBH}\lesssim 10^{17}g, which can emit copious amounts of high energy Hawking radiation Hawking (1974, 1976); Page (1976). Consequently, this parameter space has been constrained by a variety of experiments Carr et al. (2010, 2016); Boudaud and Cirelli (2019); Huang and Zhou (2024); Kim (2021); Laha et al. (2021) searching for this emission. The next generation of observatories Addazi et al. (2022); Acharya et al. (2018); Albert et al. (2019) will further improve the sensitivity to high energy \gamma and cosmic rays, enabling them detect a small but significant population of evaporating PBHs. PBHs with masses below 5\times 10^{14}g are usually considered to have completely evaporated by the present day Page (1976); MacGibbon and Webber (1990) and could be detectable by their traces on the Cosmic Microwave Background (CMB) and nuclide abundances from Big Bang Nucleosynthesis (BBN) Carr et al. (2010); Acharya and Khatri (2020); Chluba et al. (2020). These bounds extend down to M\gtrsim 10^{9}g, below which the PBH lifetime is short enough to evaporate before the epoch of BBN \sim 1s. Recently, a series of papers Inomata et al. (2020); Papanikolaou et al. (2021); Domènech et al. (2021a, b) have shown that extremely light PBHs could usher in an era of early matter-domination (eMD) and source a large gravitational wave (GW) background when they evaporate, during the rapid transition from matter-domination to radiation-domination (RD). Although generally undetectable by present-day GW experiments, these GWs would constitute an excess contribution to the radiation energy density. As a result, \Delta N_{\rm eff} limits derived from CMB and BBN observations constrain the abundance of PBHs down to M\gtrsim 10^{-2}g Domènech et al. (2021a, b). In this paper, we consider PBH reformation during and after the eMD era. Shortly after matter-radiation equality, the overdensities associated with the PBHs grow and could become substantial before they evaporate. During the eMD phase, the probability of PBH reformation is enhanced, following a power law Khlopov and Polnarev (1980); Polnarev and Khlopov (1981); Harada et al. (2016, 2017); Kokubu et al. (2018) instead of the usual exponential dependence Carr (1975); Young et al. (2014). This enables the possibility that collections of PBHs may collapse and form much larger PBHs. A similar concept was explored in Ref. De Luca et al. (2023), where they discussed mergers of solar mass PBHs into supermassive black hole seeds, enhanced by strong clustering. In contrast, we focus on very light PBHs that would otherwise evaporate, and could initiate an eMD era in the early Universe, with collapse of a PBH gas rather than individual mergers as the formation mechanism. These reformed PBHs would be much lighter, actively emitting Hawking radiation which could be detected along with the coincident GW signal. We start by reviewing the relevant quantities and scales for PBH formation and the onset of an eMD era in Sec. II. The power spectrum and its growth is discussed in Sec. III which leads to the PBH reformation during eMD in Sec. IV. The accompanying GW background is discussed in Sec. V. Then in Sec. VI, we analyze PBH reformation after the reheating, introducing clustering effects during the initial formation. Finally, we conclude in Sec. VII."
https://arxiv.org/html/2411.07301v1,The impact of the cosmological constant on past and future star formation,"We present an extended analytic model for cosmic star formation, with the aim of investigating the impact of cosmological parameters on the star formation history within the \LambdaCDM paradigm. Constructing an ensemble of flat \LambdaCDM models where the cosmological constant varies between \Lambda=0 and 10^{5} times the observed value, \Lambda_{\rm obs}, we find that the fraction of cosmic baryons that are converted into stars over the entire history of the universe peaks at \sim 27% for 0.01\lesssim\Lambda/\Lambda_{\rm obs}\lesssim 1. We explain, from first principles, that the decline of this asymptotic star-formation efficiency for lower and higher values of \Lambda is driven respectively by the astrophysics of star formation, and by the suppression of cosmic structure formation. However, the asymptotic efficiency declines slowly as \Lambda increases, falling below 5% only for \Lambda>100\,\Lambda_{\rm obs}. Making the minimal assumption that the probability of generating observers is proportional to this efficiency, and following Weinberg in adopting a flat prior on \Lambda, the median posterior value of \Lambda is 539\,\Lambda_{\rm obs}. Furthermore, the probability of observing \Lambda\leq\Lambda_{\rm obs} is only 0.5\%. Although this work has not considered recollapsing models with \Lambda<0, the indication is thus that \Lambda_{\rm obs} appears to be unreasonably small compared to the predictions of the simplest multiverse ensemble. This poses a challenge for anthropic reasoning as a viable explanation for cosmic coincidences and the apparent fine-tuning of the universe: either the approach is invalid, or more parameters than \Lambda alone must vary within the ensemble.","Galaxies are the most obvious tracers of large-scale structure in the universe, and understanding how they emerge and evolve throughout cosmic history has been a longstanding goal of cosmology and astrophysics. The \LambdaCDM paradigm approaches this via the gravitational collapse of dark matter haloes from an initial quasi-homogeneous universe. This process is well understood, thanks to early analytical models of halo assembly (Lacey & Cole, 1993), which were subsequently validated numerically with full N-body simulations (Springel et al., 2005; Klypin et al., 2011; Angulo et al., 2012; Fosalba et al., 2015). However, many of the detailed baryonic processes that govern the build up of galaxies, such as gas accretion, star formation and outflows, are not yet fully understood. A successful theory of galaxy formation needs to account for these processes in order to predict the main observed properties of galaxies. In particular, reproducing the observed efficiency of star formation, both locally within individual galaxies and globally over a cosmologically representative volume, constitutes a crucial test for any model of galaxy formation (see the review by Madau & Dickinson, 2014). In this paper, we consider predictions for this efficiency, and how it depends on cosmological parameters, specifically the cosmological constant. Early fully analytic models of star formation were based on simple prescriptions for the cooling time for the gas within galaxies and the typical time scale for converting it into stars (e.g. Hernquist & Springel, 2003), while following the growth of structures with well established analytical forms for the halo mass function (Press & Schechter, 1974; Sheth & Tormen, 1999, 2002). However, these insightful models neglected more sophisticated mechanisms such as feedback processes driven by stars or active galactic nuclei (AGN). With a slightly different approach, White & Frenk (1991) implemented the effect of sub-galactic physics through a series of approximate formulae, while keeping the treatment of structure formation nearly analytical. This seminal work set the stage for a sequence of refinements of the modelling (e.g. Kauffmann et al., 1993; Cole et al., 1994; Guiderdoni et al., 1998; Kauffmann et al., 1999; Cole et al., 2000). Further extensions included the assembly of the central black hole, which enabled a description of the co-evolution of galaxies and quasars (e.g. Kauffmann & Haehnelt, 2000; Somerville et al., 2008; Henriques et al., 2015; Lacey et al., 2016). Other semi-analytical techniques followed the formation of dark matter haloes in full N-body simulations, coupled with analytical recipes for the baryonic physics (e.g. Croton et al., 2006; Henriques et al., 2020). Full hydrodynamical simulations incorporate several physical processes into the modelling and follow the evolution of baryons and dark matter from first principles. Several cosmological simulations (e.g. Schaye et al., 2010; Dubois et al., 2014; Hopkins et al., 2014; Vogelsberger et al., 2014; Lukić et al., 2015; Schaye et al., 2015; Davé et al., 2016; McCarthy et al., 2017; Pillepich et al., 2018; Davé et al., 2019), often based on different numerical approaches (Springel et al., 2005, 2001a; Almgren et al., 2013; Bryan et al., 2014; Hopkins, 2015; Springel et al., 2021), generally managed to reproduce a plethora of observations to a satisfactory level of accuracy. However, feedback processes are implemented via various different numerical ‘subgrid’ prescriptions, whose parameters are tuned to reproduce certain observations. It is therefore important to ask if the predictions of these calculations are robust, or whether they are fine-tuned to our universe and have a high sensitivity to model parameters. While a substantial body of literature has focused on the effect on star formation history of varying the subgrid parameters (see the review by Somerville & Davé, 2015), the impact of changing the cosmological parameters has historically received less attention. This may partially reflect the tremendous progress in constraining the parameters of the \LambdaCDM paradigm (e.g. Planck Collaboration et al. 2020; but see also Verde et al. 2019 for a discussion on recent tensions). But studying the predictions of hydrodynamical simulations regarding the star formation history in counter-factual cosmological models would constitute an interesting ‘stress-test’ of our understanding of galaxy formation. There is also a more fundamental reason to explore this question. Although the \LambdaCDM model is highly successful in many ways, it has theoretical issues that are hard to ignore (see e.g. the review by Bull et al., 2016). While the cosmological constant \Lambda is required to explain the accelerating expansion of the universe, there is no consensus on its physical meaning. If \Lambda is a manifestation of the energy of the quantum vacuum, then one can estimate its value by integrating over the zero-point energy of all possible modes. Adopting a cutoff energy E_{\rm v} yields a vacuum density of E_{\rm v}^{4} in natural units; but the observed \Lambda requires a cutoff at rather low energies, E_{\rm v}\sim 1\,{\rm meV}, in gross conflict with the scale of new physics at perhaps 10 TeV or above (Martin, 2012). This common calculation is in fact deeply flawed, as it is non-relativistic and yields the wrong vacuum equation of state (Koksma & Prokopec, 2011): a more sophisticated calculation yields a vacuum density of order M^{4}, in terms of the particle mass. But since particles exist with M near to the TeV scale (0.17 TeV for the top quark), the discrepancy in the estimated \Lambda is much the same as in the naive approach. For a more detailed discussion of this ‘cosmological constant problem’ see e.g.Weinberg (2000a) or Abel et al. (2002). Many attempts have been made to move beyond the idea of \Lambda representing a simple vacuum density, and to hypothesise some more general ‘dark energy’ contributing to an effective \Lambda that may vary with time. Ratra & Peebles (1988) proposed that a dynamical scalar field could cause the accelerating expansion of the universe. While the scalar field proposed by Ratra & Peebles (1988) was not motivated by fundamental physics, later models tried to connect it to extensions of the standard model of particle physics. In practice, the kinetic or potential terms of the Lagrangian of the scalar field depend on some fundamental mass scale (e.g. Zlatev et al., 1999; Armendariz-Picon et al., 2001). An alternative view is that the cosmic acceleration in fact shows the need for some modified theory of gravity (see e.g. Joyce et al., 2016). Other approaches include mechanisms that prevent vacuum energy from gravitating (Kaloper & Padilla, 2014). However, all these models effectively move the problem of the value of the \Lambda to the fine-tuning of some other parameter of the theory, such as the mass scale associated with the scalar field (see e.g. Amendola & Tsujikawa, 2010). There is also a more radical position asserting that the debate on the physical nature of the cosmological constant is moot, and that \Lambda is simply a fundamental constant emerging within the theory of General Relativity (Bianchi & Rovelli, 2010). Regardless of the physical interpretation of the cosmological constant, the oddly small non-null value of \Lambda gives rise to a number of coincidences that cry out for an explanation. Perhaps the most well-known of these is that the universe became \Lambda-dominated at z\approx 0.39, near to the formation time of the Sun. We therefore appear to live near the unique era when \Lambda transitions from being negligible to dominating the universe: this is the ‘why-now problem’ (Velten et al., 2014). A further time coincidence is that recombination occurred at the same time of baryon-radiation equality. In addition, Lombriser & Smer-Barreto (2017) pointed out that the equality between \Lambda and radiation occurs around the midpoint of cosmic reionisation. All these timescale puzzles are in principle distinct from the cosmological constant problem described earlier (but see also the discussion in Lombriser, 2023). A possible explanation for the ‘why now’ problem was suggested by Weinberg (1987). He noted that if the cosmological constant had been much larger than observed, the accelerating expansion of the universe would have set in at earlier times, freezing out the growth of structure before galaxy-scale haloes had been able to form. Thus the star formation in galaxies that is necessary for the creation of observers would not occur if \Lambda was substantially larger than the observed value. Such an argument is an example of anthropic reasoning (Carter, 1974), which effectively considers the existence of observers (such as ourselves) as a ‘data point’ and explores the implications for the cosmological parameters conditional on this information. To make this Bayesian argument (probability of \Lambda given that it is observed), we need there to be some physical mechanism that allows \Lambda to vary. It is also common to invoke a multiverse: an ensemble of different universes. The probability calculus is the same whether or not the members of the ensemble actually exist, or merely have the potential to do so. But the idea of a concrete multiverse underlying Weinberg’s argument was given stronger motivation by the theory of inflation. Here, a multiverse of causally disconnected ‘bubble universes’ arises in models of stochastic inflation where inflation proceeds eternally (Vilenkin, 1983; Linde, 1986; Guth, 2007; Freivogel, 2011). All causally disconnected bubbles evolve as independent universes, each characterised by a different set of constants, including \Lambda. The advantage of this picture is that for a sufficiently large ensemble there are guaranteed to exist universes suitable for the formation of structure. Thus, that would explain the existence of our universe, no matter how atypical it is within the ensemble. Anthropic arguments often encounter significant resistance, with many physicists arguing that efforts should be focused on finding solutions to cosmological puzzles from first principles (e.g. Kane et al., 2002). And of course such efforts should always be pursued. But we can note that anthropic approaches pervade other fields of astronomy, without generating controversy. A prime example is the concept of circumstellar ‘habitable zone’, which is defined based on the conditions that can sustain life on a planet (see e.g. Kasting et al., 1993). Of course, unlike with exoplanets, there is only one universe that can actually be observed (although see Aguirre & Kozaczuk, 2013; Wainwright et al., 2014; Johnson et al., 2016). As such, anthropic arguments cannot be tested in the Galilean sense ingrained in the scientific method. But they can still be tested, since we can in principle predict the probability distribution of observed values of cosmological parameters over the ensemble. This prediction can be compared with the single ‘data point’ of our universe, and a sufficiently large deviation from the average serves to rule out the model on which the prediction was based. After the initial formulation given by Weinberg (1987), investigations of the anthropic approach became progressively more refined. Weinberg (1989) extended his argument by noting that observer selection does not require \Lambda to be exactly zero, and thus a small non-zero (in principle even negative) value of \Lambda would be anthropically predicted. A critical element of this argument is the idea that the prior distribution of \Lambda should be flat because \Lambda=0 is not a special point, and therefore any continuous prior must be treatable as a constant in a narrow range near zero. Efstathiou (1995) revisited the argument by making more detailed estimates of the abundance of galaxies in universes with \Lambda\geq 0, subject to the constraint that the observed temperature of the Cosmic Microwave Background (CMB) is equal to 2.73\,\rm K. He concluded that anthropic arguments may explain a value of \Lambda close to the one that is observed. Later works allowed for more than one cosmological parameter to vary. Garriga et al. (1999) considered simultaneous variations of \Lambda and the density contrast at the time of recombination. Peacock (2007) explored anthropic arguments treating both the CMB temperature and \Lambda as free parameters, and further allowing \Lambda to assume negative values. Bousso & Leichenauer (2010) also considered the variation of multiple cosmological parameters, as well as negative values of \Lambda. In their work, the generation of observers is tied to the global efficiency of star formation in the different members of the multiverse ensemble. The star formation history is predicted with a semi-analytic model, under the assumption that the astrophysics of star formation does not vary throughout the multiverse (Bousso & Leichenauer, 2009). Other semi-analytic work by Sudoh et al. (2017) showed that incorporating the astrophysics of galaxy formation in anthropic reasoning can affect the range of the anthropically favoured values of \Lambda by almost an order of magnitude. More recently, progress in numerical power has enabled the testing of anthropic reasoning with full hydrodynamical simulations, exploring the simulated past and future star formation history for a wide range of \Lambda (Barnes et al. 2018; Salcido et al. 2018; Oh et al. 2022). While such simulations include several astrophysical processes (albeit in an approximate parameterised form), it is hard to probe a wide parameter space, or to explore the far future of the universe (beyond \sim 100 Gyr cosmic time), without a massive commitment of computational resources. Analytic models of star formation therefore represent an attractive complementary approach, as they are not subject to the same limitations as hydrodynamical simulations. While inevitably simplified in terms of astrophysics, they are an efficient technique that can offer a more intuitive picture of the evolution of star formation (e.g. Rasera & Teyssier, 2006; Davé et al., 2012; Sharma & Theuns, 2019; Salcido et al., 2020; Fukugita & Kawasaki, 2022). Recently, Sorini & Peacock (2021) generalised the Hernquist & Springel (2003) model of cosmic star formation such that it can be applied to arbitrarily large times. In this work, we further adapt the Sorini & Peacock (2021) formalism to explore the impact of different values of \Lambda on past and future star formation. The main result of our study will be that values of \Lambda in the range 0.01\lesssim\Lambda/\Lambda_{\rm obs}\lesssim\Lambda_{\rm obs} maximise the global efficiency of star formation. However, the interesting question is how effectively star formation is truncated by large values of \Lambda. If this suppression is not effective until \Lambda is vastly beyond the observed value, then Weinberg’s flat prior will mean that the observed universe risks being an implausibly rare outlier. The main aim of this paper is to quantify just how unusual our universe is in this respect. This manuscript is organised as follows. In § 2 we give an overview of the formalism in Sorini & Peacock (2021), and improve it by introducing extra features. In § 3 we explain how we adapt it to \LambdaCDM models with arbitrary non-negative values of \Lambda and discuss the impact of changing the cosmological constant on the halo mass function and on the efficiency of star formation within haloes. In § 4 we present our predictions for the long-term efficiency of star formation for different values of \Lambda. In § 5 we discuss the implications of our results for anthropic reasoning. We mainly focus on the cosmological constant problem, but also marginally consider the why-now problem. We also compare our results with previous literature on the subject and discuss the limitations of our model. Finally, in § 6 we summarise the main conclusions of our work and discuss the future developments of our line of research. Throughout this work, unless otherwise indicated, units of distance are understood to be proper units. Co-moving units are designated with a ‘c’ prefix (e.g. ckpc, cMpc)."
https://arxiv.org/html/2411.07289v1,Cosmography from accurate mass modeling of the lens group SDSS J0100+1818: five sources at three different redshifts,"Systems where multiple sources at different redshifts are strongly lensed by the same deflector allow one to directly investigate the evolution of the angular diameter distances as a function of redshift, and thus to learn about the geometry of the Universe. We present measurements of the values of the total matter density, \Omega_{\rm m}, and of the dark energy equation of state parameter, w, through a detailed strong lensing analysis of SDSS J0100+1818, a group-scale system at z=0.581 with five lensed sources, from z=1.698 to 4.95. We take advantage of new spectroscopic data from the Multi Unit Spectroscopic Explorer (MUSE) on the Very Large Telescope to securely measure the redshift of 65 sources, including the five multiply imaged background sources (lensed into a total of 18 multiple images) and 19 galaxies on the deflector plane, all employed to build robust strong lensing models with the software GLEE. The total mass distribution of the deflector is described in a relatively simple way, and includes an extended halo, the brightest group galaxy (BGG) with a measured stellar velocity dispersion of (380.5\pm 4.4)\,$\mathrm{k}\mathrm{m}\,\mathrm{s}^{-1}$, and fainter members. We measure \Omega_{\rm m}=0.14^{+0.16}_{-0.09} in a flat \Lambda cold dark matter (CDM) model, and \Omega_{\rm m}=0.19^{+0.17}_{-0.10} and w=-1.27_{-0.48}^{+0.43} in a flat wCDM model. Given the presence of different sources angularly close in projection, we quantify through a multi-plane approach their impact on the inferred values of the cosmological parameters. We obtain consistent median values, with uncertainties for only \Omega_{\rm m} increasing by approximately a factor of 1.5. Thanks to the remarkably wide radial interval where the multiple images are observed, ranging from 15 to 77 kpc from the BGG, we accurately measure the total mass profile and infer the stellar over total mass profile of the deflector. They result in a total mass of (1.55\pm 0.01)\times 10^{13} M⊙ within 50 kpc and a stellar over total mass profile decreasing from 45.6^{+8.7}_{-8.3}\% at the BGG effective radius to (6.6\pm 1.1)\% at R\approx 77 kpc. Our results confirm that SDSS J0100+1818 is one of the most massive (lens) galaxies known at intermediate redshift and one of the most distant candidate fossil systems. We also show that group-scale systems that act as lenses for \geq 3 background sources at different redshifts enable to estimate the values of the cosmological parameters \Omega_{\rm m} and w with an accuracy that is competitive with that obtained from lens galaxy clusters.","In the currently accepted \Lambda cold dark matter (CDM) scenario, the Universe is almost flat and expanding, and the expansion is accelerating (Riess et al. 1998; Perlmutter et al. 1999). The Universe is composed of baryons and dark matter (DM) for \approx 30\% (\Omega_{\rm m}\approx 0.3), and the cosmic acceleration is due to the remaining \approx 70\% (\Omega_{\Lambda}\approx 0.7), represented by the so-called dark energy. Our understanding of dark energy is very poor. We believe that it exerts a negative pressure, and it has an equation of state with w\approx-1, where w is defined as the ratio between pressure and energy density, p/\rho c^{2}. The \LambdaCDM model successfully describes the Universe at large scales (\gtrsim 1 Mpc), but presents difficulties in explaining some properties related to the formation of structures at smaller scales, like the sub-halo structures in galaxy clusters (Grillo et al. 2015; Meneghetti et al. 2020) and the value of the inner slope of DM halos (e.g., Gnedin et al. 2004; Newman et al. 2013a, b; Martizzi et al. 2012). Moreover, in extended cosmological models, currently available data cannot accurately measure the values of \Omega_{\rm m} and \Omega_{\Lambda}, and different models can be reconciled with the observations (e.g., Motta et al. 2021). This motivates the continuous investigation and testing of the \LambdaCDM model, through new projects and observations. In this context, the use of different and independent cosmological probes is crucial, as they are subject to different systematics and degeneracies. Thus, they can offer valuable help in investigating the current tensions in cosmology (Verde et al. 2019; Moresco et al. 2022). Strong gravitational lensing is an extremely powerful tool for extragalactic and cosmological studies (e.g., Bartelmann 2010; Treu 2010). Among the many applications, such as characterizing the total and DM mass distributions of clusters of galaxies (Acebron et al. 2022; Bergamini et al. 2023a; Granata et al. 2023) and galaxies (Vegetti et al. 2012; Schuldt et al. 2019; Ballard et al. 2024) acting as lenses, gravitational lensing can be used to probe the geometry of the Universe. Beside being particularly effective in measuring the value of the current expansion rate of the Universe (the Hubble constant, H_{0}) through the observations of multiply lensed variable sources, such as quasars or supernovae, both on galaxy and cluster scales (e.g., Refsdal 1964; Suyu et al. 2017; Grillo et al. 2018; Birrer et al. 2019; Grillo et al. 2020; Wong et al. 2020; Rusu et al. 2020; Shajib et al. 2023; Grillo et al. 2024), it also allows one to measure the values of \Omega_{\rm m}, w, and \Omega_{\rm k} (the latter parametrizing the curvature of the Universe, \Omega_{\rm k}=0 in a flat geometry), when kinematic data for lens galaxies are available (e.g., Grillo et al. 2008; Cao et al. 2012) or in systems where two or more sources are multiply imaged by the same deflector (Tu et al. 2009; Collett & Auger 2014; Tanaka et al. 2016; Smith & Collett 2021, and with clusters of galaxies). Massive clusters of galaxies can produce tens to hundreds of multiple images from several background sources and in the last decade they have been employed to measure the values of \Omega_{\rm m}, \Omega_{\Lambda}, w, and \Omega_{\rm k} (e.g., Jullo et al. 2010; Caminha et al. 2016, 2022; Grillo et al. 2024), also thanks to the advent of very deep Integral Field Spectroscopic (IFS) observations, that represent the most effective way to spectroscopically confirm and discover lensed sources. For instance, the number of spectroscopically confirmed multiple images lensed by the Hubble Frontier Field galaxy cluster MACS J0416.1-2403 has increased, in less than 10 years, from 10 sources observed in 30 multiple images (Grillo et al. 2015) to 88 sources lensed into 237 multiple images (Bergamini et al. 2023b). Unfortunately, the total mass distribution of clusters of galaxies is usually very complex, and this requires several mass components to properly model it, whose parameters may be degenerate with the cosmological ones. On the other hand, galaxy-scale systems are in general easier to model and the lens can often be described with an effective single total mass profile, but the limited number of background sources makes them prone to be affected by the mass-sheet degeneracy (Schneider 2014). Galaxy or group-scale systems with a larger number (\geq 3) of lensed background sources might represent the best compromise between these two regimes to learn about the geometry of the Universe. They allow one to measure the values of \Omega_{\rm m} and w, independently of that of H_{0}, but only a few of these systems are known to date and are suitable for cosmological studies (e.g., Smith & Collett 2021). Their analysis, even individually, can offer competitive estimates of the cosmological parameter values, and will pave the way to the exploitation of a larger number of systems of this kind, that are foreseen to be discovered with Euclid and the Vera C. Rubin Observatory - Legacy Survey of Space and Time (LSST) (Collett & Auger 2014; Li et al. 2024). In this paper, we extend the study published in Bolamperti et al. (2023) (hereafter, B23) on SDSS J010049.18+181827.7 (hereafter, SDSS J0100+1818), a strong lensing system (Fig. 1) included in the Cambridge And Sloan Survey Of Wide ARcs in the skY (CASSOWARY) survey (Belokurov et al. 2009; Stark et al. 2013) as a candidate fossil system at z=0.581 (Johnson et al. 2018b). In our previous work, we developed a strong lensing model of the system from the observed positions of the four multiple images of sources A and B (spectroscopically confirmed) and of the two multiple images of source C (with its redshift as a free parameter), and from the extended surface brightness distributions of the multiple images from the Hubble Space Telescope (HST) data. We employed the best-fit models to measure the cumulative total mass profile of the deflector, disentangle the DM and baryonic mass distributions, and to reconstruct the background sources. Now, we leverage on new data taken with the Multi Unit Spectroscopic Explorer (MUSE) on the Very Large telescope (VLT). These IFS observations allow us to measure the redshift of source C, discover two additional strongly lensed objects (E and F), spectroscopically confirm the group members in the MUSE field of view, and measure the stellar velocity dispersion profile of the brightest galaxy. We develop enhanced strong lensing models by including this information, that also allow us to measure the posterior probability distributions of the cosmological parameters \Omega_{\rm m} and w. Figure 1: HST F160W image of the SDSS J0100+1818 strong lensing system studied in this work. Letters label multiple images of the same background source. E and F do not show continuum in the HST image and the orange and brown crosses mark the position of their Ly\alpha emission line peaks, as detected with MUSE. The grey dashed circle represents an aperture of 50 kpc, the approximate average Einstein radius of the system. This paper is organized as follows. In Section 2, we summarize the currently available data for SDSS J0100+1818, focusing on the MUSE data reduction and spectra extraction. In Section 3, we describe the SDSS J0100+1818 system, characterizing the multiply imaged sources A, B, C, E, and F (excluding D that is not a secure system), the deflector with its velocity dispersion profile, and the group members. In Section 4, we describe the enhanced strong lensing models developed and the relative results. In Section 5, we show the strong lensing models with variable values of the cosmological parameters \Omega_{\rm m} and w. We discuss the results in Section 6, and draw conclusions in Section 7. Unless differently specified, and in Section 5, throughout this work we assume H_{0}=70\,$\mathrm{k}\mathrm{m}\,\mathrm{s}^{-1}\,\mathrm{M}\mathrm{p}\mathrm{% c}^{-1}$, \Omega_{\rm m}=0.3 and \Omega_{\Lambda}=0.7. In this model, 1 arcsec corresponds to a linear size of 6.585 kpc at the deflector redshift of z=0.581. All magnitudes are given in the AB system (Oke 1974) and are measured in the HST F160W filter, unless differently specified."
https://arxiv.org/html/2411.08035v1,Simulating continuum-based redshift measurement in the  High Latitude Spectroscopy Survey,"We investigate the capability of the Nancy Grace Roman Space Telescope’s (Roman) Wide-Field Instrument (WFI) G150 slitless grism to detect red, quiescent galaxies based on the current reference survey. We simulate dispersed images for Roman reference High-Latitude Spectroscopic Survey (HLSS) and analyze two-dimensional spectroscopic data using the grism Redshift and Line Analysis (Grizli) software. This study focus on assessing Roman grism’s capability for continuum-level redshift measurement for a redshift range of 0.5\leq z\leq 2.5. The redshift recovery is assessed by setting three requirements of: \sigma_{z}=\frac{\left|z-z_{\mathrm{true}}\right|}{1+z}\leq 0.01, signal-to-noise ratio (S/N) \geq 5 and the presence of a single dominant peak in redshift likelihood function. We find that, for quiescent galxaies, the reference HLSS can reach a redshift recovery completeness of \geq 50\% for F158 magnitude brighter than 20.2 mag. We also explore how different survey parameters, such as exposure time and the number of exposures, influence the accuracy and completeness of redshift recovery, providing insights that could optimize future survey strategies and enhance the scientific yield of the Roman in cosmological research.","Red, quiescent galaxies represent a fundamental component of the cosmic landscape, playing a pivotal role in the study of cosmological evolution and structure formation. These galaxies, characterized by their lack of significant star formation and their red colors due to older stellar populations, serve as important tracers of the mass assembly history of the universe (Slob et al., 2024; Choi et al., 2014; Beverage et al., 2024; Zhuang et al., 2023; Khullar et al., 2022; Marsan et al., 2022). Understanding the distribution and properties of red, quiescent galaxies across cosmic time provides insights into the mechanisms that drive galaxy evolution and the influence of environment on these processes. In cosmology, red, quiescent galaxies are often utilized as proxies for identifying high-density environments like galaxy clusters and groups. The presence of the 4000Å break in their spectra makes them ideal candidates for accurate redshift estimation. Due to their brightness and distinct spectral features, these galaxies are easily detectable over a wide range of redshifts, making them ideal probes for studying the large-scale structure of the universe, such as with Baryon Acoustic Oscillation (BAO) (DESI Collaboration et al., 2024; Percival et al., 2007; Carnero Rosell et al., 2022; Eifler et al., 2021) and Galaxy Clustering (GC) (Padmanabhan et al., 2007; Pandey et al., 2022; Yuan et al., 2024; Sailer et al., 2024; White et al., 2022). Based on the current reference survey design, the Nancy Grace Roman Space Telescope (Roman) will conduct an infrared (IR) survey over a \sim2000 deg2 area as part of the High Latitude Wide Area Survey (HLWAS; Spergel et al., 2015; Wang et al., 2022) to enable weak lensing and clustering measurements. During its 5-year prime mission, Roman will conduct the HLWAS which includes an imaging component (High Latitude Image Survey; HLIS) and a spectroscopic component (High Latitude Spectroscopic Survey; HLSS). The HLSS has a slitless grism component. In particular, the Roman grism enables spectroscopy with resolution of 600 over wavelength range of \lambda=1-1.93\mu m, which is capable of probing red, quiescent galaxies at high redshift by capturing the 4000 Å break redshifted into the coverage of the grism. While the primary target of the Roman grism is emission line galaxies (ELGs), specifically those with H\alpha and [OIII] lines, which are critical for BAO and Redshift Space Distortion (RSD) measurements (Eifler et al., 2021), it is also worth noting that several previous studies (see, e.g., Ryan et al., 2007; Joshi et al., 2019; D’Eugenio et al., 2021) have shown that continuum-based redshifts derived from low-resolution grism spectroscopy and photometry based on 4000Å or Balmer breaks can also provide reasonably accurate redshift determinations (on the order of {\sim}1-2%). One of the earlier works by Gladders & Yee (2000) showed that a {\sim}10% redshift accuracy can be expected for galaxies in clusters in certain redshift ranges with just a color cut using two filters. Joshi et al. (2019) also showed that the expected number of red galaxies exhibiting a 4000Å break is comparable to ELGs with prominent emission lines, albeit with higher redshift uncertainty, allowing for the analysis of a much larger sample relative to a sample with just ELGs. Furthermore, previous studies have confirmed the presence of red, quiescent galaxies at high redshifts up to z\ {\approx}\ 4 with VANDELS survey (Carnall et al., 2019, 2020), the Hubble Space Telescope grism WFC3/G141 (Whitaker et al., 2013; D’Eugenio et al., 2021) and the James Webb Space Telescope (JWST) NIRSpec spectroscopy (Nanayakkara et al., 2024; Carnall et al., 2024). Consequently, the Roman grism could potentially yield a substantial amount of information about red, quiescent galaxies at z>1 given its NIR coverage and wide survey area of 2000\deg^{2}. Depending on the sample size, this dataset could be used independently or combined with other samples to enhance analyses, such as those for BAO and galaxy clustering. Efforts have been make to assess the performance of the spectroscopic components of Roman. For example, Wang et al. (2022) simulated Roman grism images based on a proposed reference survey design, Joshi et al. (2022) simulated Roman P127 prism observations to quantify the efficiency of recovered Type Ia supernova redshifts as a function of exposure time, and Wold et al. (2023) simulated and proposed an observing strategy for the Roman grism deep fields to study high redshift Lyman-\alpha emitters. This paper primarily focuses on assessing the impact of exposure time on the redshift recovery efficiency of red galaxies with the Roman grism. We use simulated direct images for Roman HLIS from Troxel et al. (2023) as input to simulate and analyze two-dimensional (2D) spectroscopy images through the Roman grism with the Grism Redshift and Line Analysis software, Grizli111https://github.com/gbrammer/grizli (Brammer, 2019). Our study specifically targets the redshift estimation of red, quiescent galaxies in these simulations. We estimate redshifts using two separate packages – Grizli and Bagpipes222https://bagpipes.readthedocs.io/en/latest/ (Carnall et al., 2018, 2019). This paper is structured as follows: Section 2 describes the simulation tool and inputs used to simulate Roman grism images. In Section 3, we provide an overview of Grizli’s contamination modeling and redshift fitting pipeline. Section 4 details our selection criteria for choosing red galaxies with accurate redshift estimations in our simulations. We present and discuss the results in Section 5. Finally, we summarize our findings."
https://arxiv.org/html/2411.08031v1,\tilde{\xi}-attractors in metric-affine gravity,"We propose a new class of inflationary attractors in metric-affine gravity. Such class features a non-minimal coupling \tilde{\xi}\,\Omega(\phi) with the Holst invariant \tilde{\cal R} and an inflaton potential proportional to \Omega(\phi)^{2}. The attractor behaviour of the class takes place with two combined strong coupling limits. The first limit is realized at large \tilde{\xi}, which makes the theory equivalent to a \tilde{\cal R}^{2} model. Then, the second limit considers a very small Barbero-Immirzi parameter which leads the inflationary predictions of the \tilde{\cal R}^{2} model towards the ones of Starobinsky inflation. Because of the analogy with the renown \xi-attractors, we label this new class as \tilde{\xi}-attractors.","Cosmic inflation, i.e. an accelerated expansion during the very early Universe, is the current paradigm for explaining the flatness and homogeneity of the Universe at large scales [1, 2, 3, 4]. Moreover, it also provides an origin for the small inhomogeneities observed in the Cosmic Microwave Background radiation. In its minimal version, inflation is usually formulated by adding to the Einstein-Hilbert action one scalar field, the inflaton, whose energy density induces a near-exponential expansion. The latest combination of Planck, BICEP/Keck and BAO data [5] has sensibly reduced the allowed parameters space, strongly favouring nearly-flat concave inflaton potentials and already ruling out many proposed models. Nevertheless, the most popular inflationary realizations, the Starobinsky model [1] and Higgs-inflation [6], still sit in the allowed region. Both models can be described by a scalar field non-minimally coupled to gravity (e.g. [7] and references therein). However, when theories are non-minimally coupled to gravity there is more than one choice of the dynamical degrees of freedom. In the more popular metric gravity, the metric tensor is the only dynamical degree of freedom, while the connection is fixed to be the Levi-Civita one. On the other hand, in metric-affine gravity (MAG), both the metric and the connection are dynamical variables and their corresponding equations of motion will dictate the eventual relation between them. When the gravity action contains only the term linear in the curvature scalar, the two approaches lead to equivalent theories (e.g. [8] and refs. therein), otherwise the theories are completely different [8, 9, 10] and lead to different phenomenological predictions, as recently investigated in e.g. [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]. Moreover, MAG admits two, rather than just one, two-derivative curvature invariants: the usual Ricci-like scalar and the Holst invariant [64, 65, 66], which can be used to construct new models (e.g. [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78]). The purpose of this article is to study a new model in MAG, where the Jordan frame inflaton scalar potential is proportional to the square of the non-minimal coupling function involving the Holst invariant and/or the Ricci-like curvature scalar. As we will see later, this kind of setup will induce a new class of inflationary attractors. The discussion is organized as follows. In Section 2 we introduce the action for our inflationary model in metric-affine gravity. For the sake of minimality, we consider only constructions where the physical degrees of freedom are only the graviton and the inflaton. In Section 3 we present an analytical study of the inflaton potential and describe how the attractor configuration is reached. Then, in Section 4 we present a detailed numerical study with the corresponding inflationary predictions. Finally, in Section 5 we summarize our conclusions. In addition, in Appendix A, we show the full analytical equations of the slow-roll parameters and the inflationary observables."
https://arxiv.org/html/2411.07988v1,"Lecture notes for the 52 (March 2023) Saas-Fee Advanced School, Switzerland.","These are exciting times for studies of galaxy formation and the growth of structures. New observatories and advanced simulations are revolutionising our understanding of the cycling of matter into, through, and out of galaxies. This chapter first describes why baryons are essential for galaxy evolution, providing a key test of \Lambda-Cold Dark Matter cosmological model. In particular, we describe a basic framework to convert measurements of the gas properties observed in absorption spectra into global estimates of the condensed (stars and cold gas) matter mass densities. We then review our current understanding of the cycling of baryons from global to galactic scales, in the so-called circumgalactic medium. The final sections are dedicated to future prospects, identifying new techniques and up-coming facilities as well as key open questions. This chapter is complemented with a series of hands-on exercises which provide a practical guide to using publicly available hydrodynamical cosmological simulations. Beyond providing a direct connection between new observations and advanced simulations, these exercises give the reader the necessary tools to make use of these theoretical models to address their own science questions. Ultimately, our increasingly accurate description of the circumgalactic medium reveals its crucial role in transforming the pristine early Universe into the rich and diverse Universe of the present day.","1 The Baryon Census While tremendously successful, the \Lambda-Cold Dark Matter (\Lambda-CDM) model of cosmology remains incomplete. Indeed, 95% of the expected matter-energy content is in a form which is currently unknown. The \Lambda-CDM model posits the need of an elusive matter component, coined dark matter, which together with the dark energy dominate the Universe’s matter-energy budget. Figure 1 provides a visual representation of this. Figure 1: A visual representation of the Universe’s matter-energy budget. Dark energy is an unknown form of energy which affects the largest scales of the Universe. The \Lambda-CDM model postulates the existence of an elusive matter component, dubbed dark matter, which together with the dark energy dominate the Universe’s component budget. Baryonic matter refers to the ordinary, non-dark matter, material. Importantly, only a minority of the baryonic matter can be probed by the observations of the light from stars within galaxies. A central tenet is that the vast majority (90%) of the normal matter is in the form of intergalactic gas. Another challenge comes from the formation of structure in the Universe which is based on a well accepted cosmological framework. In a nutshell, matter and radiation initially coupled in the hot, post big-bang plasma where matter was distributed evenly throughout the cosmos. Small primordial density fluctuations at the epoch of the Cosmic Microwave Background (CMB) are amplified with cosmic time through gravitational instability. We currently have limited empirical information about this epoch dubbed ‘dark ages’. This is the period, however, at which the framework for the large-scale structures of the Universe begins to assemble. Dark matter produced the first cosmic web structures through collapse, while ordinary baryonic matter is able to gravitationally follow this structure. As fluctuations grow, they create a network of filaments of dark matter and an overdense gaseous medium. With only hydrogen and helium in atomic form and no metals yet, this epoch of structure formation gives rise to the first (Population III) stars which, to this date, remain challenging to observe. These stars are however important both because they likely produce the first ionising photons that would then reionise the Universe, but also because they pollute the pristine interstellar media with metals Welsh23 . Figure 2: Observations of light element abundances. These measurements, together with Big Bang nucleosynthesis, provide an estimate of baryonic matter density of \Omega_{\rm baryons}\approx 0.0490, in agreement with both Cosmic Microwave Background (CMB) anisotropies planck2016 and dispersion measures in Fast Radio Bursts (FRBs) estimates Macquart20 . The total amount of baryons in the Universe is thus well-constrained Cooke24 . Galaxy formation continues through Dark Matter halos growth and merging; baryonic matter accretes through cosmic web filaments, and fuels star formation and galaxy formation to make the first objects already observable at z\sim10 Carniani24 . The gas is of prime interest since it will flow along the filaments and feed the formation of galaxies, groups and clusters. However, several physical processes affect the dynamics, thermodynamics and composition of the gas. First of all, filaments are heated by gravitational contraction and shocks Kang05 . Second, the ultraviolet background from young stars and quasars photoionises this gas and heats it Gnedin10 ; Hoeft06 . At the same time, cooling effects in the densest regions, radiate the thermal energy of the gas. Finally, supernovae-driven winds enrich and heat this gas in the neighbourhood of star-forming galaxies Cen06 . Both hydrodynamical cosmological simulations and observations indicate that the rate of star formation is to first order predicted by the amount of cold, dense gas available at any cosmic epoch (see Section 3.1). These results help us understand how efficiently (in terms of conversion of gas into stars) and where exactly stars form. 1.1 Why is Baryon Physics key? Figure 3: Left: Recently, estimates of Dispersion Measures (DM) of Fast Radio Bursts (FRBs) with known redshifts has resulted in a third independent estimate of \Omega_{\rm baryons} Macquart20 . Right: Such studies will flourish as more observations are accumulating fast with upcoming facilities including CHIME which leads to tens of FRBs detections a day Abdalla22 . Baryonic matter refers to the ordinary, non-dark matter, material. The total amount of baryons in the Universe has been quantified by three independent sets of observations, all of which converge to similar value of \Omega_{\rm baryons}\approx 0.0490. On one hand, Cosmic Microwave Background (CMB) anisotropies planck2016 provides a measure of \Omega_{\rm baryons}. Additionally, observations light element abundances together with Big Bang nucleosynthesis cooke2018 ; Mossa20 ; Cooke24 lead to a measure of the baryon density (Figure 2). Indeed, the first synthesis of light elements (such as deuterium, helium and lithium) happened in the early Universe while heavier elements have been produced through stellar nucleosynthesis. Observations can be used to determine the primordial abundances of elements formed in the Big Bang, which provides a unique measure of the baryonic density of the Universe, \Omega_{\rm baryons}. Recently, Macquart20 estimated Dispersion Measures (DM) of Fast Radio Bursts (FRBs) with known redshifts resulting in a third independent estimate of \Omega_{\rm baryons} (Figure 3 left panel). Such studies will flourish as more observations are accumulating fast with upcoming facilities including CHIME which will lead to tens of FRB detections a day (Figure 3 right panel, Abdalla22 ). Importantly, only a minority of the baryonic matter can be probed by the observations of light in galaxies. The vast majority (90%) of the normal matter is in the form of gas as predicted by simulations in a typical phase diagram (Figure 4). This gas, notably the cold gas, provides the reservoir of fuel for forming stars and ultimately planets. A large reservoir of this gas still remains elusive and up to 20-50% of the baryons are thought to be hidden into the so-called Warm-Hot Intergalactic Medium (WHIM) often referred to as the “missing baryons” Cen99 ; Dave01 ; Driver21 as illustrated in 5. Accompanying hands-on #1 provides a starter guide to access similar particle properties in the TNG simulations Nelson13 . It also proposes a number of exercises to naviguate the interface to the simulations for the reader to get familiar with the working framework. Figure 4: Simulated phase-diagram at redshift z=0. The figure displays the density distribution of simulated particles in a temperature-density parameter space. Star formation is labeled ”cond” for condensed material, the cold diffuse gas as ”diffuse”, the Warm-Hot Intergalactic Medium is refered to as WHIM, while hot gas in groups of galaxies and galaxy clusters are labeled ”hot”. The fractions of gas-mass within each region, whose boundaries are indicated with black lines, is provided on the plot. This figure evidences the large contribution of the so-called WHIM to the matter budget torrey19 . Figure 5: Current census of baryons. Low-redshift budget of the baryonic matter probed by observations. About 20% of baryons have not yet been located or most importantly, their physical properties have not been characterised. These are referred to as the “missing baryons” DeGraaff19 . The current \Lambda-CDM paradigm successfully describes cosmological evolution of the large-scale structures of the Universe, organised in galaxies, clusters, sheets, and filaments separated by voids. However, \Lambda-CDM over-predicts the amount of small-scale structures, including the number of dwarf galaxies, a problem referred to as the “missing satellites problem” Bullock10 ; Perivolaropoulos22 . The lack of observed small-scale structures may imply the existence of physical mechanisms which would suppress small-scale fluctuations in particular. The missing dwarf galaxies in cosmological simulations Bullock10 relates to the poorly resolved baryonic processes on the small-scales, including gas inflows and outflows. Furthermore, constraining the matter distribution in the Universe and its evolution with cosmic time from the next generation of galaxy surveys will rely on various type of measurements. Weak gravitational lensing of galaxies in particular offers a promising avenue by measuring per-cent-level distortions of galaxies’ ellipticities which are caused by bending of the path of photons due to the effects of gravity Mandelbaum18 ; Mellier24 . These distortions map the distribution of matter in the Universe at various epochs and thus set new constraints on the physical properties of dark energy, theories of gravity, as well as the nature of dark matter. VanDaalen11 showed that the distribution of matter, which an inherent input to weak lensing analyses, is significantly affected by baryonic effects. The major impact takes place at Mpc-scales, where the power is suppressed due to active galactic nuclei-driven gas ejection. Cosmological hydrodynamical simulations make predictions of these physical processes at the relevant scales and their impact on the total matter power spectrum. Predictions vary vastly from one model to another, in part because the numerical methods differ but also because of the various implementation of baryonic (‘sub-grid’) processes (Figure 6). These effects have been major topic of research in recent years Semboloni11 ; Chisari18 ; Foreman20 ; Chisari18 ; Schneider15 ; Schneider19 ; Huang19 ; Debackere20 ; vanDaalen20 ; Amon22 ; Salcido23 ; Arico23 . Indeed, baryons are known to have effect on haloes: they compress the mass near the center, but suppress the density beyond the core which results in a reduction of the halo mass (and hence mass function) that becomes weaker towards higher masses Velliscig14 ; Cui14 ; Cusworth14 ; Bocquet16 ; HernandezAguayo23 . Currently, there is relatively little quantitative agreement among simulations regarding a robust description of the properties of gas flows, and specifically the implementation of feedback processes. Only detailed observations of star-forming and active galactic nuclei-driven winds (leading to measurements of velocity, gas phase, mass loading factor) will better constrain these complex physical processes. Observations of the baryonic matter are thus a unique means of tackling this degeneracy and providing new constraints on feedback models. Therefore, the modelled baryonic physics impact the properties of warm dark matter and thus the inference of cosmological parameters from weak lensing measurements from KIDS Janis19 , DES Abbott22 and the next generation of surveys such as LSST Ivezic19 , Euclid Amendola18 and Roman/WFIRST Spergel15 . Figure 6: Dark Matter power spectrum. The figure presents the ratio of the predictions from models including baryonic physics (labelled ”hydro”) to Dark Matter Only (labelled ”DMO”). The lines display various state-of-the-art cosmological hydrodynamical simulations. Both the sign and amplitude of the difference between these predictions is important, especially at large k-values, corresponding to small physical scales where baryonic physics dominate Huang19 . {trailer} Hands-on to analyzing cosmological galaxy formation simulations These hands-on sections walk through a “getting started” guide for analyzing cosmological hydrodynamical simulations of galaxy formation – like IllustrisTNG. Additional documentation and further information is available at www.tng-project.org/data. The content is split into four sections, with a brief setup first. 1. A first plot from the Group Catalog 2. Galaxy population relations and integral properties 3. Observables: predicting gas absorption/emission 4. The baryon cycle: measuring mass flow rates [0] Setup First, we import the helper scripts for loading the simulation data: import illustris_python as il import matplotlib.pyplot as plt import numpy as np [Hands-on #1] A first plot from the Group Catalogs Define the path for the data, i.e. choose the simulation we want to work with. Always start with a low resolution simulation for testing, as the size of data (i.e. time needed to load and plot) is smaller. Once you have a piece of code working, you can always change the path to a higher resolution simulation. basePath = ’sims.TNG/TNG100-3/output/’ We can then load fields from the z=0 group catalog. We specify the redshift by the snapshot number: snap_number = 99 We can load any field which is available (i.e. pre-computed) in the group catalogs. In particular, plot the relationship between star formation rate and stellar mass for galaxies. To do so, we will load both quantities, for all subhalos: • masses ”by type” (SubhaloMassInRadType), • star formation rates (SubhaloSFRinRad). Note: the ”InRad” suffix indicates that both measurements are considering only particles/cells within twice the stellar half mass radius, which is an OK first definition for the size of a galaxy. Other measurements of these same quantities, following other definitions, are also available. fields = [’SubhaloMassInRadType’,’SubhaloSFRinRad’] subhalos = il.groupcat.loadSubhalos(basePath, snap_number, fields=fields) Then we can inspect the result: subhalos.keys() dict_keys([’count’, ’SubhaloMassInRadType’, ’SubhaloSFRinRad’]) subhalos[’count’] 118820 subhalos[’SubhaloMassInRadType’].shape (118820, 6) Inspecting the return, we see it is a normal dictionary. The ”count” indicates that there are about 100k total subhalos (in TNG100-3, versus 4.4 million in TNG100-1). Each requested field is returned as a numpy array. We can now make a plot of star formation rate versus stellar mass. To do so, we need to select the values of interest from the catalog arrays, and be careful of physical unit conventions. (Always check the units and definition of fields before use). # select which particle type we are interested in mass_stars_code = subhalos[’SubhaloMassInRadType’][:,4] # careful of units! mass_stars_msun = mass_stars_code * 1e10 / 0.6774 sfr_msun_per_yr = subhalos[’SubhaloSFRinRad’] # plot fig, ax = plt.subplots() ax.plot(mass_stars_msun, sfr_msun_per_yr, ’.’, ms=1.5) ax.set_xscale(’log’) ax.set_yscale(’log’) ax.set_xlabel(’Galaxy Stellar Mass [M$_\odot$]’) ax.set_ylabel(’Star Formation Rate [M$_\odot / yr$]’); Figure 7: Hands-on plot: the relationship between galaxy star formation rate and galaxy stellar mass, for the TNG100-3 simulation at z=0. Exercise * 1. Change the simulation above (e.g. to a higher resolution, or to a different TNG box), and remake the plot. Do you see any different features? What is your interpretation? Caution: are there any subhalos with ‘SFR == 0‘? They would have gotten lost due to the y-log axis. Find them, and include them on the plot. What is your interpretation of these subhalos? * 2. Quenched galaxies have largely stopped forming new stars. There are many definitions of quenched: one is that the specific star formation rate \rm{sSFR}=\rm{SFR}/M_{\star} is below a constant threshold value of 10^{-11}\rm{yr}^{-1} (at z=0). Compute and plot sSFR versus stellar mass for all galaxies. Add a horizontal line for this threshold. Then, measure the fraction of quenched galaxies as a function of mass (i.e. in N bins of stellar mass) and plot quenched fraction versus stellar mass. Does it make sense? 1.2 The Baryon Cycle Galaxies are not isolated islands. Their interactions with their environment profoundly influence their evolution. They form as gas cools and condenses at the centres of a population of massive halos growing by gravitational amplification of fluctuations in an initially near-uniform distribution of pre-existing dark matter. The canonical picture has galaxy growth being fed by inflows of gas from the intergalactic medium, IGM Dekel09 ; vandeVoort11 . Gas acquired through mergers or through accretion of IGM replenishes the fuel needed for star formation. These baryons from the cosmic web cool into a dense atomic then a molecular phase, which fuels star formation. These processes are collectively described as a self-regulation, where part of the gas is consumed in star formation while outflows eject some other quantity of gas from the system. Molecular gas is formed by complex small-scale physics including cloud collisions, dynamical/orbital features, or turbulence making the clouds become self-gravitating, and initiating star formation Gronke2017 ; gronke18 ; Gronke2022 . Another important component of these processes is the metal enrichment of the interstellar medium which in turns will affects the efficiency of star formation since not only do metals act as coolants but they also are fundamental to dust production by shielding molecules from dissociating radiation. Figure 8: The expected (black line) and observed (red line) galaxy luminosity function. The discrepancies in the low- and high-mass ends is related to Supernovae and Active Galactic Nuclei feedback, respectively Silk12 . The \Lambda-CDM cosmology also makes strong predictions on the halo mass function. Departures from these expectations with respect to the observed stellar mass function of galaxies are apparent at both the low and the high mass ends Silk12 . These deviations indicate a lower star formation efficiency in these two different mass regimes (Figure 8). To reconcile predictions from theory with observations, current simulations invoke feedback from various astrophysical processes at both mass scales. In low-mass galaxies, models include supernovae feedback to reproduce the observations while at high masses, feedback from an accreting super-massive black holes is required. The canonical picture is thus that once stars are formed, galaxies enrich the intergalactic medium with ionising photons and heavy elements formed in stars and supernovae, by driving galactic and active galactic nuclei-driven winds into the surrounding Pettini03 , some of which will fall back onto the galaxies in so-called galactic fountains Fraternali17 ; Bish19 . Gas flowing out of the galaxy (winds driven by supernovae or a super-massive black hole in the galaxy center) quenches star formation. Yet another important open question for galaxy formation models is to reproduce the low star formation efficiency observed in galaxies and dark matter halos. Figure 9: Illustration of intergalactic transfer. Hydrodynamical FIRE simulations predict the amount of baryonic material acquired through fresh accretion (purple), wind recycling (blue) or transfer from another haloe (green) as a function of redshift anglesalcazar17 . From a theoretical point of view, the difficulty is related to the various scales involved: simulating the large cosmological scales together with the pc-scale typical of interstellar physics poses an major dynamical scale computing challenge Crain23 . Recently, advances in numerical methods and computing capabilities have enabled extraordinary progress in the simulation of structure formation. Accompanying chapter by Jeremy Blaizot covers most of the fundamental processes implemented in current state-of-the-art simulations. It is a remarkable achievement that the overall physical properties (density and temperature) of the gas (specially HI) in these models reproduce observations of the absorbers column densities (i.e. the number of atoms along the line-of-sight) and line widths Fumagalli2011 ; Rahmati2014 ; gaikwad2017 . However, simulating the multiphase interstellar medium still remain challenging even in “zoom-in” simulations even with modern powerful computers. To overcome this challenge, it is necessary to make use of sub-grid modules to model unresolved physical processes, such as winds from dying stars, and supernovae teyssier2019 ; Maio22 ; Butsky24 . Only by implementing the most realistic physics will simulations be able to interpret contemporary observations. Important questions still to answer include which objects and media contribute to the global quantities. The question remains of the relation between the intergalactic gas relation and the interstellar medium of galaxies, including small-scale cold clumps and eventually molecular gas in the ISM of galaxies. Reaching a full understanding of the cycling of baryons between HI, H2 and the ionised phase of the gas requires even more advanced simulations. Hands-on #2 provides an opportunity to look at the gas properties in TNG simulations. A description of galaxy evolution thus requires to depict fully the ‘baryon cycle’, i.e., how material cycles through different phases (hot, warm and cold) and locations from outside galaxies into the interstellar medium and back Tumlinson17 ; FaucherGiguere23 . This baryon cycle is instrumental to the fueling and the regulation of star formation through and accretion and outflowing processes. The availability of the gas reservoir drives the instantaneous star formation rate and therefore directly governs the growth of galaxies and super massive black holes. More globally, the temporal and spatial evolution of material describes these processes of motion and transformation of the baryons Walter2020 ; Tacconi2020 ; PerouxHowk20 . The inflows and outflows exchange gas, metals, and angular momentum between the galaxy and its atmosphere. A detailed probe of baryons exchanges is of paramount importance for understanding these processes (Figures 9). Since gas, stars, and metals are intimately connected, gas flows affect the history of star formation and chemical enrichment in galaxies. Understanding this cosmic baryon cycle is crucial for understanding galaxy evolution. A comprehensive model of structure formation must thus consider our Universe as one large, complex ecosystem in which galaxies, stars and ultimately planets can form on different scales. 1.3 The Circumgalactic Medium In this context, the enriched circumgalactic gas, or CGM, surrounding galaxies provides the most direct probe of inflows and winds, which are driven by either stars, supernovae, active galactic nuclei and/or cosmic rays. Ultimately, we would like to measure the mass moves in these processes and how this evolves with time as well as obtain a description on how material enriched in metals, dust and energy eventually cycles back onto the galaxy. New observations and modern simulations reveal that the large, diffuse gas reservoir of the CGM is both multi-scale and multi-phase FaucherGiguere23 . Recently, it has been suggested the cloudlets of cold gas could be entrained in hotter medium those providing a theoretical framework for the recent observational results Gronke2022 . Cold CGM gas also plays a role in modulating halo cooling, act as a reservoir for star formation, and thus affects the galactic feedback processes. The CGM therefore has become a central component of studies of galaxy evolution and there structure formation. Over the past decade, new observations have revolutionised the study of the circumgalactic gas surrounding galaxies Tumlinson11 ; Borthakur13 ; Werk14 ; Heckman17 ; Lehner18 ; Muzahid18 ; Prochaska19 ; Chen2020 ; Peroux2020 . These studies show that the CGM is multiphase (with cold, warm, and hot gas) and makes up a large fraction of the baryon and metal budget. More recently, some works focused on the material closer to the galaxy (i.e. <0.5 Rvir), referring to it as the inner CGM. The CGM is increasingly recognised for its significant role in driving the evolution of galaxies. Hydrodynamical simulations (e.g., IllustrisTNG naiman18 ; pillepich18b ; nelson18a ; marinacci18 ; springel18 , EAGLE crain15 ; schaye2015 ; mcalpine2016 , SIMBA dave16 , FIRE FaucherGiguere16 ) also show the complexity of the CGM. A large fraction of the CGM seems to be bound to the galaxy and may have circulated multiple times through the galaxy (Figures 9). Recently, several groups have implemented different solutions to improve the spatial resolution in the lower density CGM of hydrodynamical simulations and to systematically vary the properties of simulated halos. An increasing number of physical processes, such as galactic winds and jets are being incorporated, essential to increase the realism of the simulations. Cosmic rays, turbulence, and magnetic fields likely also impact CGM physics significantly pakmor17 ; vandeVoort2021 ; Ramesh2023 . Full radiation hydrodynamics simulations in large boxes are also now becoming feasible, providing insight into the role that radiation and thermal pressure play in regulating the hydrogen and helium reionization processes and the structure of the Lyman-\alpha forest. Finally, efficient emulators that utilise GPU acceleration or machine learning techniques are now being developed for sampling the large parameter space relevant for cosmological analyses VillaescusaNavarro21 ; Appleby23 ; Gebek23 . {trailer} Hands-on to analyzing cosmological galaxy formation simulations [Hands-on #2] Galaxy population relations and integral properties When a physical quantity that you are interested in is already in the group catalogs (or can be computed from quantities in the catalogs), this is a good starting point. We consider a ”galaxy gas fraction” defined as f_{\rm gas}=M_{\rm gas}/(M_{\rm gas}+M_{\star}). For a quick look, we load the first 5 subhalos of TNG100-1 at z=0, and print this gas fraction. basePath = ’sims.TNG/TNG100-1/output/’ snap = 99 for i in range(5): sub = il.groupcat.loadSingle(basePath, snap, subhaloID=i) gas_mass = sub[’SubhaloMassInHalfRadType’][0] # units? stars_mass = sub[’SubhaloMassInHalfRadType’][4] # units? fgas = gas_mass / (gas_mass + stars_mass) print(i, fgas) 0 0.097318634 1 0.0105384765 2 0.014834739 3 0.0027430148 4 0.0047753155 Q: Within what radius (or ”aperture”) is this gas fraction computed? Q: Why is the first f_{\rm gas}\sim 10\%, but the other four are \lesssim 1\%? Exercise Make a scatterplot of the gas fraction of galaxies as a function of galaxy stellar mass: use ‘loadSubhalos()‘. Overplot a line representing a running mean (or median). What is your interpretation of the typical behavior with mass? Exercise What if we were interested in the ”halo gas fraction” f_{\rm gas}=(M_{\rm gas}/M_{\rm total}) instead? This would tell us about the gaseous content of the CGM, rather than of the galaxy itself. What aperture/definition would you want to use? What field from the group catalogs would be most appropriate? Make a scatterplot of halo gas fraction as a function of (i) galaxy stellar mass, (ii) total halo mass. Exercise Until now, we have been plotting all subhalos in the group catalog – that is, both ”centrals” and ”satellites”, where a satellite is another haloe within the Virial radius of the central. It is always a good idea to consider centrals and satellites separately. Look at the list of halo fields, and subhalo fields, in the group catalog. Which fields provide the links between the two? (There are ”links” in both directions). 1. Create a three lists of subhalo indices (or ”IDs”): (a) all subhalos, (b) central subhalos only, and (c) satellite subhalos only. 2. Print out the list of central subhalo IDs. What do the gaps represent? 3. Plot the ”satellite fraction” of subhalos as a function of mass. 4. Repeat the gas fraction plot from above, (over)plotting centrals and satellites separately. What is your interpretation? Exercise (Challenge) We have not yet seen the “merger trees”. We can explore how we follow a subhalo through time, and see how it evolves. We will use the ‘SubLink‘ merger tree. 1. Look at the documentation for the il.sublink.loadTree() function. 2. Consider TNG100-1 at z=0. Select all central subhalos whose **parent halo** has 11.5<M_{\rm 200c}/\rm{M}_{\odot}<11.6. How many are there? 3. For the first N=10 of these, load the ”main progenitor branch” (MPB) of each. (You should do this in a loop.) 4. Make a plot of gas fraction versus time (i.e. snapshot number, or redshift if possible), overplotting all N halos. Label each, in the legend, with the halo ID. 5. Add the median relation, taking the median across the N halos, as a solid black line. What is your interpretation?"
https://arxiv.org/html/2411.07980v1,"Nonlinear Effects in Black Hole Ringdown Made Simple:
Quasi-Normal Modes as Adiabatic Modes","The nonlinear nature of general relativity manifests prominently throughout the merger of two black holes, from the inspiral phase to the final ringdown. Notably, the quasi-normal modes generated during the ringdown phase display significant nonlinearities. We show that these nonlinear effects can be effectively captured by zooming in on the photon ring through the Penrose limit. Specifically, we model the quasi-normal modes as null particles trapped in unstable circular orbits around the black holes and show that they can be interpreted as adiabatic modes, perturbations that are arbitrarily close to large diffeomorphisms. This enables the derivation of a simple analytical expression for the QNM nonlinearities for Schwarzschild and Kerr black holes which reproduces well the existing numerical results.","References Abbott et al. (2021a) R. Abbott et al. (LIGO Scientific, Virgo), Phys. Rev. D 103, 122002 (2021a), arXiv:2010.14529 [gr-qc] . Abbott et al. (2021b) R. Abbott et al. (LIGO Scientific, VIRGO, KAGRA), (2021b), arXiv:2112.06861 [gr-qc] . Capano et al. (2023) C. D. Capano, M. Cabero, J. Westerweck, J. Abedi, S. Kastha, A. H. Nitz, Y.-F. Wang, A. B. Nielsen, and B. Krishnan, Phys. Rev. Lett. 131, 221402 (2023), arXiv:2105.05238 [gr-qc] . Finch and Moore (2022) E. Finch and C. J. Moore, Phys. Rev. D 106, 043005 (2022), arXiv:2205.07809 [gr-qc] . Isi and Farr (2022) M. Isi and W. M. Farr, (2022), arXiv:2202.02941 [gr-qc] . Cotesta et al. (2022) R. Cotesta, G. Carullo, E. Berti, and V. Cardoso, Phys. Rev. Lett. 129, 111102 (2022), arXiv:2201.00822 [gr-qc] . Siegel et al. (2023) H. Siegel, M. Isi, and W. M. Farr, Phys. Rev. D 108, 064008 (2023), arXiv:2307.11975 [gr-qc] . Berti et al. (2006) E. Berti, V. Cardoso, and C. M. Will, Phys. Rev. D 73, 064030 (2006), arXiv:gr-qc/0512160 . Ota and Chirenti (2020) I. Ota and C. Chirenti, Phys. Rev. D 101, 104005 (2020), arXiv:1911.00440 [gr-qc] . Bhagwat et al. (2020) S. Bhagwat, X. J. Forteza, P. Pani, and V. Ferrari, Phys. Rev. D 101, 044033 (2020), arXiv:1910.08708 [gr-qc] . Pitte et al. (2024) C. Pitte, Q. Baghi, M. Besançon, and A. Petiteau, Phys. Rev. D 110, 104003 (2024), arXiv:2406.14552 [gr-qc] . London et al. (2014) L. London, D. Shoemaker, and J. Healy, Phys. Rev. D 90, 124032 (2014), [Erratum: Phys.Rev.D 94, 069902 (2016)], arXiv:1404.3197 [gr-qc] . Mitman et al. (2023) K. Mitman et al., Phys. Rev. Lett. 130, 081402 (2023), arXiv:2208.07380 [gr-qc] . Cheung et al. (2023) M. H.-Y. Cheung et al., Phys. Rev. Lett. 130, 081401 (2023), arXiv:2208.07374 [gr-qc] . Ma et al. (2022) S. Ma, K. Mitman, L. Sun, N. Deppe, F. Hébert, L. E. Kidder, J. Moxon, W. Throwe, N. L. Vu, and Y. Chen, Phys. Rev. D 106, 084036 (2022), arXiv:2207.10870 [gr-qc] . Redondo-Yuste et al. (2024) J. Redondo-Yuste, G. Carullo, J. L. Ripley, E. Berti, and V. Cardoso, Phys. Rev. D 109, L101503 (2024), arXiv:2308.14796 [gr-qc] . Cheung et al. (2024) M. H.-Y. Cheung, E. Berti, V. Baibhav, and R. Cotesta, Phys. Rev. D 109, 044069 (2024), [Erratum: Phys.Rev.D 110, 049902 (2024)], arXiv:2310.04489 [gr-qc] . Zhu et al. (2024) H. Zhu et al., Phys. Rev. D 109, 104050 (2024), arXiv:2401.00805 [gr-qc] . Kehagias et al. (2023) A. Kehagias, D. Perrone, A. Riotto, and F. Riva, Phys. Rev. D 108, L021501 (2023), arXiv:2301.09345 [gr-qc] . Perrone et al. (2024) D. Perrone, T. Barreira, A. Kehagias, and A. Riotto, Nucl. Phys. B 999, 116432 (2024), arXiv:2308.15886 [gr-qc] . Ma and Yang (2024) S. Ma and H. Yang, Phys. Rev. D 109, 104070 (2024), arXiv:2401.15516 [gr-qc] . Bourg et al. (2024) P. Bourg, R. Panosso Macedo, A. Spiers, B. Leather, B. Bonga, and A. Pound, (2024), arXiv:2405.10270 [gr-qc] . Bucciotti et al. (2024a) B. Bucciotti, L. Juliano, A. Kuntz, and E. Trincherini, (2024a), arXiv:2405.06012 [gr-qc] . Khera et al. (2024) N. Khera, S. Ma, and H. Yang, (2024), arXiv:2410.14529 [gr-qc] . Yi et al. (2024) S. Yi, A. Kuntz, E. Barausse, E. Berti, M. H.-Y. Cheung, K. Kritos, and A. Maselli, Phys. Rev. D 109, 124029 (2024), arXiv:2403.09767 [gr-qc] . Lagos et al. (2024) M. Lagos, T. Andrade, J. Rafecas-Ventosa, and L. Hui, (2024), arXiv:2411.02264 [gr-qc] . (27) R. Penrose, in Differential geometry and relativity, Any space-time has a plane wave as a limit, Reidel, Dordrecht (1976). Cardoso et al. (2009) V. Cardoso, A. S. Miranda, E. Berti, H. Witek, and V. T. Zanchin, Phys. Rev. D 79, 064016 (2009), arXiv:0812.1806 [hep-th] . Araneda (2023) B. Araneda, Class. Quant. Grav. 40, 025006 (2023), arXiv:2204.13673 [gr-qc] . Weinberg (2003) S. Weinberg, Phys. Rev. D 67, 123504 (2003), arXiv:astro-ph/0302326 . Fransen (2023) K. Fransen, Class. Quant. Grav. 40, 205004 (2023), arXiv:2301.06999 [gr-qc] . Giataganas et al. (2024) D. Giataganas, A. Kehagias, and A. Riotto, JHEP 09, 168 (2024), arXiv:2403.10605 [gr-qc] . Bucciotti et al. (2024b) B. Bucciotti, L. Juliano, A. Kuntz, and E. Trincherini, JHEP 09, 119 (2024b), arXiv:2406.14611 [hep-th] . Maldacena (2003) J. M. Maldacena, JHEP 05, 013 (2003), arXiv:astro-ph/0210603 . Kehagias and Riotto (2013) A. Kehagias and A. Riotto, Nucl. Phys. B 873, 514 (2013), arXiv:1302.0130 [astro-ph.CO] . Peloso and Pietroni (2013) M. Peloso and M. Pietroni, JCAP 05, 031 (2013), arXiv:1302.0223 [astro-ph.CO] ."
https://arxiv.org/html/2411.07875v1,From Dark Matter Minihalos to Large-Scale Radiative Feedback: A Self-Consistent 3D Simulation of the First Stars and Galaxies using Neural Networks,"A key obstacle to accurate models of the first stars and galaxies is the vast range of distance scales that must be considered. While star formation occurs on sub-parsec scales within dark matter (DM) minihalos, it is influenced by large-scale baryon-dark matter streaming velocities (v_{\rm bc}) and Lyman-Werner (LW) radiative feedback which vary significantly on scales of \sim100 Mpc. We present a novel approach to this issue in which we utilize artificial neural networks (NNs) to emulate the Population III (PopIII) and Population II (PopII) star formation histories of many small-scale cells given by a more complex semi-analytic framework based on DM halo merger trees. Within each simulation cell, the NN takes a set of input parameters that depend on the surrounding large-scale environment, such as the cosmic overdensity, \delta(\vec{x}), and v_{\rm bc} of the cell, then outputs the resulting star formation far more efficiently than is possible with the semi-analytic model. This rapid emulation allows us to self-consistently determine the LW background intensity on \sim100 Mpc scales, while simultaneously including the detailed merger histories (and corresponding star formation histories) of the low-mass minihalos that host the first stars. Comparing with the full semi-analytic framework utilizing DM halo merger trees, our NN emulators yield star formation histories with redshift-averaged errors of \sim10.2% and \sim9.2% for PopII and PopIII, respectively. When compared to a simpler sub-grid star formation prescription reliant on halo mass function integration, we find that the diversity of halo merger histories in our simulation leads to enhanced spatial fluctuations, an earlier transition from PopIII to PopII dominated star formation, and more scatter in star formation histories overall.","The initial era of star and galaxy formation in the Universe is crucial to our understanding of cosmic evolution. Beginning \sim100 Myr after the Big Bang, the first stars are predicted to have formed within dark matter (DM) “minihalos” with masses below the atomic cooling limit of hydrogen (i.e. M_{\rm vir}\simeq 10^{5}-10^{6} \rm M_{\odot}). The primordial gas clouds within these halos radiated energy via \rm{H_{2}} molecular transitions, which limited gas cloud fragmentation as these transitions could not cool the gas as efficiently as metals. Simulations predict that this resulted in much larger characteristic stellar masses [M_{*}=10-1000 \rm M_{\odot}, e.g., 1, 2, 3] than what is observed in the local universe [see 4, for a recent review]. These Population III (PopIII) stars lived relatively short lives of a few Myr [5] and critically initiated the processes of cosmic reionization and metal enrichment [e.g. 6, 7, 8]. Since they form in small numbers and at high cosmic redshifts [9], PopIII stars have eluded direct observations thus far. While observations by JWST have started hinting at possible PopIII sources [e.g. 10, 11, 12], many previous works have attempted to constrain the properties of first stars using indirect observations. For example, studies in “stellar archaeology"" examine local extremely metal-poor stars to place lower limits on the PopIII initial mass function [e.g. 13, 14, 15, 16, 17, 18, 19, 20]. The optical depth of cosmic microwave background (CMB) radiation due to electron scattering has similarly been used to place upper limits on the efficiency of PopIII star formation [21, 22, 23, 24, 25]. Further, line-intensity mapping of the 1640 Å He II recombination line has been proposed as a method to constrain PopIII stellar properties and calibrate models of star formation [26, 27]. Other studies have focused on the observability of PopIII pair-instability supernovae [28, 29] and PopIII gamma-ray bursts [30, 31, 32] to constrain their properties. Lastly, the cosmological 21-cm signal is sensitive to the star formation rate density (SFRD) and three-dimensional (3D) clustering of the first stars and galaxies [see 33, for a detailed review]. This is true for both global observations such as EDGES [34] and LEDA [35], as well as those aiming to measure 3D spatial fluctuations such as HERA [36] and SKA [37]. Theoretical predictions are necessary to guide and interpret these observations. One of the largest hurdles in accurately modelling the first stars is the vast range of distance scales that must be considered [for detailed reviews on simulations of the first stars and galaxies, see 38, 39]. While the process of star formation occurs on distance scales of parsecs (e.g., the virial radius of a \sim 10^{5}\ \mathrm{M_{\odot}} halo at z=40 is \sim 40 pc), relative baryon-DM streaming velocities (v_{\rm bc}) fluctuate significantly on 100 Mpc scales [40, 41] and can significantly hinder star formation as matter advects out of the shallow gravitational potential wells of the DM halos [42, 43, 44]. Radiative feedback from star formation can similarly affect cosmological volumes as ultraviolet (UV) radiation within the Lyman-Werner (LW) band ({E_{\rm LW}} = 11.2–13.6 eV) may freely stream up to \sim100 Mpc before cosmological expansion redshifts the photons into a Lyman series line and it is absorbed [45, 46]. This feedback may shine on to other DM halos, dissociating \rm{H_{2}} molecules within them and further suppressing PopIII star formation [45, 47, 48, 49, 46]. To accurately model early star and galaxy formation then, one must simultaneously capture the star formation occurring on small spatial scales as well as the large-scale v_{\rm bc} and radiative feedback. In an effort to address this challenge, many works have analytically calculated the formation and large-scale distribution of the first stars to make predictions of observables and constrain their properties [e.g. 50, 51, 52, 53, 54]. For example, several works have implemented the method of analytic halo mass function integration as a means of rapidly determining star formation in a given volume of space [e.g. 26, 24, 55]. This method, however, requires fine-tuned parameters, such as the halo mass ranges over which one integrates to yield accurate SFRD evolutions when compared with more sophisticated calculations [56, henceforth CF24]. Further, this method can only predict statistically-averaged star formation for a given halo mass function at a given time, ignoring any previous star formation, radiation, gas heating and ejection, etc. that are required to accurately model the spatial fluctuations of star formation in non-global scenarios. Semi-analytic models (SAMs) are another class of models that introduce more complex astrophysics on top of analytic frameworks to more accurately predict star and galaxy formation [e.g. 57, 58, 59, 60]. These typically implement DM halo merger trees to serve as the scaffolding for star and galaxy formation, and can include processes like self-consistent LW feedback [49, 61, 62], the inclusion of relative streaming velocities [44], and models for more complex halo physics induced by mergers, metal enrichment, and radiation [e.g. 18, 63, 64]. These SAMs, along with simpler analytic models, may be used within large 3D semi-numeric models to simulate the star formation within a subregion of the overall volume with the goal of reproducing observables that require spatial fluctuations and correlation [e.g. 65, 66, 67, 68, 69]. Such semi-numeric models therefore more accurately predict the 3D distribution of high-redshift star formation and are typically used to both study the effects of the first stars on large-scale observables and constrain their properties [70, 71, 72, 73, 74]. Note, semi-numeric models like the one presented in this work are distinguished from SAMs by including small-scale resolution elements in which an assumed sub-grid prescription dictates star formation. Such semi-numeric models, however, are often limited by computational resources and finite spatial resolution, meaning they typically cannot resolve the DM minihalos in which early star formation occurs while simultaneously capturing the large-scale evolution of the volume. Recently, Magg et al. (2022) [75] elevated this sub-grid technique by implementing the semi-analytic framework A-SLOTH [76] to characterize the transition redshift between PopIII and PopII star formation in terms of cosmic overdensity, \delta(\vec{x}), then showed how the 21-cm signal changes with transition redshift. The authors utilized the A-SLOTH SAM to simulate star formation in N-body merger trees for a range of critical temperature thresholds and recovery times between the first PopIII supernova and the onset of PopII star formation. The results of these realizations were then used to populate cells of their 21-cm signal model with star formation based on the cell overdensities. While this represents perhaps the best effort to simultaneously account for both large and small distance scales in the literature to date, the authors did not self-consistently account for the effects of LW radiation and v_{\rm bc} in their star formation model, only incorporating them into their 21-cm signal calculation. Here we present a new semi-numerical framework that self-consistently predicts the 3D distribution of PopIII and PopII star formation within a representative large-scale volume of the early universe. Our framework is the first to self-consistently compute large-scale effects, such as 3D spatial fluctuations in LW background intensity, while simultaneously capturing realistic DM halo merger trees that resolve the hosts of the first stars. We achieve this by developing many neural network emulators which reproduce the star formation results of a more computationally demanding SAM. Once trained, these neural networks rapidly and accurately emulate the star formation within a small-scale cell of our simulation volume while taking into account the larger scale environment of the cell (e.g., the specific LW intensity observed in that cell from the rest of the large-scale volume). Machine learning techniques such as neural networks have recently been implemented in various fields of astrophysical research to efficiently explore large and complex datasets. For example, they have been utilized to emulate the distribution of galaxy halo masses and velocities [77], the 3D clustering of galaxies [78], the connection between galaxy formation and the DM halos which host them [79, 80], overall structure formation [81], gravitational lensing [82], and even the cosmic 21-cm signal to both constrain its parameters for future observations [83, 84] and identify the type of radio background present within it [85]. Recognizing their potential, we implemented machine learning techniques in the semi-numeric framework presented here to rapidly populate sub-grid regions in our simulation with star formation. The rest of this paper is organized as follows. In Section 2 we discuss the physical processes and parameters included in our simulation framework, and illustrate the neural net training process. In Section 3 we discuss the results of our model, and place them into context with other simulations. Finally, we summarize our results in Section 4 and discuss planned future work with this new large-scale model. Unless otherwise stated, all distance scales are in comoving units and all baryon-DM streaming velocities are those found at Recombination. Also note that the {J_{\rm LW}(z)} values referred to in this work have units of J_{\rm 21}=10^{-21}\ \rm erg\ s^{-1}\ cm^{-2}\ Hz^{-1}\ sr^{-1}. Finally, we assume a \LambdaCDM cosmology throughout, consistent with [86], adopting the following parameter values: \Omega_{\rm m}=0.32, \Omega_{\rm\Lambda}=0.68, \Omega_{\rm b}=0.049, h=0.67."
https://arxiv.org/html/2411.07713v1,Wormhole-Induced ALP Dark Matter,"Non-perturbative gravitational effects induce explicit global symmetry breaking terms within axion models. These exponentially suppressed terms in the potential give a mass contribution to the axion-like particles (ALPs). In this work we investigate this scenario with a scalar field charged under a global U(1) symmetry and having a non-minimal coupling to gravity. Given the exponential dependence, the ALP can retain a mass spanning a wide range, which can act as a dark matter component. We specify pre-inflationary and post-inflationary production mechanisms of these ALPs, with the former from the misalignment mechanism and the latter from both the misalignment and cosmic-string decay. We identify the allowed parameter ranges that explain the dark matter abundance for both a general inflation case and a case where the radial mode scalar drives inflation, each in metric and Palatini formalisms. We show that the ALP can be the dominant component of the dark matter in a wide range of its mass, m_{a}\in[10^{-21}~{}\mathrm{eV},\,\mathrm{TeV}], depending on the inflationary scenario and the U(1) breaking scale. These results indicate that ALPs can be responsible for our dark matter abundance within a setup purely from non-perturbative gravitational effects.","Among beyond the Standard Model (BSM) theories, axion or axion-like particles (ALPs) have been obtaining much attention due to their unique characteristics.111In this work, the term ‘axion’ refers to the QCD axion which is particularly motivated to solve the string CP problem and have a designated coupling to Standard Model fields. Other than that, we will use the terminology ‘ALP’. First of all, the existence of the axion has the potential to address two major unsolved problems in particle physics and cosmology, namely the strong CP problem Peccei:1977hh ; Peccei:1977ur ; Wilczek:1977pj ; Weinberg:1977ma and the nature of dark matter Preskill:1982cy ; Abbott:1982af ; Dine:1982ah . For the latter case, frameworks that generalize beyond the QCD axion can also provide good dark matter candidates. These ALP models are generally associated with global U(1) symmetries, where the ALP mass is determined by the symmetry-breaking terms and can span a much wider range. In general, at the very least, all global symmetries are expected to be explicitly broken in the presence of gravity Kallosh:1995hi ; Banks:2010zn ; Witten:2017hdv ; Harlow:2018jwu ; Harlow:2018tng . One definite origin of the symmetry breaking is coming from non-perturbative gravitational effects, which include gravitational instantons represented by Euclidean wormhole solutions Lee:1988ge ; Giddings:1989bq ; Abbott:1989jw ; Coleman:1989zu ; Kallosh:1995hi ; Hebecker:2016dsw ; Alonso:2017avz ; Hertog:2018kbz ; Hebecker:2018ofv ; Loges:2022nuw ; Andriolo:2022rxc ; Loges:2023ypl ; Jonas:2023ipa ; Kanazawa:2023xzy ; Martucci:2024trp ; Hertog:2024nys . For the QCD axion scenario, this explicit PQ symmetry breaking becomes another source in addition to the QCD instanton effects, which shifts the vacuum expectation value (vev) away from the desired value that solves the strong CP problem, leading to the ‘axion quality problem’ Dine:1986bg ; Kamionkowski:1992mf ; Barr:1992qq ; Holman:1992us ; Ghigna:1992iv . However, the possible implications of gravitational global symmetry breaking reach out to any Goldstone bosons, including ALPs beyond the QCD axion. These symmetry breaking terms can be a universal source giving the mass of general pseudo Nambu-Goldstone bosons (pNGBs) Alonso:2017avz ; Alvey:2020nyh . These now massive ALPs can then be a dark matter component, in analogy to axion dark matter. A nonzero initial angle induces a misalignment mechanism Preskill:1982cy ; Abbott:1982af ; Dine:1982ah , and cosmic strings associated with the global symmetry can also emit ALP dark matter particles Sikivie:1982qv ; Vilenkin:1984ib ; Davis:1986xc ; Vincent:1996rb ; Kawasaki:2014sqa ; Vilenkin:1986ku , with the mass, in this case, being solely determined through gravitational effects. The mass range highly depends on the wormhole action value, which itself depends on the axion model context. Especially, in Refs. Hamaguchi:2021mmt ; Cheong:2022ikv ; Cheong:2023hrj , it was shown that a large non-minimal coupling to gravity with coupling \xi significantly alters the wormhole properties with respect to minimal gravity cases. This motivates the necessity to rigorously revisit the possible effects of these gravitational instantons on ALP DM. In this paper, we explore the possibility that non-perturbative gravitational effects can induce explicit global symmetry breaking in ALP models. Specifically, we consider a global U(1) scalar field non-minimally coupled to gravity, and investigate how the resulting ALP mass, determined by exponentially suppressed symmetry breaking terms, can span a wide range. Notably, for \xi=0, the wormhole-induced ALP mass resulting from explicit U(1) symmetry breaking is too large to be a viable dark matter candidate. This also emphasizes the need to introduce a non-minimal coupling as a minimal mechanism to suppress the U(1) global symmetry breaking caused by the wormhole. Our study covers both pre-inflation and post-inflation production mechanisms. We examine the allowed parameter spaces that explain the dark matter abundance, considering both general inflationary scenarios and cases where the radial mode scalar drives inflation. Our analysis includes both the metric and Palatini formalisms, and we demonstrate that ALPs can account for the observed dark matter density over a broad range of mass and symmetry breaking scales, driven purely by non-perturbative gravitational effects. This paper is organized as follows. We first review the wormhole properties within an analytic framework, and obtain the ALP mass expression in Section 2. In Section 3, we further analyze the ALP dark matter production mechanisms, considering the global symmetry breaking scale. We then present possible parameter ranges that explain our dark matter abundance after identifying relevant constraints for both general inflation in Section 4 and the case when the radial mode of the complex scalar being the inflaton field in Section 5. In Section 6, we conclude with possible further implications of these wormhole dark matter candidates."
https://arxiv.org/html/2411.07692v1,Crunch from AdS bubble collapse in unbounded potentials,"We consider a scalar field theory with a Minkowski false vacuum and an unbounded (or very deep) true vacuum. We show compelling evidence that an AdS bubble of vanishing total energy, embedded in asymptotically flat spacetime, generically undergoes a spherical collapse which leads to a space-like curvature singularity after the formation of trapped surfaces and apparent horizons. The crunch singularity, which is hided behind an apparent horizon, occurs before the true vacuum is reached, and the existence of a lower bound of the scalar field potential is not a necessary condition for its formation.","Scalar fields with zero-energy false vacua and deep true vacua are commonplace in high energy physics, e.g., Bosonic String theory and the String theory landscape Polchinski (2007); Agmon et al. (2022), possibly the Standard Model Higgs Elias-Miro et al. (2012), etc. In the context of early-universe cosmology, transitions of a scalar field into the negative true vacuum could lead to rich phenomenology, e.g., expanding and/or collapsing Anti-de Sitter (AdS) domains, primordial black hole formation, constraining inflationary scenarios Espinosa et al. (2015); Jain and Hertzberg (2020); De Luca et al. (2022), and dark matter production scenarios Espinosa et al. (2018). The evolution of a gravitating bubble with a negative energy-density core has been mainly studied analytically, employing the thin-wall approximation Weinberg (2012); Coleman and De Luccia (1980); Abbott and Coleman (1985); Espinosa et al. (2015); De Luca et al. (2022). In recent years, there have been numerical Strumia and Tetradis (2022) and analytical Dong and Harlow (2011); Kanno and Soda (2012); Kanno et al. (2012); Espinosa (2020); Espinosa et al. (2021) studies (based on analytic continuations from Euclidean Instanton and Bounce solutions), going beyond the thin-wall approximation, for potentials of specific forms. It has been established that the interior of the AdS bubble undergoes a gravitational collapse, leading to the formation of trapped surfaces, and a space-like singularity behind them, known as the AdS crunch. Depending on the details of the bubble configuration and its embedding Espinosa et al. (2015), e.g., if the total (ADM) mass is positive, zero or negative, and on whether the asymptotic spacetime is dS, Minkowski or AdS, the exterior of the bubble could collapse – leading to a black hole, or expand – engulfing the entire false vacuum. In this work we study for the first time the gravitational collapse of an AdS bubble of a scalar field with a Minkowski false vacuum and an unbounded (or very deep) true vacuum. We show that the existence of a lower bound of the scalar field potential is not a necessary condition for the formation of trapped surfaces and apparent horizons. We discover an apparent horizon can be generated while the scalar field is rolling down the slope of the unbounded potential (or towards its deep true vacuum, if the potential is bounded). Namely, a naked singularity does not form. This is in contrast with the commonly accepted picture that trapped surfaces appear after the true vacuum is reached. We consider a natural scalar field potential – a quadratic false vacuum, and an infinite (or very deep) true vacuum, approached along a quartic slope. We do not employ the thin wall approximation. We assume the bubble collapse follows its nucleation through an O(4)-symmetric instanton, known as the Coleman-De Luccia instanton. We also assume the subsequent bubble collapse is O(3,1) symmetric, reducing its equations of motion to a system of ODEs which is readily solved. The validity of the O(3,1) symmetry is straightforwardly proven for the bubble exterior. If the solution is analytic, it immediately follows that the bubble interior is also O(3,1) symmetric. However, it needs to be shown in general, as the solution may not be analytic. To this end, we independently numerically solve the Einstein and Klein-Gordon PDEs, using different coordinate system and slicing condition, without assuming O(3,1) symmetry but only spherical symmetry. We employ the 3+1 ADM formalism. The simulations confirm the O(4) initial data gives rise to a globally O(3,1) bubble, validating our ODE approach and its conclusions. We observe the scalar field continuously speeds up its roll along the steepening negative slope, the positive kinetic energy grows, eventually overtakes the negative potential energy, rendering the core energy density positive. The positive energy-density core leads to the formation of trapped surfaces, and the subsequent rapid unbounded growth of the energy density (and associated geometric invariants) signifies the formation of the crunch singularity. This is in accordance with the notion that the formation of trapped surfaces is one of the sufficient (along with others) conditions for geodesic incompleteness, i.e., singularity existence Hawking and Ellis (2023). The work is organised as follows. In Section II we present the details of the model. Section III outlines the initial conditions for the bubble collapse, which are based on the quantum bubble nucleation from a false vacuum. The subsequent bubble evolution is studied in Section IV. We present our summary in Section V. Technical details about scaling symmetries of the model, numerical GR studies and their results are delegated to the appendices. Throughout the paper, we work in units in which \hbar=c=1, and the reduced Planck mass is m_{pl}=1/\sqrt{8\pi G_{N}}. We use the Einstein summation convention for repeated Greek and Latin indices."
https://arxiv.org/html/2411.07648v1,Primordial Black Hole Formation from Type II Fluctuations with Primordial Non-Gaussianity,"This study investigates the formation of primordial black holes (PBHs) resulting from the collapse of adiabatic fluctuations with large amplitudes and non-Gaussianity. Ref. [1] showed that fluctuations with large amplitudes lead to the formation of type B PBHs, characterized by the existence of the bifurcating trapping horizons, distinct from the more common type A PBHs without a bifurcating trapping horizon. We focus on the local type non-Gaussianity characterized by the curvature perturbation \zeta given by a function of a Gaussian random variable \zeta_{\rm G} as \beta\zeta=-\ln(1-\beta\zeta_{\rm G}) with a parameter \beta. Then we examine how the non-Gaussianity influences the dynamics and the type of PBH formed, particularly focusing on type II fluctuations, where the areal radius varies non-monotonically with the coordinate radius. Our findings indicate that, for \beta>-2, the threshold for distinguishing between type A and type B PBHs decreases with increasing \beta similarly to the threshold for black hole formation. Additionally, for large positive values of \beta, the threshold for type B PBHs approaches that for type II fluctuations. We also find that, for a sufficiently large negative value of \beta\lesssim-4.0, the threshold value is in the type II region of \mu, i.e., there are fluctuations of type II that do not form black holes. Lastly, we calculate the PBH mass for several values of \beta. Then we observe that the final mass monotonically increases with the initial amplitude within the parameter region of type A PBHs, which differs from previous analytical expectations.","Primordial Black Holes (PBHs) are Black Holes (BHs) formed in the early universe, which can play a crucial role in exploring the early universe [2, 3, 4, 5]. They are unique probes of statistical properties of the small-scale primordial inhomogeneity. In addition, they are considered as one of the candidates for dark matter and could be detected as black hole binaries by gravitational wave interferometers [6, 7, 8, 9, 10]. In the standard scenario of PBH formation, PBHs are formed during the radiation-dominated era when regions of extremely high density, generated by fluctuations during the inflationary period, undergo gravitational collapse [4]. In this scenario, an enhancement of the power spectrum of the curvature fluctuations is typically necessary to produce a considerable fraction of PBHs in the form of dark matter, which can be followed by the generation of non-Gaussianity. Non-Gaussianities can have a significant effect on the PBH formation process and abundance [11, 12, 13, 14, 15, 16, 17, 18, 19, 20], and on induced gravitational waves [21, 22, 23, 24, 25] (see [26] for a focused review on non-Gaussianities). On the other side, when the initial amplitude of the primordial fluctuation is extremely large, we may have a non-monotonic behaviour of the areal radius as a function of the radial coordinate r, and we call it type II fluctuation following Ref. [27], while standard fluctuations with monotonic behavior of the areal radius type I fluctuation. The study of PBH formation associated with type II fluctuations has been conducted using the Lemaitre-Tolman-Bondi (LTB) solution, which is the analytic solution for the dust fluid, in Ref. [27]. Type II fluctuations may have a very low probability of generation due to their huge amplitude far above its threshold, and they are usually considered to contribute negligibly to the abundance of PBHs. However, it has been pointed out that if the statistics of the curvature fluctuations exhibit non-Gaussianity, the threshold of the amplitude for PBH formation may be close to that for the realization of type II fluctuations, and type II fluctuations may significantly contribute to the PBH abundance [28]. Moreover, in Ref. [29], type II fluctuations have been observed surrounding the bubbles (which eventually will produce PBHs [30]) formed when the inflaton overshoots the barrier separating a local minimum of the inflaton potential in a single-field inflationary model. As non-Gaussianity increases, the vacuum bubble formation channel becomes dominant in comparison with the adiabatic one. As a result, the mass and abundance of PBHs are primarily given by those of type II fluctuations. Recently, Ref. [1] simulated PBH formation from type II fluctuations. It was proposed to classify PBH types into type A/B depending on the existence of the bifurcating trapping horizons, independently of the type of the initial fluctuations, type I/II. We also follow this convention in this paper. In this work, we mainly explore the behaviour of fluctuations of type II under the effect of non-gaussianities. In particular, based on the typical profile depending on a non-Gaussian parameter, we investigate the threshold between type A/B PBH and how the PBH mass depends on the initial amplitude of the fluctuation. Our study aims to give insights and get a deeper understanding of the behavior of fluctuations of type II, specifically focusing on the scenario of non-Gaussianities. This paper is organized as follows. In Section 2, we introduce spherically symmetric curvature fluctuations with non-Gaussianity on the super-horizon scale. In Section 3, we introduce the compaction function, which serves as an indicator of the threshold for PBH formation, and present the threshold values derived from analytical approaches. Section 4 presents the numerical simulation setup and the results obtained. Finally, Section 5 is devoted to summary. Throughout this paper, we use the geometrized units in which both the speed of light and Newton’s gravitational constant are set to unity, c=G=1."
https://arxiv.org/html/2411.07552v1,Landscape of Modular Cosmology,"We investigate the global structure of the recently discovered family of SL(2,\mathbb{Z})-invariant potentials describing inflationary \alpha-attractors. These potentials have an inflationary plateau consisting of the fundamental domain and its images fully covering the upper part of the Poincaré half-plane. Meanwhile, the lower part of the half-plane is covered by an infinitely large number of ridges, which, at first glance, are too sharp to support inflation. However, we show that this apparent sharpness is just an illusion created by hyperbolic geometry, and each of these ridges is physically equivalent to the inflationary plateau in the upper part of the Poincaré half-plane.","The idea that string theory inspired supergravity has a kinetic term with SL(2,\mathbb{R}) symmetry of the form {n\over 4}\,{\partial\tau\partial\bar{\tau}\over({\rm Im}\tau)^{2}} (1) was proposed in Ferrara:1989bc . The integer n here is related to the Kähler curvature of SL(2,\mathbb{R})/U(1) coset space111 Much later this same integer 3\alpha=n in case of cosmological \alpha-attractors was proposed and studied in Ferrara:2016fwe ; Kallosh:2021vcf as a target of the future cosmological experiments, like LiteBIRD LiteBIRD:2022cnt . These are 7 cases n=1,2,3,4,5,6,7 of Poincaré disks related to the upper half-plane by a Cayley transform. which is \mathbb{R}_{K}=-{2\over n}. The proposal in Ferrara:1989bc concerning supergravity potentials was that SL(2,\mathbb{R}) symmetry can be broken down, due to world-sheet instantons, to its discrete subgroup SL(2,\mathbb{Z}). This led to the conclusion that potentials depend on modular forms, like holomorphic modular function j(\tau), holomorphic Dedekind function \eta(\tau), and an almost holomorphic regularized Eisenstein function \hat{G}_{2}(\tau,{\rm Im}\tau). This symmetry of supergravity in Ferrara:1989bc was dubbed “target space modular invariance” to distinguish it from duality symmetry in string theory. SL(2,\mathbb{Z}) invariant cosmological theories are described by a four-dimensional bosonic actions S(\tau,\bar{\tau})=\int d^{4}x\sqrt{-g}\Big{(}{R\over 2}+{3\alpha\over 4}\,{% \partial\tau\partial\bar{\tau}\over({\rm Im}\tau)^{2}}-V(\tau,\bar{\tau})\Big{% )}\,. (2) They depend on a single complex field \tau(x)=\tau_{1}+i\tau_{2}, where \tau_{1} and \tau_{2} are functions of 4 space-time coordinates x^{\mu}. Each term in the action is SL(2,\mathbb{Z}) invariant when \tau\to{a\tau+b\over c\tau+d},\qquad a,b,c,d\in\mathbb{Z}\,,\quad ad-bc=1\ . (3) It was proposed in Casas:2024jbw to study SL(2,\mathbb{Z}) invariant cosmological theories with plateau potentials. Their potential was given by a specific function of the Dedekind function and \tilde{G}_{2} Eisenstein modular form of weight 2. In Kallosh:2024ymt , we studied SL(2,\mathbb{Z}) cosmological plateau-type models of general type, depending on SL(2,\mathbb{Z}) invariant Klein’s j-function and Dedekind function. We have shown that these models, as well as the model of Ref. Casas:2024jbw , represent a novel class of \alpha-attractors studied earlier in Kallosh:2013yoa ; Galante:2014ifa ; Carrasco:2015uma ; Carrasco:2015rva ; Carrasco:2015pla ; Kallosh:2021mnu . We continue to study SL(2,\mathbb{Z}) inflationary models in KalLin2024 ; CKLR . The early studies of target space modular invariance in Font:1990nt revealed an important feature of modular invariant potentials: they have an infinity of degenerate minima, saddle points, and maxima, whose positions are related to each other by modular SL(2,\mathbb{Z}) transformations. It was observed there that the choice of the minimum leads to a spontaneously broken target-space modular invariance. Cosmological inflation models based on SL(2,\mathbb{Z}) invariant action in Eq. (2) were studied in Schimmrigk:2014ica ; Schimmrigk:2016bde ; Schimmrigk:2021tlv . In particular, a model of j-inflation was proposed in Schimmrigk:2014ica ; Schimmrigk:2016bde ; Schimmrigk:2021tlv . A global structure of the potential of j-inflation was investigated in Schimmrigk:2021tlv . It was observed there that the potential near the boundary at {\rm Im}\,\tau\to 0 has an intricate structure reminiscent of a fractal structure. But the contour plot of the potential has to be complimented by the behavior of the hyperbolic metric, which diverges as the saddles approach the real axis. The difference between j-inflation studied in Schimmrigk:2014ica ; Schimmrigk:2016bde ; Schimmrigk:2021tlv and the new SL(2,\mathbb{Z}) invariant models developed in Kallosh:2024ymt is that j-inflation potentials at large \tau_{2} grow exponentially in \tau_{2}, i.e. double exponentially in the canonically normalized inflaton field, whereas in Kallosh:2024ymt the potentials depending on j(\tau),\eta(\tau) approach a plateau. Therefore, in j-inflation, inflation begins at a saddle point, whereas in Kallosh:2024ymt , inflation starts at the plateau, which has the same large-field behavior as the \alpha-attractor potentials. Here we will study the landscape of SL(2,\mathbb{Z}) cosmology. The existence of this landscape, for example, of the infinite number of minima of the potentials, or saddle points, is due to the fact that the value of the potential is the same after the modular transformation. Namely, any point \tau^{\prime}={a\tau+b\over c\tau+d} with arbitrary numbers a,b,c,d with ad-cb=1 is a modular image of any original point \tau since V(\tau^{\prime})=V(\tau). But the plateau of the potential is a big area; for example, in the fundamental domain in Fig. 1, it is a grey area at large y. From the first glance at the plot of the SL(2,\mathbb{Z}) 3D potentials in Kallosh:2024ymt , we find that in Cartesian coordinates, there is one plateau and many ridges. This is a consequence of the fact that the metric in the hyperbolic space in Cartesian coordinates blows up near the boundary y=0 in the half-plane. Therefore, evaluating the shape of the potential without taking into account the metric may be misleading. In this paper, we will develop new tools which will help us to address this issue. Figure 1: Tessellation of the hyperbolic plane \tau=x+iy, (\ -\infty<x<\infty,\,y>0) shows vertical bands which repeat each other. The grey area is a fundamental domain, -0.5\leq x\leq 0.5; \tau\bar{\tau}\geq 1, the other bands are shifted by 1 or -1 and repeated. There is a symmetry under reflection: x\to-x. The proliferation of the images of the points in the boundary of the fundamental domain in the grey area is a result of the eternal continuation of the geodesics, either vertical or semi-circular ones, towards the boundary y\to 0, see Wikipedia. Note that in the cosmological setting in space-time, as opposed to string theory on the world-sheet, \tau=\theta+i\exp{\sqrt{2/3\alpha}\,\varphi} is a space-time dependent complex scalar field \tau(x^{\mu}). One should solve equations of motion, defining the evolution of scalars \theta(t) and \varphi(t) in time. During our investigation of inflation starting at the inflationary plateau in the fundamental domain, we have found that after inflation, the scalar fields roll down and cross the boundary of the fundamental domain, its lower arc, where the minima of the potentials in Casas:2024jbw ; Kallosh:2024ymt are located. Their subsequent evolution, including the process of reheating, takes the scalars out of the fundamental domain. This is not surprising: the whole hyperbolic half-plane is geodesically complete, but the fundamental domain is not. Moreover, as we have found out in Kallosh:2024ymt , inflation in these models may begin outside of the fundamental domain. Thus, if we want to fully understand the cosmological evolution in this scenario, we should explore the global structure of the potential in the entire half-plane. This is the main goal of our investigation."
https://arxiv.org/html/2411.07525v2,The Dependence of Dark Matter Halo Properties on the Morphology of Their Central Galaxies from Weak Lensing,"Xu & Jing (2022) reported a monotonic relationship between host halo mass (M_{h}) and the morphology of massive central galaxies, characterized by the Sérsic index (n), at fixed stellar mass, suggesting that morphology could serve as a good secondary proxy for halo mass. Since their results were derived using the indirect abundance matching method, we further investigate the connection between halo properties and central galaxy morphology using weak gravitational lensing. We apply galaxy-galaxy lensing to measure the excess surface density around CMASS central galaxies with stellar masses in the range of 11.3<\log M_{*}/{\rm M_{\odot}}<11.7, using the HSC shear catalog processed through the Fourier_Quad pipeline. By dividing the sample based on n, we confirm a positive correlation between n and M_{h}, and observe a possible evidence of the positive correlation of n and halo concentration. After accounting for color, we find that neither color nor morphology alone can determine halo mass, suggesting that a combination of both may serve as a better secondary proxy. In comparison to hydrodynamic simulations, we find that TNG300 produce much weaker correlations between M_{h} and n. Furthermore, disabling jet-mode active galactic nuclei feedback in SIMBA simulations results in the disappearance of the positive n-M_{h} relationship, suggesting that the star formation history influenced by black holes may be a contributing factor.","In the \LambdaCDM cosmological model, the large-scale structure of the universe is shaped by the gravitational influence of dark matter, such as halos, filaments and voids. These dark matter halos subsequently act as “seeds” for galaxy formation. Dark matter, a substance that has not yet been directly observed, constitute the major mass of the universe, whereas stars are the luminous objects we can observe. The Stellar-to-Halo Mass Relation (SHMR) reveals the intimate connection between the stellar mass of galaxies and the mass of dark matter halos, indicating the link between galaxy formation and the evolution of dark matter halos. It is generally believed that massive dark matter halos host more massive galaxies. Therefore, by utilizing SHMR, we can assign galaxy masses to halos in N-body simulations, or infer halo mass from stellar mass. This allows for a deeper understanding of the evolutionary history of galaxies, and their coevolution with dark matter halos. Although SHMR provides critical insights into the connection between galaxies and halos, it also exhibits significant scatter (Cooper et al., 2010; Xu et al., 2023; Zentner et al., 2014; Zu et al., 2021, 2022), primarily due to the influence of other galaxy properties such as size, color, and morphology. For instance, some studies find that halos of blue (star-forming) galaxies are less massive than those of red (quenched/passive) galaxies with the same stellar mass (Rodríguez-Puebla et al., 2015; Mandelbaum et al., 2016; Taylor et al., 2020; Wang et al., 2021; Xu & Jing, 2022). Conversely, other studies (Tinker et al., 2013; Moster et al., 2018; Guo et al., 2019) report the opposite conclusion. The morphology of galaxies is also found to correlate with halo mass. Galaxy morphology is typically described by the Sérsic profile (Sérsic, 1963), which characterizes the brightness, size, and the morphology of a galaxy. The Sérsic profile is modeled as: I(r)=I_{e}{\rm exp}\bigg{\{}-b_{n}\bigg{[}\bigg{(}\frac{R}{R_{e}}\bigg{)}^{% \frac{1}{n}}-1\bigg{]}\bigg{\}}, (1) where b_{n} is defined through \gamma(2n;b_{n})=\Gamma(2n), \Gamma and \gamma are the Gamma function and the lower incomplete Gamma function, respectively. I_{e} is the surface brightness at the half-light radius R_{e}. The Sérsic index n quantifies the curvature of the galaxy’s luminosity profile, with higher values indicating more concentrated core of light distributions. In this work, we focus on the relation between halo masses and the morphology of galaxies, characterized by the Sérsic index (n). Studies from Sonnenfeld et al. (2019) utilize lensing data from the Hyper Suprime-Cam Survey (HSC) to study the relationship between halo mass and stellar mass, galaxy size, and Sérsic index in the CMASS sample. They find no significant correlation between halo mass and size or Sérsic index at a fixed stellar mass. However, Taylor et al. (2020) find that for galaxies with a stellar mass of around 10^{10.5}{\rm M_{\odot}}, the Sérsic index and effective radius are the best indicators of host halo mass, while showing weaker correlations with star formation rate or color. Applying the Photometric Objects Around Cosmic Webs (PAC; Xu et al., 2022) method, Xu & Jing (2022) find that more compact, red, and larger galaxies tend to be located in more massive halos. But their method of estimating halo mass relies on abundance matching, which assumes a one-to-one correspondence between stellar mass and the satellite distribution of the host halo. Therefore, the relationship between galaxy morphology and halo mass remains unclear, and the physical origin influencing the SHMR also remains an open question. Weak gravitational lensing can serve as a crucial probe to the total mass of dark matter halos and the matter distribution, which distorts background galaxy images around massive objects, known as shear. Galaxy-galaxy lensing, in particular, can directly measure the average density profile of halos around galaxies or galaxy clusters, thus reflecting the halo mass. In this work, we utilize the lensing data from the third public data release of HSC to further analyze and discuss the dependence of the host halo mass on the morphology of massive central galaxies with stellar masses in the range of 11.3<{\rm log}(M_{*}/{\rm M_{\odot}})<11.7. We introduce our galaxy sample and shear catalog in Section 2, and describe our lensing measurement methods in Section 3. In Section 4, we analyze the measured lensing signals and the corresponding dependence on central galaxy morphologies and the properties of the galaxy clusters. Subsequently, in Section 5, we explore the underlying physical origins using the SIMBA and TNG simulations. Finally, we summarize our findings in Section 6."
https://arxiv.org/html/2411.07492v1,Combining neural networks with galaxy light subtraction for discovering strong lenses in the HSC SSP,"Galaxy-scale strong gravitational lenses are valuable objects for a variety of astrophysical and cosmological applications. Strong lensing galaxies are rare, so efficient search methods, such as convolutional neural networks, are often used on large imaging datasets. In this work, we apply a new technique to improve the performance of supervised neural networks by subtracting the central (lensing) galaxy light from both the training and test datasets. We use multiband imaging data from the Hyper Suprime-Cam Subaru Strategic Program (HSC SSP) as our training and test datasets. By subtracting the lensing galaxy light, we increase the contrast of the lensed source compared to the original imaging data. We also apply the light subtraction to non-lenses in order to compare them to the light-subtracted lenses. Residual features resulting from poor light subtraction can adversely affect the performance of networks trained on the subtracted images alone. We find that combining the light-subtracted images with the original gri-band images for training and classification can overcome this and improve the overall classification accuracy. We find the area under the receiver operating characteristic curve can be improved to 0.841 using the combination of the fiducial images and light-subtracted images, compared to 0.808 for the fiducial imaging dataset alone. This may be a promising technique for improving future lens searches using CNNs.","Galaxy-scale strong gravitational lenses are valuable objects for a variety of astrophysical and cosmological applications, including studies of the mass structure of galaxies (e.g., [Koopmans et al. (2006), Sonnenfeld et al. (2013), Shajib et al. (2021)]), the properties of dark matter (e.g., [More et al. (2009), Vegetti et al. (2014), Hezaveh et al. (2016), Nierenberg et al. (2017), Gilman et al. (2020)]), resolved studies of distant sources at high resolution (e.g., [Rybak et al. (2015), Cañameras et al. (2017)]), and the measurement of cosmological parameters (e.g., [Refsdal (1964), Wong et al. (2020), Birrer et al. (2020)]). However, strong lensing galaxies are rare, requiring a chance alignment of a foreground galaxy (the “lens"", often a massive elliptical galaxy) with a bright background object (the “source"", often a star-forming galaxy or a quasar). As a result, the best datasets for discovering strong lenses are deep, wide-field, multiband imaging surveys that cover a large area of the sky. For example, the Sloan Digital Sky Survey (SDSS; e.g., [Bolton et al. (2006)]), the Kilo-Degree Survey (KiDS; e.g., [Li et al. (2021), He et al. (2020), Petrillo et al. (2017)]) and the Hyper Suprime-Cam Subaru Strategic Program (HSC SSP; e.g., [Sonnenfeld et al. (2018), Sonnenfeld et al. (2020), Chan et al. (2020), Chan et al. (2024), Wong et al. (2022)]) provide such datasets, sometimes with supplementary spectroscopic data that can aid lens searches. The Vera C. Rubin Observatory’s Legacy Survey of Space and Time (LSST; [Ivezić et al. (2019)]) will eventually cover a majority of the sky to unprecedented depths. Therefore, efficient methods are needed to search for strong lenses in these large datasets, such as arc-finding algorithms (e.g., [More et al. (2012)]), citizen science searches (e.g., [More et al. (2016), Sonnenfeld et al. (2020)]), and machine learning methods such as convolutional neural networks (CNNs). CNNs can process imaging data efficiently and identify characteristic patterns. CNNs have been proven to be able to classify strong lensing objects with reasonable accuracy in multiband imaging data (e.g., [Jacobs et al. (2017), Jacobs et al. (2019), Petrillo et al. (2019), Huang et al. (2020), Li et al. (2020)]), substantially reducing the number of lens candidates that need to be visually inspected to verify them. The main challenge in identifying galaxy-scale gravitational lenses is distinguishing the multiple lensed images from the foreground galaxy light, as the image separation tends to be comparable to the atmospheric seeing, particularly for lenses with small Einstein radius. Using multiband data is helpful, as the sources tend to be blue star-forming galaxies while the lenses tend to be red elliptical galaxies, but faint sources can still be challenging for CNNs to identify. One way to increase the contrast between the lens galaxy and the lensed images is to subtract the lens galaxy light from the images. The RINGFINDER algorithm (Gavazzi et al., 2014) used lens light subtraction, which helped to detect many small angular separation lenses that were missed by citizen scientists who were not able to view such subtracted images (see Figure 7 of More et al. (2016)). A subsequent citizen science search by Sonnenfeld et al. (2020) used the YattaLens software (Sonnenfeld et al., 2018) to subtract the central galaxy light, which helped to improve the discovery of these lenses, demonstrating the effectiveness of light subtraction for this purpose. In this work, we attempt to improve the performance of lens-finding CNNs by subtracting the central galaxy light from both the training and test datasets of our networks using YattaLens to highlight the light from the lensed source compared to the original imaging data. A similar technique was tried by Canameras et al. (2023), who used difference images between multiband observations as a proxy for lens light subtraction. We use a more sophisticated method to subtract the central galaxy light, but compare our results qualitatively to those of Canameras et al. (2023). Such light subtraction has generally not been applied to CNN-based lens searches in the past, but may be important for upcoming ground-based surveys (e.g., LSST). We also apply the subtraction to non-lenses in order to make the conditions the same, as we expect an ideal light subtraction to leave no residual features since there is no background source. Therefore, our neural networks can more easily find the characteristics of the source galaxy and classify strong lensing objects correctly. We use multiband imaging from the Wide layer of the HSC SSP as our training and test data. The HSC SSP is an ideal dataset for discovering strong lenses due to its depth and area, and many galaxy-scale strong lenses have already been discovered as part of the Survey of Gravitationally lensed Objects in HSC Imaging (SuGOHI111https://www-utap.phys.s.u-tokyo.ac.jp/ oguri/sugohi/; Sonnenfeld et al. (2018); Wong et al. (2018, 2022); Chan et al. (2024)) project. Several studies, including some working with the public HSC SSP data, have made use of neural networks to discover lens candidates in this dataset (Cañameras et al., 2021; Shu et al., 2022; Andika et al., 2023; Jaelani et al., 2023; Schuldt et al., 2024). We construct three datasets consisting of the same objects for training: the fiducial HSC gri-band imaging dataset, the dataset with light subtraction applied, and the combination of the two. We first optimize our CNN for the fiducial dataset, train it on the other two datasets, and compare the performance of each model. Then, we build new models optimized for each dataset and compare their performance in order to determine whether the light subtraction is able to improve the ability of CNNs to identify strong lenses. This paper is organized as follows. We describe the data used in this study and our galaxy light subtraction procedure in Section 2. We describe our neural network design and architecture in Section 3. We present the performance of our networks when applying our light subtraction procedure and evaluate the characteristics of correctly and incorrectly classified objects in Section 4. We summarize our conclusions and discuss potential future work in Section 5. Throughout this paper, all magnitudes given are on the AB system."
https://arxiv.org/html/2411.07370v1,:  guide for authors,"This is a guide for preparing papers for Monthly Notices of the Royal Astronomical Society using the mnras LaTeX package. It provides instructions for using the additional features in the document class. This is not a general guide on how to use LaTeX, and nor does it replace the journal’s instructions to authors. See mnras_template.tex for a simple template.","The journal Monthly Notices of the Royal Astronomical Society (MNRAS) encourages authors to prepare their papers using LaTeX. The style file mnras.cls can be used to approximate the final appearance of the journal, and provides numerous features to simplify the preparation of papers. This document, mnras_guide.tex, provides guidance on using that style file and the features it enables. This is not a general guide on how to use LaTeX, of which many excellent examples already exist. We particularly recommend Wikibooks LaTeX111https://en.wikibooks.org/wiki/LaTeX, a collaborative online textbook which is of use to both beginners and experts. Alternatively there are several other online resources, and most academic libraries also hold suitable beginner’s guides. For guidance on the contents of papers, journal style, and how to submit a paper, see the MNRAS Instructions to Authors222http://www.oxfordjournals.org/our_journals/mnras/for_authors/. Only technical issues with the LaTeX class are considered here."
https://arxiv.org/html/2411.07318v1,The Simons Observatory: Laboratory Beam Characterization for the first Small Aperture Telescope,"The Simons Observatory is a ground-based telescope array located at an elevation of 5200 meters, in the Atacama Desert in Chile, designed to measure the temperature and polarization of the cosmic microwave background. It comprises four telescopes: three 0.42-meter small aperture telescopes (SATs), focused on searching for primordial gravitational waves, and one 6-meter large aperture telescope, focused on studying small-scale perturbations. Each of the SATs will field over 12,000 TES bolometers, with two SATs sensitive to both 90 and 150 GHz frequency bands (SAT-MF1, and SAT-MF2), while the third SAT is sensitive to 220 and 280 GHz frequency bands. Prior to its deployment in 2023, the optical properties of SAT-MF1 were characterized in the laboratory. We report here on measurements of beam maps acquired using a thermal source on SAT-MF1, along with measurements of near-field beam maps using a holographic method that enables characterization of both the amplitude and phase of the beam response, yielding an estimate of the far-field radiation pattern received by the telescope. We find that the near-field half-width-half-maximum (HWHM) requirements are met across the focal plane array for the 90 GHz frequency band, and through most of the focal plane array for the 150 GHz frequency band. Namely, the mean of the bandpass averaged HWHM of the edge-detector universal focal plane modules match the simulated HWHM to 10.4 \%, with the discrepancy caused by fringing in the simulation. The measured radial profile of the beams matches simulations to within 2 dB from the beam center to at least the -10 dB level. Holography estimates of the far-field 90 GHz beams match the full-width-half-maximum from simulation within 1\%, and the beam radial profiles deviate by less than 2 dB inside the central lobe. The success of the holography and thermal beam map experiments confirmed the optical performance were sufficient to meet the science requirements. SAT-MF1 was deployed to Chile in June, 2023. On-site observations are currently underway.","1 INTRODUCTION The Simons Observatory (SO) is a ground-based telescope array dedicated to creating high-resolution and high-sensitivity temperature and polarization maps of the cosmic microwave background (CMB). It is currently deployed to the Atacama Desert and is composed of three small aperture telescopes (SATs) and one large aperture telescope (LAT). These telescopes work in tandem to observe the CMB over 6 frequency bands and over a large range of angular scales, using a combined \approx 60,000 transition-edge sensor (TES) detectors [1]. The six observed frequency bands are: the low frequency (LF) bands centered at 30 and 40 GHz, the middle frequency (MF) bands centered at 90 and 150 GHz, and the ultra high frequency (UHF) bands centered at 220 and 280 GHz. The LF bands are used to observe low-frequency synchotron emission, the UHF bands are used to characterize galactic dust and foreground signal, while the MF bands are used to analyze the peak CMB signal. The LAT has a 6m aperture and an 8^{\circ} field of view, allowing for the LAT to resolve the CMB sky at arcminute angular resolution [2]. The LAT’s science goals focus on small-scale anisotropies. The SATs have a 42 cm aperture and an 35^{\circ} field of view. The SATs have an angular resolution of a half-degree, and will be used to study large-scale B-modes [3]. Each SAT holds one optics tube and has \approx 12,000 detectors. Two of them will be sensitive to MF (SAT-MF1 and SAT-MF2), and one will be sensitive to UHF (SAT-UFH). One of the primary science goals of SO is to observe or constrain the tensor-to-scalar ratio, r, on the order of r=0.01 with uncertainty of \sigma(r)<0.003 [4]. SO requires accurate measurement of its beam shape and precise characterization of systematic errors throughout the optical system to observe the CMB sky at the precision needed to constrain r. Intensity-only (thermal) beam maps are used in cosmology as confirmation and calibration of telescope optics, testing the angular response of the telescope as a thermal source scans across the field of view of the focal plane. The full-width-half-max (FWHM), ellipticity, and side-lobes of thermal beam maps provide an accurate characterization of the telescope beam shape [5], allowing for the deconvolution of the telescope beam from the observed CMB signal [6] [7]. It is ideal to create beam maps in the far-field of the telescope before deploying the telescope to the site. However, observing far-field calibration sources from the laboratory is challenging. Near-field thermal beam maps act as a practical, first-order test of the telescope optics. Systematic errors in the observed near-field beam can be studied, allowing for problems to be rectified in the laboratory. Collaborators at the University of Chicago in 2021 applied the holographic method to measure the near-field beam intensity and polarization of the LAT-test receiver optics tube, providing an estimate of the far-field beam response of the telescope [8], [9]. From late 2022 to early 2023, we performed near-field holography and thermal beam map experiments to study the telescope optics of SAT-MF1 at the University of California San Diego. In this proceedings, we validate the near-field beam shape of the Simons Observatory SAT-MF1 via the near-field thermal beam maps, and estimate its far-field beam shape via the holography experiment. Section 2 details the experimental setup for the holography and thermal beam map experiments, and outlines the analysis pipeline used to compare experimental results to simulations. Section 3 introduces the validation metrics of the optics, and discusses the results of the experiments compared against these metrics."
https://arxiv.org/html/2411.07304v1,Phase Space of Binary Black Holes from Gravitational Wave Observations to Unveil its Formation History,"Gravitational Wave (GW) sources offer a valuable window to the physical processes that govern the formation of binary compact objects (BCOs). However, deciphering such information from GW data is substantially challenging due to the difficulty in mapping from the space of observation to the space of numerous theoretical models. We introduce the concept of BCO Phase-Space that connects the observable space to the evolution trajectories of the BCO formation channels with cosmic time and apply it to the third GW transient catalog (GWTC-3) that brings new insights into probable astrophysical formation scenarios of nearly 90 events. Our study reveals that two events, GW190425 and GW230529, show an overlap with a BCO Phase Space trajectory of the same formation channel arising from a sub-solar mass black hole scenario that has grown into a higher mass by accretion, hinting towards the common primordial origin of both these sources. Though the actual formation channel is yet to be confirmed, with the availability of more GW events, the BCO Phase Space can delve into distinguishing features of different formation channels for both astrophysical and primordial origin and opens the possibility of bringing new and deeper insights on the formation and evolution of BCOs across all observable masses over most of the cosmic time.","Acknowledgments The authors express their gratitude to Michela Mapelli for reviewing the manuscript and providing useful comments as a part of the LIGO publication policy. This work is part of the ⟨data|theory⟩ Universe-Lab, supported by TIFR and the Department of Atomic Energy, Government of India. The authors express gratitude to the system administrator of the computer cluster of ⟨data|theory⟩ Universe-Lab and the TIFR computer center HPC facility for computing resources. Special thanks to the LIGO-Virgo-KAGRA Scientific Collaboration for providing noise curves. LIGO, funded by the U.S. National Science Foundation (NSF), and Virgo, supported by the French CNRS, Italian INFN, and Dutch Nikhef, along with contributions from Polish and Hungarian institutes. This collaborative effort is backed by the NSF’s LIGO Laboratory, a major facility fully funded by the National Science Foundation. The research leverages data and software from the Gravitational Wave Open Science Center, a service provided by LIGO Laboratory, the LIGO Scientific Collaboration, Virgo Collaboration, and KAGRA. Advanced LIGO’s construction and operation receive support from STFC of the UK, Max-Planck Society (MPS), and the State of Niedersachsen/Germany, with additional backing from the Australian Research Council. Virgo, affiliated with the European Gravitational Observatory (EGO), secures funding through contributions from various European institutions. Meanwhile, KAGRA’s construction and operation are funded by MEXT, JSPS, NRF, MSIT, AS, and MoST. This material is based upon work supported by NSF’s LIGO Laboratory which is a major facility fully funded by the National Science Foundation. We acknowledge the use of the following packages in this work: Numpy (van2011numpy, ), Scipy (jones2001scipy, ), Matplotlib (hunter2007matplotlib, ), and Astropy (robitaille2013astropy, )."
https://arxiv.org/html/2411.07082v1,A model-independent reconstruction of the matter power spectrum,"We propose a new model-independent reconstruction method for the matter power spectrum based on its time dependence and a combination of observations from different redshifts. The method builds on a perturbative expansion in terms of the linear growth function, with each coefficient in the expansion being a free function of scale, to be reconstructed from the data. When using the linear growth function of a specific cosmological model, e.g. \LambdaCDM, the reconstruction can serve as a consistency check for non-linear modeling in that given model, as well as a new method for detecting departures from the assumed model in the data. As an application, we show how using DES Y3 3x2pt and Planck PR4 CMB lensing data, assuming a \LambdaCDM linear growth and first order expansion, the reconstructed matter power spectrum P_{\rm m}(k) is compatible with that computed from \LambdaCDM and halo model. In particular, we show that the method reconstructs the non-linear part of P_{\rm m}(k) for k\gtrsim 1\ \rm{Mpc}^{-1} without the need of assuming a non-linear model.","The cosmological constant cold dark matter (\LambdaCDM) model has been very successful in fitting the state-of-art observations of background and linearly perturbed cosmology, especially the spectrum of temperature anisotropies in the cosmic microwave background (CMB) Aghanim et al. (2020); Aiola et al. (2020); Balkenhol et al. (2023), baryon acoustic oscillations (BAO) Alam et al. (2021) and distance measurements from type Ia supernovae (SNIa) Brout et al. (2022). However, with the advent of precision cosmology, some inconsistencies have started to arise between different recent observations, when interpreted within \LambdaCDM. The most prominent one is the so-called Hubble tension, in which the Hubble constant H_{0}, characterizing the current expansion rate of the Universe, derived from CMB+BAO assuming \LambdaCDM differs from that measured locally by Cepheid calibrated SNIa at 5\sigma Riess (2019); Rubin et al. (2023). Furthermore, the value of S_{8}\equiv\sigma_{8}\sqrt{\Omega_{m}/0.3}, \sigma_{8} being the mean amplitude of linear matter perturbation in a sphere of radius 8h^{-1}\ \rm{Mpc}, as measured by weak lensing surveys is 2-3\sigma smaller than that derived from CMB+BAO within \LambdaCDM Asgari et al. (2021); Abbott et al. (2022, 2023a). More recently, the first BAO data release from DESI, combined with CMB and some recent type Ia data Abbott et al. (2018); Rubin et al. (2023); Brout et al. (2022), also shows a preference for dynamical dark energy over a cosmological constant Adame et al. (2024), even hinting at non-minimally coupled gravity Ye et al. (2024). These inconsistencies, see e.g. Efstathiou (2024); Perivolaropoulos and Skara (2022); Abdalla et al. (2022) for some recent reviews, have triggered extensive discussions about testing \LambdaCDM against the newest data as well as explorations of new physics that could explain these tensions. In this context, it is crucially important to develop model-independent approaches to the observations, on one hand to check the consistency with \LambdaCDM, on the other hand to thoroughly assess potential inconsistencies and link them to new physics. In this paper we develop such a method for the matter power spectrum. The linear dynamics of large scale structure (LSS) is well-understood in \LambdaCDM as well as in many beyond \LambdaCDM models. With the help of the publicly available Einstein-Boltzmann solvers CAMB Lewis et al. (2000) and CLASS Lesgourgues (2011), as well as their extensions such as EFTCAMB Hu et al. (2014); Raveri et al. (2014) and hi_class Zumalacárregui et al. (2017); Bellini et al. (2020) it is now possible to produce subpercent level theoretical predictions for all cosmological observables within the linear regime for \LambdaCDM and most dark energy and modified gravity models of interest (see e.g. Bellini et al. (2018)). However, interpreting observational data from galaxy clustering and weak lensing Asgari et al. (2021); Abbott et al. (2022); Alam et al. (2021), often requires modeling of the non-linear regime as well. This will be crucial for Stage IV LSS surveys that are deploying in these years Laureijs et al. (2011); Mandelbaum et al. (2018). Lacking a proper modeling of the non-linear regime, one has to resort to conservative scale cuts which result in throwing away a significant part of the data, (see e.g. Abbott et al. (2018)). For cold dark matter, non-linear evolution can be studied via N-body simulations or emulators and fast approximate methods such as COLA, Bacco and Pinocchio, see Winther et al. (2015, 2019) for overviews and comparisons of N-body codes and emulators available for \LambdaCDM and beyond. As we progress towards smaller scales, it becomes important to model also the baryonic feedback, either via some prescription for the baryonic physics or with hydro-dynamical simulations ; this is already complicated and computationally expensive for \LambdaCDM van Daalen et al. (2011); Chisari et al. (2019), with the difficulties and uncertainties becoming larger for beyond \LambdaCDM models. Recently, it has been pointed out that the S_{8} tension might originate from incomplete knowledge of the non-linear sector Amon and Efstathiou (2022). Correspondingly, constructions of the full matter power spectrum agnostic to the exact non-linear physics have been proposed as an alternative way to interpret the data. For example, in Amon and Efstathiou (2022) it has been suggested to marginalizing over the non-linear correction with one parameter A_{\rm mod} by setting P_{\rm NL}=P_{\rm L}+A_{\rm mod}(P^{\rm theory}_{\rm NL}-P_{\rm L}); this has later been generalized to a reconstruction of P_{\rm NL} by promoting A_{\rm mod} to a function of scale and redshift Preston et al. (2023, 2024). In Broxterman and Kuijken (2024), the authors considered a reconstruction by expanding P_{m}(k,a) in powers of k and a. We propose a new model-agnostic reconstruction of P_{\rm{m}}(k,z) that exploits the different time dependence in its linear and non-linear parts, namely we write P^{(n)}_{\rm{m}}(k,z)=\sum_{i=0}^{n}\alpha_{i}(k)D_{g}^{2(i+1)}(z_{0},z) (1) where \alpha_{i}’s are k-dependent coefficients with the same dimension as P(k) and D_{g}(z_{0},z) is the linear growth factor from redshift z_{0} to z, with z_{0} being some reference redshift. From a perturbation theory point of view, Eq.(1) represents the time dependence of perturbative loop expansion and one might compute the analytic form of \{\alpha_{i}\} to finite order within a given cosmological model and non-linear prescription. For example, the effective field theory of large scale structure (EFTofLSS) Baumann et al. (2012); Carrasco et al. (2012) provides a systematic parameterization of \{\alpha_{i}\}. To this end, Eq.(1), with {\alpha_{i}} as free functions to reconstruct, is a more general series expansion of P_{\rm m} in both k and z independent of non-linear modeling, and thus can serve as a consistency test for the non-linear prescriptions. In this paper we present a proof of concept study of this idea, showing that it can already reconstruct the linear and non-linear matter spectrum simultaneously at k\gtrsim 1\ \rm{Mpc}^{-1} with current data."
https://arxiv.org/html/2411.07034v1,Investigating the intracluster medium viscosity using the tails of GASP jellyfish galaxies,"The microphysics of the intracluster medium (ICM) in galaxy clusters is still poorly understood. Observational evidence suggests that the effective viscosity is suppressed by plasma instabilities that reduce the mean free path of particles. Measuring the effective viscosity of the ICM is crucial to understanding the processes that govern its physics on small scales. The trails of ionized interstellar medium left behind by the so-called jellyfish galaxies can trace the turbulent motions of the surrounding ICM and constrain its local viscosity. We present the results of a systematic analysis of the velocity structure function (VSF) of the H\alpha line for ten galaxies from the GASP sample. The VSFs show a sub-linear power law scaling below 10 kpc which may result from turbulent cascading and extends to 1 kpc, below the supposed ICM dissipation scales of tens of kpc expected in a fluid described by Coulomb collisions. Our result constrains the local ICM viscosity to be 0.3-25\% of the expected Spitzer value. Our findings demonstrate that either the ICM particles have a smaller mean free path than expected in a regime defined by Coulomb collisions, or that we are probing effects due to collisionless physics in the ICM turbulence.","The intracluster medium (ICM) microphysics in galaxy clusters is poorly known. It is usually assumed to be a fluid dominated by Coulomb collisions, which can explain its large-scale properties (e.g., Sarazin, 1988). However, the properties of the gas density fluctuations (Zhuravleva et al., 2019), and the observational evidence of cosmic ray electron re-accelerated by ICM turbulence (e.g., Brunetti & Jones, 2014), indicate that the effective ICM viscosity is orders of magnitude smaller than expected from the thermal ion-ion Coulomb collisions (e.g., Spitzer, 1962). This is because the ICM is a ‘weakly collisional’ plasma due to various plasma instabilities, (e.g., Schekochihin, 2022, for a review) that can perturb the ICM magnetic field on small scales, thus strongly reducing the effective ICM particles’ mean free path, and, hence, also the effective viscosity of the fluid. Cluster galaxies can be used to probe the ICM properties, especially the so-called jellyfish galaxies (e.g., Smith et al., 2010; Ebeling et al., 2014; Fumagalli et al., 2014; Poggianti et al., 2017a; Boselli et al., 2022), that are the result of infalling spiral galaxies interacting with the ICM. These galaxies show characteristic ‘tails’ of ionized plasma, produced by the interstellar medium (ISM) being displaced outside of the stellar disk via ram pressure stripping (RPS, Gunn & Gott, 1972), where the ISM and the ICM can interact via mixing. Observational evidence of ISM-ICM mixing has been collected in various forms, from metallicity gradients along the tails (Franchetto et al., 2021), to extended X-ray emission associated with the H\alpha emission (e.g., Sun et al., 2010; Poggianti et al., 2019a; Campitiello et al., 2021; Sun et al., 2021; Bartolini et al., 2022), to peculiar optical line ratios (Poggianti et al., 2019b; Campitiello et al., 2021), to the presence of diffuse ionized gas (Tomičić et al., 2021; Pedrini et al., 2022), or large-scale magnetic fields accreted from the ICM (Müller et al., 2021). All these results point toward the fact that, in the tails of jellyfish galaxies, the stripped ISM dynamic is affected by the ICM small-scale motions, such as turbulence. Therefore, the diffuse ISM H\alpha emission might probe the ICM turbulent motions and, thus, trace the turbulent cascade down to its dissipation scale, hence constraining the ICM viscosity. Li et al. (2023) pioneered this method by analyzing the velocity structure function (VSF) in the tail of the nearby jellyfish galaxy ESO137-001. The VSF is a two-point correlation function that quantifies the kinetic energy fluctuations as a function of the scale l in a velocity field. The VSF can trace both the energy cascade expected in turbulent flows, and coherent motions such as collapse, rotation, or blast waves, which appear in the form of coherent velocity differences (e.g., Kolmogorov, 1941; Heyer & Brunt, 2004; Chira et al., 2019). For fluid particles in fully developed, homogeneous, and isotropic turbulence, Kolmogorov (1941) predicted that the VSF is well-described by a power-law relation which extends down to a dissipation scale, \eta, which depends on the fluid viscosity. Li et al. (2023) observed a turbulent cascade in the VSF of ESO137-001 extending for two orders of magnitude below the supposed dissipation scale, thus constraining the ICM viscosity to \sim0.01 of the predicted value. In this paper, we extend the VSF analysis to a sample of ten galaxies from the GASP survey111https://web.oapd.inaf.it/gasp/ (GAs Stripping Phenomena in galaxies with MUSE, Poggianti et al., 2017a). By operating on a larger sample, we can simultaneously test the results of Li et al. (2023), and explore the variation in ICM viscosity for different galaxy cluster properties and regions. The manuscript is structured as follows. In Section 2 we present the sample and the data preparation, from MUSE data processing to the VSF construction. In Section 7 we report and discuss the results, and in Section 4 we list the main caveats of our analysis. Finally, in Appendix A and B we report a series of diagnostic plots, in Appendix C we show the VSF for the entire sample, and in Appendix D we describe the simulation setup adopted to compare the observed VSFs. In this work we adopt the standard concordance cosmology parameters H_{0}=70\,\rm km\,s^{-1}\,Mpc^{-1}, {\Omega}_{M}=0.3 and {\Omega}_{\Lambda}=0.7. At the redshifts of the clusters in which galaxies are located, 1^{\prime\prime}\simeq 0.9-1.1 kpc."
https://arxiv.org/html/2411.06356v1,Measuring cosmic curvature with non-CMB observations,"The cosmic curvature \Omega_{K} is an important parameter related to the inflationary cosmology and the ultimate fate of the universe. In this work, we adopt the non-CMB observations to constrain \Omega_{K} in the \LambdaCDM model and its extensions. The DESI baryon acoustic oscillation, DES type Ia supernova, cosmic chronometer, and strong gravitational lensing time delay data are considered. We find that the data combination favors an open universe in the \LambdaCDM model, specifically \Omega_{K}=0.108\pm 0.056 at the 1\sigma confidence level, which is in 2.6\sigma tension with the Planck CMB result supporting our universe being slightly closed. In the \LambdaCDM extensions, the data combination is consistent with a spatially flat universe. However, the central value of \Omega_{K} is positive and has a significant deviation from zero. We adopt the Akaike information criterion to compare different cosmological models. The result shows that non-flat models fit the observational data better that the flat \LambdaCDM model, which adds evidence to the argument that flat \LambdaCDM is not the ultimate model of cosmology.","Whether our universe is spatially open, flat or closed is a fundamental issue in cosmology. A non-zero curvature would have profound implications for the primordial inflation paradigm and the ultimate fate of the universe. Measuring the sign and value of \Omega_{K} is of great significance for understanding the evolution of the universe and the nature of dark energy. The question has gained a lot of interest in the past few years, particularly in light of the Planck cosmic microwave background (CMB) observations Aghanim et al. (2020); Park and Ratra (2019); Handley (2021); Di Valentino et al. (2019); Efstathiou and Gratton (2020). It is found that the CMB data alone favor a slightly closed universe, \Omega_{K}=-0.044_{-0.015}^{+0.018} Aghanim et al. (2020). Spatial flatness is an indicator of inflation. If the universe is not flat, this would cast serious doubt on the possibility that inflation could have happened. This deviation from the flat universe is interpreted as the undetected systematics or new physics beyond the standard model of cosmology, i.e., the \Lambda cold dark matter (\LambdaCDM) model. The ability of CMB data to constrain \Omega_{K} is limited by the geometrical degeneracy which can be broken by including other observational data Zaldarriaga et al. (1997); Efstathiou and Bond (1999); Bond et al. (1997). For instance, the CMB data combined with the baryon acoustic oscillation (BAO) measurements give \Omega_{K}=0.0007\pm 0.0019, suggesting that our universe is flat to within the 1\sigma confidence level Aghanim et al. (2020). This is inconsistent with the result of CMB data alone. Some people expressed doubts about the soundness of such combinations, believing that there may be a “curvature tension” in the current data Handley (2021). Of course, there are different voices. Efstathiou and Gratton claimed that the CMB data are consistent with a flat universe Efstathiou and Gratton (2020). Whether this tension exists is still an open question. It should be pointed out that in addition to the possible curvature tension, there are other measurement inconsistencies that do exist between early- and late-universe observations, such as the measurements for the Hubble constant H_{0} and the amplitude of the matter power spectrum (see Ref. Verde et al. (2019) for a review). Remarkably, the H_{0} value measured by the Cepheid-supernova distance ladder is in above 5\sigma tension with that inferred from the CMB observation assuming \LambdaCDM Riess et al. (2022). When combining observations for parameter constraints, it is necessary to consider the measurement inconsistencies between the data. The existence of these tensions reduces the rationality of combining CMB with late-universe probes for measuring the curvature parameter. The motivation of the present work is to constrain the curvature parameter using only the late-universe observations. The mainstream late-time probes include BAO (standard ruler), type Ia supernova (SN, standard candle) and cosmic chronometer (CC, standard clock). For a long time before, these non-CMB observations could not well constrain the curvature parameter. However, the situation may have changed. Recently, the Dark Energy Spectroscopic Instrument (DESI) collaboration released the high-precision BAO data based on the precise observations of galaxies, quasars, and Lyman-\alpha forests Adame et al. (2024a, b, c). Earlier, the Dark Energy Survey (DES) program published the high-quality samples of SN Ia discovered during its five-year operation Abbott et al. (2024). Furthermore, more than 30 CC data have been available for cosmological parameter inference so far Moresco et al. (2022). We will utilize the data from these three probes. To further tighten the constraints, we will also consider the strong gravitational lensing time delay (TD) observations Suyu et al. (2010); Jee et al. (2019); Suyu et al. (2014); Chen et al. (2019a); Wong et al. (2017); Birrer et al. (2019); Rusu et al. (2020); Shajib et al. (2020); Agnello et al. (2017). These four probes observe the universe from different perspectives, and combining them is expected to break cosmological parameter degeneracies, thereby narrowing the constraints. Currently, many curvature measurements are achieved after making assumptions about the nature of dark energy, i.e., dark energy behaves like a cosmological constant \Lambda with an equation of state (EoS) of w=-1. However, the observational data leaves room for dark energy EoS to deviate from -1, which weakens the persuasiveness of the \Omega_{K} constraints assuming \LambdaCDM, since \Omega_{K} and w are strongly degenerate. In addition, many people believe that \LambdaCDM is not the ultimate model of cosmology. On one hand, it has some theoretical problems Weinberg (1989); Sahni and Starobinsky (2000); on the other hand, the CMB results for \LambdaCDM are in tension with some late-universe observations Verde et al. (2019). Cosmologists have conceived many theories beyond it to solve the problems and reconcile the tensions Guo et al. (2019). It is necessary to measure the curvature parameter in those extended models. In this work, we shall consider some typical extensions to the \LambdaCDM model, mainly for the evolutionary behavior of dark energy. We shall also consider the possible interaction between dark energy and dark matter. This paper focuses on measuring the cosmic curvature in \LambdaCDM and its extensions using the non-CMB observations, and study the impact of various extensions on \Omega_{K} constraints. The remainder of this paper is organized as follows. We briefly describe the methodology in Sec. II. Sec. III contains the observational data we adopted. We present the results and make some discussions in Sec. IV. Finally, we give our conclusions in Sec. V."
https://arxiv.org/html/2411.06190v2,"Gravitational reheating
formulas and bounds in oscillating backgrounds II: Constraints on the spectral index and gravitational dark matter production","The reheating temperature plays a crucial role in the early universe’s evolution, marking the transition from inflation to the radiation-dominated era. It directly impacts the number of e-folds and, consequently, the observable parameters of inflation, such as the spectral index of scalar perturbations. By establishing a relationship between the gravitational reheating temperature and the spectral index, we can derive constraints on inflationary models. Specifically, the range of viable reheating temperatures imposes bounds on the spectral index, which can then be compared with observational data, such as those from the Planck satellite, to test the consistency of various models with cosmological observations. Additionally, in the context of dark matter production, we demonstrate that gravitational reheating provides a viable mechanism when there is a relationship between the mass of the dark matter particles and the mass of the particles responsible for reheating. This connection offers a pathway to link dark matter genesis with inflationary and reheating parameters, allowing for a unified perspective on early universe dynamics.","The relation between the reheating temperature and the spectral index of scalar perturbations plays a crucial role in understanding the dynamics of the early universe, particularly in the context of inflationary cosmology Guth (1981); Linde (1982). Inflation, a rapid expansion of the universe in its earliest moments, produces scalar perturbations, which later evolve into the large-scale structures we observe today, see for instance Refs. Barrow and Turner (1981); Spokoiny (1984); Steinhardt and Turner (1984); Lucchin and Matarrese (1985); Hawking (1985); Lyth (1985); Belinsky et al. (1985); Mijic et al. (1986); Khalfin (1986); Silk and Turner (1987); Mijic et al. (1986); Burd and Barrow (1988); Olive (1990); Ford (1989); Adams et al. (1991); Freese et al. (1990); Wang (1991); Polarski and Starobinsky (1992); Liddle and Lyth (1992); Linde (1994); Barrow (1993, 1994); Vilenkin (1994); Peter et al. (1994); Sasaki and Stewart (1996); Barrow and Parsons (1995); Lidsey et al. (1997); Parsons and Barrow (1995); Liddle (1998); Guth (2000); Riotto (2003); Feinstein (2002); Ashcroft et al. (2004); Boubekeur and Lyth (2005); Conlon and Quevedo (2006); Ferraro and Fiorini (2007); Cheung et al. (2008); Chen et al. (2008); Baumann and Peiris (2009); Koivisto and Mota (2008); Pal et al. (2010); Martin et al. (2014a, b); Sebastiani et al. (2014); Hamada et al. (2014); Freese and Kinney (2015); van de Bruck and Paduraru (2015); van de Bruck and Longden (2016a, b); Ratra (2017); Rubio (2019); Giarè et al. (2019); van de Bruck and Daniel (2021); Forconi et al. (2021); Odintsov et al. (2023); Giarè et al. (2023); Jinno et al. (2024); Garfinkle et al. (2023). The spectral index, n_{s}, characterizes the distribution of these perturbations and offers insights into the physics of the inflationary period. In addition, the number of e-folds between the horizon crossing and the end of inflation is influenced by the reheating temperature, T_{\rm reh}. Since the spectral index n_{s} depends on this last number of e-folds, the reheating temperature impacts the predicted value of n_{s}. Higher reheating temperatures generally reduce the duration of the reheating phase, leading to fewer e-folds and a lower predicted value of the spectral index, while lower reheating temperatures result in more e-folds and a higher spectral index. As a consequence, by studying the connection between T_{\rm reh} and n_{s}, different inflationary models can be tested and compared with the observational data, such as the measurements from the Planck satellite Akrami et al. (2020). This relationship serves as a valuable tool for constraining models of inflation, and only those predicting a viable range of n_{s} consistent with observations, are considered plausible. In the context of gravitational reheating and considering a heavy scalar quantum field conformally coupled to gravity, the created particles must decay into lighter ones to reheat the universe. First, continuing with our previous work de Haro et al. (2024), we derive the formula for the reheating temperature as a function of the decay rate and the mass m_{\chi} of the produced particles, identifying viable values of m_{\chi} and the range of possible reheating temperatures. Using this range and considering the relationship between the reheating temperature and the spectral index, we are able to constrain the latter. Indeed, these spectral index values are more tightly constrained than those obtained experimentally by Planck’s data Akrami et al. (2020). On the other hand, in the absence of strong couplings between the inflaton and standard particles, gravitational reheating can produce a non-thermal spectrum of particles, including potential dark matter candidates. This mechanism is especially intriguing in models where dark matter particles have minimal interactions with ordinary matter. During gravitational reheating, dark matter production can occur efficiently through gravitational interactions alone, providing a natural and model-independent production pathway. This scenario could help explain the relic density of dark matter observed today without requiring direct couplings to the inflaton field. Having in mind this fact, we investigate the gravitational production of dark matter within the framework of gravitational reheating. Specifically, we consider two distinct scalar fields conformally coupled to gravity, which generate two types of particles with different masses. One type decays into lighter particles, facilitating the reheating of the universe, while the other, a candidate for explaining the dark matter content, does not decay. In this work, we determine the range of particle masses that yield both a viable reheating temperature and a present-day value for the cold dark matter energy density, thereby contributing to a more complete understanding of the dark matter and energy composition of the universe. The paper is structured as follows: In Section II, we use the Wentzel-Kramers-Brillouin (WKB) method in the complex plane to analytically calculate the \beta-Bogoliubov coefficient, the key component for determining particle production. In Section III, we constrain the viable values of the spectral index by relating it to the reheating temperature through the last few e-folds. In Section IV, we derive an analytical formula for the reheating temperature when reheating occurs via gravitational particle production. This formula allows us to identify the viable mass range for the produced particles and further constrain the spectral index. Section V addresses the gravitational production of dark matter, where we establish the relationship between the mass of dark matter and that of the scalar field responsible for gravitational reheating. Finally, in Section VI we summarize the present work. Throughout the manuscript we use natural units, i.e., \hbar=c=k_{B}=1, and the reduced Planck’s mass is denoted by M_{\rm pl}\equiv\frac{1}{\sqrt{8\pi G}}\cong 2.44\times 10^{18} GeV."
https://arxiv.org/html/2411.06014v1,Non-linear Cosmological Perturbations for Coupled Dark Energy,We estimate the one-loop perturbation kernels for a minimal modified gravity model in which dark energy is coupled to dark mater via a constant coupling. We derive the time-dependent kernels via analytical and numerical solutions and provide accurate fitting functions. These kernels can be directly employed to test for modified gravity in forthcoming large-scale surveys.,"Testing gravity at cosmological scales [1, 2, 3] is finally becoming possible thanks to several large-scale surveys like BOSS [4], DES [5], DESI [6], Euclid [7] and, in the near future [8], LSST [9] and SKA [10, 11]. A common characteristic of most alternatives to Einstein’s gravity is the introduction of a non-minimal coupling of a scalar field to matter or to the metric, thereby adding at least a new parameter to the cosmological set. In this paper we focus on what could be classified as the simplest model of modified gravity: a constant coupling between a scalar field and dark matter. The scalar field can drive the cosmic acceleration, and therefore be a form of dark energy, but this is not a necessary condition. This dark-dark coupling bypasses the stringent conditions of local gravity since ordinary matter (i.e. baryons) are left uncoupled and feel only standard gravity. We refer to this model simply as coupled dark energy (CDE) [12, 13, 14, 15, 16, 17]. Previous work on the observable effects of this minimal model has focused on the linear regime (e.g. [18, 19, 20, 21, 22, 23]), corresponding to very large scales, typically larger than 50 Mpc/h. It is however now clear that the mildly non-linear regime, reaching down to roughly 10 Mpc/h, contains a wealth of additional information, allowing, in particular, to break some degeneracies among cosmological parameters. The extension of cosmological data analysis to the non-linear regime can be performed in two ways: either resorting to N-body simulations, or by going higher in perturbation theory. The first avenue is of course more powerful and is clearly preferable if one deals with a restricted class of models that can be reliably and efficiently simulated. In the present context of uncertainty about what could be the preferred class of models beyond \LambdaCDM, however, the perturbation approach has still some appeal, since it can straightforwardly encompass large classes of models. In this paper we consider therefore the next-to-leading order perturbation for a minimal dark-dark model, characterized by a coupling constant \beta and a generic scalar field potential. We consider two potentials: an exponential potential, as suggested by theoretical considerations [24]; and a linear approximation that is suitable when the scalar field does not change much during the relevant period (i.e., within the range of observations). The linear potential has the advantage that an analytical solution for the background can be obtained. In the limit in which the potential reduces to a cosmological constant, this coupled model introduces a single constant beyond standard, and represents therefore a minimal modified gravity model. In higher-order perturbation theory, the perturbation variables are expressed as convolutions of the first order variables. The main difficulty consists therefore in deriving the kernels of the convolutions. In this paper we obtain the kernels for a generic cosmology following the approach of [25] and then we specialize our results to the CDE model, providing analytical and/or numerical forms of the kernels as a function of time, of the coupling parameter, and of the potential slope. These forms are ready to be used in future work to forecast the performance of cosmological surveys and to analyse real data."
https://arxiv.org/html/2411.06000v1,Cosmology From CMB Lensing and Delensed EE Power Spectra Using 2019-2020 SPT-3G Polarization Data,"From CMB polarization data alone we reconstruct the CMB lensing power spectrum, comparable in overall constraining power to previous temperature-based reconstructions, and an unlensed E-mode power spectrum, with clear detections of the third through tenth acoustic peaks. The observations, taken in 2019 and 2020 with the South Pole Telescope (SPT) and the SPT-3G camera, cover 1500 deg2 at 95, 150, and 220 GHz with arcminute resolution and roughly 4.9 \muK-arcmin coadded noise in polarization. The power spectrum estimates, together with systematic parameter estimates and a joint covariance matrix, follow from a Bayesian analysis using the Marginal Unbiased Score Expansion (MUSE) method. The E-mode spectrum at \ell\,{>}\,2000 and lensing spectrum at L\,{>}\,350 are the most precise to date. Assuming the \Lambda{\rm CDM} model, and using only these SPT data and priors on \tau and absolute calibration from Planck, we find H_{0}\,{=}\,66.81\,{\pm}\,0.81 km/s/Mpc, comparable in precision to the Planck determination and in 5.4\,\sigma tension with the most precise H_{0} inference derived via the distance ladder. We also find S_{8}\equiv\sigma_{8}(\Omega_{\rm m}/0.3)^{0.5}\,{=}\,0.850\,{\pm}\,0.017, providing further independent evidence of a slight tension with low-redshift structure probes. The \Lambda{\rm CDM} model provides a good simultaneous fit to the combined Planck, ACT, and SPT data, and thus passes a powerful test. Combining these CMB datasets with BAO observations, we explore extensions to the \Lambda{\rm CDM} model. We find that the effective number of neutrino species, spatial curvature, and primordial helium fraction are consistent with standard model values, and that the 95% confidence upper limit on the neutrino mass sum is 0.075 eV, close to the minimum sum expected from observations of solar and atmospheric neutrino oscillations. The SPT data are consistent with the somewhat weak ({<}\,3\,\sigma) preference for excess lensing power seen in Planck and ACT data relative to predictions of the \Lambda{\rm CDM} model given the combined Planck, ACT, and BAO data sets. We also detect at greater than 3\,\sigma the influence of non-linear evolution in the CMB lensing power spectrum and discuss it in the context of the S_{8} tension. Forthcoming SPT-3G analyses will feature deeper and wider observations in temperature and polarization, providing even tighter constraints and more powerful tests of the \Lambda{\rm CDM} model.","Since the successful completion of the Planck satellite’s sky surveys [1], the frontier in the study of the cosmic microwave background (CMB) anisotropies has been its polarization. Here we present the strongest cosmological constraints to date derived from polarization data alone, constraints that are, by some measures, comparable to those from temperature data alone. Low-noise polarization measurements allow for new tests of the standard cosmological model, as the cosmological parameter inference relies on different signals, and they allow for additional robustness to many sources of potential systematic error. The data we use come from the SPT-3G camera [2] installed on the South Pole Telescope (SPT) in early 2017. By October of 2024, SPT-3G had been used to survey 10,000 deg2, with the bulk of the Austral winter observing time spent on a 1500 deg2 patch. The resulting data allow for powerful tests of the standard cosmological model, \Lambda{\rm CDM}, via the primary and CMB lensing signals [3], improved constraints on primordial gravitational waves via de-lensing of the degree-scale observations of the BICEP/Keck Collaboration [4], and many other science applications [5, 6, 7, 8, 9, 10, 11]. This paper is the first of a series of papers on CMB power spectra, the CMB lensing power spectrum, and cosmological parameter estimates inferred from the 2019 and 2020 Austral winter observations of the 1500 deg2 patch. The temperature and polarization maps used in these analyses are the result of four times as much observing time and twice as many detectors as were used for similar analyses that used a half season of data taken in 2018 with a partial SPT-3G focal plane [12, 13, 14, 15]. This paper is distinct from the others in the coming series by its focus on polarization data only, and by its use of a novel algorithm for optimally and simultaneously estimating the CMB lensing power spectrum, the unlensed111We note a subtle distinction between a de-lensed spectrum, which contains a residual lensing component which must be modeled, and our inference of an unlensed spectrum, which contains no such residual, independent of cosmological model. Although of minor impact, this explains our usage of “unlensed” throughout the text. CMB E-mode polarization power spectrum (EE), and systematics parameters. This algorithm, the Marginal Unbiased Score Expansion (MUSE), was developed by Millea & Seljak [16] and Millea [17], and is applied to real data here for the first time. We present the analysis in some detail and emphasize its advantages, including straightforward incorporation of systematic effects into a statistical model, a lensing potential map with lower noise than if it were produced by a quadratic estimator [e.g. 18], and an unlensed EE power spectrum with reduced sample variance relative to any inference of the lensed EE power spectrum [19]. De-lensed power spectra from quadratic estimators have been estimated previously from data by Planck Collaboration et al. [20] and Han et al. [21]. The methodology for optimal lensing reconstruction has been principally developed in these papers: Hirata & Seljak [22], Carron & Lewis [23], Millea et al. [24, 25], Millea & Seljak [16]; and an application to SPTpol data was described in Millea et al. [26]. With the SPT-3G data we analyze here, we are crossing the threshold of polarization noise level below which optimal methods lead to significant improvements in lensing reconstruction. We focus solely on polarization anisotropies in order to avoid modeling extragalactic foregrounds and their non-Gaussian properties. Such modeling would likely be necessary for a similar analysis which included temperature anisotropies, where the relative impact of foregrounds is larger. Due to the low noise and high angular resolution of the SPT-3G measurements, this initial conservative approach still leaves us with an inferred CMB lensing power spectrum with high signal-to-noise ratio. In the part of sky we observe, over the angular scales used in the analysis, galactic foregrounds are also negligibly small [12, 27]. We compare with other CMB measurements in both temperature and polarization, including those from the Planck Collaboration [28] and the Atacama Cosmology Telescope (ACT) Collaboration [29, 30, 31, 32]. We check for consistency of our results in the context of the \Lambda{\rm CDM} model. We also determine cosmological parameters assuming the \Lambda{\rm CDM} model and extensions from SPT data alone and in combination with these other datasets. Although the SPT observations we use here cover much less sky relative to Planck and ACT, the very low noise (4.9 \muK-arcmin in coadded polarization maps) and arc-minute angular resolution enable measurements of the EE power spectrum and CMB lensing potential power spectrum (\phi\phi) that are the most precise measurements ever made at \ell\,{>}\,2000 for EE and L\,{>}\,350 for \phi\phi. We discuss how the signals in our data allow for the determination of cosmological parameters assuming the \Lambda{\rm CDM} model. The origins of these determinations have interesting differences compared to those from Planck CMB data. For example, the Hubble constant (H_{0}) inference depends heavily on the lensing information; without it, the error increases by more than 10 times. Consistency between the Planck and SPT inferences of H_{0} places further constraints on attempts to solve the H_{0} tension [33] with changes to cosmological models [34, 35]. We pay particular attention to the implications of our data for the “negative neutrino mass” problem and its relation to excess lensing power pointed out by Craig et al. [36], and to the suggestion from Amon & Efstathiou [37] that the \sigma_{8} tension is associated with non-linear evolution, either due to mismodeling the \Lambda{\rm CDM} predictions or by neglect of some new physics that modifies evolution on non-linear scales. The rest of this paper is constructed as follows. In Sect. II we describe the process for making CMB maps from real and mock observations. We give an overview of MUSE and our data modeling in Sect. III. We present our pipeline and data validation processes, the blinding procedure, and results in Sect. IV. We show the resulting bandpowers and cosmological parameter constraints in Sect. V. We conclude in Sect. VI. Throughout this work we use a series of abbreviations for external datasets tabulated in Tab. 3."
https://arxiv.org/html/2411.05965v1,Running gravitational constant induced dark energy as a solution to \sigma_{8} tension,"We consider a modified gravity model with a running gravitational constant coupled to a varying dark energy fluid and test its imprint on the growth of structure in the universe. Using Redshift Space Distortion (RSD) measurement results, we show a tension at the 3\sigma level between the best fit \LambdaCDM and the corresponding parameters obtained from the Planck data. Unlike many modified gravity-based solutions that overlook scale dependence and model-specific background evolution, we study this problem in the broadest possible context by incorporating both factors into our investigation. We performed a full perturbation analysis to demonstrate a scale dependence in the growth equation. Fixing the scale to k=0.1h Mpc-1 and introducing a phenomenological functional form for the varying Newton coupling G with only one free parameter, we conduct a likelihood analysis of the RSD selected data. The analysis reveals that the model can bring the tension level within 1\sigma while maintaining the deviation of G from Newton’s gravitational constant at the fifth order.","The standard model of cosmology, namely the \LambdaCDM model, provides an excellent description of the expansion history of the universe that ties together the density fluctuations observed in the cosmic microwave background (CMB) with the current observations of large scale structures (LSS) [1, 2, 3, 4]. Over the last three decades, the measurements of the cosmological parameters have been continuously improving and at present we are in the era of precision cosmology. The parameters of the \LambdaCDM model have been measured with remarkable accuracy by the Planck mission in recent times and we use these values to define the standard model as Planck18/\LambdaCDM [1]. However, the model seems incomplete as the nature of its major constituents dark energy (DE, which appears to be well described by the cosmological constant \Lambda) and cold dark matter (CDM) are still unknown. At the same time, the model requires an ad-hoc period of accelerated expansion in the early universe in order to have a nearly scale invariant initial condition [5, 6, 7]. To make matters worse, the increased precision of current observations and new analysis techniques have brought to light several tensions between values of different parameters among different observables and experiments. For example, some independent cosmological observations in the redshift range z\lesssim 0.6 are in disagreement with Planck18/\LambdaCDM. Two of such major tensions are one in the local measurement of the Hubble parameter H_{0} (the so-called Hubble tension), and another in the estimates of the amplitudes of the matter power spectrum on the scales of 8h^{-1} Mpc (often referred to as \sigma_{8} tension). Both appear to be in conflict with the corresponding values obtained from Planck18/\LambdaCDM. Although the tensions might arise as a result of systematic errors, their persistence after several years of data analysis hints towards a possible breakdown of the standard scenario and the eventual requirement of new physics (see e.g. [8] for a recent review). In the present article, we shall attempt to address the so-called \sigma_{8} tension. The parameter \sigma_{8} describes the variance of the linear matter perturbations on scales of the order of 8h^{-1} Mpc and it can be estimated from several observations, most notably, the weak lensing correlation function obtained by the CFHTLenS collaboration [9], the galaxy cluster count [10] and the Redshift Space Distortion (RSD) measurements [11, 12]. At present there exists a 2\sigma tension between the constraints of these local measurements on the matter density \Omega_{m} and the amplitude \sigma_{8} of matter fluctuation, and those from the Planck mission [13, 14]. This tension can be reconciled by modifications to the standard scenario and in the literature, there are several suggestions to alleviate the tension such as different clustering of dark matter at smaller and larger scales [15], a hot dark matter component [16], dark energy-dark matter interaction [17, 18, 19] and modifications of general relativity (GR) [20, 21]. However, many of these solutions are analytically complex, leading to the use of Planck18/\LambdaCDM background, or H(a) for simplicity, which can induce undesired bias in the cosmological parameters [22]. On the other hand, there exist some proposed models to alleviate the H_{0} tension, for example with an early dark energy component, which have consequences for the \sigma_{8} tension (see e.g. [23, 24]). It would be preferable that any proposal to alleviate one tension would not make the other worse. In this context, examining simpler, less intrusive modifications becomes essential. In this article, we consider a simple modification of GR [25] (which we shall call the Markov-Mukhanov or MM model), which introduces in the Einstein-Hilbert action a multiplicative coupling of gravity and geometry through a scalar function of the energy density \xi(\rho). This modification in the action leads to a running Newton’s constant G(\rho) and a corresponding induced dark energy component \Lambda(\rho), both of which are related to the coupling function. In principle, one is free to choose one of the three functions \xi, G or \Lambda and obtain the others from the model. Here we consider a model-independent parametrization of Newton’s constant and obtain the variable \Lambda from the MM prescription. We then perform a numerical analysis to constrain the parametrization of G in our model from late-time structure formation data. We show that the Markov-Mukhanov model can help reduce the \sigma_{8} tension, thereby shedding light on the proposed running and coupled nature of Newton’s constant and dark energy. Our findings hint towards a slightly more repulsive force (or slight increase in the running dark energy density) in the early times of matter domination which agrees with the results reported in [26]. However, the difference between the present analysis and [26] is that, we do not use the Planck18 baseline and let H(a) vary according to the specific dynamics of the MM model. The article is organized as follows: In Section 2.1, we outline the Markov-Mukhanov action and derive the background equations. In Section 2.2, we perturb the model to obtain the equation describing the evolution of the density perturbations. In Section 2.3, we introduce the model-independent parametrization of Newton’s constant and in Section 3.1, we discuss how the model can be incorporated into the standard data analysis methodology. Finally, Section 3.2 presents the findings obtained by fitting the parameterization of G with the data. Throughout the article, we work with the units c=\hbar=1."
https://arxiv.org/html/2411.05913v1,"Cross-correlating the EMU Pilot Survey 1 with
CMB lensing: Constraints on cosmology and galaxy bias with harmonic-space power spectra","We measured the harmonic-space power spectrum of galaxy clustering auto-correlation from the Evolutionary Map of the Universe Pilot Survey 1 data (EMU PS1) and its cross-correlation with the lensing convergence map of cosmic microwave background (CMB) from Planck Public Release 4 at the linear scale range from \ell=2 to 500. We applied two flux density cuts at 0.18 and 0.4mJy on the radio galaxies observed at 944MHz and considered two source detection algorithms. We found the auto-correlation measurements from the two algorithms at the 0.18mJy cut to deviate for \ell\gtrsim 250 due to the different criteria assumed on the source detection and decided to ignore data above this scale. We report a cross-correlation detection of EMU PS1 with CMB lensing at \sim5.5\sigma, irrespective of flux density cut. In our theoretical modelling we considered two redshift distribution simulation models that yield consistent results, a linear and a non-linear matter power spectrum, and two linear galaxy bias models. That is a constant redshift-independent galaxy bias b(z)=b_{g} and a constant amplitude galaxy bias b(z)=b_{g}/D(z). By fixing a cosmology model and considering a non-linear matter power spectrum, we measured a constant galaxy bias at 0.18mJy (0.4mJy) with b_{g}=2.32^{+0.41}_{-0.33} (2.18^{+0.17}_{-0.25}) and a constant amplitude bias with b_{g}=1.72^{+0.31}_{-0.21} (1.78^{+0.22}_{-0.15}). When \sigma_{8} is a free parameter for the same models at 0.18mJy (0.4mJy) with the constant model we found \sigma_{8}=0.68^{+0.16}_{-0.14} (0.82\pm 0.10), while with the constant amplitude model we measured \sigma_{8}=0.61^{+0.18}_{-0.20} (0.78^{+0.11}_{-0.09}), respectively. Our results agree at 1\sigma with the measurements from Planck CMB and the weak lensing surveys and also show the potential of cosmology studies with future radio continuum survey data.","A primary goal of large-scale structure experiments probing the late Universe, is to provide answers on the history of the growth of cosmic structures and also discover the nature of the unknown components that dominate in the Universe leading it to its recent accelerated expansion (Huterer, 2023). To achieve this, the tracers we choose should be able, on one hand, to cover a large patch of the observed sky, accessing this way both large and small cosmological scales and, on the other hand, to be deep enough so that we can reconstruct the growth of structure history as a function of time. However, these probes alone, are not able to address these aspects simultaneously. For instance, probes like weak gravitational lensing on galaxies, which is the effect of the distortions of galaxy shapes caused by the underlying matter field between us and the galaxies, or on the cosmic microwave background (CMB) (Bartelmann & Schneider, 2001), is an unbiased tracer of the matter field in the Universe. Nonetheless, it provides poor information on the redshift evolution of the galaxies and also has lower statistical power compared to the other large-scale structure probe, called galaxy clustering. This probe, though, is a biased tracer of the total matter field and the modelling needed to connect the two has been proven to be quite complex (Kaiser, 1987; Sánchez et al., 2016; Abbott et al., 2018; Desjacques et al., 2018). One way to overcome this and reconstruct the growth of structures, is to use redshift-space distortions in case there are accurate redshift estimates which are obtained spectroscopically (Guzzo et al., 2008; Blake et al., 2013; Howlett et al., 2015; Pezzotta et al., 2017; Alam et al., 2021). Another way to overcome the limitations from the individual experiments, is to combine weak lensing and galaxy clustering data measurements (Hu, 2002; de la Torre et al., 2017; Peacock & Bilicki, 2018; Wilson & White, 2019; Heymans et al., 2021; White et al., 2022; García-García et al., 2021; Alonso et al., 2023). In addition, this multi-tracing increases the statistical power by accessing as much information as possible in the different cosmological scales as well as in redshift. In this framework, there has been a growing interest in deep radio continuum galaxy surveys. These surveys have the ability to scan enormous patches of the sky thanks to the large field of view of modern radio interferometers operating at low frequencies. There has been a variety of forecasting analyses in the literature arguing for their cosmological potential using the Square Kilometer Array (hereafter SKAO Raccanelli et al., 2012; Jarvis et al., 2015; Maartens et al., 2015; Bacon et al., 2020) and also the benefit reaped when different radio populations are combined in a multi-tracer approach. In particular, several ultra-large scale effects can be detected with multi-tracing such as relativistic effects and the primordial non-Gaussianity (Ferramacho et al., 2014; Alonso et al., 2015; Fonseca et al., 2015; Bengaly et al., 2019; Gomes et al., 2019). When observing the Universe at frequencies between 0.1-10 GHz, wavelengths larger than those in optical and infrared, the main radio continuum emission mechanism is synchrotron radiation111Although, at 5-10GHz free-free emission starts to be also important. (Condon, 1992). This is caused by relativistic electrons as they spiral in the magnetic fields. For this reason, the dominant populations of the radio galaxies are active galactic nuclei (hereafter AGNs), and star forming galaxies (hereafter SFGs). Regarding AGNs, there is a variety in the origin of sources as well as in their classifications. This includes the accretion mechanism of infalling material into central supermassive black holes (Best & Heckman, 2012; Heckman & Best, 2014), AGN orientation with respect to the observer (Antonucci, 1993; Urry & Padovani, 1995) and also their morphology (Type I & II, Fanaroff & Riley, 1974). As for SFGs, these are mainly spiral galaxies and they fall into two main categories. First is starburst galaxies, in which intensive star formation is present (star formation rate \gtrsim 100\text{ M}_{\odot}\text{yr}^{-1}). The other category is normal star forming galaxies (star formation rate \lesssim 100\text{ M}_{\odot}\text{yr}^{-1}) (Alonso et al., 2021). One of the main advantages of observations at these frequencies is that dust contamination is negligible in the line-of-sight direction as well as in the intergalactic medium due to the long wavelengths at radio frequencies. This is especially relevant for SFG studies where their radio emission is an unbiased probe of the star formation rate (Bell, 2003; Davies et al., 2016; Gürkan et al., 2018). There have been a number of past large-area radio continuum experiments like the NRAO VLA Sky Survey (NVSS at 1.4GHz, Condon et al., 1998; Hotan et al., 2021), the TIFR GMRT Sky Survey (TGSS-ADR at 150MHz, Intema et al., 2017) and the Sydney University Monongolo Sky Survey (SUMSS, Mauch et al., 2003). However, the current generation of radio surveys like the Australian Square Kilometre Array Pathfinder (hereafter ASKAP, Johnston et al., 2007), the Meer Karoo Array Telescope (hereafter MeerKAT, Jonas, 2009) and the LOw Frequency ARray (hereafter LOFAR, van Haarlem et al., 2013), all of them precursors of SKAO, make an advance through much deeper observations together with the large sky coverage. In particular, ASKAP has a field of view of \sim 30\text{ deg}^{2} operating at 700-1800MHz thanks to its phased array feeds. LOFAR, similarly, has a field of view of \sim 30\text{ deg}^{2} at 150MHz, while MeerKAT has a field of view of \sim 1\text{ deg}^{2} at 1.2GHz. With these large fields of view achieved with radio interferometers, large and contiguous patches of the sky can be observed, accessing in this way large-scale structure information at very large scales (angular separations). In this work we use the Pilot Survey 1 of the Evolutionary Map of the Universe (hereafter EMU PS1, Norris et al., 2011; Norris et al., 2021) which uses ASKAP at 944MHz, covering a contiguous patch of \sim 270\text{ deg}^{2} at a depth of 25-30 \mu\text{Jy/beam rms (root mean square)} and with a spatial resolution of 11-18 arcsec. By the end of its operation, EMU will cover the whole of the southern sky. As already mentioned, the radio continuum emission mechanism is synchrotron radiation, whose spectrum is a featureless smooth power law. The absence of any sharp lines, renders redshift measurements impossible222Techincally, some sources can have spectral lines and the issue is that any potential frequency information is collapsed down.. This results in large uncertainties on the redshift distribution of the galaxy sample and its properties, like the mass of host halos and galaxy bias. To shed light on radio sources’ clustering properties, one solution is to cross-match with optical sources (Lindsay et al., 2014; Hale et al., 2017; Mazumder et al., 2022). Radio continuum sources overlap in redshift with the CMB lensing convergence field. This probe is sensitive to inhomogeneities of the matter distribution at high redshifts (peaking at z\sim 2) and at comparable large volumes, making it ideal for cross-correlations with radio galaxies (Planck Collaboration et al., 2014; Allison et al., 2015) and also in the context of de-lensing studies (Namikawa et al., 2016). Previous works on cross-correlation of radio galaxies with CMB lensing include Smith et al. (2007), where this combination was used to make the first CMB lensing detection and also Allison et al. (2015) and Piccirilli, et al. (2023) to infer the galaxy bias of radio galaxies. Furthermore, the first and second data releases of the LOFAR Two-metre Sky Survey (LoTSS; Shimwell et al. 2019, 2022) radio catalogues were cross-correlated with CMB lensing from the Planck satellite (Planck Collaboration et al., 2020a), in order to constrain the redshift distribution and the galaxy bias of the sample (Alonso et al., 2021; Nakoneczny et al., 2024). These works have also shown that this cross-correlation can lift the degeneracy between the galaxy bias and the amplitude of the matter fluctuations. Here, we explore the auto-correlation and the cross-correlation of EMU PS1 with the latest CMB lensing convergence data (PR4) from Planck (Carron et al., 2022) to place constraints on the galaxy bias of the sample and on the matter fluctuations amplitude and leave the redshift distribution parameterisation of radio sources with the help of optical surveys for a future work. The paper is structured as follows: In Sect. 2 we describe the theoretical observables we use in our modelling. Then, in Sect. 3 we present the data we use in our analysis. In Sect. 4 we introduce the method used to construct the auto-correlation and cross-correlation measurements from the data, and also discuss the models and the error estimates we assume for our statistical analysis. The main results concerning the detection significance and the constraints on the galaxy bias and cosmology are shown in Sect. 5. Finally, we discuss our conclusions in Sect. 6."
https://arxiv.org/html/2411.05905v1,Robustness of Neural Ratio and Posterior Estimators to Distributional Shifts for Population-Level Dark Matter Analysis in Strong Gravitational Lensing,"We investigate the robustness of Neural Ratio Estimators (NREs) and Neural Posterior Estimators (NPEs) to distributional shifts in the context of measuring the abundance of dark matter subhalos using strong gravitational lensing data. While these data-driven inference frameworks can be accurate on test data from the same distribution as the training sets, in real applications, it is expected that simulated training data and true observational data will differ in their distributions. We explore the behavior of a trained NRE and trained sequential NPEs to estimate the population-level parameters of dark matter subhalos from a large sample of images of strongly lensed galaxies with test data presenting distributional shifts within and beyond the bounds of the training distribution in the nuisance parameters (e.g., the background source morphology). While our results show that NREs and NPEs perform well when tested perfectly in distribution, they exhibit significant biases when confronted with slight deviations from the examples seen in the training distribution. This indicates the necessity for caution when applying NREs and NPEs to real astrophysical data, where high-dimensional underlying distributions are not perfectly known.","One of the most striking open problems in modern astrophysics is that the nature of \sim 80\% of the matter content of the universe is remains unknown (e.g., Davis et al., 1985; Blumenthal et al., 1984; White et al., 1987; Bertone et al., 2005). This form of matter, called dark matter, is believed to consist of new, invisible particles that do not interact with regular matter electromagnetically. Shedding light on the nature of the dark matter particle is one of the main goals of modern cosmology and particle astrophysics (e.g., Drlica-Wagner et al., 2019). It has been well understood that different dark matter particle properties can result in different spatial distributions of dark matter structures on various scales (e.g., Kuhlen et al., 2012). Thus, by measuring the spatial distribution of dark matter it is possible to discriminate between different dark matter models. The most commonly accepted dark matter model, Cold Dark Matter (CDM) has been able to explain the observations of the large scale structure (e.g., the Cosmic Microwave Background, the Baryon Acoustic Oscillations, weak lensing, etc.) with great precision and accuracy (e.g., Hinshaw et al., 2013; Planck Collaboration et al., 2020). However, on subgalactic scales a number of discrepancies between the predictions of CDM and the observations of the dwarf satellites of the Milky Way have given rise to the possibility of alternative dark matter models (e.g., Kravtsov & Borgani, 2012). Strong gravitational lensing, the formation of multiple images of distant light sources due to the deflection of their light rays by the gravity of intervening structures, is a powerful probe of the subgalactic distribution of matter in the lensing galaxies and along the line-of-sight to the background sources due to its purely gravitational nature. Since different spatial distributions for the projected matter density can result in different distortions in the images, the analysis of lensed images allows the inference of these projected densities, including the abundance and distribution of subhalos. These can then be related to population-level parameters such as the halo mass function on these subgalactic scales, which are directly linked to predictions of dark matter models. However, measuring the effect of small halos on lensed images is a challenging, nonlinear inverse problem. The signal is weak and suffers from multiple degeneracies with other nuisance parameters, such as the morphology of the background source. Furthermore, the properties of a population of dark matter subhalos correspond to a high-dimensional space (e.g., the positions and masses of a large number of subhalos), making the inference of the abundance and distribution of subhalos a difficult problem for traditional, explicit likelihood modeling methods. Past works have introduced a number of approximations in an attempt to make the problem tractable, for example by assuming Gaussian priors, linearizing the lensing model, limiting the analysis to only modeling the effect of the most massive subhalos, or performing a power-spectrum analysis (e.g., Vegetti et al., 2010; Hezaveh et al., 2016b, a; Cyr-Racine et al., 2016; Birrer et al., 2017; Brennan et al., 2019; Despali et al., 2020). Despite these simplifications, these methods are still generally computationally costly, which limits the possibility of extensive testing them for potential biases. Neural network inference frameworks, such as Neural Ratio Estimators (NREs) and Neural Posterior Estimators (NPEs), have recently emerged as a promising solution to these problems since they can be trained to approximate the intractable likelihood or the posterior of parameter distributions directly from high-dimensional input data (e.g., Cranmer et al., 2015; Baldi et al., 2016; Papamakarios & Murray, 2016; He et al., 2016; Brehmer et al., 2018, 2018, 2020). In principle, both analysis frameworks can marginalize over large numbers of nuisance parameters and return an optimal, unbiased likelihood or posterior for the parameters of interest. Within the context of subhalo studies with strong gravitational lensing, this means directly inferring parameters describing the population-level distribution of subhalos (e.g., the subhalo mass function) from a collection of strong lensing systems, while marginalizing over nuisance parameters, including the source galaxy morphologies, the macro-lens parameters, and the parameters of an a priori unknown number of individual subhalos. Recent work has demonstrated the promise of such approaches to circumvent approximations of the intractable likelihood in strong gravitational lensing (e.g., Brehmer et al., 2019; Coogan et al., 2022; Mishra-Sharma, 2022; Zhang et al., 2022; Karchev et al., 2023; Wagner-Carena et al., 2023, 2024; Zhang et al., 2024). Despite their potential, these methods face challenges when applied to real observational data. While these methods have been shown to work well when tested on data coming from the same distribution as the training data, their performance on out-of-distribution (OOD) data is not guaranteed. These subhalo inference frameworks rely on producing large volumes of labeled, simulated data for training, requiring a detailed match between the distributions of real and simulated data for unbiased inference. However, even using the most sophisticated simulation pipelines, it is to be expected that there will always remain some level of mismatch between synthetic and real data distributions. Given the weakness of the expected signal of interest, this leaves NPEs and NREs vulnerable to biased inference. In this work, we investigate the performance of NREs and NPEs for the inference of the subhalo mass function in the presence of realistic distributional shifts between the training and test datasets. We produce several simulated datasets with minor variations in background source morphologies, the distribution of lens macro model parameters, subhalo profiles, and observational noise statistics and show that when the training and test datasets are distributionally shifted, the performance of the NRE and NPE can be dramatically affected. Note that these experiments are not meant to explore an exhaustive list of possible distributional shifts, but rather to establish the vulnerability of this class of methods to such shifts in strong lensing analysis. In Section 2 we describe the simulations and the inference frameworks. In Section 3 we describe the data generation process for the inference frameworks. In Section 4, we present the details of the distributional shifts considered. In Section 5 we present the performance of the inference frameworks on OOD tests and present our conclusions in Section 6. In what follows, we adopt a flat \LambdaCDM cosmology with parameters from Planck Collaboration et al. (2020)."
https://arxiv.org/html/2411.07164v1,"Gravitational wave propagation beyond General Relativity: 
geometric optic expansion and lens-induced dispersion","The nature of gravity can be tested by how gravitational waves (GWs) are emitted, detected, and propagate through the universe. Propagation tests are powerful, as small deviations compound over cosmological distances. However, GW propagation tests of theories beyond Einstein’s general relativity (GR) are limited by the high degree of symmetry of the average cosmological spacetime. Deviations from homogeneity, i.e. gravitational lenses, allow for new interactions, e.g., between standard GW polarization and new scalar or vector fields, with different spin. Therefore, GW lensing beyond GR offers novel tests of cosmological gravity. Here we present the theory of GW propagation beyond GR in the short-wave expansion, including corrections to the leading-order amplitude and phase for the first time. As an example, we compute the dispersive (frequency-dependent) corrections to all metric and scalar field perturbations in Brans-Dicke, the simplest modified theory exhibiting GW dispersion. GW lensing effects are too small to observe in Brans-Dicke theories compatible with solar system and binary pulsar limits. Nevertheless, our formalism opens the possibility of novel tests of gravity, including dark-energy theories and screening mechanisms.","Einstein’s General Relativity (GR) has been remarkably successful in describing gravitational phenomena on a wide range of systems, across vastly different scales. However, open questions remain on both extremely small and large scales. Small-scale/high-energy problems of GR include the nature of spacetime singularities and the quantum completion of the theory. On cosmological scales, GR has led to the need to include dark matter and dark energy, accounting for 95% of the universe’s content today [1, 2, 3, 4]. In particular dark energy, responsible for cosmic acceleration [5, 6, 7, 8, 5, 9, 10, 11], could be interpreted as a breakdown of the attractive nature of gravity on the largest scales. This hypothesis has resulted in a widespread investigation of alternative theories [12, 13, 14], as well as an ambitious observational program to test them using cosmological observations [15, 16, 17, 18, 19, 20, 21, 22, 23]. The detection of GWs [24] opened a new frontier in the study of gravitational physics, both illuminating the foundations of GR and casting away alternative theories to the shadows. Observed GW signals are emitted by relativistic compact objects, thus probing the regime of strong-field and dynamical gravity [25, 26, 27]. In addition to emission, GW propagation across the universe are highly sensitive to other properties of gravity, such as the GW speed [28] and the graviton mass [29]. GW propagation tests probe gravitational interactions directly and can achieve high sensitivity, as anomalous effects accumulate over cosmological distances. Most crucially, they apply directly to theories that modify cosmological dynamics, constraining many dark-energy theories [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]. Many GW propagation tests, including measuring the anomalous speed and amplitude, are limited by require an electromagnetic counterpart or otherwise determining the redshift of the source. Moreover, only a handful of effects exist on the homogeneous and isotropic Friedman-Robertson-Walker (FRW) cosmological background [42, 43] that describes the average universe. GW propagation over inhomogeneous spacetimes, i.e. GW lensing, drastically extend the range of phenomena that can be used to test gravity. The basic principle is that lenses/inhomogeneities break the FRW symmetries, allowing interactions between fields of different spin. In alternative theories, this means that the GR standard degrees of freedom (d.o.f.), the +,\times metric polarizations, mix with new fields such as scalars or vectors. In addition to the many gravitational lensing effects in GR [44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57], lens-induced effects provide a powerful discriminant between theories through novel wave-optic pheonomena. Interactions with new fields involving two derivatives cause lens-induced birefringence (LIB), a difference in speed between the + and \times components of GWs [58]. The absence of birefringence in GW data provided constraints on cosmological theories of gravity comparable to those based on the GW speed [59]. Related birefringent effects are predicted in strong gravitational fields in GR [60, 52, 53], as well as in parity-violating theories [61]. Although progress has been made to characterize GW lensing beyond GR [58, 62, 63, 64, 65, 66], computational difficulties and the wide landscape of theories have prevented a systematic characterization of these phenomena. One of the ways in which the theory of GW lensing beyond GR needs to be developed is by including frequency-dependent propagation effects. Most analyses rely on the geometric optics (GO) approximation [67, 63, 62, 68, 69, 60, 70, 71], which holds when the signal’s wavelength is significantly smaller than the local curvature scale, the i.e. short-wave approximation. In this regime, GWs travel along null geodesics, and their polarization is parallel-transported along them [60, 69, 51]. Corrections beyond geometric optics (bGO) [72, 73, 74, 75, 55, 76] include lens-induced dispersion (LID) on GW signals. Such modifications, by the nature of the short-wave approximation, are frequency-dependent and can be probed by GW interferometers.The characterization of the bGO regime beyond GR remains an open problem. A significant step forward was the first explicit computation of bGO corrections in GR [72, 73], showing how bGO effects modify the amplitude and phase of GWs and discussing the emergence of apparent additional scalar polarizations modes, absent in the geometric optics limit. GW lensing beyond GR offers potential for even richer phenomena due to the presence of additional degrees of freecom [77]. In this work, we extend the study of GW lensing beyond GR, developing a formalism that incorporates bGO corrections that describe dispersive phenomena. For simplicity, we focus on the Brans-Dicke theory as an example of scalar-tensor models [78, 79]. Using the short-wave expansion approximation, we compute the bGO corrections to the leading-order scalar observables for tensor and scalar waves passing through a point-like gravitational lens. The work is organized as follows: In Sec. II we will introduce a general framework describing the full-propagation of the gravitational and scalar radiation. By employing the short-wave approximation we will show the general equations, order by order in the expansion parameter, the equations governing the geometric optics regime and the first corrections to it. Sec. III will be devoted to review calculations for GR, thus introducing the null-tetrad formalism and thereby presenting the geometric and beyond geometric optics equations and the respective formal solutions for the gravitational radiation. In Sec. IV we will present the full propagation in the BD theory and show the solution in the geometric optics regime of the leading-order scalar and tensor amplitude. Then we will present, for the first time up to our knowledge, the general expression describing beyond geometric optics (bGO) corrections. In Sec. V, we will explicitly evaluate the dispersive bGO corrections, analytically, in the special case of a point-like lens, reviewing the GR case shown in Ref. [73] and subsequently extending to BD. Notation. Table. 1 sumarizes the main definitions that we will use throughout the work. Note that we will employ two equivalent descriptions of the theory, the Jordan (JF) and Einstein frame (EF). We will work with c=\hbar=1. Symmetrized and anti-symmetrized indices will be denoted as T_{(\mu\nu)}\equiv(T_{\mu\nu}+T_{\nu\mu})/2 and T_{[\mu\nu]}\equiv(T_{\mu\nu}-T_{\nu\mu})/2. Fields Amplitude decomposition GR \tilde{h}_{\mu\nu}, Eqs. (21) \tilde{h}^{(n)}_{\mu\nu}\equiv\tilde{\alpha}^{(n)}_{AB}\Theta^{AB}_{\mu\nu}, Eq. (36) EF (\tilde{h}_{\mu\nu},\delta\tilde{\phi}), Eqs. (60) \tilde{h}^{(n)}_{\mu\nu}\equiv\tilde{\alpha}^{(n)}_{AB}\Theta^{AB}_{\mu\nu}, Eq. (36) JF (h_{\mu\nu},\delta\phi) {h}^{(n)}_{\mu\nu}\equiv{\alpha}^{(n)}_{AB}\Theta^{AB}_{\mu\nu} Table 1: Notation used in the work."
https://arxiv.org/html/2411.07045v1,The effective speed of sound in cosmological perturbation theory,"In a multi-field/fluid cosmological system consisting of a number of minimally coupled canonical scalar fields, non-canonical scalar fields and barotropic perfect fluids, we introduce a new definition of effective speed of sound of the entire system for describing the evolution of cosmological perturbations. This effective speed of sound is not just gauge invariant but also a background dependent quantity and therefore can be treated as a parameter to quantify perturbations in such multi-field/fluid systems. It is with this effective speed that the gauge invariant Bardeen potential and the curvature perturbation propagate at scales much smaller than the sound horizon. Further, the effective speed of sound defined in this paper generalizes the one defined by Garriga and Mukhanov for a single non-canonical scalar field to a system consisting of many minimally coupled barotropic perfect fluids, canonical and non-canonical scalar fields. Moreover, just like in the case of a single pure-kinetic non-canonical scalar field, this effective speed of the sound of the total system turns out to be identically equal to the total adiabatic speed of sound when the dynamic of the universe is driven by a number of pure kinetic non-canonical scalar fields making such a system tantamount to a system of equivalent multi barotropic perfect fluids. We also derive a set of equations which governs the evolution of perturbations in a general multi-field/fluid universe containing barotropic perfect fluids, canonical and non-canonical scalar fields. Using these equations we show that, in the large scale limit (k=0 scales), if the perturbations are initially adiabatic, then it continues to remain so at that scales throughout the evolution of the universe. Consequently, at that scales, such multi-field/fluid universe dynamically behaves as if it contains only a single barotropic perfect fluid.","Within the framework of Einstein’s general theory of relativity, the current understanding of the evolution of the universe, from inflationary epoch to the present accelerated expansion phase, is reasonably well established and is in concordance with the observational data [1, 2, 3, 4, 5, 6]. The expansion of the universe is driven by different components at various stages of its evolution. Scalar field drives the inflationary phase [7, 8, 9, 10, 11, 12, 13, 14, 15] followed by the radiation and cold dark matter dominated expansion phase which can be described as barotropic perfect fluids. By definition, barotropic perfect fluids are perfect fluids whose pressure is only a function of energy density at the background as well at all orders of perturbations [16, 17]. The nature of dark energy [18, 19, 20], driving the late time accelerated expansion of the universe, is not well understood, however, many models of dark energy such as cosmological constant and scalar field dark energy etc. are viable as far as observational data are concerned [21, 22, 23, 24, 25, 26]. Scalar fields having standard form of the lagrangian are classified as canonical scalars whereas those having a lagrangian which is a general function of the kinetic term X=(1/2)\partial_{\mu}\phi\partial^{\mu}\phi and the scalar field \phi are classified as non-canonical scalars [27]. Non-canonical scalar field have widely been studied to model inflation [28, 29] and also as a model of dark energy driving the late time accelerated expansion of the Universe [30, 31]. Unlike the canonical scalar field models of inflation with monomial power law potentials of the form V(\phi)=V_{0}\phi^{n}, non-canonical scalar field models with \mathcal{L}=X^{\alpha}-V_{0}\phi^{n}, where \alpha and n are constants, can leads to observationally viable values for scalar spectral index and the tensor-to-scalar ratio for integral values of n such as n=2 and n=4 [32, 33]. This is possible in non-canonical scalar field models because of it smaller speed of sound which consequently lowers the tensor-to-scaler ratio to a value well within the observational bounds [34, 35, 36]. Further, non-canonical scalar can also drive the late time accelerated expansion of the universe [37] and can also provide a unified description of dark matter and dark energy [38, 39, 40]. However, it is impossible to uniquely determine the fundamental nature of the non-canonical lagrangian merely from cosmological observations as two different non-canonical scalar fields models can leads to same observational effects [41]. For each type of matter content of the Universe, one can define an equation of state parameter w as the ratio of the pressure to the energy density. It is also possible to define the equation of state parameter of the total system consisting of different types of matter content in the universe. Once the equation of state parameter and its evolution is known, the rate of expansion of the universe can be determined from Friedmann equations. In addition to w, the effective speed of sound of matter content is an important parameter which determines the evolutions of perturbations in cosmological perturbation theory. For barotropic perfect fluids, the effective speed of sound is related to the equation of state parameter whereas for canonical scalars, it is identically equal to unity [42]. For non-canonical scalars, the effective speed of sound can take any value depending on the form of its lagrangian [27]. In a multi-fluid/field system, although, it is possible to define the effective speed of sound of each matter content separately, it is not known how, in a similar way, one can define an effective speed of sound of the total system. In cosmological perturbation theory, simply defining the square of speed of sound as the ratio of perturbation in pressure \delta p to that in energy density \delta\rho does not work in all cases. This is because both \delta p and \delta\rho are gauge depended quantities and in the case of scalar fields, the gauge in which \delta\rho=0 does not coincide with the gauge in which \delta p=0. Consequently, the speed of sound defined as \delta p/\delta\rho becomes infinite in that gauge and therefore this definition of speed of sound being gauge dependent is ambiguous. The speed of sound must depend on the property of matter not on the choice of gauge. One therefore defines the speed of sound as ratio of a gauge invariant pressure perturbation to the corresponding gauge invariant perturbation in energy density [41]. The speed of sound thus defined for a single scalar field or barotropic perfect fluid is not just gauge invariant but also a background depended quantity. In the case of multi-field system, it is possible to define an effective speed of sound as the ratio of pressure perturbation to the energy density perturbation in the comoving gauge [43, 44, 45]. However, this defines the speed of sound in a particular gauge and it also a space depended quantity. We, in this paper, introduces a new definition of the effective speed of sound of a multi-field/fluid driven universe which is not just gauge invariant but space independent (or k independent) and a background dependent quantity. Being a background dependent quantity, it can be treated as a parameter to quantify cosmological perturbations in a single or multi-field/fluid driven universe. This paper is organized as follows. In the following section we briefly review the first order cosmological perturbation with scalar degree of perturbations. All the quantities required to describe scalar degree of cosmological perturbations, their corresponding gauge invariant quantities and the equations governing their evolutions are described in that section. In Sec. III, we discuss the evolution of cosmological perturbation when the dynamics of the universe is driven by a single matter content which could be a barotropic perfect fluid or a scalar field (canonical or non-canonical scalar field. In that section, the effective speed of sound is introduced by considering the fact the relation between the quantities describing the matter perturbation must be independent of the choice of gauge. This analysis is extended in the case of a two-fluid/field driven Universe in Sec. IV and further extended to the multi-fluid/field system in Sec. V. In Sec. VI, the evolution curvature perturbation in the multi-fluid/field system is described and it is shown that at small scales it satisfies a wave equation with same speed of sound defined in Sec. V. A closed set of equations governing the evolution of perturbations in a multi-fluid/field driven universe is described in Sec. VII . Further, in Sec. VIII, it is shown that, in a multi-fluid/field system, if the perturbations are adiabatic at the super horizon scales, it will remain so throughout the evolution of the universe. In Sec. IX, the evolution of effective speed of sound in unified dark sector models are discussed. Finally in Sec. X, the main results of this paper are summarised. The conventions and the notations used in this paper are as follow. All the cosmological perturbations are described on a spatially flat Freidmann Robertson Walker metric with the metric signature (+, - , - , -). The units used in this paper are such that the speed of light is set to unity. An over dot denote derivative with respect to time while an over prime denote derivative with respect to conformal time."
https://arxiv.org/html/2411.07020v1,Premerger observation and characterization of massive black hole binaries,"We demonstrate an end-to-end technique for observing and characterizing massive black hole binary signals before they merge with the LISA space-based gravitational-wave observatory. Our method uses a zero-latency whitening filter, originally designed for rapidly observing compact binary mergers in ground-based observatories, to be able to observe signals with no additional latency due to filter length. We show that with minimal computational cost, we are able to reliably observe signals as early as 14 days premerger as long as the signal has accrued a signal-to-noise ratio of at least 8 in the LISA data. We also demonstrate that this method can be used to characterize the source properties, providing early estimates of the source’s merger time, chirp mass, and sky localization. Early observation and characterization of massive black holes is crucial to enable the possibility of rapid multimessenger observations, and to ensure that LISA can enter a protected operating period when the merger signal arrives.","The late 2030s will see the first data from the Laser Interferometer Space Antenna (LISA) gravitational-wave observatory [1, 2]. LISA will enable us to observe the gravitational-wave spectrum between \sim 10^{-4} Hz and \sim 10^{-1} Hz and allow the observation of a wide range of astrophysical phenomena [3]. One of the primary science motivators for LISA, and the focus of this work, is the observation of Massive Black Hole Binary (MBHB) mergers. LISA will have peak sensitivity to MBHB mergers with total mass between 10^{4} and 10^{7}M_{\odot} [3, 4, 5, 6, 7]. The problem of observing and characterizing MBHB mergers with LISA has been explored over the last two decades, primarily in the context of the ongoing LISA Data Challenges (formerly called the Mock LISA Data Challenges) [8, 9, 10, 11, 12, 13, 14, 15]. Such signals are expected to have a very large signal-to-noise ratio, rendering the identification of sources trivial [3]. However, this large signal-to-noise ratio makes the characteriziation of these sources, as performed using numerical Bayesian analysis toolkits, challenging [3, 8, 9, 10, 11, 12, 13, 14, 15]. The main problems that need to be solved in advance of LISA’s launch are how to characterize these signals in the presence of a large number of other overlapping signals—LISA’s “Global Fit” problem [16]—and how to produce waveforms with sufficient accuracy [17]. Tackling these well known problems is not the focus of this work. The large signal-to-noise ratio of MBHB mergers offers another interesting science possibility. In particular, these systems may be observable in the days (or even weeks) before they merge [18, 19]. If it were possible to identify these systems before they merge, and constrain the sky location where the signal originates from, it would open up the possibility of alerting electromagnetic astronomers that a MBHB merger is about to occur [18, 19]. As MBHB mergers can occur in gas-rich environments, the gravitational-wave signal may be accompanied by EM emission [20]. One of the Science Objectives for the LISA Mission is to “Identify the electromagnetic counterparts of massive Black Hole binary coalescences” with a stated aim to alert astronomers hours to days before merger [2]. In addition, it would be very useful for operation of the LISA satellite to know that an MBHB merger is imminent. LISA science data will contain schedulded interruptions due to antenna re-pointing or discharge of test masses. If we know the merger time of a MBHB in advance, LISA can enter a protected period in which scheduled interruptions are rearranged [2]. Depending on ground station availability, LISA can also enter a low-latency period in which data is downloaded in quasi-real time for the merger itself [2]. The topic of premerger observation of MBHBs and in particular the ability to localize MBHBs premerger has been explored already in the literature (for e.g. [21, 22, 23, 24, 25, 26, 18, 19, 27, 28, 29]). However, the majority of these works are carried out using Fisher Matrix analyses, or Bayesian inference using waveforms that are abruptly terminated in the frequency domain. As we will discuss and explain in detail below, traditional likelihood computations would require waiting for additional data (up to a day) to arrive before one can accurately whiten the waveforms and compute a matched filter. We do note the recent works of [29, 30], which propose a direct search using convolutional neural networks and could provide an alternative to the methods presented here. In this work, we present an end-to-end method for identifying and characterizing MBHB signals before merger. This method incorporates methodology developed by the GstLAL search team in the context of ground-based searches [31], to allow us to analyse LISA data immediately as it arrives. We illustrate the utility of the method by demonstrating the recovery of signals added to simulated LISA noise between 0.5 and 14 days before merger. We also demonstrate that this method can be used to characterize these simulated signals without needing to wait for additional data to arrive, by interfacing with one [32] of the Bayesian inference toolkits now available for LISA parameter estimation [33, 34, 35, 36, 37]. We highlight the importance of having LISA data available below 10^{-4}\,Hz: Many signals will have dominant mode emission <10^{-4}\,Hz in the days before merger, and so having sensitivity below this will be crucial for early identification and localization. The layout of this work is as follows: In section II we explore the motivation for this work and describe the data and noise curves that we will consider. In section III we describe our new technique for observing and characterizing MBHB systems in the days to weeks before merger, applying techniques developed in the LIGO-Virgo-KAGRA (LVK) context to achieve minimal latency. In section IV we demonstrate that we can effectively observe MBHB mergers, as soon as the accrued signal-to-noise ratio becomes larger than 8, with insignificant computational cost. In section V we apply Bayesian parameter inference tools to demonstrate that this method can be used to provide rapid premerger constraints on the value of the MBHB’s sky location, chirp mass and time of arrival. Finally, we conclude in section VI. All of our results and figures are fully reproducible, with the code used to make them publicly available. Please see our https://icg-gravwaves.github.io/lisa_premerger_paper/ data release page for more detail."
https://arxiv.org/html/2411.06957v1,Orthogonal splitting in degenerate higher-order scalar-tensor theories,"We explore a comprehensive analysis of the formalism governing the gravitational field equations in degenerate higher-order scalar-tensor theories. The propagation of these theories in the vacuum has a maximum of three degrees of freedom and is at most quadratic in the second derivative of the scalar field. We investigate the gravitational field equation for spherically symmetric anisotropic matter content along with its non-conserved equations. Our analysis focuses on the evaluation of structure scalars to assess their behavior under Einstein’s modification. We present a realistic mass contribution that sheds light on both geometric mass and total energy budget evaluations for celestial objects. Ultimately, we discuss two viable models restricted as minimal complexity and conformal flatness to enhance the scientific contribution of the present manuscript.","The theory of General Relativity (GR) einstein1915feldgleichungen ; bonnor1981junction ; scientific2016tests provides an all-encompassing explanation for the nature of space, gravity and matter. The basic principles of the structure and geometry of spacetime are covered by GR. The Friedmann equations, derived from the Einstein field equations, depict how the universe evolves in a homogeneous and isotropic spacetime. The conventional model of the early universe, which involves epochs dominated by radiation giudice2001largest and matter freese2002cardassian ; kokubu2018effect , can be adequately explained through GR. Modern astrophysics has brought to our attention the undeniable fact that a significant amount of “dark matter” and “dark energy” huterer1999prospects ; padmanabhan2003cosmological ; amendola2010dark exist. These mysterious entities are necessary for justifying observable cosmic dynamics farajollahi2011cosmic ; sami2012cosmological and large-scale structures tegmark2004cosmological ; springel2006large ; carrasco2012effective . Without them, we may continually face a gap between theories and observations. Therefore, acknowledging this discovery leads us towards understanding the fundamental principles of the cosmos in an entirely different level. New approaches for classifying increasingly complex alternative theories to Einstein’s are needed for better analysis. With the inclusion of extra degrees of freedom, modified Einstein models have become harder to analyze. Therefore, it has become essential to develop new techniques that can classify these diverse models and make them more manageable. This can also help us to grasp the attributes of the theory, problems that might arise, and its observable effects. In short, it may enhance our insight into the prevailing alternative theories of gravity capozziello2008extended ; capozziello2011extended ; astashenok2015extreme . For creating models of inflation and dark energy, a scalar tensor theories (STT) bronnikov1973scalar ; goenner2012some of gravity could be considered helpful. These theories have been widely used in attempts to get beyond GR because of their simplicity. Horndeski rabochaya2016note ; ogawa2020relativistic developed STT by combining Lagrangian with a scalar field. This scalar field involves a second-order derivative. Also, it has the constraint that the Euler-Lagrange equations cannot exceed second order. These are four-dimensional theories that describe a scalar interacting with a spin-2 tensor field. According to these theories, the metric interacts with an additional scalar degree of freedom, which could promote cosmic expansion. Scalar fields combined with gravitational fields make up non-minimally coupled and higher-order components through conformal transformations maeda1989towards ; capozziello1998recovering ; gottlober1990sixth ; crisostomi2016extended . Regarding velocities, the Hessian matrix of the Lagrangian is referred to as “degenerate.” A degenerate Hessian matrix suggests that there are main limitations since the system of momenta cannot be reversed. The “Degenerate Higher Order Scalar Tensor Theories” (DHOST) were presented by Langlois and Noui langlois2016degenerate . Second-order derivatives of the scalar field are permitted by DHOST theories. These theories permit higher-order Euler-Lagrange equations, and they must only have one scalar degree of freedom in order to prevent Ostrogradski instabilities. These instabilities are frequently observed in systems with higher-order time derivatives. Furthermore, they are connected with additional degrees of freedom (ADF). Crisostomi et al. crisostomi2019cosmological examined cosmic solutions and the factors that affect their stability in DHOST theory. Moreover, they investigated the fixed points of dynamics and identified the prerequisites for the presence of a de-Sitter attractor in late times. In modified gravity models defined by DHOST theories, Boumaza et al. boumaza2020late investigated the late cosmic development from the non-relativistic matter-dominated period to the dark energy era. Additionally, they determined the areas of parameter space where the models exhibit perturbation stability. Thipaksorn and Karwan thipaksorn2022cosmic addressed the cosmic evolution using the fixed points from the dynamical analysis of the scaling DHOST theory. They determined that these theories also meet the condition that the gravitational wave propagation speed is equal to the speed of light. Furthermore, gravitational waves (GW) do not decay to dark energy disturbances in addition to the scaling solutions. Langlois langlois2019dark considered the self-gravitating objects in the context of DHOST theories. Moreover, he observed that these theories could be an efficient approach to explain the dark energy in cosmology. Achour et al. achour2020hairy analyzed the prospect of exploring new areas of the solution space of DHOST theories through the implementation of disformal field redefinitions. Furthermore, they evaluated various novel solutions. These solutions include asymptotically locally flat black holes with a deficit solid angle and stealth solutions. In addition to the traditional Einstein-Hilbert term, Langlois langlois2021quadratic found a surprisingly simple reformulation of quadratic DHOST theories. These theories are based on a Lagrangian, including a few geometrical terms related to the three-dimensional constant hypersurfaces. Boumaza and Langlois boumaza2022neutron investigated the neutron stars in the context of DHOST theories. In order to explain the equation of state of the neutron star matter, they employed many equations of state. In each instance, they numerically estimated the neutron star profile using arbitrary values of energy density and modified gravity parameters. Frusciante et al. frusciante2019tracker examined quadratic DHOST theories that meet degeneracy conditions. He chose this theory to avoid Ostrogradsky instability, constraining GW speed and determining the bound on the decay of GW to dark energy perturbations. Furthermore, they generated the most generic Lagrangians capable of reproducing tracking and scaling characteristics individually. In the quadratic DHOST theories, Takahashi and Motohashi takahashi2021black investigated perturbations regarding the stealth Schwarzschild-de-Sitter solution. In addition to that, they investigated the sound speed in the neighborhood of the black hole (BH). Subsequently, they discovered that the gradient instability manifests itself in either the radial or angular direction. Chen et al. chen2021testing investigated the disformal Kerr BH frequency in quadratic DHOST using the relativistic precession model. Moreover, they found out that there is an almost negative disformal parameter in the range of 1. They do so under some restrictions and keeping in mind the GRO J1655-40 observational data. On cosmic scales, the complexity factor is particularly significant for self-gravitating systems. These systems exhibit a variety of properties that have been extensively researched, including energy density, pressure, stability, mass-radius ratio, and brightness. To explore the complexity of the stellar structure the probability distribution has been reformulated in the form of energy density sanudo2009complexity ; de2012entropy . However, energy density alone is insufficient to characterize complexity since the primary factor pressure component of the energy-momentum tensor, which is critical in the development of self-gravitating systems, is lacking herrera2018new . Herrera herrera2009structure conducted the study of self-gravitating relativistic fluids with spherical symmetry in a systematic manner. The approach relies on scalar functions that are obtained from the orthogonal separation of the Riemann tensor. According to this, the total number of scalars are five for dissipative and anisotropic fluids, which can be reduced to two when considering dissipationless dust or static anisotropic fluids. Furthermore, only one scalar remains for static isotropic fluids. Also, a new notion of complexity has been presented by him herrera2018new for relativistic fluid distributions with static spherically symmetric properties. The underlying premise assumes that a less complex system pertains to a homogeneous energy density and isotropic pressure in the fluid. As an apparent indicator of complexity, the scalar Y_{TF} emerges as a viable candidate for measurement purposes. Yousaf et al. yousaf2023quasi examined the quasi-static evolution of self-gravitating structures under strong curvature regimes, considering higher-order curvature gravity and the Palatini formalism of f(R) gravity. Furthermore, the role of vorticity and structure scalars in revealing fluid anisotropy effects and the influence of Palatini-based factors on shearing motion is highlighted. Eventually, they performed a comparison-based study to explore the effects of distinct curvature factors on the propagation of axial sources. Several researches has been done on this vast topic of complexity by Herrera and different authors lopez1995statistical ; crutchfield2000comment ; herrera2018definition ; herrera2019complexity ; herrera2019complexitya ; bhatti2021electromagnetic ; jasim2021anisotropic ; brassel2021inhomogeneous ; gumede2021first ; yousaf2022f ; yousaf2023role Motivated by the work of Herrera herrera2009structure and the significance of scalar-tensor theories, we will construct the unique notion of complexity within the context of DHOST theory. This manuscript is organized in such a manner that Sec. II presents the comprehensive mathematical formalism of theory with couplings between the scalar field and the Einstein tensor, extracting the compact form of modified field equations in the context of DHOST theory. In Sec. III, we adapt a well-suited spherically symmetric anisotropic matter content in order to evaluate the modified field equations and non-conserved equation. The major goal of our manuscript, i.e., to obtain the set of structure scalars, is determined in Sec. IV. Section V deals with the calculation of different masses. In Sec. VI, physical models under the constraint of minimal complexity and the conformal flatness are discussed. Lastly, the summary as well as the final outcomes of our investigations are described in Sec. VII. This manuscript will ultimately open up the field of astrophysics to work in this modified gravity using a variety of geometries. This study may assist in further unravelling the enigma of the universe."
https://arxiv.org/html/2411.06945v1,Toward Gravitational Lensing in Modified Theories of Gravity,"In this study, we investigate gravitational lensing within modified gravity frameworks, focusing on the Hu-Sawicki f(R) and normal branch Dvali-Gabadadze-Porrati (nDGP) models, and we compare these results with those obtained from general relativity (GR). Our results reveal that both modified gravity models consistently enhance key lensing parameters relative to GR, including the Einstein radius, lensing optical depth, and time delays. Notably, we find that the Hu-Sawicki f(R) and nDGP models yield significantly larger Einstein radii and higher lensing probabilities, especially at greater redshifts, indicating an increased likelihood of lensing events under modified gravity. Our analysis of time delays further shows that the broader mass distributions in these frameworks lead to pronounced differences in high-mass lens systems, providing potential observational markers of modified gravity. Additionally, we observe amplified magnification factors in wave optics regimes, highlighting the potential for gravitational wave (GW) lensing to differentiate modified gravity effects from GR predictions. Through these findings, we propose modified gravity theories as compelling alternatives to GR in explaining cosmic phenomena, with promising implications for future high-precision gravitational lensing surveys.","Gravitational lensing describes how the path of null waves originating from a distant source is altered as they pass near a massive object. This change in direction results from the gravitational field of the massive lens, which bends the path of these waves in line with general relativity (GR). This phenomenon, affecting both electromagnetic (EM) waves and gravitational waves (GWs), can produce interference patterns and distinct image structures shaped by the characteristics of the lens and the configuration of various potential light paths. Gravitational lensing thus offers a valuable tool in astrophysics and cosmology, influencing how we detect and interpret all forms of radiation from distant sources (Wambsganss, 1998; Bartelmann, 2010; Cunha and Herdeiro, 2018; Chowdhuri et al., 2023; Takahashi and Nakamura, 2003; Hou et al., 2020; Diego et al., 2021). Lensing of EM waves has already transformed cosmological studies by enabling precise measurements of dark matter distributions, identifying faint celestial objects, and even detecting exoplanets around distant stars Bond and et al. (2004); Massey et al. (2010a); Welch and et al. (2022). Since the landmark 2015 detection of GW event GW150914 by LIGO-Virgo, we now recognize that GWs also undergo lensing, a process which holds the potential to further enhance our cosmological understanding by providing new insights into the distribution of mass in the Universe and the properties of dark matter Maggiore (2007); Liao et al. (2022). For example, lensed GWs can address issues such as the mass-sheet degeneracy that limits EM lensing Cremonese et al. (2021) and open new avenues for testing gravitational theories, such as through the study of GW birefringence and propagation speed Ezquiaga and Zumalacárregui (2020); Baker and Trodden (2017). One of the most profound implications of gravitational lensing lies in its potential to reveal the nature of dark matter Massey et al. (2010b); Vegetti et al. (2023). In cosmology, dark matter remains an elusive but critical component of the Universe, inferred from gravitational interactions observed in galactic and sub-galactic structures. Studies suggest that sub-galactic dark matter halos could serve as strong lenses for compact background sources, producing characteristic milli-arcsecond scale lensing effects Loudas et al. (2022). By observing these “milli-lenses,” one can test dark matter models, such as cold dark matter halos, whose lensing effects could vary based on the properties of null waves Jung and Shin (2019). Thus, gravitational lensing provides a unique lens through which to investigate dark matter structures and distributions that are otherwise invisible to direct observation. In gravitational lensing research, selecting suitable dark matter halo models is crucial Tizfahm et al. (2024). Dark matter halo formation scenarios have been discussed in various models, e.g., White (2022); Delos and Silk (2023); Del Popolo and Fakhry (2023); Fakhry et al. (2023a); Del Popolo and Fakhry (2024). In this regard, recent studies have demonstrated that realistic dark matter halo models yield predictions more closely aligned with contemporary cosmological data, see, e.g., Fakhry et al. (2021, 2022a, 2022b, 2023b); Fakhry and Del Popolo (2023); Fakhry et al. (2023c); Fakhry (2024); Fakhry et al. (2024a, b). Yet, while GR has successfully described gravitational lensing, it encounters challenges in accounting for the dark sector of the Universe, where large amounts of unseen dark matter are required to explain observed gravitational effects, see, e.g., Tucker and Wang (1998); Srivastava et al. (2023). As the exact nature of dark matter remains unknown, modified gravity theories have gained attention by offering explanations for these observations without invoking dark matter, see, e.g., Brans and Dicke (1961); Dvali et al. (2000); Moffat (2006); Sotiriou and Faraoni (2010); Harko et al. (2011); Xu et al. (2019). Such theories propose that gravitational behavior might differ on galactic or cosmic scales, suggesting that the apparent need for dark matter could instead reflect limitations in our understanding of gravity. These alternative frameworks provide an exciting path to explore whether modifications to gravity itself could explain lensing phenomena and reveal new aspects of cosmic structure Schmidt (2008); Nzioki et al. (2011); Gao et al. (2019); Ren et al. (2021); Kuang et al. (2022); Kalita et al. (2023); Davies et al. (2024). In this work, we focus on two widely studied modified gravity models: the Hu-Sawicki f(R) gravity model Hu and Sawicki (2007) and the normal branch of the Dvali-Gabadadze-Porrati (nDGP) model Dvali et al. (2000). Both models propose modifications to GR that allow for differences in gravitational effects at large scales. For instance, the Hu-Sawicki model introduces a functional form of the Ricci scalar that mimics GR on local scales but can drive cosmic acceleration without dark energy on larger scales Sawicki and Hu (2007); Santos et al. (2012); Hu et al. (2016); MacDevette et al. (2022). In contrast, the nDGP model suggests a braneworld scenario where our Universe exists within a four-dimensional “brane” embedded in a higher-dimensional space, resulting in gravity that behaves normally on small scales but changes at cosmic distances Liu et al. (2021); Song et al. (2024). These modified theories predict subtle but measurable deviations from GR in gravitational lensing signals, offering potential tests of their validity. For example, f(R) gravity could lead to enhanced lensing effects, while the nDGP model could yield scale-dependent changes in lensing across different distances. In this work, we examine gravitational lensing under the Hu-Sawicki f(R) and nDGP models, analyzing how each theory impacts key observables such as Einstien radius, lensing strength, and time delays in strong lensing systems. By comparing these predictions to GR, we aim to identify distinct signatures that could be detected by next-generation observational tools, such as advanced GW and EM lensing observatories Euclid Collaboration et al. (2024); Shajib et al. (2024). These efforts could ultimately clarify whether GR or modified gravity better explains the cosmos, potentially advancing our understanding of the Universe’s structure. The work is structured as follows: Section II presents the theoretical foundations of the Hu-Sawicki f(R) and nDGP models, highlighting key distinctions from GR. Section III describes our methods for calculating lensing properties in these modified gravity frameworks, including screening mechanisms to limit effects in high-density regions. Section IV details our findings, and Section V concludes with a summary and suggestions for future research."
https://arxiv.org/html/2411.06698v1,Discovery of a Unique Close Quasar-DSFG Pair Linked by a [C II] Bridge at z=5.63,"We report the discovery of a unique quasar-dusty star-forming galaxy (DSFG) system at z=5.63, consisting of the bright quasar J1133+1603 (M_{\rm UV}=-27.42) and its compact, dust-obscured companion, J1133c. ALMA observations reveal a prominent [C II] bridge connecting the quasar and DSFG, indicating ongoing interaction at a projected separation of 1.8″ (\sim10 proper kpc). J1133c exhibits unusually bright and broad [C II] emission (L_{\rm[CII]}>10^{43}\,\rm erg\,s^{-1}, \rm FWHM>500\,\rm km\,s^{-1}), with a [C II] luminosity five times that of the quasar, suggesting intense star formation or potential AGN activity. The inferred star formation rate from [C II] is approximately 10^{3}M_{\odot}\rm yr^{-1}. The remarkable properties of this pair strongly suggest that galaxy interactions may simultaneously trigger both starburst and quasar activity, driving rapid evolution in the early universe.","Mergers are considered significant triggers of quasar activity and star formation, with up to 50% of luminous high-redshift quasars linked to major mergers (e.g., Di Matteo et al., 2005; Hopkins et al., 2006; Decarli et al., 2017). High-redshift quasar-dusty star-forming galaxy (DSFG), quasar-quasar, and quasar-submillimeter galaxy (SMG) pairs (e.g., Lee et al., 2019; Izumi et al., 2024; Decarli et al., 2019) are rare but essential for studying galaxy-quasar co-evolution during the reionization epoch. Such systems typically reside in dense environments where interactions profoundly impact both star formation and black hole growth, as seen in SMGs and active galactic nuclei (AGNs) (e.g., Alexander et al., 2005, 2008; Umehata et al., 2015). The [C II] 158\mum line can trace star formation and gas dynamics, offering insights into interstellar medium properties of AGNs and star-forming galaxies (e.g., Stacey et al., 1991; Lagache et al., 2018). The quasar-DSFG pair reported here, connected by a [C II] bridge, provides a unique opportunity to explore the effects of such interactions on the growth of massive galaxies. Similar [C II] bridges have been noted in other high-redshift systems, indicating active gas flows between interacting galaxies (e.g., Ginolfi et al., 2020; Umehata et al., 2021). This work uses the Planck18 cosmology (Planck Collaboration et al., 2020) for luminosity calculations."
https://arxiv.org/html/2411.06582v1,Vacuum energy and cosmological constant in QFT in curved spacetime,"The cosmological constant term (CC), \Lambda, is a pivotal ingredient in the standard model of cosmology or \LambdaCDM, but it is a rigid quantity for the entire cosmic history. This is unnatural and inconsistent. Different theoretical and phenomenological conundrums suggest that the \LambdaCDM necessitates further theoretical underpinning to cope with modern observations. An interesting approach is the framework of the ‘running vacuum model’ (RVM). It endows \Lambda with cosmic dynamics within a fundamental framework since it is based on QFT. In the RVM, the vacuum energy density (VED) appears as a series of powers of the Hubble function and its derivatives, \rho_{\rm vac}(H,\dot{H},...). In the current universe, \rho_{\rm vac} changes as \sim H^{2}. Higher order effects {\cal O}(H^{4}), on the other hand, can be responsible for a new mechanism of inflation (RVM-inflation). On the practical side the RVM can alleviate the cosmological tensions on \sigma_{8} and H_{0}. An intriguing smoking gun signature of the RVM is that its equation of state can mimic quintessence, as recently observed by DESI, so the vacuum can be the sought-for dynamical DE. At a deeper theoretical level, the RVM-renormalized form of the VED can avoid extreme fine tuning related to the well-known cosmological constant problem. Overall, the RVM has the capacity to impinge positively on relevant theoretical and practical aspects of modern cosmology.","The peak point of the cosmological constant (CC) term, \Lambda, in Einstein’s equations was its actual measurement at the end of 20th century using type Ia Supernovae (SnIa) luminosity distances. The nonvanishing positive value that was obtained provided for the first time evidence of the accelerated expansion of the Universe [1]. As of its introduction by Einstein in the field equations [2], \Lambda remains a fundamental ingredient of GR. And after more than a century it is still at the core of modern cosmology since it is a key element of the concordance model of cosmology, \LambdaCDM [3]. Despite the more general notion of Dark Energy (DE) was introduced much later to ‘explain’ the speeding up of the universe, within the standard model \Lambda provides the simplest candidate for DE. Nowadays, very precise measurements have been performed [4] on \Lambda or, equivalently, on the value of the associated parameter \Omega^{0}_{\rm vac}=\rho^{0}_{\rm vac}/\rho^{0}_{c}. The vacuum energy density (VED) is associated to \Lambda through the relation \rho^{0}_{\rm vac}=\Lambda/(8\pi G_{N}), where G_{N} is Newton’s constant and \rho^{0}_{c}=3H_{0}^{2}/(8\pi G_{N}) is the critical density in our time, where H_{0} is the current value of the Hubble parameter. The observational value of \Omega^{0}_{\rm vac} is found to be around 0.7. Despite \Lambda is taken from granted within the \LambdaCDM, it undercovers a great source of mystery. To start with, its nature and its origin remain undisclosed at the fundamental level. Naive estimations within quantum field theory (QFT) lead to ‘gargantuan’ values that cannot be minimally reconciled with observations. This abhorrent mismatch is usually said to stem from calculations of the zero-point energy (ZPE) associated to matter fields and also from the Higgs potential of the Standard Model (SM) of particle physics. If m is the mass of one of such SM fields, contributions to the ZPE of order m^{4} are expected[5]. Since the observational value of the VED lies around \rho_{\rm vac}\sim 10^{-47} GeV4 in natural units, comparison with \sim m^{4} for any particle of the SM, even the lightest ones, leads to nonsense. For example, for the electron field we find a difference of 34 orders of magnitude: \rho_{\rm vac}^{\rm obs}/m_{e}^{4}\sim 10^{-34}. The problem persists when comparing with the ground state energy of the effective Higgs potential, leadibng to 56 orders of discrepancy: V_{\rm eff}, we find \rho_{\rm vac}^{\rm obs}/\langle V_{\rm eff}\rangle\sim 10^{-56}. These two examples, describe in a conspicuous way what is known as the ‘Cosmological Constant Problem’ (CCP) [6]. Far from being a mere artifact produced by the simplicity of \Lambda in the cosmological model, the difficulties seem to lie in the very conception of vacuum energy in Quantum Field Theory (QFT) and affects all forms of DE and not just the vacuum energy[7, 8, 9, 10, 11, 12] 222For an informal, but rather vivid, introduction to the Cosmological Constant Problem, see e.g. [13].. Another aspect of this intriguing topic is the fact that, at the present time, \rho_{\rm vac}^{\rm obs} and the energy density associated to Cold Dark Matter (CDM), \rho_{\rm CDM}, are observed to be of the same order of magnitude, despite the fact that \rho_{\rm CDM} is assumed to decrease with the expansion as a^{-3} (a being the scale factor), whereas \rho_{\rm vac} maintains constant. This is the so-called cosmic coincidence problem [14], although not everybody agrees that this is a real problem [15]. There are other worrisome problems with the concordance model of more practical nature. For example, there are tensions with the measurement of the current Hubble parameter H_{0}\equiv 100h km/s/Mpc (h\simeq 0.7) and the growth of LSS structures. The growth rate is usually tracked by means of the parameters S_{8} or \sigma_{8}, related to the root mean square fluctuations of the matter density in spheres of size 8h^{-1} Mpc. The \LambdaCDM predicts an excess of structure formation at low redshift at a level of \sim 2-3\sigma with respect to actual measurements. The H_{0}-tension, on the other hand, is more serious. It involves an acute disagreement between the value of H_{0} inferred from CMB observations (which make use of fiducial \LambdaCDM cosmology) and the corresponding value extracted from the local (distance ladder) measurements. The two types of measurements of H_{0} lead to inconsistencies of \sim 5\sigma c.l. See e.g. [16, 17] for reviews on these tensions. It is still not known if these tensions are the result of systematic errors, but the possibility that new physics may be ultimately responsible for the observed deviations from the \LambdaCDM predictions cannot be excluded at all. A wide panoply of strategies have been proposed in the literature to alleviate some of the above tensions, see the above mentioned references. In this work, we review exclusively the running vacuum model (RVM) approach, which provides a fundamental framework to tackle fundamental cosmological problems as well as the mentioned tensions with tested efficiency, see [18] and [19, 20]. Let us also mention a RVM-inspired model (wXCDM) [21] which recently provided a very robust fit to the data, being also consistent with quintessence-like behavior near our time, as reported by DESI[22]. The model wXCDM is inspired in the old \LambdaXCDM framework [23], a composite DE system in which the RVM is entangled with an extra X component (behaving as ‘phantom matter’) with which it can exchange energy. The idea of ‘composite running vacuum’ appears fruitful and highly efficient to account for the observations and to cut down the tensions, see [24] and references therein. As said, in this work we review recent advances in the arduous task of understanding the CCP and the role played by the vacuum in cosmology. Our framework is QFT. We shall, however, not consider quantum gravity here, but QFT in cosmological spacetime, specifically Friedmann-Lemaître-Robertson-Walker (FLRW) spacetime with flat three-dimensional hypersurfaces. The main results have been presented in [25, 26, 27, 28], see also [12] for a comprehensive review. These studies show that the VED is dynamical in QFT since it depends on the Hubble rate and its derivatives, \rho_{\rm vac}=\rho_{\rm vac}(H,\dot{H},...)– denoted hereafter simply as \rho_{\rm vac}(H). These studies also show that contrary to naive expectations there are no Zeldovich type terms of the form \sim m^{4} like those mentioned at the beginning. Hence we can avoid extreme fine tuning, in contradistinction to the standard folklore on these matters. The RVM thus leads to an effective form of the \LambdaCDM in which the physical value of \Lambda ‘runs’ smoothly with the cosmic expansion thanks to the quantum matter effects. Such a running, in fact, can be conceived as a renormalization group running, see [10] and references therein. As a result, the quantum vacuum does not remain rigid throughout the cosmic history. To put it in a nutshell: there is no such thing as a ‘cosmological constant’ in the QFT context. Interestingly, the running nature of the vacuum has also been accounted for in the context of low-energy effective string theory [29, 30, 31, 32, 33, 34] 333 See also the forthcoming comprehensive review [35].. In all these formulations, the physical \Lambda appears as a mildly running quantity since it originates from quantum effects. Nonetheless the consequences on the \LambdaCDM behavior with a running \Lambda are not negligible; and in fact they can be essential to cure the cosmological tensions. Here, however, we shall exclusively focus on the QFT approach. For a more comprehensive review encompassing as well the low energy stringy effects on the vacuum dynamics, see [35]. Despite many curious attempts existing in the literature, to discuss about the CCP in flat spacetime makes no sense at all. Thus, in what follows we study the RVM renormalization of the energy-momentum tensor (EMT) in cosmological FLRW spacetime. We demonstrate that the values of the VED at different scales are related by a smooth function of the Hubble rate, H, and this function does not contain quartic powers \sim m^{4} of the masses, a fact that drastically eliminates the need for fine tuning. We also discuss the RVM mechanism for inflation based on the higher powers of H. In addition, we show the consequences for the equation of state of the vacuum in an expanding universe, which no longer remains near -1 but can display quintessence-like behavior owing to the quantum corrections."
https://arxiv.org/html/2411.06489v1,Varying Newton’s constant: a cure for gravitational maladies?,"We show that a slowly varying Newton’s constant, consistent with existing bounds, can potentially explain a host of observations pertaining to gravitational effects or phenomena across distances spanning from planetary to the cosmological, relying neither on the existence of Dark Matter or (and) Dark Energy, nor on any expected high proportions of either of them in the Universe. It may also have implications at very short distances or quantum gravity scales.","Newton’s theory of gravity and its subsequent relativistic formulation by Einstein, viz. General Relativity (GR), are among the most successful physical theories in explaining gravitational phenomena spanning from the planetary scale all the way to the galactic and cosmological scales. However, despite being largely successful, these theories have significant limitations, notably the following: (i) the need for an as-yet unidentified entity known as Dark Matter (DM) for explaining gravitational phenomena observed at the galactic scales and beyond, (ii) the requirement of a small, positive cosmological constant or more generally, Dark Energy (DE), also of an unknown origin, in order to explain observations at the cosmological scales, and (iii) the challenge in constructing a self-consistent quantum theory of gravity, notwithstanding the success of perturbative quantum field theory in describing the other fundamental forces of nature. Our objective in this paper is to explore potential resolutions to some of these issues (specifically the first two, which would be relevant for the third one as well) by making a reasonable proposal that the Newton’s constant G may not strictly be a ‘constant’, but may instead vary slowly with space and possibly with time as well. Of course, any such proposal must need to be consistent with the known experimental bounds of observable effects ranging from the planetary and astrophysical scales to the cosmological scale. While there exist huge volumes of literature on gravitational theories incorporating a variation or a scale-dependence of G, which we shall discuss in some detail in the section 2 below, the one we propose is a simple Taylor expansion in the radial coordinate r, about an arbitrarily chosen origin r=0. Presuming the zeroth order term in the expansion to be the experimentally measured Newton’s constant at terrestrial and planetary scales, the physical relevance of a few higher order terms can be realized at once, under certain stipulations. For instance, the first order term, if stipulated to be positive definite, modifies the Newtonian gravitational potential by a logarithmic term, well-known for providing an alternative to the dark matter (DM). The second order term, on the other hand, modifies the Newtonian force by a constant term, which can be interpreted as that due to a gravitational retardation, similar to what one encounters in electrodynamics. The third order term in the expansion, if stipulated to be negative definite, leads to the perception of an effective positive cosmological constant, the most favoured candidate for dark energy (DE). Beyond the third order though, any of the individual expansion terms in G(r) cannot be readily interpreted, regardless of any stipulation. Henceforth, limiting our attention to this order in the expansion, we endeavor to examine whether such an expansion can lead to any observable effect(s) of significance in astrophysics and cosmology. The paper is organized as follows: starting with the general layout of the gravitational theory involving the radial expansion of G in section 2, we discuss the consequences at different length scales, specifically, in accounting for astrophysical effects, such as the flatness of the rotation curves of galaxies, the galactic or cluster mass estimation via gravitational lensing and from the perspective of the virial theorem, as well as the cosmological dynamics, in section 3. A detailed cosmological study is then carried out in this context, albeit in a quasi-Newtonian framework, in section 4, and consequently the parameter estimation using standard statistical techniques and the combination of two widely used observational data-sets, for various emerging cosmological scenarios. A comparative study of such scenarios is also done by resorting to three statistical indicators, viz. the minimized \chi^{2} per degree of freedom (dof), AIC and BIC. Possible small scale effects of the varying G are then discussed briefly in section 5, before summarizing and concluding with a discussion on the open issues, work that remains to be done and works that are in progress, in section 6. A possible embedding of the varying G in a covariant framework is illustrated in the Appendix."
https://arxiv.org/html/2411.05911v1,Field Equations in Chern-Simons-Gauss-Bonnet Gravity,"We investigate the effects of Chern-Simons-Gauss-Bonnet gravity on fundamental metrics. This theory involves perturbative corrections to general relativity, as well as two scalar fields, the axion and the dilaton, that arise from Chern-Simons and Gauss-Bonnet gravity modifications respectively. The combined Chern-Simons-Gauss-Bonnet gravity is motivated by a wide range of theoretical and phenomenological perspectives, including particle physics, string theory, and parity violation in the gravitational sector. In this work, we provide the complete set of field equations and equations of motion of the Chern-Simons-Gauss-Bonnet modified gravity theory for a suite of fundamental metrics (Friedmann-Lemaître-Robertson-Walker, Schwarzschild, spherically symmetric, and perturbed Minkowski), under no prior assumptions on the behavior of the fields. The full set of field equations and equations of motion can be numerically solved and applied to specific observables under certain assumptions, and can be used to place constraints on the Chern-Simons-Gauss-Bonnet modified gravity theory.","Einstein’s theory of general relativity (GR) has been shown to agree remarkably well with observations [1, 2, 3, 4, 5, 6, 7, 8, 9]. Despite these successes, there are strong theoretical and experimental arguments that point to the need for extensions to the current formulation of GR. From the theoretical point of view, challenges toward a quantum theory of gravity suggest that GR may require modifications in the strong field regime [10]. These corrections are generally motivated from a high-energy ultraviolet theory that at low energies leads to corrections to GR as an effective field theory (see e.g. [11]). Moreover, there are a wide range of modified gravity theories and extensions to GR that may be motivated from the fact that GR is non-renormalizable at high energies (for a review on the topic see e.g., [12, 13]). From the experimental side, although tests of GR such as the observations of gravitational waves by the LIGO-Virgo-Kagra collaboration have thus far not observed significant deviations, modified gravity theories can have a variety of effects on gravitational waves, which can be characterized by modifications to the gravitational wave amplitude and/or phase. Extensive work has been done to derive the gravitational wave modifications induced by certain modified gravity theories, with the idea that such effects may be detectable by future generation detectors such as the Laser Interferometer Space Antenna (LISA) [14] or Einstein Telescope (ET) [15]. However even though gravitational wave waveforms can be used to test GR, the experimental indication that gravity may not be described exactly by GR comes from cosmology. Even though the Friedmann-Lemaître-Robertson-Walker (FLRW) metric is admitted as a solution to the field equations of GR, the presence of a dark energy with a negative equation of state, imposed by a plethora of independent experiments, is rather unsettling and has led to many ideas on how GR can accommodate such behavior on the largest scales [16, 17, 18, 19, 20]. More recently, ever increasing precision observations of the expansion rate of the universe has led to a disagreement between different cosmological observations [21]. Dubbed as the Hubble and S_{8} tensions, the experimentally obtained value of the present-day expansion of the universe measured locally [22] seems to be in a disagreement with the value inferred from observations of the Cosmic Microwave Background (CMB) [23] under a \Lambda Cold Dark Matter (\Lambda\text{CDM}) cosmological model [24]. A disagreement (albeit at a lesser significance) is also showcased in amplitude measurements of the variance of fluctuations on scales of 8\text{h}^{-1}\text{Mpc} (\sigma_{8}), or equivalently S_{8} (S_{8}\equiv\sigma_{8}\sqrt{\Omega_{m}/0.3}, where \Omega_{m} is the matter density in the universe) [25], obtained from the CMB [23] and late universe measurements of a wide range of low redshift probes [26, 27, 28]. Solutions to the observed disagreements have been explored extensively in literature [29, 30], from the reshuffling of energy densities at different epochs in the expansion history, such as early dark energy [31], or changes in the microphysics that governs the universe at small scales, e.g., decaying dark matter [32]. A different method of proposed solutions can be motivated from a change in our understanding of spacetime as used in cosmology, with modifications to GR having also been proposed as alternatives to inflation, dark matter, and dark energy (e.g., [13, 33, 34, 35]). In GR, the spacetime manifold is the central object of study; different assumptions when solving the Einstein field equations result in different solutions. If one modifies GR, then the field equations will also be modified, thus changing their solutions. At each position in the manifold, there is a choice to be made besides the coordinate system; one must also choose the values that dictate the geometrical and casual structure of the manifold, i.e. what metric the manifold takes [36]. The equations of motion provide a prescription to describe past and future events through the geometrical connection between curvature and stress-energy, since energy-momentum from all sources affects the geometry of spacetime. As such, modifications to the field equations can be thought of as additional effects to the geometry through changes to the curvature and stress-energy, describing regions of the manifold where unmodified GR is ill-defined. It is thus appealing to explore the effects of such modifications under various metrics. One well-studied modification of GR is Chern-Simons gravity111In almost all instances in this paper, we will be referring to the dynamical formulation of Chern-Simons gravity, known as dynamical Chern-Simons gravity [37]., which can be motivated from the context of particle physics [38, 39] and leptogenesis [40, 41], as well as in other areas such as string theory [42, 43, 44] and loop quantum gravity [45, 46, 47]. Furthermore, from a phenomenological perspective, such a theory could give rise to parity violation in the gravitational sector [48, 49, 37, 50, 51] and the CMB [52, 53, 54, 55, 56]. Gauss-Bonnet gravity is another well-motivated modified gravity theory, initially arising from an attempt to generalize GR [57, 58, 59, 60]; it has also been suggested to arise from string theory [61, 62, 63, 64, 65]. Its phenomenological implications have been extensively studied, including its predicted effect on compact objects such as black holes and neutron stars [66, 67, 68, 69, 70, 71, 72, 73, 74], and its implications for inflation [75, 76, 77, 78, 79, 80]. It is well-known that Gauss-Bonnet gravity appears in the 4D gravitational theory predicted by heterotic string theory [81, 82, 83, 84]222In string theory, the heterotic string is a mixture of the right-moving sector of the superstring and the left-moving sector of the bosonic string., but recently it was shown that Gauss-Bonnet gravity alone cannot be the full theory in four dimensions, as it lacks an axion field. However, an axion field can arise from Chern-Simons gravity; thus, the full 4D gravitational theory coming from the heterotic string is in fact a combination of Chern-Simons and Gauss-Bonnet gravities, a result that does not depend on the choice of compactification [85]. We refer to this theory as Chern-Simons-Gauss-Bonnet (CS-GB) gravity. It is imperative to attempt to obtain an experimental signature of any modification to GR. In this paper, we study how field equations are changed in CS-GB gravity, in a set of commonly used and in some ways testable metric inputs. This can be thought of as a first attempt in quantifying “observational” signatures of CS-GB gravity. In a subsequent paper, we plan to solve the resulting field equations under observational constraints and thus obtain a direct handle at the level to which CS-GB gravity is an allowed modification to GR. The outline for this paper is as follows: after reviewing the theory and modified field equations in Sec. II, we compute the effects of the modified theory on various metrics in Sec. III. We discuss directions for future work and conclude in Sec. IV. Throughout this paper, we use geometric units such that G=c=1, and we assume a (-,+,+,+) metric signature; Greek letters (\mu,\nu,…) range over all spacetime coordinates, Latin letters (i,j,…) range over spatial indices, square brackets denote anti-symmetrization over indices, and derivatives take the form \partial_{\alpha}=\frac{\partial}{\partial x^{\alpha}}."
https://arxiv.org/html/2411.05906v1,"Galaxy Tomography with the Gravitational Wave Background 
from Supermassive Black Hole Binaries","The detection of a stochastic gravitational wave background by pulsar timing arrays suggests the presence of a supermassive black hole binary population. Although the observed spectrum generally aligns with predictions from orbital evolution driven by gravitational wave emission in circular orbits, there is a discernible preference for a turnover at the lowest observed frequencies. This turnover could indicate a significant hardening phase, transitioning from early environmental influences to later stages predominantly influenced by gravitational wave emission. In the vicinity of these binaries, the ejection of stars or dark matter particles through gravitational three-body slingshots efficiently extracts orbital energy, leading to a low-frequency turnover in the spectrum. By analyzing the NANOGrav 15-year data, we assess how the gravitational wave spectrum depends on the initial inner galactic profile prior to disruption by binary ejections, accounting for a range of initial binary eccentricities. Our findings suggest a parsec-scale galactic center density around \displaystyle 10^{6}\,M_{\odot}/\textrm{pc}^{3} across most of the parameter space, offering insights into the environmental effects on black hole evolution and combined matter density near galaxy centers.","Recent advances by pulsar timing arrays (PTAs), leveraging precise measurements of timing residuals within a galactic-scale detector, have ushered in a new era of stochastic gravitational wave background (SGWB) detection. The SGWB, defined by a superposition of incoherent gravitational waves (GWs), initially emerged as a common-spectrum process [1, 2, 3, 4]. Subsequent data provided robust evidence of a quadrupolar correlation function [5, 6, 7, 8], famously known as the Hellings-Downs curve [9], further affirming the SGWB’s presence and characteristics. The observed spectrum of the SGWB is consistent with expectations for a population of supermassive black hole binaries (SMBHBs), dominated by binaries with comparable mass ratios, total masses in the range of \displaystyle 10^{9.2-10.4}\,M_{\odot}, and redshifts from \displaystyle 0.15 to \displaystyle 0.9 [10, 11], where \displaystyle M_{\odot} represents the solar mass. Although the spectrum is consistent with a steady spectral slope driven by GW emission from circular binaries, it also suggests a low-frequency turnover [10, 12]. This feature implies an acceleration in the rate of orbital hardening, offering a potential solution to the final parsec problem [13, 14]. Possibilities include interactions with environmental factors—such as gas [15, 16], stars [17], and dark matter (DM) [18]—and the impact of significant initial orbital eccentricities [19, 20, 21]. An intriguing aspect of environmental interactions involves the role of stars and particle DM, which are noted for their potentially high density in galactic centers (GCs) [22, 23, 24]. Both stars and DM can be expelled from the system through gravitational slingshots during encounters with binary components, thereby extracting orbital energy [17]. This process involves three-body scattering, where the energy extraction efficiency is significantly higher than that of two-body dynamical friction [25], especially when the binary components have comparable masses and are sufficiently close. Such three-body slingshot interactions can substantially alter the density profile of the GC, particularly flattening the inner distribution [18, 26]. This underscores the importance of considering the co-evolution of the density profile and the binary orbit. A pivotal study by Ref. [27] demonstrates that the orbital hardening rate observed in N-body simulations can be effectively approximated by results from scattering experiments [17] within environments characterized by the GC distribution at the SMBHB influence radius [28] prior to disruption. In this study, we examine the impact of initial GC density profiles and SMBHB eccentricities on the SGWB spectrum, utilizing NANOGrav’s 15-year dataset to constrain the parameter space of these two factors."
