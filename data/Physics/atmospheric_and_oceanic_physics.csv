URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10144v1,DaYu: Data-Driven Model for Geostationary Satellite Observed Cloud Images Forecasting,"In the past few years, Artificial Intelligence (AI)-based weather forecasting methods have widely demonstrated strong competitiveness among the weather forecasting systems. However, these methods are insufficient for high-spatial-resolution short-term nowcasting within 6 hours, which is crucial for warning short-duration, mesoscale and small-scale weather events. Geostationary satellite remote sensing provides detailed, high spatio-temporal and all-day observations, which can address the above limitations of existing methods. Therefore, this paper proposed an advanced data-driven thermal infrared cloud images forecasting model, ""DaYu."" Unlike existing data-driven weather forecasting models, DaYu is specifically designed for geostationary satellite observations, with a temporal resolution of 0.5 hours and a spatial resolution of 0.05\degree \times 0.05\degree. DaYu is based on a large-scale transformer architecture, which enables it to capture fine-grained cloud structures and learn fast-changing spatio-temporal evolution features effectively. Moreover, its attention mechanism design achieves a balance in computational complexity, making it practical for applications. DaYu not only achieves accurate forecasts up to 3 hours with a correlation coefficient higher than 0.9, 6 hours higher than 0.8, and 12 hours higher than 0.7, but also detects short-duration, mesoscale, and small-scale weather events with enhanced detail, effectively addressing the shortcomings of existing methods in providing detailed short-term nowcasting within 6 hours. Furthermore, DaYu has significant potential in short-term climate disaster prevention and mitigation.","Numerical Weather Prediction (NWP) primarily serves as the foundation for traditional weather forecasting (Charney et al., 1950), which employs mathematical and physical models to forecast future atmospheric states. The main process of NWP involves fusing collected observational data to generate an initial field as close as possible to the actual atmospheric state. This is followed by solving atmospheric dynamics equations and thermodynamic equations to simulate the evolution of the atmosphere. Starting from the initial condition, numerical methods are used to step forward in time, generating forecast results. However, the complexity in solving these equations engenders computational inefficiencies (Bauer et al., 2015), significantly limiting the timeliness of weather forecast. In addition, accurately describing and solving some complex physical processes, such as those in cloud microphysics, using formulas in NWP is highly challenging (Morrison and Milbrandt, 2015). These limitations prompt researchers to explore alternative methods that improve computational efficiency and enhance the forecasting of cloud elements. Driven by large amounts of data, AI-based weather forecasting systems can provide faster inference speeds and higher accuracy than traditional NWP. In recent years, researchers have begun exploring AI-based weather forecasting methods. (Rasp et al., 2020) firstly used ResNet to forecast with a spatial resolution of 5.625 \degree for up to 5 days. The FourCastNet (Pathak et al., 2022), leveraging the Adaptive Fourier Neural Operator (Guibas et al., 2021) and Vision Transformer (Dosovitskiy, 2020), delivers a global weather forecast for 7-day at a spatial resolution of 0.25\degree. These methods can achieve forecast accuracy comparable to traditional NWP. Pangu-Weather (Bi et al., 2023), as the first data-driven AI weather forecast model with accuracy surpassing traditional NWP, demonstrates excellent performance in 7-day forecasts at a spatial resolution of 0.25\degree. It marks the beginning of a new era in the research of AI-based weather forecasting methods. Additionally, GraphCast (Lam et al., 2023) implements an autoregressive model based on Graph Neural Networks, outperforming HRES in 90% of the variable and lead time combinations for 10-day predictions. FengWu (Chen et al., 2023a) and FuXi (Chen et al., 2023b) achieve effective forecast duration of at least 10 days, with a maximum reach of 15 days, while further enhancing forecast accuracy. NeuralGCM (Kochkov et al., 2024) incorporates physical constraints to ensure physically plausible predictions and can be extended to climate simulations. These methods have demonstrated the potential of data-driven AI-based methods in weather forecasting with their fast inference speed and high accuracy. Although these AI-based methods represent a significant advancement in weather forecasting, they still suffer some limitations. The inputs of them are primarily reanalysis data, such as ECMWF Reanalysis v5 (ERA5) (Hersbach et al., 2020). Reanalysis data assimilates observational data from a past period with NWP outputs, which leads to delayed timeliness. More importantly, the temporal resolution of existing AI-based methods is 6 hours, which is insufficient for short-term nowcasting. Within the 6-hour time window, there are short-lived convective weather events, which are often reflected by the formation and development of clouds. Existing methods are unable to accurately forecast clouds in the short term, and therefore cannot effectively monitor and warn of short-lived convective weather disasters. Additionally, the spatial resolution of existing AI-based methods is 0.25\degree \times 0.25\degree. As a result, these methods cannot accurately capture the mesoscale and small-scale weather events. And mesoscale and small-scale weather events typically involve specific cloud structures and systems. To further enhance the forecasting capabilities for short-lived mesoscale and small-scale weather system, it is necessary to design an AI-based short-term forecasting model using high-frequency real-time observational data. Data-driven methods using observations from geostationary satellite imager show great promise. First, geostationary satellite imager observation, with its advantages of high spatio-temporal resolution and wide coverage, enable continuous observing for rapidly changing events such as such as mesoscale convective systems and typhoons. Based on satellite cloud images, meteorological operational departments can continuously track the position and intensity of tropical cyclones via Dvorak technique (Velden et al., 1998; Olander and Velden, 2019), and monitor mesoscale convective weather systems (Fiolleau and Roca, 2013; Whitehall et al., 2015), providing important information for disaster warnings, maritime shipping safety and so on. In addition, the thermal infrared cloud images of the imagers enable continuous all-day, allowing geostationary satellites to monitor weather changes in a certain region consistently. Infrared cloud images use thermal infrared radiation to depict cloud top temperatures, thereby characterizing the structure and height of clouds (Tana et al., 2023; Li et al., 2022; Zhao et al., 2023; Li et al., 2023). The geostationary satellite observation data-driven methods can help address the shortcomings of existing AI weather forecast methods which struggle to capture short-term mesoscale and small-scale weather events. (Dai et al., 2024) uses cloud images at the 10.8 {\mu}m wavelength observed band for short-term thunderstorm forecasting, but this can only achieve four-hour forecasts for small areas. (Hatanaka et al., 2023) has implemented a combination of brightness temperature and solar radiation forecasting, but the observation area is limited to within 1\degree \times 1\degree, resulting in limited coverage. Clearly, existing forecasting models based on satellite remote sensing observations cannot meet the requirements for high spatio-temporal resolution forecasting. Based on the insights mentioned above, we have innovatively proposed a satellite remote sensing data-driven AI cloud images forecasting model, DaYu. The name ""DaYu"" (Chinese: {CJK*}UTF8gbsn 大禹 ) is derived from the legendary story of Dayu controlling the floods in ancient China, which tells of a great case of water management engineering initiated by Yu. Naming this AI model DaYu symbolizes the prevention of meteorological natural disasters. The DaYu model is a transformer-based autoregressive cascaded model, with each cascaded model predicting time spans of 0-6 hours and 6-12 hours, respectively. Specifically, this study selected the whole observational data from the Advanced Himawari Imager (AHI), with a temporal resolution of 0.5 hours and a spatial resolution of 0.05\degree \times 0.05\degree. Following extensive experimental validation, DaYu has demonstrated the capability for accurate 12-hour cloud images forecasting. DaYu can also effectively capture mesoscale and small-scale extreme weather events. This fully demonstrates DaYu’s advanced capabilities in short-term forecasting, effectively addressing the shortcomings of existing AI methods in short-term forecasting of mesoscale and small-scale weather events. Overall, our contribution to this work can be summarized as follows: • We have innovatively developed an AI-based model ""DaYu"" for high temporal and spatial resolution satellite cloud images forecasting, achieving a temporal resolution of 0.5 hours and a spatial resolution of 0.05° \times 0.05°. This addresses the gap in temporal and spatial resolution found in current AI-based weather forecasting methods. • DaYu has demonstrated excellent forecasting performance, achieving a skillful forecast lead time (PCC > 0.7) for up to 12 hours in advance. And DaYu has shown robustness in capturing mesoscale and small-scale extreme weather events and has been capable of enhancing the temporal frequency of skillful forecasts to 10 minutes."
https://arxiv.org/html/2411.10108v1,Identifying Key Drivers of Heatwaves: A Novel Spatio-Temporal Framework for Extreme Event Detection,"Heatwaves (HWs) are extreme atmospheric events that produce significant societal and environmental impacts. Predicting these extreme events remains challenging, as their complex interactions with large-scale atmospheric and climatic variables are difficult to capture with traditional statistical and dynamical models. This work presents a general method for driver identification in extreme climate events. A novel framework (STCO-FS) is proposed to identify key immediate (short-term) HW drivers by combining clustering algorithms with an ensemble evolutionary algorithm. The framework analyzes spatio-temporal data, reduces dimensionality by grouping similar geographical nodes for each variable, and develops driver selection in spatial and temporal domains, identifying the best time lags between predictive variables and HW occurrences. The proposed method has been applied to analyze HWs in the Adda river basin in Italy. The approach effectively identifies significant variables influencing HWs in this region. This research can potentially enhance our understanding of HW drivers and predictability.","The occurrence of heatwaves (HWs), characterized by prolonged periods of abnormally high temperatures exceeding typical local conditions, has become a pressing concern in recent years due to their severe societal and environmental impacts [1, 2]. Since 1950, extensive regions worldwide have witnessed numerous prolonged and intense HWs, resulting in significant consequences for human mortality, regional economies, and natural ecosystems [3, 4, 5, 6, 7]. In agriculture, heat stress on crops can significantly reduce yields, leading to food insecurity. In addition, increased demand for electricity for cooling during HWs substantially strains power grids. The escalation in the frequency of HWs has been documented in various parts of the globe in recent years and is at least partly attributed to the temperature increases driven by anthropogenic warming [8, 9]. Numerous studies [10, 11, 12] have consistently highlighted that the ongoing increase in global surface temperatures will lead to significant alterations in the frequency and intensity of HWs across Europe by the end of this century. This trend is not confined to Europe; globally, there is also a growing prevalence of heat extremes, with projections indicating that these events will continue to increase in the coming decades [13, 14, 15]. Regional differences can be encountered in HW projections. Hence, this leads to diverse drivers and climate forcings on regional scales. The identification of these drivers plays a key role in understanding regional variations and in developing effective mitigation and adaptation strategies, as different regions may experience distinct climate impacts due to a combination of local factors and global climate forces. Moreover, understanding these drivers is crucial for improving forecasts on sub-seasonal scales, allowing for more accurate predictions of HWs and other extreme events. When tackling the challenge of HW detection or prediction, it is necessary to understand the mechanisms responsible for these extreme events. Although the underlying processes remain not entirely understood [1], an increasing number of studies have delved into these mechanisms and physical drivers that contribute to the formation and prediction of HWs [16, 17]. HWs are the product of intricate interactions between large- and small-scale processes that operate across diverse temporal scales. These events are highly influenced by atmospheric circulation, often regarded as a fast-acting driver, as well as anomalous conditions in slowly changing climate components, which can serve as proximate factors (e.g., land surface) or remote factors (e.g., upper ocean temperature, or sea ice) affecting HWs occurrence [18, 19, 20]. In the extratropics, atmospheric circulation patterns that influence HWs include quasi-stationary synoptic-scale high-pressure systems (anticyclones) [21, 22], whose predictability at a seasonal scale is low due to the influence of the chaotic variability of the atmosphere [23]. Finally, long-term trends in frequency, duration, and intensity of HWs are primarily driven by anthropogenic forcings, including global factors such as greenhouse gas concentrations and regional factors like land-use/land-cover changes and aerosol emissions [24]. However, these are out of the scope of this paper. In close relation to the previous discussion, and considering the vast volume of available spatial and temporal data, employing data-driven methodologies becomes indispensable for uncovering potential HW drivers. A limited body of literature addresses this subject using ML and feature selection and dimensionality-reduction approaches. Some works [25, 26, 27] employed Principal Components Analysis (PCA) to reduce and optimize the number of highly correlated variables, using them as inputs in some ML algorithms. In [28], authors aimed to identify the role of the individual drivers for five HWs in the recent decade through factorial experiments, which force the model toward observations for one or several key components at a time, allowing to identify how much of the observed temperature anomaly of each event can be attributed to each driver. Other feature selection approaches have been used for different weather problems in searching for optimal input variables. In [29], an extreme gradient boosting feature selection algorithm was applied with ML models in a problem of short-term relative humidity prediction. In [30], a nested loop of roughly pruned random forests was used for identifying significant drivers of daily streamflow from large-scale atmospheric circulation in Norway. In [31], a clustering method was applied to divide Morocco into regions that are spatially consistent in terms of extreme precipitation and to identify its drivers by analyzing atmospheric circulation anomalies during the occurrence of regional events. In [32], ML regression-based algorithms were used to identify the drivers of drought dynamics in the Free State Province. [33] shows the influence of different drivers to understand the causal mechanism of HWs over South-West India. For that purpose, climate model simulations and long-term observational data were proposed. This study proposes a general framework for HW driver identification, which can be applied to other extreme events in the context of detection and event short-term prediction. The framework is illustrated here to detect HWs in a European location. Specifically, the framework proposed in this work follows a two-phase methodology to obtain robust HW driver identification. In the first phase, a clustering algorithm is applied to variables identified as potential drivers, extracted from the ERA5 reanalysis dataset [34], and presented as time series. This clustering step reduces the dimensionality of the spatial domain by grouping nodes with similar time series patterns. In the second phase, a wrapper feature selection approach based on a multi-method ensemble evolutionary algorithm (PCRO-SL) [35] is employed to identify the most skilful drivers and periods for HW forecast over short-term (days to weeks) and seasonal horizons. The optimization algorithm’s fitness function performs a driver selection by evaluating the performance of an ML model for HW classification based on a subset of clustered drivers. The proposed framework is applied to the agricultural districts in the Adda river basin, located downstream of Lake Como, in the Lombardy region, Northern Italy. These districts are part of the Po Valley, one of the most productive European agricultural areas, which provides one-third of the national agricultural production [36]. Understanding the crop risks associated with extreme temperatures is becoming increasingly crucial to planning effective climate change adaptation strategies. The manuscript is organized as follows. First, a description of the data, including potential drivers and target variables used for developing the experiments, is provided in Section 2. Then, the spatio-temporal feature selection methodology is presented and detailed in Section 3. Subsequently, the experimental work and the results obtained are further described in Section 4. Finally, in Section 5, there is a discussion on the potential uses of the framework in wider-scale driver detection and on implications for forecasting."
https://arxiv.org/html/2411.10112v1,On the solutions of a factorized wave equation,"Long-distance transmission of energy by waves is a key mechanism for many natural processes. It becomes possible when the inhomogeneous medium is arranged in such a manner that it enables a specific type of waves to propagate with virtually no reflection or scattering. If the corresponding wave equation admits factorization, at least one of the waves it describes propagates without reflection. The paper is devoted to searching for conditions under which both solutions of a one-dimensional factorized wave equation of the second order describe traveling waves, that is, waves propagating without reflection. Possible variants of wave structure are found and the results are compared with those obtained in previous studies.","Studying the possibility of wave propagation without reflection and scattering in inhomogeneous environment is necessary to explain phenomena of energy transmission by waves over long distances observed in nature. In the Earth’s atmosphere, the energy is transferred from the lower layers to the upper ones by acoustic-gravity and internal waves [1, 2]. In the seas and oceans, storm waves and tsunamis are able to transmit the energy over long distances [3, 4]. In space and astrophysical plasmas, this work is often performed by Alfvén waves [5], that is, by transverse (with respect to the directions of the magnetic field and phase velocity) oscillations which practically do not disturb the density. In particular, it is Alfvén waves that play a key role in the transfer of energy from the lower solar atmosphere into the corona and in the corona heating (see, for example, [6, 7] and references cited). Theoretical search for configurations of an inhomogeneous medium that allow wave propagation without reflection is very difficult and analytical results have been obtained, as a rule, for cases of one spatial dimension (or those reducible to one-dimensional) which we shall assume below. The main way to finding the parameters of inhomogeneous non-reflective media is in reducing the equation for waves to some reference equation. For media at rest, the equations with coordinate-independent coefficients are usually chosen as such, for example, the classical equation for waves propagating with a velocity c, \displaystyle\frac{\partial^{2}f}{\partial t^{2}}-c^{2}\displaystyle\frac{% \partial^{2}f}{\partial x^{2}}=0, (1) whose general solution f(x,t)=F_{1}\left(\displaystyle\frac{x}{c}+t\right)+F_{2}\left(\displaystyle% \frac{x}{c}-t\right) represents two waves of arbitrary forms running in opposite directions, or the Klein-Gordon equation (see [8, 9, 10, 11] and references cited). Recently, the set of reference equations has been extended [12, 13, 14] to include the Euler-Darboux-Poisson equation, \displaystyle\frac{\partial^{2}f}{\partial t^{2}}-\displaystyle\frac{\partial^% {2}f}{\partial\xi^{2}}-\displaystyle\frac{2m}{\xi}\,\displaystyle\frac{% \partial f}{\partial\xi}=0,\quad\xi=\int\!\displaystyle\frac{{\rm d}x}{c(x)}, (2) and some related equations. For m=0 Eq. (2) is equivalent to Eq. (1), and for m=1 it describes spherically symmetric traveling waves in homogeneous three-dimensional environment. For any integer m, its solution also describes two independent traveling waves, each in the form of a finite sum (see, for example, [15], §12.4), f(\xi,t)=\sum_{k=0}^{|m|}C_{k;m}\xi^{k+m_{0}}\displaystyle\frac{\partial^{k}F}% {\partial\xi^{k}},\qquad F(\xi,t)=F_{1}(\xi+t)+F_{2}(\xi-t),\quad C_{k;m}=% \mbox{const}, (3) where F_{1} and F_{2} are arbitrary functions, m_{0}=1-2m for m>0, m_{0}=0 for m<0, C_{0;m}=1, and the remaining constants can be easily found by substituting into Eq. (2). We call attention to the fact that F_{1}(Z) serves as a generating function for all functions dependent on the phase (\xi+t) of the first wave, and F_{2}(Z) plays the same role for the functions dependent on the phase (\xi-t) of the second wave. Taking these results into account, one can formulate an extended concept of traveling waves. Namely, such a wave [with the speed c(x)] can be described by a finite sum, f(x,t)=\sum_{k=1}^{n}a_{k}(x)\Phi_{k}\left(t-\!\displaystyle\int\limits% \displaystyle\frac{{\rm d}x}{c(x)}\right)\,, (4) in which the form factors a_{k}(x) are determined by the medium arrangement, and the phase functions \Phi_{k}(Z) have a common generating function \Phi(Z). In isotropic media, an oppositely propagating traveling wave is represented by a similar sum with the same form factors but its own generating function \tilde{\Phi}\Bigl{(}t+\int{\rm d}x/c(x)\Bigr{)}. If the inhomogeneous medium moves with a velocity V(x), the waves are carried away by the flow and propagate with different velocities, V(x)+c(x) and V(x)-c(x). In addition, the flow creates a preferred direction and violates isotropy. Therefore, to search for non-reflective configurations of moving media, it was proposed another approach [16, 17] based on factorization of wave equations. To do this, the function f(x,t) describing wave motion is represented as f(x,t)=a(x){\cal F}(x,t) and then the conditions for the flow parameters and the dependence of a(x) on these parameters are found under which the wave equation takes the form \left[\displaystyle\frac{\partial}{\partial t}+w_{1}(x)\,\displaystyle\frac{% \partial}{\partial x}+G(x)\right]\left[\displaystyle\frac{\partial}{\partial t% }+w_{2}(x)\,\displaystyle\frac{\partial}{\partial x}\right]{\cal F}(x,t)=0, (5) where w_{1,2}(x) are the wave velocities. By this method, wide classes of shallow water flows were found, in which surface [16, 17] or internal [18, 19, 20] gravity waves can propagate without reflection. It turned out that for all these diverse flows G(x) is equally related to w_{1,2}(x), G(x)=G_{0}(x)=\displaystyle\frac{w_{1}(x)w^{\prime}_{2}(x)-w^{\prime}_{1}(x)w_% {2}(x)}{w_{1}(x)-w_{2}(x)}, (6) and is invariant under the permutation of w_{1} and w_{2} (hereinafter the prime denotes the derivative with respect to the function argument). Moreover, the equation (5) is invariant as well and the general solution to the problem, f(x,t)=a(x){\cal F}(x,t)=a(x)\left[{\cal F}_{a}\left(t-\displaystyle\int% \limits_{x_{0}}^{x}\displaystyle\frac{{\rm d}x_{1}}{w_{1}(x_{1})}\right)+{\cal F% }_{b}\left(t-\displaystyle\int\limits_{x_{0}}^{x}\displaystyle\frac{{\rm d}x_{% 1}}{w_{2}(x_{1})}\right)\right],\quad x_{0}=\mbox{const}, (7) is a superposition of two waves of arbitrary shape having a common form factor a(x) specified by the flow arrangement. A similar study for Alfvén waves in plasma flows along magnetic field [21] has shown, however, that there are two types of non-reflective currents. For the currents of one type, G=G_{0} and the wave structure is described by Eq. (7). By contrast, for the currents of another type, even very similar structurally, G\neq G_{0}, the symmetry with respect to w_{1} and w_{2} permutation is broken, and the waves differ in both the speed and structure. Equation (5) is integrable in quadratures. In its general solution {\cal F}(x,t)=\!\displaystyle\int\limits_{x_{0}}^{x}\!\displaystyle\frac{{\rm d% }x_{1}}{w_{2}(x_{1})}\,\exp\left[\!-\!\displaystyle\int\limits_{x_{0}}^{x_{1}}% \!\displaystyle\frac{{\rm d}x_{2}G(x_{2})}{w_{1}(x_{2})}\right]H_{1}\left(t\!-% \!\!\displaystyle\int\limits_{x_{0}}^{x_{1}}\!\displaystyle\frac{{\rm d}x_{3}}% {w_{1}(x_{3})}\!-\!\!\displaystyle\int\limits_{x_{1}}^{x}\!\displaystyle\frac{% {\rm d}x_{3}}{w_{2}(x_{3})}\right)+H_{2}\left(t\!-\!\!\displaystyle\int\limits% _{x_{0}}^{x}\!\displaystyle\frac{{\rm d}x_{1}}{w_{2}(x_{1})}\right) (8) an arbitrary function H_{2}(Z) describes the second wave (running at speed w_{2}(x)). The argument of another arbitrary function, H_{1}(Z), is arranged in such a manner that a part of the path between x_{0} and x is passed with the speed of the first wave, w_{1}(x), and the remainder – with the speed of the second wave. In general case, the corresponding contribution to {\cal F}(x,t) describes a mutual transformation of waves in the course of their propagation. In other words, if H_{1}=0, there is the second wave only. But if H_{1}\neq 0, then both waves a generally present for any H_{2}(Z). From a physical point of view, such a fundamental difference between waves seems strange. In the case of Alfvén waves, it was possible to show that the solution (8) with properly chosen H_{2}(Z) does not contain the second wave, hence, there is no wave transformation. This fact is a consequence of the specific to Alfvén waves dependence of G on the flow parameters. Our main objective is to find out what a dependence of G on w_{1} and w_{2} should be in order that Eq. (5) will describe a pair of independently propagating waves, and what the structure these waves should have. Section 2 is devoted to a detailed analysis of the problem. The results obtained and their relation to the results of previous studies are discussed in Sec. 3."
https://arxiv.org/html/2411.09215v2,A note on an inversion algorithm for vertical ionograms for the prediction of plasma frequency profiles,"Building upon the concept of utilizing quasi-parabolic approximations to determine plasma frequency profiles from ionograms, we present a refined multi-quasi-parabolic method for modeling the E and F layers. While a recent study [AIP Advances 14, 065034 (2024)] [1] introduced an approach in this direction, we identified several inaccuracies in its mathematical treatment and numerical results. By addressing these issues, we offer a clearer exposition and a more robust algorithm. Our method assumes a parabolic profile for the E layer and approximates the F layer with a series of concatenated quasi-parabolic segments, ensuring continuity and smoothness by matching derivatives at the junctions. Applied to daylight ionograms from the Jicamarca Observatory in Lima, our inversion algorithm demonstrates excellent agreement between the synthetic ionograms generated from our predicted plasma frequency profiles and the original measured data.","Most of our knowledge about the ionosphere comes from ionogram records. These h^{\prime}(f) records give the apparent or virtual heights of reflection h^{\prime}(f) of a vertically transmitted radio wave, as a function of the wave frequency f. This paper aims to retrieve the electronic density profile from measured ionograms. The analysis of ionograms consists basically of converting an observed h^{\prime}(f) curve, which gives the virtual height of reflection h^{\prime} as a function of the wave frequency f, into an N(h) curve giving the variation of the electron density N with height h. These two curves are related by h^{\prime}(f)=\int_{0}^{h_{r}}\mu^{\prime}\,dh, (1) where the group refractive index \mu^{\prime} is a complicated function of f, N, and the strength and direction of the magnetic field. The height of reflection, h_{r} for the frequency f depends on f, N, and (for the extraordinary rays only) the strength of the magnetic field. Previous efforts to solve this ill-posed problem have included lamination techniques, least-squares polynomial approximations, and ray tracing methods. Since there is no analytic solution to Eq. (1)—that is, no direct expression for N(h) in terms of h^{\prime}(f)—researchers have explored various numerical and approximation strategies. The lamination technique proposed by Reilly [4] involves assuming various N(h) model curves, passing them through a forward model to generate corresponding ionograms, and then comparing the resulting h^{\prime}(f) curves with those observed experimentally. In contrast, Reinisch et al. [3, 5] utilized Chebyshev polynomial methods to approximate the F layer, aiming for a more efficient process in ionogram analysis. However, their software is proprietary, making it difficult to replicate and improve upon their work. More recently, Ankita et al. [6] introduced a different approach by using electromagnetic wave propagation simulations based on Hamiltonian formulations for ray tracing. In this study, we build upon the concepts introduced by Niu et al. [1], who proposed using multivariate quasi-parabolic layers to develop an inversion algorithm for approximating the plasma frequency profiles of the E and F ionospheric layers. While their underlying idea holds potential, we have identified several mathematical errors, mislabeled equations, and inconsistencies in their manuscript that make the methodology challenging to follow and replicate. To address these issues, we offer a clearer and more accurate exposition of the intended approach, providing detailed computations for transparency and understanding. This work is intended to be pedagogical, aiming to enhance comprehension of the inversion process. Additionally, recognizing the scarcity of open-source software for ionogram inversion, we are releasing our code to the community to facilitate further research and application. An introduction for the QP layers and a treatment of how to use them to reconstruct the electron density profile is presented in Sec. II. In Sec. III we present a detailed inversion algorithm using ideas from the previous section. In Sec. IV we describe the forward model, a series of detailed calculations to obtain the virtual heights given a plasma frequency profile. Finally, in Sec. V we show some results of the predicted profiles based on a given ionograms."
https://arxiv.org/html/2411.09234v1,"Wavelet analysis of possible association between sunspot number and rainfall over Kerala, India : A case study","Global attention has been focused on extreme climatic changes. This paper investigates the relationship between different phases of solar activity and extreme precipitation events in Kerala, India. Sunspot number and rainfall data were analysed over 122 years (1901-2022) and separated into winter, pre-monsoon, monsoon, and post-monsoon seasons on an annual scale. The study analysed climatic effects using 31-year mean values and conducted correlation and wavelet analyses (XWT and WTC). A negative correlation was observed in the winter and post-monsoon seasons, while positive correlations were seen in the pre-monsoon and monsoon seasons, all of which were statistically significant. Using cross-wavelet transform (XWT), the temporal relationship between sunspot number and rainfall values was investigated, revealing significant cross-power at an 8-12 year scale across all seasons. Wavelet coherence between the two data sets demonstrated significant correlation at the 2-4 and 4-8 year scales throughout the four seasons. Strong connections were evident at higher periods, such as the 8-16 year scale in the monsoon and post-monsoon seasons. The results show that the seasonal rainfall over Kerala is related to solar activity.The solar phases of Solar Cycles 14-24 were determined for all seasons, and the years with excessive and insufficient rainfall were identified. It was observed that the descending phase had an impact on excess rainfall events during the winter and pre-monsoon seasons, while the ascending phase notably affected the monsoon and post-monsoon seasons. The study specifically examined the different magnetic polarities of sunspots in alternating solar cycles, focusing on even and odd cycles. It was found that extreme rainfall events were more frequent during the winter and pre-monsoon seasons in the even cycles, whereas in the odd cycles, they were more prevalent during the monsoon and post-monsoon seasons. These findings are presented for the first time and may offer new perspectives on how different phases affect rainfall. This study suggests a physical link between solar activity and extreme precipitation in Kerala, which could increase predictability.","Fig. 1: Location map of Kerala Global climate change poses a hazard to human existence. The sun and anthropogenic factors exert a significant influence on weather and climate. The Sun’s magnetic fields exhibit various spatial, temporal, and energetic phenomena. Sunspots, solar flares, solar wind, coronal mass ejections, etc., are all expressions of magnetic activity in the Sun, collectively known as solar activity (Usoskin, 2017). Sunspot number quantifies sunspots and is widely used because of its long-term availability. There has long been concern about how the sun affects precipitation on Earth. Precipitation in different parts of the world appears to be affected by the sun at various intervals. The effect of solar activity on rainfall varies with time scale and region, leading to both positive and negative correlations (Tsiropoula, 2003; Zhao et al., 2004; Wasko & Sharma, 2009; Mauas et al., 2011; Rampelotto et al., 2012). Recently, few studies have been conducted on the relationship between solar and precipitation in China (Zhai, 2017; Yu et al., 2019; Song et al., 2022), the United States (Nitka & Burnecki, 2019), Europe (Laurenz et al., 2019), Africa (Mohamed & El-Mahdy, 2021), Argentina (Heredia et al., 2019), Nepal (Tiwari et al., 2021), and Northeast Asia (Song et al., 2022). The economy, agriculture, and ecosystem in India could be seriously impacted by changing rainfall patterns (Doranalu Chandrashekar et al., 2017). Many researchers have looked into the potential of a connection between solar activity and rainfall throughout India or in various regions (Jagannathan & Bhalme, 1973; Ananthakrishnan & Parthasarathy, 1984; Hiremath & Mandi, 2004a; Bhattacharyya & Narasimha, 2005; Agnihotri et al., 2011; Badruddin & Aslam, 2015; Warrier et al., 2017; Thomas & Abraham, 2022b). The direct and indirect effects were studied, and the results were often localised and contradicted other authors (Jagannathan & Parthasarathy, 1973; Bhalme et al., 1981; Hiremath, 2006; Bhattacharyya & Narasimha, 2007; Lihua et al., 2007; Selvaraj et al., 2009; Selvaraj & Aditya, 2011; Selvaraj et al., 2013; Hiremath et al., 2015; Malik & Brönnimann, 2018; Thomas et al., 2023). Kerala is located at the southwest tip of India, bounded east by the Western Ghats and the west by the Arabian Sea. It extends between 8∘15′ and 12∘50′ north latitudes and between 74∘50′ and 77∘30′ east longitudes. It shares boundaries with Karnataka in the north, Tamil Nadu in the east, and the Arabian Sea in the west. Kerala has a wet and tropical climate, and the major contribution is from the southwest monsoon and post-monsoon. The diverse features of Kerala make it more susceptible to climate change. It is known as the ”Gateway of summer monsoon”. Studies of long-term rainfall variability revealed that rainfall during the southwest monsoon significantly reduced while rainfall during the post-monsoon rose (Krishnakumar et al., 2009; Kothawale & Rajeevan, 2017). Recently, few studies have reported the influence of sunspot number on the rainfall over Kerala (Thomas & Abraham, 2022a, b; Thomas et al., 2023). It is crucial to evaluate how solar activity influences rainfall patterns in various parts of the world as this helps comprehend regional variations, enhancing our knowledge of climate change and its localised effects helping in extreme weather events forecasts. In Kerala, recent extreme rainfall events have resulted in landslides or floods that have claimed lives and destroyed property. In India, several studies have linked solar activity to extreme weather events (see, e.g. Bhalme & Mooley (1981); Azad (2011)). However, research on the influence of different solar phases over rainfall is limited. Therefore, looking at extreme rain over the Kerala region during different solar phases will be interesting. This paper studies the possible relation of rainfall over Kerala with sunspot number using Cross-wavelet transform (XWT) and Wavelet coherence (WTC). The solar phases of Solar Cycles SC14 - SC24 are identified, and their relation with extreme rainfall events is evaluated. Section 2 discusses the data and methodology of analysis. Section 3 presents the results and discussion about the wavelet analysis and occurrences of extreme rainfall events during different solar activity phases during different seasons. Section 4 presents the conclusions."
https://arxiv.org/html/2411.07562v2,Thermodynamic consistency and structure-preservation in summation by parts methods for the moist compressible Euler equations,"Moist thermodynamics is a fundamental driver of atmospheric dynamics across all scales, making accurate modeling of these processes essential for reliable weather forecasts and climate change projections. However, atmospheric models often make a variety of inconsistent approximations in representing moist thermodynamics. These inconsistencies can introduce spurious sources and sinks of energy, potentially compromising the integrity of the models.Here, we present a thermodynamically consistent and structure preserving formulation of the moist compressible Euler equations. When discretised with a summation by parts method, our spatial discretisation conserves: mass, water, entropy, and energy. These properties are achieved by discretising a skew symmetric form of the moist compressible Euler equations, using entropy as a prognostic variable, and the summation-by-parts property of discrete derivative operators. Additionally, we derive a discontinuous Galerkin spectral element method with energy and tracer variance stable numerical fluxes, and experimentally verify our theoretical results through numerical simulations.","Atmospheric models typically utilise multiple different, and often inconsistent, thermodynamic approximations throughout a single code. These thermodynamic inconsistencies introduce resolution independent spurious sources and sinks of energy and hence violate the first law of thermodynamics. For climate models these energy errors can be significant, and are believed to adversely affect the integrity of long-time climate simulations Lauritzen et al. (2018); Eldred et al. (2022). To improve the energy budgets of global climate models, recent studies have explored the application of thermodynamic potentials to ensure thermodynamic consistency Thuburn (2017); Staniforth and White (2019); Eldred et al. (2022). This approach focuses on approximating a single thermodynamic quantity—known as the thermodynamic potential—and deriving all other relevant quantities from it. The most commonly employed potentials in this context are internal energy and the Gibbs function. For a comprehensive overview of thermodynamic potentials we refer the reader to Staniforth and White (2019) and Eldred et al. (2022). The thermodynamic potential approach was first proposed in Thuburn (2017), which introduces a thermodynamically consistent semi-Lagrangian model for a two-phase liquid-vapor system utilizing the Gibbs potential as a function of its natural variables: pressure and temperature. However, extending this method to include a third ice phase poses challenges, as the Gibbs potential does not uniquely define an equilibrium state at the triple point. To address this issue, Bowen and Thuburn (2022a, b) explore an alternative approach that employs internal energy as the potential in a semi-Lagrangian discretization of the three-phase moist compressible Euler equations, effectively sidestepping the triple point ambiguity associated with the Gibbs potential. Similarly, Guba et al. (2024) presents a thermodynamically consistent method that also uses internal energy as the potential, implemented within a horizontally semi-Lagrangian and vertically spectral element discretization. While these methods ensure thermodynamic consistency and therefore discretize an energy-conserving set of equations, it is important to note that the discretizations themselves may not guarantee energy conservation or stability. To the best of their knowledge, the authors are not aware of any mimetic or structure-preserving spatial discretizations for the moist compressible Euler equations, however it is noteworthy that significant progress has been made in developing such methods for the dry compressible Euler equations, as well as for related shallow water and thermal shallow water equations McRae and Cotter (2014); Lee et al. (2018); Lee and Palha (2020); Taylor and Fournier (2010); Eldred et al. (2019); Natale et al. (2016); Cotter (2023). These advancements encompass a variety of approaches, including mixed finite element, continuous Galerkin, and discontinuous Galerkin methods. Although the specifics differ among these techniques and equations, they all maintain structure preservation by discretising the equations in a vector-invariant skew-symmetric form and utilizing summation-by-parts (SBP) operators. Structure preservation may reduce model biases and more accurately represent physical processes, but it does not guarantee numerical stability for compressible flows with active tracers, such as the dry and moist compressible Euler equations and the thermal shallow water equations. We hypothesize that this limitation arises because energy is not a mathematical entropy for these equations. In this context, mathematical entropy refers to any conserved quantity that is a convex function of the prognostic variables; for instance, energy in the shallow water equations and physical entropy in the conservative dry Euler equations. Numerous studies Waruszewski et al. (2022); Ranocha (2020); Ducros et al. (2000); Morinishi (2010); Sjögreen and Yee (2019); Gassner et al. (2016); Hennemann et al. (2021); Pirozzoli (2010) have demonstrated that discrete mathematical entropy stability is both necessary and often sufficient for ensuring numerical stability. These methods often utilize the conservative form of the equations and aim to conserve the physical entropy by employing techniques such as splitting derivative operators, SBP, and entropy stable numerical fluxes. In Ricardo et al. (2024a, b), the authors introduce numerical methods for the thermal shallow water and dry compressible Euler equations that are both mimetic and mathematical entropy stable. These studies identify tracer variance as a form of mathematical entropy and propose a split-form, skew-symmetric formulation of the equations. The resulting formulations are discretized using SBP operators, ensuring both entropy and energy stability. In this work, we extend this structure-preserving and mathematical entropy-stable approach to the moist compressible Euler equations. The result is a thermodynamically consistent skew-symmetric formulation that, when discretized using a carefully designed SBP scheme, maintains both structure preservation and mathematical entropy stability. We then develop a novel energy and mathematical entropy stable discontinuous Galerkin spectral element method (DG-SEM) for these equations. The rest of the paper is as follows. In section 2 we derive an operator-split skew-symmetric formulation of the moist compressible Euler equations. We demonstrate that energy and mathematical entropy conservation can be proved using only integration by parts, and therefore any SBP discretisation of these equations will inherit discrete analogues of these conservation properties. In section 3 we outline the thermodynamic approximation we use in our numerical method. The discrete conservation and stability results are independent of the particular thermodyanmic approximation used but we include this for completeness. In section 4 we introduce DG-SEM and detail our novel DG-SEM discretisation. Following this, in section 5 we prove that our discrete method: conserves mass, water, and entropy; and is semi-discretely energy and (mathematical) entropy stable. These theoretical results are then verified via numerical experiments in section 6. Finally, our conclusions from this study are presented in section 7."
https://arxiv.org/html/2411.07814v1,Community Research Earth Digital Intelligence Twin (CREDIT),"Recent advancements in artificial intelligence (AI) numerical weather prediction (NWP) have significantly transformed atmospheric modeling. AI NWP models outperform physics-based systems like the IFS on several global metrics while requiring far fewer computational resources. Despite these successes, existing AI NWP models face limitations related to their training datasets and timestep choices, often leading to artifacts that hinder model performance. To begin to address these challenges, we introduce the Community Research Earth Digital Intelligence Twin (CREDIT) framework, developed at NSF NCAR. CREDIT provides a flexible, scalable, and user-friendly platform for training and deploying AI-based atmospheric models on high-performance computing systems, offering an end-to-end pipeline for data preprocessing, model training, and evaluation that democratizes access to advanced AI NWP. We showcase CREDIT’s capabilities on a new AI NWP model: WXFormer, a novel deterministic vision transformer designed to autoregressively predict atmospheric states while mitigating common AI NWP model pitfalls, such as compounding error growth, through techniques like spectral normalization, padding, and extensive multi-step training. To show the flexibility of CREDIT and to have a state-of-the-art model comparison we train the FUXI architecture within the CREDIT framework. Our results demonstrate that both FuXi and WXFormer, when trained on 6-hourly hybrid sigma-pressure level ERA5, generally outperform IFS HRES on 10-day forecasts, potentially offering significant improvements in efficiency and forecast accuracy. The modular nature of the CREDIT platform enables researchers to experiment with various models, datasets, and scaled training options, fostering collaboration and innovation in the scientific community.","Rapid advancements in artificial intelligence (AI) numerical weather prediction (NWP) have shaken the foundations of the meteorological community. Spurred by the release of the WeatherBench framework [1] on the ERA5 reanalysis dataset [2], multiple teams spanning motivated individuals [3], universities [4, 5], tech companies [6, 7, 8], non-profits [9], and government agencies [10] have developed a variety of AI NWP models that have quickly advanced and surpassed the headline global verification scores of the ECMWF integrated forecast system (IFS) global model. In addition to improved verification scores, the AI NWP models require orders of magnitude fewer computational resources to run than conventional NWP. The combination of improved forecast performance at minimal cost opens the door for a flurry of new possibilities in how we can interact with NWP models, including much larger ensembles, more rapid updates, and potentially improved forecast performance relative to traditional NWP. Unfortunately, these seemingly major advancements come with some caveats that have become more apparent once the meteorological community started meticulously investigating these AI models. Despite these significant advancements, however, a deeper look into the published AI NWP models reveals common limitations, particularly regarding the data used for training. Most of these models, including the ECMWF IFS NWP model, rely on just five state variables (temperature, u-wind, v-wind, water vapor mixing ratio, and surface pressure), while all other variables are diagnosed from those or are updated solely within parameterizations. Additionally, the IFS vertical coordinate system uses hybrid sigma-pressure levels (i.e., model levels), which follow terrain near the surface and relax to pressure levels aloft. The WeatherBench ERA5 dataset uses pressure level data instead, which works well aloft but intersects with terrain near the surface. ERA5 persists the surface values at pressure levels that are below terrain height with the exceptions of temperature and geopotential height, which are extrapolated [11]. The existing AI NWP models are still able to produce successful forecasts, but these data choices may be causing artifacts in the predictions that then have to be addressed by complex choices in terms of architecture, training procedure, time-stepping, and post-processing. Additionally, most of the published AI NWP models use a 6-hour timestep, or in the case of Pangu-Weather [8], a mixture of models each with a different timestep to delay the accumulation of regression artifacts. Stepping forward the 1-hour Pangu-Weather model results in dramatic error growth after roughly 12 hours, and the error curve with time does not follow the chaotic error growth pattern expected of physics-based models. Our approach aims to address some of the deficiencies of current AI NWP models through several key improvements. We utilize a more fit-for-purpose training dataset that better represents the complexities of atmospheric dynamics. This is complemented by a carefully selected set of input variables that capture essential meteorological processes. Additionally, we employ a computationally efficient and scalable neural network architecture, adapted to handle the intricacies of weather and climate prediction across various temporal and spatial scales. Central to our work is the Community Research Earth Digital Intelligence Twin (CREDIT) framework, developed by the Machine Intelligence Learning for Earth Systems (MILES) group at the NSF National Center for Atmospheric Research (NCAR). While other groups have released model weights and necessary code to run these models, CREDIT aims to provide a comprehensive, end-to-end pipeline. It is designed to scale on standard HPC systems while remaining easily accessible and fully supported. It is fully supported on NSF NCAR’s Derecho supercomputer, a globally recognized resource for atmospheric and climate research. This framework represents a significant step towards democratizing access to weather and climate emulation technologies. By providing a robust, user-friendly platform that encompasses the entire modeling process—from data preprocessing to model training and evaluation—CREDIT enables the broader scientific community, including researchers, educators, and enthusiasts, to engage with and contribute to advanced atmospheric modeling without the typical barriers to entry. To demonstrate the versatility and power of the CREDIT platform, we present two key components in this paper. Firstly, we showcase CREDIT’s ability to support and modify existing models from the literature, such as the FuXi model [5]. This capability allows researchers to build upon and refine established approaches within a standardized framework. Secondly, we introduce WXFormer, a new vision transformer model developed within the CREDIT framework. WXFormer is the latest advancement in deterministic AI-driven atmospheric modeling, specifically designed to autoregressively predict the state of the atmosphere at a selected time resolution. We present 6-hour intervals in this manuscript as many other models currently do and comment on challenges involved with smaller time steps. WXFormer implements several improvements to mitigate compounding error growth, such as spectral normalization of neural network layers, and integrates physical knowledge into its datasets through static variables like solar radiation at the top of the atmosphere. WXFormer incorporates padding techniques to handle the spherical nature of Earth in its global weather simulations. The model employs boundary padding along the map boundaries of [0°-360°] longitude and [-90°-90°] latitude, addressing the challenges posed by polar regions and the dateline. Additionally, spectral normalization is utilized to enhance model stability during training and inference. These design choices enable WXFormer to maintain data continuity and physical consistency across the entire globe, allowing for extended simulation periods. To effectively handle the spherical nature of the Earth in its simulations, WXFormer employs circular padding along the 0-360° longitude line, wrapping data from one edge to the opposite edge to simulate periodic boundaries. This ensures seamless transitions across the dateline. Additionally, a 180-degree shift is applied to align the data correctly at the poles before padding. The top rows from the North Pole are flipped upside down and added above the original data, while the bottom rows from the South Pole are also flipped and added below. This method guarantees smooth transitions at the poles, respecting the convergence of longitudes and preventing discontinuous seams in the data. These enhancements allow WXFormer to perform simulations over extended time periods, depending on the training protocol and goals. The selection of the CrossFormer vision transformer [12] as the backbone for WXFormer provides several advantages over vanilla ViT models. Primarily, it facilitates a hierarchical attention scheme, allowing for smaller patch sizes without dramatically increasing the model size or memory footprint. This architecture also supports efficient scaling across multiple GPUs, surpassing the capabilities of graph-based networks and enabling effective management of escalating data volumes and complexity. With WXFormer we aim to strike an optimal balance between attention window size and the number of parameters, thereby optimizing computational resources. Moreover, its design yields faster performance compared to similarly sized models, promoting rapid iterations and facilitating real-time applications in atmospheric modeling. In this manuscript, we present a comprehensive evaluation of the CREDIT framework and specifically WXFormer’s performance compared to the FuXi model, providing multiple metrics that demonstrate the model’s fidelity across different timescales, from short-term weather predictions to longer-term atmospheric state projections. We also address the model’s shortfalls, acknowledging the challenges that are pervasive in the broader landscape of autoregressive machine-learned atmospheric models. By openly discussing these limitations, we aim to foster a transparent dialogue within the community and pave the way for future improvements in AI-driven atmospheric science."
https://arxiv.org/html/2411.07263v1,Analysis and Forecasting of the Dynamics of a Floating Wind Turbine Using Dynamic Mode Decomposition,"This article presents a data-driven equation-free modeling of the dynamics of a hexafloat floating offshore wind turbine based on the Dynamic Mode Decomposition (DMD). The DMD is here used to provide a modal analysis and extract knowledge from the dynamic system. A forecasting algorithm for the motions, accelerations, and forces acting on the floating system, as well as the height of the incoming waves, the wind speed, and the power extracted by the wind turbine, is developed by using a methodological extension called Hankel-DMD, that includes time-delayed copies of the states in an augmented state vector. All the analyses are performed on experimental data collected from an operating prototype. The quality of the forecasts obtained varying two main hyperparameters of the algorithm, namely the number of delayed copies and the length of the observation time, is assessed using three different error metrics, each analyzing complementary aspects of the prediction. A statistical analysis exposed the existence of optimal values for the algorithm hyperparameters. Results show the approach’s capability for short-term future estimates of the system’s state, which can be used for real-time prediction and control. Furthermore, a novel Stochastic Hankel-DMD formulation is introduced by considering hyperparameters as stochastic variables. The stochastic version of the method not only enriches the prediction with its related uncertainty but is also found to improve the normalized root mean square error up to 10% on a statistical basis compared to the deterministic counterpart.","In the strive of containing the global temperature increase under 2∘C below pre-industrial levels, as set by the Paris Agreement [19], most countries have committed to reaching the goal of net zero emissions by 2050, meaning that all the greenhouse gas emissions must be counterbalanced by an equal amount of removals from the atmosphere. To reach this critical and ambitious task for sustainable growth, the decarbonization of our society is a key aspect that passes through the decarbonization of energy production [44, 26]. The shift from fossil fuels to renewable sources for power production is to be considered the fundamental step in the process. Power generation is, in fact, responsible for 30% of the global carbon dioxide emissions at the moment. In 2018, the European Union set intermediate targets of 20% of energy obtained from renewable resources by 2020 and 32% by 2030, the latter has been raised to 42.5% (with the aspiration of reaching 45%) by amending the Renewable Energy Directive in 2021 [68]. Reaching the mentioned targets means almost doubling the existing share of renewable energy in the EU. Wind energy technology has been identified as one of the most promising ones, along with photovoltaic, for power production from renewable sources. Several growth scenarios predict a prominent role of wind power, exceeding the 35% share of the total electricity demand by 2050 [47], representing a nearly nine-fold rise in the wind power share in the total generation mix compared to 2016 levels. Offshore wind energy production has a bigger growth potential compared to its onshore counterpart. The reasons are connected to fewer technical, logistic, and social restrictions of the former. Offshore installed turbines may exploit abundant and more consistent winds, helped by the reduced friction of the sea surface and the absence of surrounding hills and buildings [31]. In addition, offshore wind farms benefit a greater social acceptance, a minor value of their occupied space, and the possibility of installing larger turbines with fewer transportation issues than onshore [49, 50, 39]. The 2023 Global Offshore Wind Report predicts the installation of more than 380 GW of offshore wind capacity worldwide in the next ten years [9]. The exponential growth of the sector passes through the possibility of realizing floating offshore plants, enabling the exploitation of sea areas with deeper water that make fixed-foundation turbines not a feasible/affordable solution (indicatively deeper than 60 m). The main advantage of exploiting deep offshore sea areas relies on the abundant and steady winds characterizing them. One of the main limiting factors in the reduction of the levelized cost of energy (LCOE) of advanced floating offshore wind turbines (FOWTs) is the current size and cost of their platforms. Its reduction, alongside the development of advanced moorings, improved control systems, and maintenance procedures is among the most impacting technical goals research activities are focusing on. The power production by FOWTs presents additional challenges compared to the fixed wind turbine counterpart (on- or offshore) which are inherent to their floating characteristic that adds six degrees of freedom to the structures. Nonlinear hydrodynamic loads, wave-current interactions, aero-hydrodynamics coupling producing negative aerodynamic damping, and wind-induced low-frequency rotations are among the main causes of large amplitude motions of the platform. These are in turn causes of a reduction in the average power output of the power plant and of an increase in the fluctuations of the produced power. Both quantity and quality of the power production are affected and the structure and all the components (blades, cables, bearings, etc.) also suffer increased fatigue-induced wear from non-constant loadings [43] (about 20% of operation and maintenance costs come from blade failures [20] and almost 70% of the gearbox downtime is due to bearing faults [10]). The floating wind turbine operations are considerably altered by the stochastic nature of wind, waves, and currents in the sea environment, which excite platform motion leading to uncertainties in structural loads and power extraction capability. As shown first in [27], waves are responsible for a large part of the dynamic excitation of a FOWT. Rotor speed fluctuations are 60% larger when the same turbine is operated in a floating environment compared to onshore installation, and the difference has been shown to increase with increasing wave conditions. Therefore, it is essential to develop appropriate strategies to improve the FOWTs’ platform stability and maximize the turbines’ energy conversion rate, achieving better LCOE with higher power production and lower maintenance operation costs. Both passive and active technologies have been studied and developed to the scope, such as tuned mass dampers [63, 37, 14] mounted on different floaters, ballasted buoyancy cans [67, 22, 23], gyro-stabilizers [45], blade pitch and/or torque controllers [39, 56, 69, 49, 50]. Developing advanced control systems is a high-potential cost reduction strategy for offshore wind turbines impacting at multiple levels: effective control strategies may increase the energy production which has a direct impact on LCOE; a reduction of the platform motions may help in reducing their sizes and costs; reduced vibratory loads on the turbine’s components helps increasing their lifetime and reducing maintenance costs. A thorough comparison of various controllers designed for managing vibratory loads is provided in [2]. Among all the methods analyzed, the strategies that employ blade pitch control, collective or individual, emerge as the most effective and promising, combining feasibility and reliability. Particular care must be taken in floating systems when applying controls: closed-loop instability may be induced by the coupling between the motions of the platform (particularly pitch dynamics) and the aerodynamic loads [35, 28] for poorly designed controllers acting on the pitch of the blades. In floating wind turbines, power production, load reduction, pitch activity, and platform stabilization are multiple conflicting objectives the controller has to deal with, and a trade-off between the opposing needs is to be found [36]. Traditional control algorithms (PI, Gain Scheduled PI, PID) are typically of single-input-single-output type and struggle to deal with multiple objectives [35, 28, 72, 36]. Optimal algorithms, such as linear quadratic regulator [49, 57], model predictive control [71], and H_{2} and H_{\infty} methods [3, 5] have also been applied to FOWT with good results overcoming the mentioned issue, but at the cost of an increased controller design complexity. Recently, a multi-objective multilayer control algorithm was proposed in [50], combining two resonant controllers based on collective pitch actuation and a proportional-integral controller providing the blade cyclic pitch. The authors demonstrated the strategy to effectively reject the vibratory loads induced by the sea waves, and reduce the blade root loads at the rotor revolution frequency and the floater motions. However, high pitch rates and actuation power were found to be needed for individual pitch control. All the mentioned strategies work with a feedback loop on the actuation variable. On the other side, there is the feedforward control system, often coupled with model predictive control, which is based on knowledge about the dynamic system in the form of a predictive model and the disturbances it experiences. The two philosophies may also be effectively coupled, creating a feedforward control with a feedback loop such as in [1], where real-time prediction of the free-surface elevation is exploited to compensate for the wave disturbances on the FOWT. The wave-induced rotor speed variations, and consequently structural loads on the turbine, are successfully reduced in a high-fidelity simulated environment. Feedforward controllers have been deemed and demonstrated to be effective strategies for further improving the control of FOWTs [24, 34, 15, 16, 17, 64]. A collective pitch feedforward controller was designed in [58] showing the ability to regulate rotor speed under an extreme gust, using information about the wind field by a LIDAR (light detection and ranging) system virtually mounted on the turbine’s nacelle. The methodology was extended to a model predictive individual pitch control in [51]. In general, the availability of environmental data to the controller enhances its control action capability: the use of information about incoming waves to design a feedforward control strategy for a 10MW FOWT is presented in [21], showing improved quality of the produced power, and increased turbine fatigue life. In [40], a finite-horizon LQR (linear–quadratic regulator) controller is designed using wave elevation and forces data from a real-time forecasting algorithm. The controller is applied to a tension-leg platform FOWT to mitigate structural loads on the tower. Such feedforward algorithms rely on forecasting techniques to estimate upcoming disturbances or changes in the state to be controlled. Hence, the predictive algorithm is invested of a primary role in the success of the control strategy. Several machine learning algorithms have been implemented in the literature to predict relevant quantities for FOWT and control. In [53], the delay introduced by the hydraulic actuator of the blades’ pitch angle control is addressed. The blade pitch angle is predicted by a long short-term memory neural network and its signal is exploited to compensate for the actuation delay, resulting in improved control performances. In [40], two different forecasting algorithms are developed to predict the wave elevation and feed a feedforward LQR controller: one is based on the Prony method and identifies the signal as a sum of exponentials; the other uses Least-squares Support Vector Machines regression to estimate the forecasted signal from a high dimensional feature space of the input vector. The study in [4] adopts a convolutional neural network merged with a gated recurrent unit network for the forecasting of the dynamic behavior of FOWTs. Albeit powerful, machine learning and deep learning methods typically require large training datasets, comprising a range of operating conditions as complete as possible to learn patterns that generalize to new situations. The training of such algorithms can be computationally expensive, and typically not compatible with real-time learning and digital twinning, where the system characteristics and response to external perturbations change with the system aging. The objective of this paper is to explore the usage of Dynamic Mode Decomposition (DMD) as a method to extract knowledge and develop a forecasting algorithm for relevant quantities of a FOWT from experimental data, avoiding the shortcomings of machine learning approaches to obtain a continuously learning reduced order model more suitable for digital twinning. DMD is a powerful data-driven technique used for analyzing the temporal evolution of complex systems. It was originally developed in the field of fluid dynamics where it has been employed to identify coherent structures, e.g. in vorticity generated by bluff bodies [66], jet turbulence [60], noise generated in jets mixing layers [65], and predict their evolution or perform stability analysis of complex flows [54, 59] and magnetized plasma [30]. DMD has then found applications in various scientific and engineering domains, including structural dynamics, epidemiology analyses [48], and neuroscience [6] extracting spatial-temporal coherent patterns in large-scale neural recordings. At its core, DMD extracts coherent spatiotemporal patterns from high-dimensional data, and it has been originally introduced as a method to identify linear normal modes in linear systems. The DMD has then evolved to capture the underlying dynamics and coherent structures of complex, nonlinear systems, leveraging the concept of the Koopman operator. As a main characteristic, the DMD is an equation-free and data-driven method, operating on measured or simulated data without requiring detailed knowledge of the equations governing the evolution of the state of the system under analysis. DMD doesn’t assume any specific system dynamics; instead, it uncovers underlying structures directly from the data. This makes DMD particularly well-suited for practical applications, where obtaining a precise mathematical model is challenging or impractical, contributing to its growing success in the scientific community in several fields [8]. Given a time series of data, the DMD computes a set of modes and frequencies approximating the eigenmodes and eigenvalues of the Koopman operator (also called the composition operator), an infinite-dimensional linear operator associated with the time evolution of the system under analysis, even in the presence of nonlinearities. Some main challenges have to be addressed in using the algorithm: i) DMD is sensitive to noise in the data, which deteriorates its capability of mode extraction and hence predictive ability; ii) DMD relies on Krylov subspaces for model approximation, which proper closure is challenging in nonlinear systems: if the system exhibits strong nonlinear behavior, DMD may struggle to capture all relevant modes. Despite these limitations, DMD remains a valuable tool for extracting patterns from time series data. The modes and frequencies identified by the DMD may also be used to build a predictive model of the system under analysis. Compared to the other applications, the literature about time series forecasting by using DMD is less extensive. Nevertheless, interesting applications in very different fields can be found: an algorithmic trading strategy based on DMD is demonstrated in [41], allowing for a prediction of the market dynamics to inform a financial investment strategy; the work in [18] describes a power systems engineering application in which a model able to forecast power grid loads is obtained by applying the DMD with state augmentations; the analysis and forecast of the trajectories/motions/forces of ships operating in waves is presented in [12, 62], where the DMD is employed to produce short-term future estimates of the system’s state, suggesting the relevance for real-time prediction and control; the same naval application is studied in [13] developing a hybrid prediction algorithm combining DMD and artificial neural network, and in [11] comparing the performances of different neural network architecture and a DMD implementation using an augmented state by embedding delayed time histories and time derivatives. All the mentioned works show promising results and good prediction capabilities in short-term and medium-term perspectives with accuracy and reduced computational effort. The DMD does not fall under the category of machine learning techniques, however, the machine learning terminology can be borrowed to highlight some peculiarity of the method. In this view, the evaluation of the DMD modes and frequencies can be called the training phase, and the time signals to feed the algorithm the training set. The main advantage of the DMD over proper machine learning and deep learning methods is its one-shot training time and corresponding lower computational cost, only related to direct linear algebra operations as detailed in Section 2.2. The DMD is inherently suitable for continuous learning, and its system identification naturally adapts to a changing system and environment, making it a promising tool also in the context of digital twins. In this paper, for the first time to the best of the authors’ knowledge, the DMD is used to improve knowledge and develop a forecasting algorithm for relevant quantities of a FOWT. In particular, a methodology extension of the DMD, called Hankel-DMD (HDMD) (also called Augmented DMD or delay DMD) [7, 29, 62] is applied to this scope. This specific version of the DMD algorithm has been developed to deal with the cases in which only partial observations of the system are available such that there are latent variables. The state vector is thus augmented, embedding time-delayed copies of the original variables, resulting in an intrinsic coordinate system that forms a Koopman-invariant subspace (the time-delays form a set of observable functions that span a finite-dimensional subspace of Hilbert space, in which the Koopman operator preserves the structure of the system) [7, 46]. The method is applied to real-life measured data obtained by various sensors mounted on the floating platform and the turbine, focusing on short-term and medium-term prediction horizons using the incoming wave period as a reference. The experimental activity has been conducted on a scale prototype of a 5MW Hexafloat FOWT, the first of its type, as part of the National Research Project RdS-Electrical Energy from the Sea, funded by the Italian Ministry for the Environment (MaSE) and coordinated by CNR-INM. Two main hyperparameters of the algorithm and their effect on the predictions in the studied case are studied and discussed by testing a full-factorial combination of the setting parameters. To this aim, three evaluation metrics are statistically assessed for each combination against a relevant number of time sequences samples, randomly extracted from the data record. Moreover, a stochastic version of the Hankel-DMD (SHDMD) is obtained by considering the hyperparameters as random variables with uniform distribution in a suitable range, obtaining a mean prediction with associated uncertainty. It is finally shown that the application of the SHDMD leads to improved predictive performances compared to the deterministic counterpart. The paper is organized as follows. Section 2 presents the wind turbine test case, details the DMD methods applied, and introduces the performance metrics used to assess the predictive performances of the algorithms. The numerical setup and the data preprocessing are described in Section 3. Section 4 collects the results from the modal analysis and the forecasting of the quantities of interest obtained with the deterministic Hankel-DMD and its stochastic version. Finally, conclusions about the conducted analyses are resumed in Section 5."
https://arxiv.org/html/2411.07248v1,Do Inner Greenland’s Melt Rate Dynamics Approach Coastal Ones?,"The Greenland Ice Sheet may be nearing a tipping point, transitioning to permanent melting. This article analyses two melt rate time series using the Bayesian Langevin estimation (BLE), providing further evidence for destabilizing melt dynamics, along with new insights from the method’s nonlinear parameterisation. Comparing the results for Western Central Greenland and the Nuussuaq peninsula suggests that inland melt dynamics may approach less stable coastal dynamics on slow and fast scales. Both datasets show a significant increase in fast-scale amplitudes since the 1970s, possibly driven by large-scale atmospheric fluctuations. Additionally, the BLE’s nonlinear drift proves crucial for obtaining these results, as a linear Ornstein-Uhlenbeck process fails to capture these dynamics due to the oversimplification of a strictly positive autocorrelation function.","Climate change is one of the most critical issues of the 21st century and a major focus in the global scientific community. Without a doubt, the Earth’s climate is a complex system, for which nonlinear modelling and empirical time series analysis have become essential tools to deepen our understanding. Building on other pioneering works in the field, Svante Arrhenius [1] provided the first explanation of the greenhouse effect in 1896. Since then, many phenomena and dependencies of the Earth’s climate have been profoundly studied, often focusing on climate subsystems [2], such as the Greenland Ice Sheet (GrIS) [3]. The GrIS is believed to be nearing a tipping point or may potentially already be in a transient phase beyond it. In 2021, Boers and Rypdal [3] analysed melt rate time series extracted from stacked ice cores from Central Western Greenland (CWG) and an ice core from the Nuussuaq (NU) peninsula. Their results imply that the GrIS is threatened by a nearby tipping point at which its current state is no longer maintainable. Furthermore, the primary driver of the changing dynamics is identified as the melt-elevation feedback, i.e., the increase in melt rates with decreasing ice sheet height. They utilized the non-parametric statistical lag-1 autocorrelation (AR1) \rho_{1} and standard deviation (STD) \hat{\sigma}_{\rm d} and found significant positive trends in both time series. In Section 3.1, we complement these analyses by the parametric Bayesian Langevin estimation (BLE) [4; 5], which adds further evidence for decreasing resilience, along with new insights. The results suggest that the inland melt rate dynamics from CWG started to destabilize around 1914, approaoching the less stable niveau of coastal areas around 1975. Additionally, a recent increase of fast-scale contributions in the late 1990s in both CWG and the NU peninsula may be driven by a common fast-scale driver such as large-scale fluctuations in the atmospheric circulation. In particular, the results and implications of the BLE analysis for both datasets are not reproducible using the transformation from the AR1 and STD into the quantitative metrics of the Ornstein-Uhlenbeck estimation (OUE) (cf. Section 3.2). This limitation arises due to the multidecadal variability [6] present in the GrIS melt rates and the overall notably less accurate OUE. These findings are illustrated by a comparison between the BLE and the OUE in Section 3.2. For a streamlined presentation, both the OUE and BLE are described beforehand in Section 2."
https://arxiv.org/html/2411.06623v1,"Forecast error growth: A dynamic–stochastic
model","There is a history of simple forecast error growth models designed to capture the key properties of error growth in operational numerical weather prediction (NWP) models. We propose here such a scalar model that relies on the previous ones and incorporates multiplicative noise in a nonlinear stochastic differential equation (SDE). We analyze the properties of the SDE, including the shape of the error growth curve for small times and its stationary distribution, and prove well-posedness and positivity of solutions. We then fit this model to operational NWP error growth curves, showing good agreement with both the mean and probabilistic features of the error growth. These results suggest that the dynamic–stochastic error growth model proposed herein and similar ones could play a role in many other areas of the sciences that involve prediction.This paper is dedicated to the memory of Eugenia Kalnay and to her fundamental contributions to research in predictability and weather forecasting.","Quantifying and understanding forecast error growth plays a key role in prediction in many fields of the natural and socio-economic sciences. Besides the extensive literature on the study of error growth in numerical weather prediction (NWP), it also plays an important role in predicting other parts of the Earth systemWirth and Ghil (2000), as well as in biologyBaird and Suthers (2010); Pei and Shaman (2017) and the social sciencesFarmer and Lafond (2016). Consider a dynamical system \mathrm{d}x=f(x)\,\mathrm{d}t+\Sigma^{1/2}(x)\,\mathrm{d}B, (1) where \Sigma is a symmetric positive definite covariance matrix and B is a Wiener process. Given a true trajectory x^{\dagger}(t) and a forecast trajectory x(t), both generated from Eq. (1), one can measure the forecast error as e^{2}(t)=\|x(t)-x^{\dagger}(t)\|^{2}_{2}, (2) where \|\cdot\|_{2} is the Euclidean norm; other norms can also be considered. If x^{\dagger}(0)\neq x(0) and the system is chaotic in the absence of noise, e(t) will generally grow in time until reaching a saturation level. If the noise realization is different in x^{\dagger}(t) and x(t), the noise will also contribute to the error growth. Moreover, models typically differ from the system being modeled—i.e., x^{\dagger}(t) and x(t) come from different systems—and this systematic model error contributes to the error growth. I.1 Stochastic treatment of error growth Suppose we have some uncertainty in the knowledge of the initial conditions x^{\dagger}(0), represented by a probability distribution \rho_{0}(x), such that x^{\dagger}(0)\sim\rho_{0}(x). This probability distribution \rho(x,t) will evolve under the dynamics according to the Fokker–Planck equation: \frac{\partial\rho}{\partial t}=-\nabla\cdot(\rho f)+\frac{1}{2}\nabla\cdot% \bigl{(}\nabla\cdot(\rho\Sigma)\bigr{)}, (3) with \rho(x,0)=\rho_{0}(x), and where the divergence of a matrix S is defined by the identity (\nabla\cdot S)\cdot a=\nabla\cdot(S^{T}a) for any vector a. If we further assume that a forecast was initialized from x(0)\sim\rho_{0}, then x(t)\sim\rho(x,t). e(t) will be a random variable whose properties can be derived from those of \rho(t). For example, the expected squared error \displaystyle\mathbb{E}[e(t)^{2}] \displaystyle=\mathbb{E}[\|x(t)-x^{\dagger}(t)\|_{2}^{2}] (4a) \displaystyle=\mathbb{E}[\|x(t)-\mathbb{E}[x(t)]\|_{2}^{2}+\|x^{\dagger}(t)-% \mathbb{E}[x(t)]\|_{2}^{2}], \displaystyle=2\text{tr}(\text{Cov}[x(t)]), (4b) where the cross-covariance terms vanish because x and x^{\dagger} are independent and identically distributed. If \rho(t) converges to a unique invariant measure, that is, \rho(t)\to\rho_{\infty} as t\to\infty, then \text{Cov}[x(t)]\to C_{\infty}, known in climate contexts as the climatological variance. When the forecast error reaches the climatological variance, the forecast has lost skill completely, and the forecast with the lowest error is the mean of \rho_{\infty}, with an expected squared error of \text{tr}(C_{\infty}); see Leith (1974)Leith (1974). Additional statistical properties of e(t) can also be considered. In ensemble prediction, such as that used in NWPKalnay (2019), a set of M initial conditions drawn from \rho_{0}, \{x^{(i)}\}_{i=1}^{M}, are evolved in time under the dynamics Eq. 1. For a perfect ensemble, each ensemble member will be statistically indistinguishable from the true trajectory Palmer et al. (2006). Thus, the error of each member will be a sample of the random variable e(t) discussed above. The preceding discussion shows that uncertainty in the initial conditions leads naturally to a probabilistic treatment of error growth, even in the case of a deterministic dynamical system where \Sigma=0. Moreover, sources of error growth in deterministic systems that may nonetheless be modeled as stochastic include fluctuations in the finite-time Lyapunov exponentsNicolis (1992), the impact of small and fast scales on large and slow ones, and model error Ehrendorfer (1994). Other works to take a probabilistic perspective on forecast error growth include Balgovind et al. (1983)Balgovind et al. (1983), Benzi and Carnevale (1989)Benzi and Carnevale (1989), Ehrendorfer (1994)Ehrendorfer (1994), Ivanov et al. (1994)Ivanov, Kirwan, and Melnichenko (1994), and Chu and Ivanov (2002)Chu and Ivanov (2002). Here we do not deal with these sources of error explicitly, but do consider their cumulative effect. I.2 Previous error growth models in NWP Considerable attention has been given to forecast error growth in the forecast–assimilation (FA) cycle of operational NWP models, with a number of scalar error growth models having been proposed so far. For illustration purposes, we start by considering here four different models for error growth in NWP, namely those of C. E. Leith Leith (1978), E. N. Lorenz Lorenz (1982), A. Dalcher and E. Kalnay Dalcher and Kalnay (1987), and C. NicolisNicolis (1992); see also Appendix A of Crisan and Ghil (2023)Crisan and Ghil (2023) and Krishnamurthy (2019)Krishnamurthy (2019) for reviews of the literature. Letting v(t)=e(t)^{2}, the forecast error model of Leith (1978)Leith (1978) is given by the scalar, linear inhomogeneous ordinary differential equation (ODE) \frac{\text{d}v}{\text{d}t}=\alpha v+s, (5) where \alpha>0 measures the rate of growth of small errors, s is the systematic model error, and t is the lead time. The solution to this ODE is given explicitly by v(t)=\left(v_{0}+\frac{s}{\alpha}\right)e^{\alpha t}-\frac{s}{\alpha}, (6) where v_{0}=v(0) is the initial error. Note that short-time forecast errors grow exponentially and that the systematic model error acts to increase the coefficient of this growth. Leith’s forecast error model can only apply for short-time error growth, since it does not saturate. Another model of forecast error growth was proposed by Lorenz (1982)Lorenz (1982) and is governed by the scalar ODE with quadratic right-hand side \frac{\text{d}e}{\text{d}t}=ae(e_{\infty}-e), (7) where e_{\infty} is a saturation value. While Lorenz’s model does include a nonlinear saturation term, it does not incorporate systematic model error. The error model proposed by Dalcher and Kalnay (1987)Dalcher and Kalnay (1987), henceforth DK, combines the key features of the Leith and Lorenz models, \frac{\text{d}v}{\text{d}t}=(\alpha v+s)(1-v/v_{\infty}); (8) it thus includes both a saturation level v_{\infty} and systematic model error s. For short-time error growth, we can take v_{\infty}\to\infty, recovering Leith’s model. For s=0, the model is similar to that of Lorenz. DK fit this error growth model to operational NWP forecasts from the European Centre for Medium-Range Weather Forecasting (ECMWF), finding close agreement out to 10 days. Note that s has previously been taken to represent sources of error other than systematic model error as well Zhang et al. (2019). Nicolis (1992)Nicolis (1992) proposed a stochastic error model of the form \text{d}\xi=(\sigma-g\xi)\xi\,\text{d}t+q\xi\,\text{d}W, (9) where \xi is a measure of error along the unstable direction, W is a Wiener process, and g\geq 0 and q are constants. To the best of our knowledge, this work is the only stochastic model of error growth previously proposed. The formulation (9) is justified in the paper by considering a toy model of the atmosphere and introducing stochasticity coming from fluctuations in the finite-time Lyapunov exponents. This model was also analyzed by Chu et al. (2002)Chu et al. (2002). If we remove the stochasticity by setting q=0, (9) has the same form as the Lorenz model. This model does not appear to have been calibrated to operational NWP error growth. Other error growth models include those of Schubert and Suarez (1989)Schubert and Suarez (1989) and Stroe and Royer (1993)Stroe and Royer (1993). In the present paper, we build on the previous error growth models above by adding multiplicative noise, as in the Nicolis model, to the DK model. The paper is structured as follows. In the next section, we introduce the proposed stochastic model with multiplicative noise and study its properties, including its stationary distribution and first passage times. We then fit the model to operational NWP error curves in Sec. III. In Sec. IV, we discuss the results so obtained. We also comment on the likelihood of these results applying to other areas of the sciences where prediction is important. In Appendix A, we prove well-posedness and positivity of our nonlinear stochastic model, with other model properties discussed in Appendices B and C."
https://arxiv.org/html/2411.06604v1,An Analysis of Deep Learning Parameterizations for Ocean Subgrid Eddy Forcing,"Due to computational constraints, climate simulations cannot resolve a range of small-scale physical processes, which have a significant impact on the large-scale evolution of the climate system. Parameterization is an approach to capture the effect of these processes, without resolving them explicitly. In recent years, data-driven parameterizations based on convolutional neural networks have obtained promising results. In this work, we provide an in-depth analysis of these parameterizations developed using data from ocean simulations. The parametrizations account for the effect of mesoscale eddies toward improving simulations of momentum, heat, and mass exchange in the ocean. Our results provide several insights into the properties of data-driven parameterizations based on neural networks. First, their performance can be substantially improved by increasing the geographic extent of the training data. Second, they learn nonlinear structure, since they are able to outperform a linear baseline. Third, they generalize robustly across different \text{CO}_{2} forcings, but not necessarily across different ocean depths. Fourth, they exploit a relatively small region of their input to generate their output. Our results will guide the further development of ocean mesoscale eddy parameterizations, and multiscale modeling more generally.","Despite advances in hardware, climate simulations spanning decades or centuries are limited in their spatial resolution to tens of kilometers ([balaji2021climbing]). This resolution is insufficient to resolve mesoscale eddies that are crucial for the exchange of momentum, heat, and mass ([capet2008mesoscale, salmon1980baroclinic]) as well as for large-scale ocean circulation ([hallberg2013using, keating2012new, waterman2011eddy]). Parameterization is an approach to capture the effect of small-scale processes on the large-scale variables in climate models without resolving them explicitly. The main idea is to modify the governing equations of a low-resolution model by adding a forcing term called subgrid forcing. This term encapsulates the missing physics, representing the effect of the unresolved physical processes on the resolved variables in the climate model. The key challenge is that the subgrid forcing is a function of high-resolution quantities, but only low-resolution quantities are observed in the model. Traditional parameterization approaches are typically rooted in first-principle analysis of the climate-model physics ([randall2003breaking, bony2015clouds, schneider2017climate]). In recent years, data-driven parameterizations have been developed with promising results ([yuval2020stable, beucler2021enforcing, frezat2021physical, GUAN2022111090, zanna2020data, guillaumin2021stochastic, zhang2023implementation, sane2023verticalmixing, bodner2023submeso, perezhogin2023implementation]). In the data-driven framework, a high-resolution simulation capable of resolving the physical processes of interest is utilized as ground truth. The resolution of the data is then artificially reduced via filtering and coarse-graining to enable the computation of ""ground-truth"" subgrid forcing. Machine learning (ML) algorithms are used to predict this forcing from the low-resolution data. In the case of mesoscale eddies, applying this framework to convolutional neural networks (CNNs) has achieved promising results ([zanna2020data, guillaumin2021stochastic]). Our goal in this work is to provide an in-depth study of CNNs for parameterization. We utilize the CM2.6 dataset as our source of high-resolution data. CM2.6 is a publicly-available advanced coupled climate model ([griffies2015handbook]) with an approximate resolution of 0.1∘, making it well-suited for accurately representing mesoscale eddies ([HALLBERG201392]). The dataset includes surface and subsurface data for two levels of \text{CO}_{2} in the atmosphere. [guillaumin2021stochastic] proposed a CNN-parameterization for surface momentum fields in CM2.6 at 0.4^{\circ} resolution. Here, we extend their approach to parameterize temperature, and build upon it to study the following key questions about CNN-based mesoscale-eddy parameterizations. Does the geographic extent of the training dataset matter? In guillaumin2021stochastic, the training domain was constrained to four relatively small regions of the Pacific and Atlantic Oceans. We show that extending the training dataset to encompass the entire global ocean surface leads to a substantial improvement in performance, particularly for temperature. Do the properties of CNN parameterizations change at different resolutions? When training a CNN parameterization, an important consideration is how to choose the resolution of the target low-resolution climate model. We studied the performance of CNN-based parameterizations at resolutions of 0.4^{\circ}, 0.8^{\circ}, 1.2^{\circ}, and 1.6^{\circ}. Our results show to what extent reducing resolution decreases the skill of the parameterizations, and also demonstrate that their properties remain similar across the different resolutions. Do CNN-parameterizations just invert the coarse-graining and filtering operator? As explained above, CNN-parameterizations estimate subgrid forcing from low-resolution data, obtained from a high-resolution model via filtering and coarse-graining. These filtering and coarse-graining operators are linear and can be partially inverted to estimate subgrid turbulence fields (and hence the subgrid forcing itself) (langford1999optimal). Examples of subgrid parameterizations based on partial filter inversion include the velocity gradient model (clark1979evaluation; chow2005explicit), the scale-similarity model (bardina1980improved; meneveau2000scale) and the approximate deconvolution model (stolz1999approximate). An important question is whether CNN-parameterizations just learn to implement this partial inversion. We developed a baseline parameterization solely based on linear inversion to answer it. Our results indicate that CNNs outperform this baseline parameterization, suggesting that they are able to leverage physical structure and do not merely learn to perform inversion. Do data-driven parameterizations generalize across \text{CO}_{2} levels and ocean depths? Understanding the generalization properties of data-driven parameterizations is crucial for their deployment in realistic models. In particular, determining their behavior at different \text{CO}_{2} levels is critical for long-term climate simulations and predictions. Our results show that a CNN-based parameterization trained at pre-industrial \text{CO}_{2} levels generalizes robustly at significantly increased \text{CO}_{2} levels. In addition, we investigate the generalization ability these models across different ocean depths, ranging from the surface down to 728 meters. In MOM-suite climate models, the ocean is represented as vertically stacked, irregularly spaced horizontal layers (griffies2015handbook), with substantially different dynamics. At the surface, wind, solar radiation, and atmospheric interactions dominate circulation, whereas at greater depths, dynamics depend more on temperature and salinity (salmon1980baroclinic). Our results show that surface-trained models have difficulties generalizing at greater depths, and, conversely, models trained beyond 55 meters do not generalize robustly to the surface. What spatial extent do CNN-parameterizations require as input? The spatial extent needed to compute a parameterization at each grid point greatly influences its computational cost. In addition, it informs the degree of nonlocality needed to approximate the local subgrid forcing. In order to investigate its impact on CNN-parameterizations, we evaluated the performance of multiple CNNs with a range of input sizes. These experiments were complemented with a gradient-based sensitivity analysis. Our results indicate that a much smaller input size than the one used in previous models is sufficient to achieve strong performance. The paper is organized into five sections, including the introduction. In Section 2, we describe the governing equations of the climate simulation, and explain how we generate the data to train the data-driven parameterizations. In Section 3, we define data-driven parameterizations based on deep learning, as well as a baseline parameterization based on linear inversion. In Section 4, we describe our experiments and present the results. Finally, in Section 5, we summarize our conclusions and discuss directions for future research."
https://arxiv.org/html/2411.06574v1,Machine Learning-based Denoising of Surface Solar Irradiance simulated with Monte Carlo Ray Tracing,"Simulating radiative transfer in the atmosphere with Monte Carlo ray tracing provides realistic surface irradiance in cloud-resolving models. However, Monte Carlo methods are computationally expensive because large sampling budgets are required to obtain sufficient convergence. Here, we explore the use of machine learning for denoising direct and diffuse surface solar irradiance fields. We use Monte Carlo ray tracing to compute pairs of noisy and well-converged surface irradiance fields for an ensemble of cumulus cloud fields and solar angles, and train a denoising autoencoder to predict the well-converged irradiance fields from the noisy input. We demonstrate that denoising diffuse irradiance from 1 sample per pixel (per spectral quadrature point) is an order of magnitude faster and twice as accurate as ray tracing with 128 samples per pixel, illustrating the advantage of denoising over larger sampling budgets. Denoising of direct irradiance is effective in sunlit areas, while errors persist on the edges of cloud shadows. For diffuse irradiance, providing additional atmospheric information such as liquid water paths and solar angles to train the denoising algorithm reduces errors by approximately a factor of two. Our results open up possibilities for coupled Monte Carlo ray tracing with computational costs approaching those of two-stream-based radiative transfer solvers, although future work is needed to improve generalization across resolutions and cloud types.","Monte Carlo ray tracing is one of the the most accurate techniques for simulating atmospheric radiative transfer [Cahalan \BOthers. (\APACyear2005)]. Monte Carlo ray tracing can deliver surface irradiance patterns closely resembling observations, capturing the characteristic bimodal distribution under cumulus clouds and associated areas with cloud enhancements [Gristey \BOthers. (\APACyear2020\APACexlab\BCnt2), Z. He \BOthers. (\APACyear2024)]. Recently, \citeAVeerman2022 and \citeATijhuis2024 coupled a Monte Carlo ray tracer to a Large-Eddy Simulation (LES), showing that these irradiance patterns can have strong consequences for cloud evolution, with locally enhanced surface heat fluxes resulting in thicker and wider clouds. Monte Carlo ray tracing requires large sampling budgets to obtain sufficiently converged irradiance fields, which is computationally expensive and limits its application in operational contexts [Z. He \BOthers. (\APACyear2024), Gristey \BOthers. (\APACyear2020\APACexlab\BCnt1), Villefranque \BOthers. (\APACyear2023)]. While performance can be improved by constructing acceleration grids [Villefranque \BOthers. (\APACyear2019)], or by utilizing the computing capabilities of modern graphics processing units (GPUs) [Veerman \BOthers. (\APACyear2022)], Monte Carlo-based radiative transfer methods remain costly. The search for computationally efficient Monte Carlo ray tracing has been more extensive in the movie and video game industry [<]e.g.,¿[]Chaitanya2017, Dahlberg2019, Huo2021, Isik2021. There, solutions to reduce computational costs of Monte Carlo ray tracing are divided into two main categories: advanced sampling strategies and post-processing reconstruction schemes [Huo \BBA Yoon (\APACyear2021)]. The focus of our study will be on the latter, also known as denoising. Classical denoising methods comprise various filtering techniques [Fan \BOthers. (\APACyear2019)]. Examples include Gaussian, bilateral [Tomasi \BBA Manduchi (\APACyear1998)], non-local means [Buades \BOthers. (\APACyear2005)], total variation [Rudin \BOthers. (\APACyear1992)], and wavelet [Chang \BOthers. (\APACyear2000)] filters. Although such classical denoising methods perform reasonably well and are relatively simple to implement, they come with drawbacks such as edge blurring and loss of high frequency information [Fan \BOthers. (\APACyear2019), Tian \BOthers. (\APACyear2020)]. In recent years, the application of machine learning (ML) algorithms for removing Monte Carlo noise has led to notable advancements [Huo \BBA Yoon (\APACyear2021), Tian \BOthers. (\APACyear2020)]. The principle of ML-based denoising is to use a noisy image and a set of auxiliary variables as inputs for an algorithm which learns to produce an approximation of the converged image. State-of-the-art ML-based denoising methods are typically based on Convolutional Neural Networks (CNNs) [Bako \BOthers. (\APACyear2017), Fan \BOthers. (\APACyear2019), Huo \BBA Yoon (\APACyear2021), Tian \BOthers. (\APACyear2020), Zhang \BOthers. (\APACyear2017)]. In particular denoising autoencoders, advanced variants of CNNs, have proven effective in denoising of Monte Carlo scenes with low sampling budgets and high corresponding noise levels [Chaitanya \BOthers. (\APACyear2017), Kuznetsov \BOthers. (\APACyear2018), Mao \BOthers. (\APACyear2016)]. In this study, we explore the potential of denoising autoencoders for speeding-up surface irradiance computations based on Monte Carlo ray tracing. We investigate the optimal sampling budget of the initial Monte Carlo estimate, which is a trade-off between speed and accuracy. A larger sampling budget provides more information to the denoising algorithm, but increases computational cost. Furthermore, we investigate the added value of providing auxiliary input variables to the autoencoder, such as liquid water paths and solar angles. Consequently, the main question of this paper is: How well can an autoencoder denoise ray-traced surface irradiance? We define the following subquestions to provide a quantitative answer to the main question: • How does the sampling budget affect the trade-off between speed and accuracy? • How do auxiliary inputs contribute to prediction accuracy? Surface solar irradiance is generally considered as the sum of a direct and diffuse component, where the diffuse component is all radiation scattered at least once within the atmosphere. In this study, we choose to denoise direct and diffuse irradiance independently, since these components present distinct noise patterns. Additionally, we focus on the irradiance beneath shallow cumulus clouds, which are common over large parts of the world [Berg \BOthers. (\APACyear2011)], are strongly coupled to the earth’s surface and produce complex spatial irradiance patterns [Burleyson \BOthers. (\APACyear2015), Gristey \BOthers. (\APACyear2020\APACexlab\BCnt2), Z. He \BOthers. (\APACyear2024)]. In section 2, we introduce the methodology regarding generation of training data, network architecture and model training. Section 3 presents the results, discussing the sampling budget trade-off and impact of auxiliary variables. In section 4, we summarize the main conclusions and discuss their implications."
https://arxiv.org/html/2411.07041v1,Stochastic parameterisation: the importance of nonlocality and memory,"Stochastic parameterisations deployed in models of the Earth system frequently invoke locality assumptions such as Markovianity or spatial locality. This work highlights the impact of such assumptions on predictive performance. Both in terms of short-term forecasting and the representation of long-term statistics, we find locality assumptions to be detrimental in idealised experiments. We show, however, that judicious choice of Markovian parameterisation can mitigate errors due to assuming Markovianity. We propose a simple modification to Markovian parameterisations, which yields significant improvements in predictive skill while reducing computational cost. We further note a divergence between parameterisations which perform best in short-term prediction and those which best represent time-invariant statistics, contradicting the popular concept of seamless prediction in Earth system modelling.","In the simulation of physical systems, such as the atmosphere or ocean, it is often impossible to resolve dynamics on the full range of relevant scales at once. Limited by computational cost, numerical models covering planetary scales can offer only a truncated view of such systems. Parameterisations are the attempts made to mitigate the errors introduced by failing to resolve certain dynamics, typically by supplementing the equations solved in our models with additional terms. The classical approach to parameterisation is to introduce terms which diagnose approximately the influence of the unresolved dynamics on the resolved dynamics. Stochastic parameterisation arises when it is inappropriate to assume that a deterministic relationship holds. Instead the probabilistic relationship between the unresolved and resolved dynamics is modelled with stochastic terms. Stochastic parameterisations promise benefits for both ‘weather’ and ‘climate’ modelling, interpreted broadly as dynamical modelling with the aim of capturing finite-time and long-time average behaviour, respectively. In the weather paradigm the primary advantage of stochastic parameterisations is that they allow for ensemble simulation. Ensembles allow to quantify uncertainty in predictions and can also be used to produce improved point-estimates, i.e. the ensemble mean of a model with stochastic parameterisations may provide a better prediction than that of a model with deterministic parameterisations. In the climate paradigm one hopes that the added variability introduced by stochastic parameterisations leads to a better representation of equilibrium statistics. Indeed there is evidence for all of the above. Stochastic parameterisations have seen significant successful use over the past three decades (Berner et al. 2017), both in research (Wilks 2005, Crommelin & Vanden-Eijnden 2008, Kwasniok 2012, 2014, Arnold et al. 2013, Porta Mana & Zanna 2014, Chorin & Lu 2015, Grooms 2016, Guillaumin & Zanna 2021, Levine & Stuart 2022) and in operational weather forecasting (Toth & Kalnay 1997, Buizza et al. 1999, Berner et al. 2009, Palmer et al. 2009). In constructing stochastic parameterisations it is common to make simplifying assumptions about the relationship between resolved and unresolved variables. E.g. that, given the resolved state, the model error is uncorrelated or Markovian in time and/or space, or that it is Gaussian. This article discusses the impact of such assumptions on the performance of stochastic parameterisations. The article is structured as follows. In Section 2 we describe the problem in a general mathematical notation. We also discuss the simplifying assumptions mentioned above in detail. In Section 3 we present idealised numerical experiments with the Lorenz ’63 model (Lorenz 1963), wherein the original system is forced with artificial model error generated with non-Markovian and spatially-correlated stochastic processes. We show that Markovian and spatially-local parameterisations can fail to reproduce the behaviour of the forced system. On the other hand we show that judicious choice of Markovian approximation can alleviate this error. In Section 4 we explore the same issues in the two-scale Lorenz ’96 system (Lorenz 1996), wherein the model error arises from neglecting the small-scale variables. We find that modelling spatial correlation in the model error, given the large-scale variables, is critical. We also propose a method for obtaining improved Markovian parameterisations at no additional cost. In Section 5 we conclude and discuss the implications of these results for weather and climate modelling in particular."
https://arxiv.org/html/2411.06629v2,A dual-pairing summation-by-parts finite difference framework for nonlinear conservation laws,"Robust and stable high order numerical methods for solving partial differential equations are attractive because they are efficient on modern and next generation hardware architectures. However, the design of provably stable numerical methods for nonlinear hyperbolic conservation laws pose a significant challenge. We present the dual-pairing (DP) and upwind summation-by-parts (SBP) finite difference (FD) framework for accurate and robust numerical approximations of nonlinear conservation laws. The framework has an inbuilt ""limiter"" whose goal is to detect and effectively resolve regions where the solution is poorly resolved and/or discontinuities are found. The DP SBP FD operators are a dual-pair of backward and forward FD stencils, which together preserve the SBP property. In addition, the DP SBP FD operators are designed to be upwind, that is they come with some innate dissipation everywhere, as opposed to traditional SBP and collocated discontinuous Galerkin spectral element methods which can only induce dissipation through numerical fluxes acting at element interfaces. We combine the DP SBP operators together with skew-symmetric and upwind flux splitting of nonlinear hyperbolic conservation laws. Our semi-discrete approximation is provably entropy-stable for arbitrary nonlinear hyperbolic conservation laws. The framework is high order accurate, provably entropy-stable, convergent, and avoids several pitfalls of current state-of-the-art high order methods. We give specific examples using the in-viscid Burger’s equation, nonlinear shallow water equations and compressible Euler equations of gas dynamics. Numerical experiments are presented to verify accuracy and demonstrate the robustness of our numerical framework.","Nonlinear hyperbolic conservation laws, defined by systems of nonlinear partial differential equations (PDEs), are widespread in modelling several important physical phenomena and industrial problems such as fluid flow, combustion, black holes, magneto-hydrodynamics, astrophysical and atmospheric fluid dynamics, to name but only a few. Analytical solutions are intractable for most realistic situations. Therefore, accurate and efficient numerical simulations of nonlinear conservation laws are important to academia and industry, and are critical for developing modern technologies, deepening scientific knowledge and paving the way for new inventions and discoveries. The derivation of effective numerical methods for computing approximate solutions of nonlinear conservation laws that are fast, accurate and robust has been a hot research topic beginning from the seminal and early works of von Neumann and Richtmyer [1], Lax and Wendroff [2, 3], Godunov [4], Harten [5], and in recent times has continued to attract substantial interests from the computational and applied mathematics community, see e.g. [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]. Historically, routine numerical approaches to solving nonlinear conservation laws were dominated by low order (first and second order) finite volume and finite difference methods [4, 21, 22, 3] which rely on Godunov-type approximate Riemann solvers. Low order accurate methods can be robust but, because of their inherent excessive numerical dissipation, are not expected to effectively and accurately resolve fine scale and nonlinear features present in the solutions, for example shocks, turbulence, vortices, and highly dispersive wave-dominated phenomena such as gravitational and Rossby wave propagation. High order numerical methods are less dissipative and are known to be efficient and well-suited for wave dominated problems, see e.g., the pioneering work by Kreiss and Oliger [23]. Not all high order methods are acceptable, however, for nonlinear conservation laws. High order accurate numerical simulations of nonlinear conservation laws are challenging as initial attempts often result in crashes due to compounding numerical errors or the presence of undesirable numerical artefacts which can pollute numerical simulations everywhere. Desirable high order accurate methods for nonlinear hyperbolic conservation laws must be robust (provably stable) and preserve several important invariants present in the system. While discrete structure preservation of conservation laws and model symmetries of nonlinear conservation laws can improve accuracy, mitigate against numerical instabilities and enhance the robustness of high order numerical approximations, this also introduces additional theoretical and computational demands, such as proof of nonlinear stability, mimetic reformulations and discretizations. In particular, the proof of nonlinear stability for high order numerical methods for systems of nonlinear conservation laws can be technically challenging. Many previous studies have advocated for numerical methods with provably linear stability, where the assumption of smoothness of solutions is then used to simulate nonlinear problems [24, 25, 26, 27]. However, linearly stable numerical methods are generally not robust for many nonlinear problems and will not yield desirable results in many situations. For instance, turbulence and shocks are highly nonlinear phenomena where linearly stable numerical methods are not sufficient to deliver desirable results. It has been shown and observed in many numerical simulations published in the literature [28, 29, 8, 20], that nonlinear numerical stability is critical to ensuring robustness and accuracy of high order numerical methods for several nonlinear conservation laws. However, before we proceed, first at the continuous level, we must define what it means for a system of nonlinear conservation laws to be nonlinearly stable. We can then mimic the stability properties of the continuous model at the discrete level, leading to a provably nonlinearly stable numerical method. There are two parallel (and sometimes intersecting) definitions of nonlinear stability, namely, energy stability and entropy stability. On the one hand, energy stability, which follows from a linear stability theory, aims to bound/conserve a mathematical energy of the solution, a nonnegative functional of the solution (typically a weighted L_{2}-norm of the solution for first order systems) at a future time by the energy of the initial (and boundary) data [24, 25, 26, 27, 30, 31, 32]. The energy estimate may sometime coincide with the estimate of the physical energy in the system. However, it is also noteworthy that for certain systems (e.g. Einstein’s equations of general relativity) the physical energy is not always nonnegative and may not be useful in deriving nonlinearly stable numerical approximations. On the other hand, an entropy stable numerical method is designed to bound/conserve a mathematical entropy [28, 33, 32, 6, 22, 34, 35, 29, 36, 5], a convex scalar functional that is conserved by the PDE system, for smooth solutions. The notion of entropy stability also has a physical interpretation. For example, a solution of the compressible Euler equations should not only satisfy the nonlinear conservation laws, but also the second law of thermodynamics. A direct implication of this is that the total specific entropy should only increase in a closed system. Consequently, this translates into the solution satisfying an extra partial differential inequality, also known as entropy inequality. A weak solution that satisfies the entropy inequality is considered as the physically correct weak solution [28, 21, 22, 37]. For some systems, such as the Burgers equation and the nonlinear shallow water equations where the energy defines a mathematical entropy, energy stability and entropy stability can be used interchangeably. However, in this paper we will define nonlinear stability in terms of entropy stability. That is, a system of nonlinear conservation laws is entropy stable if there is a mathematical entropy which is conserved by the system for smooth solutions. For an entropy stable system of PDEs, a provably nonlinearly stable numerical method can be derived by replicating the continuous analysis at the discrete level. An essential ingredient for the development of provably stable numerical methods for PDEs is the summation-by-parts (SBP) principle [38, 39, 40, 41, 42, 43], with its mimetic structure, which allows provably stable methods to be constructed at the semi-discrete level, as long as careful numerical treatments of the boundary conditions are provided. At the continuous level, however, the key ingredients that are necessary in proving stability results are integration by parts, and possibly the chain rule and the product rule. So, while SBP numerical derivative operators are designed to mimic integration by parts property at the discrete level, it is challenging for SBP operators to mimic the chain rule or product rule at the discrete level. To be able to replicate the continuous analysis at the discrete level, the system of nonlinear conservation laws at the continuous level must be rewritten into the so-called skew-symmetric or entropy-conserving split-form so that we can negate the use of the chain rule and product rule at the discrete level. For standard systems of nonlinear conservation laws there are several artful ways of doing this, see e.g. [6, 7, 8, 9, 44, 45, 20, 15, 14, 30]. We note, however, for several systems of PDEs finding a mathematical entropy and the skew-symmetric formulation that can be targeted by SBP discretizations can be cumbersome and nontrivial. Once an entropy functional is identified and an entropy conserving or mimetic reformulation of the PDE system is given, then the system can be approximated by SBP operators leading to an entropy conserving numerical method. While, theoretically, this may yield a provably nonlinearly stable numerical method, however, traditional SBP operator for the first derivative based central FD stencils [38, 39, 40, 41] or collocated discontinuous Galerkin spectral element method (DG-SEM) [42, 43, 14] can suffer from spurious unresolved wave-modes, which can destroy the accuracy of numerical solutions and crash the simulation of a nonlinear conservation law. It was recently shown in [29] that the local linear stability property of high order methods is necessary for convergent numerical simulations of nonlinear conservation laws. Sadly, state-of-the-art high order accurate SBP FD and DG-SEM entropy stable schemes for nonlinear conservation in skew-symmetric split form are not locally energy stable, and do not satisfy this necessary criterion. Therefore, a sufficient amount of dissipation must be added, by filtering, limiting or artificial numerical dissipation, to stabilize the simulation and obtain acceptable results. Numerical dissipation helps in many ways, but it can lower numerical accuracy, compromise the conservative properties of the system and introduce interesting and undesirable numerical artefacts. In this study we propose a new high order accurate and provably nonlinearly stable numerical framework for nonlinear conservation laws that significantly minimizes oscillations from shocks and avoids several pitfalls of current state-of-the-art high order methods. We are currently investigating the local energy-stability properties of the proposed DP SBP framework and comparing with other high-order schemes. The results of these will be reported in our forthcoming paper. The dual-pairing (DP) SBP framework [46, 47, 48] was recently introduced to improve the accuracy of FD methods for wave problems and nonlinear conservation laws, in complex geometries. The DP-SBP operators are a pair of high order (backward and forward) difference stencils which together preserve the SBP principle. The DP-SBP operators also have additional degrees of freedom which can be tuned to diminish numerical dispersion errors, yielding the so-called dispersion-relation-preserving (DRP) DP-SBP FD operators [48]. However, the DP-SBP operators are asymmetric and dissipative, can potentially destroy symmetries that exist in the continuum problem. To obtain acceptable results, the DP-SBP operators must be combined in a careful manner so that we can prove numerical stability and preserve model symmetries. Recent applications of DP-SBP operators to nonlinear conservation laws [49, 50] combines the operators with classical finite volume flux splitting [21, 4, 22] so that the upwind features of DP-SBP operators induces some dissipation. The resulting numerical methods [49, 50] are provably linearly stable. As we have recounted linear stability is not sufficient to simulate highly nonlinear phenomena such as turbulence. This is corroborated by the numerical experiments of turbulent flows performed later in this paper and in [49] for linearly stable methods using the DP-SBP operators, where the simulations of the classical Kelvin-Helmholtz instability crashed when the flow became turbulent. In this paper we present the DP upwind SBP FD framework for efficient, accurate and robust numerical approximations of nonlinear conservation laws. The DP SBP FD operators are designed to be upwind, that is they come with some built-in dissipation everywhere, as opposed to DG-SEM which can only induce dissipation through numerical fluxes acting at element interfaces. We combine the DP SBP operators together with skew-symmetric and upwind flux splitting of nonlinear hyperbolic conservation laws. The semi-discrete approximation is conservative and provably entropy-stable for arbitrary nonlinear hyperbolic conservation laws. That is, for smooth solutions it conserves entropy and dissipates entropy when solutions are non-smooth. We give specific examples using the in-viscid Burger’s equation, nonlinear shallow water equations and compressible Euler equations of gas dynamics. We focus primarily on initial value problems (IVP) driven by initial conditions or internal forcing with periodic boundary conditions. Extensive and detailed numerical experiments are presented verifying accuracy, stability and the conservative properties of the method. Long time numerical simulations of merging vortices, barotropic shear instability and the Kelvin-Helmholtz instability, with fully developed turbulence, demonstrate the robustness and accuracy of the framework when compared with the state-of-the-art methods. The rest of the paper is organised as follows. In Section 2 we perform the continuous analysis, give a quick review of skew-symmetric and entropy conserving flux splitting of nonlinear conservation laws. Next, in section 3, we introduce the SBP FD framework for first derivative d/dx and detail the necessary assumptions that define the traditional SBP framework [38, 39] and the DP upwind SBP framework [46, 47, 48]. In section 4 we derive the semi-discrete approximations for an arbitrary conservation and prove the conservative and stability properties of the numerical framework. In section 5 we present detailed numerical experiments verifying accuracy, discrete conservative and stability properties, and the robustness of the proposed method. Section 6 summarises the results of the study and speculates on the direction of future research."
https://arxiv.org/html/2411.06155v1,HiHa: Introducing Hierarchical Harmonic Decomposition to Implicit Neural Compression for Atmospheric Data,"The rapid development of large climate models has created the requirement of storing and transferring massive atmospheric data worldwide. Therefore, data compression is essential for meteorological research, but an efficient compression scheme capable of keeping high accuracy with high compressibility is still lacking. As an emerging technique, Implicit Neural Representation (INR) has recently acquired impressive momentum and demonstrates high promise for compressing diverse natural data. However, the INR-based compression encounters a bottleneck due to the sophisticated spatio-temporal properties and variability. To address this issue, we propose Hierarchical Harmonic decomposition implicit neural compression (HiHa) for atmospheric data. HiHa firstly segments the data into multi-frequency signals through decomposition of multiple complex harmonic, and then tackles each harmonic respectively with a frequency-based hierarchical compression module consisting of sparse storage, multi-scale INR and iterative decomposition sub-modules. We additionally design a temporal residual compression module to accelerate compression by utilizing temporal continuity. Experiments depict that HiHa outperforms both mainstream compressors and other INR-based methods in both compression fidelity and capabilities, and also demonstrate that using compressed data in existing data-driven models can achieve the same accuracy as raw data. The source code can be found at https://anonymous.4open.science/r/HiHa-EB13/.","Meteorology, a significant natural science, necessitates the analysis and integration of multiple atmospheric variables over hundreds of years (Bi et al. 2023). Produced by meteorological organizations (e.g. ECMWF, CMA, JMA, etc.), petabytes of atmospheric data requires to be shared with researchers globally per day. Therefore, the rapid development of meteorological research has spawned a huge demand for atmospheric data storage and transmission, while bringing about substantial costs citeprasp2024weatherbench. This presents an enduring challenge, bringing huge stress to data storage and transmission (Han, Guo et al. 2024). To alleviate the issue, many compression methods have been developed, such as BUFR, GRIB1, GRIB2 (Murrieta-Mendoza 2015), NetCDF (Rew Russ 1990), etc. These lossless compressions are able to provide up to 4\times compression ratio, but still has >100 TB for 40 years’ hourly data of 5 variables (basic data for training Pangu (Bi et al. 2023)), which is insufficient in addressing the performance bottleneck of atmospheric data transmission (Han, Guo et al. 2024). Figure 1: Harmonic decomposition of atmospheric data (an example of humidity in 500hpa). Implicit Neural Representation (INR), using periodic activation function (Sitzmann et al. 2020), has brought innovation to data representation, and possesses the ability to fit models accurately and rapidly in a concise manner (Mildenhall, Srinivasan et al. 2020). The INR-based compression technique, Implicit Neural Compression (INC), is gaining success on natural data compression ratio and enhanced spatial continuity, such as SCI (Yang, Xiao et al. 2023), MINER (Saragadam et al. ) and Gaussian Splatting (Zhang, Ge et al. 2024), etc. The inherent continuity in INR is highly compatible with the spectral characteristics of dynamic equations of atmospheric motion (Sitzmann et al. 2020). In addition, given sufficient parameters, the universal approximation theorem of neural networks promises high fidelity representation (Gallant 1988). The above perspectives illustrate the potential of INR to generate an efficient and compact representation of atmospheric data. Despite the high efficiency on other natural data (Yang 2023; Kim, Bauer et al. 2024), the INC methods often encounter over-smoothness and low efficiency on dealing with atmospheric data. As shown in Fig. 1, the sophisticated spatio-temporal properties and variability caused by superposition of multiple complex harmonics leads to a high requirement of the fitting ability of neural networks(Sasamori 1972), and thereby causing the issue. The issue can be further listed into the following crucial insights: • Atmospheric data contain various harmonics and the fitting rules are sophisticated. The description of such processes within a confined parameter space poses significant challenges (Shin, Kim et al. 2024); • Spatial singularities will affect the searching process of the best parametric space as noisy bias, increasing the difficulty of the INR convergence (Martel, Lindell et al. 2021); • Due to the spherical topology of earth, employing Cartesian basis vector would result in the geographical information loss (Rußwurm, Klemmer et al. 2023); • The existing methods disregard the temporal continuity in atmospheric data, causing inefficiency in temporal data compression(Yang 2023); Inspired by the above insights, we made statistical analysis of different harmonic components and propose Hierarchical Harmonic decomposition implicit neural compression for atmospheric data (HiHa). We explore the characteristics and spatial distribution of multiple harmonics in atmospheric data, and decompose the harmonic components into low, mid and high-frequency based on the harmonic frequency. Leveraging spherical coordinates basis, we conduct a frequency-based hierarchical compression strategy including a multi-scale INR module, an iterative decomposition module and a sparse storage module on harmonics of low, medium and high frequency, hierarchically. Moreover, we institute temporal residual compression module and utilizes a temporal multiscale Laplacian pyramid architecture for accelerating compression of atmospheric data over successive periods. Comprehensive experimental results demonstrates that HiHa achieves impressive compression accuracy and efficiency compared with other baselines. Specifically, at an atmospheric acceptable accuracy, HiHa achieves a compression ratio of over 200\times within 43 seconds in error of 1e-3, surpassing other INC baselines in terms of both speed and accuracy. Moreover, HiHa can achieve error of 1e-5 compared to other methods, surpassing their limitations, with a compression ratio of up to 27\times within minutes. Overall, the contribution of this paper are summarized in the following three aspects: • The spatial characteristics are explored and analyzed in mathematical analysis, and theoretically deduce the separation criterion for harmonic; • A frequency-based hierarchical compression strategy is proposed, incorporating hierarchical harmonic decomposition and utilizing spherical coordinates and atmospheric pressure-level as the fundamental vectors; • Temporal residual module is established which contains a multiscale Laplacian Pyramid architecture and a retraining process to accelerate compression over successive periods; • The results of comprehensive experiments demonstrate that HiHa can achieve better compression accuracy while incurring minimal compression overhead compared with other methods."
