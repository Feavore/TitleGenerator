URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10205v1,"Satellite monitoring of long period ocean-induced
magnetic field variations","Satellite magnetic field observations have the potential to provide valuable information on dynamics, heat content and salinity throughout the ocean. Here we present the expected spatio-temporal characteristics of the ocean-induced magnetic field at satellite altitude on periods of months to decades. We compare these to the characteristics of other sources of Earth’s magnetic field, and discuss whether it is feasible for the ocean-induced magnetic field to be retrieved and routinely monitored from space.We focus on large length scales (spherical harmonic degrees up to 30) and periods from one month up to five years. To characterize the expected ocean signal we make use of advanced numerical simulations taking high resolution oceanographic inputs and solve the magnetic induction equation in 3D including galvanic coupling and self induction effects. We find the time-varying ocean-induced signal dominates over the primary source of the internal field, the core dynamo, at high spherical harmonic degree with the cross-over taking place at degree 15 to 20 depending on the considered period. The ionospheric and magnetospheric fields (including their Earth induced counterparts) have most power on periods shorter than one month and are expected to be mostly zonal in magnetic coordinates at satellite altitude.Based on these findings we discuss future prospects for isolating and monitoring long period ocean induced magnetic field variations using data collected by present and upcoming magnetic survey satellites.","Satellite observations are essential to our modern view of the ocean. They provide global information on circulation patterns [1, 2], temperature [3], salinity [4], and biochemistry such as chlorophyll [5] on horizontal scales ranging from 10 to 10,000 km and spanning timescales of weeks to decades, enabling variability and trends in ocean properties to be monitored. These observations are nevertheless heavily focused on the ocean surface; a full understanding of the ocean and its changes also requires information on processes happening at depth. Float programs, in particular ARGO, have in recent years provided much valuable information in this regard [6]. Our knowledge of ocean flows, temperature and salinity as a function of depth nonetheless remains incomplete, especially at depths below 2000 m. Satellite measurements of gravity and magnetic fields involve an intrinsic integration of ocean properties over depth, so they provide complementary observational constraints on the ocean as a whole. For example, gravity measurements provide constraints on total ocean mass variations and ocean bottom pressure variations which depend on gravity via density integrated over a vertical water column [7]. The GRACE mission has already provided extremely valuable information in this regard, and the upcoming MAGIC mission aims to push such monitoring to higher spatial and temporal resolution [8]. Our focus here is instead on satellite measurements of the magnetic field and the information they can provide on integrated ocean properties on large length scales (of order 1000 km and larger) and long (monthly to decadal) time scales. Magnetic fields are generated by motional induction in the ocean, when electrically-conducting water moves across magnetic field lines originating in Earth’s core [9, 10, 11]. We refer here to such fields as Ocean-Induced Magnetic Fields (OIMF), though it is important to note that they also depend on the electrical conductivity of the solid Earth where the induced currents close. Two classes of oceanic motions contribute to the OIMF. First, and so far best studied, are the magnetic signals due to ocean motions driven by periodic gravitational forcing of the luni-solar tides. Such tidal flows are predominantly two-dimensional, and they give rise to pronounced magnetic signals at the discrete frequencies of the individual tidal constituents [12, 13, 14], although non-linear compound tides also exist [15]. The second class, which is our focus here, involves magnetic signals resulting from the general circulation of the ocean and its dynamics. The latter motions are driven both by winds and buoyancy (also known as thermohaline) effects including surface temperature variations and freshwater release, with timescales ranging from days up to centuries[16, 17, 18]. These general circulation motions of the ocean are again largely horizontal, although the small vertical transport plays a crucial consequence role in the dynamics of the thermohaline part of the circulation. The kinetic energy of near-surface motions is dominated by mesoscale eddies (on length scales of hundreds of kms and shorter, and with typical timescales of days to months) [19], however, it is larger (basin-scale) gyres and overturning circulations that dominate thermohaline transport processes over longer timescales. Examples of large-scale ocean currents systems relevant here include the Antarctic Circumpolar Current (ACC), the Atlantic Meridional Overturning Circulation (AMOC) including the Gulf stream, the Indian Ocean Circulation with its Monsoon dynamics, the Pacific Equatorial Currents that are related to the El Niño-Southern Oscillation climate pattern, and the North and South Pacific Subpolar Currents including the Kuroshio Current [20, 16] . The evolution of these large-scale current systems and their depth variations are of great interest for understanding Earth’s climate system [17]. The OIMF is expected to carry volume-integrated information on such processes, if it can be reliably extracted. In addition to being dependent on the amplitude and geometry of flows in the ocean, the OIMF also depends on the electrical conductivity of sea water, which in turn depends on temperature and salinity [21, 22]. The ocean’s electrical conductivity thus varies with geographic location, depth and time, including seasonal variations and long-term trends. Variations in the large-scale OIMF will therefore also be produced by large-scale changes in ocean temperature (i.e. heat content) and salinity. Over the past decade there has been significant progress in understanding and simulating the OIMF. There is now consensus regarding the basic features and expected amplitude of the time-averaged ocean-induced magnetic field based on advanced simulations that make use of oceanographic data [23]. Simulations suggest the OIMF originates primarily from the upper 2000m of the ocean where motions are larger and the temperature (hence electrical conductivity) is higher [24]. At Earth’s surface and satellite amplitude it is dominated by the signature of the Antarctic Circumpolar current [25, 26, 27, 28]. There is however less consensus regarding the magnetic signal of ocean variability on timescales of months to decades; this depends on the input oceanographic information. For such simulations to be accurate it is important to treat ocean motions, temperature and salinity (as well as the derived electrical conductivity) in a self-consistent manner. This requires input of realistic 4D (3D plus temporal) ocean properties while acknowledging that even the most up-to-date oceanographic information is incomplete. Systematic numerical experiments are required to address such uncertainties; here we present only an illustrative example of the OIMF from one such simulation, a more extensive set of simulations is underway and will be reported in a future study. Turning to observations of the OIMF, ocean tidal signals can now be routinely extracted from satellite magnetic measurements. The largest and easiest to extract is the M_{2} tide [12, 29, 30] which is now readily available as an L2 Data Product (SW\_MTI\_SHA\_2C) from ESA’s Swarm satellite mission. The magnetic signals of the N_{2} and O_{1} tides have also been extracted [31, 13] and most recently the weaker Q_{1} tide (amplitude at satellite altitude of order 0.1 nT) has been convincingly isolated [14]. In the most recent study by Grayver et al. [14] spherical harmonic models of the internal magnetic field with a specific period are fit to observations from the CHAMP and Swarm satellites, using only night side data and after correcting for core and magnetospheric fields. The observed OIMF signals agree well with predictions from numerical simulations taking the rather well known tidal flows as input. These observations of the OIMF tidal signals are important demonstrations that the OIMF can be reliably retrieved from satellite magnetic measurements, despite the small signal amplitude, given appropriate processing and modelling of the signal. Trends in tidal properties, which may reflect trends in temperature, salinity or changes in underlying ocean circulation patterns, are of great interest; this is the next target for studies of the tidal OIMF. Turning to the more general OIMF signal from the ocean general circulation, as far as we are aware there has only been one published claim to have detected this, based on observations from the CHAMP satellite [32]. This result has however not yet been independently reproduced by other workers. Recently Hornschild et al. [33] sought to make progress by assuming the OIMF signal from a simulation to be correct, except for a spatially-varying rescaling factor, which they estimated. There is clearly still much to do regarding observational studies of the general circulation driven OIMF. The aim of this article is to document the space-time characteristics expected for the general circulation component of the OIMF, and to compare these to other (non-oceanic) magnetic field signals measured by satellites. Based on these comparisons the feasibility of extracting the OIMF will be assessed, and ways to make progress in this direction proposed. In section 2 we review the physics of the OIMF, describe how it can be modelled and present its space-time characteristics. In section 3 we review the present status of satellite geomagnetism, give an overview of sources of Earth’s magnetic field and describe the space-time characteristics of fields originating in the core, crust, ionosphere and magnetosphere. In section 4 we discuss strategies for extracting the OIMF before finishing with discussion and conclusions in section 5."
https://arxiv.org/html/2411.09794v1,"Forces and grain-grain contacts in bidisperse beds sheared by viscous fluids
This article appeared in Phys. Fluids 36, 113342 (2024) and may be found at https://doi.org/10.1063/5.0238582.","In a recent paper (Gonzalez et al., 2023), we investigated the motion of grains within a granular bed sheared by a viscous fluid, and showed how segregation and hardening occur in the fluid- (bedload) and solid-like (creep) regions. In this paper, we inquire further into the mechanisms leading to grain segregation in a bidisperse bed, and how the forces are distributed. For that, we carried out numerical simulations at the grain scale by using CFD-DEM (computational fluid dynamics-discrete element method), with which we were able to track the positions, velocities, forces, and solid contacts underwent by each grain. We show that during the upward motion of large grains the direct action of fluid forces is significant in the middle and upper parts of the bedload layer, while only contact forces are significant in the creep layer and lower part of the bedload layer. We also show that in all cases the particles experience a moment about a -45∘ contact point (with respect to the horizontal plane) when migrating upward, whether entrained by other contacts or directly by the fluid. In addition, we show the variations in the average solid-solid contacts, and how forces caused either by solid-solid contacts or directly by the fluid are distributed within the bed. Our results provide the relationship between force propagation and reorganization of grains in sheared beds, explaining mechanisms found, for example, in river beds and landslides.","I INTRODUCTION When a granular bed is sheared by a liquid flowing under moderate shear stresses (shear forces comparable to the grains’ weight), grains are transported as bedload: a moving layer in which grains are entrained by rolling and sliding over each other [1, 2]. The bedload layer moves over a creep layer, where small and slow rearrangements of grains deform this part of the bed [3, 4]. Because the creep layer is deformed over long times, while the bedload layer presents a durable and high rate of deformation, they are described as solid- and fluid-like layers, respectively [3, 4]. Bedload in liquids is known to take place after a given threshold of the shear stress is surpassed, since below that threshold the fluid drag is not strong enough for causing grains to roll over one another (promoting motion). For conditions below that threshold, the bed has been described, until recently, as static. In addition, because shear stresses of the liquid decrease to the threshold value within the bedload layer (evaluating the shear stress profile from top to bottom), the layer beneath bedload has also been described as static [2, 5]. However, Houssais et al. [3] and Allen and Kudrolli [6] showed that that portion of the bed creeps, and that this creeping layer can exist even when shear stresses are below the threshold for bedload, i.e., without relevant grain transport (there are only rearrangements of grains). Investigating further the motion of sheared beds, Houssais et al. [3] showed that there is a continuous transition between bedload and creep, and that the transition occurs at a height characterized by the ratio between the microscopic (related to the rearrangements of grains) and macroscopic (related to the macroscopic rate of deformation) timescales. In the case of viscous liquids this ratio corresponds to the viscous number [7] I_{v}, and Houssais et al. [3] found that transition occurs at the height for which I_{v} = 10-7. The threshold for bedload is not constant, but varies over time [8]. Under constant flow velocities, the threshold stress increases with time, implying the decrease in grains’ mobility. Moreover, in cases of small applied stresses it can stop completely bedload from a certain time on. The decrease in granular mobility over time is known as bed hardening, and it is caused by (i) the reorganization of grains within the bed (happening in both mono and polydisperse beds); and, (ii) the armoring due to particle segregation in polydisperse beds, i.e., an increase in the concentration of larger particles on the bed surface, sheltering from the fluid flow the smaller (and more mobile) particles. For the reorganization of grains in a monodisperse bed, we can identify isotropic and anisotropic contributions. The isotropic contribution is that caused by purely geometric rearrangements of grains, in which grains percolate within the bed toward vacancies, leading, thus, to an increase in bed compactness. This idea has been proposed, for example, by Charru et al. [8] and Masteller and Finnegan [9], who carried out flume experiments in which they measured the decay in the mobility of a bedload layer. This decay was also measured in the field by Masteller et al. [10], who identified hardening in the bed of the Erlenbach river by analyzing a 19-year-series dataset of fluid stress and sediment transport. However, the explanation of bed hardening caused only by percolation does not suffice: there is also an anisotropic contribution due to the organization of grains in preferential directions, forming chains that percolate forces by solid-solid contacts. The formation of those force chains, and transmission of forces through a contact force network, was initially studied in the context of static or quasi-static granular matter [11, 12, 13]. In particular, Cates et al. [11] showed that fragile states where force chains are aligned in preferential directions can appear, so that a granular matter in those states jams and supports loading in such directions, but not in others, while Bi et al. [13] showed that granular matter is prone to fragile states and shear jamming (when external shear stresses are applied) at particle fractions lower than those necessary for jamming by an isotropic compression. Later, Cúñez et al. [14] investigated experimentally the time evolution of the structure of a granular bed sheared by a viscous fluid. In their experiments, the bed was submitted to different shear cycles, with and without variations in direction, in order to inquire into the formation and breakage of contact chains. They showed that, when sheared by a viscous flow in a constant direction during a certain period, an anisotropic structure appears, but this structure is broken by reversing the flow direction, while the bed compaction, which corresponds to the isotropic hardening, is not affected. This implies a decay in sediment transport if the fluid flow is sustained in one direction, and the recovery of a portion, corresponding to the anisotropic structure, if the flow is reversed. The portion due to compaction is only recovered if the shear is strong enough for dilating the bed. Concerning polydisperse beds, the migration of larger particles toward the bed surface leads to size armoring: smaller particles become sheltered from regions where the flow velocities are higher [15]. For this reason, some of previous studies investigated grain segregation in bidisperse bedload and debris flow [16, 17, 18, 19, 20, 4]. In particular, Rousseau et al. [18] investigated the upward motion of a single particle inserted in a bedload layer consisting of smaller grains, and showed that its upward motion has two distinct phases: an intermittent and slow motion from deeper regions within the bedload, followed by a fast motion to the bed surface (later, Gonzalez et al. [4] conjectured that the first phase corresponds to the limit between the creep and bedload layers). Ferdowsi et al. [20] investigated segregation and hardening in a bidisperse bed sheared by a steady viscous flow by carrying out experiments in which the ratio between the solid and fluid densities S was 1.13 (close to unity), while in nature S \approx 2.65 (water and sand). In addition, they performed numerical simulations for the grains only (without fluid), in which the particles were sheared by an imposed boundary condition. They found that in deeper regions creep leads to a slow segregation in a diffusion-like manner, while closer to the surface bedload leads to a fast, shear-dependent segregation, in agreement with the results of Rousseau et al. [18]. Both Zhou et al. [16] and Cui et al. [17] investigated the effect of an interstitial fluid in particle segregation, the former for debris flows and the latter for shear flows, and found that segregation decreases with increasing the fluid viscosity. They proposed that the fluid diminishes particle-particle contacts and dampens particle fluctuations (an effect that increases with viscosity), weakening, thus, particle segregation. In a recent paper [4], we investigated experimentally bed hardening and particle segregation in a bidisperse bed sheared by a viscous liquid, for the case S = 2.7 (close to values found in nature). Each experimental run was carried out for 140 hours and we used refractive index matching, allowing measurements of both bedload and creep in a plane far from the channel walls (a plane inside the bed). Among other findings, we showed that there exist diffusive, advective and constrained regions, that segregation occurs within the bedload layer and in the 5% topmost region of the creep layer (at least for the total duration of our experiments), that most of segregation occurs during the very first stages of the flow, that bed hardening becomes stronger while bedload and creep weaken along time, and we proposed characteristic times for segregation and hardening. In our experiments, the creep-bedload transition occurred at I_{v} \approx 2 \times 10-8, instead of 10-7. For the moment, to the best of our knowledge, there is not a real understanding of the forces driving the larger particles upward. For instance, mechanisms of kinetic sieving, for which an interstitial fluid would hinder segregation, have been proposed, but no measurements of the solid-solid contact forces and of the fluid forces acting on each grain have been reported. Measurements of the forces caused by the fluid would help understanding its real role (of promoting or hindering segregation), while those of contact forces would explain if there is kinetic sieving or other mechanism pushing large grains upward. In addition, the knowledge of the magnitude and direction of forces would show if vertical forces act directly on the segregating particle, or if horizontal components make then roll over specific contact points (moving, thus, upward). The previous studies have advanced our knowledge on segregation and hardening, but important questions on the mechanics of hardening and segregation remain, however, to be answered. For example, what are the magnitudes of forces acting on grains being segregated? What are the relative contributions of forces due directly to the fluid and to solid-solid contacts? How are solid-solid contacts distributed around segregating particles, and how they evolve as those particles move upward? And how the network of contact forces varies along time? In this paper, we aim to answer these questions by carrying out numerical simulations at the grain scale of a bidisperse bed sheared by a viscous fluid. We made use of CFD-DEM (computational fluid dynamics-discrete element method), and, because we computed the motion of each grain at all times, we tracked the positions, velocities, forces, and solid contacts underwent by each one of them. For the larger grains moving upward, we show that the direct action of fluid forces is significant in the middle and upper parts of the bedload layer, while only contact forces are significant in the creep layer and lower part of the bedload layer. We also show that those grains experience a moment about a -45∘ contact point (with respect to the horizontal plane) whether entrained by other contacts or directly by the fluid. In addition, we present the variations in the average solid-solid contacts, and how forces caused either directly by the fluid or solid-solid contacts are distributed within the bed. In the following, Sec. II presents the fundamental equations and numerical setup, Sec. III shows the results and discussion, and Sec. IV concludes the paper."
https://arxiv.org/html/2411.08903v1,Turkey’s Earthquakes: Damage Prediction and Feature Significance Using A Multivariate Analysis,"Accurate damage prediction is crucial for disaster preparedness and response strategies, particularly given the frequent earthquakes in Turkey. Utilizing datasets on earthquake data, infrastructural quality metrics, and contemporary socioeconomic factors, we tested various machine-learning architectures to forecast death tolls and fatalities per affected population. Our findings indicate that the Random Forest model provides the most reliable predictions. The model highlights earthquake magnitude and building stability as the primary determinants of damage. This research contributes to the reduction of fatalities in future seismic events in Turkey.","Earthquakes serve as one of the most catastrophic natural disasters, claiming over 61,000 lives in 2023 alone. [1, 2]Turkey, in particular, is struck by thousands of earthquakes annually because of the multiple major seismic fault lines running through its geography. Most notably, the recent 2023 Turkey–Syria earthquakes, of magnitudes 7.8 and 7.7, killed a confirmed 53,537 people.[3] Recent predictive models, such as the RECAST model, excel at forecasting future earthquakes, along with information such as power level, location, and time of formation. [4] However, in order to better prepare earthquake mitigation strategies, it is necessary to predict the damage outcome and feature significance of these earthquakes. Our study bridges the gap in understanding earthquake damage by predicting its severity based on major influencing factors such as building stability, earthquake depth, and population density per province. We also seek to determine the significance of these factors in contributing to the overall destruction caused by earthquakes. The primary objective of this research is to formulate a model that accurately predicts the death toll and fatalities per affected population in Turkish earthquakes by testing various machine learning architectures and using the most accurate one to assess the significance of each factor, thereby enhancing earthquake preparedness and resistance (see results)."
https://arxiv.org/html/2411.08925v1,Retrieval of sun-induced plant fluorescence in the O-A absorption band from DESIS imagery,"We provide the first method allowing to retrieve spaceborne SIF maps at 30 m ground resolution with a strong correlation (r^{2}=0.6) to high-quality airborne estimates of sun-induced fluorescence (SIF). SIF estimates can provide explanatory information for many tasks related to agricultural management and physiological studies. While SIF products from airborne platforms are accurate and spatially well resolved, the data acquisition of such products remains science-oriented and limited to temporally constrained campaigns. Spaceborne SIF products on the other hand are available globally with often sufficient revisit times. However, the spatial resolution of spaceborne SIF products is too small for agricultural applications. In view of ESA’s upcoming FLEX mission we develop a method for SIF retrieval in the O2-A band of hyperspectral DESIS imagery to provide first insights for spaceborne SIF retrieval at high spatial resolution. To this end, we train a simulation-based self-supervised network with a novel perturbation based regularizer and test performance improvements under additional supervised regularization of atmospheric variable prediction. In a validation study with corresponding HyPlant derived SIF estimates at 740 nm we find that our model reaches a mean absolute difference of 0.78\,\,\mathrm{mW\,nm^{-1}\,sr^{-1}\,m^{-2}}.","The potential of sun-induced flurorescence (SIF) for agricultural management and phenotyping tasks was recognized early in the development of retrieval algorithms [41]. Since SIF is fuelled by a residual energy flux of photosynthetically active radiation (PAR) that is not consumed by processes related to the plant’s photochemistry and thermal energy dissipation it provides a causal link between radiance measurements and the photosynthetic status of plants [42, 50, 61, 62]. Various studies have utilized this relationship as the theoretical basis for stress detection and monitoring [1, 12, 49, 14, 68], the estimation of photosynthetic activity and gross primary productivity [10, 59, 58, 69], crop monitoring and yield predictions [25, 56, 37, 48] and disease monitoring [8, 51] from SIF estimates derived from remote sensing data at various spatial scales. Quantitative estimates of SIF allow for more sensitive and causally founded physiological assessments compared to purely reflectance based indices commonly used for such tasks. Different studies have shown the increased explanatory power of SIF estimates measured at canopy level in a range of tasks [12, 45, 65, 39]. SIF retrieval methods for a variety of sensors have been developed as the number of airborne and spaceborne sensors with sufficient spectral resolution has increased [43]. However, no spaceborne sensor designed specifically for fluorescence retrieval has yet been operationalized. ESA’s Earth Explorer Mission FLEX [16], planned to be launched in 2025, will be the first such instrument. Spaceborne SIF estimates to this day are derived from data acquired by satellite missions for atmospheric characterization (e.g., GOSAT [34], GOME [33, 27], SCIAMACHY [35], OCO-2/3 [57, 17], TROPOMI [26, 28], TanSAT [67]) as their spectral resolution (SR) and signal-to-noise ratio (SNR) allow for SIF retrieval from Fraunhofer lines [23, 24, 16]. However, the spatial resolution of these atmospheric sensors (> 4 km2) is insufficient for most agricultural applications. FLEX, on the other hand, will provide radiance data with a pixel size of 300 m which still imposes severe limits on its usability for a wide range of applications in heterogeneous agricultural landscapes. As an exploratory step towards spaceborne SIF retrieval at high spatial resolution, we therefore propose a deep learning architecture and a loss formulation for the first SIF retrieval from hyperspectral imagery of the DLR Earth Sensing Imaging Spectrometer (DESIS). SIF retrieval from DESIS imagery has the benefit of providing spaceborne SIF products at an unprecedented spatial resolution of 30 m which principally allows for the targeted acquisition of auxiliary validation data at high spatial resolution for the upcoming FLEX mission. However, the SR and SNR of DESIS are insufficient for consistent SIF retrieval with current traditional retrieval methods leveraging data in the O2-A absorption band [22, 13, 40] where the fluorescence signal contribution to the at-sensor signal has a local maximum. Airborne SIF retrieval with similar methods applied to radiance data at lower SR has however been shown to yield consistent relative SIF estimates [3]. As a solution, we extend the simulation-based self-supervised deep learning approach of [5, 7], called Spectral Fitting Method Neural Network (SFMNN), originally developed with airborne hyperspectral imagery. As in other self-supervised simulation-based learning schemes, this approach leverages the implicit constraints of a differentiable simulator of the physical image generation in the loss [30, 32] and primarily does not rely on labels for training. Further regularization terms that enforce physical and physiological domain constraints allow this encoder-decoder architecture to decompose and reconstruct hyperspectral data in the spectral range around the O2-A absorption band. In this contribution we introduce regularization terms in the SFMNN framework allowing consistent SIF retrieval in DESIS imagery despite its lower SNR and SR. Firstly, we propose a perturbation based augmentation scheme to promote the decorrelation of the predicted SIF from other confounding variables affecting the at-sensor signal. Secondly, we show that including ancillary atmospheric data from DESIS L2A products by means of a secondary supervised downstream learning task improves the performance of our model."
https://arxiv.org/html/2411.08732v1,Quantitative imaging of the fresh/saltwater interface with airborne electromagnetics: examining different sources of uncertainty,"Knowing the distribution between fresh and saline groundwater is imperative for sustainable and integrated management of water resources in coastal areas. The airborne electromagnetic (AEM) method is increasingly used for hydrogeological mapping over large areas via bulk electrical resistivity. However, accurately and reliably mapping the fresh/saltwater interface (FSI) requires accurate knowledge about the transition zone. The objective is to quantify the uncertainty in using AEM data to inform on the depth of the FSI. The study mimics a dual-moment time-domain SkyTEM sounding recorded in the Belgian coastal plain based on borehole data. It quantifies uncertainty using a differential evolution adaptive Metropolis algorithm to sample the posterior distribution. The results indicate the importance of reliable altitude, pitch and roll logging. Gathering prior knowledge about the transition zone, for example, through borehole logs, significantly improves the estimation of the FSI. The Resolve frequency-domain system, especially in context with very shallow to shallow FSIs, is more suitable for salinity mapping than the time-domain SkyTEM used in the field survey. The depth of the FSI may be defined via various threshold values. The uncertainty of three different thresholds is studied. The FSI based on the middle of the transition zone is the most reliable, while the FSI based on the 1500 mg/L total dissolved solids threshold is the least robust.","Planning for water security in the context of global environmental change is one of the most pressing challenges worldwide, as articulated by the 6th Sustainable Development Goal: Clean water and sanitation (UNESCO,, 2019; Taylor et al.,, 2013; Elshall et al.,, 2020). Knowing the distribution between fresh and saline groundwater is imperative for sustainable and integrated management of water resources in coastal areas. The Belgian coastal plain is quite a complex distribution of fresh and saltwater. The distribution is a result of the geologic evolution after the last ice age (salinization) and recent human intervention by land reclamation and impoldering (see, e.g. Vandenbohede and Lebbe, (2012); Vandenbohede, (2014)). The latter caused freshwater lenses on top of ‘fossil’ saltwater. Precipitation deficits due to the drier summers or overexploitation of these freshwater lenses can (negatively) affect the saltwater interface. Moreover, rising sea levels will put pressure on the freshwater lenses in the dunes, which could increase saltwater intrusion and the polders’ saline seepage pressure. This effect could negatively affect the future underground bearing of precipitation surpluses (Oude Essink et al.,, 2010).Therefore, Flanders Environmental Agency (VMM) wanted to update an older freshwater/saltwater interface map (De Breuck et al.,, 1989) to prepare for future climate adaptation. From 2017 to 2019, a SkyTEM airborne electromagnetic survey covering the coastal area was obtained, which was commissioned by VMM. The survey recorded over 67,500 soundings along 2412 km with a 250 m line spacing (Delsman et al.,, 2019). Airborne Electromagnetic (AEM) mapping, in general, is an increasingly used method to image near-surface geological features over large areas via bulk electrical resistivity. Common applications are, for example, in mineral exploration (Macnae and Milkereit,, 2007), contamination (Pfaffhuber et al.,, 2017), hydrogeological mapping (Mikucki et al.,, 2015), and saltwater intrusion (Pedersen et al.,, 2017; Siemon et al.,, 2019; Gottschalk et al.,, 2020; Deleersnyder et al.,, 2023). While for some applications, anomalies in the data be directly interpreted, other applications require geophysical inversion which seeks inversion models matching the observed data. For some saltwater imaging applications, when one is only interested in the qualitative distribution of the fresh and saltwater lenses, standard inversion approaches may be sufficient (Viezzoli et al.,, 2009), but mapping the fresh/saltwater interface (FSI) requires an accurate knowledge about the transition from fresh to salt. Otherwise, one may find a deviation of a few meters. However, King, (2022) and Deleersnyder, (2023) demonstrate that accurately imaging the electrical conductivity right on top of a saltwater lens may yield highly uncertain results, impeding such quantitative mapping. This uncertainty is related to the non-unicity of the inverse problem. Multiple inversion models fit the data equally well. Standard interpretation approaches rely on deterministic inversion, in which the goal is to locate one optimal model that can be obtained using some form of smoothness or sharpness constraints implied through several available regularization techniques (Constable et al.,, 1987; Farquharson and Oldenburg,, 1998; Vignoli et al.,, 2015; Viezzoli et al.,, 2009; Deleersnyder et al.,, 2021). There is, however, no guarantee that the specific regularization scheme will represent realistic features. For example, with Tikhonov regularization, overly smooth inversion models can be obtained, which will impact the reliability of the depth of such a FSI (In Section 2.3, the various thresholds in Electrical Conductivity (EC) at which the FSI is defined are described). Probabilistic methods offer an alternative in which the solution is not one model, but a collection of thousands of models all of which fit the data within the noise level. From the ensemble, statistical properties about the non-uniqueness and uncertainty of the models can be derived. Probabilistic inversion has already been conducted for AEM data (Hansen and Minsley,, 2019; Hansen,, 2021; Bai et al.,, 2021) and with SkyTEM data (Brodie and Reid,, 2013). To our knowledge, the uncertainty related to the estimation of the depth of an interface in a fresh/saltwater environment with EM has not yet been studied. Moreover, we will test the reliability of three different definitions of such a threshold. Faster (approximate) alternatives exist, such as Bayesian Evidential Learning (BEL) (Ahmed et al.,, 2024) or deep learning approaches (Hansen and Finlay,, 2022; Oh and Byun,, 2021). Those alternatives exist because probabilistic inversion for highly-dimensional problems tends to be computationally very demanding, which is why deterministic approaches prevail today. Let us mention that alternative deterministic inversion schemes exist, which can tune the degree of sharpness (Klose et al.,, 2022; Deleersnyder et al.,, 2023) and, therefore, can generate an ensemble of inversion models with different features. In Deleersnyder et al., (2024), we have found that such a wavelet-based inversion scheme can create an ensemble with a wide range overlapping with the true posterior, making them useful to qualitatively assess the range of the uncertainty. However, it tends to sample shallower interfaces more often, which is attributed to the minimum-structure inversion. The density of the samples can thus not be interpreted as a distribution, and full probabilistic approaches are still needed for an accurate uncertainty quantification. Here, we have sufficient prior knowledge and we will limit the computational burden of the probabilistic approach in that way. This work aims to quantify the uncertainty using AEM data to inform the FSI depth. It studies the effect of flight altitude, altitude uncertainty, and tilt of the transmitter-receiver frame, compares it with an FDEM system, and examines the effect of prior knowledge about the sharpness and salinity of the saltwater. Following the Hansen and Minsley, (2019) philosophy, we explicitly choose a prior model based on the available information (borehole information); see Section 2.2. A realistic synthetic model is constructed, and corresponding data is generated to exactly mimic field conditions according to the 2019 SkyTEM survey (Delsman et al.,, 2019). Our probabilistic inversion scheme, explained in Section 2.1, relies on the Differential evolution Markov chain by Laloy and Vrugt, (2012). Section 2.3 discusses the different possibilities for thresholds for a FSI. After the results in Section 3, some suggestions for further research and new surveys with a view to mapping the FSI are listed in Section 4."
https://arxiv.org/html/2411.08354v1,Developing a Foundation Model for Predicting Material Failure,"Understanding material failure is critical for designing stronger and lighter structures by identifying weaknesses that could be mitigated, predicting the integrity of engineered systems under stress to prevent unexpected breakdowns, and evaluating fractured subsurface reservoirs to ensure the long-term stability of the reservoir walls, fluid containment, and surrounding geological formations. Existing full-physics numerical simulation techniques involve trade-offs between speed, accuracy, and the ability to handle complex features like varying boundary conditions, grid types, resolution, and physical models. While each of these aspects is important, relying on a single method is often insufficient, and performing a comprehensive suite of simulations to capture variability and uncertainty is impractical due to computational constraints. We present the first foundation model specifically designed for predicting material failure, leveraging large-scale datasets and a high parameter count (up to 3B) to significantly improve the accuracy of failure predictions. In addition, a large language model provides rich context embeddings, enabling our model to make predictions across a diverse range of conditions. Unlike traditional machine learning models, which are often tailored to specific systems or limited to narrow simulation conditions, our foundation model is designed to generalize across different materials and simulators. This flexibility enables the model to handle a range of material properties and conditions, providing accurate predictions without the need for retraining or adjustments for each specific case. Our model is capable of accommodating diverse input formats, such as images and varying simulation conditions, and producing a range of outputs, from simulation results to effective properties. It supports both Cartesian and unstructured grids, with design choices that allow for seamless updates and extensions as new data and requirements emerge. Our results show that increasing the scale of the model leads to significant performance gains (loss scales as N^{-1.6}, compared to language models which often scale as N^{-0.5}). This model represents a key stepping stone to advancing predictive capabilities of material science and related fields.","Fracturing is a common phenomenon observed across a wide range of scientific domains and engineering applications, including subsurface geology [14], earthquake rupture [13], pipeline integrity [24], steel and concrete structures [7], response of human-made systems to impulsive loads [16], material design [20], high explosives performance [11], and biological structures such as bones [9], just to name a few. Accurately simulating how these fractures interact with the surrounding stress field and how they may propagate under strain is crucial for advancements in these fields, but presents significant challenges due to the high computational costs (full physics simulators) and complexity (highly heterogenous materials) involved. Machine learning, and especially natural language processing, has seen remarkable progress through the development of large-scale models trained on big and diverse datasets [4, 17, 5]. Scientific domains could similarly benefit from such approaches. However, scientific data presents unique challenges: it is expensive to generate, challenging for quality control, and often multimodal, encompassing various formats and simulation types. Despite these hurdles, some scientific fields have successfully applied large-scale modeling techniques. In protein folding prediction, models like AlphaFold [15] have achieved unprecedented accuracy in predicting 3D protein structures. In drug discovery, machine learning models have accelerated the identification of potential therapeutic compounds [36]. Similarly, in climate modeling, large-scale models have improved the accuracy of long-term climate predictions [30, 3], and in computational chemistry, they have enhanced the understanding of molecular interactions [31, 1]. These successes demonstrate the potential of large-scale models in addressing complex scientific challenges. In this work, we introduce a multimodal foundation model for predicting material failure. Our model can perform multiple tasks related to fracture prediction – namely predicting how long it will take a material to fail after loading and the pattern of fractures that cause the material to fail. The model is trained on data from three different fracture simulators in a curriculum-style approach. The first is a rule-based model that captures first order fracture growth behavior and can generate vast amounts of data on the fly. The second is a quasi-static phase-field fracture simulator that represents the fracture evolution without considering the inertia of the system. The third uses the combined finite-discrete element method to capture the full dynamics of a system under load, with the corresponding fracture initiation, propagation, and arrest, coupled with the resolution of the interaction between the discrete blocks or parts. This numerical model, while offering the highest fidelity, is also the most computationally demanding. Our framework serves as a guide for future work by showcasing how a single foundation model can progressively learn from datasets with varying levels of physical complexity – ranging from simple, rule-based simulations to high-fidelity, finite-discrete element models. The model handles diverse input formats, including both Cartesian and unstructured grids, and supports multiple prediction tasks such as fracture patterns and time-to-failure across different materials and loading conditions. This unified approach, which accommodates varied simulation environments and material behaviors, demonstrates the potential of foundation models to address complex physical systems in ways that traditional, system-specific models cannot. We employ a transformer-based architecture derived from the Senseiver [32] model, training models with up to 3 billion parameters. Our hypothesis is that by scaling to billion-parameter models, coupled with our architecture, the model will exhibit foundational properties – similar to those observed in large language models – enabling it to predict material failure across varying material types and boundary conditions. While still in early stages, this approach shows promise for generalizing beyond specific datasets and simulation conditions, making it more versatile for a range of scientific applications. We describe the techniques required to effectively conduct the training of data-driven models at this scale using scientific data. We also study how the model performance scales both in terms of the data size and the number of parameters. We find that model accuracy improves much more rapidly as parameters increase than is found in language models, suggesting that scientific data may have a structure that can be accurately modeled using fewer parameters than language models. Figure 1: Schematic of our foundation model for predicting material failure."
https://arxiv.org/html/2411.08039v1,Uncertainty Quantification of Fluid Leakage and Fault Instability in Geologic CO Storage,"Geologic CO2 storage is an important strategy for reducing greenhouse gas emissions to the atmosphere and mitigating climate change. In this process, coupling between mechanical deformation and fluid flow in fault zones is a key determinant of fault instability, induced seismicity, and CO2 leakage. Using a recently developed methodology, PREDICT, we obtain probability distributions of the permeability tensor in faults from the stochastic placement of clay smears that accounts for geologic uncertainty. We build a comprehensive set of fault permeability scenarios from PREDICT and investigate the effects of uncertainties from the fault zone internal structure and composition on forecasts of CO2 permanence and fault stability. To tackle the prohibitively expensive computational cost of the large number of simulations required to quantify uncertainty, we develop a deep-learning-based surrogate model capable of predicting flow migration, pressure buildup, and geomechanical responses in CO2 storage operations. We also compare our probabilistic estimation of CO2 leakage and fault instability with previous studies based on deterministic estimates of fault permeability. The results highlight the importance of including uncertainty and anisotropy in modeling of complex fault structures and improved management of geologic CO2 storage projects.","As a response to global climate change, carbon capture and storage (CCS) has been proposed as a mitigation technology to significantly reduce atmospheric CO2 emissions and achieve net-zero emissions by 2050 (Metz \BOthers., \APACyear2005; Orr Jr, \APACyear2009; Szulczewski \BOthers., \APACyear2012; Cozzi \BOthers., \APACyear2020; Krevor \BOthers., \APACyear2023). Injecting CO2 into geologic formations requires displacement or compression of the ambient groundwater and leads to pressure buildup in the storage aquifer. Potential hazards introduced from the operation include compromising the caprock by creating fractures and activating faults (Birkholzer \BBA Zhou, \APACyear2009; Zoback \BBA Gorelick, \APACyear2012), induced shear slip and triggered seismicity (Rutqvist \BOthers., \APACyear2007, \APACyear2008; Chiaramonte \BOthers., \APACyear2008; Rutqvist \BOthers., \APACyear2010; Morris, Detwiler\BCBL \BOthers., \APACyear2011; Morris, Hao\BCBL \BOthers., \APACyear2011; Cappa \BBA Rutqvist, \APACyear2011\APACexlab\BCnt1, \APACyear2011\APACexlab\BCnt2; Jha \BBA Juanes, \APACyear2014; White \BOthers., \APACyear2014; White \BBA Foxall, \APACyear2016; Jagalur-Mohan \BOthers., \APACyear2018), CO2 leakage and impacts on groundwater (Keating \BOthers., \APACyear2010; Newell \BBA Ilgen, \APACyear2018; Meguerdijian \BBA Jha, \APACyear2021). Therefore, comprehensive uncertainty quantification and risk assessment are essential for safe reservoir engineering designs, proper mitigation plans, and long-term management of CO2 storage. Quantitative probabilistic risk assessment of CCS is highly challenging for two reasons. First, the modeling of CO2 geological storage requires time consuming and computationally intensive simulations for coupling between multiphase flow and geomechanics (Jha \BBA Juanes, \APACyear2014; Silva \BOthers., \APACyear2024; Krevor \BOthers., \APACyear2023). Computational challenges in multiscale modeling include various resolution requirements for different regions (e.g., highly resolved grids are needed around injection wells and fault zones) and extremely large spatial-temporal domains because CO2 plume, pressure buildup and strain/stress responses propagate at different rates. Although various coupling schemes have been introduced to model the interactions between flow and geomechanics (Dean \BOthers., \APACyear2006; Jeannin \BOthers., \APACyear2007; Jha \BBA Juanes, \APACyear2007; Mainguy \BBA Longuemare, \APACyear2002; Minkoff \BOthers., \APACyear2003; Settari \BBA Mourits, \APACyear1998; Settari \BBA Walters, \APACyear2001; Tran \BOthers., \APACyear2004, \APACyear2005; Kim \BOthers., \APACyear2011, \APACyear2013; White \BOthers., \APACyear2016; Both \BOthers., \APACyear2017), restrictive conditions may need to be satisfied for stability and long iterations may be needed for convergence of highly nonlinear systems. Second, the inherent uncertainty in highly heterogeneous porous media (Kitanidis, \APACyear2015; Mallison \BOthers., \APACyear2014), together with the presence of faults and fractures (Rinaldi \BBA Rutqvist, \APACyear2013; Morris, Hao\BCBL \BOthers., \APACyear2011), demands probabilistic descriptions of the material properties accounting for heterogeneity and/or anisotropy. Due to our incomplete knowledge of fault zones, stochastic modeling of the fault properties is imperative (Saló-Salgado \BOthers., \APACyear2023). Because parameterization of uncertainties in fault properties can be high-dimensional and complex, uncertainty quantification of CO2 migration and fault stability requires running a large number of accurate numerical simulations, making the procedure prohibitively expensive. In recent years, a number of deep-learning-based surrogate models have been developed as a promising alternative to expensive numerical simulators for subsurface problems. One approach (e.g., Zhu \BBA Zabaras (\APACyear2018); Mo \BOthers. (\APACyear2019); Tang \BOthers. (\APACyear2020); Wen, Tang\BCBL \BBA Benson (\APACyear2021); Wen, Hay\BCBL \BBA Benson (\APACyear2021); Wen \BOthers. (\APACyear2023)) relies on data-driven learning of the underlying physics by approximating the mapping from diverse input field properties (e.g., permeability) to output state fields (e.g., fluid saturation and pressure buildup) given, as training data, a large number of high-fidelity simulations. Therefore, the size of the neural network scales with the size of the grid and a convolutional neural network (CNN) architecture (e.g., U-Net (Ronneberger \BOthers., \APACyear2015)) is used essentially to conduct image-to-image regression. A similar framework was later extended to coupled flow-geomechanics problems in Tang \BOthers. (\APACyear2022). Alternatively, neural operators (Lu \BOthers., \APACyear2019; Li \BOthers., \APACyear2020) are designed to find a discretization-invariant representation of the same mapping, so that the size of the network does not scale with grid resolution. Neural operators still require a large amount of data for training, however. More recently, graph neural network (GNN)-based surrogate models (Sanchez-Gonzalez \BOthers., \APACyear2020; Wu \BOthers., \APACyear2022; Ju \BOthers., \APACyear2024) have been proposed to handle unstructured meshes with stencils that vary in size and shape, removing the constraint of having regular grids and greatly improving applicability to complex geological features such as faults and fractures. While significant computational gains can be achieved through cheap inference with the aforementioned deep-learning surrogate models, the computational costs of acquiring the training data and training the network can be substantial. A different approach, known as physics-informed neural networks (PINNs) (Raissi \BOthers., \APACyear2019), integrates data with physics constraints in the form of a PDE-based loss function. PINNs have recently been applied to subsurface flow, transport and geomechanics problems (Fuks \BBA Tchelepi, \APACyear2020; He \BOthers., \APACyear2020; Haghighat \BOthers., \APACyear2021, \APACyear2022; Amini \BOthers., \APACyear2022; Yan \BOthers., \APACyear2022). The amount of training data needed for PINNs is less than in purely data-driven methods, but the neural network is usually more difficult to train because of the complex non-convex and multi-objective loss function (Wang \BOthers., \APACyear2020; Chen \BOthers., \APACyear2018). Constructing an efficient surrogate model to be used in uncertainty quantification and risk assessment for the coupled processes of mechanical deformation and fluid flow in fault zones is particularly challenging for several reasons. First, the complexity of the governing equations and the coupling of flow and geomechanics make PINNs particularly difficult and expensive to train. Second, purely data-driven methods require a substantial amount of training data, and in the data-limited regime, they are prone to overfitting. Additionally, the training data may not be sufficiently representative for the neural network to learn the underlying physics of state fields. For example, the uncertainties from the fault zone are very local and produce pressure buildup responses only within the storage reservoir and the fault, providing much less variability in training data than a full domain permeability field. Lastly, fixed time-window methods are unsuitable for capturing the dynamic processes in CO2 sequestration, as these systems require models that can adapt to varying time horizons based on the occurrence of physical events such as fault slip. This underscores the need for a dynamic method, which will be explored in the next section. Given the aforementioned challenges, we propose to use flow map learning (FML) (Qin \BOthers., \APACyear2019; Fu \BOthers., \APACyear2020; Qin \BOthers., \APACyear2021; Churchill \BBA Xiu, \APACyear2023), a deep-learning-based numerical approximation for dynamical systems, to construct efficient surrogate models directly for the target quantities of interest (QoIs) in the coupled process of flow and geomechanics. The QoIs are low-dimensional quantities that represent the flow migration, pressure buildup and geomechanical responses in CO2 storage operations, which can be used directly to monitor hazards like fault instability and induced seismicity. Compared with learning the full state field, learning the low-dimensional QoIs is computationally much more affordable, requires less training data and provides modeling outputs that are more directly relevant. The dynamical formulation of FML also allows for flexibility in the time horizon of interest. In section 2, we review the physics-based modeling of coupled flow and geomechanics, and specify the QoIs and sources of uncertainties in a representative two dimensional model. In section 3, we give an overview of the FML methodology with a detailed description of data preparation and training protocol. Finally, in section 4, we employ the FML surrogate models for accelerated uncertainty quantification to validate its accuracy, efficiency and robustness."
https://arxiv.org/html/2411.08064v1,Energy and entropy conserving compatible finite elements with upwinding for the thermal shallow water equations,"In this work, we develop a new compatible finite element formulation of the thermal shallow water equations that conserves energy and mathematical entropies given by buoyancy–related quadratic tracer variances. Our approach relies on restating the governing equations to enable discontinuous approximations of thermodynamic variables and a variational continuous time integration. A key novelty is the inclusion of centred and upwinded fluxes. The proposed semi-discrete system conserves discrete entropy for centred fluxes, monotonically damps entropy for upwinded fluxes, and conserves energy. The fully discrete scheme reflects entropy conservation at the continuous level. The ability of a new linearised Jacobian, which accounts for both centred and upwinded fluxes, to capture large variations in buoyancy and simulate thermally unstable flows for long periods of time is demonstrated for two different transient case studies. The first involves a thermogeostrophic instability where including upwinded fluxes is shown to suppress spurious oscillations while successfully conserving energy and monotonically damping entropy. The second is a double vortex where a constrained fully discrete formulation is shown to achieve exact entropy conservation in time.","Mathematical entropies, or entropy, are convex functionals arising from a positive definite Hessian [Fisher2013, Leveque1992, Chapter 3] that correspond to quadratic buoyancy–related tracer invariants for the thermal shallow water equations [Ricardo2023, Ricardo2024-dg]. The thermal shallow water equations are a useful stepping stone from simpler atmospheric systems, like the rotating shallow water equations, to the full three-dimensional compressible Euler equations typically used to describe atmospheric dynamics in operational weather models [Maynard2020, Adams2019, Lee2021]. The thermal shallow equations are analogous to the compressible Euler equations with an identical Poisson bracket and entropy, and include a thermodynamic scalar quantity reflective of temperature [Eldred2019, Ricardo2023, Ricardo2024-dg]. Such thermodynamic quantities are not included in the rotating shallow water equations [Cotter2014, BauerCotter2018, Mcrae2013, Wimmer2020], meaning these other models do not capture the effect of thermodynamic transport on the horizontal pressure gradient [Kurganov2021]. In this study, we develop a novel finite element approximation of the thermal shallow water equations that provably conserves energy and entropy at the semi-discrete level. Large scale simulations show the resulting scheme is conservative, captures turbulent dynamics, and can stably simulate non-linear flow over long time scales where a mature turbulent state is reached. The desired properties of state-of-the-art numerical solvers for atmospheric models, discussed by \citetStaniforth2012, emphasise conserving system invariants over long time scales [Gibson2019, Thuburn2008]. Energy is a key invariant that can be conserved by exploiting the non-canonical Hamiltonian form of the governing equations at the semi-discrete level [Shepherd1990, Salmon1998]. Discrete entropy conservation improves model stability of hyperbolic systems that involve thermodynamic quantities by bounding unstable growth associated with grid scale variance [Ricardo2023, Ricardo2024-dg]. Recent studies achieve semi-discrete entropy conservation by rewriting the equations of motion to allow for discontinuous approximations of thermodynamic variables [Ricardo2023, Ricardo2024-dg]. In this study, we take a similar approach and restate the thermal shallow water equations to obtain semi-discrete entropy conservation under continuous time integration. Entropy conservation requires preserving certain conformity requirements in space and time that may not be inherited by the numerical approximations. A previous entropy conserving study uses a mixed finite element method in conjunction with Galerkin projections to enforce the necessary regularity of buoyancy fluxes [Ricardo2023]. Such continuity requirements are achieved in a different study via a discontinuous Galerkin method [Ricardo2024-dg]. In the current study, we consider a compatible finite element method that includes internal element boundary fluxes, cast in either a centred or upwinded form. The centred fluxes conserve energy and entropy, while the upwinded fluxes are proven to conserve energy and monotonically damp entropy. Care is taken to derive buoyancy terms that ensure entropy exchanges are balanced in both space and time for all forcing terms. In doing so, we ensure the only source of entropy conservation error is from temporal derivatives. The semi-discrete formulation also conserves total mass, buoyancy, vorticity, and supports compatible advection of buoyancy, thus satisfying the criteria of a compatible finite element discretisation for the thermal shallow water equations [Eldred2019, Ricardo2024-dg]. Constructing Poisson time integrators for non-canonical Hamiltonian systems often requires exploiting specific structures of the model problem [Hairer2006, Chapter VII.4]. The Poisson time integrator proposed by \citetCohen2011 conserves energy through exact temporal integration of the variational derivatives of the Hamiltonian [Eldred2019, BauerCotter2018, Lee2021, Lee2022, Wimmer2020, Cotter2014]. Quadratic invariants are also temporally preserved via the Poisson integrator [Cohen2011], while cubic invariants, which represent mathematical entropies in the form of tracer variances, are generally not. Previous studies [Ricardo2023, Ricardo2024-dg] regarding entropy conservation consider strong stability preserving time integrators [Shu1988, Durran2010] for which entropy is not conserved exactly in time and energy variance is damped. In this work, we take a different approach and use a Poisson integrator [Cohen2011] to construct a fully discrete scheme that conserves energy. Discrete buoyancy is represented as a linear polynomial in time such that discrete entropy is a cubic polynomial in time, which is not temporally conserved point-wise by the chosen Poisson integrator [Cohen2011]. We prove the loss in exact entropy conservation depends on the accuracy of the temporal approximation, and consequently propose a constrained formulation using Lagrange multipliers for which entropy is exactly conserved in time. This article is structured as follows: in Section 2 we reformulate the thermal shallow water equations at the continuous level and analyse the regularity requirements for continuous entropy conservation. Finite element approximations are derived in Sections 3 and 4, where conservation at the semi- and fully discrete levels is evaluated. In Section 5, convergence of the new scheme under h-p refinement is demonstrated using a steady thermogeostrophic balance test case [Eldred2019]. By considering small perturbations from the solution at progressive time levels instead of the usual mean flow state [Eldred2019, Lee2022], we propose a new linearised Jacobian and quasi-Newton approach that shows robust convergence in the presence of large variations in buoyancy for both centred and upwinded fluxes. Thus, we stably simulate thermally unstable flows to well evolved turbulent states, as demonstrated for transient case studies involving a thermogeostrophic instability [Eldred2019, Gouzien2017, Kurganov2021, Zeitlin2018] and a double vortex [Giorgetta2009, Eldred2019]. Conservation of invariants over long time scales is assessed, and Lagrange multipliers are used to correct small temporal losses in entropy conservation for centred numerical fluxes. The inclusion of upwinded numerical fluxes is shown to suppress spurious oscillations while conserving energy and monotonically damping entropy."
https://arxiv.org/html/2411.08050v1,Spherically symmetric Earth models yield no net electron spin,"Terrestrial experiments that use electrons in Earth as a spin-polarized source have been demonstrated to provide strong bounds on exotic long-range spin-spin and spin-velocity interactions. These bounds constrain the coupling strength of many proposed ultralight bosonic dark-matter candidates. Recently, it was pointed out that a monopole-dipole coupling between the Sun and the spin-polarized electrons of Earth would result in a modification of the precession of the perihelion of Earth. Using an estimate for the net spin-polarization of Earth and experimental bounds on Earth’s perihelion precession, interesting constraints were placed on the magnitude of this monopole-dipole coupling. Here we investigate the spin associated with Earth’s electrons. We find that there are about 6\times 10^{41} spin-polarized electrons in the mantle and crust of Earth oriented anti-parallel to their local magnetic field. However, when integrated over any spherically-symmetric Earth model, we find that the vector sum of these spins is zero. In order to establish a lower bound on the magnitude of the net spin along Earth’s rotation axis we have investigated three of the largest breakdowns of Earth’s spherical symmetry: the large low shear-velocity provinces of the mantle, the crustal composition, and the oblate spheroid of Earth. From these investigations we conclude that there are at least 5\times 10^{38} spin-polarized electrons aligned anti-parallel to Earth’s rotation axis. This analysis suggests that the bounds on the monopole-dipole coupling that were extracted from Earth’s perihelion precession need to be relaxed by a factor of about 2000.","Many extensions of the Standard Model suggest new bosonic particles, several of which (e.g. the axion) are also interesting dark-matter candidates [1]. The virtual exchange of these bosons can result in new exotic (non-Standard Model) forces between particles [2, 3, 4]. Searches for exotic spin-dependent forces provide important constraints on the coupling strength of these bosons [5, 6, 7, 8, 9] as well as on torsion gravity [10]. A comprehensive review of existing experimental and observational constraints on exotic spin-dependent interactions can be found in Ref. [11]. Spin-polarized electrons within Earth’s mantle and crust (geoelectrons) have been demonstrated to be a vast and valuable spin source in searches for exotic long-range spin-spin [12] and velocity-dependent spin-spin [13] interactions. Recently, it has been suggested that this large reservoir of spin-polarized geoelectrons can be combined with experimental measurements of the precession of the perihelion of Earth in order to establish bounds on exotic monopole-dipole couplings between the Sun and the spin-polarized electrons of Earth [14]. To establish such a bound, the magnitude and direction of the net electronic spin of Earth must be known. Here we use the electron-spin model developed in Ref. [12] to estimate the magnitude and direction of the net spin associated with these geoelectrons. Assuming only that Earth’s mantle and crust are non-conductive and that their temperature, density, pressure, and chemical composition are spherically symmetric, we come to the general and somewhat surprising conclusion that the net spin of Earth’s geoelectrons is zero. This is despite the fact that we estimate that there are about 6\times 10^{41} spin-polarized geoelectron’s in Earth’s mantle and crust pointing anti-parallel to their local magnetic field. In order to determine a lower bound on the magnitude of the net spin along Earth’s rotation axis we consider the breakdown of Earth’s spherical symmetry associated with the large low shear-velocity provinces (LLSVPs), the crustal composition, and the oblate spheroid of Earth. We conservatively estimate that there are at least 5\times 10^{38} spin-polarized electrons aligned anti-parallel to Earth’s rotation axis. This analysis suggests that the bounds on the monopole-dipole coupling established in [14] need to be relaxed by a factor of \sim 2000."
https://arxiv.org/html/2411.07888v1,Realization of a clock-based global height system: A simulation study for Europe and South America,"Ongoing efforts aim at achieving a globally uniform and consistent International Height Reference System (IHRS), as a global standard for accurately determining physical (height-)coordinates across the world. Near the Earth’s surface, two stationary standard clocks that are separated by 1\,cm in height have a redshift of about 10^{-18}\, according to Einstein’s theory of general relativity. Thus, clock comparison allows for accurate height determination in high-performance clock networks. In such networks, frequency differences can be observed between clock sites and corresponding gravity potential differences can be derived. The heights can be represented as geopotential numbers and measured potential differences between clock locations in a dedicated clock network can be used to estimate the transformation parameters between regional/national height reference frames and resolve distortions in individual height systems. In our study, chronometric levelling is employed in closed-loop simulations across two different regions, Europe and Brazil. A set of realistic offsets and tilts in the local height data is assumed by considering, e.g., systematic tilts in latitude and longitude direction, errors related to the distance from the tide gauges, the elevation of levelling points, and the presence of noisy levelling lines. External effects such as tidal effects (solid earth tide, ocean load tide, pole tide), propagation errors due to fibre and space link uncertainties, random noise, and the presence of outliers are included in the simulation of the unification process. The best configuration is determined by analyzing the standard deviations of the estimated error parameters from the covariance matrix, which vary based on the spatial distribution of the clocks. An optimal setup includes placing clocks at corners, tide gauges, and the highest points of the local height systems. The added value of chronometric levelling is demonstrated for the realization of an IHRS.","The necessity of a global height system is paramount for various geodetic applications. Regional height systems are established with respect to local datums that are defined based on the local mean sea level at rest and determined using the classical levelling technique and the GNSS/geoid approach [1]. Local datum points may not coincide with the global geoid due to several factors such as sea surface topography, ocean circulation, local currents and tides. Parameters such as heights defined with respect to these local references are inappropriate for international applications as they can not be compared unless the transformation parameters between the local systems are known. Hence, the three pillars of Geodesy, Earth’s geometry, rotation, and gravity field, defined in a physical reference system offer consistency and reliability worldwide [2]. A frame defined in relation to the Earth’s gravity field supports the simultaneous determination and monitoring of the related parameters [3]. An International Height Reference System (IHRS) can be established with a proper network of atomic clocks, either by unifying existing local systems with the aid of clock observations, or by fixing a frame which is (entirely) based on distributed atomic clock networks – a chronometric reference frame. Those networks can also be used for, e.g., monitoring mass redistribution and surface deformations [4], [5]. Note that from a fundamental point of view, purely chronometric reference frames can be established on the basis of level sets of two distinct relativistic potentials related to so-called gravitoelectric and gravitomagnetic effects, i.e., gravity degrees of freedom arising from mass density and the rotation thereof [6, 7]. This allows to formulate frames based entirely on clock comparison using redshift measurements and Sagnac constellations."
https://arxiv.org/html/2411.07248v1,Do Inner Greenland’s Melt Rate Dynamics Approach Coastal Ones?,"The Greenland Ice Sheet may be nearing a tipping point, transitioning to permanent melting. This article analyses two melt rate time series using the Bayesian Langevin estimation (BLE), providing further evidence for destabilizing melt dynamics, along with new insights from the method’s nonlinear parameterisation. Comparing the results for Western Central Greenland and the Nuussuaq peninsula suggests that inland melt dynamics may approach less stable coastal dynamics on slow and fast scales. Both datasets show a significant increase in fast-scale amplitudes since the 1970s, possibly driven by large-scale atmospheric fluctuations. Additionally, the BLE’s nonlinear drift proves crucial for obtaining these results, as a linear Ornstein-Uhlenbeck process fails to capture these dynamics due to the oversimplification of a strictly positive autocorrelation function.","Climate change is one of the most critical issues of the 21st century and a major focus in the global scientific community. Without a doubt, the Earth’s climate is a complex system, for which nonlinear modelling and empirical time series analysis have become essential tools to deepen our understanding. Building on other pioneering works in the field, Svante Arrhenius [1] provided the first explanation of the greenhouse effect in 1896. Since then, many phenomena and dependencies of the Earth’s climate have been profoundly studied, often focusing on climate subsystems [2], such as the Greenland Ice Sheet (GrIS) [3]. The GrIS is believed to be nearing a tipping point or may potentially already be in a transient phase beyond it. In 2021, Boers and Rypdal [3] analysed melt rate time series extracted from stacked ice cores from Central Western Greenland (CWG) and an ice core from the Nuussuaq (NU) peninsula. Their results imply that the GrIS is threatened by a nearby tipping point at which its current state is no longer maintainable. Furthermore, the primary driver of the changing dynamics is identified as the melt-elevation feedback, i.e., the increase in melt rates with decreasing ice sheet height. They utilized the non-parametric statistical lag-1 autocorrelation (AR1) \rho_{1} and standard deviation (STD) \hat{\sigma}_{\rm d} and found significant positive trends in both time series. In Section 3.1, we complement these analyses by the parametric Bayesian Langevin estimation (BLE) [4; 5], which adds further evidence for decreasing resilience, along with new insights. The results suggest that the inland melt rate dynamics from CWG started to destabilize around 1914, approaoching the less stable niveau of coastal areas around 1975. Additionally, a recent increase of fast-scale contributions in the late 1990s in both CWG and the NU peninsula may be driven by a common fast-scale driver such as large-scale fluctuations in the atmospheric circulation. In particular, the results and implications of the BLE analysis for both datasets are not reproducible using the transformation from the AR1 and STD into the quantitative metrics of the Ornstein-Uhlenbeck estimation (OUE) (cf. Section 3.2). This limitation arises due to the multidecadal variability [6] present in the GrIS melt rates and the overall notably less accurate OUE. These findings are illustrated by a comparison between the BLE and the OUE in Section 3.2. For a streamlined presentation, both the OUE and BLE are described beforehand in Section 2."
https://arxiv.org/html/2411.07788v1,Librations and obliquity of the largest moons of Uranus,"Following the discovery of several ocean worlds in the solar system, and the selection of Uranus as the highest priority objective by the Planetary Science and Astrobiology Decadal Survey 2023-2032, the five largest moons of Uranus (Miranda, Ariel, Umbriel, Titania and Oberon) have been receiving renewed attention as they may also harbor a subsurface ocean. We assess how rotation measurements could help confirm the internal differentiation of the bodies and detect internal oceans if any. Because of the time-varying gravitational torque of Uranus on the flattened shape of its synchronous satellites, the latter librate with respect to their mean rotation and precess with a non zero obliquity. For a range of interior models with a rocky core surrounded by a hydrosphere, either solid or divided into an outer ice shell with a liquid ocean underneath, we compute their diurnal libration amplitude and obliquity. We find that if the Uranian satellites were two-layer solid bodies, libration measurement accuracies from around 0.25 m for Oberon to around 6 m for Miranda would rule out the possibility of homogeneous interiors. In combination with independent estimates of the mean moment of inertia (MOI), libration measurements could also be used to detect the presence of an ocean, the measurement precision required for this depending on the actual value of the libration amplitude. To compute the obliquity, we first build series for the orbital precession of all five satellites with a secular perturbations model. With the exception of Miranda, we show that due to the mutual gravitational interactions between the satellites, the obliquity of the large Uranian moons exhibits relatively large periodic variations around the mean value. We find that an obliquity measurement accuracy from around 1 m for Ariel to around 400 m for Oberon can rule out the homogeneous case. The presence of an internal global ocean could allow a resonant amplification of the obliquity, facilitating its detection. If no such resonance occurs, the obliquity would be almost indistinguishable from that expected for a solid body. The effect of tidal deformations on the rotation of the small to medium-sized Uranian moons is showed to be limited. Librations would be reduced by up to 10\% and obliquity increased by up to 15\% for Titania and Oberon, the effects being negligible for Miranda.","Named after characters of the English literature, Miranda, Ariel, Umbriel, Titania, and Oberon are the five largest moons of Uranus. They are small or medium-sized icy satellites about which we still have almost everything to learn. Discovered by William Herschel (Titania and Oberon, 1787), William Lassell (Ariel and Umbriel, 1851), and Gerard Kuiper (Miranda, 1948), they have only been visited by one space mission, during the flyby of Uranus by Voyager 2 in 1986. The last few decades have shown that water can exist in liquid state in the outer Solar System, beneath the surface of icy moons of Jupiter and Saturn (Nimmo and Pappalardo, 2016). The Galileo mission extensively studied the satellite system of Jupiter, discovering induced magnetic fields in the vicinity of Europa, Ganymede, and Callisto, indicating the existence of subsurface oceans (Khurana et al., 1998; Kivelson et al., 2002). Magnetic induction is just one of the multiple techniques for detecting these oceans. Observing the surface can be evocative, as in the case of Europa and its ‘icebergs’ (Carr et al., 1998). More recent Hubble Space Telescope (HST) observations of the low-amplitude oscillations of auroral ovals on Ganymede indicate the presence of an underground ocean beneath a thick layer of ice (Saur et al., 2015). The Cassini mission has extended the range of techniques used to detect the subsurface oceans of icy satellites. The small Enceladus showed extraordinary activity at its anomalously warm south pole, with plumes of water vapor and ice particles interpreted as evidence of liquid water beneath the surface, in contact with an underlying rocky core (Porco et al., 2006; Spencer et al., 2006, 2009; Postberg et al., 2011). The shape, gravity field, and diurnal librations of Enceladus are consistent with a global water reservoir beneath an ice shell around 20 or 30 km thick (Park et al. (2024) and references therein). The gravity and shape of Dione can also be explained in terms of an ice shell overlying a global water ocean (Beuthe et al., 2016; Zannoni et al., 2020). The observed obliquity of Titan is about three times larger than expected for an entirely solid and rigid Titan (Baland et al. (2019) and references therein). The significant diurnal tidal deformations experienced by the satellite also testify to the presence of a global ocean inside Titan (Iess et al., 2012; Durante et al., 2019; Goossens et al., 2024), whereas a thick ice shell over a global ocean explains the electric field measurements by the Huygens probe (Béghin et al., 2012). The latest to join the club of icy satellites with an ocean may be the highly cratered Mimas. Its libration may be compatible with the presence of a liquid layer at depth (Tajeddine et al., 2014) and the evolution of its orbit suggests the presence of a very young ocean at a depth of 20-30 km below the surface (Lainey et al., 2024). Voyager, Galileo, and Cassini-Huygens were Flagship-class missions. Neptune, Uranus and their moons are still awaiting ambitious exploration missions. The Uranus Orbiter and Probe (UOP) has been prioritized as the next Flagship-class mission by the 2023-2033 Planetary Science and Astrobiology Decadal Survey (National Academies of Sciences, Engineering, and Medicine, 2023). Among other things, the mission, during its 4-year tour, would aim to address important questions regarding the five largest Moons. What are their rock-to-ice mass ratios and internal structures? Which moons have internal oceans? The answers to these questions will also help constraining their formation and evolution. In this paper, we aim to define ranges for the rotational observables (libration and obliquity) of Uranus’s largest satellites that could be estimated, among other techniques, by landmark tracking from images obtained with the Narrow and Wide Angle Cameras (NAC and WAC) that will be part of the future UOP spacecraft’s payload. Optical data could be used jointly with the radiometric data from the radioscience experiment aboard the spacecraft to infer the satellites’ interior (Filice et al., 2024). Although not considered in this study, magnetic induction has also been proposed for investigating subsurface oceans within the largest moons of Uranus, e.g. Cochrane et al. (2021). The paper is organized as follows. In the Section 2, based on existing literature, we define a range of three-layer interiors (shell, ocean and core) for each satellite, assumed to be in hydrostatic equilibrium. The interiors are constrained by the mass and radius and are characterized by the total mean moment of inertia (MOI) and degree-2 gravity coefficients, as well as by the density, dimension and flattenings of each layer. Previous studies did not explore the influence of a difference in density between the ocean and the ice shell. We briefly discuss how this might affect the interpretation of an estimated MOI and gravity coefficients in terms of hydrosphere thickness and/or global differentiation. In Section 3, we compute the amplitude of diurnal libration and the obliquity of the solid layers (shell and core) of the three-layer interiors defined in Section 2, assuming that the satellites are locked in a Cassini state. We discuss the precision required on rotational measurements so that these measurements can be interpreted in terms of interior properties and/or global differentiation. Concluding remarks are presented in Section 4, including a discussion about the similarities and differences between our libration results and the recent study by Hemingway and Nimmo (2024)."
