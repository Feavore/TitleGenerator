URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10409v1,Design of Dedicated Tilt-to-Length Calibration Maneuvers for LISA,"Tilts of certain elements within a laser interferometer can undesirably couple into measurements as a form of noise, known as Tilt-To-Length (TTL) coupling. This TTL coupling is anticipated to be one of the primary noise sources in the Laser Interferometer Space Antenna (LISA) mission, after Time Delay Interferometry (TDI) is applied. Despite the careful interferometer design and calibration on the ground, TTL is likely to require in-flight mitigation through post-processing subtraction to achieve the necessary sensitivity. Past research has demonstrated TTL subtraction in simulations through the estimation of 24 linear coupling coefficients using a noise minimization approach. This paper investigates an approach based on performing rotation maneuvers for estimating coupling coefficients with low uncertainties. In this study, we evaluate the feasibility and optimal configurations of such maneuvers to identify the most efficient solutions. We assess the efficacy of TTL calibration maneuvers by modulating either the spacecraft (SC) attitude or the Moving Optical Sub-Assembly (MOSA) yaw angle. We found that sinusoidal signals with amplitudes of around 30\text{\,}\mathrm{nrad}\text{/} and frequencies near 43\text{\,}\mathrm{mHz}\text{/} are practical and nearly optimal choices for such modulations. Employing different frequencies generates uncorrelated signals, allowing for multiple maneuvers to be executed simultaneously. Our simulations enable us to estimate the TTL coefficients with precision below 15\text{\,}\mathrm{\SIUnitSymbolMicro m}\text{/}\mathrm{rad} (1-\sigma, in free space) after a total maneuver time of 20 minutes. The results are compared to the estimation uncertainties that can be achieved without using maneuvers.","LISA is a space-based Gravitational Wave (GW) detector that will operate within a measurement band ranging from approximately 0.1\text{\,}\mathrm{mHz}\text{/} to 1\text{\,}\mathrm{Hz}\text{/} [1]. The detector is composed of three SC, arranged in an almost equilateral triangle with 2.5 million \mathrm{km}\text{/} arm length, that will trail behind the Earth in a heliocentric orbit. The LISA mission is led by the European Space Agency (ESA) and was recently adopted for an expected launch in the 2030s. Each SC will host two MOSA, each comprising a telescope, an Optical Bench (OB) and a Test Mass (TM). Each MOSA will be pointed towards one of the two remote SC. The MOSA will be designated using the notation shown in Fig. 1, where MOSA ij refers to the assembly on SC i facing SC j. The lengths of the three arms of the LISA constellation will vary with time, unlike ground-based GW detectors. This leads to the interferometric measurements being highly affected by laser frequency noise. To mitigate this noise, the TDI [2] algorithm will be applied to generate TDI output variables, which mimic three virtual equal-arm interferometers. In this study we examine the second generation Michelson X,Y,Z combinations. These variables will be affected by TTL coupling, i.e. they contain error terms which depend on the MOSA tilt angles with respect to (w.r.t.) the incident beam. This TTL should be estimated and subtracted from the measurements in post-processing. In this paper we investigate the possibility of using rotation maneuvers to estimate the TTL coefficients, i.e. the parameters of the TTL model. By injecting a modulation signal into the angles that cause TTL, the signal for the fit is enhanced. This can reduce the uncertainty in the TTL coefficient estimation. This option could be used if TTL is less well separable from other noise terms or GW signals than anticipated. In such a case, the maneuvers could serve as a beneficial backup plan. It may also be decided to perform such maneuvers once within the commissioning phase. TTL maneuvers have already been performed in the LISA Pathfinder (LPF) mission, cf. [3, 4]. A comparable approach has successfully been used in the GRACE Follow-On (GFO) mission and is considered for future geodesy missions as well [wegener_phd, wegener_2020]. Other sources addressing TTL in LISA include [5, 6, 7, 8, 9, 10]. Wanner et al. [5] provide a comprehensive analytical description of TTL in the individual interferometers of LISA as well as in the TDI Michelson variables. In [6] and [7], it is described how the TTL error can be estimated through noise minimization and subtracted from the TDI variables, utilizing pointing angles measured by Differential Wavefront Sensing (DWS) [11]. George et al. [8] apply a Fisher information matrix analysis to derive lower bounds for the uncertainty with which the TTL coefficients can be estimated and use these to analyze the residual TTL noise after post-processing subtraction. Figure 1: LISA SC constellation, MOSA index notation. Image credit: [6]. In [9], the observability of TTL in the TDI Michelson variables is shown by propagating the TTL contributions through the TDI algorithm. The two options of estimating the TTL coefficients with or without rotation maneuvers are discussed. Periodic maneuvers at frequencies outside the LISA measurement band are considered, in order not to degrade the science measurements. Thus, large amplitudes are required, however, the feasibility of such maneuvers is not discussed. This study is extended in [10] by additionally considering GW signals in the measurements and introducing a separation of TDI variables that allows performing TTL maneuvers without disturbing science operations. The maneuvers discussed in [10] are stochastically generated, instead of periodic stimuli. A quantitative analysis of the estimation error is performed, however, a rather long integration time of 15 hours was assumed. This paper follows a different approach of designing dedicated TTL maneuvers, focussing on sinusoidal stimuli at frequencies within the LISA measurement band. We investigate what angular amplitudes are achievable when implemented via SC or MOSA rotations. A detailed analysis of the estimation uncertainty shows a strong dependency on the maneuver frequency, which can be optimized subsequently. In order to maximize the efficiency, we develop a plan to perform several maneuvers simultaneously. This facilitates very good estimation of the TTL coefficients after an integration time of merely 20 minutes. With simulations we quantify the improvement that such maneuvers provide over the noise minimization approach. The notation and the TTL model are defined in Sec. II. In Sec. III, the simulator settings are specified. The parameter estimation method is briefly described in Sec. IV. Section V on the maneuver design is the main part of this paper. In particular, we discuss the optimal maneuver frequency, and how multiple maneuvers can be performed simultaneously. The simulation results are reported in Sec. VI."
https://arxiv.org/html/2411.09234v1,"Wavelet analysis of possible association between sunspot number and rainfall over Kerala, India : A case study","Global attention has been focused on extreme climatic changes. This paper investigates the relationship between different phases of solar activity and extreme precipitation events in Kerala, India. Sunspot number and rainfall data were analysed over 122 years (1901-2022) and separated into winter, pre-monsoon, monsoon, and post-monsoon seasons on an annual scale. The study analysed climatic effects using 31-year mean values and conducted correlation and wavelet analyses (XWT and WTC). A negative correlation was observed in the winter and post-monsoon seasons, while positive correlations were seen in the pre-monsoon and monsoon seasons, all of which were statistically significant. Using cross-wavelet transform (XWT), the temporal relationship between sunspot number and rainfall values was investigated, revealing significant cross-power at an 8-12 year scale across all seasons. Wavelet coherence between the two data sets demonstrated significant correlation at the 2-4 and 4-8 year scales throughout the four seasons. Strong connections were evident at higher periods, such as the 8-16 year scale in the monsoon and post-monsoon seasons. The results show that the seasonal rainfall over Kerala is related to solar activity.The solar phases of Solar Cycles 14-24 were determined for all seasons, and the years with excessive and insufficient rainfall were identified. It was observed that the descending phase had an impact on excess rainfall events during the winter and pre-monsoon seasons, while the ascending phase notably affected the monsoon and post-monsoon seasons. The study specifically examined the different magnetic polarities of sunspots in alternating solar cycles, focusing on even and odd cycles. It was found that extreme rainfall events were more frequent during the winter and pre-monsoon seasons in the even cycles, whereas in the odd cycles, they were more prevalent during the monsoon and post-monsoon seasons. These findings are presented for the first time and may offer new perspectives on how different phases affect rainfall. This study suggests a physical link between solar activity and extreme precipitation in Kerala, which could increase predictability.","Fig. 1: Location map of Kerala Global climate change poses a hazard to human existence. The sun and anthropogenic factors exert a significant influence on weather and climate. The Sun’s magnetic fields exhibit various spatial, temporal, and energetic phenomena. Sunspots, solar flares, solar wind, coronal mass ejections, etc., are all expressions of magnetic activity in the Sun, collectively known as solar activity (Usoskin, 2017). Sunspot number quantifies sunspots and is widely used because of its long-term availability. There has long been concern about how the sun affects precipitation on Earth. Precipitation in different parts of the world appears to be affected by the sun at various intervals. The effect of solar activity on rainfall varies with time scale and region, leading to both positive and negative correlations (Tsiropoula, 2003; Zhao et al., 2004; Wasko & Sharma, 2009; Mauas et al., 2011; Rampelotto et al., 2012). Recently, few studies have been conducted on the relationship between solar and precipitation in China (Zhai, 2017; Yu et al., 2019; Song et al., 2022), the United States (Nitka & Burnecki, 2019), Europe (Laurenz et al., 2019), Africa (Mohamed & El-Mahdy, 2021), Argentina (Heredia et al., 2019), Nepal (Tiwari et al., 2021), and Northeast Asia (Song et al., 2022). The economy, agriculture, and ecosystem in India could be seriously impacted by changing rainfall patterns (Doranalu Chandrashekar et al., 2017). Many researchers have looked into the potential of a connection between solar activity and rainfall throughout India or in various regions (Jagannathan & Bhalme, 1973; Ananthakrishnan & Parthasarathy, 1984; Hiremath & Mandi, 2004a; Bhattacharyya & Narasimha, 2005; Agnihotri et al., 2011; Badruddin & Aslam, 2015; Warrier et al., 2017; Thomas & Abraham, 2022b). The direct and indirect effects were studied, and the results were often localised and contradicted other authors (Jagannathan & Parthasarathy, 1973; Bhalme et al., 1981; Hiremath, 2006; Bhattacharyya & Narasimha, 2007; Lihua et al., 2007; Selvaraj et al., 2009; Selvaraj & Aditya, 2011; Selvaraj et al., 2013; Hiremath et al., 2015; Malik & Brönnimann, 2018; Thomas et al., 2023). Kerala is located at the southwest tip of India, bounded east by the Western Ghats and the west by the Arabian Sea. It extends between 8∘15′ and 12∘50′ north latitudes and between 74∘50′ and 77∘30′ east longitudes. It shares boundaries with Karnataka in the north, Tamil Nadu in the east, and the Arabian Sea in the west. Kerala has a wet and tropical climate, and the major contribution is from the southwest monsoon and post-monsoon. The diverse features of Kerala make it more susceptible to climate change. It is known as the ”Gateway of summer monsoon”. Studies of long-term rainfall variability revealed that rainfall during the southwest monsoon significantly reduced while rainfall during the post-monsoon rose (Krishnakumar et al., 2009; Kothawale & Rajeevan, 2017). Recently, few studies have reported the influence of sunspot number on the rainfall over Kerala (Thomas & Abraham, 2022a, b; Thomas et al., 2023). It is crucial to evaluate how solar activity influences rainfall patterns in various parts of the world as this helps comprehend regional variations, enhancing our knowledge of climate change and its localised effects helping in extreme weather events forecasts. In Kerala, recent extreme rainfall events have resulted in landslides or floods that have claimed lives and destroyed property. In India, several studies have linked solar activity to extreme weather events (see, e.g. Bhalme & Mooley (1981); Azad (2011)). However, research on the influence of different solar phases over rainfall is limited. Therefore, looking at extreme rain over the Kerala region during different solar phases will be interesting. This paper studies the possible relation of rainfall over Kerala with sunspot number using Cross-wavelet transform (XWT) and Wavelet coherence (WTC). The solar phases of Solar Cycles SC14 - SC24 are identified, and their relation with extreme rainfall events is evaluated. Section 2 discusses the data and methodology of analysis. Section 3 presents the results and discussion about the wavelet analysis and occurrences of extreme rainfall events during different solar activity phases during different seasons. Section 4 presents the conclusions."
https://arxiv.org/html/2411.08066v1,q-Index Degree Distribution in Random Networks via Superstatistics,"In this study, we employ a superstatistical approach to construct q-exponential and q-Maxwell-Boltzmann complex networks, generalizing the concept of scale-free networks. By adjusting the crossover parameter \lambda, we control the degree of the q-exponential plateau at low node degrees, allowing a smooth transition to pure power-law degree distributions. Similarly, the parameter b modulates the q-Maxwell-Boltzmann curvature, facilitating a shift toward pure power-law networks. This framework introduces a novel perspective for constructing and analyzing scale-free networks. Our results show that these additional degrees of freedom significantly enhance the flexibility of both network types in terms of topological and transport properties, including clustering coefficients, small-world characteristics, and resilience to attacks. Future research will focus on exploring the dynamic properties of these networks, offering promising directions for further investigation.","In nonequilibrium statistical mechanics, superstatistical models serve as a robust framework for analyzing complex systems subjected to significant environmental changes and temperature fluctuationsBeck and Cohen (2003) A superstatistical complex system is mathematically characterized by the integration of multiple statistical distributionsAlbert and Barabási (2002), one representing equilibrium statistical mechanics and the other reflecting a gradually varying system parameter. Central to this approach is the requirement for a significant separation of timescales: the local relaxation time of the system should be substantially shorter than the typical timescale of changeBeck et al. (2005). The superstatistical framework has found applications across various complex systems, including hydrodynamic turbulence Beck (2007),frequency fluctuations in power gridsSchäfer et al. (2018),Application to the SYM-H geomagnetic indexSánchez (2024), analyze complex network formation from random graph fluctuationsAbe and Thurner (2006). and air pollution statistics Williams et al. (2020). In past research, the analysis of complex networks has primarily focused on network growth models with preferential attachment, often overlooking those without preferential attachment mechanisms. These models tend to result in different degree distribution forms and network characteristicsSampaio Filho et al. (2023). While these studies have made significant progress in certain areas, they have failed to fully explain the diversity and heterogeneity of real-world complex networks, especially in the context of nonequilibrium dynamics.To address this gap, we propose a model based on superstatistics, drawing inspiration from the theory of Brownian motion in nonequilibrium physics Carro et al. (2016); Thurner and Tsallis (2005); Wedemann et al. (2009). The goal is to reveal the statistical features of complex network structures across different scalesAlbert and Barabási (2002). Specifically, our study explores how concepts such as the q-exponential distribution, the q-Maxwell–Boltzmann distribution, and power-law distributions intertwine and jointly shape the evolution of networks at both local and global scales. Our work not only fills a critical gap in network theory but also provides new insights into understanding self-organization and nonlinear phenomena within complex systems. This approach offers a powerful theoretical tool for future tasks in network optimization and dynamic prediction"
https://arxiv.org/html/2411.08039v1,Uncertainty Quantification of Fluid Leakage and Fault Instability in Geologic CO Storage,"Geologic CO2 storage is an important strategy for reducing greenhouse gas emissions to the atmosphere and mitigating climate change. In this process, coupling between mechanical deformation and fluid flow in fault zones is a key determinant of fault instability, induced seismicity, and CO2 leakage. Using a recently developed methodology, PREDICT, we obtain probability distributions of the permeability tensor in faults from the stochastic placement of clay smears that accounts for geologic uncertainty. We build a comprehensive set of fault permeability scenarios from PREDICT and investigate the effects of uncertainties from the fault zone internal structure and composition on forecasts of CO2 permanence and fault stability. To tackle the prohibitively expensive computational cost of the large number of simulations required to quantify uncertainty, we develop a deep-learning-based surrogate model capable of predicting flow migration, pressure buildup, and geomechanical responses in CO2 storage operations. We also compare our probabilistic estimation of CO2 leakage and fault instability with previous studies based on deterministic estimates of fault permeability. The results highlight the importance of including uncertainty and anisotropy in modeling of complex fault structures and improved management of geologic CO2 storage projects.","As a response to global climate change, carbon capture and storage (CCS) has been proposed as a mitigation technology to significantly reduce atmospheric CO2 emissions and achieve net-zero emissions by 2050 (Metz \BOthers., \APACyear2005; Orr Jr, \APACyear2009; Szulczewski \BOthers., \APACyear2012; Cozzi \BOthers., \APACyear2020; Krevor \BOthers., \APACyear2023). Injecting CO2 into geologic formations requires displacement or compression of the ambient groundwater and leads to pressure buildup in the storage aquifer. Potential hazards introduced from the operation include compromising the caprock by creating fractures and activating faults (Birkholzer \BBA Zhou, \APACyear2009; Zoback \BBA Gorelick, \APACyear2012), induced shear slip and triggered seismicity (Rutqvist \BOthers., \APACyear2007, \APACyear2008; Chiaramonte \BOthers., \APACyear2008; Rutqvist \BOthers., \APACyear2010; Morris, Detwiler\BCBL \BOthers., \APACyear2011; Morris, Hao\BCBL \BOthers., \APACyear2011; Cappa \BBA Rutqvist, \APACyear2011\APACexlab\BCnt1, \APACyear2011\APACexlab\BCnt2; Jha \BBA Juanes, \APACyear2014; White \BOthers., \APACyear2014; White \BBA Foxall, \APACyear2016; Jagalur-Mohan \BOthers., \APACyear2018), CO2 leakage and impacts on groundwater (Keating \BOthers., \APACyear2010; Newell \BBA Ilgen, \APACyear2018; Meguerdijian \BBA Jha, \APACyear2021). Therefore, comprehensive uncertainty quantification and risk assessment are essential for safe reservoir engineering designs, proper mitigation plans, and long-term management of CO2 storage. Quantitative probabilistic risk assessment of CCS is highly challenging for two reasons. First, the modeling of CO2 geological storage requires time consuming and computationally intensive simulations for coupling between multiphase flow and geomechanics (Jha \BBA Juanes, \APACyear2014; Silva \BOthers., \APACyear2024; Krevor \BOthers., \APACyear2023). Computational challenges in multiscale modeling include various resolution requirements for different regions (e.g., highly resolved grids are needed around injection wells and fault zones) and extremely large spatial-temporal domains because CO2 plume, pressure buildup and strain/stress responses propagate at different rates. Although various coupling schemes have been introduced to model the interactions between flow and geomechanics (Dean \BOthers., \APACyear2006; Jeannin \BOthers., \APACyear2007; Jha \BBA Juanes, \APACyear2007; Mainguy \BBA Longuemare, \APACyear2002; Minkoff \BOthers., \APACyear2003; Settari \BBA Mourits, \APACyear1998; Settari \BBA Walters, \APACyear2001; Tran \BOthers., \APACyear2004, \APACyear2005; Kim \BOthers., \APACyear2011, \APACyear2013; White \BOthers., \APACyear2016; Both \BOthers., \APACyear2017), restrictive conditions may need to be satisfied for stability and long iterations may be needed for convergence of highly nonlinear systems. Second, the inherent uncertainty in highly heterogeneous porous media (Kitanidis, \APACyear2015; Mallison \BOthers., \APACyear2014), together with the presence of faults and fractures (Rinaldi \BBA Rutqvist, \APACyear2013; Morris, Hao\BCBL \BOthers., \APACyear2011), demands probabilistic descriptions of the material properties accounting for heterogeneity and/or anisotropy. Due to our incomplete knowledge of fault zones, stochastic modeling of the fault properties is imperative (Saló-Salgado \BOthers., \APACyear2023). Because parameterization of uncertainties in fault properties can be high-dimensional and complex, uncertainty quantification of CO2 migration and fault stability requires running a large number of accurate numerical simulations, making the procedure prohibitively expensive. In recent years, a number of deep-learning-based surrogate models have been developed as a promising alternative to expensive numerical simulators for subsurface problems. One approach (e.g., Zhu \BBA Zabaras (\APACyear2018); Mo \BOthers. (\APACyear2019); Tang \BOthers. (\APACyear2020); Wen, Tang\BCBL \BBA Benson (\APACyear2021); Wen, Hay\BCBL \BBA Benson (\APACyear2021); Wen \BOthers. (\APACyear2023)) relies on data-driven learning of the underlying physics by approximating the mapping from diverse input field properties (e.g., permeability) to output state fields (e.g., fluid saturation and pressure buildup) given, as training data, a large number of high-fidelity simulations. Therefore, the size of the neural network scales with the size of the grid and a convolutional neural network (CNN) architecture (e.g., U-Net (Ronneberger \BOthers., \APACyear2015)) is used essentially to conduct image-to-image regression. A similar framework was later extended to coupled flow-geomechanics problems in Tang \BOthers. (\APACyear2022). Alternatively, neural operators (Lu \BOthers., \APACyear2019; Li \BOthers., \APACyear2020) are designed to find a discretization-invariant representation of the same mapping, so that the size of the network does not scale with grid resolution. Neural operators still require a large amount of data for training, however. More recently, graph neural network (GNN)-based surrogate models (Sanchez-Gonzalez \BOthers., \APACyear2020; Wu \BOthers., \APACyear2022; Ju \BOthers., \APACyear2024) have been proposed to handle unstructured meshes with stencils that vary in size and shape, removing the constraint of having regular grids and greatly improving applicability to complex geological features such as faults and fractures. While significant computational gains can be achieved through cheap inference with the aforementioned deep-learning surrogate models, the computational costs of acquiring the training data and training the network can be substantial. A different approach, known as physics-informed neural networks (PINNs) (Raissi \BOthers., \APACyear2019), integrates data with physics constraints in the form of a PDE-based loss function. PINNs have recently been applied to subsurface flow, transport and geomechanics problems (Fuks \BBA Tchelepi, \APACyear2020; He \BOthers., \APACyear2020; Haghighat \BOthers., \APACyear2021, \APACyear2022; Amini \BOthers., \APACyear2022; Yan \BOthers., \APACyear2022). The amount of training data needed for PINNs is less than in purely data-driven methods, but the neural network is usually more difficult to train because of the complex non-convex and multi-objective loss function (Wang \BOthers., \APACyear2020; Chen \BOthers., \APACyear2018). Constructing an efficient surrogate model to be used in uncertainty quantification and risk assessment for the coupled processes of mechanical deformation and fluid flow in fault zones is particularly challenging for several reasons. First, the complexity of the governing equations and the coupling of flow and geomechanics make PINNs particularly difficult and expensive to train. Second, purely data-driven methods require a substantial amount of training data, and in the data-limited regime, they are prone to overfitting. Additionally, the training data may not be sufficiently representative for the neural network to learn the underlying physics of state fields. For example, the uncertainties from the fault zone are very local and produce pressure buildup responses only within the storage reservoir and the fault, providing much less variability in training data than a full domain permeability field. Lastly, fixed time-window methods are unsuitable for capturing the dynamic processes in CO2 sequestration, as these systems require models that can adapt to varying time horizons based on the occurrence of physical events such as fault slip. This underscores the need for a dynamic method, which will be explored in the next section. Given the aforementioned challenges, we propose to use flow map learning (FML) (Qin \BOthers., \APACyear2019; Fu \BOthers., \APACyear2020; Qin \BOthers., \APACyear2021; Churchill \BBA Xiu, \APACyear2023), a deep-learning-based numerical approximation for dynamical systems, to construct efficient surrogate models directly for the target quantities of interest (QoIs) in the coupled process of flow and geomechanics. The QoIs are low-dimensional quantities that represent the flow migration, pressure buildup and geomechanical responses in CO2 storage operations, which can be used directly to monitor hazards like fault instability and induced seismicity. Compared with learning the full state field, learning the low-dimensional QoIs is computationally much more affordable, requires less training data and provides modeling outputs that are more directly relevant. The dynamical formulation of FML also allows for flexibility in the time horizon of interest. In section 2, we review the physics-based modeling of coupled flow and geomechanics, and specify the QoIs and sources of uncertainties in a representative two dimensional model. In section 3, we give an overview of the FML methodology with a detailed description of data preparation and training protocol. Finally, in section 4, we employ the FML surrogate models for accelerated uncertainty quantification to validate its accuracy, efficiency and robustness."
https://arxiv.org/html/2411.07189v1,Deterministic criticality & cluster dynamics hidden in the Game of Life,"Conway’s Game of Life (GOL) is a cellular automaton showing how complex dynamical behavior emerges from simple local interactions. Although it has often been shown that the dynamics of GOL lies close to some sort of critical behavior, this system has never been studied in the context of a deterministic phase transition. Here, we study the critical dynamics of equal-state clusters that emerge in the logistic GOL: an extension of Conway’s GOL with a parameter that alters the dynamics by expanding the binary state space into a Cantor set, while maintaining the deterministic nature of the system. Upon tuning the parameter, we find that the logistic GOL comprises at least three types of asymptotic behavior, i.e. phases, that are separated by two critical points. One critical point defines the boundary between a sparse-static and a sparse-dynamic asymptotic phase, whereas the other point marks a deterministic percolation transition between the sparse-dynamic and a third, dense-dynamic asymptotic phase. Moreover, we identify distinct power-law distributions of cluster sizes near the critical points, and discuss the underlying mechanisms that give rise to such critical behavior. Overall, our work highlights the idea that scale invariance can emerge even in systems where clusters are generated by a purely deterministic process.","Criticality has always been associated with universal scaling behavior and universality classes in non-equilibrium systems [1, 2, 3, 4, 5, 6, 7]. In particular, deterministic criticality has been a focal point for structures that emerge from fixed rules, particularly in the context of percolation, a theme which has itself united various branches of mathematics [8, 9, 10, 11, 12] and physics [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]. Deterministic criticality plays a crucial role in understanding how systems governed by fixed rules can exhibit complex behaviors, including phase transitions and scaling laws, without relying on randomness. It has been examined through deterministic processes governing invasion percolation [32], electrical [33] and elastic [34] responses in deterministic fractal networks, as well as deterministic ansatz for fractal-like critical snapshots [35]. Moreover, it has also been demonstrated in deterministic walks, sandpile models [36, 37], bootstrap percolation on trees with hierarchical dynamics [38, 39]. In this work, we report the surprising emergence of deterministic criticality in a closed Game of Life (GOL) system, a cellular automaton governed by exceedingly simple deterministic evolution rules. First introduced in 1970 [40, 41], this discrete dynamical system initially gained attention as (i) an exquisite illustration of the idea that complex behavior can emerge from simple interactions, and as (ii) a universal Turing machine [42, 43]. Moreover, Conway’s GOL has been often considered an elegant starting point for studying emergent phenomena such as artificial life [44, 45, 46, 47], neural networks [48], ecology [49, 50] self-organization [51, 52, 53, 54], quantum systems [55, 56, 57, 58], criticality [59, 60, 61] and phase transitions [62, 63, 64, 65, 66, 67, 68]. Conway’s GOL has been widely investigated in the context of a phase transition because it is an inherently complex system. Indeed, the long transient dynamics and localized structures – that emerge from parallel local interactions (i.e. automaton rules) among binary states in a square lattice of sites – support the notion that this system resides between ‘order and chaos’ [69]. However, statistical investigations [64, 70, 71] have revealed that GOL is subcritical in terms of its late time behavior, with an asymptotic density significantly lower than the mean-field prediction [68]. In this respect, it has been tempting to introduce extensions of GOL with control parameters that, by incrementally modifying the automaton rules away from the original system, can identify how far the GOL dynamics is from a critical phase transition. Examples of such extensions include asynchronous [62] or mass-conserving [63] update schemes, probabilistic rules of time evolution [64, 65, 66], varying ranges of interactions [67], etc., and find that the rules of GOL are close to both continuous and discontinuous critical phase transitions. However, while most of these extensions are variations that employ probabilistic/random components, this deterministic automaton has never been investigated in the context of a deterministic phase transition [72, 73]. In this paper, we analyze the phase transitions that occur in the logistic GOL [53]: an extension inspired by the logistic map [74], where a control parameter changes the rate of update of sites by expanding the initially binary state space into a Cantor set. As the parameter ‘drifts’ the system away from Conway’s GOL, its asymptotic dynamics transitions from a sparse-inactive (I) phase (like Conway’s GOL) to a sparse-active (II), and then a dense-active (III) phase. We identify the points that separate these three distinct dynamical regimes numerically and study their critical properties by in-depth analyses of cluster dynamics. We find that the critical point separating phases I and II defines the bound of a peculiar form of self-organized criticality in the sparse-active phase, where quiescent clusters surrounded by activity follow a power-law distribution. Moreover, detailed cluster analyses at the critical point separating phases II and III, indicate a deterministic, continuous percolation transition. Interestingly, the identified percolation exhibits a Fisher exponent below two, differing from random percolation models and aligning with no enclave percolation [75]. Beyond showcasing critical behavior in deterministic dynamical systems, the logistic GOL introduces a broad range of deterministic tunability. This tunability allows for precise control of the cluster shapes and the ‘fatness’ of cluster size distributions, studied here for the first time in a variation of GOL. The paper is organized as follows. In Sec. II we discuss how we extend Conway’s GOL to the logistic GOL. In Sec. III, we report results from simulations of the logistic GOL and identify the different asymptotic dynamical regimes separated by critical points. In Sec. IV, we perform extensive cluster analyses and to study the percolation transition between phases II and III, whereas in Sec. V we investigate both critical points in terms of their power-law cluster size distributions. Lastly, in Sec. VI we provide a summary and discussion on the deterministic critical behavior identified in the logistic GOL."
https://arxiv.org/html/2411.07136v1,Trap Identification in Molecular Charge Transport Networks,"This paper introduces a method to identify traps in molecular charge transport networks as obtained by multiscale modeling of organic semiconductors. Depending on the materials, traps can be defect-like single molecules or clusters of several neighboring ones, and can have a significant impact on the dynamics of charge carriers. Our proposed method builds on the random walk model of charge dynamics on a directed, weighted graph, the molecular transport network. It comprises an effective heuristic to determine the number of traps or trap clusters based on the eigenvalues and eigenvectors of the random walk Laplacian matrix and uses subsequent spectral clustering techniques to identify these traps. In contrast to currently available methods, ours enables identification of trap molecules in organic semiconductors without having to explicitly simulate the charge dynamics. As a prototypical system we study an amorphous morphology of bathocuproine, a material with known high energetic disorder and charge trapping. Based on a first-principle multiscale model, we first obtain a reference charge transport network and then modify its properties to represent different trap characteristics. In contrast to currently available methods, our approach successfully identifies both single trap, multiple distributed traps, and a combination of a single-molecule trap and trap regions on an equal footing.","Organic semiconductors (OSCs) are materials composed of organic molecules that are often organized in a disordered, amorphous structure, and exhibit semiconducting properties. Unlike traditional inorganic semiconductors, OSCs are flexible and allow for much easier tuning of charge mobility, so they have found applications in sensing devices [1], high-performance computing [2], organic light-emitting diodes [3], and organic photovoltaic cells [4]. The functionality and controllable charge mobility of OSC are to a large extent credited to so-called traps, which at the microscopic level are the molecules that can be occupied by charge carriers resulting in a significant change of charge mobility [5, 6, 7, 8]. Those traps are usually single molecules, or a region consisting of very few molecules. Charge carriers can easily occupy those trapping molecules, while altering external conditions likely results in the release of carriers from the traps. Such behaviors lead to sensitive and controllable charge mobility of OSC. A wide range of OSC applications [9, 10, 11, 12, 13] revealed that by tuning the number of charge carriers one can achieve controllable charge mobility. In trap dominated materials where the carrier number is greater than that of traps, only a portion of the carriers is captured by the traps and the remaining carriers can experience fast transport. For example, [14, 15] show that in the Gaussian disorder models used for the theoretical study of charge transport in OSC, when the carrier number is increased by two times, the mobility can increase by approximately 100 times. Zooming into the molecular resolution, charge transport in OSCs is a sequence of transition events between the localized states [16, 17] and is modeled as a continuous time random walk (CTRW) process [18]. The transition rates of carriers depends on all the individual molecules’ geometries and relative orientation, which affect electronic structure properties such as the energy levels, electronic coupling elements between the molecules, and reorganization energies. Those quantities can be calculated from a first-principle multiscale model detailed in Section 2. On a macroscopic level, traps are often considered in the literature in terms of the energy density of state (DOS) p(E), typically assumed to be Gaussian curves or exponential. In equilibrium, the mean energy of a charge carrier in the DOS is E_{\infty}=\frac{\int_{-\infty}^{\infty}Eg(E)p(E)dE}{\int_{-\infty}^{\infty}g(% E)p(E)dE}. (1) Here g(E)=[\exp(\frac{E-E_{F}}{k_{B}T})+1]^{-1} is the Fermi-Dirac distribution with the Fermi energy E_{F} determined by \int_{-\infty}^{\infty}g(E)p(E)dE=N_{c}, with N_{c} being the number of charge carriers. Molecules with energies much lower than E_{\infty} are then considered as (deep) traps. However, such a qualitative criterion is insufficient to identify traps in a molecular charge transport network for several reasons: First, the estimate of E_{\infty} is based on a chosen model DOS which has some assumed continuous distribution. A realistic material, even on the scale 100 nm, will, however, not exhibit such a continuous DOS. Second, a discrete version of Eq. (1) depends on the number of molecules in the system, and the equilibrium energy in such a discrete DOS is dependent on system size [19]. Third, focusing on the DOS alone ignores other contributing factors to the charge dynamics, or the features of the transport network, such as electronic coupling elements between pairs of neighboring molecules, structural details of the material and or spatial correlations. These details are connected to the variety of physical sources for taps, e.g., interfacial effects, defects in molecular packing, or chemical impurities. This makes it difficult to provide a quantitative definition of traps that can be used for identification. At present, there are no methods for the identification of traps in molecular charge transport networks that perform reliably for all different trap types. Few attempts have been reported in identifying trap regions, or clusters, based on analyzing the actual simulated dynamics, e.g., via kinetic Monte Carlo (KMC) [20]. Qualitatively, once entered into such a trap region, the random walk (representing the charge dynamic of a single carrier) transitions mostly within it and escaping it is a rare event, making such KMC simulations very time-consuming. Two methods to accelerate KMC simulations which indirectly involve trap identification have previously been discussed. One is based on the (stochastic) watershed algorithm filling regions (”basins”) in the spatially resolved energy distribution [21]. This purely energy-based criterion does, however, not consider additional details of the factors influencing the molecular charge transport network. The second method [22] is based on a graph-theoretic decomposition (GD) and makes use of the fact that in the presence of trapping regions the Markov chain on the molecular charge transport network is nearly completely decomposable [23], allowing the associated graph to be partitioned into subgraphs. While this method takes the full information of the hopping-type dynamics into account, it is sensitive to the choice of parameters (related to, e.g., graph connectivity properties or transition rate ratios) and is not successful in identifying single trap nodes in the graph (as we will also discuss in Section 4.1.1). In this paper, we propose a new method that builds upon the idea of graph partitioning by using spectral clustering based on a specific type of Laplacian matrix of the graph [24, 25]. The aim of this method is to separate the graph into partitions, by minimizing a normalized cut cost function, such that the random walk processes rarely transitions between different partitions. While obtaining a minimized normalized cut is a NP-hard problem, a relaxed solution of this discrete optimization problem can be obtained from the eigenvectors of the random walk Laplacian matrix which will be introduced in Section 3. Our proposed method includes an effective heuristic for the determination of the number of traps or trap clusters based on these eigenvalues and eigenvectors of this random walk Laplacian and subsequent performing spectral clustering (using K-means clustering) to identify the traps. The former depends on a single threshold parameter for which we find an optimal choice neatly independent of the specific system. Using the charge transport network resulting from a multiscale model of an amorphous morphology of bathocuproine (BCP) [26], a molecular material with known high energetic disorder [27] and complex charge trapping behavior, we demonstrate that our approach successfully identifies both a single trap, multiple distributed traps, and a combination of a single-molecule trap and trap regions on an equal footing. We also find a strong relation between the cost function associated with the normalized cut and the charge-carrier dynamics simulated in a time-of-flight setup [28, 29], as well as the physical characteristics of the trap (regions). In what follows, Section 2 will introduce the elements of the first-principle multiscale model used to obtain the molecular charge transport network of BCP based on a combination of classical molecular dynamics (MD) with quantum electronic structure theory on the level of density-functional theory (DFT), and the calculation of the time-of-flight to assess charge-carrier dynamics from the model. In Section 3 we give the details of the spectral-clustering based trap identification method we propose in this work, including the determination of the cluster number and K-means clustering. The results of the application of this method to the BCP system and its modifications to cover different trap types is presented and discussed in Section 4. A brief conclusion and discussion concludes the paper."
https://arxiv.org/html/2411.06461v1,A fast transferable method for predicting the glass transition temperature of polymers from chemical structure,"We present a new method that successfully predicts the glass transition temperature T_{\!\textrm{g}} of polymers based on their monomer structure. The model combines ideas from Group Additive Properties (GAP) and Quantitative Structure Property Relationship (QSPR) methods, where GAP (or Group Contributions) assumes that sub-monomer motifs contribute additively to T_{\!\textrm{g}}, and QSPR links T_{\!\textrm{g}} to the physico-chemical properties of the structure through a set of molecular descriptors. This method yields fast and accurate predictions of T_{\!\textrm{g}} for polymers based on chemical motifs outside the data sample, which resolves the main limitation of the GAP approach. Using a genetic algorithm, we show that only two molecular descriptors are necessary to predict T_{\!\textrm{g}} for PAEK polymers. Our QSPR-GAP method is readily transferred to other physical properties, to measures of activity (QSAR), or to different classes of polymers such as conjugated or bio-polymers.","Polymers are remarkably versatile materials, and the combined control of monomer chemistry and chain length allows for superior tuneability of physical properties. As a polymer melt is cooled, the time-scale \tau_{\alpha} characterising its structural (\alpha) relaxation increases dramatically, and in the absence of crystallisation the structure freezes into an amorphous solid, a glass, at the glass transition temperature T_{\!\textrm{g}} [1]. Since molecular motions are controlled by T_{\!\textrm{g}}, this is a key parameter for understanding and predicting material behaviour, and it is thus essential to develop methods for accurately predicting T_{\!\textrm{g}} directly from the chemical structure. For long-chain polymers, T_{\!\textrm{g}} is molecular weight (M) independent [2, 3, 4, 5], but strongly affected both by intramolecular dihedral barriers [6, 7] (chain flexibility) and intermolecular packing effects, both of which are chemistry-specific [5]. Importantly, it has been shown that the \alpha relaxation, which defines T_{\!\textrm{g}}, is linked to relaxations on a relatively ‘local’ sub-monomer length-scale [8, 9, 10, 11, 12, 13, 14], which in turn suggests that models that predict T_{\!\textrm{g}} from monomer structure should be achievable. In this paper we present such a model, and apply it to the poly(aryl ether ketone) (PAEK) family of polymers. Predictive models that relate structure-based properties and T_{\!\textrm{g}}, and are suitable for small data sets with low chemical variability, have been proposed for polymers [15, 16, 17, 18, 19, 5, 20, 21]. For instance, an approximate correlation has been found between T_{\!\textrm{g}} and monomer-scale properties such as the molecular weight per conformational (or flexible) degree of freedom of the chain (M_{\phi}) [16, 18, 5, 15, 17], where M_{\phi} captures both chain flexibility and chain bulkiness (reflecting molecular packing). As one example, Schut et al. [18] correlated T_{\!\textrm{g}} with the mass per flexible bond for a data set divided into three polymer classes by introducing flexible groups into both the main chain and the side chains; an out-of-sample mean absolute error (MAE) for T_{\!\textrm{g}} of \lesssim 6\,\textrm{K} (per polymer class) was obtained. In another example, Xie et al. [19] assigned an ad-hoc mobility factor to each atom based on the chemical group it belongs to (e.g., alkyl, phenyl or thiophene). The monomer’s mobility was then averaged over the atomic contributions, followed by a regression of T_{\!\textrm{g}} on the monomer mobility. For a family of 32 conjugated polymers, a RMSE \simeq 13\,\textrm{K} was attained for in-sample T_{\!\textrm{g}} predictions. These methods are easily applicable and intuitive; e.g. by linking a relevant physical property, such as a molecular weight or volume, to each ‘flexible bond’, where ad-hoc rules are often introduced to quantify the influence of different bonds. However, the approaches are typically tailored to specific data sets and are not generalisable to a wider set of polymer structures [20]. Conversely, a more generalisable approach is the so-called group contribution, or group additive properties (GAP) method [22, 23, 24]. It assumes that a polymer property can be expressed by a composition-weighted average over contributions from sub-monomer motifs (fragments). The fragment contributions can be determined directly from data by a linear regression. van Krevelen [24] applied GAP to predict various polymer properties, such as transition temperatures, solubility, mechanical, optical and electrical properties; while Weyland et al. [23] quoted in-sample MAE \simeq 10\,\textrm{K} for predictions of T_{\!\textrm{g}}. Despite their broad applicability, a fundamental flaw of GAP models is that they cannot be used to make predictions for polymers containing fragments outside of the data sample [25, 26, 21]. A method that addresses some shortcomings of GAP models is the so-called quantitative structure-property relationship (QSPR) approach. QSPR-based methods use molecular descriptors [27, 28], which quantify electronic, topological or geometric properties that are calculated from atomistic representations of molecules. For polymers, QSPR methods are normally applied either to the monomer [21, 29, 25, 30] or to oligomers consisting of a few monomers [31, 32, 33], and statistical or machine learning (ML) techniques are used to determine the relationship between the descriptors and the investigated property (such as T_{\!\textrm{g}}) [34, 35, 30]. For QSPR methods applied to T_{\!\textrm{g}} predictions, RMSEs typically vary from \simeq 4-35\,\textrm{K} [21, 36, 37, 25], depending on the chemical variation within the data set. Models on larger data sets [38, 39], with higher chemical variation, typically yield prediction errors exceeding 25 K [36, 40]. A significant drawback of QSPR models is that accurate descriptor calculations can be computationally costly, especially for large monomers or oligomers. GAP and QSPR methods have usually been applied separately [21, 29, 19, 20]. However, Hopfinger et al. [26] proposed a linear regression-based model for predicting T_{\!\textrm{g}} based on a GAP-like averaging scheme, combined with associating physical properties (conformational entropy and mass) with individual bonds. Inspired by this approach, we suggest extending QSPR methods to a smaller structural scale than the monomer unit, assuming interactions between these sub-monomer motifs negligibly contribute to the property of interest. Here, we resolve the shortcomings of both the GAP and standard QSPR models, by developing a hybrid QSPR-GAP method: a molecule is divided into sub-monomer fragments for which molecular descriptors are calculated, and various linear regression methods are used to link T_{\!\textrm{g}} to the fragment structure. Our approach significantly accelerates the descriptor calculations and addresses the weakness of GAP methods, also providing accurate predictions for polymers containing fragments outside the data sample. We apply our new QSPR-GAP method to a data set of 146 linear homo- and copolymers of poly(aryl ether ketone) (PAEK) – an important class of linear polymers characterised by alternating stiff (aryls such as phenyls or biphenyls) and flexible linker (such as ethers or ketones) moieties, as shown in Fig. 1-A. The properties of PAEK polymers are highly tuneable by varying these moieties, making them suitable for a wide range of applications including smart-phone speakers, electrical insulation, automotive gears, medical implants and aircraft components [41]. To design PAEK polymers with optimised properties for specific applications, reliable structure-property relationships are essential. We use our QSPR-GAP method to predict T_{\!\textrm{g}} from the monomer structure, with an RMSE \simeq 5-12\,\textrm{K} (out-of-sample). In cases where the GAP model is known to fail (i.e., predicting polymers containing fragments outside of the training set), the model makes accurate predictions. Moreover, by identifying the molecular descriptors most important for predicting T_{\!\textrm{g}}, we reach new insights into how local molecular structure relates to the glass transition temperature in polymers. Our findings offer a pathway to predict the properties of highly complex polymer structures using small data sets, thus circumventing the need for more elaborate ML methods, which typically require larger data sets. Our method is readily generalisable to both a wider range of polymer properties (such as mechanical, optical, or electrical properties), and different classes of polymers."
https://arxiv.org/html/2411.05870v1,An Adaptive Online Smoother with Closed-Form Solutions and Information-Theoretic Lag Selection for Conditional Gaussian Nonlinear Systems,"Data assimilation (DA) combines partial observations with a dynamical model to improve state estimation. Filter-based DA uses only past and present data and is the prerequisite for real-time forecasts. Smoother-based DA exploits both past and future observations. It aims to fill in missing data, provide more accurate estimations, and develop high-quality datasets. However, the standard smoothing procedure requires using all historical state estimations, which is storage-demanding, especially for high-dimensional systems. This paper develops an adaptive-lag online smoother for a large class of complex dynamical systems with strong nonlinear and non-Gaussian features, which has important applications to many real-world problems. The adaptive lag allows the DA to utilize only observations within a nearby window, significantly reducing computational storage. Online lag adjustment is essential for tackling turbulent systems, where temporal autocorrelation varies significantly over time due to intermittency, extreme events, and nonlinearity. Based on the uncertainty reduction in the estimated state, an information criterion is developed to systematically determine the adaptive lag. Notably, the mathematical structure of these systems facilitates the use of closed analytic formulae to calculate the online smoother and the adaptive lag, avoiding empirical tunings as in ensemble-based DA methods. The adaptive online smoother is applied to studying three important scientific problems. First, it helps detect online causal relationships between state variables. Second, its advantage of computational storage is illustrated via Lagrangian DA, a high-dimensional nonlinear problem. Finally, the adaptive smoother advances online parameter estimation with partial observations, emphasizing the role of the observed extreme events in accelerating convergence.","Complex turbulent nonlinear dynamical systems (CTNDSs) have broad applications across various fields [1, 2, 3, 4]. These systems are characterized by their high dimensionality and multiscale structures, with strong nonlinear interactions occurring between state variables at different spatiotemporal scales. Extreme events, intermittency, and non-Gaussian probability density functions (PDFs) are commonly observed in these systems [5, 6, 7]. State estimation is essential for parameter estimation, prediction, optimal control, and generating complete datasets [8, 9, 10]. However, the turbulent nature of dynamics can amplify small errors in model structure, spatiotemporal solutions, or initial conditions when relying solely on forecasts. Data assimilation (DA), which integrates observations with system dynamics, is widely used to improve state estimation [11, 12, 13, 14, 15]. Given the inevitable uncertainty in state estimation, especially for the unobserved variables in a CTNDS, probabilistic state approaches via Bayesian inference are natural choices. The model provides a prior distribution, while observations inform the likelihood. They are combined to form the posterior distribution for state estimation. DA can be classified into two categories based on when observational data is incorporated. Filtering uses observations only up to the current time. Serving as the initialization, filter-based state estimation is the prerequisite for real-time forecasts. In contrast, smoothing [15, 16, 17, 18, 19] leverages data from the entire observation period, including future data, which makes it highly effective for optimal state estimation in offline data postprocessing. This helps to fill missing values, minimize bias, and create complete datasets [20]. With the extra information from future observations, smoothing often produces more accurate and less uncertain state estimates than filtering. When the system dynamics and observational mappings are linear, with additive Gaussian noise, the corresponding filtering and smoothing methods are the Kalman filter and the Rauch-Tung-Striebel (RTS) smoother, respectively [16, 21, 22], where the posterior distribution can be computed using closed-form analytical solutions. Due to the intrinsic nonlinear dynamics and non-Gaussian statistics of CTNDSs, analytic solutions for DA are rarely available. As a result, various numerical and approximate methods have been developed, including the ensemble Kalman filter/smoother and the particle (or sequential Monte Carlo) filter/smoother [14, 15, 23, 24, 25, 26, 27]. These methods are widely used but often face tremendous computational costs, especially in high-dimensional systems [28], which limits the number of particles or ensemble members, potentially causing biases and numerical instabilities [29, 30, 31]. Empirical tuning techniques, such as noise inflation, localization, and resampling, are widely used in practice to mitigate these issues [23, 32, 33, 34]. However, these ad hoc tuning methods are usually quite challenging to implement systematically. Closed-form analytic solutions for DA are thus highly desirable, as they improve computational efficiency, stability, and accuracy, especially in capturing non-Gaussian features, including intermittency and extreme events. They also facilitate theoretical analysis of error and uncertainty in state estimation. Instead of refining DA schemes directly, computational challenges in state estimation can be addressed by developing approximate models that yield analytic solutions for the posterior distribution. While linear approximations allow for standard methods like the Kalman filter or RTS smoother, linearizing a strongly nonlinear system often leads to biases and instabilities. An alternative is a recently developed class of nonlinear systems that includes many turbulent models in geophysics, fluids, engineering, and neuroscience [35, 36, 37, 38, 39]. Despite their nonlinear dynamics and non-Gaussian statistics, the conditional distributions of unobserved state variables given observations are Gaussian (which is precisely the posterior distribution in the DA context), leading to the term conditional Gaussian nonlinear systems (CGNSs). The CGNS framework allows the use of closed analytic formulae for solving these conditional distributions, helping develop efficient algorithms for filtering, smoothing, and sampling without the ad hoc tuning often needed in ensemble-based DA methods. It also facilitates rigorous analysis of these methods. Additionally, CGNSs have been utilized as surrogate models in various applications, including DA, prediction, preconditioning, and machine learning [39, 40, 41, 42]. The standard smoother-based state estimation procedure involves executing a forward pass for filtering across the entire observational period, followed by a backward pass for smoothing [16, 17]. However, the standard offline smoother requires storing the filter solution for the entire duration before initiating the backward pass, which requires substantial computational storage, particularly in high-dimensional systems. Due to the wide application of smoother-based state estimation, it is of practical importance to develop a computationally efficient and accurate algorithm that significantly reduces storage. This paper presents a forward-in-time online smoother algorithm with adaptive lags for the CGNS framework, eliminating the need for a full backward pass. The online smoother sequentially updates the current state as new observations become available. By doing so, it effectively addresses the computational storage issue. While online schemes exist for the RTS smoother and ensemble-based methods, the CGNS online smoother has several unique advantages. First, despite the intrinsic nonlinearity of the underlying dynamics, closed analytic formulae are available to compute the nonlinear online smoother. These analytic formulae provide precise and accurate solutions, which avoid numerical and sampling errors as in ensemble-based methods. Second, due to the turbulent nature of the system, observations influence the estimated state only within a short time window, which enhances computational efficiency and reduces storage needs. Third, different from fixed-lag smoothers [17, 27, 43, 44], the lag in the CGNS smoother is adaptively determined. Online lag adjustment is essential for studying turbulent systems, where temporal autocorrelation varies significantly over time due to intermittency, extreme events, and nonlinearity. A fixed lag usually either overuses storage (if the lag is overestimated) or introduces a large bias (if the lag is underestimated). In contrast, an adaptive lag optimizes the use of data and computational storage. Finally, the adaptive lag is systematically determined using an information criterion based on the uncertainty reduction in the posterior distribution [38, 45]. It emphasizes the importance of the posterior uncertainty and differs from some of the existing adaptive lag selection criteria that rely solely on the posterior mean [46]. As closed analytic formulae are available for posterior distributions, the information gain can be computed efficiently and accurately. The adaptive online smoother for the CGNS framework is applied to studying three important scientific problems. First, the online update of the smoother estimate allows for quantification of the improvement in state estimation by incorporating future information. It facilitates revealing causal dependence between state variables. A nonlinear dyad model with strong non-Gaussian features is utilized for such a study. Second, the CGNS framework is applied to Lagrangian data assimilation (DA), which is a high-dimensional nonlinear problem that has a significant storage requirement [42, 47, 48, 49]. The online smoother allows for the estimation of the unobserved flow states based on Lagrangian tracers. The study highlights the role of the online smoother in reducing computational storage. Finally, the online smoother facilitates developing an online parameter estimation algorithm with partial observations. It helps reveal the role of the observed intermittent extreme events in advancing parameter estimation. The remainder of this paper is organized as follows. Section 2 introduces the CGNS modeling framework, including the equations for the offline optimal nonlinear filter and smoother state estimation. Section 3 presents the adaptive online smoother. In Section 4, the application of the adaptive online smoother to the three key problems is demonstrated. Section 5 includes the conclusion. The appendices contain detailed analysis and proofs."
