{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 lxml -q"
      ],
      "metadata": {
        "id": "z8WoGpzQtgpB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm -q"
      ],
      "metadata": {
        "id": "OV15pYXTDtHF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from urllib.parse import urljoin\n",
        "from typing import List\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "0AIsCSBiCCPR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vcM6s3pUsUa",
        "outputId": "d9904568-fdc4-46ba-b9d6-4be86ef6e44f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTML scrawler\n"
      ],
      "metadata": {
        "id": "FbcyBqf5tdat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_title(soup):\n",
        "    h1_tag = soup.find(\"h1\")\n",
        "    title = h1_tag.get_text(strip=True) if h1_tag else None\n",
        "    return title\n",
        "\n",
        "def extract_abstract(soup):\n",
        "    abstract = soup.find('div', class_='ltx_abstract')\n",
        "    paragraph = abstract.find_all('p')\n",
        "    abstract = ''\n",
        "    for p in paragraph:\n",
        "        abstract += p.get_text()\n",
        "    clean_abstract = re.sub(r'\\s+', ' ', abstract)\n",
        "    return clean_abstract.strip()\n",
        "\n",
        "\n",
        "def extract_intro(soup):\n",
        "    # Attempt to find a section with an <h2> tag that includes \"Introduction\"\n",
        "    intro = None\n",
        "    for section in soup.find_all('section'):\n",
        "        h2_tag = section.find('h2', {'class': 'ltx_title ltx_title_section'})\n",
        "        if h2_tag and 'Introduction' in h2_tag.get_text():\n",
        "            intro = section\n",
        "            h2_tag.decompose()  # Remove the <h2> tag to clean the intro text\n",
        "            break\n",
        "\n",
        "    # Fallback if no <h2> with \"Introduction\" is found\n",
        "    if not intro:\n",
        "        intro = soup.find('section')\n",
        "\n",
        "    if intro:\n",
        "        # Clean and format the intro text\n",
        "        clean_intro = re.sub(r'\\s+', ' ', intro.get_text())\n",
        "        return clean_intro.strip()\n",
        "\n",
        "    # Return None if no suitable section was found\n",
        "    return None"
      ],
      "metadata": {
        "id": "1wF9PFCMzMw7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigate Arxiv structure\n"
      ],
      "metadata": {
        "id": "ZNLNpyIiByT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_content(url):\n",
        "    \"\"\"Fetches content of a page.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        print(f\"Failed to fetch {url}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "dd8x46ItyoRv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find out which paper has html format"
      ],
      "metadata": {
        "id": "BwIPaMLIwxc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_view_html_links(page_content) -> List:\n",
        "    \"\"\"Finds all <a> tags with title=\"View HTML\".\"\"\"\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    # Find all <a> tags with title=\"View HTML\"\n",
        "    view_html_links = soup.find_all(\"a\", title=\"View HTML\")\n",
        "\n",
        "    html_links = []\n",
        "    # Print each link's href and text\n",
        "    for link in view_html_links:\n",
        "        href = link.get(\"href\")\n",
        "        html_links.append(href)\n",
        "    return html_links"
      ],
      "metadata": {
        "id": "KbZxAe0Bwu-l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(find_view_html_links(get_page_content(\"https://arxiv.org/list/cs.CV/recent?skip=0&show=2000\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMfqsAi1YhB2",
        "outputId": "436e04cf-565c-4f48-a92f-7f6718427640"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "513"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrawl paper links on sub pages"
      ],
      "metadata": {
        "id": "H-vakfE016Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def active_all_sub_pages(start_url):\n",
        "    html_url_list = []\n",
        "\n",
        "    # Fetch page content\n",
        "    page_content = get_page_content(start_url)\n",
        "    if page_content is None:\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "\n",
        "    # # Pagination logic\n",
        "    # paging = soup.select_one(\"div.paging\")\n",
        "    # paging = soup.select_one(\"div.paging\")\n",
        "    # if paging:\n",
        "    #     # Find all pagination links in paging section\n",
        "    #     pagination_links = paging.find_all(\"a\", href=True)\n",
        "    #     if pagination_links:\n",
        "    #         for i in range(len(pagination_links)):\n",
        "    #             sub_page_url = urljoin(base_url, pagination_links[i][\"href\"])\n",
        "    #             print(f\"Crawling sub page: {sub_page_url}\")\n",
        "    #             sub_page_content = get_page_content(sub_page_url)\n",
        "    #             html_url_list.extend(find_view_html_links(page_content))\n",
        "    #     else:\n",
        "    #         print(\"Reached the last page, stopping crawl.\")\n",
        "    #         return None\n",
        "    # else:\n",
        "    #     return None\n",
        "\n",
        "    active_href = soup.find('div', class_='morefewer').find_all('a', href=True)\n",
        "    all_pages = active_href[len(active_href)-1]\n",
        "\n",
        "    if 'all' in all_pages.text:\n",
        "        all_pages_url = urljoin(base_url, all_pages[\"href\"])\n",
        "        all_pages_content = get_page_content(all_pages_url)\n",
        "    else:\n",
        "        all_pages_content = page_content\n",
        "\n",
        "    html_url_list.extend(find_view_html_links(all_pages_content))\n",
        "\n",
        "    return html_url_list\n"
      ],
      "metadata": {
        "id": "Ee2JdpmjB8pw",
        "collapsed": true
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(base_url, start_url):\n",
        "    paper_url_lst = active_all_sub_pages(start_url)\n",
        "    if paper_url_lst is None:\n",
        "        return None\n",
        "    data = []\n",
        "    missing_urls = []\n",
        "    fail_urls = []\n",
        "\n",
        "    for url in tqdm(paper_url_lst, desc=\"Processing URLs\"):\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "          soup = BeautifulSoup(response.text, 'lxml')\n",
        "        else:\n",
        "          print(f\"Failed to fetch {url}\")\n",
        "          return None\n",
        "        try:\n",
        "          title = extract_title(soup)\n",
        "          abstract = extract_abstract(soup)\n",
        "          intro = extract_intro(soup)\n",
        "\n",
        "          if not title or not abstract or not intro:\n",
        "              missing_urls.append(url)\n",
        "              continue\n",
        "        except:\n",
        "          fail_urls.append(url)\n",
        "          continue\n",
        "\n",
        "        # Append to data list as a dictionary\n",
        "        data.append({\n",
        "            \"URL\": url,\n",
        "            \"Title\": title,\n",
        "            \"Abstract\": abstract,\n",
        "            \"Introduction\": intro\n",
        "        })\n",
        "\n",
        "    print(f\"Failed to fetch {fail_urls}\")\n",
        "    print(f\"Missing urls: {missing_urls}\")\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "j4miQtwTAP4Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(data, filename):\n",
        "    print(\"\\t Valid data: \", len(data))\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "0IEZ5cdNOCtR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "<a href=\"/list/cs.AI/recent\" id=\"cs.AI\" aria-labelledby=\"main-cs cs.AI\">Artificial Intelligence</a>;\n",
        "<a href=\"/list/cs.CL/recent\" id=\"cs.CL\" aria-labelledby=\"main-cs cs.CL\">Computation and Language</a>;\n",
        "<a href=\"/list/cs.CC/recent\" id=\"cs.CC\" aria-labelledby=\"main-cs cs.CC\">Computational Complexity</a>;\n",
        "<a href=\"/list/cs.CE/recent\" id=\"cs.CE\" aria-labelledby=\"main-cs cs.CE\">Computational Engineering, Finance, and Science</a>;\n",
        "<a href=\"/list/cs.CG/recent\" id=\"cs.CG\" aria-labelledby=\"main-cs cs.CG\">Computational Geometry</a>;\n",
        "<a href=\"/list/cs.GT/recent\" id=\"cs.GT\" aria-labelledby=\"main-cs cs.GT\">Computer Science and Game Theory</a>;\n",
        "<a href=\"/list/cs.CV/recent\" id=\"cs.CV\" aria-labelledby=\"main-cs cs.CV\">Computer Vision and Pattern Recognition</a>;\n",
        "<a href=\"/list/cs.CY/recent\" id=\"cs.CY\" aria-labelledby=\"main-cs cs.CY\">Computers and Society</a>;\n",
        "<a href=\"/list/cs.CR/recent\" id=\"cs.CR\" aria-labelledby=\"main-cs cs.CR\">Cryptography and Security</a>;\n",
        "<a href=\"/list/cs.DS/recent\" id=\"cs.DS\" aria-labelledby=\"main-cs cs.DS\">Data Structures and Algorithms</a>;\n",
        "<a href=\"/list/cs.DB/recent\" id=\"cs.DB\" aria-labelledby=\"main-cs cs.DB\">Databases</a>;\n",
        "<a href=\"/list/cs.DL/recent\" id=\"cs.DL\" aria-labelledby=\"main-cs cs.DL\">Digital Libraries</a>;\n",
        "<a href=\"/list/cs.DM/recent\" id=\"cs.DM\" aria-labelledby=\"main-cs cs.DM\">Discrete Mathematics</a>;\n",
        "<a href=\"/list/cs.DC/recent\" id=\"cs.DC\" aria-labelledby=\"main-cs cs.DC\">Distributed, Parallel, and Cluster Computing</a>;\n",
        "<a href=\"/list/cs.ET/recent\" id=\"cs.ET\" aria-labelledby=\"main-cs cs.ET\">Emerging Technologies</a>;\n",
        "<a href=\"/list/cs.FL/recent\" id=\"cs.FL\" aria-labelledby=\"main-cs cs.FL\">Formal Languages and Automata Theory</a>;\n",
        "'''"
      ],
      "metadata": {
        "id": "HJOQqqsMyGGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "html = \"\"\"\n",
        "<a href=\"/list/cs.GL/recent\" id=\"cs.GL\" aria-labelledby=\"main-cs cs.GL\">General Literature</a>;\n",
        "<a href=\"/list/cs.GR/recent\" id=\"cs.GR\" aria-labelledby=\"main-cs cs.GR\">Graphics</a>;\n",
        "<a href=\"/list/cs.AR/recent\" id=\"cs.AR\" aria-labelledby=\"main-cs cs.AR\">Hardware Architecture</a>;\n",
        "<a href=\"/list/cs.HC/recent\" id=\"cs.HC\" aria-labelledby=\"main-cs cs.HC\">Human-Computer Interaction</a>;\n",
        "<a href=\"/list/cs.IR/recent\" id=\"cs.IR\" aria-labelledby=\"main-cs cs.IR\">Information Retrieval</a>;\n",
        "<a href=\"/list/cs.IT/recent\" id=\"cs.IT\" aria-labelledby=\"main-cs cs.IT\">Information Theory</a>;\n",
        "<a href=\"/list/cs.LO/recent\" id=\"cs.LO\" aria-labelledby=\"main-cs cs.LO\">Logic in Computer Science</a>;\n",
        "<a href=\"/list/cs.LG/recent\" id=\"cs.LG\" aria-labelledby=\"main-cs cs.LG\">Machine Learning</a>;\n",
        "<a href=\"/list/cs.MS/recent\" id=\"cs.MS\" aria-labelledby=\"main-cs cs.MS\">Mathematical Software</a>;\n",
        "<a href=\"/list/cs.MA/recent\" id=\"cs.MA\" aria-labelledby=\"main-cs cs.MA\">Multiagent Systems</a>;\n",
        "<a href=\"/list/cs.MM/recent\" id=\"cs.MM\" aria-labelledby=\"main-cs cs.MM\">Multimedia</a>;\n",
        "<a href=\"/list/cs.NI/recent\" id=\"cs.NI\" aria-labelledby=\"main-cs cs.NI\">Networking and Internet Architecture</a>;\n",
        "<a href=\"/list/cs.NE/recent\" id=\"cs.NE\" aria-labelledby=\"main-cs cs.NE\">Neural and Evolutionary Computing</a>;\n",
        "<a href=\"/list/cs.NA/recent\" id=\"cs.NA\" aria-labelledby=\"main-cs cs.NA\">Numerical Analysis</a>;\n",
        "<a href=\"/list/cs.OS/recent\" id=\"cs.OS\" aria-labelledby=\"main-cs cs.OS\">Operating Systems</a>;\n",
        "<a href=\"/list/cs.OH/recent\" id=\"cs.OH\" aria-labelledby=\"main-cs cs.OH\">Other Computer Science</a>;\n",
        "<a href=\"/list/cs.PF/recent\" id=\"cs.PF\" aria-labelledby=\"main-cs cs.PF\">Performance</a>;\n",
        "<a href=\"/list/cs.PL/recent\" id=\"cs.PL\" aria-labelledby=\"main-cs cs.PL\">Programming Languages</a>;\n",
        "<a href=\"/list/cs.RO/recent\" id=\"cs.RO\" aria-labelledby=\"main-cs cs.RO\">Robotics</a>;\n",
        "<a href=\"/list/cs.SI/recent\" id=\"cs.SI\" aria-labelledby=\"main-cs cs.SI\">Social and Information Networks</a>;\n",
        "<a href=\"/list/cs.SE/recent\" id=\"cs.SE\" aria-labelledby=\"main-cs cs.SE\">Software Engineering</a>;\n",
        "<a href=\"/list/cs.SD/recent\" id=\"cs.SD\" aria-labelledby=\"main-cs cs.SD\">Sound</a>;\n",
        "<a href=\"/list/cs.SC/recent\" id=\"cs.SC\" aria-labelledby=\"main-cs cs.SC\">Symbolic Computation</a>;\n",
        "<a href=\"/list/cs.SY/recent\" id=\"cs.SY\" aria-labelledby=\"main-cs cs.SY\">Systems and Control</a>\n",
        "\"\"\"\n",
        "\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "category_dict = {}\n",
        "\n",
        "for a in soup.find_all('a'):\n",
        "    # Convert the text content to snake_case and use it as the key\n",
        "    category_name = a.text.lower().replace(\" \", \"_\").replace(\",\", \"\")\n",
        "    category_dict[category_name] = a['href']\n"
      ],
      "metadata": {
        "id": "Au7Ah6SczXNG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = \"https://arxiv.org\"\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t-EhiBlO5rXn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, start_url in list(category_dict.items()):\n",
        "    print(f\"Crawling category: {name}\")\n",
        "    start_url = urljoin(base_url, start_url)\n",
        "    try:\n",
        "      content = main(base_url, start_url)\n",
        "    except:\n",
        "      print(f\"  Invalid structure. Failed to crawl {name}\")\n",
        "      continue\n",
        "    save_to_csv(content, f\"/content/drive/MyDrive/TitleGenerator/DataSheet/{name}.csv\")"
      ],
      "metadata": {
        "id": "PY3oeauoGe-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e3b51a-e00f-4b08-c687-8d118ab551a9",
        "collapsed": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling category: general_literature\n",
            "Invalid structure. Failed to crawl general_literature\n",
            "Crawling category: graphics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 18/18 [00:05<00:00,  3.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch []\n",
            "Missing urls: []\n",
            "\t Valid data:  18\n",
            "Crawling category: hardware_architecture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 22/22 [00:07<00:00,  2.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.00815v1', 'https://arxiv.org/html/2411.00734v1', 'https://arxiv.org/html/2411.00530v1', 'https://arxiv.org/html/2411.00408v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  18\n",
            "Crawling category: human-computer_interaction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 86/86 [00:22<00:00,  3.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.03243v1', 'https://arxiv.org/html/2411.02714v1', 'https://arxiv.org/html/2411.00007v1']\n",
            "Missing urls: ['https://arxiv.org/html/2411.03287v1']\n",
            "\t Valid data:  82\n",
            "Crawling category: information_retrieval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 64/64 [00:28<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.01843v1', 'https://arxiv.org/html/2411.00702v1', 'https://arxiv.org/html/2411.00188v1']\n",
            "Missing urls: ['https://arxiv.org/html/2411.00780v1']\n",
            "\t Valid data:  60\n",
            "Crawling category: information_theory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 61/61 [01:03<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.04306v1', 'https://arxiv.org/html/2411.03054v1', 'https://arxiv.org/html/2411.00790v1']\n",
            "Missing urls: ['https://arxiv.org/html/2411.02004v1']\n",
            "\t Valid data:  57\n",
            "Crawling category: logic_in_computer_science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 17/17 [00:14<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.00026v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  16\n",
            "Crawling category: machine_learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 846/846 [08:46<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.04453v1', 'https://arxiv.org/html/2411.04396v1', 'https://arxiv.org/html/2411.04315v1', 'https://arxiv.org/html/2411.04580v1', 'https://arxiv.org/html/2411.04389v1', 'https://arxiv.org/html/2411.04265v1', 'https://arxiv.org/html/2411.03877v1', 'https://arxiv.org/html/2411.03753v1', 'https://arxiv.org/html/2411.03387v1', 'https://arxiv.org/html/2411.03320v1', 'https://arxiv.org/html/2411.02708v1', 'https://arxiv.org/html/2411.02820v1', 'https://arxiv.org/html/2411.02645v1', 'https://arxiv.org/html/2411.02635v1', 'https://arxiv.org/html/2411.02557v1', 'https://arxiv.org/html/2411.02495v1', 'https://arxiv.org/html/2411.02343v1', 'https://arxiv.org/html/2411.00851v1', 'https://arxiv.org/html/2411.00796v1', 'https://arxiv.org/html/2411.02224v2', 'https://arxiv.org/html/2411.01982v1', 'https://arxiv.org/html/2411.00833v1', 'https://arxiv.org/html/2411.00336v1', 'https://arxiv.org/html/2411.00623v1', 'https://arxiv.org/html/2411.00530v1', 'https://arxiv.org/html/2411.00446v1', 'https://arxiv.org/html/2411.00408v1', 'https://arxiv.org/html/2411.00345v1', 'https://arxiv.org/html/2411.00003v1']\n",
            "Missing urls: ['https://arxiv.org/html/2411.04280v1', 'https://arxiv.org/html/2411.04278v1', 'https://arxiv.org/html/2411.04257v1', 'https://arxiv.org/html/2411.04225v1', 'https://arxiv.org/html/2411.04975v1', 'https://arxiv.org/html/2411.03519v1', 'https://arxiv.org/html/2411.03021v1', 'https://arxiv.org/html/2411.02355v1', 'https://arxiv.org/html/2411.02271v1', 'https://arxiv.org/html/2411.02114v1', 'https://arxiv.org/html/2411.01898v1', 'https://arxiv.org/html/2411.01818v1', 'https://arxiv.org/html/2411.01808v1', 'https://arxiv.org/html/2411.01453v1', 'https://arxiv.org/html/2411.01126v1', 'https://arxiv.org/html/2411.02217v1', 'https://arxiv.org/html/2411.01783v1', 'https://arxiv.org/html/2411.01596v1', 'https://arxiv.org/html/2411.01375v2', 'https://arxiv.org/html/2411.01142v1', 'https://arxiv.org/html/2411.01076v2', 'https://arxiv.org/html/2411.00918v1', 'https://arxiv.org/html/2411.00745v1', 'https://arxiv.org/html/2411.00698v1', 'https://arxiv.org/html/2411.00263v1', 'https://arxiv.org/html/2411.00742v1']\n",
            "\t Valid data:  791\n",
            "Crawling category: mathematical_software\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch []\n",
            "Missing urls: []\n",
            "\t Valid data:  2\n",
            "Crawling category: multiagent_systems\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 27/27 [00:12<00:00,  2.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.02820v1']\n",
            "Missing urls: ['https://arxiv.org/html/2411.03519v1']\n",
            "\t Valid data:  25\n",
            "Crawling category: multimedia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 19/19 [00:05<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch []\n",
            "Missing urls: []\n",
            "\t Valid data:  19\n",
            "Crawling category: networking_and_internet_architecture\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 59/59 [00:26<00:00,  2.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.03507v1', 'https://arxiv.org/html/2411.03203v1', 'https://arxiv.org/html/2411.03017v1', 'https://arxiv.org/html/2411.01970v1', 'https://arxiv.org/html/2411.00681v1', 'https://arxiv.org/html/2411.00408v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  53\n",
            "Crawling category: neural_and_evolutionary_computing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 27/27 [00:11<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch []\n",
            "Missing urls: ['https://arxiv.org/html/2411.02406v1']\n",
            "\t Valid data:  26\n",
            "Crawling category: numerical_analysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 64/64 [01:14<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch []\n",
            "Missing urls: ['https://arxiv.org/html/2411.04145v1']\n",
            "\t Valid data:  63\n",
            "Crawling category: operating_systems\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 2/2 [00:00<00:00,  6.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch []\n",
            "Missing urls: []\n",
            "\t Valid data:  2\n",
            "Crawling category: other_computer_science\n",
            "Invalid structure. Failed to crawl other_computer_science\n",
            "Crawling category: performance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 6/6 [00:02<00:00,  2.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.00815v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  5\n",
            "Crawling category: programming_languages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 6/6 [00:02<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.00637v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  5\n",
            "Crawling category: robotics\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 169/169 [00:58<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.04374v1', 'https://arxiv.org/html/2411.04005v1', 'https://arxiv.org/html/2411.01943v1', 'https://arxiv.org/html/2411.01475v1', 'https://arxiv.org/html/2411.00785v1', 'https://arxiv.org/html/2411.00345v1', 'https://arxiv.org/html/2411.00007v1']\n",
            "Missing urls: ['https://arxiv.org/html/2411.03287v1', 'https://arxiv.org/html/2411.00659v1']\n",
            "\t Valid data:  160\n",
            "Crawling category: social_and_information_networks\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 41/41 [00:21<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.01852v1', 'https://arxiv.org/html/2411.01330v1', 'https://arxiv.org/html/2411.02005v1', 'https://arxiv.org/html/2411.00376v1', 'https://arxiv.org/html/2411.00702v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  36\n",
            "Crawling category: software_engineering\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 38/38 [00:10<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.03455v1', 'https://arxiv.org/html/2411.01601v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  36\n",
            "Crawling category: sound\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 43/43 [00:10<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.04142v1']\n",
            "Missing urls: []\n",
            "\t Valid data:  42\n",
            "Crawling category: symbolic_computation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 2/2 [00:01<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch []\n",
            "Missing urls: []\n",
            "\t Valid data:  2\n",
            "Crawling category: systems_and_control\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing URLs: 100%|██████████| 93/93 [00:54<00:00,  1.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to fetch ['https://arxiv.org/html/2411.03834v1', 'https://arxiv.org/html/2411.03271v1', 'https://arxiv.org/html/2411.01668v1', 'https://arxiv.org/html/2411.00318v1']\n",
            "Missing urls: ['https://arxiv.org/html/2411.03287v1', 'https://arxiv.org/html/2411.01380v1']\n",
            "\t Valid data:  87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}