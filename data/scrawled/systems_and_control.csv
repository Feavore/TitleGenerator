URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04833v1,Finding Control Invariant Sets via Lipschitz Constants of Linear Programs,"Control invariant sets play an important role in safety-critical control and find broad application in numerous fields such as obstacle avoidance for mobile robots. However, finding valid control invariant sets of dynamical systems under input limitations is notoriously difficult. We present an approach to safely expand an initial set while always guaranteeing that the set is control invariant. Specifically, we define an expansion law for the boundary of a set and check for control invariance using Linear Programs (LPs). To verify control invariance on a continuous domain, we leverage recently proposed Lipschitz constants of LPs to transform the problem of continuous verification into a finite number of LPs. Using concepts from differentiable optimization, we derive the safe expansion law of the control invariant set and show how it can be interpreted as a second invariance problem in the space of possible boundaries. Finally, we show how the obtained set can be used to obtain a minimally invasive safety filter in a Control Barrier Function (CBF) framework. Our work is supported by theoretical results as well as numerical examples.","I INTRODUCTION One of the key concepts in the study of dynamical systems is forward invariance, which refers to the property that, if a set contains the system state at some time, it will also contain it in the future [1]. Invariant sets have been studied for decades in the context of Lyapunov stability, constrained control or robustness analysis [2]. For controlled dynamical systems such as, e.g. robots, we say a set is control invariant or viable, if there exists a feasible control input that renders a set forward invariant. Within the field of safety-critical control, numerous methods have been proposed to guarantee control invariance such as Hamilton-Jacobi (HJ) reachability analysis [3], model predictive control [4] or Control Barrier Functions (CBFs) [5], to name a few. HJ reachability finds the largest control invariant set under input constraints and possible adversaries through dynamic programming. CBFs define a control invariant set as the super level set of a continuously differentiable function that ensures a positive flow into the set for all boundary states. Despite fast progress in the field, finding valid CBFs under actuation constraints is notoriously difficult and still remains an open problem. Early approaches to finding a valid CBF rely on sum-of-squares (SOS) programming [6, 7, 8, 9] which can be readily used when the system dynamics are represented as polynomials. A recent study [10] has explored the relationship between HJ reachability and CBFs, enabling the synthesis of CBFs for finite horizon problems directly from HJ reachability calculations. Similar to standard HJ reachability problems, the authors discretize the state space and solve the HJ Isaacs equation via dynamic programming. Backup CBFs[11, 12], which are closely related to model predictive safety filters [13] , define an implicit control invariant set by guaranteeing that there always exists a backup trajectory that will lead to a smaller control invariant set. However, since the maximum control invariant set is only defined implicitly, its exact size remains unknown before deployment. Furthermore, the size of the implicit control invariant set heavily depends on the backup strategy which typically includes domain specific knowledge. Figure 1: Illustration of the proposed invariant set expansion applied to the double integrator. Starting from a small invariant set, we apply a virtual expansion control input to each control point and ensure that there exists a control input keeping the state inside the set for all states on the boundary. Here, we vary the number of control points on the boundary to illustrate the conservatism. Alternatively to finding exact control invariant sets, many recent works leverage neural networks (NNs) to approximate CBFs which are often referred to as neural CBFs [14, 15]. Although neural networks can represent arbitrary complex functions, it is important to stress that neural CBFs need to be verified since NNs are trained on a finite set of data points. There are a variety of verification tools such as branch and bound schemes [16] or Satisfiability Modulo Theory (SMT) [17]. In [18], a verification-in-the-loop scheme is proposed to simultaneously synthesize and verify a neural CBF. Closest related to our work are approximated regions of attraction (RoA) using flow CBFs [19]. Given a small RoA, the authors expand a polygon while ensuring that a backup CBF exists for all vertices of the polygon that drives the state back to the smaller RoA. The authors show that they approach the exact RoA as the number of vertices approaches infinity. However, for any finite number, it can happen that a state on the boundary leaves the set as the authors only verify the backup strategy at discrete vertex positions. In this work, we show how to define CBFs in the space of possible boundaries which allow for safe expansion of control invariant sets. Similar ideas have been proposed in our previous work, where CBFs have been applied to alternative domains to robot state spaces such as belief spaces [20, 21] or trajectory spaces [22]. Specifically, we present an approach for finding valid control invariant sets on a continuous domain. Instead of using the zero level set of a continuously differentiable function, we directly parameterize the boundary of an invariant set and ensure that for all points on the boundary, there exists a positive flow into the set. This greatly simplifies the process of checking if a set is invariant since we can check the flow condition at discrete points on the boundary through a Linear Program (LP). To guarantee the flow condition on a continuous domain, i.e. all states on the boundary, we leverage recently proposed Lipschitz constants of LPs [23]. Finally, we propose a safe expansion law that increases the size of a set while ensuring that the set is always control invariant, see Fig. 1."
https://arxiv.org/html/2411.04791v1,A Continuification-Based Control Solution for Large-Scale Shepherding,"In this paper, we address the large-scale shepherding control problem using a continuification-based strategy. We consider a scenario in which a large group of follower agents (targets) must be confined within a designated goal region through indirect interactions with a controllable set of leader agents (herders). Our approach transforms the microscopic agent-based dynamics into a macroscopic continuum model via partial differential equations (PDEs). This formulation enables efficient, scalable control design for the herdersâ€™ behavior, with guarantees of global convergence. Numerical and experimental validations in a mixed-reality swarm robotics framework demonstrate the methodâ€™s effectiveness.","I INTRODUCTION Many new methodologies for controlling large-scale multi-agent systems rely on their macroscopic descriptions, typically represented by partial differential equations (PDEs), capturing a collective behavior of interest. For instance, describing the spatio-temporal dynamics of the density of a swarming group is often more practical than modeling each agent individually, inherently requiring a prohibitively large set of ordinary or stochastic differential equations (ODEs/SDEs) for velocities and accelerations [1, 2, 3, 4, 5]. Hence, macroscopic approaches offer several advantages, including compactness and analytical tractability, making the mathematical formulation scalable and able to address the curse of dimensionality [6]. These macroscopic solutions are typically derived under the assumption of an infinite number of agents, presenting a pressing challenge in assessing their applicability to real-world applications. In this paper, we focus on a large-scale version of the shepherding control problem [7, 8]. Inspired by the shepherding behavior exhibited by sheepdogs, we consider a scenario where a large group of follower agents (targets) must be guided into a desired region through interactions with a population of leader agents (herders) [9]. This control problem has practical applications, such as guiding robotic systems for environmental pollutant containment [10] or directing biological agents (e.g. animal or insects) to safer regions in search and rescue operations [11]. Addressing this control problem using traditional microscopic techniques becomes challenging in large-scale scenarios. In many large-scale applications, including the shepherding control problem, the objective is to influence the systemâ€™s macroscopic behavior, yet control can only be applied at the microscopic agent level [6]. A feasible macroscopic approach to these multi-scale control problems is continuification-based control [3, 1]. This method derives a continuum model from the agent-based dynamics, represented as ODEs/SDEs, and reformulates it into a compact set of PDEs to facilitate control law design at the macroscopic level. The resulting continuum control law is ultimately discretized to generate the microscopic inputs required at the agent level (see Fig. 1 for a schematic). Figure 1: Continuification control pipeline, inspired by [3]. In this work, we leverage the continuification control pipeline and the theoretical foundations established in [2] to address open challenges in large-scale shepherding control. Specifically, we (iğ‘–iitalic_i) determine the minimum number of herders and their required sensing capabilities and (iâ¢iğ‘–ğ‘–iiitalic_i italic_i) guide the targets to a desired goal region, providing analytical guarantees of global convergence in the continuum limit. We validate the proposed strategy experimentally using a mixed-reality platform with 20 differential-drive robots and a configurable number of virtual agents, similar to the set-up presented in [12]. This experimental platform was developed utilizing the facilities provided by the Robotarium [13]."
https://arxiv.org/html/2411.04548v1,Convergence and Robustness of Value and Policy Iteration for the Linear Quadratic Regulator,"This paper revisits and extends the convergence and robustness properties of value and policy iteration algorithms for discrete-time linear quadratic regulator problems. In the model-based case, we extend current results concerning the region of exponential convergence of both algorithms. In the case where there is uncertainty on the value of the system matrices, we provide input-to-state stability results capturing the effect of model parameter uncertainties. Our findings offer new insights into these algorithms at the heart of several approximate dynamic programming schemes, highlighting their convergence and robustness behaviors. Numerical examples illustrate the significance of some of the theoretical results.","Approximate dynamic programming (ADP) [1, 2, 3] is a powerful algorithmic approach designed to solve sequential decision-making problems across a broad range of applications. Two fundamental approaches in ADP are: value iteration (VI) and policy iteration (PI), which have been extensively analyzed in the literature [4, 5]. VI updates the value function of the underlying optimal control problem iteratively [6, 7], while PI evaluates and improves policies sequentially [8]. Convergence properties of these two algorithms have been studied in works such as [9] and [10]. In [9], the convergence rates of VI and PI are compared for a finite state and action Markov decision problem. In contrast, [10] investigates the conditions for asymptotic and exponential convergence of VI and PI in the context of the discrete-time linear quadratic regulator (LQR) problem. ADPâ€™s application to the LQR problem has received substantial attention due to its analytically tractable nature, making it an ideal benchmark for studying ADP in environments with continuous state and action spaces [11, 12]. Studying the performance of VI and PI for the LQR problem is an active area of research [10, 13, 14, 15]. Typically, performing VI or PI requires knowledge of the system model, and this is the setting where most results are available [10, 16, 11]. However, the system model is often unavailable in practical scenarios. To address this challenge, data can be used to either identify a model and based on the estimate apply the algorithm (indirect data-driven control) or directly design the controller (direct data-driven Control). In [17], the PI procedure is combined with online model estimation, while in [18], PI is directly formulated using online data, bypassing the need for model estimation. For both direct and indirect approaches to VI and PI, analyzing the robustness of the algorithms is crucial due to unavoidable uncertainty associated with the use of finite and potentially noisy data, which can introduce estimation errors that affect the controllerâ€™s design. In [19, 20], the robustness of PI applied to continuous-time LQR problems and stochastic LQR problems is analyzed, respectively. Inspired by [19], our previous work [17] extended this analysis to the robustness of PI for discrete-time LQR problems. In this work, we investigate the nominal (i.e. with known model) exponential convergence and robustness of VI and PI applied to the discrete-time LQR problem. For the nominal case, we extend the standard conditions for the exponential convergence of VI and PI algorithms. Building on these results, we then analyze the robustness to model uncertainty of VI and PI in the face of model uncertainties. Specifically, we study the performance of the two algorithms when estimates of the true model are used, and we analyze the effect of uncertainty on the convergence properties. A motivating example for this analysis is the use of online identification routines providing at each iteration of the PI/VI algorithm a different estimate which differs from the true one by a certain amount. We show that both VI and PI algorithms have inherent robustness to uncertainties within specific bounds. This property is crucial for the reliable deployment of indirect VI and PI algorithms, where handling uncertainties and estimation errors should be taken into account. The paper is organized as follows. Section II introduces the problem setting and some preliminaries. Section III and Section IV detail the exponential convergence and robustness analysis for both VI and PI algorithms, respectively. Section V provides simulations to illustrate some theoretical examples. Section VI concludes the work. Notations: We denote by Aâª°0succeeds-or-equalsğ´0A\succeq 0italic_A âª° 0 and Aâ‰»0succeedsğ´0A\succ 0italic_A â‰» 0 a positive semidefinite and positive definite matrix Ağ´Aitalic_A, respectively. For matrices, âˆ¥â‹…âˆ¥Fsubscriptdelimited-âˆ¥âˆ¥â‹…ğ¹\lVert\cdot\rVert_{F}âˆ¥ â‹… âˆ¥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT and âˆ¥â‹…âˆ¥delimited-âˆ¥âˆ¥â‹…\lVert\cdot\rVertâˆ¥ â‹… âˆ¥ denote respectively their Frobenius norm and induced 2222-norm. Given positive definite matrix PÏµâª¯Iprecedes-or-equalssubscriptğ‘ƒitalic-Ïµğ¼P_{\epsilon}\preceq Iitalic_P start_POSTSUBSCRIPT italic_Ïµ end_POSTSUBSCRIPT âª¯ italic_I[10, Lemma 6], we define the induced PÏµsubscriptğ‘ƒitalic-ÏµP_{\epsilon}italic_P start_POSTSUBSCRIPT italic_Ïµ end_POSTSUBSCRIPT-norm for matrices of compatible dimension as: âˆ¥â‹…âˆ¥PÏµ:=Î»mâ¢aâ¢xâ¢((â‹…)âŠ¤â¢PÏµâ¢(â‹…))\|\cdot\|_{P_{\epsilon}}:=\sqrt{\lambda_{max}((\cdot)^{\top}P_{\epsilon}(\cdot% ))}âˆ¥ â‹… âˆ¥ start_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_Ïµ end_POSTSUBSCRIPT end_POSTSUBSCRIPT := square-root start_ARG italic_Î» start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT ( ( â‹… ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_P start_POSTSUBSCRIPT italic_Ïµ end_POSTSUBSCRIPT ( â‹… ) ) end_ARG. For Xâˆˆâ„mÃ—nğ‘‹superscriptâ„ğ‘šğ‘›X\in\mathbb{R}^{m\times n}italic_X âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT, we define vâ¢eâ¢câ¢(X):=[X1âŠ¤,X2âŠ¤,X3âŠ¤,â€¦,XnâŠ¤]âŠ¤assignğ‘£ğ‘’ğ‘ğ‘‹superscriptsuperscriptsubscriptğ‘‹1topsuperscriptsubscriptğ‘‹2topsuperscriptsubscriptğ‘‹3topâ€¦superscriptsubscriptğ‘‹ğ‘›toptopvec(X):=[X_{1}^{\top},X_{2}^{\top},X_{3}^{\top},...,X_{n}^{\top}]^{\top}italic_v italic_e italic_c ( italic_X ) := [ italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT , italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT , italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT , â€¦ , italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT, where Xisubscriptğ‘‹ğ‘–X_{i}italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the iğ‘–iitalic_i-th column of matrix Xğ‘‹Xitalic_X. Kronecker product is represented as âŠ—tensor-product\otimesâŠ—. For Yâˆˆâ„mÃ—nğ‘Œsuperscriptâ„ğ‘šğ‘›Y\in\mathbb{R}^{m\times n}italic_Y âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT and r>0ğ‘Ÿ0r>0italic_r > 0, we define â„¬râ¢(Y):={Xâˆˆâ„mÃ—n|â€–Xâˆ’Yâ€–F<r}assignsubscriptâ„¬ğ‘Ÿğ‘Œconditional-setğ‘‹superscriptâ„ğ‘šğ‘›subscriptnormğ‘‹ğ‘Œğ¹ğ‘Ÿ\mathcal{B}_{r}(Y):=\{X\in\mathbb{R}^{m\times n}|\|X-Y\|_{F}<r\}caligraphic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_Y ) := { italic_X âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT | âˆ¥ italic_X - italic_Y âˆ¥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT < italic_r }. A sequence {Yi}subscriptğ‘Œğ‘–\{Y_{i}\}{ italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } is a map â„¤+â†’â„nÃ—mâ†’subscriptâ„¤superscriptâ„ğ‘›ğ‘š\mathbb{Z}_{+}\rightarrow\mathbb{R}^{n\times m}blackboard_Z start_POSTSUBSCRIPT + end_POSTSUBSCRIPT â†’ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_m end_POSTSUPERSCRIPT. For bounded scalar sequences, we denote by âˆ¥Yâˆ¥âˆ:=supiâˆˆâ„¤+{Yi}assignsubscriptdelimited-âˆ¥âˆ¥ğ‘Œsubscriptsupremumğ‘–subscriptâ„¤subscriptğ‘Œğ‘–\lVert Y\rVert_{\infty}:=\sup\limits_{i\in{\mathbb{Z}_{+}}}\{Y_{i}\}âˆ¥ italic_Y âˆ¥ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT := roman_sup start_POSTSUBSCRIPT italic_i âˆˆ blackboard_Z start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT { italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }."
https://arxiv.org/html/2411.04510v1,Sliding Mode Roll Control of Active Suspension Electric Vehicles,"Vehicle roll control has been a well studied problem. One of the ubiquitous methods to mitigate vehicle rollover in the automobile industry is via a mechanical anti-roll bar. However with the advent of electric vehicles, rollover mitigation can be pursued using electric actuation. In this work, we study a roll control algorithm using sliding mode control for active suspension vehicles, where the actuation for the roll control signal is generated by electric motors independently at the four corners of the vehicle. This technology precludes the need for any mechanical actuation which is often slower as well as any anti-roll bar to mitigate vehicle rollover situations. We provide an implementation of the proposed algorithm and conduct numerical experiments to validate the functionality and effectiveness. Specifically, we perform Slalom and J-turn maneuvering tests on an active suspension electric vehicle with sliding model roll control and it is shown to mitigate rollover by atleast 50%percent\%% compared to passive suspension vehicles, while simultaneously maintaining rider comfort.","The development and integration of active suspension systems for electric vehicles has been the subject of extensive research in the automobile industry. The rationale for better suspensions in automobiles stems from the need to achieve a greater degree of ride quality and electric vehicle handling by keeping the chassis of the electric vehicle parallel to the road when turning corners, and reducing the impact of disturbances from the road surface on the vehicle. Vehicle suspensions can be categorized into three broad categories: passive, semi-active, and active suspensions. Passive suspension systems constitute large springs at the automobile wheels that dampen the vertical movements caused by the road surface on the vehicle body. The parameters of passive suspension, specifically the spring constant and damping coefficient, are fixed to ensure a certain degree of balance between reducing road surface transmitted disturbances and enhancing road holding. The former enhances ride quality while the later ensures dynamic stability of the vehicle. In this paper, we consider a electric vehicle body equipped with active suspensions at each of the four corners, where a motor provides a linear, vertical force. We also consider that pitch, heave, and roll are simultaneously controlled via these vertical forces. This assumption is critical to the optimal distribution of the roll control commands to the electric vehicle corners. However, the focus of this paper would on roll control of the vehicle that is synonymous with vehicle stability. In Section II we provide a detailed review of the roll dynamics model for active suspension vehicles. In Section III we provide the sliding mode roll control algorithm for active suspension electric vehicles and we present the corresponding implementation in Section III-B. In Section IV we establish the validity of the proposed roll control algorithm via MATLAB simulations on an active suspension electric vehicle. We conclude in Section V."
https://arxiv.org/html/2411.04467v1,A Distributionally Robust Control Strategy for Frequency Safety based on Koopman Operator Described System Model,"As the proportion of renewable energy and power electronics in the power system increases, modeling frequency dynamics under power deficits becomes more challenging. Although data-driven methods help mitigate these challenges, they are exposed to data noise and training errors, leading to uncertain prediction errors. To address uncertain and limited statistical information of prediction errors, we introduce a distributionally robust data-enabled emergency frequency control (DREFC) framework. It aims to ensure a high probability of frequency safety and allows for adjustable control conservativeness for decision makers. Specifically, DREFC solves a min-max optimization problem to find the optimal control that is robust to distribution of prediction errors within a Wasserstein-distance-based ambiguity set. With an analytical approximation for VaR constraints, we achieve a computationally efficient reformulations. Simulations demonstrate that DREFC ensures frequency safety, low control costs and low computation time.","I-A Motivation As the proportion of renewable energy increases, the complexity and uncertainty of grid operations will also grow. The variability of operating conditions, uncertainty of disturbances and faults, weak disturbance resistance of power electronic devices, and high flexibility of control make response-driven emergency frequency control increasingly important. Response-driven emergency frequency control typically employs historical data to model system dynamics and forecast future system trajectories. These forecasts serve as constraints, enabling the design of optimization-based control strategies. However, using historical data to model system dynamics may encounter the following issues: 1) Data acquisition phase: issues such as measurement noise, communication delays, and limited data availability and 2) Dynamics representation phase: potential errors such as incorrect assumptions about the library of basis functions and training inaccuracies. These factors lead to unavoidable and highly uncertain prediction errors in data-driven system dynamics representation. Consequently, using these predictions as constraints in optimal control problems may not yield optimal solutions. Addressing the uncertainty in prediction errors is a critical concern. Currently, extensive research has been conducted on addressing uncertainty in power systems. Among these, stochastic optimization (SO) and robust optimization (RO) have been widely applied. While SO is well-established for managing uncertainty, it assumes random variables follow a specific distribution, making it less robust for extreme scenarios. RO seeks optimal decisions under worst-case scenarios by setting bounds on uncertain parameters and is widely applied in power systems. However, ROâ€™s lack of probabilistic information results in conservative outcomes. Consequently, distributionally robust optimization (DRO), which considers the worst-case PD of uncertain parameters, is gaining attention. This paper proposes a new distributionally robust emergency frequency control model, which incorporates Value at Risk (VaR) constraints to ensure a high probability of frequency safety. A data-driven GMM-based ambiguity set, explicit and convex, is used to hedge against the uncertain true distribution of frequency prediction error. An approximate method for the VaR constraint is then proposed, allowing it to be expressed analytically. Unlike DRO based on empirical distribution, the proposed methodâ€™s computational efficiency is not significantly affected by the increase in historical data. I-B Literature Review Modern power systems require emergency frequency control (EFC) strategies to manage significant under-frequency events. The traditional second defense line approach, which involves reliable solutions in advance, ensures system stability during rare, severe faults. However, as renewable energy penetration increases, time-domain simulations struggle to handle the complexity and variability of operating conditions, leading to a higher risk of strategy mismatch. Consequently, data-driven methods are gaining traction in the industry. These methods leverage collected data to capture system dynamics, predict future behaviors, and compute optimal control sequences. Data-driven models offer rapid updates based on system responses, addressing the challenges of varying conditions and overcoming limitations of traditional mechanistic modeling. For example, Ref. [1] proposed to use of a CNN-LSTM-based strategy has improved voltage stability by adaptively evaluating the stability margin. Ref. [2] proposed a coordinated AC/DC control based on real-time power system response data. However, data-driven control continues to face challenges, such as noisy data and training bias, which can introduce inaccuracies in predicting future trajectories and undermine effective system stabilization. A common approach to addressing these challenges is the integration of robust control. For linear systems, Ref. [3] proposed a data-driven robust model predictive control based on the Fundamental Lemma for grid-following converters. Additionally, Ref. [4] theoretically derives the relationship between the min-max formulation of robust control and regularization problems, applying this method to design data-driven control for grid-following converters. This control strategy is further extended to nonlinear systems in Ref. [5]. For nonlinear systems, Ref. [6] introduces a stability margin design for frequency security, based on a system model described by the Koopman operator, which accounts for prediction errors and control deviations in the data-driven model. Ref. [7] proposes a robust model predictive control based on the Koopman operator, considering additive model errors and providing a box uncertainty set representation for data-driven additive error uncertainty. However, robust control can sometimes be overly conservative, increasing control costs. In the context of EFC, such conservatism could lead to significant economic losses (e.g., thousands of MW in load shedding). Therefore, a control strategy that balances safety and efficiency is needed. Distributionally Robust Control (DRC) is employed to achieve this balance. DRC seeks decision outcomes under the worst-case PD of uncertain parameters, ensuring robustness while mitigating conservatism through probabilistic distribution (PD). Ref. [8] offers a comprehensive review of state-of-the-art DRC, including constraint conditions, the selection of ambiguity sets, and common DRO construction methods. Currently, applying distributionally robust control to emergency frequency control is not yet widely adopted. Most existing studies remain theoretical, relying on a physical linear time-invariant (LTI) system model under model predictive control (MPC). For linear systems, Ref. [9, 10] constructed the empirical distribution of additive uncertainties in state-space equations using historical data. These papers define uncertainty sets based on the Wasserstein distance and analyze the analytical representation of uncertainty trajectory ranges after the temporal propagation of these sets. Ref. [11, 12] define uncertainty sets for additive errors in linear systems using second-moment information, considering Value at Risk (VaR) and Conditional Value at Risk (CVaR), respectively, with the latter convertible to convex semidefinite programming. Ref. [13] discusses a class of distributionally robust model predictive controllers (MPC) for nonlinear stochastic processes, while Ref. [14] proposes a distributionally robust reinforcement learning strategy, proving that the worst-case distribution is deterministic and static, thereby simplifying the problem. For nonlinear systems, there are currently no direct distributionally robust control methods. Ref. [15] linearizes the system at the operating point and then applies distributionally robust control methods designed for linear systems. To achieve the desired properties of DRC, Gaussian Mixture Models (GMM), known for their ability to fit complex characteristics of arbitrary PDs, have been widely used to simulate uncertainties in power systems. Several studies have integrated GMM with distributionally robust optimization (DRO) in power system dispatch problems. For instance, Ref. [16] proposed a distributionally robust optimal power flow (OPF) model for transmission grids with wind power generation, where GMM is integrated into CVaR constraints and a data-driven GMM-based ambiguity set to ensure distributional robustness. Ref. [17] developed a DRO model for day-ahead scheduling of distribution systems with electric vehicles (EVs), using GMM to model the uncertainties of EV parameters. Additionally, Kullback-Leibler (KL) divergence is used to design the ambiguity set. These examples show that GMM-based DRO has been extensively studied for long-term power system issues. However, applying it directly to EFC presents challenges. For instance, in Ref. [16], while a scalable cutting plane method is designed to improve DRO computational efficiency, it may still struggle to meet the time requirements of EFC, typically within 500ms. Furthermore, although the KL-divergence technique transforms the original DRO problem into a more computationally tractable one, DRC involves predicting future system behavior by propagating the PD of uncertain parameters through system dynamics. However, the desired propagation properties may not hold when using KL divergence to measure discrepancies between PDs. Therefore, finding a method that ensures both computational efficiency and effective propagation in DREFC remains a challenge. In summary, current DRC methods face common challenges, such as limitations in application (e.g., requiring known linear models, affine or linear control, or light-tailed distributions). Crucially, EFC demands high control timeliness, and ensuring the computational efficiency of DRC remains an open challenge. This paper aims to address and partially resolve these issues. I-C Contribution According to the literature review, to realize DREFC, the contributions of this paper are as follows: 1. A Distributionally Robust Emergency Frequency Control framework (DREFC) is proposed to distributionally robustify the control strategies against prediction errors. This framework involves constructing the distribution of system frequency prediction errors using GMM, adopting a data-driven GMM-based ambiguity set and solving min-max optimization problem with VaR-constraints. 2. We show explicitly how the Min-Max problem can be simplified into tractable minimization problems, enabling real-time implementation of the proposed DREFC framework. 3. The proposed DREFC can be applied to both one-shot load shedding and close-loop DC power reference regulation. For one-shot load shedding, the above framework can be directly applied. For closed-loop DC power reference regulation, we provide a method to recursively update the GMM-based ambiguity set online during the control period. This approach further improves the probability of the ambiguity set containing all distributions of prediction errors, thus ensuring frequency safety. The paper is organized as follows: Section II-A presents a general framework for DREFC incorporating a VaR constraint. Section II-B adopts GMM to discribe the uncertainty and a Wasserstein-type distance to define the ambiguity set, which can be reformulated into a convex and explicit structure. Section II-C derives an explicit and approximate expression for the inverse cumulative distribution function (ICDF) of the GMM. Leveraging the developments in Sections II-B and II-C, Section II-D offers a computationally tractable reformulation of DREFC. In Section III, the reformulated DREFC is applied to load shedding and ancillary DC power reference regulation, with a particular focus on an online update method for the ambiguity set in the latter. Section IV validates the proposed DREFC through a case study on the CIGRE-LF system. Finally, Section V concludes the paper."
https://arxiv.org/html/2411.04274v1,Effective Capacity of a Battery Energy Storage System Captive to a Wind Farm,"Wind energyâ€™s role in the global electric grid is set to expand significantly. New York State alone anticipates offshore wind farms (WFs) contributing 9GW by 2035. Integration of energy storage emerges as crucial for this advancement. In this study, we focus on a WF paired with a captive battery energy storage system (BESS). We aim to ascertain the capacity credit for a BESS with specified energy and power ratings. Unlike prior methods rooted in reliability theory, we define a power alignment function, which leads to a straightforward definition of capacity and incremental capacity for the BESS. We develop a solution method based on a linear programming formulation. Our analysis utilizes wind data, collected by NYSERDA off Long Islandâ€™s coast and load demand data from NYISO. Additionally, we present theoretical insights into BESS sizing and a key time-series property influencing BESS capacity, aiding in simulating wind and demand for estimating BESS energy requirements.","The assessment of a power sourceâ€™s capacity credit has traditionally been rooted in thermal power and relies on probabilistic methods, typically involving the calculation of metrics like the Loss of Load Probability (LOLP) and related quantities such as the effective load carrying capability (ELCC). Over time, these methodologies have been adapted to account for renewable resources such as wind farms (WFs) and solar-PV [1, 2], as well as energy-limited sources like battery energy storage systems (BESSs) [3, 4]. The challenge of sizing BESSs to mitigate the variability of power sources or to provide peaking capacity has garnered significant attention in recent research. A multi-objective optimization model which considers economic and reliability aspects to determine optimal sizing of BESSs is proposed in [5]. A stochastic optimization framework for sizing BESSs which considers uncertainties in renewable energy generation is developed in [6]. Studies such as [7, 8] have evaluated the effectiveness of BESSs in smoothing renewable energy output fluctuations and reducing the need for conventional generation reserves. The deployment of BESSs for providing peaking capacity has also been a subject of interest. In [9, 10] the role of BESSs in meeting peak demand and reducing the reliance on expensive peaking power plants is considered. These studies demonstrate the potential of BESSs to provide flexible and reliable peaking capacity, particularly when coupled with renewable energy sources. The complexity of computing the effective capacity motivated the simplified mathematical model presented in [11] and approximations in [12], notably the peak demand reduction credit. Our work is in the spirit of [11, 12]â€”to present and work with a simple analytic model. Specifically, our contribution is the definition of the power alignment function and its computation, in showing that effective capacity and incremental capacity values can be derived from it, and in obtaining theoretical results and statistical insights into wind and demand modeling. The key role of a BESS is in time-shifting or aligning the power availability to demand. Since a BESS does not generate power, it must be coupled with a power source to assess its capacity credit. In this context, we aim to define the capacity credit that a BESS should receive when it is tied in to a time-varying power source such as a wind farm. This is referred to as a captive BESS for that wind farm. Our approach leads to a natural definition of power capacity and incremental power capacity for a captive BESS, based on a time series methodology rather than traditional LOLP-based methods. The paper is organized as follows. Section II defines the power alignment function, which is fundamental to capacity estimation. Section III describes the physical setup, the governing equations and the method for computing the power alignment function and the capacity. Numerical capacity estimates based on NYSERDA wind speed measurements and NYISO demand data are in Sec. IV. Theoretical results are sketched in Sec. V. The paper is summarized and conclusions are presented in Sec. VI. Some proofs are presented in the Appendix."
https://arxiv.org/html/2411.04202v1,Observability and Generalized Sensor Placement for Nonlinear Quality Models in Drinking Water Networks,"This paper studies the problem of optimal geographic placement of water quality (WQ) sensors in drinking water distribution networks (WDNs), with a specific focus on chlorine transport, decay, and reaction models. Such models are traditionally used as suitable proxies for WQ. The literature on this topic is indeed inveterate, but has a key limitation: it utilizes simplified single-species decay and reaction models that do not capture WQ transients for nonlinear, multi-species interactions. This results in sensor placements that do not account for nonlinear WQ dynamics. Furthermore, and as WQ simulations are parameterized by hydraulic profiles and demand patterns, the placement of sensors are often hydraulics-dependent. This study produces a simple algorithm that addresses the two aforementioned limitations. The presented algorithm is grounded in nonlinear dynamic system sciences and observability theory, and yields sensor placements that are robust to hydraulic changes. Thorough case studies on benchmark water networks are provided. The key findings provide practical recommendations for WDN operators.","\lettrine [lines=2]Sensor placement in complex infrastructure is a fundamental engineering problems. The deployment of sensors within networked systems, such as power grids, water distribution networks (WDNs), and transportation systems, enables the monitoring and safe operation of these engineered systems. From this perspective, the real-time monitoring of WDNs allows water quality (WQ) controllers to trace the evolution of disinfectants and contaminants within the network. Its aim is to regulate WQ and apply feedback control while maintaining pathogen-free water. However, to achieve effective monitoring of WQ within WDNs, the dynamical system representing the WQ model must be observable. In the context of such a dynamical system and WQ control, observability refers to the ability of water system operators to monitor the levels of contaminants and chlorine. This, in turn, allows operators to plan WQ profiles for different but typical hydraulic settings and possible, though less typical, contaminant intrusion conditions within the network. Furthermore, the optimal placement of WQ sensors within a WDN allows achieving such an observable system. However, the cost of sensors and their installation requires optimal placement at specific network locations [1, 2]. This optimal placement of WQ sensors is considered a complex binary selection problem that becomes challenging for mid- to -large networks, particularly WQ networks with fluctuating hydraulic settings due to varying water demand patterns. The optimal sensor configuration set should enable the estimation of most WQ state variables. However, it is typically assumed that observability conditions are satisfied when considering the state estimation of WQ dynamics [3]. An optimal SP framework thereby identifies the sensor configuration required to fully characterize the WQ states in the WDN. With that in mind, this paper aims to provide a generalized robust sensor configuration that allows the accurate depiction and estimation of multi-species WQ dynamics within WDNs under different operational and hydraulic conditions. The variability of WDN input parameters, such as nodal demands, pipe roughness coefficients, and WQ parameters, raises several challenges that must be addressed by the chosen sensor configuration [4]. This necessitates a framework that accounts for this variability. The literature approaches solving the optimal geographic placement of WQ sensors using objective-based methods that assign either public health metrics related to contamination events [3] or, more recently, network-wide observability metrics [5] to determine the optimal sensor configuration. Comprehensive surveys on different SP optimization frameworks are presented in [6, 7]; herein, we provide a brief review of the more recent literature. Relevant Literature. A real-time contaminant classification method is introduced in [8]; it is based on qualitatively analyzing data from multiple types of conventional WQ sensors. A heuristic algorithm for solving the SP problem when sensors are not reliable is proposed in [9]. The framework utilizes a maximum set covering model with weighted edges, where reliability is defined as the probability that a sensor measurement might not be available. A dual pressure and WQ sensor selection problem is solved by considering leakage and contamination detection in WDNs [10]. The proposed method relies on a fuzzy C-means algorithm to classify the junctions via a sensitivity analysis matrix. Reference [11] introduced a new perspective that consider the placement of WQ sensors on pipes and nodes of the WDN. Reference [12] proposes a generalized decision support framework to optimize sensor configuration for effective leakage control in sensor deficient WDNs. This framework uses network sensitivity analysis and multi-criteria decision-making to evaluate pressure response to leaks across various hydraulic scenarios, maximizing spatial efficiency and leak detection accuracy. The importance of considering the variability in the demand and supply schedule when making sensor placement decisions is demonstrated in [13]. A comprehensive framework that further evaluates the robustness of chosen sensor configurations is developed in [14] using an evolutionary-based optimization approach. The study shows that demand and supply have a significant influence on the robustness of the SP. A dynamic graph-theoretic approach to SP in WQ networks for detecting water contamination in near real-time is proposed in [15]. A multi-objective function (volume of contamination and time of contamination detection) is solved using an evolutionary algorithm that adapts sensor locations based on the suspected contamination source. A graph-theoretic approach to solve the optimal SP problem while maximizing the resilience of a given WQ sensor configuration in WDNs against failures is developed in [16]. Graph centrality measures are employed in [17] to identify network coverage and time to detection, concepts related to system observability but from a network graph theory perspective. A weighted SP optimization problem based on a discrete-time dynamical systems approach is developed in [18], considering sensor cost, information richness, and reliability. The method is applied to wastewater networks and offers observability measures for evaluating strategies related to average corrosion degree and maintenance cost. From such perspective, it is illustrated that an allocated configuration of sensors enables the detection and response to water quality issues [19]. However, the limited availability of sensors impedes the monitoring and management of WDNs. The concept of observability of a WDN, required prior to the application of estimation methods, is introduced in [20]. An unmeasured state variable needs to be reliably estimated using available measurements; this is an essential step for the control of a WDN [21]. The impact of a chosen sensor configuration on the quality of WQ dynamics state estimation is explored in [22]. The study illustrates that WQ dynamics can be estimated from a limited sensor configuration; however, the location of such sensors affects the accuracy of the state estimates. The SP problem, which considers minimizing WQ state estimation errors under time-varying WQ dynamics, is studied in [5]. This was the first attempt to adopt a control-theoretic approach that takes into account single-species WQ observability in WDNs while solving the SP problem using a submodular set maximization approach. Typically, sensor selection problems posed as a mixed-integer programming (MIP) formulation, as presented in [23] for WDNs, are solved using global solvers. Submodular set optimization algorithms for solving the SP problem in WDNs were first introduced in [24], providing an efficient approach toward optimizing sensor configurations against contaminant intrusions in WDNs. The aforementioned methods typical adopt the use of simplified single-species decay and reaction models. As such, the resulting sensor configurations are not robust to changes in the networkâ€™s advanced WQ models that capture far more than chlorine decay. The evolution of chlorine residuals in WDNs is typically modeled with single-species decay and reaction dynamics that focus on only spatiotemporal chlorine concentrations [25]. To that end, in this work, we introduce a robust observability-based multi-species WQ sensor placement framework. The underlying WQ model is based on a multi-species reaction dynamics representation; it enables contaminant reactivity modeling. Furthermore, we leverage the submodular properties of the constructed nonlinear observability measures to solve for the optimal WQ sensor configuration problem. Under such formulation, greedy algorithms are employed to solve the combinatorial set optimization problem. Accordingly, as compared to the approach developed in [5], the proposed SP framework offers the following: (i) a generalized robust solution to fluctuations in water demand patterns and varying WQ conditions; (ii) a scalable algorithm that enables its applicability to large-scale networks, resulting from the submodularity of the proposed WQ sensor placement problem; and (iii) a dynamical model that considers the nonlinear multi-species WQ dynamics, depicting chlorine and reactant concentrations within WDNs. Paper Contributions. The paperâ€™s contributions follow. â€¢ We introduce a control-theoretic approach for quantifying the observability of the nonlinear multi-species WQ dynamics in WDNs. The observability framework is based on a moving horizon formulation that accounts for varying WQ dynamics, depicting the evolution of spatiotemporal multi-species dynamics. The proposed nonlinear observability metric is used to quantify the observability of WQ states and is related to the estimation of WQ concentrations resulting from a given sensor configuration. â€¢ We provide robust observability measures that ensure the submodularity and modularity of the sensor selection objective function are retained. In particular, we leverage the modularity and submodularity constructed observability measures to perform the SP. Based on such measures we attain an optimal sensor configuration that is robust towards varying demand patterns and WQ conditions/parameters. Furthermore, under this formulation, greedy algorithms are employed to solve the combinatorial set optimization problem, rendering the SP problem scalable for larger networks. â€¢ A comprehensive case study is provided on benchmark water networks with different scales of water networks, number of allocated sensors, water demand profiles, and WQ conditions. The sensor placement framework is solved by considering several system observability measures. This offers different observations that WDN system operators can consider, such as varying hydraulic profiles arising from daily water demand patterns, the interactions of different WQ reactants and contamination intrusion. Notation. Let â„•â„•\displaystyle\mathbb{N}blackboard_N, â„â„\displaystyle\mathbb{R}blackboard_R, â„nsuperscriptâ„ğ‘›\displaystyle\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and â„pÃ—qsuperscriptâ„ğ‘ğ‘\displaystyle\mathbb{R}^{p\times q}blackboard_R start_POSTSUPERSCRIPT italic_p Ã— italic_q end_POSTSUPERSCRIPT denote the set of natural numbers, real numbers, real-valued row vectors with size of nğ‘›\displaystyle nitalic_n, and pğ‘\displaystyle pitalic_p-by-qğ‘\displaystyle qitalic_q real matrices respectively. The cardinality of a set ğ’©ğ’©\displaystyle\mathcal{N}caligraphic_N is denoted by |ğ’©|ğ’©\displaystyle|\mathcal{N}|| caligraphic_N |. The symbol âŠ—tensor-product\displaystyle\otimesâŠ— denotes the Kronecker product. The operators detâ¢(ğ‘¨)detğ‘¨\displaystyle\mathrm{det}(\bm{A})roman_det ( bold_italic_A ) returns the determinant of matrix ğ‘¨ğ‘¨\displaystyle\bm{A}bold_italic_A, traceâ¢(ğ‘¨)traceğ‘¨\displaystyle\mathrm{trace}(\bm{A})roman_trace ( bold_italic_A ) returns the trace of matrix ğ‘¨ğ‘¨\displaystyle\bm{A}bold_italic_A. The operator {ğ±i}i=0Nâˆˆâ„Nâ¢nsuperscriptsubscriptsubscriptğ±ğ‘–ğ‘–0ğ‘superscriptâ„ğ‘ğ‘›\displaystyle\{\bm{\mathrm{x}}_{i}\}_{i=0}^{N}\in\mathbb{R}^{Nn}{ bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_N italic_n end_POSTSUPERSCRIPT constructs a column vector that concatenates vectors ğ±iâˆˆâ„nsubscriptğ±ğ‘–superscriptâ„ğ‘›\displaystyle\bm{\mathrm{x}}_{i}\in\mathbb{R}^{n}bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT for all iâˆˆ{0,1,â€¦,N}ğ‘–01â€¦ğ‘\displaystyle i\in\{0,1,\ldots,N\}italic_i âˆˆ { 0 , 1 , â€¦ , italic_N }. The operator diagâ¢{Î³j}j=1nâˆˆâ„nÃ—ndiagsuperscriptsubscriptsubscriptğ›¾ğ‘—ğ‘—1ğ‘›superscriptâ„ğ‘›ğ‘›\displaystyle\mathrm{diag}{\{\gamma_{j}\}}_{{j}=1}^{n}\in\mathbb{R}^{n\times n}roman_diag { italic_Î³ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT constructs a diagonal matrix of vector Î³âˆˆâ„nÃ—nğ›¾superscriptâ„ğ‘›ğ‘›\displaystyle\gamma\in\mathbb{R}^{n\times n}italic_Î³ âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT. For any two matrices ğ‘¨ğ‘¨\displaystyle\bm{A}bold_italic_A and ğ‘©âˆˆâ„nÃ—nğ‘©superscriptâ„ğ‘›ğ‘›\displaystyle\bm{B}\in\mathbb{R}^{n\times n}bold_italic_B âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT, the notation {ğ‘¨,ğ‘©}ğ‘¨ğ‘©\displaystyle\{\bm{A},\bm{B}\}{ bold_italic_A , bold_italic_B } represents [ğ‘¨âŠ¤â¢ğ‘©âŠ¤]âŠ¤superscriptdelimited-[]superscriptğ‘¨topsuperscriptğ‘©toptop\displaystyle\left[\bm{A}^{\top}\bm{B}^{\top}\right]^{\top}[ bold_italic_A start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT bold_italic_B start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT. Paper Organization. The paper is organized as follows: Section II introduces the water quality dynamics model. Section III provides preliminaries on observability quantification and introduces the proposed WQ observability measures. In Section IV, we introduce the proposed optimal SP framework. The numerical case studies are presented in Section V, and Section VI provides a conclusion to the paper while addressing the paperâ€™s limitations and future work."
https://arxiv.org/html/2411.04809v1,Minimax Linear Regulator Problems forPositive Systems,"Exceptional are the instances where explicit solutions to optimal control problems are obtainable. Of particular interest are the explicit solutions derived for minimax problems, which provide a framework for tackling challenges characterized by adversarial conditions and uncertainties. This work builds on previous discrete-time research [1], extending it to a multi-disturbance minimax linear framework for linear time-invariant systems in continuous time. Disturbances are considered to be bounded by elementwise linear constraints, along with unconstrained positive disturbances. Dynamic programming theory is applied to derive explicit solutions to the Hamilton-Jacobi-Bellman (HJB) equation for both finite and infinite horizons. For the infinite horizon a fixed-point method is proposed to compute the solution of the HJB equation. Moreover, the Linear Regulator (LR) problem is introduced, which, analogous to the Linear-Quadratic Regulator (LQR) problem, can be utilized for the stabilization of positive systems. A linear program formulation for the LR problem is proposed which computes the associated stabilizing controller, it it exists. Additionally necessary and sufficient conditions for minimizing the l1subscriptğ‘™1l_{1}italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-induced gain of the system are derived and characterized through the disturbance penalty of the cost function of the minimax problem class. We motivate the prospective scalability properties of our framework with a large-scale water management network.","1.1 Motivation Minimax optimal control problems are widespread across control theory and engineering disciplines. They offer a methodology for formulating and addressing challenges characterized by competitive elements and uncertainties. Such problems are prevalent in areas including robust control, multi-agent systems and game theory, where it finds direct application in Hâˆsubscriptğ»H_{\infty}italic_H start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT robust control [2, 3, 4, 5, 6, 7]. Tackling these problems presents significant difficulties, particularly when we deal with large-scale systems due to the computational complexity and dimensionality of the associated optimization problems. A generic nonnegative linear cost minimax setting is studied to design control systems that are robust to worst-case uncertainties and disturbances. The study of optimal control problems with nonnegative cost dates back to [8, 9, 10]. In particular, this work draws inspiration from the unified dynamic programming framework developed by Prof. D. Bertsekas [11, 12]. Despite significant advancements in the theory of dynamic programming, obtaining exact solutions often remains challenging. A notable exception is the linear quadratic problem, as presented in the pioneering work of Kalman [13]. In both quadratic and linear nonnegative cost settings, the optimal cost is determined by an algebraic equation. For large-scale systems, when we choose the linear quadratic approach, the algebraic equation is the Riccati equation, and the number of unknown parameters grows quadratically with the state dimension. However, in the linear regulator setting, the algebraic equation is given in [14] and the unknown parameters growth rate is linear with respect to the dimension of the state. This difference becomes particularly significant when the state dimension becomes large. Inspired by the novel class of optimal control problems involving nonnegative linear costs [14], recent work in discrete time [1] presents an explicit solution for a class of minimax optimal control problems with linear objective functions, positive linear systems, and homogeneous constraints on control and disturbances. In the current paper, these results are extended to a minimax setting in continuous time with multiple disturbances. Two types of disturbances are considered: bounded by elementwise linear constraints, and unconstrained nonnegative disturbances. Recent work on compartmental models in continuous time for bilinear systems [15] focuses on optimizing a class of time-varying controllers using a shooting method for the finite-horizon case and an iterative scheme for the infinite-horizon, and characterizes the l1subscriptğ‘™1l_{1}italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-induced gain of the system in the presence of a positive disturbance. This paper introduces significant advancements to the minimization framework by an extension to minimax problems, and presents a linear program for solving the Hamilton Jacobi Bellman (HJB) equation of the infinite horizon problem setting. The linear program and its dual are studied, and stabilizability and detectability properties are formulated to establish the equivalence to the HJB approach. The contributions of this manuscript are as follows: (1) Necessary conditions on the parameters of the minimax control problem are formulated, for which, if the optimal solution is finite, the closed-loop system is a positive system (Theorem 1, 2) and the cost is minimized. (2) Dynamic programming is applied without imposing predefined constraints on linearity or sparsity; instead, these properties arise naturally from the optimization criteria and constraints. This approach differs from methods in which the control design explicitly dictates specific structures. (3) In both finite and infinite horizon settings, the minimization and the two distinct maximization problems are decoupled. This property enables the explicit solution of the HJB equation to be extended to the minimax framework. The HJB equation takes the form of an ordinary differential equation (ODE) in the finite horizon setting, whereas it reduces to an algebraic equation in the infinite horizon case. (4) A fixed-point method for computing the solution to the algebraic HJB equation in the presence of elementwise bounded disturbances (Theorem 3). (5) A linear program is presented which, under pertinent assumptions, finds the optimal solution to the Bellman equation of the Linear Regulator (LR) problem. An analysis of the primal and dual linear program are provided. Necessary conditions are presented for which the LR problem stabilizes the system (Section 5). (6) Analysis of the l1subscriptğ‘™1l_{1}italic_l start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-induced gain of the system and its relationship to the disturbance penalty in the cost function of the minimax setting is given (Section 6). 1.2 Notation Let â„+subscriptâ„\mathbb{R}_{+}roman_â„ start_POSTSUBSCRIPT + end_POSTSUBSCRIPT denote the set of nonnegative real numbers, â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}roman_â„ start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT the nğ‘›nitalic_n dimensional Euclidean space and â„+nsubscriptsuperscriptâ„ğ‘›\mathbb{R}^{n}_{+}roman_â„ start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT + end_POSTSUBSCRIPT the positive orthant. â„mÃ—nsuperscriptâ„ğ‘šğ‘›\mathbb{R}^{m\times n}roman_â„ start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT denote the set of mğ‘šmitalic_m by nğ‘›nitalic_n matrices. Any vector is, by default, a column vector. Row vectors are specified explicitly. We use 00 to denote the scalar zero or the vector/matrix with all zero elements and ğŸ™double-struck-ğŸ™\mathbb{1}blackboard_ğŸ™ to denote a column vector with unit entries in â„â„\mathbb{R}roman_â„ of appropriate dimensions. For a real matrix Xğ‘‹Xitalic_X we use the notation |X|ğ‘‹\left|X\right|| italic_X | to denote the matrix obtained by replacing the elements of Xğ‘‹Xitalic_X with their absolute values. A real matrix Xğ‘‹Xitalic_X is called nonnegative Xâ‰¥0ğ‘‹0X\geq 0italic_X â‰¥ 0 (respectively nonpositive Xâ‰¤0)X\leq 0)italic_X â‰¤ 0 ) if all the elements of Xğ‘‹Xitalic_X are nonnegative (nonpositive). If all the elements of Xğ‘‹Xitalic_X are strictly positive (strictly negative), we call Xğ‘‹Xitalic_X a positive X>0ğ‘‹0X>0italic_X > 0 (negative X<0ğ‘‹0X<0italic_X < 0) matrix. For real matrices X,Yğ‘‹ğ‘ŒX,Yitalic_X , italic_Y, the inequality Xâ‰¥Yğ‘‹ğ‘ŒX\geq Yitalic_X â‰¥ italic_Y (Xâ‰¤Y)ğ‘‹ğ‘Œ(X\leq Y)( italic_X â‰¤ italic_Y ) means that all the elements of the matrix Xâˆ’Yğ‘‹ğ‘ŒX-Yitalic_X - italic_Y are nonnegative (nonpositive). Because column and row vectors are special forms of matrices, the function |â‹…|\left|\cdot\right|| â‹… | and relations â‰¥\geqâ‰¥, â‰¤\leqâ‰¤, >>>, <<< apply to them. The signature of a scalar is defined as the set-valued map signâ¡(x)={{+1}if â¢x>0{âˆ’1}if â¢x<0[âˆ’1,+1]if â¢x=0signğ‘¥cases1if ğ‘¥01if ğ‘¥011if ğ‘¥0\displaystyle\operatorname{sign}(x)=\scalebox{0.9}{$\begin{cases}\{+1\}&\text{% if }x>0\\ \{-1\}&\text{if }x<0\\ [-1,+1]&\text{if }x=0\end{cases}$}roman_sign ( italic_x ) = { start_ROW start_CELL { + 1 } end_CELL start_CELL if italic_x > 0 end_CELL end_ROW start_ROW start_CELL { - 1 } end_CELL start_CELL if italic_x < 0 end_CELL end_ROW start_ROW start_CELL [ - 1 , + 1 ] end_CELL start_CELL if italic_x = 0 end_CELL end_ROW and is extended to vectors in an element-wise fashion."
https://arxiv.org/html/2411.04676v1,A Comparative Study of Distributed Feedback Optimizing Control Architectures,"This paper considers the problem of steady-state real-time optimization (RTO) of interconnected systems with a common constraint that couples several units, for example, a shared resource. Such problems are often studied under the context of distributed optimization, where decisions are made locally in each subsystem, and are coordinated to optimize the overall performance. Here, we use distributed feedback-optimizing control framework, where the local systems and the coordinator problems are converted into feedback control problems. This is a powerful scheme that allows us to design feedback control loops, and estimate parameters locally, as well as provide local fast response, allowing different closed-loop time constants for each local subsystem. This paper provides a comparative study of different distributed feedback optimizing control architectures using two case studies. The first case study considers the problem of demand response in a residential energy hub powered by a common renewable energy source, and compares the different feedback optimizing control approaches using simulations. The second case study experimentally validates and compares the different approaches using a lab-scale experimental rig that emulates a subsea oil production network, where the common resource is the gas lift that must be optimally allocated among the wells.","Feedback-optimizing control, a class of optimization methods originally introduced in [1], is a powerful technique that transforms the economic optimization problem into a feedback control problem. This method has garnered significant research attention and widespread application across various fields, including process control and autonomous systems [2, 3]. The key reason for its popularity lies in its ability to maintain (near-)optimal system performance by promptly (on fast timescales) responding to changing conditions and uncertainties using simple feedback tools such as Proportional Integral Derivative (PID) controllers and selectors. Often the optimal operation corresponds to case when some of the constraints are active. In such cases, optimal operation can be achieved by active constraint control, where some of the available degrees of freedom can be used to control the constraint to its limiting value using feedback controllers. The unconstrained degrees of freedom and the changing set of active constraints requires special treatment, whereby one has to select controlled variables, known as self-optimizing variables, which when kept at constant setpoint, leads to (near-)optimal operation at steady-state [4]. The ideal self-optimizing variable is in fact the steady-state cost gradient with respect to the unconstrained degrees of freedom, which when controlled to a constant setpoint of zero, satisfies the necessary condition of optimality [5]. The idea of estimating the steady-state cost gradient and driving it to zero using feedback control is also the backbone of several different approaches, including extremum seeking control [6], NCO tracking control [7], Neighboring extremal control [8] to name a few. The main difference in these approaches lies in how the steady-state cost gradient is estimated. See for example [2] for a comprehensive overview of model-based and model-free steady-state gradient estimation schemes that can be used in the context of feedback optimizing control. The works mentioned above primarily concentrated on bridging the optimization problem and the control problem by identifying and estimating the optimal self-optimizing controlled variables. However, when dealing with real-world processes, many of them are large-scale in nature with several interconnected subsystems, posing practical challenges in constructing feedback-optimizing control structures at such a scale. Individual user preferences, autonomy, and privacy also necessitate the need for local decision-making within each unit without the need to share local information in the form of models, real-time measurements, operational schedules etc. Hence, it becomes imperative to integrate decomposition methods [9, 10] into the framework of feedback-optimizing control. In this context, feedback optimizing controllers are designed locally for each susbsytem, which are coordinated to achieve optimal performance of the overall system. We refer to this class of problems as distributed feedback-optimizing control, wherein both the control and estimation techniques can be decomposed and designed independently within each subsystem. In the distributed optimization literature, there are two main strategies that can be used to decompose a large-scale problem into several smaller subproblems, namely, into: â€¢ Primal decomposition - which is an opportunity cost-based framework. In this framework, the master coordinator directly allocates the shared resource to subsystems, which report their incurred opportunity costs (also known as marginal costs) associated with the allocated resource. Subsequently, the allocation of the shared resource is adjusted to ensure equal opportunity cost across all subsystems. â€¢ Dual decomposition - which is a price-based coordination framework. In a price-based coordination framework, subsystems interact through a market-like mechanism where prices of the shared resources are adjusted by a central coordinator to achieve market equilibrium. Subsystems then adjust their resource usage based on these prices. In general, the distributed feedback-optimizing control scheme holds several crucial benefits for interconnected process control systems (in order of the importance): 1. This scheme enables formulating each local problem independently (including independent local estimator), assuming that the overall objective consists of local objectives that are additively separable. 2. Fast local responses are achievable because one may use different closed-loop time constants for each local system. 3. Solutions can be implemented immediately after each iteration, eliminating the need to wait for convergence over several iterations. 4. This scheme circumvents slow response and numerical robustness issues like divergence, often encountered in Real-Time Optimization (RTO) methods that solve a numerical optimization problem online. 5. The distributed feedback-optimizing control scheme serves as an alternative to numerical-based RTO approaches, which may be computationally expensive. Recently, we proposed distributed feedback optimizing control approaches based on the two decomposition frameworks listed above to design control architectures to achieve optimal steady-state operation of the overall system using only feedback controllers in each subsystem [11, 12, 13, 14, 15]. While these methods have been studied and developed independently, a comprehensive comparative study analyzing the advantages and disadvantages of the different distributed feedback optimizing control architectures is notably lacking. Main contribution This paper builds upon our previous works [11, 12, 13, 14, 15] to conduct a comparative analysis of three distributed feedback optimizing control architectures. We leverage two case studies, one experimental and one simulation-based, to provide a comparative study of the three distributed feedback optimizing control architectures. Our key contributions are as follows. â€¢ Our previous work [13] experimentally validated the feedback optimizing control structure based on dual decomposition on a lab-scale gas-lifted oil well rig. One of the contributions of this paper is the experimental validation of the control architectures resulting from primal decomposition and dual decomposition with override, which has not been done before. â€¢ We then compare the three schemes of distributed feedback-optimizing control using the experimental test facility. â€¢ This paper also considers a different case study that aims to achieve demand response within a residential energy hub powered by renewable energy to further demonstrate and compare the different distributed feedback optimizing control architectures. The use of distributed feedback optimizing control for demand response management has not been studied before. The reminder of the paper is organized as follows. Section II lays the groundwork by describing the three distributed feedback optimizing control architectures in detail. Section III present a comparative study of the three control architectures using simulations on a demand response case study. Section IV presents a comparative study of the three control architectures using experiments conducted in a lab-scale test facility that emulates a subsea oil production network. Section V provides a comprehensive comparison of the three control architectures across various aspects, drawing insights from both the simulation and experimental results, before concluding the paper in Section VI."
https://arxiv.org/html/2411.04675v1,"Advancing Multi-Connectivity in Satellite-Terrestrial Integrated Networks: Architectures, Challenges, and Applications","Multi-connectivity (MC) in satellite-terrestrial integrated networks (STINs), included in 3GPP standards, is regarded as a promising technology for future networks. The significant advantages of MC in improving coverage, communication, and sensing through satellite-terrestrial collaboration have sparked widespread interest. In this article, we first introduce three fundamental deployment architectures of MC systems in STINs, including multi-satellite, single-satellite single-base-station, and multi-satellite multi-base-station configurations. Considering the emerging but still evolving satellite networking, we explore system design challenges such as satellite networking schemes, e.g., cell-free and multi-tier satellite networks. Then, key technical challenges that severely influence the quality of mutual communications, including beamforming, channel estimation, and synchronization, are discussed subsequently. Furthermore, typical applications such as coverage enhancement, traffic offloading, collaborative sensing, and low-altitude communication are demonstrated, followed by a case study comparing coverage performance in MC and single-connectivity (SC) configurations. Several essential future research directions for MC in STINs are presented to facilitate further exploration.","Fueled by the increasing demand for high-quality seamless connectivity and the rapid proliferation of data-intensive applications such as video streaming, cloud services, and the Internet of Things (IoT), the evolution of network architectures continues to advance inexorably. Conventional terrestrial networks (TNs) can often be restricted by insufficient infrastructure, particularly in remote and underserved areas. This gives rise to the emergence of multi-connectivity (MC), a solution that enables a devicesâ€™ simultaneous connections to multiple networks to enhance communication robustness, performance, and resilience [1]. Recently, the next generation of wireless communication systems, that is, the sixth generation (6G), has embarked on the extension of the capabilities and scenarios of the fifth generation (5G) such as network coverage, traffic offloading, integrated sensing and communication, low-altitude economy, etc., to global IoT. By allowing devices to connect to multiple network nodes concurrently, increased coverage, higher data throughput, and improved reliability can be ensured for connections between transmitters and user equipments (UEs). To satisfy this growing demand in wider areas, it is essential to understand the developments in potential architectures, technical challenges, and potential real-world applications of MC from a system-level perspective. In recent years, satellite communication (SatCom) is envisioned to be a perfect supplement for TNs in the context of its extensive coverage and efficient multi-cast and broadcast capabilities. However, challenges such as weather sensitivity, the high cost of deployment and maintenance, and deep fading at higher frequencies make it difficult for SatCom systems to independently support global services. Considering the non-negligible role of TNs, satellite-terrestrial integrated networks (STINs) have attracted the attention of standardization institutes, network operators, research fraternities, and related industries [2]. By reaping the benefits of the wide coverage provided by satellites and the high data rates of both networks, STINs act as a prospective paradigm for future networks in providing global coverage and ubiquitous wireless access. Note that if certain satellites or base stations (BSs) become overloaded due to a large number of UEs, their resources are strained, while other satellites or BSs may remain underutilized. This imbalance leads to inefficient use of available network resources. One promising solution to tackle such a problem is to redistribute the load by incorporating MC in STINs to switch or add UEs to less congested satellites and/or BSs. In this case, while no additional bandwidth is provided, UEs experience greater flexibility in connecting to multiple serving entities in the TN and/or non-terrestrial network (NTN). Obviously, by exploiting MC in STINs, the excess available resources could be utilized more efficiently. Moreover, the interoperability of satellites and TNs can support seamless handover and reduce the probability of handover failure, which will improve the reliability of the connection. In addition, the information interaction among satellites and BSs can be made more efficient so that more dynamic and adaptive space-air-ground resource coordination and optimization can be realized. It is worth mentioning that MC in terrestrial mobile networks was once studied in terms of protocols, data and control in the transport layer and the network layer [3]. The authors in [4] also presented the concept of MC in NTNs, but did not take into account the inherent TNs. An overview of the activities and goals of an ongoing STIN project was provided in [5], without leveraging the advantages offered by MC. It is worth mentioning that none of the above works considered the capabilities and application scenarios in future generation networks. Therefore, in this article we seek to address this gap in existing research. Although MC in STINs is expected to offer potential benefits, several questions remain to be answered. â€¢ Q1: What will be the basic architectures for deployment when the MC is integrated into current networks? â€¢ Q2: What will be the challenges to the system design of MC in STINs from a system-level perspective? â€¢ Q3: What will be the real-world applications that MC will have in STINs for future communication systems? This article first provides an overview of MC in STINs in Fig. 1. The main contributions are summarized as follows. FIGURE 1: An overview of MC in STINs. â€¢ We present three deployment architectures for MC when integrating satellite networks into current TNs. Importantly, we provide a comprehensive introduction to these architectures, detailing their characteristics and differences in user pairing and scheduling in each architecture. â€¢ We investigate critical challenges in designing the MC system in STINs from a wireless networking perspective. Specifically, we elaborate on the challenges in the establishment of satellite networks, the use of transmission strategies, the application of channel estimation techniques, and the execution of synchronization operations. â€¢ We propose four typical applications of MC in current and future networks, including coverage enhancement, traffic offloading, collaborative sensing, and low-altitude communication. Then, a case study is conducted to show the improvement in coverage when applying MC compared to single-connectivity networks. Finally, the article concludes with a discussion on future research directions for exploring additional technical opportunities of MC in STINs."
https://arxiv.org/html/2411.04494v1,Online Omnidirectional Jumping Trajectory Planning for Quadrupedal Robots on Uneven Terrains,"Natural terrain complexity often necessitates agile movements like jumping in animals to improve traversal efficiency. To enable similar capabilities in quadruped robots, complex real-time jumping maneuvers are required. Current research does not adequately address the problem of online omnidirectional jumping and neglects the robotâ€™s kinodynamic constraints during trajectory generation. This paper proposes a general and complete cascade online optimization framework for omnidirectional jumping for quadruped robots. Our solution systematically encompasses jumping trajectory generation, a trajectory tracking controller, and a landing controller. It also incorporates environmental perception to navigate obstacles that standard locomotion cannot bypass, such as jumping from high platforms. We introduce a novel jumping plane to parameterize omnidirectional jumping motion and formulate a tightly coupled optimization problem accounting for the kinodynamic constraints, simultaneously optimizing CoM trajectory, Ground Reaction Forces (GRFs), and joint states. To meet the online requirements, we propose an accelerated evolutionary algorithm as the trajectory optimizer to address the complexity of kinodynamic constraints. To ensure stability and accuracy in environmental perception post-landing, we introduce a coarse-to-fine relocalization method that combines global Branch and Bound (BnB) search with Maximum a Posteriori (MAP) estimation for precise positioning during navigation and jumping. The proposed framework achieves jump trajectory generation in approximately 0.1 seconds with a warm start and has been successfully validated on two quadruped robots on uneven terrains. Additionally, we extend the frameworkâ€™s versatility to humanoid robots.","Quadruped robots have demonstrated significant potential in navigating complex terrains and performing agile maneuvers, similar to natural quadrupeds. Achieving agile jumping capabilitiesâ€”such as omnidirectional jumps and descending from high platformsâ€”is crucial for expanding the operational scope of these robots in real-world environments. The primary challenge involves generating real-time jumping trajectories that adhere to the dynamic and kinematic constraints of underactuated systems while ensuring stable landings to minimize hardware stress. Previous studies have showcased quadruped robots executing backflips and forward jumps using offline trajectory optimization methods, such as Katz (2018); Ding et al. (2020); Nguyen et al. (2019); Gilroy et al. (2021). However, the offline-generated trajectories are unsuitable for real-time applications and are limited to specific scenarios. Advances in locomotion gaits have also been achieved through model-based approaches, like Di Carlo et al. (2018), enabling robots to handle various obstacles. But, the obstacle-crossing ability of these pure locomotion methods is limited. Additionally, some works have focused on planning the Center of Mass (CoM) trajectory and employing kinodynamic tracking controllers for jumping tasks, like Chignoli et al. (2021b). Approaches that consider only CoM trajectory optimization often neglect comprehensive dynamic and kinematic constraints. This requirement hinders direct implementation with tracking controllers or low-level controllers. Moreover, most existing studies construct the jumping problem depending on specific platforms affecting the algorithmâ€™s scalability and concentrate on the take-off phase without designing landing controllers capable of mitigating impact forces during descent, which is essential for reducing hardware damage. Many existing jumping frameworks may not fully integrate environmental perception or address relocalization challenges from map positioning failures caused by high-impact landings. To address these issues, we introduce an omnidirectional jumping framework that enables real-time trajectory optimization while accounting for the robotâ€™s dynamic and kinematic constraints. An omnidirectional jumping plane is utilized to formulate the optimization problem for ground reaction forces (GRFs) and the CoM trajectory based on a reduced dynamics model. To accelerate optimization by meeting real-time requirements, the framework utilizes Latin Hypercube Sampling(LHS)Ayyub and Lai (1989), enhancing the initial population quality and performing configuration space analysis intensively to reduce the searching space. Moreover, we propose a warm start strategy with the help of the Pre-motion library and carefully design a prioritized fitness function to ensure the stable convergence of the algorithm. Table 1: Supplementary video list in this paper Video Links 1 Summary Video 2 Omnidirectional Jumping 3 Agile Motions 4 Jumping with Navigation An evolutionary algorithm solves the complex constrained optimization problem, producing outputs that include GRFs, CoM trajectory, joint configurations, and timing for each jump stage. An impedance controller is implemented as an active compliance landing controller to handle large impact forces during landing. Additionally, a Whole Body Controller (WBC) is employed for trajectory tracking, and a proportional-derivative (PD) controller manages the flight phase, ensuring systematic control throughout the jumping process. Environmental perception is integrated to enable jumping on uneven terrains, combined with navigation capabilities. A coarse-to-fine relocalization method is proposed to address localization failures to enhance localization stability after high-impact landings. 1.1 Contribution This work presents a novel and comprehensive framework for online jump trajectory optimization, with the following key contributions: 1. Development of a novel and general framework for quadrupedal omnidirectional jumping: We introduce a general and comprehensive omnidirectional jumping trajectory framework that includes online trajectory optimization, real-time trajectory tracking, and an active compliant landing controller. This framework enables effective jumping in multiple directionsâ€”including forward, backward, left, right, and diagonal movements like front-right. By integrating environmental perception, the framework allows the quadruped robot to overcome uneven terrain through jumping. Additionally, this method is successfully extended to support bipedal jumping. 2. Novel online optimizable jumping problem definition and solution: By constructing an omnidirectional jumping plane and formulating the optimization problem using a reduced-order model, we generate ground reaction forces (GRFs) and Center of Mass (CoM) trajectories along the jumping plane, with a feasible target point as input. Kinodynamic constraints are integrated into the optimization to ensure dynamic and kinematic feasibility. To achieve real-time optimization within approximately 0.1 seconds, we employ a differential evolution algorithm enhanced with strategies such as configuration space considerations, Latin Hypercube Sampling for efficient initial sampling, a pre-motion library as a warm start, and an innovatively designed fitness function (discussed in Sec. 4 and Sec. 8). 3. Advanced localization robustness with large impact upon landing: To ensure the accuracy of environmental perception assisted jumping on uneven terrain. We introduce a robust localization recovery mechanism using a novel coarse-to-fine relocalization method within our jumping framework. In the coarse stage, a global Branch-and-Bound (BnB) search algorithm provides an accurate initial pose. The refinement stage involves a Maximum a Posteriori (MAP) estimation that fuses sensor data from Inertial Measurement Units (IMUs), motor encoders, and LiDAR to achieve precise localization (as detailed in Sec. 7). 4. Extensive real-world experiments on two quadrupedal robots and simulations on a humanoid robot validate the effectiveness of our novel jumping framework for legged systems: Through comprehensive simulations and real-world experimentsâ€”including multi-directional jumps and autonomous navigation (including indoor and outdoor) involving substantial landing impacts, such as jumping from elevated platformsâ€”we have validated the robustness and practicality of our algorithm (see in Tab. 1). Furthermore, the jumping framework has been extended to accommodate a full-sized humanoid robot in simulated environments, demonstrating its versatility and effectiveness (as detailed in Sec. 9). 1.2 Related Works Model-free reinforcement learning (RL) has achieved significant advancements in quadruped locomotion, as highlighted in recent literature, like Lee et al. (2020); Li et al. (2023); Choi et al. (2023). However, the direct application of these techniques to jumping tasks faces challenges due to the limited jumping duration and high impact forces encountered during landing. This limitation has prompted researchers to explore the application of RL in enhancing jumping capabilities in robotics in Liu et al. (2023); Bellegarda et al. (2024). In particular, combining jumping with traditional locomotion has shown promise in enabling robots to navigate complex obstacles, such as desks and gaps in Cheng et al. (2024); Zhuang et al. (2023). Additionally, the integration of jumping functionalities has been explored in specific robotic tasks, including improving the agility of robotic goalkeepers in Huang et al. (2023). Notable achievements in this area include the development of robots capable of performing long-distance forward jumps, with some achieving distances greater than their length, and adopting cat-like landing strategies to ensure smooth touch-down and effective impact absorption like Bellegarda et al. (2024); Kurtz et al. (2022); Rudin et al. (2021). Despite these advancements, research on omnidirectional jumping and developing a unified control policy for executing various complex jumping maneuvers while ensuring post-landing hardware integrity remains limited. Gradient-based trajectory optimization has emerged as an important technique in enhancing quadruped robot agility, enabling precise trajectory following through leveraging the robotâ€™s dynamic models for optimal joint trajectory computation, like Bledt et al. (2018); Mastalli et al. (2020); Jenelten et al. (2020). This approach has been similarly practical in augmenting jumping capabilities, as demonstrated by MIT researchers who utilized gradient-based algorithms to enable a Cheetah3 robot to ascend a 0.76m platform in Nguyen et al. (2019). Similarly, this methodology facilitated the Mini Cheetahâ€™s ability to execute real-time, efficient, and dynamically stable 3D omnidirectional jumps predicated on predefined contact sequences and desired launch velocities in Chignoli et al. (2021b, a). Further advancements have been achieved through collocation-based optimization for jump execution within defined safety constraints in Gilroy et al. (2021) and Mixed-Integer Convex Optimization for forward jumps that circumvent the need for initial guesses, targeting the robots with two legs in Ding et al. (2020). Despite advancements, existing approaches to quadruped robot jumping often mandate predefined contact sequences or velocities, do not consider kino-dynamics at the trajectory generation, and lack support for omnidirectional jumps. These methods, using online or offline optimization, have not achieved the simultaneous goals of speed, stability, and real-time execution. Furthermore, there is a pressing demand for an autonomous technique that enables omnidirectional jumping based directly on the target position, eliminating reliance on preset motion references. Heuristic algorithms offer an efficient solution to optimization challenges with complex constraints, presenting novel avenues for jump motion generation. Differential Evolution (DE), introduced by Ahmad et al. (2022), stands out as a heuristic algorithm that has seen applications across robotics, signal processing, and other domains for tackling intricate optimization tasks. Our prior research utilized DE to obtain offline jumping trajectories for quadruped robots and get impressive results in Song et al. (2022). Nonetheless, this offline methodology falls short for systems necessitating online re-planning. Additionally, Yue et al. (2023) using Latin hypercube sampling (LHS) for initializing DE populations has enhanced convergence rates in scenarios with low-dimensional spaces in Wang et al. (2022). But the method introduced in Yue et al. (2023) can not achieve omnidirectional jumps. In recent years, lidar odometry and mapping methods have been extensively utilized in autonomous driving and robotic navigation. Several notable frameworks have been proposed, including LOAM (Zhang et al., 2014), LeGO-LOAM (Shan and Englot, 2018), LIO-SAM (Shan et al., 2020), and FAST-LIO (Xu et al., 2022). Among these, FAST-LIO has been widely adopted due to its superior accuracy and efficiency. However, these frameworks lack failure recovery strategies, making them unsuitable for direct application in quadrupedal robot navigation tasks involving jumping motions, where localization tracking failures frequently occur during landing. Therefore, a re-localization component is essential for quadrupedal robot navigation frameworks with jumping. The re-localization component in lidar SLAM essentially involves point cloud registration. To enhance the robustness of re-localization, we propose a global search method using the Branch-and-Bound (BnB) algorithm, which can obtain the global optimal solution and is well-suited for robust estimation problems. Unlike existing 6D point-to-point registration approaches employing BnB Yang et al. (2015); Campbell and Petersson (2016); Liu et al. (2018), we introduce a 3D point-to-plane registration algorithm tailored for our re-localization problem. This approach improves the algorithmâ€™s efficiency and accuracy while maintaining robustness."
https://arxiv.org/html/2405.00695v1,Joint torques prediction of a robotic arm using neural networks,"Accurate dynamic models are crucial for many robotic applications. Traditional approaches to deriving these models are based on the application of Lagrangian or Newtonian mechanics. Although these methods provide a good insight into the physical behaviour of the system, they rely on the exact knowledge of parameters such as inertia, friction and joint flexibility. In addition, the system is often affected by uncertain and nonlinear effects, such as saturation and dead zones, which can be difficult to model. A popular alternative is the application of Machine Learning (ML) techniques - e.g., Neural Networks (NNs) - in the context of a â€œblack-boxâ€ methodology. This paper reports on our experience with this approach for a real-life 6 degrees of freedom (DoF) manipulator. Specifically, we considered several NN architectures: single NN, multiple NNs, and cascade NN. We compared the performance of the system by using different policies for selecting the NN hyperparameters. Our experiments reveal that the best accuracy and performance are obtained by a cascade NN, in which we encode our prior physical knowledge about the dependencies between joints, complemented by an appropriate optimisation of the hyperparameters.","I INTRODUCTION One of the major problems in the robotics field is the identification of dynamic models. The problem is becoming increasingly important in a class of emerging applications where the control of contact forces and torques between the robot and the environment is a crucial safety concern [1, 2]. For instance, in the context of human-robot cooperation, safety is essential and requires the implementation of appropriate algorithms to control and limit the forces exerted on the operator when s/he comes into contact with the robot [3, 4]. An accurate knowledge of the torques of each joint is key to the design of this type of controller and is the main motivation behind the present work. The most immediate way of reconstructing the information about the joint torques is by direct measurement. This solution requires a set of sensors placed at each joint. However, this approach is expensive, firstly because the hardware is expensive in its own right, and secondly because the system requires frequent maintenance operations to recalibrate the sensors. A possible alternative is to model the dynamics of the system, which relates the torques to the joint state variables (see Equation (1)). This approach is affected by two main issues: (1) the physical parameters are difficult to identify due to the nonlinearity of the system; (2) there are several sources of uncertain dynamic effects (e.g., dead zones and saturation), which are unknown or difficult to model. In these cases, ML techniques come to the rescue. Specifically, modelling the system by using a black-box approach allows us to use an NN to learn the complex relationship within the data, also generalising across a wide range of scenarios and adapting to changes in the system configuration. In particular, NNs hold the promise to capture the workings of factors such as friction, inertia and nonlinear behaviour. This promising avenue requires a proper understanding of the most suitable NN architecture for the problem at hand and of its hyperparameters. In this paper, we investigate the aforementioned problem in a specific application scenario: the prediction of the joint torques of a 6 DoF industrial robotic arm. Our key contribution lies in the design and implementation of three distinct NN architectures. The core problem is how to exploit the dependencies between the joints, and each of the three solutions addresses this in a different way. The first architecture neglects these dependencies altogether and attempts to learn them from the data. The second one takes the dependencies into account by employing three separate NNs for each joint group. The third one adopts a hybrid approach that maintains joint groupings while also incorporating inter-group dependencies. The selection of the optimal model is influenced by a cost function that evaluates the prediction error and depends on the choice of NN hyperparameters. In this work, we considered the prediction error as a performance metric and our experimental results show that the hybrid architecture is the most accurate one when accompanied by an adequate optimisation of the hyperparameters."
https://arxiv.org/html/2411.04106v1,A Comparative Study of Deep Reinforcement Learning for Crop Production Management,"Crop production management is essential for optimizing yield and minimizing a fieldâ€™s environmental impact to crop fields, yet it remains challenging due to the complex and stochastic processes involved. Recently, researchers have turned to machine learning to address these complexities. Specifically, reinforcement learning (RL), a cutting-edge approach designed to learn optimal decision-making strategies through trial and error in dynamic environments, has emerged as a promising tool for developing adaptive crop management policies. RL models aim to optimize long-term rewards by continuously interacting with the environment, making them well-suited for tackling the uncertainties and variability inherent in crop management. Studies have shown that RL can generate crop management policies that compete with, and even outperform, expert-designed policies within simulation-based crop models. In the gym-DSSAT crop model environment, one of the most widely used simulators for crop management, proximal policy optimization (PPO) and deep Q-networks (DQN) have shown promising results. However, these methods have not yet been systematically evaluated under identical conditions. In this study, we evaluated PPO and DQN against static baseline policies across three different RL tasks, fertilization, irrigation, and mixed management, provided by the gym-DSSAT environment. To ensure a fair comparison, we used consistent default parameters, identical reward functions, and the same environment settings. Our results indicate that PPO outperforms DQN in fertilization and irrigation tasks, while DQN excels in the mixed management task. This comparative analysis provides critical insights into the strengths and limitations of each approach, advancing the development of more effective RL-based crop management strategies.","Crop production management is the process of taking logical actions on a crop field in order to achieve crop production goals. Unfortunately, it can be difficult for farmers to choose optimal practices due to complex physical, chemical, and biological phenomena which take place on a crop field and random external factors, such as weather (Husson et al., 2021). With the advent of machine learning and its success in the field of agricultural science (Liakos et al., 2018), researchers have begun to study how data-intensive algorithms can be used to learn optimal policies for crop management (Gautron et al., 2022a). In particular, methods such as reinforcement learning (RL) have been applied to crop management problems and have produced competitive results against existing expert policies on crop simulators (Gautron et al., 2022b, Tao et al., 2022). RL algorithms work by training an agent on a trial-and-error basis in an environment with the objective of maximizing cumulative returns; after training, the algorithms output static policies that can be deployed for real-world applications. Recent research has focused on the development of crop field RL environment software and the exploration of how different RL algorithms perform when applied to these virtual environments. One of the first crop RL environments introduced was CropGym, an OpenAI gym environment developed by Overweg et al. (2021) for the problem of Nitrogen fertilization management. Proximal policy optimization (PPO) was used in this study to successfully learn fertilization policies which lowered negative environmental impact. Gautron et al. (2022b) then developed gym-DSSAT, a flexible gym environment that utilized the popular crop simulator Decision Support System for Agrotechnology Transfer (DSSAT) to support fertilization and irrigation problems. PPO was shown to outperform expert baseline policies in the gym-DSSAT environment. To address long-term policy learning, Turchetta et al. (2022) introduced CyclesGym, a crop environment based on multi-year, multi-crop CGM Cyles. This RL environment was also tested using PPO. Tao et al. (2022) implemented deep Q-network (DQN) on the gym-DSSAT environment to attain improved RL agent performance. They also implemented imitation learning (IL) for the scenario where only a few state variables are available. Wu et al. (2022) continued this study by testing DQN against soft actor-critic (SAC) and other baseline policies on gym-DSSAT for the Nitrogen management problem. Finally, Wu et al. (2024) combined a language model (LM) with DQN to obtain a more optimal policy on the gym-DSSAT environment. This was achieved by using the LM to convert state variables into more informative language. While these studies demonstrated promising results, they primarily focused on evaluating individual RL algorithms, without systematically comparing different methods under consistent settings. In this study, we conduct a comprehensive comparison between the PPO and DQN algorithms, focusing exclusively on RL within the gym-DSSAT environment. To ensure a fair comparison, we evaluate both algorithms using the reward functions and baseline methods from Gautron et al. (2022b) for the fertilization and irrigation tasks. For the mixed problem, which considers fertilization and irrigation simultaneously, we apply the economic profit reward function from Tao et al. (2022). Additionally, both models are trained with random weather conditions, addressing a recommendation from previous DQN studies (Tao et al., 2022, Wu et al., 2022, 2024), which used non-random weather but suggested randomization for future research. This comparison aims to provide valuable insights into the relative strengths of these algorithms, helping to inform the selection of appropriate RL methods for real-world crop management applications. The remainder of this paper is structured as follows. First, we introduce the fundamental concepts of RL and provide a detailed overview of the competing algorithms, DQN and PPO. Next, we describe the gym-DSSAT environment, including its default settings and the three RL tasks it supports: fertilization, irrigation, and mixed management. Finally, we present and analyze the results of our experiments, followed by a discussion of key findings and suggestions for future research directions."
https://arxiv.org/html/2411.04011v1,Predicting and Publishing Accurate Imbalance Prices Using Monte Carlo Tree Search,"The growing reliance on renewable energy sources, particularly solar and wind, has introduced challenges due to their uncontrollable production. This complicates maintaining the electrical grid balance, prompting some transmission system operators in Western Europe to implement imbalance tariffs that penalize unsustainable power deviations. These tariffs create an implicit demand response framework to mitigate grid instability. Yet, several challenges limit active participation. In Belgium, for example, imbalance prices are only calculated at the end of each 15-minute settlement period, creating high risk due to price uncertainty. This risk is further amplified by the inherent volatility of imbalance prices, discouraging participation. Although transmission system operators provide minute-based price predictions, the system imbalance volatility makes accurate price predictions challenging to obtain and requires sophisticated techniques. Moreover, publishing price estimates can prompt participants to adjust their schedules, potentially affecting the system balance and the final price, adding further complexity. To address these challenges, we propose a Monte Carlo Tree Search method that publishes accurate imbalance prices while accounting for potential response actions. Our approach models the system dynamics using a neural network forecaster and a cluster of virtual batteries controlled by reinforcement learning agents. Compared to Belgiumâ€™s current publication method, our technique improves price accuracy by 20.4% under ideal conditions and by 12.8% in more realistic scenarios. This research addresses an unexplored, yet crucial problem, positioning this paper as a pioneering work in analyzing the potential of more advanced imbalance price publishing techniques.","In light of recent developments in climate change research, decarbonization of energy sources is a necessary step to reduce global CO2 emissions [1]. For this reason, an increasing amount of Renewable Energy Sources (RES) is being installed [2, 1, 3]. Although RES such as photovoltaic panels and wind farms have low carbon emissions, their energy supply is hard to accurately predict due to weather dependencies. This complicates the task of balancing power production and consumption, with direct consequences for the gridâ€™s operational safety. Hence, the electrical grid is being gradually redesigned to handle local and uncertain variations of electrical production [4]. For instance, some Transmission System Operators (TSOs) in Europe offer remuneration to Balancing Service Providers (BSPs) in exchange for contracted energy transactions that help balance the grid. Moreover, a specific imbalance tariff is applied to Balance Responsible Parties (BRPs), penalizing deviations in their energy schedules that disrupt the balance of the grid. By strategically designing the imbalance fee structure, TSOs can create an implicit Demand Response (DR) framework for the BRPs, which will then react to the prices and help to reduce the gridâ€™s System Imbalance (SI) magnitude. In Belgium, the imbalance prices are calculated at the end of each settlement period, spanning 15 minutes. This implies that BRPs face significant risk, as each transaction is subject to a price that remains unknown at the time of execution. This obstructs active participation from BRPs in the imbalance mechanism, as the prices might end up being less attractive than the expected ones, or even unprofitable for BRPs in case a significant swing of the SI occurs within the quarter hour. To tackle this, the Belgian TSO publishes an estimation of the imbalance price within each quarter hour on a minute basis, reducing the risks involved in the imbalance settlement participation. However, the imbalance prices are dependent on the gridâ€™s SI, making these values highly volatile. Hence, obtaining an accurate approximation of the imbalance prices is challenging. Moreover, publishing real-time prices might trigger an implicit reaction from BRPs, altering the SI and thereby impacting the price determined at the closure of the settlement period. By implicit response, we mean a voluntary, though undeclared, deviation of BRPs energy schedules aimed at taking advantage of the imbalance prices. This recursive cycle (illustrated in Fig. 1) adds complexity to the problem, highlighting a significant potential for the application of advanced planning techniques in the price publication problem. Figure 1: Influence cycle between the BRPs and the TSO. The TSO publishes an approximation of the imbalance price, triggering an implicit reaction from the BRPs. The reaction influences the gridâ€™s SI, hence impacting the final imbalance price at the closure of the settlement period. To address the challenge of publishing accurate real-time imbalance prices, two main approaches can be chosen: a model-based, and a model-free one. A model-based approach uses a model of the systemâ€™s dynamics to obtain each prediction (and publication) of the final price. Conversely, a model-free approach would obtain predictions by trying to anticipate the future behavior of the system without directly formulating a model of the latter. Given the complexity of the task, we believe that the most promising approach is model-based, allowing the publication technique to access insightful information about the gridâ€™s dynamics, thereby enhancing the accuracy of price predictions. To solve the problem, a good technique needs to: \edefmbx(a) provide accurate publications of the imbalance price, \edefmbx(b) have a low computational time for each prediction to enable real-time (minute-based) price publications, and \edefmbx(c) offer flexibility in handling varying levels of model complexity, enabling it to accommodate intricate models in its predictions. We propose a solution based on Monte Carlo Tree Search (MCTS), which is a search technique that mostly gained importance in board games such as Go and Chess [5, 6, 7]. MCTS has reached state-of-the-art performance in several complex decision making problems, some of those requiring fast computations due to time limitations, effectively addressing items a and b. Moreover, the only requirement for MCTS is to have a simulable environment that approximates the dynamics of the system. This allows for highly non-linear models such as Neural Networks (NNs) based forecasters, addressing item c. To model the systemâ€™s dynamics, we deployed an NN-based forecaster of the SI with a prediction horizon of 15 minutes and a granularity of 1 minute. The forecaster is composed of an ensemble of NNs, based on Van Gompel et al. [8], using a Constant Variable Selection Network (C-VSN) architecture to accurately forecast the SI. We then modeled the BRPs implicit reaction to the published imbalance prices using a cluster of virtual batteries. Each battery is controlled by a Reinforcement Learning (RL) agent, similar to [9]. By doing so, our publication technique takes into account the effect of its publication on the gridâ€™s balance, enhancing its prediction accuracy. To analyze the importance of the batteriesâ€™ capacity, we considered different battery sizes, each representing a different degree of influence the batteries exert on the grid. Using Belgian data from 2023, we quantitatively analyze our techniqueâ€™s price prediction accuracy, influence on the gridâ€™s SI, TSO balancing costs, and feasibility. Our contributions can be summarized as follows: 1. We deployed and analyzed an MCTS technique to predict and publish accurate imbalance prices from a TSO perspective. The technique is then benchmarked with the current publication method used in Belgium, obtaining superior results in terms of price accuracy. 2. We analyzed different reward functions to investigate the techniqueâ€™s potential when incorporating additional objectives beyond price accuracy, such as reducing the SI or the balancing costs. 3. Finally, we evaluated the techniqueâ€™s feasibility by integrating an SI forecaster and implicit response inaccuracies in the system dynamics model. Compared to the current publication method used in Belgium, our technique provides a price accuracy improvement of up to 20.4% when provided with ideal conditions (i.e., perfect knowledge of the grid dynamics) and up to 12.8% when considering more realistic conditions. To the best of our knowledge, this is the first time an MCTS technique has been proposed as part of a pricing method in a DR scenario. Moreover, this is to our understanding the first research work that predicts and publishes real-time imbalance prices from a TSO point of view."
https://arxiv.org/html/2411.03998v1,Dynamic Virtual Inertia and Damping Control for Zero-Inertia Grids,"In this paper virtual synchronous generation (VSG) approach is investigated in application to low- and zero-inertia grids operated by grid-forming (GFM) inverters. The key idea here is to introduce dynamic inertia and damping constants in order to keep power gird stable during different types of faults, islanding or large power balance oscillations. In order to achieve such robustness, we introduce frequency and phase angle shift functions to VSG along with dynamics virtual generator parameters. The stability of such approach is theoretically proven and theoretical results are supported by detailed case studies in RTDS (Real-Time Digital Simulator) NovaCor 1.0 with GFM inverters dynamics simulated with 1-3 microseconds timestep using two-level universal inverter model. Case studies include all aforementioned types of faults and demonstrate increased power grid robustness and survivability in comparison with traditional synchronous generation of comparable size.","Integration of inverter-based resources (IBRs) into modern power grids drastically affects dynamic performances of power systems. As a result, reduced power system inertia requires faster response from frequency control algorithms. Majority of IBRs operate in grid-following control (GFL) strategy with voltage and frequency explicitly measured in the point of interconnection with phase-locked loop (PLL) [1, 2]. This strategy is limited to grids with a significant amount of synchronous generation capable of providing adequate reference points. High penetration of GFL-based inverters is impractical and can cause instability [3, 4, 5]. As renewable energy integration increases, the focus has shifted to grid-forming (GFM) control. In this type of control IBRs create local voltage phasors and manage synchronization through frequency droop [6], allowing direct control over output frequency and active power, and similarly over voltage and reactive power [7]. Previous studies suggest that GFMs can address frequency stability issues in low inertia systems [8, 9]. Previous studies have focused on small-signal stability in systems with GFMs, often concluding the necessity of some SG presence for stability while overlooking new frequency regulation possibilities with GFMs [9]. Simultaneously, the concept of grid-forming (GFM) control ideally operates without synchronous generation. However, its practical implementation presents challenges, as illustrated in several studies [10, 11, 12]. Generally, grid-following inverters utilize a PLL for synchronization [13], whereas GFM methods are thought not to use PLL, or only for initial synchronization [12]. Nonetheless, PLL-based frequency and angle measurements can be used to enhance GFM control [14]. For parallel operation, all GFM units require synchronization, and PLL usage does not inherently conflict with GFM functionality. Power grids can operate solely with inverters using an advanced current-controlled scheme [15]. Therefore, it is not entirely accurate to claim that GFMs never use PLL [16]. Other concept of GFM control is virtual synchronous generation. Idea of virtual synchronous generation was firstly presented in [17] and is aimed to remedy the aforementioned instabilities in zero-inertia grids. The possibility of 100% GFM generation was presented in [18] with numerical simulation of corresponding EMT. Later, in [19] the authors demonstrated SG operation together with GFM. Particularly in scenarios where GFMs augment SG-driven inertial responses. Other research highlights the damping contributions of droop-controlled GFMs to frequency dynamics, typically within SG-dominated frameworks [20]. The development of control designs in this area has complications due to high standard for verifiable simulations. As it was shown in [21], standard timestep of quarter electrical cycle is usually insufficient, and detailed simulations of inverter dynamics should be done in the order of microseconds. The main contributions of this work can be separated into 3 following items: 1. VSG control system that dynamically adjusts system inertia and dumping in order to maximize robustness of zero-inertia power grid. 2. Simulation of 100% IBR 9 bus power grid [22] with universal converter models operating at 1-3 microseconds timestep [23] implemented in RTDS NovaCor 1.0. 3. Detailed robustness analysis of the proposed control system, including usage of IBRs only and IBRs with SG under different system faults. The paper is organised in the following way. Section II contains development of the VSG controller with dynamic inertia and dumping coefficients. Section III contains RTDS model of 9-bus system with possibility to switch between IBRs and SGs. Section IV is dedicated to system robustness tests. Finally section V is conclusion of the work."
https://arxiv.org/html/2411.03909v1,Direct Adaptive Control of Grid-Connected Power Converters via Output-Feedback Data-Enabled Policy Optimization,"Power electronic converters are gradually becoming the main components of modern power systems due to the increasing integration of renewable energy sources. However, power converters may become unstable when interacting with the complex and time-varying power grid. To deal with this problem, an adaptive control design scheme for power converters is preferable, which can capture the closed-loop dynamical interaction between the converter and the grid via online data. In this paper, we propose an adaptive data-driven control method, called data-enabled policy optimization (DeePO), to stabilize power converters by using only online input-output data. Our contributions are threefold. First, we propose a covariance parameterization of partially observed linear systems with input-output data. Second, we develop a DeePO algorithm, which updates the parameterized policy with data-based gradient descent to achieve computationally efficient adaptive control. Third, we use high-fidelity simulations to verify that DeePO can effectively stabilize grid-connected power converters and quickly adapt to the changes in the power grid.","Modern power systems feature a large-scale integration of power electronic converters, as they act as interfaces between the AC power grid and renewable energy sources, high-voltage DC (HVDC) systems, energy storage systems, and electric vehicles [1]. The large-scale integration of converters is fundamentally changing the power system dynamics, as they are significantly different from traditional synchronous generators (SGs). Usually, multiple nested control loops, based on fixed-parameter PI regulators, are needed in converters to achieve voltage, current, and power regulations. Under these control loops, power converters exhibit complicated interaction with the power grid and may easily tend to be unstable due to unforeseen grid conditions [2, 3, 4]. Such instability issues have been widely observed in practice [5], which poses challenges to the secure operation of modern power systems and impedes further integration of renewables. The instability in converter systems is caused by the closed-loop interaction between the converter and the complex power grid, which often occurs when the converterâ€™s control strategy does not fit into the grid characteristics [6, 7]. Hence, the control design of converters should take into account the power grid dynamics for the sake of stability. However, the power grid is unknown, nonlinear, and time-varying from the perspective of a converter. Moreover, the grid structure and parameters are difficult to obtain in real time. Hence, it is nearly impossible to establish an exact dynamical model of a power grid for the control design of converters. As a remedy, engineers often use an overly simplified model for the power grid (e.g., an infinite bus) and tune the controller based on engineering experience and iterative trial-and-error approaches, which can be expensive, time-consuming, and lacking stability guarantees due to the model mismatch. While existing robust control methods can be used to handle the model mismatch [8, 9], they usually lead to conservative controllers when large changes appear in the power grid (e.g., tripping of transmission lines or even HVDC stations). Ideally, the controller of converters should be adaptive, i.e., it is able to perceive and quickly adapt to changes in the power grid by using online data. Recently, there has been a renewed interest in direct data-driven control, which bypasses the system identification (SysID) step and learns the controller directly from a batch of persistently exciting data [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]. This approach is end-to-end, easy to implement, and has seen many successful applications [20, 21, 22]. Following this line, our previous work proposes a direct data-driven linear quadratic regulator (LQR) method [23, 24]. It is adaptive in the sense that the control performance is improved in real time by using online closed-loop data. We call this method Data-enabled Policy Optimization (DeePO), where the policy is parameterized with sample covariance of input-state data and updated using gradient methods. DeePO has a recursive algorithmic implementation and hence is computationally efficient. From a theoretical perspective, it meets provable stability and convergence guarantees for linear time-invariant systems. All these key features render DeePO an ideal data-driven control method for power converters. In this paper, we propose a DeePO method to mitigate oscillations in power converter systems. Since the power converter system is only partially observed, i.e., the full state is unmeasurable, we first extend the covariance parameterization of the system with input-state data in [23, 24] to that with input-output data. This is achieved by regarding a finite length of past input-output trajectory as the state and using the corresponding covariance to parameterize the closed-loop system. Then, we propose the DeePO algorithm for direct adaptive control of partially observed systems. Finally, we apply the DeePO algorithm to stabilize power converter and direct-drive wind generator systems, both of which are unknown, state-unmeasurable, and encounter sudden change of dynamics due to grid changes. Simulation results show that DeePO enables efficient online adaptation and effectively prevents instabilities in grid-connected power converters. Compared with our previous works applying data-enabled predictive control (DeePC) to power converters [21, 25], DeePO offers a significantly reduced online computational burden, making it more suitable for scenarios where the processor cannot solve a quadratic program in real time. The rest of the paper is organized as follows. Section II formulates the stabilization problem of power converter systems. Section III introduces the adaptive LQR control and the DeePO method. Section IV extends the DeePO algorithm to input-output systems. Section V performs simulations on the converter systems. Conclusion is made in Section VI. Notation. We use Insubscriptğ¼ğ‘›I_{n}italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT to denote the nğ‘›nitalic_n-by-nğ‘›nitalic_n identity matrix. We use Ïâ¢(â‹…)ğœŒâ‹…\rho(\cdot)italic_Ï ( â‹… ) to denote the spectral radius of a square matrix. We use Aâ€ superscriptğ´â€ A^{\dagger}italic_A start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT to denote the pseudoinverse of a matrix Ağ´Aitalic_A. We use (S)isubscriptğ‘†ğ‘–(S)_{i}( italic_S ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to denote the iğ‘–iitalic_i-th column of a block matrix Sğ‘†Sitalic_S."
https://arxiv.org/html/2411.03772v1,"Analyzing Ultra-Low Inter-Core Crosstalk Fibers in Band and Space Division Multiplexing EONs (Pre-Print)â€ â€ thanks:Farhad Arpanaei acknowledges support from the CONEX-Plus programme funded by Universidad Carlos III de Madrid and the European Unionâ€™s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 801538. The authors from UC3M would
like to acknowledge the support of the EU-funded SEASON project (grant No.101096120) and Spanish-funded Fun4date-Redes project (grant No.PID2022-136684OB-C21).","In the ultra-low inter-core crosstalk (UL-ICXT) working zone of terrestrial multi-band and multi-core fiber (MCF) elastic optical networks (EONs), the ICXT in all channels of all cores is lower than the ICXT threshold of the highest modulation format level (i.e., 64QAM) for long-haul distances (i.e., 10,000 km). This paper analyzes the performance of this type of MCF in multi-band EONs (MB-EONs). Two band and space division multiplexing (BSDM) scenarios are investigated: MCF and a bundle of multi-fiber pairs (BuMFP). Moreover, the performance of the UL-ICXT of two MCFsâ€”one with the standard cladding diameter (CD = 125 Î¼ğœ‡\muitalic_Î¼m) with 4 cores and another with a nonstandard larger CD with 7 coresâ€”are evaluated in the US backbone network. Our findings show that by accurately designing the physical structure of the MCF, even with a standard CD, it is possible to achieve UL-ICXT in C+L+S-band long-haul BSDM EONs. Furthermore, the simulation results show that the network throughput for BSDM EONs with MCFs in the UL-ICXT regime is up to 12% higher than the BuMFP scenario. Additionally, capacity increases linearly with the number of cores.","With the increasing demand for bandwidth, communication service providers (CSPs) are compelled to enhance their networks [1]. The adoption of multi-band technology has emerged as a cost-effective solution to address this need [2]. However, forecasts indicate that by 2030, the evolution of the 6th generation (6G) mobile communication ecosystem and the proliferation of artificial intelligence applications will necessitate a transition to spatial multiplexing networks [3]. Designing a flexible and versatile network that integrates band and space division multiplexing (BSDM) technologies poses a complex challenge. To meet this challenge while ensuring the stability of CSPâ€™s infrastructure and compatibility with legacy, recent proposals have introduced multi-core fibers (MCFs) with a cladding diameter identical to traditional single-core fibers, typically 125 Î¼â¢mğœ‡ğ‘š\mu mitalic_Î¼ italic_m in cladding diameter [4, 5]. However, reducing the cladding diameter (CD) decreases the distance between embedded cores, leading to increased ICXT [6]. Effective crosstalk management requires limiting the number of cores within the standard CD. Moreover, managing crosstalk becomes critical, especially in the absence of multi-input multi-output (MIMO) digital processing along a lightpath (LP) in the uncoupled or weakly coupled MCFs [7]. The crosstalk between adjacent cores must be managed to facilitate all-optical transmission without relying on MIMO digital processing [8]. This management is influenced by factors such as transmission distance, distance between adjacent cores, number of adjacent cores, and modulation format level [7]. The complexity of crosstalk management is further compounded in multi-band systems due to phenomena like inter-channel stimulated Raman scattering (ISRS), which significantly impacts transmission quality based on frequency as well. Additionally, the frequency-dependent power coupling coefficient across different bands, unlike C-band systems, cannot be disregarded [5]. Recent advancements have proposed four-core fibers of standard gauge to operate across all bands [6]. In this work, we not only evaluate the performance of these fibers against non-standard cladding diameter fibers (<<< 230 Î¼â¢mğœ‡ğ‘š\mu mitalic_Î¼ italic_m) but also compare UL-IXCT MCFs systems with a bundle of single-mode fiber pairs (BuMFP). Furthermore, determining the suitable operating range of MCFs based on the distance between adjacent cores and trench width is crucial [9, 10]. The suitable operating range ensures that crosstalk resulting from core proximity does not compromise the bit rate of transceivers, thus relying solely on linear and non-linear noise factors for determining bit rates. This working zone is called the UL-ICXT zone, where the modulation format level selection is independent of the ICXT. Therefore, network planning is not as complex as ICXT-aware service provisioning which is necessary for ICXT-sensitive weakly coupled MCFs. Moreover, the total capacity of the network is higher than that of weakly-coupled MCFs that are ICXT-sensitive. In ICXT-sensitive MCFs, the modulation format of each lightpath depends on the ICXT experienced [11]."
https://arxiv.org/html/2411.03633v1,Privacy-Preserving Resilient Vector Consensus,"This paper studies privacy-preserving resilient vector consensus in multi-agent systems against faulty agents, where normal agents can achieve consensus within the convex hull of their initial states while protecting state vectors from being disclosed. Specifically, we consider a modification of an existing algorithm known as Approximate Distributed Robust Convergence Using Centerpoints (ADRC), i.e., Privacy-Preserving ADRC (PP-ADRC). Under PP-ADRC, each normal agent introduces multivariate Gaussian noise to its state during each iteration. We first provide sufficient conditions to ensure that all normal agentsâ€™ states can achieve mean square convergence under PP-ADRC. Then, we analyze convergence accuracy from two perspectives, i.e., the Mahalanobis distance of the final value from its expectation and the Hausdorff distance based alteration of the convex hull caused by noise when only partial dimensions are added with noise. Then, we employ concentrated geo-privacy to characterize privacy preservation and conduct a thorough comparison with differential privacy. Finally, numerical simulations demonstrate the theoretical results.","\IEEEPARstart Enabling multi-agent systems to collaborate effectively in solving complex problems or accomplishing tasks has garnered significant interest in fields such as aviation, robotics, and others [1, 2, 3]. In many application scenarios, such as distributed machine learning, multi-robot systems, and platoon, vector consensus is frequently used, aiming to make all agentsâ€™ state vectors reach agreement under a predefined rule. However, in multi-agent systems, there may be faulty or adversarial agents111In the subsequent text, both are collectively referred to as faulty agents. against the rule to break the consensus of the system or drive the consensus to an unsafe value. To maintain the safe consensus of normal agents, the concept of resilient vector consensus has been developed. Resilient vector consensus means that all normal agents can achieve vector consensus where the final value lies within the convex hull of the initial states of normal agents despite the presence of faulty agents. Under resilient vector consensus, many distributed systems can work correctly under complex and changing circumstances. Therefore, resilient vector consensus is a very important concept in distributed fields. In most consensus/resilient algorithms, agents directly send their states to their neighbors. However, if the transmitted states are intercepted by a malicious entity, this could lead to the disclosure of agentsâ€™ valuable information, resulting in a privacy leakage. For example, in distributed machine learning applications like the financial and medical sectors, the exchange of grid information during the training process may lead to the leakage of sensitive data from training sets. The sensitive data may include bank customersâ€™ account balances, transaction records, as well as patientsâ€™ diagnosis results and medication histories. Such disclosure can potentially result in financial fraud and serious breaches of patient privacy, indicating that privacy preservation is a crucial task in the resilient vector consensus. Due to the scarcity of communication and computation resources in distributed systems and the high demand of encryption algorithms on these resources, differential privacy has become the primary choice for privacy protection in consensus algorithms. Typically, in the resilient vector consensus, we aim to protect the initial state vectors of the normal agents without the existence of a trusted control center, where local differential privacy (local-DP) [4] is suitable to be utilized. Different from centralized differential privacy (central-DP), local-DP protects information during transmission from being intercepted, unrelated to differential attacks, and thus does not involve the concept of neighboring inputs. Local-DP also ensures that any pair of inputs generate similar outputs. However, the initial state range for each agent can be very large, and requiring pairs of distant initial states to produce similar outputs would reduce the systemâ€™s utility222Utility refers to the extent to which a system can still perform its original functions after the application of privacy-preserving mechanisms.. This motivates us to use the concept of geo-privacy, characterizing the Euclidean distance between agents as one of the metrics for privacy protection. Specifically, we propose to use the concept of concentrated geo-privacy [5], which is a generalization of differential privacy, to describe the privacy preservation of resilient vector consensus. Apart from a more precise description of privacy preservation, it also offers advantages such as Gaussian mechanism and advanced composition. Much attention has been paid to resilient consensus algorithms, which can be roughly classified into two categories. One is to detect and preclude faulty agents[6, 7, 8], while the other one is to try to find a safe value despite the presence of faulty ones[9, 10, 11, 12]. In this paper, we focus on the second one as it has more analytical results and related works regarding resilient scalar/vector consensus and privacy-preserving resilient scalar consensus are provided below. Resilient scalar consensus is designed for the normal agents to converge to a common state that lies between the maximum and minimum values of normal agentsâ€™ initial states. There are two frequently used ways: One way is to use â€œmedianâ€. In [13], Zhang et al. propose a median consensus algorithm, where each normal agent updates by using its own value and the median of the states from its neighborhood. As for the other way, most of the algorithms rely on the strategy of simply disregarding suspicious values. For example, in a series of algorithms named Mean-Subsequence-Reduced (MSR) algorithm [9, 10], the main idea is to discard a fixed number of largest and smallest values in each normal agentâ€™s neighborhood. Conversely, in the modification of Weighted MSR (W-MSR) algorithm [11, 12], each normal agent only discards values that are larger or smaller than its own to maintain its state. Resilient vector consensus is ensured by that each normal agent must seek a point which is always located in the convex hull of its normal neighboring state vectors at each iteration. The existing work can be classified into three categories. The first category utilizes Tverberg partitions to compute Tverberg points. Specifically, it includes Byzantine vector consensus algorithm [14] and Approximate Distributed Robust Convergence algorithm [2]. The second category calculates the intersection of multiple convex hulls. This involves Algorithm 1111 proposed in [15] and the multidimensional approximate agreement algorithm described in [16]. The algorithm of the last category utilizes the concept of â€œcenterpointâ€, exemplified by â€œApproximate Distributed Robust Convergence Using Centerpointâ€ (ADRC) algorithm [17]. However, these algorithms are rather time-consuming, and most of them require approximation algorithms, which consequently reduce fault tolerance. Notably, among these algorithms, the ADRC [17] stands out with its better tolerance for the mature approximation algorithm. In each iteration, every normal agent computes the centerpoint (an extension of the median in higher dimensions) of its neighborhood and adjusts its position accordingly. Resilient vector consensus can be achieved as long as the communication topology and the number of faulty agents within the neighborhood meet certain requirements. Privacy-preserving resilient scalar consensus can be achieved by two general approaches, i.e., cryptography-based methods [18, 19] and noise-adding-based methods [18, 20]. Although cryptography-based algorithms have higher convergence accuracy compared to noise-adding-based ones, encryption, information exchange, and decryption are time-consuming processes and thus significantly decrease efficiency. Besides, the noise-adding-based ones are typically more flexible, as it allows for the adjustment of noise intensity and distribution based on specific application scenarios and privacy requirements. In [20], Fiore et al. proposed a Differentially Private MSR (DP-MSR) algorithm, adding decaying, zero-mean Laplace noise to the scalar states of normal agents. They also analyzed resilient scalar consensus in the sense of probability and differential privacy. Note that privacy-preserving resilient vector consensus is still an open issue. One simple way is to conduct a privacy-preserving resilient scalar consensus algorithm dğ‘‘ditalic_d times, where dğ‘‘ditalic_d is the dimension of states. However, this may cause the final value to fall outside the convex hull of the initial states of the normal agents [17]. Additionally, analyzing the performance of privacy-preserving resilient vector consensus by noise adding involves several key challenges: (i) The noise added makes the convergence non-deterministic and random, and the existing definition of resilient vector consensus is inapplicable. (ii) Due to the noise and resilience, we cannot derive the analytical solution of the final value, making it difficult to describe the accuracy of convergence. Additionally, high-dimensional convex hulls are inherently difficult to characterize and require specific values to obtain. After adding noise, the vertices and boundaries of the convex hull change randomly, making it challenging to characterize the convex hull after multiple iterations. (iii) At each iteration, each agent transmits and updates state information and the specific distribution of local state at each iteration is non-deterministic. We need to analyze the extent to which this information leaks the initial state, which is also a very difficult task. To solve the above challenges, we consider a modification of ADRC, named Privacy-Preserving ADRC (PP-ADRC), and provide rigorous theoretical analysis for convergence and privacy performance. Compared to our conference version [21], we use Gaussian noise instead of Laplace noise and conduct a more rigorous performance analysis, Meanwhile, we add the final value distribution analysis by characterizing the deviation of the final value from the expectation. Moreover, we use CGP instead of DP to characterize the privacy-preservation and compare it with DP comprehensively. We also add more simulations under a 3âˆ’limit-from33-3 -dimensional case to illustrate our theoretical results. The contributions of this paper are given as follows. â€¢ We consider a modification of ADRC, named Privacy-Preserving ADRC (PP-ADRC), where each agent adds Gaussian noise to the local state vector for local interaction. We show sufficient conditions to ensure a resilient consensus of expectation. â€¢ We analyze the final value from two perspectives. First, we utilize the Mahalanobis distance to analyze the residuals of the final value from the expected one given a specified probability upper bound. Secondly, we employ the Hausdorff distance to assess the change between the convex hulls with and without noise with a specified probability upper bound, where only partial fixed dimensions are added with noise for each iteration. â€¢ We employ ÏğœŒ\rhoitalic_Ï-concentrated geo-privacy to characterize privacy preservation without detailed distributions of the outputs. Through a detailed comparison with differential privacy, we demonstrate that Ïâˆ’limit-fromğœŒ\rho-italic_Ï -concentrated geo-privacy can provide advantages for resilient vector consensus. The organization of this work is as follows. We introduce preliminaries and problem formulation in Sections 2 and 3, respectively. Then, we analyze the convergence condition and final value in Section 4. The privacy analysis is presented in Section 5. Simulation results are presented in Section 6. Finally, conclusions and avenues for future research are outlined in Section 7."
https://arxiv.org/html/2411.03582v1,Privacy Preserving Mechanisms for Coordinating Airspace Usage in Advanced Air Mobility,"Advanced Air Mobility (AAM) operations are expected to transform air transportation while challenging current air traffic management practices. By introducing a novel market-based mechanism, we address the problem of the on-demand allocation of capacity-constrained airspace to AAM vehicles with heterogeneous and private valuations. We model airspace and air infrastructure as a collection of contiguous regions (or sectors) with constraints on the number of vehicles that simultaneously enter, stay, or exit each region. Vehicles request access to the airspace with trajectories spanning multiple regions at different points in time. We use the graph structure of our airspace model to formulate the allocation problem as a path allocation problem on a time-extended graph. To ensure that the cost information of AAM vehicles remains private, we introduce a novel mechanism that allocates each vehicle a budget of â€œair-creditsâ€ (an artificial currency) and anonymously charges prices for traversing the edges of the time-extended graph. We seek to compute a competitive equilibrium that ensures that: (i) capacity constraints are satisfied, (ii) a strictly positive resource price implies that the sector capacity is fully utilized, and (iii) the allocation is integral and optimal for each AAM vehicle given current prices, without requiring access to individual vehicle utilities. However, a competitive equilibrium with integral allocations may not always exist. We provide sufficient conditions for the existence and computation of a fractional-competitive equilibrium, where allocations can be fractional. Building on these theoretical insights, we propose a distributed, iterative, two-step algorithm that: 1) computes a fractional competitive equilibrium, and 2) derives an integral allocation from this equilibrium. We validate the effectiveness of our approach in allocating trajectories for two emerging urban air mobility services: drone delivery and air taxis.","The emergence of advanced air mobility (AAM) operations, including urban air mobility (UAM) and unmanned aerial vehicles (UAVs), is expected to transform the landscape of the air transportation system. These new aerial platforms can provide air taxi services that better connect rural and suburban communities with urban centers, facilitate package and medical delivery services, and support infrastructure and public safety (Guo et al., 2024). However, these new technologies bring new challenges concerning air traffic management. Current estimates of the density, type, and number of these new flights have led the Federal Aviation Administration (FAA) in the United States to declare that â€œexisting Air Traffic Management (ATM) system infrastructure and associated resources cannot cost-effectively scale to deliver servicesâ€ (Administration, 2022). While current systems focus primarily on fixed-wing aircraft, scheduled flight operations, and airport infrastructure, AAM includes novel electric vertical take-off and landing (eVTOL) aircraft and UAVs that will fly on demand, which creates challenges for scheduling flights. Additionally, like current aerial operations (Ball et al., 2018, 2020), AAM operations will have heterogeneous valuations for timely airspace access (while a passenger air taxi may be traveling on a strict schedule, a regional cargo flight could be delayed with less issue (Skorup, 2019; Seuken et al., 2022)), yet it is extremely important for these valuations to remain private because they can contain important business or personal information. These characteristics have led the FAA and other aviation agencies to call for novel air traffic management tools and strategies, developed by third-party Service Providers (SPs), to support AAM aircraft and use cases. These tools must work in conjunction with existing Air Navigation Service Providers (ANSPs, e.g., the FAA) to safely and efficiently actualize new air transport opportunities (Administration, 2022; Federal Aviation Administration, 2023; European Organisation for the Safety of Air Navigation (2022), EUROCONTROL). We propose an algorithm that schedules AAM vehicles with heterogeneous, private valuations requesting access to airspace in an on-demand manner. While ideas from conventional air traffic management (Bertsimas and Patterson, 1998, 2000; Bertsimas et al., 2011; Roy and Tomlin, 2007; Odoni, 1987) can accommodate private and heterogeneous valuations from flight operators, they are not designed for the dynamic and adaptable nature of AAM operations (Skorup, 2019; Seuken et al., 2022). This work aims to answer the following question: Can we design an efficient mechanism that allocates desired airspace resources to AAM vehicles with heterogeneous private valuations without requiring them to disclose their valuations to anyone? To address this, we model the airspace as a set of contiguous regions, each with specific capacity constraints on the number of AAM vehicles (modeled as eVTOLs) that can arrive, depart, or stay at any given time (see Fig. 1). Vehicles submit requests for airspace access through a menu of time-trajectories (or air corridors), detailing which regions they will occupy at different time steps. We frame the allocation of these time-trajectories within capacity-constrained airspace as a path allocation problem on a time-extended graph (Definition 2.1), where all capacity constraints are represented as constraints on the graphâ€™s edges. Figure 1: Model of airspace as a set of contiguous regions, each having arrival, departure, and transit constraints. Some regions are at the cruising altitude, while others encompass vertiport/launch pads. We introduce a new mechanism with artificial currency that is implemented in a receding horizon manner. In every round of this approach, the SP allocates â€œair-creditsâ€ (the artificial currency) to each AAM vehicle requesting access to airspace in that round and charges an anonymous price (in air credits) for the use of various airspace regions. Based on these prices, each vehicle calculates its most preferred bundle of regions on the time-extended graph that maximizes its valuation while adhering to its budget constraint and some additional individual constraints to ensure that the set of bundles forms a feasible path. The goal of the SP is to design prices and allocate airspace regions efficiently and safely, guided by the following desiderata: (i)ğ‘–(i)( italic_i ) Given the price vector, the allocation by the SP should be optimal for every AAM vehicle, maximizing each vehicleâ€™s private valuation subject to budget constraints; (iâ¢i)ğ‘–ğ‘–(ii)( italic_i italic_i ) The capacity constraints of the airspace must be respected; (iâ¢iâ¢i)ğ‘–ğ‘–ğ‘–(iii)( italic_i italic_i italic_i ) Prices must be non-negative, and if they are strictly positive, the capacity of each airspace region must equal its demand. A tuple of allocation and prices that satisfy these requirements is referred to as a competitive equilibrium in economics, which may not always exist (Budish, 2011). However, inspired by the literature on Fisher markets under linear constraints (Jalota et al., 2023), we demonstrate that a fractional competitive equilibrium â€” a relaxation of the competitive equilibrium that allows fractional allocations â€” always exists (Proposition 3.3). Furthermore, we show that the prices at a fractional-competitive equilibrium can be computed as an optimal dual multiplier of a â€budget-adjusted welfare problemâ€ (see (3)), which is a convex optimization problem (Lemma 3.4). Notably, the budget adjustment for each vehicle is determined by the optimal dual multiplier associated with its individual constraints. As a result, computing a fractional-competitive equilibrium reduces to calculating a fixed point (Proposition 3.5). Building on these theoretical insights, we propose a two-step algorithmic procedure to allocate AAM vehicles to airspace. In the first step, we develop a two-loop algorithm to compute the fractional-competitive equilibrium without requiring information about the private valuations of vehicles. Specifically, this step involves solving the fixed point problem stated in Proposition 3.5 via a two-loop algorithm (see Algorithm 1) that mimics fixed point iteration. The inner loop solves a reformulated budget-adjusted welfare problem in a distributed manner using an Alternating Direction Method of Multipliers (ADMM) update, ensuring that AAM vehicles do not have to share their valuations with anyone. The outer loop then updates the budget adjustment parameter using the most recent value of the dual multiplier corresponding to the individual constraints of the AAM vehicles. In the second step, we derive an integral allocation from the fractional-competitive equilibrium obtained in the first step, keeping the prices unchanged. We rank the vehicles based on the fractional allocation they received for their most desired resource in the first step. The social planner then allocates resources to the vehicles sequentially according to this ranking, updating the remaining capacity after each allocation (Algorithm 2). Algorithm 1 can be viewed as feedback between the learning algorithm used by AAM vehicles and the SP. The AAM vehicles use (anonymized) signals from the SP, such as prices and other parameters, to adjust their demand for resources. The SP, in turn, uses the updated demand for resources from each AAM vehicle to adjust the prices and related variables. Notably, this two-step process (Algorithms 1-2) does not require vehicles to disclose their private valuations to either the SP or to other vehicles (Remark 4.2). To validate the effectiveness of our approach, we explore two upcoming applications. First, we analyze drone-based package delivery using a dataset of drone trajectories generated with realistic physical models by Airbus over the city of Toulouse, France. Second, we investigate the scheduling problem for electric air taxis on a hypothesized air traffic network in Northern California, United States. 1.1 Related Works Market mechanisms for airspace management. Su et al. (Su et al., 2024) utilized a generalized Vickreyâ€“Clarkeâ€“Groves (VCG) auction for AAM resource management taking into account social welfare, safety, and congestion while considering proportional fairness by weighting different fleet operators accordingly. We adopt a similar time-extended graph model from their work to represent airspace with capacity constraints. However, when agents have budget constraints, as in our case, truthful bidding is no longer a dominant strategy (Ausubel and Milgrom, 2004), which limits the applicability of VCG-based mechanisms. Furthermore, the requirement for truthful bidding poses privacy concerns for vehicles as they have to reveal their exact valuations through bids. Other proposed approaches include second-price auctions combined with congestion management algorithms (Qin and Balakrishnan, 2022; Chin et al., 2023) and combinatorial auctions (Leet and Morris, ). Unlike our approach, these related works only considered unit-capacity regions. Balakrishnan and Chandranâ€™s work (Balakrishnan and Chandran, 2017) introduced a column generation algorithm that dynamically updates prices to respect capacity constraints, approximating a competitive equilibrium. Their approach, however, requires aircraft to report both their desired trajectory and the associated benefits. In contrast, our ADMM-based approach simplifies the process by requiring aircraft to report only their desired trajectories in response to announced prices. While the former approach may benefit from computational efficiency due to the additional information available to the central planner, our approach prioritizes privacy by limiting the amount of information shared. Another key distinguishing feature of our work, compared to existing market mechanisms for AAM, is the use of artificial currency. While, as noted by (Seuken et al., 2022), money can effectively elicit preferences, mechanisms involving monetary transactions may disproportionately favor operators with greater financial resources. Our approach mitigates this issue by introducing a system of artificial currency that helps ensure fairness. Connections with other economic mechanisms. Our airspace allocation formulation incorporates four essential features: complementarities, indivisibilities, the absence of monetary transfers, and quasilinear preferences. In settings with substitutable and divisible goods, the classical concept of Competitive Equilibrium from Equal Incomes (CEEI) ensures efficient and fair outcomes by endowing agents with an artificial currency that holds no value outside the market (Varian, 1974). However, in cases involving complementary and indivisible goods, a competitive equilibrium may not exist. A common approach in such cases is to seek an approximate CEEI (A-CEEI), which preserves efficiency, fairness, and incentive properties while being supported by computationally efficient algorithms tested in real-world applications like course allocation (Budish, 2011; Budish et al., 2023). Although we adopt the idea of artificial currency, we deviate from this model by allowing agents to save currency for future use. When the saved budget influences the agentâ€™s utility in a quasilinear form, our model more closely aligns with combinatorial auction environments, where monetary transfers play a role in eliciting agentsâ€™ preferences. Combinatorial auctions allow participants to bid on combinations of items rather than individual items (Cramton et al., 2006a). Given the inherent exponential complexity of these auctions, there is no universal format that applies in all settings. Below, we highlight formats particularly suited to scenarios with budget constraints and strong complementarities. For a more comprehensive list, refer to (Cramton et al., 2006b, Table 2.2). In environments with budget constraints, dynamic auction formats often outperform static mechanisms like VCG auctions. These formats allow agents to observe current prices and adjust their bids according to their budget constraints before final submission. Examples include the Simultaneous Ascending Auction, the Ascending Proxy Auction, and the Clock-Proxy Auction (Ausubel and Cramton, 2004; Cramton et al., 2006b). These dynamic formats have an additional advantage: bids are submitted incrementally, reducing the risk of privacy violations and preventing the exponential growth of bundle combinations. However, they are still susceptible to computational complexity issues, which can even be exacerbated due to the inherent nature of combinatorial auctions. In our approach, we model complementarities using linear equality constraints and leverage recent advances in Fisher markets with linear constraints to develop an algorithmic approach that only needs to solve convex optimization problems. Particularly, we compute a fractional-competitive equilibrium, which provides a relaxed solution that is easier to compute. From this fractional-competitive equilibrium, we then derive an integral allocation that satisfies the capacity constraints in the problem. 1.2 Organization The article is organized as follows: In Section 2, we introduce the model of airspace management studied in this paper. In Section 3, we describe the proposed market mechanism with artificial currency and the theoretical results on fractional-competitive equilibrium. Section 4 presents the algorithmic procedure to compute an approximate market mechanism. In Section 5, we validate the performance of our mechanism using a drone delivery dataset generated based on a real drone dynamics model from Airbus. In Section 6, we study an additional AAM scenario of vertiport reservation for air-taxi services in Northern California. We conclude this study in Section 7 and state some interesting directions for future work. Proofs and additional supplementary material are provided in the appendix. In Section A, we present a simple example to illustrate the time-extended graph and constraints (2b)-(2c). In Section B, we provide proofs for all theoretical results discussed in this paper. Finally, in Section C, we consolidate all important notations used in this work in the form of a table."
https://arxiv.org/html/2411.03465v1,Digital Twin for Autonomous Surface Vessels: Enabler for Safe Maritime Navigation,"Autonomous surface vessels (ASVs) are becoming increasingly significant in enhancing the safety and sustainability of maritime operations. To ensure the reliability of modern control algorithms utilized in these vessels, digital twins (DTs) provide a robust framework for conducting safe and effective simulations within a virtual environment. Digital twins are generally classified on a scale from 0 to 5, with each level representing a progression in complexity and functionality: Level 0 (Standalone) employs offline modeling techniques; Level 1 (Descriptive) integrates sensors and online modeling to enhance situational awareness; Level 2 (Diagnostic) focuses on condition monitoring and cybersecurity; Level 3 (Predictive) incorporates predictive analytics; Level 4 (Prescriptive) embeds decision-support systems; and Level 5 (Autonomous) enables advanced functionalities such as collision avoidance and path following. These digital representations not only provide insights into the vesselâ€™s current state and operational efficiency but also predict future scenarios and assess life endurance. By continuously updating with real-time sensor data, the digital twin effectively corrects modeling errors and enhances decision-making processes. Since DTs are key enablers for complex autonomous systems, this paper introduces a comprehensive methodology for establishing a digital twin framework specifically tailored for ASVs. Through a detailed literature survey, we explore existing state-of-the-art enablers across the defined levels, offering valuable recommendations for future research and development in this rapidly evolving field.","Commercial shipping is a cornerstone of the global economy, enabling the transport of vast quantities of goods across oceans. In 2020, the maritime sector moved approximately 10,648 million tonnes of seaborne goods, emphasizing its critical role in international trade (UNCTD, 2021). However, the industryâ€™s rapid growth has also contributed to significant environmental challenges. Shipping currently accounts for over 1 billion tonnes of carbon dioxide emissions annually, representing roughly 2.5% of global emissions (International Maritime Organization, 2020). Due to rising environmental concerns, developing cost-effective and environmentally sustainable solutions in commercial shipping has become an urgent priority for many countries. One promising solution is autonomous operation. Autonomous vessels have the potential to revolutionize maritime operations by increasing efficiency and reducing environmental impact. These vessels can follow optimal navigational routes, maximize fuel efficiency, and lower CO2 emissions. Additionally, since human error is responsible for nearly 80% of maritime accidents (SÃ¡nchez-Beaskoetxea et al., 2021), autonomy could significantly enhance safety by satisfying strict safety protocols, detecting hazards beyond human capability, and making real-time decisions without fatigue. Despite these potential benefits, the transition to autonomous shipping introduces a range of technological challenges. Implementing autonomous surface vessels (ASVs) in real-world maritime environments, particularly in high-stakes scenarios such as deep-sea shipping with large cargo, poses substantial risks. Minor technological errors in such settings can lead to devastating accidents, environmental disasters, and financial losses. To mitigate these risks, the concept of a digital twin has emerged as a game-changing technology. A digital twin generates a virtual model of a vessel and its operating environment, enabling the simulation and evaluation of operational strategies prior to real-world implementation. This capability significantly reduces the risks associated with the introduction of autonomous systems by offering a safe testing ground for innovative technologies. By simulating a wide range of scenarios, digital twins can accelerate the deployment of autonomous shipping operations that are not only safer and more sustainable but also more cost-effective. However, viewing digital twins merely as computer-aided design (CAD) models or basic simulators is a narrow and restrictive perspective. To fully exploit the potential of digital twins in autonomous shipping, a more refined and context-specific understanding of the concept is essential. The current work seeks to address this by the following: â€¢ Defining digital twin and its capability levels for maritime applications: Digital twins are categorized into six levels (0 to 5), each representing increasing levels of sophistication. â€¢ Overview of current relevant technologies: A comprehensive overview of the current state of technologies essential for creating digital twins for ASVs is provided. These include advances in sensor integration, condition monitoring, predictive analytics, and advanced collision avoidance systems. â€¢ Identification of opportunities and challenges: The potential benefits and challenges of developing highly functional digital twins for autonomous shipping are identified. â€¢ Recommendations for stakeholders: Recommendations are offered to various stakeholders, outlining how they can maximize the value derived from digital twin technology in autonomous maritime systems in return of their contributions. These suggestions highlight priority areas for future research and practical strategies for technology development. For a better organization of the review and related perspective, this article is structured into five sections. The current introduction section outlined the significance of autonomous shipping and the necessity of digital twin technology to address current challenges. The following Section 2 explains the general concept of a digital twin, detailing its progression from Level 0 (standalone systems) to Level 5 (fully autonomous operations). In Section 3, the approach taken to compile and analyze existing research is described, ensuring a thorough examination of relevant studies. Section 4 is divided into subsections that explore each level of digital twin capability, from Level 0-Standalone to Level 5-Autonomous, discussing the advancements and applications at each stage. Finally, the article concludes with Section 5, summarizing key findings and proposing future directions for research in digital twin technology for autonomous maritime operations."
https://arxiv.org/html/2411.03380v1,A networked small-gain theorem based on discrete-time diagonal stabilityâ€ â€ thanks:The research of RO is partly supported by a Khazanov Scholarship and the Air Force Office of Scientific Research under Grant No.Â FA9550-23-1-017. The research of MM is partially supported by a research grant from the Israeli Science FoundationÂ (ISF).,"We present a new sufficient condition for finite-gain L2subscriptğ¿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT input-to-output stability of a networked system. The condition requires a matrix, that combines information on the L2subscriptğ¿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT gains of the sub-systems and their interconnections, to be discrete-time diagonally stable (DTDS). We show that the new result generalizes the standard small gain theorem for the negative feedback connection of two sub-systems. An important advantage of the new result is that known sufficient conditions for DTDS can be applied to derive sufficient conditions for networked input-to-output stability. We demonstrate this using several examples. We also derive a new necessary and sufficient condition for a matrix that is a rank one perturbation of a Schur diagonal matrix to be DTDS.","A powerful approach for analyzing large-scale or networked systems is based on deducing properties of the networked system by combining properties of the sub-systems and their interconnection pattern. In particular, the input-output analysis approach [14] is based on combining input-output properties of the sub-systems and their interconnections to deduce input-output properties of the networked system. This approach usually ignores the internal structure of the sub-systems that are described as input-output operators, and thus yields robustness to uncertainty in the dynamics and parameter values. Nevertheless, under suitable detectability and controllability conditions it is possible to deduce global asymptotic stability of the networked system (see, e.g., [13]). Important examples of the input-output approach include: (1) the small gain theorem [8, Chapter 5] that provides a sufficient condition for the input-to-output stability of the negative feedback interconnection of two sub-systems; (2) a condition that guarantees the passivity of a networked system based on continuous-time diagonal stability of a matrix that combines information about the passivity properties of the sub-systems and their interconnection structure (see the elegant presentation in [1]); and (3) small gain conditions for networked stability based on input-to-state stability, see e.g. [6, 9]. Here, we present a sufficient condition for L2subscriptğ¿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT input-to-output stability of a system composed of nğ‘›nitalic_n sub-systems interconnected via a linear network. The condition requires a matrix, that combines the L2subscriptğ¿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT gains of the sub-systems and the interconnection strengths, to be discrete-time diagonally stable (DTDS). We show that this condition is a generalization of the classical small-gain condition for L2subscriptğ¿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT input-to-output stability. We stress that while our condition is based on discrete-time diagonal stability, the result is applicable to both discrete-time and continuous-time networked systems. The remainder of this note is organized as follows. The next section reviews known definitions and results that are used later on. Section III presents the main result and its proof. An important advantage of the new result is that it allows to use known conditions for DTDS to derive conditions for L2subscriptğ¿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT input-to-output stability of a networked system. Section IV demonstrates this using several applications. We also derive a new necessary and sufficient condition for a matrix that is a rank one perturbation of a Schur diagonal matrix to be DTDS. The final section concludes and describes possible directions for further research. We use standard notation. Small [capital] letters denote vectors [matrices]. For a vector vâˆˆâ„nğ‘£superscriptâ„ğ‘›v\in\mathbb{R}^{n}italic_v âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, diagâ¡(v)diagğ‘£\operatorname{diag}(v)roman_diag ( italic_v ) is the nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n diagonal matrix with visubscriptğ‘£ğ‘–v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT at entry (i,i)ğ‘–ğ‘–(i,i)( italic_i , italic_i ). If vi>0subscriptğ‘£ğ‘–0v_{i}>0italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > 0 for all iğ‘–iitalic_i then diagâ¡(v)diagğ‘£\operatorname{diag}(v)roman_diag ( italic_v ) is called a positive diagonal matrix. The transpose of a matrix Ağ´Aitalic_A is AâŠ¤superscriptğ´topA^{\top}italic_A start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT, and |A|ğ´|A|| italic_A | is the matrix obtained from Ağ´Aitalic_A be replacing every entry by its absolute value. For a square matrix Ağ´Aitalic_A, det(A)ğ´\det(A)roman_det ( italic_A ) is the determinant of Ağ´Aitalic_A. The maximal [minimal] eigenvalue of a symmetric matrix Sğ‘†Sitalic_S is denoted by Î»maxâ¢(S)subscriptğœ†ğ‘†\lambda_{\max}(S)italic_Î» start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ( italic_S ) [Î»minâ¢(S)subscriptğœ†ğ‘†\lambda_{\min}(S)italic_Î» start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( italic_S )]. A matrix Pâˆˆâ„nÃ—nğ‘ƒsuperscriptâ„ğ‘›ğ‘›P\in\mathbb{R}^{n\times n}italic_P âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT is called positive-definite, denoted Pâ‰»0succeedsğ‘ƒ0P\succ 0italic_P â‰» 0, if Pğ‘ƒPitalic_P is symmetric and xâŠ¤â¢Pâ¢x>0superscriptğ‘¥topğ‘ƒğ‘¥0x^{\top}Px>0italic_x start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_P italic_x > 0 for all xâˆˆâ„nâˆ–{0}ğ‘¥superscriptâ„ğ‘›0x\in\mathbb{R}^{n}\setminus\{0\}italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 }. In this case, P1/2superscriptğ‘ƒ12P^{1/2}italic_P start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT is the unique positive-definite matrix such that P1/2â¢P1/2=Psuperscriptğ‘ƒ12superscriptğ‘ƒ12ğ‘ƒP^{1/2}P^{1/2}=Pitalic_P start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT italic_P start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT = italic_P. A matrix Pâˆˆâ„nÃ—nğ‘ƒsuperscriptâ„ğ‘›ğ‘›P\in\mathbb{R}^{n\times n}italic_P âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT is called negative-definite, denoted Pâ‰º0precedesğ‘ƒ0P\prec 0italic_P â‰º 0, if âˆ’Pğ‘ƒ-P- italic_P is positive-definite. We use âˆ¥â‹…âˆ¥:â„nâ†’â„â‰¥0\|\cdot\|:\mathbb{R}^{n}\to\mathbb{R}_{\geq 0}âˆ¥ â‹… âˆ¥ : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT to denote the Euclidean norm, and â€–Aâ€–:=maxâ€–xâ€–=1â¡â€–Aâ¢xâ€–assignnormğ´subscriptnormğ‘¥1normğ´ğ‘¥\|A\|:=\max_{\|x\|=1}\|Ax\|âˆ¥ italic_A âˆ¥ := roman_max start_POSTSUBSCRIPT âˆ¥ italic_x âˆ¥ = 1 end_POSTSUBSCRIPT âˆ¥ italic_A italic_x âˆ¥ to denote the induced matrix norm. Then â€–Aâ€–2=Î»maxâ¢(AâŠ¤â¢A)superscriptnormğ´2subscriptğœ†superscriptğ´topğ´\|A\|^{2}=\lambda_{\max}(A^{\top}A)âˆ¥ italic_A âˆ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_Î» start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ( italic_A start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT italic_A ). The non-negative orthant in â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is â„â‰¥0n:={xâˆˆâ„n|xiâ‰¥0â¢ for all â¢i}assignsubscriptsuperscriptâ„ğ‘›absent0conditional-setğ‘¥superscriptâ„ğ‘›subscriptğ‘¥ğ‘–0 for all ğ‘–\mathbb{R}^{n}_{\geq 0}:=\{x\in\mathbb{R}^{n}\,|\,x_{i}\geq 0\text{ for all }i\}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT := { italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT | italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT â‰¥ 0 for all italic_i }. The space of signals (thought of as time functions) u:â„â‰¥0â†’â„m:ğ‘¢â†’subscriptâ„absent0superscriptâ„ğ‘šu:\mathbb{R}_{\geq 0}\to\mathbb{R}^{m}italic_u : blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT â†’ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT which are piecewise continuous and satisfy â€–uâ€–Tsubscriptnormğ‘¢ğ‘‡\displaystyle\|u\|_{T}âˆ¥ italic_u âˆ¥ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT :=(âˆ«0TuâŠ¤â¢(t)â¢uâ¢(t)â¢dt)1/2assignabsentsuperscriptsuperscriptsubscript0ğ‘‡superscriptğ‘¢topğ‘¡ğ‘¢ğ‘¡differential-dğ‘¡12\displaystyle:=\left(\int_{0}^{T}u^{\top}(t)u(t)\mathrm{d}t\right)^{1/2}:= ( âˆ« start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_u start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT ( italic_t ) italic_u ( italic_t ) roman_d italic_t ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT <âˆabsent\displaystyle<\infty< âˆ for all T>0ğ‘‡0T>0italic_T > 0 is denoted L2,emsuperscriptsubscriptğ¿2ğ‘’ğ‘šL_{2,e}^{m}italic_L start_POSTSUBSCRIPT 2 , italic_e end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT."
https://arxiv.org/html/2411.04052v1,Koopman Operators for Global Analysis of Hybrid Limit-Cycling Systems: Construction and Spectral Propertiesâ€ â€ thanks:The work was partially supported by JSPS KAKENHI (Grant No. 23H01434) and JSPS Bilateral Collaborations (Grant No. JPJSBP120242202).,"This paper reports a theory of Koopman operators for a class of hybrid dynamical systems with globally asymptotically stable periodic orbits, called hybrid limit-cycling systems. We leverage smooth structures intrinsic to the hybrid dynamical systems, thereby extending the existing theory of Koopman operators for smooth dynamical systems. Rigorous construction of an observable space is carried out to preserve the inherited smooth structures of the hybrid dynamical systems. Complete spectral characterization of the Koopman operators acting on the constructed space is then derived where the existence and uniqueness of their eigenfunctions are ensured. Our results facilitate global analysis of hybrid dynamical systems using the Koopman operator.","Hybrid dynamical systems are the form of mathematical modeling of complex dynamics involving both continuous flows and discrete transitions [1, 2]. Global theory of hybrid dynamical systems, as a generalization of that of smooth dynamical systems, is still challenging in the modern systems and control theory: see, e.g., [3, 4]. In this paper, we develop a global theory of hybrid dynamical systems with asymptotically stable periodic orbits in the Koopman operator framework [5]. The Koopman operator for a dynamical system, defined as a linear operator on observables (functions on the state space of the system), offers a linear perspective that captures global information (all evolutions of states) of the system through its spectral properties. Its eigenfunctions reveal geometric properties of the state space globally [6]: for a system with an asymptotically stable periodic orbit, one specific eigenfunction captures the so-called isochrons [7], while others capture novel geometric features [8, 9] including isostables [10]. These results are established only for smooth dynamical systems, where smooth eigenfunctions are guaranteed to exist uniquely [11, 12]. No theoretical research of Koopman operators for global analysis of non-smooth or hybrid dynamical systems is reported although their statistical properties, such as erogodicity, have been studied using the Koopman operator in [13, 14]. Our research aims to develop a theory of Koopman operators for hybrid dynamical systems, which is parallel to that for smooth dynamical systems (see, e.g., [11]). In this paper, to leverage the smooth Koopman operator theory for systems with globally asymptotically stable limit cycles [11], called limit-cycling systems, we focus on hybrid dynamical systems with globally asymptotically stable periodic orbits [4, 15], called hybrid limit-cycling systems. The hybrid limit-cycling systems are prevalent in many physical and engineering applications, including robotic locomotion [16, 17], power generation [18], and neural spiking [19]. Technically, we focus on the approach developed in [20, 21], which introduce as a generalization of smooth manifolds of smooth dynamical systems the so-called hybrifold. This approach involves to glue together manifolds corresponding to state spaces of individual modes of a hybrid dynamical system, each with locally smooth dynamics, thereby enabling us to extend the theory of smooth systems to hybrid ones. This paper reports the theoretical foundation of Koopman operators for hybrid limit-cycling systems. We construct a (semi-)group of Koopman operators with suitable spectral properties while preserving a smooth structure of the hybrifold. Our first contribution is to present a rigorous treatment of the choice of observable space on which the Koopman operators act, and that preserves the smooth structure of the hybridfold. Within this chosen space, we present their complete spectral characterization that is our second contribution. Specifically, the existence and uniqueness of eigenfunctions of the Koopman operators are presented. The spectral characterization leads the existence result of linear embeddings for the hybrid limit-cycling systems. The rest of this paper is organized as follows: In Section II, we introduce a hybrid dynamical system with a hybrid limit cycle. In Section III, we define the Koopman operators and describe the structure of their observable space. In Section IV, we demonstrate the existence and uniqueness of the eigenfunctions of the Koopman operators. An illustrative example is introduced in Section V. Conclusions are presented in Section VI with a brief summary and future directions. The proofs of lemmas and theorems are included in Appendix B."
https://arxiv.org/html/2411.04046v1,Design and control of a robotic payload stabilization mechanism for rocket flights,"The use of parallel manipulators in aerospace engineering has gained significant attention due to their ability to provide improved stability and precision. This paper presents the design, control, and analysis of â€œ STEWIE â€, which is a three-degree-of-freedom (DoF) parallel manipulator robot developed by members of the thrustMIT rocketry team, as a payload stabilization mechanism for their sounding rocket, â€™Altairâ€™. The goal of the robot was to demonstrate the attitude control of the parallel plate against the continuous change in orientation experienced by the rocket during its flight, stabilizing the payloads. At the same time, the high gravitational forces (G-forces) and vibrations experienced by the sounding rocket are counteracted. A novel design of the mechanism, inspired by a standard Stewart platform, is proposed which was down-scaled to fit inside a 4U CubeSat within its space constraints. The robot uses three micro servo motors to actuate the links that control the alignment of the parallel plate. In addition to the actuation mechanism, a robust control system for its manipulation was developed for the robot. The robot represents a significant advancement in the field of space robotics in the aerospace industry by demonstrating the successful implementation of complex robotic mechanisms in small, confined spaces such as CubeSats, which are standard form factors for large payloads in the aerospace industry.","The successful use of payloads in space missions depends largely on ensuring their stability and orientation control during the launch and spaceflight phases. Vibration motion, G-forces and dynamic disturbances caused by the rocketâ€™s orientation during flight path can affect the integrity and functionality of the payload. To address these challenges, parallel manipulators have emerged as a promising solution, offering the potential to counteract these external perturbations and maintain precise alignment. The inspiration for the design of our robot STEWIE comes from the versatility and robustness of the six degrees-of-freedom (DoF) Stewart platform, which is widely used in various industrial and research applications. Our research aims to take advantage of the principles of the Stewart platform while simplifying the kinematic structure to create a three-degrees-of-freedom (DoF) parallel manipulator suitable for payload stabilization in thrustMITâ€™s sounding rocket: Altair. The main goal of this article is to present a comprehensive study of STEWIEâ€™s design, development and performance characteristics. We address the kinematic analysis of the manipulator and illustrate its ability to provide three degrees-of-freedom for precise positioning of the payload during rocket ascent and spaceflight. In addition, we are investigating the integration of different types of joints and micro-servo motors into STEWIEâ€™s structure to improve its parallel alignment with respect to the Earthâ€™s surface. We present the results of the various structural simulations carried out on STEWIE to ensure that it can function effectively in the harsh conditions it will experience during its flight inside the rocket. We also propose a computationally in-expensive but robust controller which has high response speeds and high movement accuracy. The research results show that STEWIE has exceptional potential as an effective payload stabilization system for sounding rockets such as Altair. The ability of the manipulator to actively counteract vibrational movements, G-forces and orientation changes ensures the integrity and reliable functionality of payloads during critical phases of space missions. Furthermore, its compact, lightweight and simple design contributes to the rocketâ€™s overall efficiency and improved payload capacity. I-A Contributions of Research The main contributions of this paper to the field of robotics and aerospace are the following:- â€¢ A novel low-complexity, pwm-controlled actuator based payload stabilization mechanism for rocket flights. â€¢ A computationally lightweight controller, demonstrating high accuracy and response time for critical robotic application especially in space industries. â€¢ A simple and robust custom tuning method for pid-controllers. â€¢ An open-source platform comprising mechanical design files (STEP format), details of electronic components, and real-time-operating-system (RTOS). I-B Oranisation of Paper The paper is organized as follows: a comprehensive study on related work and recent development in the the field of robotics, especially for aerospace applications and in payloads has been presented in section II. The design of the robot detailing its geometry, kinematics involved, various components used, actuation mechanism, along with communication interfaces, sensors and circuitry design are described in section III. A detailed description of the control architecture including the controllers used, are explained in section IV. A comprehensive description of the tuning methods for the controllers is given in section V. Analysis of the simulations, and actual rocket flight results are presented in section VI. Finally, we present our conclusion in section VII."
https://arxiv.org/html/2411.04027v1,Prototyping O-RAN Enabled UAV Experimentation for the AERPAW Testbed,"The Open Radio Access Network (O-RAN) architecture is reshaping the telecommunications landscape by enhancing network flexibility, openness, and intelligence. This paper establishes the requirements, evaluates the design tradeoffs, and introduces a scalable architecture and prototype of an open-source O-RAN experimentation platform within the Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW), an at scale testbed that integrates unmanned aerial vehicles (UAVs) with advanced wireless network technologies, offering experimentation in both outdoor testbed and emulation via a custom digital twin (DT). Through a series of aerial experiments, we evaluate FlexRIC, an open-source RAN Intelligent Controller, within the AERPAW hardware-software platform for network data monitoring, providing valuable insights into the proposed integration and revealing opportunities for leveraging O-RAN to create custom service based optimizations for cellular connected UAVs. We discuss the challenges and potential use cases of this integration and demonstrate the use of a generative artificial intelligence model for generating realistic data based on collected real-world data to support AERPAWâ€™s DT.","The advent of the Open Radio Access Network (O-RAN) architecture marked a significant advancement in telecommunications, enhancing flexibility, openness, and intelligence across diverse network configurations. O-RANâ€™s modular design, featuring open interfaces, supports real-time resource management and artificial intelligence (AI) driven optimizations, fostering interoperability and innovation. This allows operators to tailor networks to evolving demands, reduce operational costs, and enhance service delivery. Key components of the O-RAN architecture include the near-real time RAN Intelligent Controller (near-RT RIC) and the non-RT RIC. These components, O-RANâ€™s disaggregated RAN, and the open interfaces enhance network flexibility, reduce costs via competitive vendor markets, and improve network intelligence [1]. There are several efforts in the United States and the rest of the world to promote open RANs and O-RAN. One of the primary goals is vendor diversity, which spurs domestic deployment, allows customized network scaling with more control over cellular network technology and parameters, and supports technology adoption and evolution toward 6G in a given region or country. The Platforms for Advanced Wireless Research (PAWR) program in the United States plays a pivotal role in integrating O-RAN technologies into its diverse testbeds, providing critical infrastructure for exploring the practical applications of O-RAN in real-world settings. The PAWR program supports five Wireless Community Testbeds (WCTs) designed to provide a controlled environment where researchers and developers can experiment with and evaluate new wireless technologies, protocols, and applications. The Cloud Enhanced Open Software Defined Mobile Wireless Testbed for City-Scale Deployment (COSMOS) focuses on millimeter wave technology, providing an urban environment to explore advanced communications and collect data. The Platform for Open Wireless Data-Driven Experimental Research (POWDER) centers around sub-6 GHz deployments, offering a flexible, programmable environment for wireless experimentation and data collection. The Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW) integrates wireless communications with aerial platforms, exploring protocols for aerial networks and gathering relevant data. The Agriculture and Rural Communities (ARA) testbed focuses on providing connectivity solutions for rural and underserved areas, supporting a range of wireless technologies for exploring innovative approaches for broader network coverage. Colosseum is a large-scale wireless network emulator that offers a highly configurable environment for testing and evaluating advanced wireless technologies. These platforms feature software-defined radio (SDR) hardware and software along with specialized radio equipment. In the realm of wireless research, the O-RAN architecture is reshaping how WCTs are used for experimentation and data collection as exemplified by O-RAN use cases in POWDER such as RAN slicing [2]. At this time COSMOS, POWDER, and Colosseum offer O-RAN experiments while AERPAW and ARA consider offering them in the future [3]. The existing O-RAN capabilities available through WCTs are either for an emulated radio frequency (RF) channel or with no or limited mobility [4]. AERPAW offers distinctive advanced wireless research features, facilitating controlled three-dimensional (3D) mobility in an at-scale outdoor testbed and in a digital twin (DT). While [4] provides insights into some of the essential requirements for O-RAN experimentation within AERPAW, it does not actualize the integration of O-RAN into the testbed. Unmanned aerial vehicles (UAVs) require adaptive networks, which O-RAN facilitates, for reliable connectivity. This paper outlines the design requirements and evaluates the available deployment choices before proposing an architecture and prototype for constructing a reproducible open-source O-RAN experimentation platform for advanced wireless UAV research. This platform furnishes the necessary interfaces and performance metrics to facilitate extensive research opportunities involving cellular network-connected UAVs. Our proposed platform lays the groundwork and the first practical integration for pioneering O-RAN experiments in AERPAW, enabling research on AI-driven UAV communication, network, and trajectory optimization [5]. The contributions of this work are: â€¢ We describe the tradeoffs and design choices for a testbed that combines O-RAN components with AERPAWâ€™s capabilities, creating an open-source, experimental research platform for UAV communications (Section II). â€¢ We offer experimental 5G key performance indicators collected between a UAV and ground nodes and introduce a generative AI (GAI) method to generate new data samples based on collected data for AERPAWâ€™s DT, enabling further research and analysis (Section III). â€¢ We identify the key challenges in integrating O-RAN research capabilities into AERPAW and propose practical solutions and future research and development (R&D) opportunities for O-RAN enabled UAV communications (Section IV)."
https://arxiv.org/html/2411.03873v1,Biomechanics-Aware Trajectory Optimization for Navigation during Robotic Physiotherapy,"Robotic devices hold promise for aiding patients in orthopedic rehabilitation. However, current robotic-assisted physiotherapy methods struggle including biomechanical metrics in their control algorithms, crucial for safe and effective therapy. This paper introduces BATON, a Biomechanics-Aware Trajectory Optimization approach to robotic Navigation of human musculoskeletal loads. The method integrates a high-fidelity musculoskeletal model of the human shoulder into real-time control of robot-patient interaction during rotator cuff tendon rehabilitation. We extract skeletal dynamics and tendon loading information from an OpenSim shoulder model to solve an optimal control problem, generating strain-minimizing trajectories. Trajectories were realized on a healthy subject by an impedance-controlled robot while estimating the state of the subjectâ€™s shoulder. Target poses were prescribed to design personalized rehabilitation across a wide range of shoulder motion avoiding high-strain areas. BATON was designed with real-time capabilities, enabling continuous trajectory replanning to address unforeseen variations in tendon strain, such as those from changing muscle activation of the subject.","Injuries affecting the musculoskeletal system are pervasive in society due to increasing population longevity, strenuous manual labor, and greater engagement in sports [1, 2, 3]. At the forefront of these injuries, disorders impacting the shoulder and the rotator cuff are particularly common, with clinical literature reporting their prevalence to be as high as 22%percent2222\%22 % in the general population [1]. Overall, the demand for therapy and rehabilitation is large and expected to grow, exacerbating the problem posed by a lack of physical therapists. Moreover, the task of treating a complex biological mechanism, such as the shoulder, is challenging due to a fundamental lack of insights into rotator-cuff behavior. Physiotherapists need to move the joint safely, gradually increasing the patientâ€™s range of motion while physically supporting the patientâ€™s arm and avoiding re-injuries [4]. Consequently, simple and limited movements are prescribed, to limit both the risk of re-injury and the physical demands on the therapist to manually manipulate the patient safely. Robotic technologies can assist in addressing these two gaps during therapy: robots can reduce the manual burden on physiotherapists and provide new tools to monitor and improve rehabilitation outcomes. Overall, rehabilitative robots are already used successfully in post-stroke rehabilitation [5, 6, 7] and gait assistance [8, 9, 10]. However, robots to treat musculoskeletal injuries (such as rotator cuff tears) remain limited. In particular, previous work remains focused on relieving physiotherapists from physically manipulating the patient. Smooth rehabilitation trajectories can be guaranteed by focusing on the control of the movement of the rehabilitative robot itself [11], and therapeutic robotic movements can be learned from expert human demonstrations to automate manual patient manipulation [12]. However, these solutions do not directly account for the patientâ€™s biomechanics. Deeper insights into human musculoskeletal mechanics can enable robots to be aware of tissue and joint loads associated with injury risks, allowing them to operate autonomously or in collaboration with therapists to holistically improve the rehabilitation process [13]. An increasingly promising strategy is to leverage human musculoskeletal models and incorporate them directly into robot control loops [14, 15, 16, 17, 18, 19]. Figure 1: BATON combines musculoskeletal modeling and human-robot interaction to enable a robotic physiotherapist to plan therapeutic movements for its patients in real-time while considering tissue loading (i.e. strain) induced in the rotator cuff tendons in the shoulder. Musculoskeletal modeling has progressed greatly over the past decades, allowing researchers to estimate the activities of individual muscles involved in producing movement [20, 21, 22] and to perform predictive simulations of human motion [23, 24]. As computational models increasingly capture the inner workings of the human body, their applicability continues to expand, particularly in physical human-robot interaction and assistive devices. Recently, biomechanical models have been used to quantify assistance needed by a human operator [14, 15, 16], reduce human metabolic cost in walking-assistive devices [17, 18, 19], and even plan search-and-rescue robotic operations [25] or the motion of supernumerary robotics limbs [26]. Through computer simulations or offline usage of human models, these works demonstrated the importance and utility of including human biomechanics in robot planning and decision-making. However, they did not leverage high-fidelity biomechanical simulations to regulate human-robot interactions in scenarios that require online adaptation to human behavior. When real-time capabilities are necessary, purely kinematic models have been used to monitor human joint positions, velocities, and torques [27], or to develop controllers for hybrid neuroprostheses of the knee [28]. While lower-fidelity models allow for computational efficiency, they are unsuitable for insights into the musculoskeletal system during shoulder rehabilitation. Biomechanical quantities from a high-fidelity human model were recently used in an online model predictive control framework, achieving predictive control of a leg prosthesis for ergonomically safe walking [29]. Fast predictions of the userâ€™s knee torque were achieved through Bayesian interaction primitives, which could estimate the unobservable biomechanical values via the inclusion of a musculoskeletal model in the data augmentation offline. However, this study did not consider the mechanical behavior of the human tissues (e.g., muscles and tendons). Building on the previously discussed applications of musculoskeletal models, we can go further to gain insights into the underlying biological tissues that are targets of physical therapy. By estimating tissue loads from external measurements based on a high-fidelity model of the human shoulder, we introduced the concept of â€œstrain mapsâ€ [30] to capture the relationship between human pose and the strain (i.e., load) induced in the rotator cuff tendons. Abstracting the rotator cuff tendon strains into an intuitive and navigable map, we achieved offline graph-based motion planning for safe robotic-mediated shoulder rehabilitation [30]. However, the planning algorithm did not account for the dynamics of the human, resulting in trajectories that presented sudden direction changes and were potentially difficult to track. Accelerations imposed on the human shoulder were not monitored, and the trajectories were executed in an open loop. Employing the strain maps, we implemented a reactive impedance-control-based approach to physical human-robot interaction in rehabilitation that enabled a subject to perform therapeutic exercises while robotic assistance protected them from potentially dangerous poses [31]. The system would react only when dangerous movements were already initiated, without anticipating the subjectâ€™s trajectory to prevent abrupt movements and experience high corrective forces from the robot. Other researchers designed optimized trajectories for controlling an ankle rehabilitation robot, to minimize joint loading [32]. A custom ankle model was used to plan the robotâ€™s trajectory offline, making online adjustments based on tracking errors attributed to excessive joint stiffness. However, the human model was not considered during the adjustments, so the reference path could become suboptimal during execution. To the best of our knowledge, a high-fidelity model of the human musculoskeletal system has never been used to synthesize adaptive robotic rehabilitative movements in real-time through coupled predictive simulations of human dynamics and tissue loading. Our primary objective is to develop a Biomechanics-Aware Trajectory Optimization for the Navigation of human tissue strains during robotic physiotherapy (BATON). Our novel approach intends to directly couple a state-of-the-art biomechanical model of the human shoulder to robotic control, generating movements for predictive robotic-assisted rotator cuff tendon rehabilitation in real-time. The resulting optimized trajectories are realized by a collaborative robot arm that delivers the movement to a healthy human subject through safe impedance control, with gravity compensation for the human arm naturally and adaptively provided by the model. This allows personalized treatment in the early stages of post-traumatic rehabilitation of the rotator cuff, where the subject does not support the weight of their own arm. Our specific aims and contributions are: â€¢ develop an automated system (BATON) to navigate underlying human biomechanical outcomes in real-time during robotic-assisted physiotherapy; â€¢ design of an optimal control problem (OCP) for trajectory optimization that incorporates a real-time shoulder musculoskeletal model by decoupling skeletal dynamics and strain behavior; â€¢ analyse performance both in simulation and on a physical robotic system applied to a test subject to demonstrate 1) the effect of OCP parameters on the planned trajectory, 2) the quality of the executed paths, 3) the ability to re-plan online based on changing strain landscapes, 4) real-time computational performance. An intuitive analogy for navigating around potentially unsafe movements in human rehabilitation is the path planning of an autonomous ship in a bay where underwater reefs are present. To know which maneuvers allow the ship to navigate safely and efficiently toward a target position, we need both a dynamic model of the vessel and a map of the underwater reefs, providing non-obvious insights about the safest path. In our situation, the human shoulder dynamics play the role of the vessel, while the strain maps offer insights into what happens beneath the surface (specifically in the recovering tendons). Only by considering the two elements jointly can safe navigation be obtained."
https://arxiv.org/html/2411.03619v1,Real-Time Safe Bipedal Robot Navigation using Linear Discrete Control Barrier Functions,"Safe navigation in real-time is an essential task for humanoid robots in real-world deployment. Since humanoid robots are inherently underactuated thanks to unilateral ground contacts, a path is considered safe if it is obstacle-free and respects the robotâ€™s physical limitations and underlying dynamics. Existing approaches often decouple path planning from gait control due to the significant computational challenge caused by the full-order robot dynamics. In this work, we develop a unified, safe path and gait planning framework that can be evaluated online in real-time, allowing the robot to navigate clustered environments while sustaining stable locomotion. Our approach uses the popular Linear Inverted Pendulum (LIP) model as a template model to represent walking dynamics. It incorporates heading angles in the model to evaluate kinematic constraints essential for physically feasible gaits properly. In addition, we leverage discrete control barrier functions (DCBF) for obstacle avoidance, ensuring that the subsequent foot placement provides a safe navigation path within clustered environments. To guarantee real-time computation, we use a novel approximation of the DCBF to produce linear DCBF (LDCBF) constraints. We validate the proposed approach in simulation using a Digit robot in randomly generated environments. The results demonstrate that our approach can generate safe gaits for a non-trivial humanoid robot to navigate environments with randomly generated obstacles in real-time.","Humanoid robots exhibit great dexterity and agility to perform different locomotion activities [1, 2, 3]. They are expected to be deployed to real-life environments such as warehouses and assembly lines. Such clustered environments pose the challenge of navigating around obstacles while maintaining stable walking in real-time [4, 5]. Yet humanoid robots are inherently underactuated due to their unilateral ground contacts; thus, a strong coupling exists between path planning and gait control. Their high-dimensional, nonlinear, and hybrid dynamics further complicate real-time motion planning. For fully actuated legged robots, one can decouple the path planning problem from motion control by finding a collision-free path without accounting for the robot dynamics and motion control [6, 7]. Then, a feedback controller aware of the robot dynamics can be developed to track the provided path. However, under-actuated humanoids would fall if the planned path did not account for the robot dynamics. This coupling of planning and dynamics usually requires solving specific gait optimization problems based on the robotâ€™s full-order [8, 9, 10] or reduced-order model [11, 12, 3]. However, using the full-order model for long-horizon path planning requires significant computation time, making them non-amenable for real-time online planning. Figure 1: Safe navigation planning is tested in MuJoCo with the Digit robot. To mitigate the computational challenge, reduced-order template models are often used to approximate the walking dynamics of the robot and plan gaits with reduced computational burden [13, 14, 15]. A famous example of such template models is the linear inverted pendulum (LIP) model [16]. In particular, the LIP model represents the robot dynamics using the center of mass (CoM) position and velocity and allows the control of the CoM velocity at the end of the next step. The LIP model provides a lower-dimensional representation of the robot dynamics that can be used to plan a desired foot placement that renders stable walking gaits. In this work, we introduce a modified 3D-LIP model with heading angles to use the LIP model for both path and gait planning and enforce necessary kinematic constraints. Including heading angles and turning rates allows for expressing kinematic constraints in the local coordinate frame of the robot, appropriately addressing the different motion constraints in sagittal and frontal planes, as well as left foot stance and right foot stance, of walking robots. A model predictive control based on the discrete-time step-to-step dynamics of this 3D-LIP model, denoted as LIP-MPC, is proposed to unify path and gait planning. In particular, our MPC formulation uses discrete control barrier functions (DCBFs) for safe obstacle avoidance. Control barrier functions have been successfully applied to controlling legged robots and are now widely used to ensure safety in path planning [17, 18, 19, 20]. DCBF, particularly, is well-suited for the discrete-time step-to-step dynamic model [21, 22]. However, many barrier functions that describe obstacles are nonlinear, leading to nonlinear constraints when foot placement is treated as the decision variable. Hence, despite the 3D-LIP model being linear, the nonlinearity in both kinematic and path constraints hinders the real-time computation of the MPC problem. To mitigate computational efficiency, this paper proposes two key ingredients: pre-computing heading angles and approximated linear DCBFs. Preprocessing the turning rate for each prediction step allows us to linearize the kinematic constraints within the MPC. Additionally, we introduce a novel linear discrete-time Control Barrier Function (LDCBF) to establish linear, feasible obstacle avoidance constraints for convex obstacles. These adjustments transform the optimization problem into a linearly constrained quadratic programming (QP) problem, significantly reducing computational demands and enabling real-time, safety-critical navigation for bipedal robots. The rest of the paper is organized as follows: Section II introduces the 3D-LIP model with heading angles and presents the formulation of the LIP-MPC with linear kinematic constraints for feasible gait planning. Section III details the design of the obstacle avoidance constraints, introducing the novel LDCBF expression. Section IV shows the application of the proposed LIP-MPC in simulation, showcasing the real-time safe navigation of the bipedal robot Digit. We presented the results of two different turning rate preprocessing strategies: global goal-oriented and subgoal-oriented. Finally, Section V concludes the contributions, limitations, and future work."
https://arxiv.org/html/2411.03581v1,Can Robotic Cues Manipulate Human Decisions? Exploring Consensus Building via Bias-Controlled Non-linear Opinion Dynamics and Robotic Eye Gaze Mediated Interaction in Human-Robot Teaming,"Although robots are becoming more advanced with human-like anthropomorphic features and decision-making abilities to improve collaboration, the active integration of humans into this process remains under-explored. This article presents the first experimental study exploring decision-making interactions between humans and robots with visual cues from robotic eyes, which can dynamically influence human opinion formation. The cues generated by robotic eyes gradually guide human decisions towards alignment with the robotâ€™s choices. Both human and robot decision-making processes are modeled as non-linear opinion dynamics with evolving biases. To examine these opinion dynamics under varying biases, we conduct numerical parametric and equilibrium continuation analyses using tuned parameters designed explicitly for the presented human-robot interaction experiment. Furthermore, to facilitate the transition from disagreement to agreement, we introduced a human opinion observation algorithm integrated with the formation of the robotâ€™s opinion, where the robotâ€™s behavior is controlled based on its formed opinion. The algorithms developed aim to enhance human involvement in consensus building, fostering effective collaboration between humans and robots. Experiments with 51515151 participants (N=51ğ‘51N=51italic_N = 51) show that human-robot teamwork can be improved by guiding human decisions using robotic cues. Finally, we provide detailed insights on the effects of trust, cognitive load, and participant demographics on decision-making based on user feedback and post-experiment interviews.","1. INTRODUCTION In recent years, the lack of safe and efficient human-robot collaboration in industrial environments has promoted negative perceptions of robots among human workers, substantially preventing the adoption of robotic technologies (Åemsettin Ã‡iÄŸdem et al., 2023; Welfare et al., 2019). As depicted in Figure 1, robots are still restricted to safety cages, reinforcing the physical segregation between robots and human-operated work spaces. Even in routine industrial assembly tasks, critical thinking and decision-making are often required, emphasizing the need for human oversight and collaboration, as robots alone are insufficient (Pfeiffer, 2016). While robots excel at repetitive tasks, humans are better suited for decision-making in uncertain conditions, necessitating mutual collaboration and co-learning between humans and robots. These insights highlight the need for safe human-robot collaboration to optimize industrial operations and the urgent need for new research into efficient, secure interactions. Figure 1. Problem-motivation example illustrating the real-world challenge of robotic arms operating in safety cages within a can packaging warehouse, highlighting the lack of direct human-robot collaboration and the separation of workspace. The existing literature often overlooks the crucial role of human contribution in human-robot teaming, instead focusing on outsmarting or handling uncertainties in human actions, which are often considered partially known information. As a result, robot control behavior is frequently designed using techniques such as partially observable Markov decision processes (or POMDPs), predictive model control, or reinforcement learning. However, human behavior is inherently unpredictable and mainly irrational (Johnson, 2021). In the absence of strategic guidance or communication toward a collaborative decision, human participants can engage in behaviors that trick, test, or attempt to surpass the capabilities of their robotic counterparts. In recent work, robots regulate human behavior using haptic feedback systems (Grushko et al., 2021) and a combination of visual cues and haptic wristbands (Habibian et al., 2023) to communicate intentions of movement, prompting workers to adjust their actions, thereby preventing collisions. However, the scalability of this approach is limited in complex industrial environments with frequent human-robot interactions, and it may struggle with highly dynamic tasks where trajectories and actions are constantly changing. In (Breazeal, 2002), the robot regulates human behavior through expressive facial expressions, body posture, and vocal signals to convey its effective state. This creates a feedback loop where the human adjusts their decisions based on the robotâ€™s cues, resulting in synchronization of body language and effective alignment. A key limitation here is the difficulty of sustaining nuanced human-robot interactions over longer durations in unpredictable environments, where emotional synchronization becomes more challenging. Furthermore, the limited anthropomorphic characteristics of the robot can reduce its overall expressive capability. A more detailed examination of robot communication and clues to humans, specifically through robotic eyes, is provided in Subsections 2.2 and 2.4 of Section 2. The process by which human opinions are formed during interactions is a crucial aspect of human-robot collaboration. Robotic cues and communication strategies must be dynamically generated and evolve in response to these opinions. Since human opinion formation is highly non-linear and influenced by various psychological factors that are not yet fully understood, integrating these factors, such as continuously evolving biases and opinions, into robot behavior presents a significant challenge. Balancing robot autonomy with human control in interactive scenarios is equally complex. Given the diversity of human behavior and the variability in individual beliefs, robot opinions must dynamically adapt to human responses and uncertainties in real time, making tuning the parameters of the decision-making model both critical and complex. Moreover, developing robots that can guide human decisions through cues while simultaneously exhibiting anthropomorphic traits and human-like decision-making abilities introduces additional complexities. This requires a robust theoretical understanding of how opinions shaped by robotic cues, acting as external stimuli, lead to agreement or disagreement under a given set of parameters for human and robot mental models, as well as the conditions that give rise to these outcomes. Incorporating humans into robotic decision-making is crucial and fosters nuanced interactions, where humans not only provide real-time feedback but also adapt their strategies based on the robotâ€™s guidance, enhancing learning for both. In ethically sensitive areas such as healthcare, integrating human oversight in robotic decision-making is important and not only ensures that moral considerations guide outcomes but also enhance trust and safety. By maintaining human control, especially in high-stakes environments, we can provide a crucial fail-safe against potential robotic errors, ensuring that decisions reflect ethical reasoning and mitigate risks effectively. Motivated by these challenges and the identified research gap in guiding human behavior during interactions with robots, as well as the numerous benefits of human involvement in human-robot collaboration, this research investigates the following key research questions. a) RQ1: Can bias or external stimulus be deliberately imparted to human decision making as a psychological factor to effectively achieve consensus with robot during interaction? b) RQ2: If assimilated, how do humans perceive and react to biases conveyed through robotic eye cues in terms of trust, persuasion, and collaboration? c) RQ3: Does persistent disagreement in early interactions motivate humans to subsequently align their choices with those of robots when mediated by visual cues as external biasing factor? d) RQ4: Under a chosen specified set of parameters, can non-linear opinion dynamics, representing both human and robot mental models, with and without bias control, accurately model real-time robot decisions and achieve controlled consensus or dissensus in response to human actions? e) RQ5: Can the robotic eye design presented in this paper serve as an effective anthropomorphic mechanism to guide humans toward a specific collaborative choice with the robot? To establish a foundation for investigating these questions, we first iteratively refined the non-linear opinion dynamics parameters through trial and error, for the experimental settings outlined in Section 3. Under the chosen optimized parameters, we demonstrate how bias can be used as a control parameter in a two-agent, two-choice non-linear opinion system to facilitate a transition from repeated disagreement to agreement. Through numerical parametric and equilibrium continuation analysis, we identified the critical bias settings that enable the transition to both consensus and dissensus. Then, to facilitate consensus between humans and robots, whose opinions evolve dynamically during interactions, we introduced a Bias Control Algorithm that continuously adjusts robot and human biases to align with the collaborative convergence choice of both agents. Through the development of a choice-based decision-making experiment involving a human and a robotic arm, acting as either cooperative or competitive agents, we explored research questions (RQ1-RQ5). The robot was proactively controlled using a behavior control algorithm, and we demonstrated how real-time human opinions can be observed and manipulated through visual cues from a robotic eye as a bias during interactions. The rest of this article is organized as follows: Section 2 provides a comprehensive review of both past and recent related work, delineating the specific contributions this article makes to the field. Section 3 details the experimental setup, including component design and the procedural methodology. Section 4 presents the bias parametric analysis for the selected non-linear dynamics parameter set, along with the proposed opinion formation and robot behavior algorithms. Section 5 presents the incorporation of human into co-learning interactions with the robot, through the use of robotic eye gaze. The subsequent sections 6, 7, and 8 present experimental demonstrations, insights from user feedback analysis, and concluding remarks, respectively."
https://arxiv.org/html/2411.03501v1,The Python LevelSet Toolbox (LevelSetPy),"This paper describes open-source scientific contributions in python surrounding the numerical solutions to hyperbolic Hamilton-Jacobi (HJ) partial differential equations viz., their implicit representation on co-dimension one surfaces; dynamics evolution with levelsets; spatial derivatives; total variation diminishing Runge-Kutta integration schemes; and their applications to the theory of reachable sets. They are increasingly finding applications in multiple research domains such as reinforcement learning, robotics, control engineering and automation. We describe the library components, illustrate usage with an example, and provide comparisons with existing implementations. This GPU-accelerated package allows for easy portability to many modern libraries for the numerical analyses of the HJ equations. We also provide a CPU implementation in python that is significantly faster than existing alternatives.","I Overview The reliability of the modern automation algorithms that we design has become paramount given the dangers that may evolve if nominally envisioned system performance falters. Even so, the need for scalable and faster numerical algorithms in software for verification and validation has become timely given the emergence of complexity of contemporary systems. The foremost open-source verification software for engineering applications based on Hamilton-Jacobi (HJ) equations [1, 2] and levelset methods [3, 4] is the CPU-based MATLABÂ®-implemented levelsets toolbox [5], developed before computing via graphical processing units (GPU) became pervasive. Since then, there has been significant improvements in computer hardware and architecture design, code parallelization algorithms, and compute-acceleration on modern GPUs. This paper describes a python-based GPU-accelerated scientific software package for numerically resolving generalized discontinuous solutions to Cauchy-type (or time-dependent) HJ hyperbolic partial differential equations (PDEs). HJ PDEs arise in many contexts including (multi-agent) reinforcement learning, robotics, control theory, differential games, flow, and transport phenomena. We focus on the numerical tools for safety assurance (ascertaining the freedom of a system from harm) in a verification sense in this paper. Accompanying the package are implicit calculus operations on dynamic codimension-one interfaces embedded within â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT surfaces, and spatial and temporal discretization schemes for HJ PDEs. Furthermore, we describe explicit integration schemes including the Lax-Friedrichs, Courant-Friedrichs-Lewy (CFL), and total variation diminishing Runge-Kutta (or TVD-RK) conditioning schemes for HJ Hamiltonians of the form ğ‘¯â¢(ğ’™,ğ’‘)ğ‘¯ğ’™ğ’‘\bm{H}(\bm{x},\bm{p})bold_italic_H ( bold_italic_x , bold_italic_p ), where ğ’™ğ’™\bm{x}bold_italic_x is the state and ğ’‘ğ’‘\bm{p}bold_italic_p is the co-state. Finally, extensions to reachability analyses for continuous and hybrid systems, formulated as optimal control or game theory problems using viscosity solutions to HJ PDEs are described. All data transfers to the GPU are based on CuPy [6] framework. In all, we closely follow the Python Enhancement Proposals (PEP) 8 style guide111Python PEP 8 style guide: peps.python.org/pep-0008/; however, in order not to break readability with respect to the original MATLABÂ®code, we err in consistency with the MATLABÂ®project layout. The Python package and installation instructions are available on the authorâ€™s github repository: levelsetpy. The CPU implementation (in Python) is on the cpu-numpy tree of the repository. Extensions to other python GPU programming language are straightforward (as detailed in the CuPy interoperability document). While our emphasis is on the resolution of safe sets in a reachability verification context, the applications of this package extend beyond control engineering."
https://arxiv.org/html/2411.03481v1,Chance-Constrained Convex MPC for Robust Quadruped LocomotionUnder Parametric and Additive Uncertainties,"Recent advances in quadrupedal locomotion have focused on improving stability and performance across diverse environments. However, existing methods often lack adequate safety analysis and struggle to adapt to varying payloads and complex terrains, typically requiring extensive tuning. To overcome these challenges, we propose a Chance-Constrained Model Predictive Control (CCMPC) framework that explicitly models payload and terrain variability as distributions of parametric and additive disturbances within the single rigid body dynamics (SRBD) model. Our approach ensures safe and consistent performance under uncertain dynamics by expressing the modelâ€™s friction cone constraints, which define the feasible set of ground reaction forces, as chance constraints. Moreover, we solve the resulting stochastic control problem using a computationally efficient quadratic programming formulation. Extensive Monte Carlo simulations of quadrupedal locomotion across varying payloads and complex terrains demonstrate that CCMPC significantly outperforms two competitive benchmarks: Linear MPC (LMPC) and MPC with hand-tuned safety margins to maintain stability, reduce foot slippage, and track the center of mass. Hardware experiments on the Unitree Go1 robot show successful locomotion across various indoor and outdoor terrains with unknown loads exceeding 50% of the robotâ€™s body weight, despite no additional parameter tuning. A video of the results and accompanying code can be found at: https://cc-mpc.github.io/.","Quadrupedal robots have demonstrated significant potential in various industrial applications and search and rescue missions. These robots enhance productivity by transporting heavy loads and traversing diverse terrains [1]. However, preventing falls in dynamic environments remains a critical challenge [2]. Inaccurate system models, external disturbances, and unpredictable payload variations can cause deviations from planned motions, resulting in unintended contact locations or timing errors [3]. As shown in Fig. 1, classical Model Predictive Control (MPC) methods often struggle with these discrepancies, leading to instability, foot slippage, or even falls. Conventional model-based control methods for quadrupedal locomotion either fail to account for dynamics uncertainties, as in Differential Dynamic Programming (DDP) [4], or become computationally infeasible when doing so, as with Stochastic Linear Complementarity Problems (SLCPs) [5]. Model-free Reinforcement Learning (RL) aims to generalize robot locomotion strategies across diverse environments through offline training, followed by online deployment. While these neural network policies work well in practice, they often lack interpretability and may require frequent re-training to ensure reliable deployment [6]. In contrast, Stochastic Model Predictive Control (SMPC) directly incorporates uncertainties into the control design by modeling them as probability distributions of disturbances [7, 8]. Unlike deterministic MPC, SMPC permits a small probability of constraint violation. In our experiments, we set this probability to 5%, corresponding to a 2â¢Ïƒ2ğœ2\sigma2 italic_Ïƒ confidence level. As a result, 95% of outcomes are expected to remain within constraints under Gaussian disturbances [9, 10]. This formulation allows the controller to balance conflicting objectives, such as following a desired trajectory while mitigating unstable behaviors across a range of real-world terrain disturbances. Figure 1: Chance-Constrained MPC (bottom) stabilizes the robot by handling a distribution of inertial uncertainties from 6 kg dumbbells (1) and contact uncertainties, from planks (2). Linear MPC (top) fails under these conditions. In this work, we propose a novel Chance-Constrained MPC (CCMPC) algorithmâ€”a specific form of SMPCâ€”to generate ground reaction forces for quadrupedal robots. Our approach models mass, inertia, and contact sequences as stochastic variables. By formulating the control problem as a quadratic programming (QP) problem [11], we achieve real-time solve rates at âˆ¼similar-to\simâˆ¼500 Hz, comparable to Linear MPC (LMPC). We validate CCMPC through extensive simulations and hardware experiments on the Unitree Go1 robot. Our approach achieves superior performance over traditional methods in maintaining stability, reducing foot slippage, and supporting payloads exceeding 50% of the robotâ€™s weight across muddy slopes, stairs, grass, and gravel. This is accomplished using a unified control policy that effectively handles different terrain conditions and payload variations without the need for parameter tuning. The key contributions of this paper are summarized as follows: â€¢ We develop a CCMPC algorithm tailored for quadrupedal robots to handle disturbances from variable payloads and complex terrain dynamics. â€¢ The control problem is formulated as a quadratic program, achieving fast solve times suitable for real-time application. â€¢ We validate our method through simulations and hardware experiments. In simulations, CCMPC achieves a 100% success rate across multiple gaits, compared to 39.2% for Linear MPC (LMPC) and 75.7% for hand-tuned MPC. To our knowledge, this is the first SMPC implementation for quadrupedal robots on hardware."
https://arxiv.org/html/2410.16383v1,Designing Robust Cyber-Defense Agents with Evolving Behavior Trees,"Modern network defense can benefit from the use of autonomous systems, offloading tedious and time-consuming work to agents with standard and learning-enabled components. These agents, operating on critical network infrastructure, need to be robust and trustworthy to ensure defense against adaptive cyber-attackers and, simultaneously, provide explanations for their actions and network activity. However, learning-enabled components typically use models, such as deep neural networks, that are not transparent in their high-level decision-making leading to assurance challenges. Additionally, cyber-defense agents must execute complex long-term defense tasks in a reactive manner that involve coordination of multiple interdependent subtasks. Behavior trees are known to be successful in modelling interpretable, reactive, and modular agent policies with learning-enabled components. In this paper, we develop an approach to design autonomous cyber defense agents using behavior trees with learning-enabled components, which we refer to as Evolving Behavior Trees (EBTs). We learn the structure of an EBT with a novel abstract cyber environment and optimize learning-enabled components for deployment. The learning-enabled components are optimized for adapting to various cyber-attacks and deploying security mechanisms. The learned EBT structure is evaluated in a simulated cyber environment, where it effectively mitigates threats and enhances network visibility. For deployment, we develop a software architecture for evaluating EBT-based agents in computer network defense scenarios. Our results demonstrate that the EBT-based agent is robust to adaptive cyber-attacks and provides high-level explanations for interpreting its decisions and actions.","Modern network defense is an increasingly difficult task due to a multitude of novel and diverse cyber-attacks and the scale of systems. Developers have crafted solutions such as alert monitors and decision logic rules to alleviate human cognitive fatigue, but this is not viable or scalable as new attacks are discovered. The capabilities and strategies of cyber-defense continue to grow and, as such, fully autonomous cyber defense agents are being considered as an alternative to optimally utilize the resources needed to mitigate adversaries. However, there are challenges regarding the transparency of these agents and their robustness. Autonomous agents contain a mix of standard and learning-enabled components (LECs) trained with machine learning (ML) and reinforcement learning (RL). These LECs are normally modeled with neural networks that lack the ability to provide high-level explanations and struggle to complete long-term objectives with multiple subtasks. Cyber-defense breaks down into a variety of subroutines (analysis, monitoring, restoring, deploying decoys, etc.) that a single component will struggle to represent and optimize. Additionally, it is unclear how autonomous agents will adapt to multiple varieties of cyber-attacks. Adversaries are versatile, they may use one or more attacks on the system targeting multiple components. The adversary may use one strategy at the beginning to explore the system in a low-detection manner and then launch a full-scale attack after they have discovered enough knowledge. The autonomous agent will need to be robust to these scenarios and explain its current perspective of the adversaryâ€™s behavior. An effective way of representing a reactive control policy between subtasks in the hierarchy is using Behavior Trees (BTs) [2]. BTs are structures for modeling complex control policies with advantages of modularity, reactivity, and explainability. The reactive nature of the BT allows for explicit switching between multiple behaviors in quick succession depending on environment changes. The LECs and other components can be coordinated and modelled as a policy we refer to as an Evolving Behavior Tree (EBT). An EBT can jointly optimize and model the control flow of optimized components. The structure of the control of behaviors in an EBT is typically manually developed, but due to the complexity of multiple subtasks and their dependencies, construction can prove tedious and potentially infeasible. Recent work has focused on automatic construction of BTs using genetic programming (GP) [3, 4] or large-language models [5, 6]. In particular, the works in GP develop abstract environments for computational efficiency that map to a realistic simulation. In this paper, we develop an autonomous cyber-defense agent that leverages the hierarchical structure of EBTs for robustness against dynamic cyber-attacks. We design the agent in three stages: (1) learning the high-level control structure of the EBT, (2) optimization of LECs for robustness, and (3) integration and deployment to a realistic cyber environment. The research objective of (1) is to enhance scalability by generating control structures without requiring detailed knowledge of network components. This approach enables learning the modular structure of the EBT prior to optimizing LECs, thereby avoiding unnecessary retraining. For (1), we utilize GP and develop a novel abstract cyber-environment referred to as the Cyber-Firefighter to map to a realistic cyber-defense simulation scenario and evaluate the structural performance. The research objective of (2) is to develop generalizable behaviors that are robust to uncertainties in a realistic computer network environment. The EBT contains LECs for choosing cyber-agent actions and determining the red agent strategy based on a non-deterministic network state. We develop a software architecture for the construction and integration of an EBT for a computer network defense scenario. In (3), we deploy and evaluate our autonomous cyber-defense agent in a realistic simulation environment for robustness and explainability. The main technical contributions of our work are: â€¢ Design, optimization, and deployment of an EBT for autonomous cyber-defense of a computer network. The EBT structure is designed using GP with a novel abstract cyber environment, the Cyber-Firefighter, that maps to cyber-defense. The EBT structure is generalizable and contains capabilities for decoy deployment, attacker strategy detection, and selection of cyber-operations. â€¢ An evaluation of the learned EBT structure in the Cyber-Firefighter to demonstrate high-level control performance against an attacker in a network. The GP algorithm in tandem with the Cyber-Firefighter is successful at learning an EBT structure that maximizes the performance metric (fitness) to promote mitigation and visibility of an attack. â€¢ Development of a software architecture to support the construction and deployment of EBTs on a computer network. The architecture utilizes a blackboard of data sources in a publish-subscribe method to facilitate interaction between the EBT and computer network environment. The computer network environment in this paper is CybORG [7], an abstracted version of a computer network, compatible with ML and RL algorithms. â€¢ An evaluation of the robustness and explainability of the EBT in CybORG [7] using CAGE Challenge Scenario 2 [1]: a computer network task where the agent must defend against an adversarial agent. We develop an adversarial red agent for this scenario that switches strategies during execution to evaluate the adaptation of our approach. The EBT is successful at defending against dynamic attacks with a 39%percent3939\%39 % increase in the average reward compared to a state-of-the-art method in CybORG CAGE Challenge Scenario 2. The explainable nature of the EBT allows us to monitor key events, such as when the strategy switches or a decoy is deployed, and model transitions between high-level subtasks."
https://arxiv.org/html/2411.03288v1,Model Predictive Control of Collinear Coulomb Spacecraft Formations,"A model predictive control scheme to stabilize desired configurations of collinear Coulomb spacecraft formations is derived in this paper. The nonlinearities of the dynamics with respect to the input make this problem difficult to solve, computationally. It is shown that the nonlinearities in the input lead to a finite horizon optimization problem which is a nonconvex quadratically-constrained quadratic program (QCQP). A convex relaxation of the nonconvex QCQP is therefore derived which can be solved quickly using a convex optimization solver. A simulation of a four spacecraft formation is provided which demonstrates why optimizing over a prediction horizon is a prudent approach to Coulomb spacecraft formation control.","The lifespan of a spacecraft formation is limited by the amount of propellant that can be brought onboard to be used for stationkeeping and other formation control maneuvers. As a measure to reduce propellant requirements for spacecraft formation control, Schuab et al. [1] first proposed the prospect of using the inter-spacecraft Coulomb forces for formation control in high orbits. By controlling the charge of the spacecraft in a formation, the Coulomb forces acting between the spacecraft can be controlled and this can, therefore, be used for formation control. Altering the charge of a spacecraft can be accomplished using negligible amounts of propellant. This, therefore, opens the possibility of spacecraft formation control using negligible amounts of propellant. With the benefit of nearly propellantless formation control comes some technical challenges inherent in Coulomb spacecraft formation control. There are two main difficulties: the first is the underactuated nature of Coulomb forces. The Coulomb forces act only along the line of sight between two charged spacecraft and come in action-reaction pairs. The Coulomb forces being internal forces cannot affect the angular momentum of the formation. In general, the use of conventional thrusters (or other external forces such as gravitational forces) in tandem with Coulomb actuation will be necessary to produce forces that cannot be achieved by Coulomb actuation alone [2, 3]. In this paper, the formations are constrained to be collinear because of this difficulty, and instead the focus of this paper is on addressing the second difficulty which is the nonlinear nature of the inputs. Recall that the Coulomb force is directly proportional to the product of the two charges. This means that in Coulomb spacecraft formation control, the equations of motion will be nonlinear in the the input, i.e. the charges. Most results in Coulomb formation control focus on the case of two spacecraft formations [1, 2, 3, 4]. In the two spacecraft case, the nonlinearity with respect to the input can be simplified by redefining the input as a product of the two charges. The dynamics are then linear in the charge product and it is trivial to compute two individual charges that form a given charge product. For formations with more than two spacecraft, a simple redefinition of the variables in terms of products of charges cannot be done because constraints need to be added to ensure that a given combination of charge products is realizable. For example, in a three spacecraft formation it is impossible for all three charge products to be negative. Moreover, the magnitudes of the charge products are inter-dependent. Generally, there are two approaches that have been taken for Coulomb spacecraft formation control with formations of more than two spacecraft: the switching approach and the analytical approach. The switching approach exploits the fact that it is simpler to design controllers for Coulomb spacecraft formations with two spacecraft. The idea is to control two of the spacecraft at a time and then switch pairs according to some switching law [5, 6]. The difficulty with this approach is that the switching law is very difficult to design and it becomes increasingly difficult as the number of spacecraft in the formation increases. It is then difficult to predict and overcome undesirable behaviors such as Zeno switching which can frequently arise when using state-dependent switching [7]. The analytical approach involves deriving an analytical expression of the feedback control law. To simplify the dynamics to design a control law, constraints on the symmetry, shape, and charges will often be made. These constraints will often limit the scalability or applicability of the analytical control laws. Hussein and Schaub [8] impose symmetry constraints on collinear three spacecraft formations which enable them to reduce the number of inputs by then also constraining two of the spacecraft to have equal charges at all times. In Jones and Schuab [9], a control law is derived by linearizing about a particular equilibrium and considering small deviations of the input from the equilibrium input. Their controller is then derived using linear control techniques. In Tahir and Narang-Siddarth [10], a control law is derived for a collinear three spacecraft formation without performing any linearization; however, the resulting control law is a large symbolic expression and its use of dynamic inversions makes the control law difficult to scale for larger formations and nonrobust. For four spacecraft formations, Vasavada and Schaub [11] constrain the formations to be square formations. Lastly, Pettazzi et al. [12], derive a control law by disregarding the input nonlinearity and then perform a projection. The approach in this paper for the stabilization of spacecraft formations of three spacecraft or more is unique from the state-of-the-art in that it is a completely optimization-based feedback control scheme. The approach addresses the nonlinearity with respect to the input without any constraints of symmetry or shape beyond collinearity. The approach in this paper is scalable in the sense that the optimization programs derived are independent of the number of spacecraft in the formationâ€“the only difference with larger formations is that the size of the optimization variables and constraints will increase. Model predictive control (MPC) has received a lot of attention in aerospace applications in recent years (see the surveys [13, 14, 15]); however, there has been no attention given to Coulomb spacecraft formation control using MPC. This paper is the first step to filling in that gap. The paper is organized as follows: Â§II states the problem to be solved formally and writes out the dynamics of the Coulomb spacecraft formation. Â§III derives the MPC algorithm by discretizing the dynamics, stating a nonconvex finite horizon optimization problem, and then deriving a convex relaxation of the finite horizon optimization problem. Â§IV provides a simulation of a four spacecraft formation and discusses the resulting control behavior as well as the computational burden of the MPC algorithm. Â§V concludes the paper. I-A Notation Let â„â„\mathbb{R}blackboard_R and â„¤â„¤\mathbb{Z}blackboard_Z denote the sets of real numbers and integers, respectively. The positive real numbers and integers are denoted by â„+subscriptâ„\mathbb{R}_{+}blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT and â„¤+subscriptâ„¤\mathbb{Z}_{+}blackboard_Z start_POSTSUBSCRIPT + end_POSTSUBSCRIPT, respectively. The transpose is denoted using a superscript âŠ¤top\topâŠ¤. The trace of a matrix Mğ‘€Mitalic_M is denoted by Trâ¢(M)Trğ‘€\text{{Tr}}(M)Tr ( italic_M ) and its rank is denoted by rankâ¢(M)rankğ‘€\text{rank}(M)rank ( italic_M ). A matrix Mğ‘€Mitalic_M is positive semidefinite if it symmetric and its eigenvalues are nonnegative, and Mâª°0succeeds-or-equalsğ‘€0M\succeq 0italic_M âª° 0 denotes that Mğ‘€Mitalic_M is positive semidefinite. The nÃ—nğ‘›ğ‘›n\times nitalic_n Ã— italic_n identity matrix is denoted by Insubscriptğ¼ğ‘›I_{n}italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and the vector of dimension nğ‘›nitalic_n of all ones is denoted by ğŸnsubscript1ğ‘›{\bf 1}_{n}bold_1 start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. If xğ‘¥xitalic_x and yğ‘¦yitalic_y are vectors of the same dimension, then xâ‰¥yğ‘¥ğ‘¦x\geq yitalic_x â‰¥ italic_y denotes elementwise inequalities."
https://arxiv.org/html/2411.03189v1,Energy-Aware Predictive Motion Planning for Autonomous Vehicles Using a Hybrid Zonotope Constraint Representation,"Uncrewed aerial systems have tightly coupled energy and motion dynamics which must be accounted for by onboard planning algorithms. This work proposes a strategy for coupled motion and energy planning using model predictive control (MPC). A reduced-order linear time-invariant model of coupled energy and motion dynamics is presented. Constrained zonotopes are used to represent state and input constraints, and hybrid zonotopes are used to represent non-convex constraints tied to a map of the environment. The structures of these constraint representations are exploited within a mixed-integer quadratic program solver tailored to MPC motion planning problems. Results apply the proposed methodology to coupled motion and energy utilization planning problems for 1) a hybrid-electric vehicle that must restrict engine usage when flying over regions with noise restrictions, and 2) an electric package delivery drone that must track waysets with both position and battery state of charge requirements. By leveraging the structure-exploiting solver, the proposed mixed-integer MPC formulations can be implemented in real time.","There has been a rising interest in the potential of autonomous electric and hybrid-electric uncrewed aerial systems (UAS) in the aviation industry. Applications include aerial package delivery vehicles [1] and air taxis or ambulances for urban air mobility (UAM)[2, 3]. Energy usage and environmental constraints, such as restrictions on aircraft noise, present significant technological challenges for these systems [3]. To address these challenges, autonomous planning algorithms must be able to account for energy utilization in addition to vehicle motion. I-A Gaps in the Literature Existing work on energy-aware planning has focused on incorporating energy considerations into high-level path planning algorithms, often using graph-based approaches such as A* or Dijkstraâ€™s algorithm [4]. Graph search algorithms are used to perform high-level, energy-aware planning for hybrid-electric UAS under energy and noise constraints in [5, 6, 7], and they are applied to energy-constrained planning for package delivery drones in [8, 9]. However, there are several key challenges when attempting to integrate high-level planners with lower-level path followers, such as discrepancies in model assumptions between the path planner and follower [10]. Intermediate-level motion planning algorithms are often used to bridge the gap between high-level planners and low-level controllers. The role of such algorithms is to locally plan system trajectories, typically using a reduced-order model of the system [11]. Introducing energy considerations into these intermediate-level planners has received comparatively little attention in the literature despite the extensive literature on UAS motion planning when energy system dynamics are not considered [12]. Energy and motion dynamics have been incorporated into Model Predictive Control (MPC) formulations, which are often used for motion planning [13]. Energy dynamics were included in a hierarchical MPC controller with one spatial dimension and convex state constraints in [14], and a terminal battery state of charge constraint was used within an MPC path planner/follower in [15]. In [16], MPC is used for UAS motion planning and control. Here, battery state of charge is maximized while adhering to obstacle avoidance constraints that are imposed using potential functions, and the system dynamics are linearized about an equilibrium condition. The resulting energy-aware motion plans may be vulnerable to entrapment in suboptimal local minima given the nonlinear programming formulation and local linearization. Increasingly, MPC optimization problems for motion planning are formulated as mixed-integer programs (MIPs) because non-convex constraint sets (e.g., an obstacle map) can be exactly represented in MIPs andâ€”for mixed-integer convex programsâ€”convergence to a global optimum is guaranteed [17]. MIPs are NP-hard [18] however, which inhibits the application of these methods in a real-time context. I-B Contributions This paper presents an efficient method for energy-aware motion planning of uncrewed aerial systems. A mixed-integer set representation, the hybrid zonotope, is used to exactly represent a non-convex constraint set that defines obstacles and regions with location-specific noise restrictions. A reduced-order, linear time-invariant model of the coupled UAS energy and motion dynamics is developed that is globally valid and conservative with respect to planned energy usage. Motion and energy states are coupled via a polytopic constraint set, and a low-complexity constrained zonotope representation of this set is presented. An MPC controller is formulated that plans system trajectories which adhere to specifications on both the motion and energy states. A mixed-integer quadratic program (MIQP) solver developed in our previous work [19, 20]111These references will be published or uploaded to arXiv before the final ACC deadline. A link to the manuscripts has been provided in the references for review purposes. (previously applied for motion planning only and not energy management) is leveraged to efficiently solve these MPC optimization problems by exploiting the structure of the hybrid zonotope and constrained zonotope set representations. Case studies show how noise-restricted areas and terminal energy constraints can be considered by the proposed controller and highlight the utility of jointly optimizing energy and motion plans."
https://arxiv.org/html/2411.03113v1,"Minimum Radiative Heat and Propellant Aerocapture Guidance with Attitude Kinematics Constraints111An initial version of this paper was presented at the 2018 AIAA Guidance, Navigation, and Control Conference, 8-12 January 2018, Kissimmee, Florida. Paper number AIAA 2018-1319.","To maximize the payload mass, an aerocapture trajectory should be flown in such a way that both the final Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V and the total heat load are minimized. For some aerocapture missions, the heating due to radiation of high temperature gases in the shock-layer is so much larger than the heat due to convection, that the latter is negligible. This paper provides analytical proof and numerical validation that radiative heat is minimized by the same trajectory that minimizes the final Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V: a single switch bang-bang trajectory, starting with full lift-up, full lift-down commands. Further, a novel guidance that plans a bang-bang trajectory with constraints in the attitude kinematics is introduced. While achieving similar performance as the current state-of-the-art, the inclusion of constraints in attitude kinematics allows for much less tuning. Finally, a lateral guidance that makes use of information on the final inclination of the predicted trajectory is introduced. Such guidance allows for very high accuracy in the inclination requirements with only two reversals, by requiring a single parameter to be tuned.","Aerocapture, first introduced by Cruz [1], is an atmospheric maneuver that can greatly facilitate missions to atmospheric celestial bodies [2]. Aerocapture achieves orbit insertion from a hyperbolic trajectory, as depicted in Fig. 1. By diving into the atmosphere, the energy of the spacecraft is reduced in a controlled way through the dissipative action of drag. After the desired amount of energy has been lost, the spacecraft exits the atmosphere and coasts to apoapsis. A small propulsive burn is then required to raise the periapsis above the region where drag is still significant. If needed a second smaller burn is later performed to correct any errors in the target apoapsis altitude. Figure 1: Scheme of an aerocapture. In red is the atmospheric part of the maneuver. An aerocapture maneuver has never been attempted, despite most of the technology being considered ready by several studies now [3, 4, 5]. Interest in aerocapture has lately been growing lately as it would be an enhancing technology for mission to Neptune and Uranus planets [6, 7, 8]; specifically, attractiveness comes from the fact that it would allow faster, higher energy trajectories, since less propellant mass would be required to decelerate. The mass reduction granted by decreased propellant consumption is partially offset by the requirement of a heat shield to protect the spacecraft while entering the atmosphere. Because of the extremely high temperatures encountered during this class of maneuvers, the heat shield is generally ablative, meaning that the mass increases for larger total heat load. Hence, an optimal aerocapture should not only minimize the Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V of the post-maneuver burns, but strike a balance between total heat load and total propellant consumption. The minimum Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V aerocapture follows a bang-bang trajectory, beginning with a full lift-up command, and with a single switch [9, 10]. The trajectory minimizing the Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V also minimizes both the heat flux peak and the peak dynamic pressure [11]. However such a trajectory maximizes the integrated convective heat, which is in turn minimized by a bang-bang trajectory that begins with a full lift-down command [12]. A large source of heat during atmospheric missions is due to the radiation of high-temperature gases in the shock layer [13, 14]. For lunar return aerocapture, which is the lowest speed aerocapture possible on Earth, heat load due to convection and heat load due to radiation are comparable in magnitude for trajectories flown by Orion. At higher initial velocities, the radiative heat load becomes increasingly more important, since it generally grows faster than convective heat at higher speeds. Further, heat flux due to radiation increases with increasing nose radius, whereas the opposite happens for convective heat flux, making radiative heat dominant for larger spacecraft. The density, composition, and scale height of the atmosphere also affect the ratio between the two components of the heat; for example, radiative heat is generally negligible for missions to Uranus [15], may be relevant on Mars depending on entry velocity [16], and is instead expected to be the major source of heat for missions to Neptune and Titan [17, 18, 19]. The first key contribution of this paper is to analytically prove, given a few assumptions, that the minimimum radiative heat load aerocapture trajectory coincides with the trajectory minimizing the Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V. The proof encompasses many of the several empirical radiative heat formulations that have been proposed in the last 40 years. The proof is validated numerically for a variety of conditions and formulations of radiative heat flux. The same proof can be used to deduce the fact that convective heat is instead maximized by that trajectory, as previously shown by Sigal and Guelman [12]; however, this proof generalizes to more empirical formulations of convective heat flux than the one considered by Sigal and Guelman. Finally, for cases where the heat due to convection is negligible compared to radiative heat, the same trajectory can minimize Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V and total heat load at the same time, avoiding the need to seek trade-offs. To fly the minimum heat, minimum Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V aerocapture, this paper introduces a novel optimal numerical predictor-corrector (NPC) guidance. Initial efforts to develop an optimal guidance for aeroassisted maneuvers focused on orbital plane change with minimum energy loss [20, 21, 22]. Early Aerocapture guidance schemes were derived from the Apollo skip entry guidance [23] which relies on the tracking of a nominal trajectory. Aerocapture NPCs seek a constant bank angle to be kept throughout the entirety of the trajectory [24] such that the desired apoapsis is targeted. Optimal aerocapture guidance was initially proposed to minimize the control effort [20]. Optimality for closed-loop guidance in terms of Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V for an aeroassisted maneuver was later considered by Evans and Dukeman for aerobraking [25]. The minimum Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V is obtained by maximizing exit velocity for a given apoapsis. To this end, the algorithm is divided into two phases. In the first phase, the algorithm integrates the trajectory with a constant, quasi full lift-down bank angle. If the predicted apoapsis is lower than the target one, full lift-up is commanded; else, Phase 2 is triggered, in which the algorithm then behaves like the guidance in Ref. [24]. This approach achieves optimality avoiding online trajectory optimization. A bank angle margin is required, which decreases performance, but reduces the risk of skipping out. Bank angle margins are constant independently of the entry conditions. This choice leads to a decrease in performance for shallow entry angles. An improvement to the approach is the Fully Numerical Predictor-corrector Aerocapture Guidance (FNPAG) [10] for aerocapture. Performance is improved by tuning the optimal bank angle margins as a function of entry angle and velocity. The parameter is tuned for a two-dimensional grid of different entry conditions; for each point of the grid, Monte Carlo runs are required for the tuning. At the beginning of the maneuver, the parameter is interpolated from the grid. Recent developments have been considering uncertainties directly in the planning via stochastic and robust optimization under uncertainty, similarly to stochastic model predictive control [26, 27], or with two stage optimization under uncertainty [28], for cases where perturbations are so large that there is no control profile that leads to feasible trajectory under all perturbations. Whilst these approaches are more robust than the FNPAG, they are also more computationally demanding. On the other hand, common alternatives to NPCs are analytical predictor-correctors (APCs) [29, 30, 31, 32], which make more simplifications than NPCs, and result in a faster, but potentially less robust, guidance algorithm. This paper improves on the FNPAG by showing that the major reason why different bank angle margins are needed for different entry conditions is the unconstrained kinematics in the motion planning during Phase 1. Taking the kinematics into account makes the planning more robust and easier to tune. The rotation from the bank angle of Phase 1 to the bank angle of Phase 2 occurs instantaneously in the planning of the guidance logics of Refs. [10] and [25]. In fact, for the majority of re-entry guidance systems, attitude kinematics are not taken into account while planning, and their effect is usually negligible. A rotation from full lift-up to full lift-down may take more to 15 seconds, which is short relatively to the entirety of an atmospheric re-entry flight. During aerocapture however, 15 seconds are enough to dissipate more than 30% of the difference in energy between initial and final states, as shown in Fig. 2. The percentage is strongly dependent on the initial flight path angle and velocity, which affect both the overall duration of the maneuver and the switching time. Therefore, a predictor-corrector guidance for aerocapture is greatly benefited by including the effects of this rotation in the prediction. The guidance proposed in this paper takes attitude kinematics constraints into account. As a result, it performs similarly to that of Ref. [10], but using the minimal tuning of Ref. [25]. Figure 2: Specific energy versus time for aerocapture flown with constant bank angle. The proposed longitudinal guidance is complemented by a novel robust lateral guidance. The lateral guidance aims to minimize the number of bank reversals. It does so by predicting the trajectory as if a bank reversal was happening immediately, and, similarly to the longitudinal guidance, it includes attitude kinematics constraints. This allows the lateral guidance to take into account several perturbations induced by the reversal, such as the finite time of the bank reversal, and the effect of accelerations that depend on the heading or on the position of the spacecraft, such as Coriolis. The proposed lateral guidance has been partly tailored to the longitudinal guidance proposed in this paper, but is easily applicable to other entry problems. The contributions of this paper can be summarized as follows: â€¢ The analytical proof that the minimum radiative heat load aerocapture is the same trajectory that leads to a minimum Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V. For very high-speed aerocapture, convective heat flux is much smaller than radiative heat flux; thus, that same trajectory minimizing heat load minimizes the total integrated heat load as well, in addition to minimizing Î”â¢VÎ”ğ‘‰\Delta Vroman_Î” italic_V. â€¢ The introduction of the optimal aerocapture guidance with attitude kinematics constraints (OAK), an optimal NPC guidance for longitudinal aerocapture guidance that achieves optimal results with minimal tuning by constraining the attitude kinematics in the planning. â€¢ The introduction of a lateral guidance that assumes immediate start of a bank reversal, allowing for almost single reversal lateral control. This paper continues as follows. Section 2 describes the dynamics of aerocapture. Section 3 provides the analytical proof of the form of the trajectory that minimizes radiative heat. Section 4 introduces both the longitudinal and the lateral logic of the novel guidance of this paper. Section 5 shows the results obtained from extensive simulation campaign. The guidance is tested for different entry conditions and vehicles. Section 6 concludes this paper."
https://arxiv.org/html/2411.02911v1,Synergizing Hyper-accelerated Power Optimization and Wavelength-Dependent QoT-Aware Cross-Layer Design in Next-Generation Multi-Band EONs,"The extension of elastic optical network technologies to multi-band transmission (i.e., MB-EON) holds significant promise in augmenting spectral efficiency, total throughput, and reducing the total cost of ownership over the long term for telecommunications operators. Nevertheless, the design intricacies of such networks engender several challenges, at the forefront of which is the optimization of physical parameters, e.g., optical power and the corresponding quality of transmission (QoT). At the heart of these challenges are frequency-dependent characteristics of the fiber, such as fiber loss, dispersion, nonlinear coefficients, and the interference effects of inter-channel stimulated Raman scattering is particularly pertinent when operating beyond the L+C (LC)-band when occupying a continuous transmission spectrum wider than 100 nm. In this investigation, we introduce a methodology to find the optimal optical power allocation on a span-by-span basis. Two hyper-accelerated power optimization (HPO) strategies have been introduced: flat launch power (FLP) and flat received power (FRP). These methodologies offer a significant acceleration in network power optimization compared to existing approaches while maintaining the stability of running services. Furthermore, we exhaustively compare the FLP and FRP models. Our research reveals that while FRP does not notably increase total capacity (showing an increase of less than 10 Tbps for an L+C+S (LCS)-band system spanning 100 km), its the impact is negligible for systems operating in the C- and LC-band scenarios. However, in the LCS scenario, FRP brings notable improvements in flatness (the difference between maximum and minimum values) and the minimum of the generalized signal- to-noise ratio (GSNR)/optical SNR (OSNR), especially in the S- band, achieving approximately 2/0 dB and 2.5/6 dB, respectively. In a network-wide analysis of several network topologies, we demonstrate that the improvement in the minimum GSNR due to the FRP technique, synergizing with wavelength-dependent QoT-aware cross-layer design, results in a throughput increase ranging from approximately 12% to 75% (depending on the network scale) at a 1% bandwidth blocking rate. Lastly, we apply HPO to local and global power optimization methods in MB-EON, showing that, while both methods exhibit similar performance, the latter is simpler and more cost-effective for larger-scale networks.","The multi-band (MB) technology is a cost-effective solution to address the growing demand for higher bandwidth in metro and core networks, which experience an annual traffic demand increase of more than 35%-40% [1, 2, 3]. However, the adoption of this technology is currently in the early stages, with only the L+C (LC)-band having been recently commercialized [4, 5]. In addition to the hardware requirements, associated with implementing the LC-band and beyond, such as amplifiers and transceivers, the planning of MB optical networks require addressing the inter-channel stimulated Raman scattering (ISRS) effects. ISRS causes the power of high-frequency channels to deplete into the low-frequency channels. This phenomenon becomes particularly significant when the dense wavelength division multiplexing (DWDM) bandwidth is wider than 10 THz [6]. Network planning becomes even more complex in EONs when the choice of the modulation format for a lightpath varies based on the reach distance, number of spans, and the number of traversed reconfigurable optical add-drop multiplexer (ROADM) components. The planning and service provisioning of MB-EONs has been studied over the past six years covering four main areas: 1- Migration scenarios from the C-band to beyond C-band, including C to LC/LCS/LCS+E/LCS+E+O/U+LCS+E+O transitions, e.g., [1, 7, 8], 2- Techno-economic studies, e.g., [8, 3, 1], 3- Power optimization and control strategies, e.g., [9, 10, 11, 12, 13, 14, 15, 16, 17], and 4- Routing, modulation format, band, and spectrum/wavelength assignment algorithms (RMBS(W)A), e.g., [18, 7, 19]. Given the focus of this paper on greenfield network planning for MB-EONs, we will not delve into the different migration strategies (e.g., day one and pay as you grow) or techno-economic studies. Launch power is one of the most critical parameters influencing the generalized signal-to-noise ratio (GSNR) value, which serves as a QoT metric [13]. Regarding the power optimization, the previous works have considered various objective functions such as link capacity (i.e., achievable information rate) maximization such as [9, 11, 17], minimum GSNR maximization like [10], or flatness of the GSNR profile in each band or across all bands such as [12, 13, 20]. However, solving the power optimization problem while considering the ISRS effects is challenging, as it is NP-hard and non-convex [9, 14]. To address this challenge, various brute-force heuristic and meta-heuristic algorithms have been proposed, including genetic algorithm [12], particle swarm optimization [9, 15], and the greedy search [16, 13]. Additionally, while some authors have tackled the problem on a channel-by-channel basis [14, 9], others have adopted a span-based strategy by considering auxiliary parameters. For instance, in [16] and [13], the authors proposed using two auxiliary parameters for each band: fixed tilt (slope) and offset, known as the (tilt, offset) approach. Additionally, in [17], four parameters - offset, slope, parabolic, and cubic constants - require tuning. Although these approaches offer faster solutions compared to the channel-by-channel approach, they may not be rapid enough for online provisioning in dynamic planning scenarios. All these proposed approaches involve analyzing an extensive search space [13, 16, 17]. Furthermore, as demonstrated in [21], for C-band WDM systems, considering flat received power at the end of the span as an objective function results in higher link capacity compared to using flat GSNR as the objective function. Additionally, the authors in [10] and [12] have shown that minimum GSNR maximization or optimizing GSNR flatness as an objective function does not maximize the system capacity. Therefore, in the context of this paper, we have opted to focus on maintaining a uniform power level after each span while considering the system capacity maximization. To achieve this, in contrast to the (tilt, offset) approach, we introduce a hyper-accelerated power optimization (HPO) method, where we seek to determine the optimal flat launch/received power by solving the Raman coupled differential equations in forward/backward mode. The runtime for this optimization averages approximately 3 [sec] for span lengths ranging from 50 to 100 km. As we will demonstrate, embracing this approach not only results in the rapid resolution of power optimization, measured in a few seconds rather than several minutes [13] or even hours [9], but it also contributes to an improvement in the average GSNR per span, taking into consideration the overall span capacity. Moreover, it increases the utilization of higher modulation formats throughout the network. Additionally, in practical terms, the OSNR is a parameter that can be easily monitored, measured, and integrated into network management systems. Therefore, the uniformity of the OSNR, rather than the GSNR, becomes particularly significant, especially when different channels or lightpaths exhibit varying GSNR values due to the disparate routes they follow [22]. Various approaches to static [18], semi-static [23], and dynamic [7] RMBS(W)A have been explored. In static planning, the traffic matrix is known in advance, and service lifecycles are considered infinite. Semi-static planning involves unknown traffic matrices but assumes infinite service lifecycles, while dynamic planning deals with unpredictable traffic matrices and lifecycles. It is worth noting that semi-static network planning is the practical case of backbone networks [24]. Therefore, it has been considered in this study. The routing in optical network planning typically relies on the k-shortest path first (K-SPF) algorithm. This involves selecting the shortest path (based on distance, number of optical hops, or multiplexing sections) with sufficient available spectrum resources as the primary choice. However, some studies have explored alternative criteria such as maximizing GSNR [13, 23] or minimizing the cost of the load balancing factor [25] when selecting paths. Additionally, distance-adaptive network planning [18, 26, 19] has been explored, with some studies considering worst-case channel scenarios [7, 13] for modulation cardinality index selection, determining the line card interfacesâ€™ (LCIs) bit rate capability. In the context of this article, we assume to transmit signal with quadrature amplitude modulation (QAM) employing probabilistic shaping. While recent works have addressed both fixed-grid [13] and flexi-grid [18] MB-EONs, the latter pose practical challenges due to ISRS effects. Despite efforts to apply flexi-grid EONs, developing a closed formula to estimate power profiles, even for LC-band scenarios, remains unfeasible [27]. Some authors have focused on QoT-aware network planning [18, 23] but have overlooked the amplified spontaneous emission (ASE)-shaped noise filler for idle channels [5], resulting in unaddressed power profile changes and QoT degradation in established lightpaths. Furthermore, optical layer grooming can be achieved through transponders or Flexponders. In transponder grooming [13, 23], each client card interface is connected to an LCI, with IP flows groomed from the client card to the corresponding LCI. Modern modems operate based on the Flexponder concept, allowing low-bit-rate IP flows between a source-destination to be groomed at any LCI with sufficient capacity and the same source-destination. As the GSNR of each channel varies, the modulation cardinality of each LCI differs based on the frequency work point. Wavelength-dependent QoT considerations, as discussed in this paper, can reduce blocking probability and result in cost savings through optimized LCI usage. Therefore, this manuscript considers real-world MB-EONs in which the bit rate variable LCIs are achieved by adjusting the modulation cardinality. The main contributions of this manuscript can be summarized as follows: 1) a hyper-accelerated power optimization approach for uniform and pre-tilt launch power profiles, termed flat launched power (FLP) and flat received power (FRP), in MB-EONs. 2) Given the non-flat GSNR profile of optimum MB-EONs, the cross-layer design presents numerous complexities. Hence, we propose a wavelength-dependent QoT-aware modulation cardinality index selection and spectrum assignment based on a Flexponder. 3) We conduct exhaustive simulations across various span lengths and network configurations to demonstrate the efficacy of the proposed HPO algorithm. 4) Fourthly, we compare the performance of local optimization versus global optimization to assess the algorithmâ€™s effectiveness. We provide an overview of the works published in the literature in Table I. TABLE I: Summary of the Related Works.Î±â¢(f)ğ›¼ğ‘“\alpha(f)italic_Î± ( italic_f ) is the fiber loss coefficient. CFM: Closed-form model. Reference QoT-Aware Cross-Layer Design of MB-EONs ISRS effect on NLI (SPM+XPM) ISRS effect on ASE ISRS effect on Î±â¢(f)ğ›¼ğ‘“\alpha(f)italic_Î± ( italic_f ) Optimum FRP ASE-shaped Noise Filler ML-based GN/EGN CFM[17, 28, 27] Flexponder Grooming LCS-band Wavelength-Dependent [7] âœ“ âœ“ âœ“ Ã—\timesÃ— âœ“ Ã—\timesÃ— Ã—\timesÃ— âœ“ Ã—\timesÃ— [18] âœ“ Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— [13] âœ“ âœ“ âœ“ Ã—\timesÃ— âœ“ Ã—\timesÃ— Ã—\timesÃ— âœ“ Ã—\timesÃ— [23] Ã—\timesÃ— âœ“ Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— âœ“ [19] âœ“ Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— [29] âœ“ âœ“ Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— [26] âœ“ Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— Ã—\timesÃ— âœ“ Ã—\timesÃ— This study âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ âœ“ The rest of this paper is structured as follows: Section II provides a summary and analysis of the QoT estimator and physical layer modeling. Section III describes the node and network architecture compatible with the proposed cross-layer design methodology. Section IV introduces the power optimization algorithms proposed in this paper, including the FLP and FRP algorithms. Section V evaluates the performance of the proposed solution. Finally, Section VI presents the conclusions of this paper."
https://arxiv.org/html/2411.03289v1,Data-Driven Sampling Based Stochastic MPC forSkid-Steer Mobile Robot Navigation,"Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https://stochasticmppi.github.io.","Skid-steer robots, recognized for their high traction and payload capacity, are widely adopted in rugged and challenging terrain navigation [1]. However, their high maneuverability comes at the cost of significant skidding and slipping, making it challenging to predict their motion given command velocities. Accurate motion models are crucial for effective control, yet existing approaches based on simple kinematic models struggle to capture complex tire-terrain interactions [2]. Figure 1: Illustration of two MPPI trajectories (Traj 1 and Traj 2). The obstacle radii are adjusted based on propagated state variance (ellipses) and the safety threshold pxsubscriptğ‘ğ‘¥p_{x}italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT (Eq. 7). Traj 1 is â€˜unsafeâ€™ due to a collision with an obstacle (Obs 1). Model Predictive Path Integral (MPPI) [3] is a sampling-based control strategy well-suited for solving non-linear, non-convex, and non-differentiable optimal control problems. It evaluates multiple candidate control trajectories and selects the optimal action based on a weighted sum of these trajectories. Nevertheless, standard MPPI approaches [4] that rely on pre-trained models to predict robot dynamics struggle with unmodeled terrain variations and real-time uncertainty [5], limiting their ability to adapt to different environments. To address these challenges, we integrate Gaussian Processes (GPs) into the MPPI framework [6]. GPs are powerful nonlinear regression tools that provide both mean and uncertainty (variance) estimates for predictions. In our work, GPs model skid-steer robot motion across varying terrains. The GP mean captures model residuals, accounting for discrepancies between nominal physics-based models and actual robot dynamics. Additionally, the GP variance, representing uncertainty in the modelâ€™s predictions, is integrated into safety constraints through chance constraints [7]. This enables safety buffers to be dynamically adjusted in real-time given varying uncertainty levels. These buffers help ensure the robot stays on its intended path and maintains a safe distance from obstacles, even in highly dynamic environments. Fig. 1 presents an overview of our approach. The key contributions of this paper are: â€¢ A data-driven, sampling based stochastic MPC method tailored for high-speed skid-steer robot navigation at speeds up to 2 m/s (the top speed of the Clearpath Jackal platform), leveraging GP regression for adaptive motion planning. The same control law ensures navigation across all terrains included in the GP training dataset, without the need for explicit terrain identification. â€¢ A GPU implementation of the GP-MPPI planner that solves stochastic optimal control problems at real-time rates of 20 Hz for path tracking and obstacle avoidance. â€¢ Extensive simulation experiments of the proposed method, demonstrating superior tracking accuracy and obstacle avoidance compared to kinematic model-based MPPI approaches. The methodâ€™s real-world applicability is also evaluated and verified via hardware experiments of a skid-steer robot navigating different terrains."
https://arxiv.org/html/2411.03277v1,Asymptotic stability equals exponential stabilityâ€”whileyou twist your eyes,Suppose that two vector fields on a smooth manifold render some equilibrium point globally asymptotically stable (GAS). We show that there exists a homotopy between the corresponding semiflows such that this point remains GAS along this homotopy.,"1 Introduction In the context of what we call today Conley index theory (see Section A), Charles C. Conley himself posed the following â€œconverse questionâ€ in the late 1970s: â€œTo what extent does the homotopy index [Conley index] itself determine the equivalence class of isolated invariant sets which are related by continuation?â€ [ref:conley1978isolated, p. 83]. Then, recently, Matthew D. Kvalheim proved that uniquely integrable C0superscriptğ¶0C^{0}italic_C start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT vector fields, on a Câˆsuperscriptğ¶C^{\infty}italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT manifold Mğ‘€Mitalic_M, rendering a compact set AâŠ†Mğ´ğ‘€A\subseteq Mitalic_A âŠ† italic_M asymptotically stable, are homotopic on an open neighbourhood UâŠ‡Ağ´ğ‘ˆU\supseteq Aitalic_U âŠ‡ italic_A such that throughout the homotopy the vector fields do not vanish on Uâˆ–Ağ‘ˆğ´U\setminus Aitalic_U âˆ– italic_A [ref:kvalheim2022obstructions, Thm. 1]. Connecting this result to Conleyâ€™s question, a follow-up question (revitalized) by Kvalheimâ€”which is the central question of this noteâ€”is the following: Question: â€œAre, dynamical systems that render a set Ağ´Aitalic_A asymptotically stable, homotopic through dynamical systems that preserve this notion of stability?â€ This question, in one form or another, inspired several works, for instance, [ref:reineck1992continuation, ref:mrozek2000conley, ref:JongeneelSchwan2024TAC]. Here, we will elaborate on commentary by the author in [ref:JongeneelSchwan2024TAC]. Specifically, in the seminal paper â€œAsymptotic stability equals exponential stability, and ISS equals finite energy gainâ€”if you twist your eyesâ€ from 1999, Lars GrÃ¼ne, Eduardo D. Sontag and Fabian R. Wirth showed that asymptotic stability â€œequalsâ€ exponential stability in the sense that if an equilibrium point is asymptotically stable under some vector field on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, then, there is a suitable change of coordinates rendering this point exponentially stable [ref:grune1999asymptotic]. Such a change of coordinates is understood to be instantaneous. However, by leveraging their work, we show in this note that asymptotic stability can (almost) always be continuously â€œtransformedâ€ into exponential stability, while preserving asymptotic stability throughout the transformation, see Theorem 3.4. Differently put, asymptotic stability equals exponential stabilityâ€”not only if you twist your eyes, but while you twist your eyes. The key to this is to observe that we can in fact select the transformation of GrÃ¼ne, Sontag and Wirth to be an element of the orientation-preserving homeomorphism group on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, not just any homeomorphism cf. [ref:JongeneelSchwan2024TAC, Sec. III], see the proof of Proposition 3.2. This result provides a partial solution to Conleyâ€™s converse question as it turns out that the asymptotically stable systems under consideration can be continuously transformed into the same exponentially stable system and hence, by transitivity, into each other. Concurrently, we emphasize in this note that results of this form typically extend to discontinuous vector fields when solutions are understood in the sense of Filippov. We also discuss intimate connections with optimization (e.g., see Example 3.2) and optimal transport (e.g., see Example 4.1). 1.1 Related work It can be argued that questions of the form above emerged from studies aimed at classifying manifolds, maps, vector fields and so forth. A successful, yet coarse, resolution has been found in the study of these objects up to homotopy, e.g., motivated early on by the fundamental group being homotopy invariant, Hopfâ€™s degree theorem, CW complexes, intractability of topological equivalence and more work at the intersection of topology and dynamical systems. We cannot do justice here to the wealth of work in this area, but let us mention that inspired by Aleksandr Andronov, Lev Pontryagin, RenÃ© Thom, Maurico Peixoto, George D. Birkhoff, John Milnor and several others, it was in particular Stephen Smale highlighting that this intersection is an interesting one for both topology and dynamical systems. As put by Sheldon Newhouse: â€œâ€¦ one of the great influences that Steve had, at least on me and I think many others, aside from excitement, was the whole idea that one might get a structure theory for general dynamical systems. That such a general structure theory for most systems could exist I think was a goal of Poincare and Birkhoff, and somehow in the intervening years, it was lost sight of by people working on specialized problems. The idea of getting a global structure of all systems has spawned a lot of development in the past, and it is still going on.â€ [ref:hirsch2012topology, p. 183]. Indeed, after seminal work by Smale on the qualitative classification of dynamical systems [ref:smale1967differentiable], several homotopy results appeared being concerned with Morse-Smale vector fields, e.g., see [ref:asimov1975homotopy, ref:newhouse1976, ref:franks1979morse]. Similarly, one can study if gradient vector fields are homotopic through gradient vector fields, e.g., while restricting equilibria [ref:parusinski1990gradient], see also [ref:reineck1991continuation, ref:kvalheim2022obstructions] for work close in spirit to ours. These works were typically concerned with structural stability and the study of the topology of spaces of dynamical systems, that is, to understand when dynamical systems are in some sense close or equivalent, e.g., see [ref:shub1975homology] for pointers and further results. Then, in control theory, these tools (i.e., certain homotopy invariants) were used to construct necessary conditions for feedback controllers to exist, e.g., if a desirable (closed-loop) dynamical system belongs to a certain homotopy class, then, there must be at least exist some feedback that renders the control system a member of this class, regardless of it resulting in the actual desirable system, e.g., see [ref:brockett1983asymptotic, ref:krasnosel1984geometrical, ref:zabczyk1989, ref:Coron1990]. Most of these results have in common that the original objects are homotoped to something simple, something â€œcanonicalâ€ where we do our analysis and computations. For instance, in the context of control systems, when our goal is stabilization of the origin on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT through feedback, then, the canonical differential equation is xË™=âˆ’xË™ğ‘¥ğ‘¥\dot{x}=-xoverË™ start_ARG italic_x end_ARG = - italic_x and we would like our closed-loop dynamical system to be in some qualitative sense equivalent to this equation. This is precisely how the â€œindex conditionâ€ by Krasnoselâ€™skiÄ­ and ZabreÄ­ko is derived [ref:krasnosel1984geometrical, Sec. 52]. Clearly, necessary conditions of this form are only as valuable as the homotopy class is distinctive, e.g., although the index condition is powerful, it cannot differentiate between xË™=xË™ğ‘¥ğ‘¥\dot{x}=xoverË™ start_ARG italic_x end_ARG = italic_x and xË™=âˆ’xË™ğ‘¥ğ‘¥\dot{x}=-xoverË™ start_ARG italic_x end_ARG = - italic_x on â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. This is precisely what motivates us, we expect that understanding Conleyâ€™s converse question can lead to stronger necessary conditions for continuous stabilizing feedback to exist. Besides possibly stronger necessary conditions for continuous stabilization, the study of Conleyâ€™s converse question directly relates to understanding topological properties of spaces of stable dynamical systems (as was partial motivation for our previous work [ref:JongeneelSchwan2024TAC]). This is of practical importance as several frameworks in optimal control and reinforcement learning aim to optimize over precisely such a space, e.g., moving from a pre-stabilized system to an optimally controlled system, we point to [ref:berkenkamp2017safe, ref:Fazel_18, ref:wang2022learning, ref:furieri2024learning] and references therein. Before detailing further technicalities, we recall that Conleyâ€™s converse question is not trivial. Example 1.1 (Trivial convex combinations can fail). Consider a linear differential equation xË™=Aâ¢(s)â¢xË™ğ‘¥ğ´ğ‘ ğ‘¥\dot{x}=A(s)xoverË™ start_ARG italic_x end_ARG = italic_A ( italic_s ) italic_x on â„2superscriptâ„2\mathbb{R}^{2}blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT parametrized by the matrices [0,1]âˆ‹sâ†¦Aâ¢(s):=sâ‹…(âˆ’1100âˆ’1)+(1âˆ’s)â‹…(âˆ’1010âˆ’1).contains01ğ‘ maps-toğ´ğ‘ assignâ‹…ğ‘ matrix11001â‹…1ğ‘ matrix10101[0,1]\ni s\mapsto A(s):=s\cdot\begin{pmatrix}-1&10\\ 0&-1\end{pmatrix}+(1-s)\cdot\begin{pmatrix}-1&0\\ 10&-1\end{pmatrix}.[ 0 , 1 ] âˆ‹ italic_s â†¦ italic_A ( italic_s ) := italic_s â‹… ( start_ARG start_ROW start_CELL - 1 end_CELL start_CELL 10 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL - 1 end_CELL end_ROW end_ARG ) + ( 1 - italic_s ) â‹… ( start_ARG start_ROW start_CELL - 1 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 10 end_CELL start_CELL - 1 end_CELL end_ROW end_ARG ) . Both Aâ¢(0)ğ´0A(0)italic_A ( 0 ) and Aâ¢(1)ğ´1A(1)italic_A ( 1 ) correspond to global asymptotically stable systems, yet, for s=(1/2)ğ‘ 12s=(1/2)italic_s = ( 1 / 2 ) we find that the system xË™=Aâ¢(s)â¢xË™ğ‘¥ğ´ğ‘ ğ‘¥\dot{x}=A(s)xoverË™ start_ARG italic_x end_ARG = italic_A ( italic_s ) italic_x is unstable. Hence, we cannot just construct straight-line homotopies between stable vector fields and expect that stability is preserved. We know from [ref:JongeneelSchwan2024TAC] that instead, for linear vector fields we should homotope via the canonical ODE xË™=âˆ’xË™ğ‘¥ğ‘¥\dot{x}=-xoverË™ start_ARG italic_x end_ARG = - italic_x. Explicitly, one could consider the following path of linear vector fields defined by Hâ¢(x;s):={(âˆ’1(1âˆ’2â¢s)â‹…100âˆ’1)â¢xsâˆˆ[0,1/2](âˆ’10(2â¢sâˆ’1)â‹…10âˆ’1)â¢xsâˆˆ(1/2,1].assignğ»ğ‘¥ğ‘ casesmatrix1â‹…12ğ‘ 1001ğ‘¥ğ‘ 012matrix10â‹…2ğ‘ 1101ğ‘¥ğ‘ 121H(x;s):=\begin{cases}\begin{pmatrix}-1&(1-2s)\cdot 10\\ 0&-1\end{pmatrix}x\quad&s\in[0,1/2]\\ \begin{pmatrix}-1&0\\ (2s-1)\cdot 10&-1\end{pmatrix}x\quad&s\in(1/2,1]\end{cases}.italic_H ( italic_x ; italic_s ) := { start_ROW start_CELL ( start_ARG start_ROW start_CELL - 1 end_CELL start_CELL ( 1 - 2 italic_s ) â‹… 10 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL - 1 end_CELL end_ROW end_ARG ) italic_x end_CELL start_CELL italic_s âˆˆ [ 0 , 1 / 2 ] end_CELL end_ROW start_ROW start_CELL ( start_ARG start_ROW start_CELL - 1 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL ( 2 italic_s - 1 ) â‹… 10 end_CELL start_CELL - 1 end_CELL end_ROW end_ARG ) italic_x end_CELL start_CELL italic_s âˆˆ ( 1 / 2 , 1 ] end_CELL end_ROW . âˆ˜\circâˆ˜ Notation and preliminaries Let râˆˆâ„•âˆª{âˆ}ğ‘Ÿâ„•r\in\mathbb{N}\cup\{\infty\}italic_r âˆˆ blackboard_N âˆª { âˆ }, then, Crâ¢(U;V)superscriptğ¶ğ‘Ÿğ‘ˆğ‘‰C^{r}(U;V)italic_C start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ( italic_U ; italic_V ) denotes the set of Crsuperscriptğ¶ğ‘ŸC^{r}italic_C start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT-smooth functions from Uğ‘ˆUitalic_U to Vğ‘‰Vitalic_V. The inner product on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is denoted by âŸ¨â‹…,â‹…âŸ©â‹…â‹…\langle\cdot,\cdot\rangleâŸ¨ â‹… , â‹… âŸ© and ğ•Šnâˆ’1={xâˆˆâ„n:â€–xâ€–2=1}superscriptğ•Šğ‘›1conditional-setğ‘¥superscriptâ„ğ‘›subscriptnormğ‘¥21\mathbb{S}^{n-1}=\{x\in\mathbb{R}^{n}:\|x\|_{2}=1\}blackboard_S start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT = { italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT : âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1 } is the embedded unit sphere in â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. By clâ¢(W)clğ‘Š\mathrm{cl}(W)roman_cl ( italic_W ) we denote the (topological) closure of a set WâŠ†Mğ‘Šğ‘€W\subseteq Mitalic_W âŠ† italic_M and by intâ¢(W)intğ‘Š\mathrm{int}(W)roman_int ( italic_W ) we denote its interior. The identity map pâ†¦pmaps-toğ‘ğ‘p\mapsto pitalic_p â†¦ italic_p on a space Mğ‘€Mitalic_M is denoted by idMsubscriptidğ‘€\mathrm{id}_{M}roman_id start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT and tangent spaces of sufficiently regular manifolds Nğ‘Nitalic_N are denoted by Tqâ¢Nsubscriptğ‘‡ğ‘ğ‘T_{q}Nitalic_T start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT italic_N, for qâˆˆNğ‘ğ‘q\in Nitalic_q âˆˆ italic_N, with Tâ¢Nğ‘‡ğ‘TNitalic_T italic_N denoting the corresponding tangent bundle. Given a function V:Mâ†’â„:ğ‘‰â†’ğ‘€â„V:M\to\mathbb{R}italic_V : italic_M â†’ blackboard_R, let Vâˆ’1â¢(c)superscriptğ‘‰1ğ‘V^{-1}(c)italic_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_c ) denote the level set {pâˆˆM:Vâ¢(p)=c}conditional-setğ‘ğ‘€ğ‘‰ğ‘ğ‘\{p\in M:V(p)=c\}{ italic_p âˆˆ italic_M : italic_V ( italic_p ) = italic_c }. We typically use Ï†ğœ‘\varphiitalic_Ï† to denote a (semi)flow. If this flow comes from a vector field Xğ‘‹Xitalic_X, we write Ï†â¢(â‹…;X)ğœ‘â‹…ğ‘‹\varphi(\cdot;X)italic_Ï† ( â‹… ; italic_X ), similarly, if Ï†ğœ‘\varphiitalic_Ï† is parametrized by sâˆˆ[0,1]ğ‘ 01s\in[0,1]italic_s âˆˆ [ 0 , 1 ] we write Ï†â¢(â‹…;s)ğœ‘â‹…ğ‘ \varphi(\cdot;s)italic_Ï† ( â‹… ; italic_s ), that is, we typically overload the meaning of Ï†ğœ‘\varphiitalic_Ï†. A homotopy [0,1]Ã—Mâˆ‹(s,p)â†¦Hâ¢(s,p)contains01ğ‘€ğ‘ ğ‘maps-toğ»ğ‘ ğ‘[0,1]\times M\ni(s,p)\mapsto H(s,p)[ 0 , 1 ] Ã— italic_M âˆ‹ ( italic_s , italic_p ) â†¦ italic_H ( italic_s , italic_p ) is said to be an isotopy when pâ†¦Hâ¢(s,p)maps-toğ‘ğ»ğ‘ ğ‘p\mapsto H(s,p)italic_p â†¦ italic_H ( italic_s , italic_p ) is a topological embedding for all sâˆˆ[0,1]ğ‘ 01s\in[0,1]italic_s âˆˆ [ 0 , 1 ]. A homeomorphism Ïˆ:â„nâ†’â„n:ğœ“â†’superscriptâ„ğ‘›superscriptâ„ğ‘›\psi:\mathbb{R}^{n}\to\mathbb{R}^{n}italic_Ïˆ : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is said to be a stable homeomorphism when it is a finite composition of homeomorphisms that equal the identity map on someâ€”not necessarily the sameâ€”non-empty open subset of â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. We denote by Homeoâ¢(â„n;â„n)Homeosuperscriptâ„ğ‘›superscriptâ„ğ‘›\mathrm{Homeo}(\mathbb{R}^{n};\mathbb{R}^{n})roman_Homeo ( blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) the group (under composition) of homeomorphisms Ïˆ:â„nâ†’â„n:ğœ“â†’superscriptâ„ğ‘›superscriptâ„ğ‘›\psi:\mathbb{R}^{n}\to\mathbb{R}^{n}italic_Ïˆ : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. Additionally, we denote by Homeo+â¢(â„n;â„n)âŠ‚Homeoâ¢(â„n;â„n)superscriptHomeosuperscriptâ„ğ‘›superscriptâ„ğ‘›Homeosuperscriptâ„ğ‘›superscriptâ„ğ‘›\mathrm{Homeo}^{+}(\mathbb{R}^{n};\mathbb{R}^{n})\subset\mathrm{Homeo}(\mathbb% {R}^{n};\mathbb{R}^{n})roman_Homeo start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ( blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) âŠ‚ roman_Homeo ( blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) the subgroup of all orientation-preserving111The reader that is unfamiliar with the notion of orientation is first directed to [ref:guillemin2010differential, Lee2] to see orientations in the smooth case. The topological definition relies on algebraic topology, see for instance [Hatcher, Sec. 3.3]. homeomorphisms. A function Î±:â„â‰¥0â†’â„â‰¥0:ğ›¼â†’subscriptâ„absent0subscriptâ„absent0\alpha:\mathbb{R}_{\geq 0}\to\mathbb{R}_{\geq 0}italic_Î± : blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT â†’ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT is said to be of class ğ’¦ğ’¦\mathcal{K}caligraphic_K when Î±ğ›¼\alphaitalic_Î± is strictly increasing and Î±â¢(0)=0ğ›¼00\alpha(0)=0italic_Î± ( 0 ) = 0. If, additionally limsâ†’+âˆÎ±â¢(s)=+âˆsubscriptâ†’ğ‘ ğ›¼ğ‘ \lim_{s\to+\infty}\alpha(s)=+\inftyroman_lim start_POSTSUBSCRIPT italic_s â†’ + âˆ end_POSTSUBSCRIPT italic_Î± ( italic_s ) = + âˆ, then, Î±ğ›¼\alphaitalic_Î± is of class ğ’¦âˆsubscriptğ’¦\mathcal{K}_{\infty}caligraphic_K start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT. These type of functions are called comparison functions, e.g., see [hahn1967stability, ref:kellett2014compendium]. 1.2 Dynamical systems In this note we study deterministic, finite-dimensional, time-invariant, continuous, (global) semi-dynamical systems comprised of the triple Î£:=(Mn,Ï†,â„â‰¥0)assignÎ£superscriptğ‘€ğ‘›ğœ‘subscriptâ„absent0\Sigma:=({M}^{n},\varphi,\mathbb{R}_{\geq 0})roman_Î£ := ( italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , italic_Ï† , blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT ). Here, Mnsuperscriptğ‘€ğ‘›{M}^{n}italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT will be a smooth nğ‘›nitalic_n-dimensional manifold diffeomorphic to â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, which we denote by Mnâ‰ƒdâ„nsubscriptsimilar-to-or-equalsğ‘‘superscriptğ‘€ğ‘›superscriptâ„ğ‘›{M}^{n}\simeq_{d}\mathbb{R}^{n}italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â‰ƒ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and Ï†:â„â‰¥0Ã—Mnâ†’Mn:ğœ‘â†’subscriptâ„absent0superscriptğ‘€ğ‘›superscriptğ‘€ğ‘›\varphi:\mathbb{R}_{\geq 0}\times{M}^{n}\to{M}^{n}italic_Ï† : blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT Ã— italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is a global semiflow, that is, a continuous map that satisfies for any pâˆˆMnğ‘superscriptğ‘€ğ‘›p\in{M}^{n}italic_p âˆˆ italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT: (i)ğ‘–(i)( italic_i ) Ï†â¢(0,p)=pğœ‘0ğ‘ğ‘\varphi(0,p)=pitalic_Ï† ( 0 , italic_p ) = italic_p (the identity axiom); and (iâ¢i)ğ‘–ğ‘–(ii)( italic_i italic_i ) (Ï†(s,Ï†(t,p))=Ï†(t+s,p)(\varphi(s,\varphi(t,p))=\varphi(t+s,p)( italic_Ï† ( italic_s , italic_Ï† ( italic_t , italic_p ) ) = italic_Ï† ( italic_t + italic_s , italic_p ) âˆ€s,tâˆˆâ„â‰¥0:={tâˆˆâ„:tâ‰¥0}for-allğ‘ ğ‘¡subscriptâ„absent0assignconditional-setğ‘¡â„ğ‘¡0\forall s,t\in\mathbb{R}_{\geq 0}:=\{t\in\mathbb{R}:t\geq 0\}âˆ€ italic_s , italic_t âˆˆ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT := { italic_t âˆˆ blackboard_R : italic_t â‰¥ 0 } (the semigroup axiom). We will usually write Ï†tâ¢(â‹…)superscriptğœ‘ğ‘¡â‹…\varphi^{t}(\cdot)italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( â‹… ) instead of Ï†â¢(t,â‹…)ğœ‘ğ‘¡â‹…\varphi(t,\cdot)italic_Ï† ( italic_t , â‹… ). In particular, we study semiflows generated by continuous vector fields over Mnsuperscriptğ‘€ğ‘›{M}^{n}italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, that is, when Ï†ğœ‘\varphiitalic_Ï† satisfies ddâ¢Ï„â¢Ï†Ï„â¢(p)|Ï„=t=Xâ¢(Ï†tâ¢(p)),âˆ€(t,p)âˆˆâ„â‰¥0Ã—Mn,formulae-sequenceevaluated-atddğœsuperscriptğœ‘ğœğ‘ğœğ‘¡ğ‘‹superscriptğœ‘ğ‘¡ğ‘for-allğ‘¡ğ‘subscriptâ„absent0superscriptğ‘€ğ‘›\displaystyle\frac{\mathrm{d}}{\mathrm{d}\tau}\varphi^{\tau}(p)|_{\tau=t}=X(% \varphi^{t}(p)),\quad\forall(t,p)\in\mathbb{R}_{\geq 0}\times M^{n},divide start_ARG roman_d end_ARG start_ARG roman_d italic_Ï„ end_ARG italic_Ï† start_POSTSUPERSCRIPT italic_Ï„ end_POSTSUPERSCRIPT ( italic_p ) | start_POSTSUBSCRIPT italic_Ï„ = italic_t end_POSTSUBSCRIPT = italic_X ( italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_p ) ) , âˆ€ ( italic_t , italic_p ) âˆˆ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT Ã— italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , (1.1) where Xğ‘‹Xitalic_X is a continuous section, that is, the map X:Mnâ†’Tâ¢Mn:ğ‘‹â†’superscriptğ‘€ğ‘›ğ‘‡superscriptğ‘€ğ‘›X:M^{n}\to TM^{n}italic_X : italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ italic_T italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is continuous and satisfies Ï€âˆ˜X=idMnğœ‹ğ‘‹subscriptidsuperscriptğ‘€ğ‘›\pi\circ X=\mathrm{id}_{M^{n}}italic_Ï€ âˆ˜ italic_X = roman_id start_POSTSUBSCRIPT italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT for Ï€ğœ‹\piitalic_Ï€ the canonical projection (p,v)â†¦Ï€â¢(p,v)=pmaps-toğ‘ğ‘£ğœ‹ğ‘ğ‘£ğ‘(p,v)\mapsto\pi(p,v)=p( italic_p , italic_v ) â†¦ italic_Ï€ ( italic_p , italic_v ) = italic_p. When Mn=â„nsuperscriptğ‘€ğ‘›superscriptâ„ğ‘›M^{n}=\mathbb{R}^{n}italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT = blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, then, the identification Tâ¢â„nâ‰ƒâ„nÃ—â„nsimilar-to-or-equalsğ‘‡superscriptâ„ğ‘›superscriptâ„ğ‘›superscriptâ„ğ‘›T\mathbb{R}^{n}\simeq\mathbb{R}^{n}\times\mathbb{R}^{n}italic_T blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â‰ƒ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT Ã— blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT results in being able to discuss vector fields as simply self-maps of â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. On â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we will work a lot with the â€œcanonicalâ€ differential equation xË™=âˆ’xË™ğ‘¥ğ‘¥\dot{x}=-xoverË™ start_ARG italic_x end_ARG = - italic_x, but also with the identity map xâ†¦xmaps-toğ‘¥ğ‘¥x\mapsto xitalic_x â†¦ italic_x, thus, to avoid notational confusion, we denote the corresponding canonical vector field by âˆ’âˆ‚xsubscriptğ‘¥-\partial_{x}- âˆ‚ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, and not âˆ’idâ„nsubscriptidsuperscriptâ„ğ‘›-\mathrm{id}_{\mathbb{R}^{n}}- roman_id start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. The focus on semiflows instead of flows allows us to look at sufficiently regular discontinuous vector fields as well. This is relevant to control theory, as the introduction of feedback usually results in a closed-loop vector field that cannot be assumed to be continuous (e.g., think of optimal control222One might also think of topological obstructions, however, as will become clear below, the type of discontinuities we consider do not allow for overcoming those obstructions, in general [ref:ryan1994brockett].). This choice of setting is also motivated by other recent work. For instance, the main research question in [ref:ozaslan2024exponential] is: â€œGiven an exponentially stable optimization algorithm, can it be modified to obtain a finite/fixed-time stable algorithm?â€. They provide some sufficient conditions, we will show that a more generic, yet less constructive viewpoint is also possible. Let Xğ‘‹Xitalic_X be some vector field on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, possibly discontinuous. To study Xğ‘‹Xitalic_X, we usually pass to some differential inclusion xË™âˆˆFâ¢(x),Ë™ğ‘¥ğ¹ğ‘¥\dot{x}\in F(x),overË™ start_ARG italic_x end_ARG âˆˆ italic_F ( italic_x ) , (1.2) where the set-valued map F:â„nâ†’2â„n:ğ¹â†’superscriptâ„ğ‘›superscript2superscriptâ„ğ‘›F:\mathbb{R}^{n}\to 2^{\mathbb{R}^{n}}italic_F : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â†’ 2 start_POSTSUPERSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT, with 2Xsuperscript2ğ‘‹2^{X}2 start_POSTSUPERSCRIPT italic_X end_POSTSUPERSCRIPT denoting the power set of Xğ‘‹Xitalic_X, is in some precise sense related to Xğ‘‹Xitalic_X. The intuition is to pass from a irregular single-valued map, to a more regular set-valued map that contains the original behaviour. Let Î»dsuperscriptğœ†ğ‘‘\lambda^{d}italic_Î» start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT denote the Lebesgue measure on â„dsuperscriptâ„ğ‘‘\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, then, solutions to these differential inclusions, are absolutely continuous curves Î¾âˆˆACâ¢(â„âŠ†â„;â„n)ğœ‰ACâ„â„superscriptâ„ğ‘›\xi\in\mathrm{AC}(\mathcal{I}\subseteq\mathbb{R};\mathbb{R}^{n})italic_Î¾ âˆˆ roman_AC ( caligraphic_I âŠ† blackboard_R ; blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) such that Î¾Ë™â¢(t)âˆˆFâ¢(Î¾â¢(t))Ë™ğœ‰ğ‘¡ğ¹ğœ‰ğ‘¡\dot{\xi}(t)\in F(\xi(t))overË™ start_ARG italic_Î¾ end_ARG ( italic_t ) âˆˆ italic_F ( italic_Î¾ ( italic_t ) ) for Î»1superscriptğœ†1\lambda^{1}italic_Î» start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT-a.e. tâˆˆâ„ğ‘¡â„t\in\mathcal{I}italic_t âˆˆ caligraphic_I. Typically, Fğ¹Fitalic_F is assumed to be upper semi-continuous and compact, convex valued. With those assumptions in mind, then, under mild conditions on Xğ‘‹Xitalic_X, a valuable solution framework follows by applying Filippovâ€™s operator â„±â„±\mathcal{F}caligraphic_F, that is, xâ†¦â„±â¢[X]â¢(x):=â‹‚Î´>0â‹‚Nâˆˆ{AâŠ‚â„n:Î»nâ¢(A)=0}convÂ¯â¢Xâ¢({xâˆˆâ„n:â€–xâ€–2<Î´}âˆ–N).maps-toğ‘¥â„±delimited-[]ğ‘‹ğ‘¥assignsubscriptğ›¿0subscriptğ‘conditional-setğ´superscriptâ„ğ‘›superscriptğœ†ğ‘›ğ´0Â¯convğ‘‹conditional-setğ‘¥superscriptâ„ğ‘›subscriptnormğ‘¥2ğ›¿ğ‘x\mapsto\mathcal{F}[X](x):=\bigcap_{\delta>0}\bigcap_{N\in\{A\subset\mathbb{R}% ^{n}:\lambda^{n}(A)=0\}}\overline{\mathrm{conv}}\,X\left(\{x\in\mathbb{R}^{n}:% \|x\|_{2}<\delta\}\setminus N\right).italic_x â†¦ caligraphic_F [ italic_X ] ( italic_x ) := â‹‚ start_POSTSUBSCRIPT italic_Î´ > 0 end_POSTSUBSCRIPT â‹‚ start_POSTSUBSCRIPT italic_N âˆˆ { italic_A âŠ‚ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT : italic_Î» start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_A ) = 0 } end_POSTSUBSCRIPT overÂ¯ start_ARG roman_conv end_ARG italic_X ( { italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT : âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < italic_Î´ } âˆ– italic_N ) . Then, solutions to xË™âˆˆâ„±â¢[X]â¢(x)Ë™ğ‘¥â„±delimited-[]ğ‘‹ğ‘¥\dot{x}\in\mathcal{F}[X](x)overË™ start_ARG italic_x end_ARG âˆˆ caligraphic_F [ italic_X ] ( italic_x ) (1.3) are understood to be â€œgeneralizedâ€ solutions to xË™=Xâ¢(x)Ë™ğ‘¥ğ‘‹ğ‘¥\dot{x}=X(x)overË™ start_ARG italic_x end_ARG = italic_X ( italic_x ), usually called Filippov solutions, i.e., solutions to (1.3) are absolutely continuous curves Î¾:â„â†’â„n:ğœ‰â†’â„superscriptâ„ğ‘›\xi:\mathcal{I}\to\mathbb{R}^{n}italic_Î¾ : caligraphic_I â†’ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT such that Î¾Ë™â¢(t)âˆˆâ„±â¢[X]â¢(Î¾â¢(t))Ë™ğœ‰ğ‘¡â„±delimited-[]ğ‘‹ğœ‰ğ‘¡\dot{\xi}(t)\in\mathcal{F}[X](\xi(t))overË™ start_ARG italic_Î¾ end_ARG ( italic_t ) âˆˆ caligraphic_F [ italic_X ] ( italic_Î¾ ( italic_t ) ) for Î»1superscriptğœ†1\lambda^{1}italic_Î» start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT-a.e. tâˆˆâ„ğ‘¡â„t\in\mathcal{I}italic_t âˆˆ caligraphic_I. For more on differential inclusions and discontinuous dynamical systems, we point the reader to [ref:filippov1988differential], [ref:bacciotti2005liapunov, Ch. 1] and [ref:cortes2008discontinuous]. 1.3 Stability To characterize stability under a differential inclusion (1.2), we need a few concepts. Starting with the regular case, for simplicity, let the vector field Fğ¹Fitalic_F be single-valued and smooth. In that case, Fğ¹Fitalic_F generates a flow, denoted Ï†â¢(â‹…;F)ğœ‘â‹…ğ¹\varphi(\cdot;F)italic_Ï† ( â‹… ; italic_F ). A point xâ‹†âˆˆâ„nsuperscriptğ‘¥â‹†superscriptâ„ğ‘›x^{\star}\in\mathbb{R}^{n}italic_x start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is an equilibrium point of Fğ¹Fitalic_F when Fâ¢(xâ‹†)=0ğ¹superscriptğ‘¥â‹†0F(x^{\star})=0italic_F ( italic_x start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT ) = 0 or equivalently Ï†tâ¢(0;F)=0superscriptğœ‘ğ‘¡0ğ¹0\varphi^{t}(0;F)=0italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( 0 ; italic_F ) = 0 âˆ€tâˆˆâ„for-allğ‘¡â„\forall t\in\mathbb{R}âˆ€ italic_t âˆˆ blackboard_R. We will set xâ‹†superscriptğ‘¥â‹†x^{\star}italic_x start_POSTSUPERSCRIPT â‹† end_POSTSUPERSCRIPT to be 00, unless stated otherwise. Then, 00 is said to be globally asymptotically stable (GAS) (under Fğ¹Fitalic_F) when (s.i)formulae-sequenceğ‘ ğ‘–(s.i)( italic_s . italic_i ) 00 is Lyapunov stable, that is, for any open neighbourhood UÎµâˆ‹00subscriptğ‘ˆğœ€U_{\varepsilon}\ni 0italic_U start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT âˆ‹ 0 there is an open set UÎ´âŠ†UÎµsubscriptğ‘ˆğ›¿subscriptğ‘ˆğœ€U_{\delta}\subseteq U_{\varepsilon}italic_U start_POSTSUBSCRIPT italic_Î´ end_POSTSUBSCRIPT âŠ† italic_U start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT such that Ï†tâ¢(UÎ´;F)âŠ†UÎµsuperscriptğœ‘ğ‘¡subscriptğ‘ˆğ›¿ğ¹subscriptğ‘ˆğœ€\varphi^{t}(U_{\delta};F)\subseteq U_{\varepsilon}italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_U start_POSTSUBSCRIPT italic_Î´ end_POSTSUBSCRIPT ; italic_F ) âŠ† italic_U start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT âˆ€tâˆˆâ„â‰¥0for-allğ‘¡subscriptâ„absent0\forall t\in\mathbb{R}_{\geq 0}âˆ€ italic_t âˆˆ blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT; (s.iâ¢i)formulae-sequenceğ‘ ğ‘–ğ‘–(s.ii)( italic_s . italic_i italic_i ) 00 is globally attractive, that is, limtâ†’+âˆÏ†tâ¢(x0;F)=0subscriptâ†’ğ‘¡superscriptğœ‘ğ‘¡subscriptğ‘¥0ğ¹0\lim_{t\to+\infty}\varphi^{t}(x_{0};F)=0roman_lim start_POSTSUBSCRIPT italic_t â†’ + âˆ end_POSTSUBSCRIPT italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ; italic_F ) = 0 for all x0âˆˆâ„nsubscriptğ‘¥0superscriptâ„ğ‘›x_{0}\in\mathbb{R}^{n}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. Remark 1.2 (On global stability). Although we work with â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, or Mnâ‰ƒdâ„nsubscriptsimilar-to-or-equalsğ‘‘superscriptğ‘€ğ‘›superscriptâ„ğ‘›M^{n}\simeq_{d}\mathbb{R}^{n}italic_M start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT â‰ƒ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, we take a topological approach akin to [ref:bhatiahajek2006local] and hence the definition of GAS is as above. When working with metrics, one typically says more, e.g., see [ref:wilson1969smoothing, ref:lin1996smoothV2], there, a metric is used to characterize how Lyapunov stability can be truly turned non-local (e.g., let Brâ¢(x)subscriptğµğ‘Ÿğ‘¥B_{r}(x)italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_x ) be some rğ‘Ÿritalic_r-metric ball at xğ‘¥xitalic_x, then, there is a some ğ’¦âˆsubscriptğ’¦\mathcal{K}_{\infty}caligraphic_K start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT function Î´ğ›¿\deltaitalic_Î´ such that for any Îµğœ€\varepsilonitalic_Îµ we have that Ï†tâ¢(BÎ´â¢(Îµ)â¢(x))âŠ†Ï†tâ¢(BÎµâ¢(x))superscriptğœ‘ğ‘¡subscriptğµğ›¿ğœ€ğ‘¥superscriptğœ‘ğ‘¡subscriptğµğœ€ğ‘¥\varphi^{t}(B_{\delta(\varepsilon)}(x))\subseteq\varphi^{t}(B_{\varepsilon}(x))italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_B start_POSTSUBSCRIPT italic_Î´ ( italic_Îµ ) end_POSTSUBSCRIPT ( italic_x ) ) âŠ† italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_B start_POSTSUBSCRIPT italic_Îµ end_POSTSUBSCRIPT ( italic_x ) ) âˆ€tâ‰¥0for-allğ‘¡0\forall t\geq 0âˆ€ italic_t â‰¥ 0.) In general these definitions are not equivalent, however, for being GAS on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, they are [ref:andriano1997global]. âˆ˜\circâˆ˜ Due to the work of Lyapunov [ref:liapunov1892general], we know that to reason about stability, it is worthwhile to look for â€œpotential functionsâ€ that capture stability, illustrated by the fact that his theory effectively replaced the definitions of stability. Specially, we look for a function VâˆˆCâˆâ¢(â„n;â„â‰¥0)ğ‘‰superscriptğ¶superscriptâ„ğ‘›subscriptâ„absent0V\in C^{\infty}(\mathbb{R}^{n};\mathbb{R}_{\geq 0})italic_V âˆˆ italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT ( blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT ), satisfying the following properties: (V.i)formulae-sequenceğ‘‰ğ‘–(V.i)( italic_V . italic_i ) Vâ¢(x)>0ğ‘‰ğ‘¥0V(x)>0italic_V ( italic_x ) > 0 for all xâˆˆâ„nâˆ–{0}ğ‘¥superscriptâ„ğ‘›0x\in\mathbb{R}^{n}\setminus\{0\}italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 } and Vâ¢(0)=0ğ‘‰00V(0)=0italic_V ( 0 ) = 0; (V.iâ¢i)formulae-sequenceğ‘‰ğ‘–ğ‘–(V.ii)( italic_V . italic_i italic_i ) âŸ¨âˆ‡Vâ¢(x),Fâ¢(x)âŸ©<0âˆ‡ğ‘‰ğ‘¥ğ¹ğ‘¥0\langle\nabla V(x),F(x)\rangle<0âŸ¨ âˆ‡ italic_V ( italic_x ) , italic_F ( italic_x ) âŸ© < 0 for all xâˆˆâ„nâˆ–{0}ğ‘¥superscriptâ„ğ‘›0x\in\mathbb{R}^{n}\setminus\{0\}italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 }; (V.iâ¢iâ¢i)formulae-sequenceğ‘‰ğ‘–ğ‘–ğ‘–(V.iii)( italic_V . italic_i italic_i italic_i ) Vğ‘‰Vitalic_V is radially unbounded, that is, Vâ¢(x)â†’+âˆâ†’ğ‘‰ğ‘¥V(x)\to+\inftyitalic_V ( italic_x ) â†’ + âˆ for â€–xâ€–2â†’+âˆâ†’subscriptnormğ‘¥2\|x\|_{2}\to+\inftyâˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â†’ + âˆ. Property (V.iiiformulae-sequenceğ‘‰iiiV.\ref{prop:iii:V}italic_V .) implies sublevel set compactness and is sometimes referred to as weak coercivity. We call such a function a (smooth, strict and proper) Lyapunov function (with respect to the pair (F,0)ğ¹0(F,0)( italic_F , 0 )). This note is all about GAS, so we will omit â€œstrictâ€ and â€œproperâ€ from now on. Then, based on converse theory by Massera, Kurzweil, Wilson and several others [ref:Massera1956, ref:kurzweil1963inversion, ref:wilson1969smoothing, ref:fathi2019smoothing], we can appeal to the celebrated theorem stating that 00 is GAS if and only if there is a (corresponding) smooth Lyapunov function [ref:bacciotti2005liapunov, Thm. 2.4]. A generalization of the above to differential inclusions (1.2) is as follows. Definition 1.1 (Strong Lyapunov pairs [ref:clarke1998asymptotic, Def. 1.1]). A pair of functions (V,W)âˆˆC0â¢(â„n;â„â‰¥0)ğ‘‰ğ‘Šsuperscriptğ¶0superscriptâ„ğ‘›subscriptâ„absent0(V,W)\in C^{0}(\mathbb{R}^{n};\mathbb{R}_{\geq 0})( italic_V , italic_W ) âˆˆ italic_C start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT ), with Vğ‘‰Vitalic_V being Câˆsuperscriptğ¶C^{\infty}italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT-smooth on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and Wğ‘ŠWitalic_W being Câˆsuperscriptğ¶C^{\infty}italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT-smooth on â„nâˆ–{0}superscriptâ„ğ‘›0\mathbb{R}^{n}\setminus\{0\}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 }, is said to be a Câˆsuperscriptğ¶C^{\infty}italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT-smooth strong Lyapunov pair for the vector field Fğ¹Fitalic_F as in (1.2), provided that Fğ¹Fitalic_F is upper semi-continuous and compact, convex valued, plus, the following conditions hold: (i) Vâ¢(x)>0ğ‘‰ğ‘¥0V(x)>0italic_V ( italic_x ) > 0 and Wâ¢(x)>0ğ‘Šğ‘¥0W(x)>0italic_W ( italic_x ) > 0 on â„nâˆ–{0}superscriptâ„ğ‘›0\mathbb{R}^{n}\setminus\{0\}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 }, with Vâ¢(0)=0ğ‘‰00V(0)=0italic_V ( 0 ) = 0; (ii) The sublevel sets of Vğ‘‰Vitalic_V are compact; and (iii) maxvâˆˆFâ¢(x)â¡âŸ¨âˆ‡Vâ¢(x),vâŸ©â‰¤âˆ’Wâ¢(x)subscriptğ‘£ğ¹ğ‘¥âˆ‡ğ‘‰ğ‘¥ğ‘£ğ‘Šğ‘¥\max_{v\in F(x)}\langle\nabla V(x),v\rangle\leq-W(x)roman_max start_POSTSUBSCRIPT italic_v âˆˆ italic_F ( italic_x ) end_POSTSUBSCRIPT âŸ¨ âˆ‡ italic_V ( italic_x ) , italic_v âŸ© â‰¤ - italic_W ( italic_x ) on â„nâˆ–{0}superscriptâ„ğ‘›0\mathbb{R}^{n}\setminus\{0\}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 }. The preposition â€œstrongâ€ comes from the fact that we look at all solutions satisfying (1.2). Similarly, one could require that at least one solution is stable and construct a â€œweakâ€ version of Definition 1.1. For further references, and generalizations of Lyapunovâ€™s stability theory, we point the reader to [ref:bhatia1970stability, ref:sontag2013mathematical, ref:bacciotti2005liapunov, ref:bhatiahajek2006local, ref:goebel2012hybrid]. Before closing this section, we explicitly illustrate why working with Câˆsuperscriptğ¶C^{\infty}italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT or even C0superscriptğ¶0C^{0}italic_C start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT vector fields is arguably overly restrictive when interested in qualitative stability questions. We start with a simple example. Example 1.3 (A semiflow corresponding to a vector field with bounded discontinuities). Consider the following discontinuous vector field on â„â„\mathbb{R}blackboard_R: xË™=X1â¢(x):=âˆ’sgnâ¢(x):={1x<00x=0âˆ’1x>0.Ë™ğ‘¥subscriptğ‘‹1ğ‘¥assignsgnğ‘¥assigncases1ğ‘¥00ğ‘¥01ğ‘¥0\dot{x}=X_{1}(x):=-\mathrm{sgn}(x):=\begin{cases}1\quad&x<0\\ 0\quad&x=0\\ -1\quad&x>0\end{cases}.overË™ start_ARG italic_x end_ARG = italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) := - roman_sgn ( italic_x ) := { start_ROW start_CELL 1 end_CELL start_CELL italic_x < 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL italic_x = 0 end_CELL end_ROW start_ROW start_CELL - 1 end_CELL start_CELL italic_x > 0 end_CELL end_ROW . (1.4) Now, consider the map Ï†1:â„â‰¥0Ã—â„â†’â„:subscriptğœ‘1â†’subscriptâ„absent0â„â„\varphi_{1}:\mathbb{R}_{\geq 0}\times\mathbb{R}\to\mathbb{R}italic_Ï† start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT : blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT Ã— blackboard_R â†’ blackboard_R defined through (t,x)â†¦Ï†1tâ¢(x):={minâ¡{0,x+t}x<00x=0maxâ¡{0,xâˆ’t}x>0.maps-toğ‘¡ğ‘¥superscriptsubscriptğœ‘1ğ‘¡ğ‘¥assigncases0ğ‘¥ğ‘¡ğ‘¥00ğ‘¥00ğ‘¥ğ‘¡ğ‘¥0(t,x)\mapsto\varphi_{1}^{t}(x):=\begin{cases}\min\{0,x+t\}\quad&x<0\\ 0\quad&x=0\\ \max\{0,x-t\}\quad&x>0\end{cases}.( italic_t , italic_x ) â†¦ italic_Ï† start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x ) := { start_ROW start_CELL roman_min { 0 , italic_x + italic_t } end_CELL start_CELL italic_x < 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL italic_x = 0 end_CELL end_ROW start_ROW start_CELL roman_max { 0 , italic_x - italic_t } end_CELL start_CELL italic_x > 0 end_CELL end_ROW . (1.5) One can check that Ï†ğœ‘\varphiitalic_Ï† is a global semiflow, describing a solution (e.g., in the sense of CarathÃ©odory, Krasovskii or Filippov [ref:filippov1988differential]) to (1.4)â€”a discontinuous vector field. In particular, note that tâ†¦Ï†1tâ¢(x)âˆˆACmaps-toğ‘¡superscriptsubscriptğœ‘1ğ‘¡ğ‘¥ACt\mapsto\varphi_{1}^{t}(x)\in\mathrm{AC}italic_t â†¦ italic_Ï† start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x ) âˆˆ roman_AC since Ï†1tâ¢(x)=Ï†10â¢(x)+âˆ«0tâˆ’sgnâ¢(Ï†1Ï„â¢(x))â¢dâ¢Ï„superscriptsubscriptğœ‘1ğ‘¡ğ‘¥superscriptsubscriptğœ‘10ğ‘¥subscriptsuperscriptğ‘¡0sgnsuperscriptsubscriptğœ‘1ğœğ‘¥dğœ\varphi_{1}^{t}(x)=\varphi_{1}^{0}(x)+\int^{t}_{0}-\mathrm{sgn}(\varphi_{1}^{% \tau}(x))\mathrm{d}\tauitalic_Ï† start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x ) = italic_Ï† start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( italic_x ) + âˆ« start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - roman_sgn ( italic_Ï† start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Ï„ end_POSTSUPERSCRIPT ( italic_x ) ) roman_d italic_Ï„ for any tâ‰¥0ğ‘¡0t\geq 0italic_t â‰¥ 0. Regarding stability, consider the Câˆsuperscriptğ¶C^{\infty}italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT Lyapunov function xâ†¦V1â¢(x):=12â¢x2maps-toğ‘¥subscriptğ‘‰1ğ‘¥assign12superscriptğ‘¥2x\mapsto V_{1}(x):=\tfrac{1}{2}x^{2}italic_x â†¦ italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) := divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and find that âˆ‡V1â¢(x)â¢X1â¢(x)=âˆ’|x|<0âˆ‡subscriptğ‘‰1ğ‘¥subscriptğ‘‹1ğ‘¥ğ‘¥0\nabla V_{1}(x)X_{1}(x)=-|x|<0âˆ‡ italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) = - | italic_x | < 0 on â„âˆ–{0}â„0\mathbb{R}\setminus\{0\}blackboard_R âˆ– { 0 } (consider a Huber loss to find a valid function W1subscriptğ‘Š1W_{1}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in the sense of Definition 1.1). This already shows that the existence of a smooth Lyapunov function, asserting that the origin is GAS, does not imply the existence of a flow, nor does it imply that convergence to 00 is merely asymptotic. Now let X2â¢(x):=âˆ’âˆ‚xassignsubscriptğ‘‹2ğ‘¥subscriptğ‘¥X_{2}(x):=-\partial_{x}italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_x ) := - âˆ‚ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT and define [0,1]âˆ‹sâ†¦Xâ¢(â‹…;s):=(1âˆ’s)â¢X1+sâ¢X2contains01ğ‘ maps-toğ‘‹â‹…ğ‘ assign1ğ‘ subscriptğ‘‹1ğ‘ subscriptğ‘‹2[0,1]\ni s\mapsto X(\cdot;s):=(1-s)X_{1}+sX_{2}[ 0 , 1 ] âˆ‹ italic_s â†¦ italic_X ( â‹… ; italic_s ) := ( 1 - italic_s ) italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_s italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Then, for any sâˆˆ[0,1]ğ‘ 01s\in[0,1]italic_s âˆˆ [ 0 , 1 ] we have that âˆ‡Vâ¢(x)â¢Xâ¢(x;s)<0âˆ‡ğ‘‰ğ‘¥ğ‘‹ğ‘¥ğ‘ 0\nabla V(x)X(x;s)<0âˆ‡ italic_V ( italic_x ) italic_X ( italic_x ; italic_s ) < 0 on â„âˆ–{0}â„0\mathbb{R}\setminus\{0\}blackboard_R âˆ– { 0 }. We know that the flow corresponding to X2subscriptğ‘‹2X_{2}italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is (t,x)â†¦Ï†2tâ¢(x):=eâˆ’tâ¢xmaps-toğ‘¡ğ‘¥superscriptsubscriptğœ‘2ğ‘¡ğ‘¥assignsuperscriptğ‘’ğ‘¡ğ‘¥(t,x)\mapsto\varphi_{2}^{t}(x):=e^{-t}x( italic_t , italic_x ) â†¦ italic_Ï† start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x ) := italic_e start_POSTSUPERSCRIPT - italic_t end_POSTSUPERSCRIPT italic_x. Better yet, by direct integration, we find that a global semiflow corresponding to Xâ¢(â‹…;s)ğ‘‹â‹…ğ‘ X(\cdot;s)italic_X ( â‹… ; italic_s ) becomes (t,x)â†¦Ï†tâ¢(x;s):={minâ¡{0,eâˆ’sâ¢tâ¢x+(1âˆ’eâˆ’sâ¢t)â¢(1âˆ’s)/s}x<00x=0maxâ¡{0,eâˆ’sâ¢tâ¢x+(eâˆ’sâ¢tâˆ’1)â¢(1âˆ’s)/s}x>0.maps-toğ‘¡ğ‘¥superscriptğœ‘ğ‘¡ğ‘¥ğ‘ assigncases0superscriptğ‘’ğ‘ ğ‘¡ğ‘¥1superscriptğ‘’ğ‘ ğ‘¡1ğ‘ ğ‘ ğ‘¥00ğ‘¥00superscriptğ‘’ğ‘ ğ‘¡ğ‘¥superscriptğ‘’ğ‘ ğ‘¡11ğ‘ ğ‘ ğ‘¥0(t,x)\mapsto\varphi^{t}(x;s):=\begin{cases}\min\{0,e^{-st}x+(1-e^{-st})(1-s)/s% \}\quad&x<0\\ 0\quad&x=0\\ \max\{0,e^{-st}x+(e^{-st}-1)(1-s)/s\}\quad&x>0\end{cases}.( italic_t , italic_x ) â†¦ italic_Ï† start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x ; italic_s ) := { start_ROW start_CELL roman_min { 0 , italic_e start_POSTSUPERSCRIPT - italic_s italic_t end_POSTSUPERSCRIPT italic_x + ( 1 - italic_e start_POSTSUPERSCRIPT - italic_s italic_t end_POSTSUPERSCRIPT ) ( 1 - italic_s ) / italic_s } end_CELL start_CELL italic_x < 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL italic_x = 0 end_CELL end_ROW start_ROW start_CELL roman_max { 0 , italic_e start_POSTSUPERSCRIPT - italic_s italic_t end_POSTSUPERSCRIPT italic_x + ( italic_e start_POSTSUPERSCRIPT - italic_s italic_t end_POSTSUPERSCRIPT - 1 ) ( 1 - italic_s ) / italic_s } end_CELL start_CELL italic_x > 0 end_CELL end_ROW . Indeed, sâ†¦Ï†â¢(â‹…;s)maps-toğ‘ ğœ‘â‹…ğ‘ s\mapsto\varphi(\cdot;s)italic_s â†¦ italic_Ï† ( â‹… ; italic_s ) parametrizes a homotopy, along global semiflows such that 00 is GAS, from Ï†1subscriptğœ‘1\varphi_{1}italic_Ï† start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to Ï†2subscriptğœ‘2\varphi_{2}italic_Ï† start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Regarding limsâ†’0+Ï†â¢(â‹…;s)subscriptâ†’ğ‘ superscript0ğœ‘â‹…ğ‘ \lim_{s\to 0^{+}}\varphi(\cdot;s)roman_lim start_POSTSUBSCRIPT italic_s â†’ 0 start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_Ï† ( â‹… ; italic_s ), use that eâˆ’sâ¢t=1âˆ’sâ¢t+âˆ‘n=2âˆ((âˆ’sâ¢t)n/n!)superscriptğ‘’ğ‘ ğ‘¡1ğ‘ ğ‘¡subscriptsuperscriptğ‘›2superscriptğ‘ ğ‘¡ğ‘›ğ‘›e^{-st}=1-st+\sum^{\infty}_{n=2}((-st)^{n}/n!)italic_e start_POSTSUPERSCRIPT - italic_s italic_t end_POSTSUPERSCRIPT = 1 - italic_s italic_t + âˆ‘ start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n = 2 end_POSTSUBSCRIPT ( ( - italic_s italic_t ) start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT / italic_n ! ) to recover (1.5). It is interesting to note that although (1.5) is the unique (AC) solution to xË™âˆˆâ„±â¢[X1]â¢(x)Ë™ğ‘¥â„±delimited-[]subscriptğ‘‹1ğ‘¥\dot{x}\in\mathcal{F}[X_{1}](x)overË™ start_ARG italic_x end_ARG âˆˆ caligraphic_F [ italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] ( italic_x ), the solution to xË™âˆˆâ„±â¢[âˆ’X1]â¢(x)Ë™ğ‘¥â„±delimited-[]subscriptğ‘‹1ğ‘¥\dot{x}\in\mathcal{F}[-X_{1}](x)overË™ start_ARG italic_x end_ARG âˆˆ caligraphic_F [ - italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] ( italic_x ) is not unique, stability is key. âˆ˜\circâˆ˜ Next, we provide an example that will appear later. Example 1.4 (An irregular gradient flow). Let Î³âˆˆğ’¦âˆğ›¾subscriptğ’¦\gamma\in\mathcal{K}_{\infty}italic_Î³ âˆˆ caligraphic_K start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT be smooth on (0,+âˆ)0(0,+\infty)( 0 , + âˆ ) and such that Î³â¢(s)/Î³â€²â¢(s)â‰¥sğ›¾ğ‘ superscriptğ›¾â€²ğ‘ ğ‘ \gamma(s)/\gamma^{\prime}(s)\geq sitalic_Î³ ( italic_s ) / italic_Î³ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_s ) â‰¥ italic_s (e.g., sâ†¦Î³â¢(s)=s(1/2)maps-toğ‘ ğ›¾ğ‘ superscriptğ‘ 12s\mapsto\gamma(s)=s^{(1/2)}italic_s â†¦ italic_Î³ ( italic_s ) = italic_s start_POSTSUPERSCRIPT ( 1 / 2 ) end_POSTSUPERSCRIPT). Now consider the function xâ†¦VÎ³â¢(x):=Î³â¢(â€–xâ€–2)maps-toğ‘¥subscriptğ‘‰ğ›¾ğ‘¥assignğ›¾subscriptnormğ‘¥2x\mapsto V_{\gamma}(x):=\gamma(\|x\|_{2})italic_x â†¦ italic_V start_POSTSUBSCRIPT italic_Î³ end_POSTSUBSCRIPT ( italic_x ) := italic_Î³ ( âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) on â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, which is Câˆsuperscriptğ¶C^{\infty}italic_C start_POSTSUPERSCRIPT âˆ end_POSTSUPERSCRIPT on â„nâˆ–{0}superscriptâ„ğ‘›0\mathbb{R}^{n}\setminus\{0\}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 }. Next, construct the vector field xË™=X3â¢(x):={âˆ’âˆ‡VÎ³â¢(x)xâ‰ 00x=0.Ë™ğ‘¥subscriptğ‘‹3ğ‘¥assigncasesâˆ‡subscriptğ‘‰ğ›¾ğ‘¥ğ‘¥00ğ‘¥0\dot{x}=X_{3}(x):=\begin{cases}-\nabla V_{\gamma}(x)\quad&x\neq 0\\ 0\quad&x=0\end{cases}.overË™ start_ARG italic_x end_ARG = italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_x ) := { start_ROW start_CELL - âˆ‡ italic_V start_POSTSUBSCRIPT italic_Î³ end_POSTSUBSCRIPT ( italic_x ) end_CELL start_CELL italic_x â‰  0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL italic_x = 0 end_CELL end_ROW . (1.6) We cannot immediately appeal to Filippovâ€™s framework as X3subscriptğ‘‹3X_{3}italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT is not necessarily a bounded operator. However, since âˆ’âˆ‡VÎ³â¢(x)=âˆ’Î³â€²â¢(â€–xâ€–2)â¢x/â€–xâ€–2âˆ‡subscriptğ‘‰ğ›¾ğ‘¥superscriptğ›¾â€²subscriptnormğ‘¥2ğ‘¥subscriptnormğ‘¥2-\nabla V_{\gamma}(x)=-\gamma^{\prime}(\|x\|_{2})x/\|x\|_{2}- âˆ‡ italic_V start_POSTSUBSCRIPT italic_Î³ end_POSTSUBSCRIPT ( italic_x ) = - italic_Î³ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) italic_x / âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT we can study solutions to (1.6) directly. To that end, decompose xâˆˆâ„nâˆ–{0}ğ‘¥superscriptâ„ğ‘›0x\in\mathbb{R}^{n}\setminus\{0\}italic_x âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT âˆ– { 0 } as x=âˆ¥xâˆ¥2â‹…x/âˆ¥xâˆ¥2=:râ‹…ux=\|x\|_{2}\cdot x/\|x\|_{2}=:r\cdot uitalic_x = âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‹… italic_x / âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = : italic_r â‹… italic_u. It readily follows that rË™=âˆ’Î³â€²â¢(r)Ë™ğ‘Ÿsuperscriptğ›¾â€²ğ‘Ÿ\dot{r}=-\gamma^{\prime}(r)overË™ start_ARG italic_r end_ARG = - italic_Î³ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_r ) while uË™=0Ë™ğ‘¢0\dot{u}=0overË™ start_ARG italic_u end_ARG = 0. In general Î³â€²â¢(s)>0superscriptğ›¾â€²ğ‘ 0\gamma^{\prime}(s)>0italic_Î³ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_s ) > 0 âˆ€sâ‰¥0for-allğ‘ 0\forall s\geq 0âˆ€ italic_s â‰¥ 0, need not be true, but suppose this is true for our choice of Î³ğ›¾\gammaitalic_Î³, e.g., pick again sâ†¦Î³â¢(s)=s1/2maps-toğ‘ ğ›¾ğ‘ superscriptğ‘ 12s\mapsto\gamma(s)=s^{1/2}italic_s â†¦ italic_Î³ ( italic_s ) = italic_s start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT. Now, suppose that 1/Î³â€²â¢(s)1superscriptğ›¾â€²ğ‘ 1/\gamma^{\prime}(s)1 / italic_Î³ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_s ) is continuous and of class ğ’¦ğ’¦\mathcal{K}caligraphic_K on â„â‰¥0subscriptâ„absent0\mathbb{R}_{\geq 0}blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT, then, we can define Î“â¢(r):=âˆ«0r1/Î³â€²â¢(Ï)â¢dÏassignÎ“ğ‘Ÿsubscriptsuperscriptğ‘Ÿ01superscriptğ›¾â€²ğœŒdifferential-dğœŒ\Gamma(r):=\int^{r}_{0}1/\gamma^{\prime}(\rho)\mathrm{d}\rhoroman_Î“ ( italic_r ) := âˆ« start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT 1 / italic_Î³ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ( italic_Ï ) roman_d italic_Ï, which is now of class ğ’¦âˆsubscriptğ’¦\mathcal{K}_{\infty}caligraphic_K start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT and hence invertible on â„â‰¥0subscriptâ„absent0\mathbb{R}_{\geq 0}blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT. Under the aforementioned assumptions, we can define a semiflow corresponding to X3subscriptğ‘‹3X_{3}italic_X start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT via (e.g., derived via the inverse function theorem): (t,x)â†¦Ï†3tâ¢(x):={minâ¡{0,Î“âˆ’1â¢(Î“â¢(â€–xâ€–2)âˆ’t)}â¢x/â€–xâ€–2â€–xâ€–2â‰¥Î“âˆ’1â¢(t)0else.maps-toğ‘¡ğ‘¥superscriptsubscriptğœ‘3ğ‘¡ğ‘¥assigncases0superscriptÎ“1Î“subscriptnormğ‘¥2ğ‘¡ğ‘¥subscriptnormğ‘¥2subscriptnormğ‘¥2superscriptÎ“1ğ‘¡0else(t,x)\mapsto\varphi_{3}^{t}(x):=\begin{cases}\min\{0,\Gamma^{-1}(\Gamma(\|x\|_% {2})-t)\}x/\|x\|_{2}\quad&\|x\|_{2}\geq\Gamma^{-1}(t)\\ 0\quad&\mathrm{else}\end{cases}.( italic_t , italic_x ) â†¦ italic_Ï† start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x ) := { start_ROW start_CELL roman_min { 0 , roman_Î“ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( roman_Î“ ( âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) - italic_t ) } italic_x / âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_CELL start_CELL âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰¥ roman_Î“ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_t ) end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL roman_else end_CELL end_ROW . We emphasize that limxâ†’0Ï†3tâ¢(x)=0subscriptâ†’ğ‘¥0superscriptsubscriptğœ‘3ğ‘¡ğ‘¥0\lim_{x\to 0}\varphi_{3}^{t}(x)=0roman_lim start_POSTSUBSCRIPT italic_x â†’ 0 end_POSTSUBSCRIPT italic_Ï† start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_x ) = 0 for any tâ‰¥0ğ‘¡0t\geq 0italic_t â‰¥ 0, which follows for t=0ğ‘¡0t=0italic_t = 0 from Ï†30=idâ„nsubscriptsuperscriptğœ‘03subscriptidsuperscriptâ„ğ‘›\varphi^{0}_{3}=\mathrm{id}_{\mathbb{R}^{n}}italic_Ï† start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = roman_id start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, whereas for t>0ğ‘¡0t>0italic_t > 0 we have that minâ¡{0,Î“âˆ’1â¢(Î“â¢(â€–xâ€–2)âˆ’t)}=00superscriptÎ“1Î“subscriptnormğ‘¥2ğ‘¡0\min\{0,\Gamma^{-1}(\Gamma(\|x\|_{2})-t)\}=0roman_min { 0 , roman_Î“ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( roman_Î“ ( âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) - italic_t ) } = 0 for all â€–xâ€–2â‰¤Î“âˆ’1â¢(t)subscriptnormğ‘¥2superscriptÎ“1ğ‘¡\|x\|_{2}\leq\Gamma^{-1}(t)âˆ¥ italic_x âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT â‰¤ roman_Î“ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_t ). Note, the latter is inherent to the definition of Î“Î“\Gammaroman_Î“. âˆ˜\circâˆ˜"
https://arxiv.org/html/2411.02975v1,Transformer-Based Fault-Tolerant Control for Fixed-Wing UAVs Using Knowledge Distillation and In-Context Adaptation,"This study presents a transformer-based approach for fault-tolerant control in fixed-wing Unmanned Aerial Vehicles (UAVs), designed to adapt in real time to dynamic changes caused by structural damage or actuator failures. Unlike traditional Flight Control Systems (FCSs) that rely on classical control theory and struggle under severe alterations in dynamics, our method directly maps outer-loop reference valuesâ€”altitude, heading, and airspeedâ€”into control commands using the in-context learning and attention mechanisms of transformers, thus bypassing inner-loop controllers and fault-detection layers. Employing a teacher-student knowledge distillation framework, the proposed approach trains a student agent with partial observations by transferring knowledge from a privileged expert agent with full observability, enabling robust performance across diverse failure scenarios. Experimental results demonstrate that our transformer-based controller outperforms industry-standard FCS and state-of-the-art reinforcement learning (RL) methods, maintaining high tracking accuracy and stability in nominal conditions and extreme failure cases, highlighting its potential for enhancing UAV operational safety and reliability.","In recent years, Unmanned Aerial Vehicles (UAVs) have been widely used to perform various applications in complex and critical scenarios, such as search and rescue or autonomous medical transportation. The operational safety and reliability of these aerial robots have become major concerns due to the potential implications of system failures. Unlike other robotics fields, such as manipulation and humanoid locomotion, where advanced control methods are essential for managing complex joint movements, UAV Flight Control Systems (FCSs) in industry typically rely on classical control techniques for low-level control layers. While modern approaches, like Model Predictive Control (MPC), offer significant advantages for high-level tasks such as trajectory planning and collision avoidance [1, 2], they require precise system models, extensive uncertainty handling, and high computational resources, which often make them impractical for low-level UAV control. The simplicity, reliability, and efficiency of classical control techniques have established them as the preferred choice for UAVs attitude control. (a) (b) (c) (d) Figure 1: Trajectory comparison between the proposed transformer-based controller (blue) and an industry-standard FCS (red). Figures (a) and (b) illustrate nominal scenario tracking, while (c) and (d) demonstrate the controllersâ€™ responses to semi-wing damage, with the FCS losing control and the proposed method stabilizing the UAV. However, complex environments and demanding tasks can cause structural damage to the UAV, altering its aerodynamic characteristics and dynamics. Fixed-wing UAVs, in particular, exhibit highly complex, nonlinear dynamics, which can be significantly disrupted if the structure is compromised. Although current FCSs are robust, they struggle to maintain performance when the vehicle dynamics deviate from the original design specifications, sometimes leading to control divergence and catastrophic failure. Fault-tolerant flight control has become a focal point for safety-critical UAV operations. Typically, fault-tolerant methods rely on fault detection and diagnosis techniques, which identify faults and then adjust controller parameters to account for the new dynamics [3, 4, 5, 6]. This approach is complex, requiring real-time fault identification and parameter adjustment in a highly nonlinear dynamic environment. Reinforcement Learning (RL) has introduced alternative solutions to the problem [7, 8, 9, 10]. With its ability to handle high-dimensional, nonlinear dynamics, RL holds promise for system fault management. However, RL algorithms are typically designed for Markov Decision Process (MDP) formulations, while fault-tolerant controlâ€”with sudden, unobserved changes in dynamicsâ€”must be framed as a Partially Observable Markov Decision Process (POMDP), making RL algorithms learning more difficult and possibly leading them to suboptimal performance [11]. Although recent works have shown possible solutions to this problem [11, 12, 13], these add considerable complexity to the algorithms. Most research focuses on actuator faults in multicopter UAVs [4, 9] or on fixed-wing UAV fault tolerance at the inner attitude control loop level, avoiding altitude, heading, and airspeed tracking [7, 8]. In this work, we propose a novel transformer-based fault-tolerant control method to directly map reference points in the outer loop of attitude UAV controlâ€”altitude (hâ„hitalic_h), heading (Î¨Î¨\Psiroman_Î¨), and airspeed (VTsubscriptğ‘‰ğ‘‡V_{T}italic_V start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT)â€”into control-surface and throttle commands, enabling full UAV control without the need for inner control loops, which complicate the system. Our method employs the attention mechanisms and in-context learning capabilities of transformer models to adapt control actions to dynamic changes during inference, thereby eliminating the need for fault detection or identification and parameter adjustments in the Flight Control System (FCS). Our transformer-based controller uses a context window of past UAV states to autonomously detect and adapt to dynamic changes. Fig. 1 shows a comparison between our proposed transformer-based controller (blue trajectory) and an industry-standard FCS (red trajectory). In Fig. 1(a) and Fig. 1(b), both systems track the commanded references in nominal conditions. Figs. 1(c) and 1(d) illustrate the response when the UAV experiences semi-wing damage, showing the FCS losing control and causing a crash, while our method stabilizes the UAV and follows the reference values. The contributions of this work are as follows: â€¢ We present a novel learning framework based on teacher-student knowledge distillation for learning fault-tolerant policies in fixed-wing UAVs. In this framework, the teacher agent is trained using RL on privileged environment information to address partial observability limitations, then a student agent is trained without privileged information using the teacherâ€™s interactions with the environment. â€¢ We design a transformer-based flight controller that utilizes in-context learning to adapt its behavior in real-time as UAV dynamics change due to failures. This controller employs a context of past states to determine actions, eliminating the need for fault detection methods. â€¢ We conduct a comparative study against state-of-the-art methods, addressing not only actuator faults but also significant structural damage scenarios."
https://arxiv.org/html/2411.02957v1,Embedding Safety into RL: A New Take on Trust Region Methods,"Reinforcement Learning (RL) agents are able to solve a wide variety of tasks but are prone to producing unsafe behaviors. Constrained Markov Decision Processes (CMDPs) provide a popular framework for incorporating safety constraints. However, common solution methods often compromise reward maximization by being overly conservative or allow unsafe behavior during training. We propose Constrained Trust Region Policy Optimization (C-TRPO), a novel approach that modifies the geometry of the policy space based on the safety constraints and yields trust regions composed exclusively of safe policies, ensuring constraint satisfaction throughout training. We theoretically study the convergence and update properties of C-TRPO and highlight connections to TRPO, Natural Policy Gradient (NPG), and Constrained Policy Optimization (CPO). Finally, we demonstrate experimentally that C-TRPO significantly reduces constraint violations while achieving competitive reward maximization compared to state-of-the-art CMDP algorithms.","Reinforcement Learning (RL) has emerged as a highly successful paradigm in machine learning for solving sequential decision and control problems, with policy gradient (PG) algorithms as a popular approach (Williams, 1992; Sutton et al., 1999; Konda & Tsitsiklis, 1999). Policy gradients are especially appealing for high-dimensional continuous control because they can be easily extended to function approximation. Due to their flexibility and generality, there has been significant progress in enhancing PGs to work robustly with deep neural network-based approaches. Variants of natural policy gradient methods such as Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO) are among the most widely used general-purpose reinforcement learning algorithms (Schulman et al., 2017a; b). While flexibility makes PGs popular among practitioners, it comes at a price: the policy is free to explore any behavior during training, which poses substantial risk for applying such methods to real-world problems. Many methods have been introduced to improve the safety of policy gradients, often based on the Constrained Markov Decision Process (CMDP) formulation. However, existing methods are either limited in their ability to ensure minimal constraint violations during training or do so by severely limiting the agentâ€™s performance. This work introduces a simple strategy that can be used alongside trust-region-based safe policy gradient approaches to improve their constraint satisfaction throughout training, without sacrificing performance. We propose a novel family of policy divergences inspired by barrier function methods in optimization and safe control. These improve constraint satisfaction by altering the policy geometry to yield trust regions consisting exclusively of safe policies. This approach is motivated by the observation that TRPO and related methods base their trust region on the state-average Kullback-Leibler (KL) divergence. It can be derived as the Bregman divergence induced by the negative conditional entropy on the space of state-action occupancies, as shown by Neu et al. (2017). The main insight of the present work is that safer trust regions can be derived by altering this function to incorporate the cost constraints. The resulting divergence is skewed away from the constraint surface, which is achieved by augmenting the negative conditional entropy by another convex barrier-like function. Manipulating the policy divergence in this way allows us to obtain a provably safe trust region-based policy optimization algorithm that retains most of TRPOâ€™s mechanisms and guarantees, simplifying existing methods, while achieving competitive returns with less constraint violations throughout training. Related work Classic solution methods for CMDPs rely on linear programming techniques, see Altman (1999). However, LP-based approaches struggle to scale, making them unsuitable for high-dimensional or continuous control problems. While there are numerous works on CMDPs, in this section, we focus on model-free, direct policy optimization methods. For example, model-based approaches, like those popularized by Berkenkamp et al. (2017), usually provide stricter guarantees, but tend to be less general. Penalty methods are a widely adopted approach, where the optimization problem is reformulated as a weighted objective that balances rewards and penalties for constraint violations. This is often motivated by Lagrangian duality, where the penalty coefficient is interpreted as the dual variable. Learning the coefficient with stochastic gradient descent presents a popular baseline (Achiam et al., 2017; Ray et al., 2019; Chow et al., 2019; Stooke et al., 2020). However, a naively tuned Lagrange multiplier may not work well in practice due to oscillations and overshoot. To address this issue, Stooke et al. (2020) apply PID control to tune the dual variable during training, which achieves less oscillations around the constraint and faster convergence to a feasible policy, see also Sohrabi et al. (2024). Methods such as P3O (Zhang et al., 2022), IPO (Liu et al., 2021), and those using (smoothed) log-barriers (Usmanova et al., 2024; Zhang et al., 2024; Ni & Kamgarpour, 2024; Dey et al., 2024) propose such weighted penalty-based policy optimization objectives from practical considerations. However, working with an explicit penalty introduces a bias and produces suboptimal policies concerning the original constrained MDP. Our approach is closely related to trust region methods, particularly Constrained Policy Optimization (CPO) (Achiam et al., 2017), which extends TRPO by intersecting the trust region with the set of safe policies to ensure safety throughout training. While CPO offers certain guarantees on constraint satisfaction, it often leads to oscillations near the constraint surface due to cost advantage estimation errors. To address this, Yang et al. (2020) proposed Projection-based CPO (PCPO), which projects onto the safe policy space between updates. However, PCPO can significantly hinder reward maximization in practice. Contributions We summarize our contributions as follows: â€¢ In Section 3, we introduce a modified policy divergence such that every trust region consists of only safe policies. We introduce an idealized TRPO update based on the modified divergence (C-TRPO), an approximate version of C-TRPO for deep function approximation, and a corresponding natural gradient method (C-NPG). â€¢ We provide an efficient implementation of the proposed approximate C-TRPO method, see Section 3.2, which comes with a minimal overhead compared to TRPO (up to the estimation of the expected cost) and no overhead compared to CPO. We demonstrate experimentally that C-TRPO yields competitive returns with smaller constraint violations compared to common safe policy optimization algorithms, see Section 5. â€¢ In Section 4, we introduce C-TRPOâ€™s improvement guarantees and contrast to TRPO and CPO. Further, we show that the C-NPG method is the continuous time limit of C-TRPO and provides global convergence guarantees towards the optimal safe policy; this is in contrast to penalization or barrier methods, which introduce a bias"
https://arxiv.org/html/2411.02824v1,Layer-Adaptive State Pruningfor Deep State Space Models,"Due to the lack of state dimension optimization methods, deep state space models (SSMs) have sacrificed model capacity, training search space, or stability to alleviate computational costs caused by high state dimensions. In this work, we provide a structured pruning method for SSMs, Layer-Adaptive STate pruning (LAST), which reduces the state dimension of each layer in minimizing model-level energy loss by extending modal truncation for a single system. LAST scores are evaluated using â„‹âˆsubscriptâ„‹\mathcal{H}_{\infty}caligraphic_H start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norms of subsystems for each state and layer-wise energy normalization. The scores serve as global pruning criteria, enabling cross-layer comparison of states and layer-adaptive pruning. Across various sequence benchmarks, LAST optimizes previous SSMs, revealing the redundancy and compressibility of their state spaces. Notably, we demonstrate that, on average, pruning 33%percent3333\%33 % of states still maintains performance with 0.52%percent0.520.52\%0.52 % accuracy loss in multi-input multi-output SSMs without retraining. Code is available at https://github.com/msgwak/LAST.","Deep state space models (SSMs) have proven effective in modeling sequential data by optimally compressing input history to internal states (Gu et al., 2020, 2021, 2022b; Gu and Dao, 2023; Zhang et al., 2023; Parnichkun et al., 2024). Given their modeling capabilities, ensuring the feasibility and stability of SSMs during training has become a crucial research focus for achieving efficient learning without divergence. Leveraging the knowledge founded in linear system theory (Kailath, 1980), various advancements have emerged, including stability-guaranteeing parameterization (Gu et al., 2022a), general system architecture (Smith et al., 2023), and efficiency improvements via frequency-domain operations, utilizing the fast Fourier transform and the transfer functions of systems (Gu et al., 2022b, a; Zhang et al., 2023; Parnichkun et al., 2024). One of the main computation and memory contributors of SSMs is the state dimension nğ‘›nitalic_n. Since the initial proposal of SSMs, a multiple single-input single-output (multi-SISO) architecture has been employed for scalable and efficient training Gu et al. (2022b, a); Gu and Dao (2023); Zhang et al. (2023); Parnichkun et al. (2024). In this architecture, rather than directly learning an nğ‘›nitalic_n-dimensional system, smaller-dimensional SISO systems are trained in parallel and then integrated through a channel-mixing layer. Within this structure, Gupta et al. (2022) presented that diagonal systems can achieve matching performance to nondiagonal systems. Gu et al. (2022a) introduced a stability-guaranteed model, where the diagonal systems are trained to satisfy the necessary and sufficient stability condition. Instead of utilizing multiple SISO systems in parallel, Smith et al. (2023) adopted a multi-input multi-output (MIMO) architecture, where the enhanced information usage through a MIMO system. This architecture provides high performance with much smaller state dimensions than equivalent block systems in multi-SISO layers. For instance, in the Path-X task that involves the longest tested sequences, this architecture showed state-of-the-art performance (Smith et al., 2023; Parnichkun et al., 2024). However, both architectures lack optimization methods for state dimensions, leading to inefficiencies when the model is over-parameterized for the task. Recently, Parnichkun et al. (2024) parameterized the transfer functions of SISO systems and proposed a state-free inference. However, this approach indirectly trains the poles of the transfer functions, resulting in a restrictive search space or stability being guaranteed only at initialization. Focusing on the stability-guaranteed diagonal SSMs, we develop and verify a layer-adaptive model order reduction (MOR) method for SSMs to identify the least significant states or subsystems in terms of their impact on task performance. Inspired by layer-adaptive neural network pruning (Evci et al., 2020; Lee et al., 2021; Xu et al., 2023) and extending the traditional MOR for a single system (Green and Limebeer, 2012), we propose Layer-Adaptive STate pruning (LAST), where importance scores for learned states are evaluated and used as global pruning criteria. LAST scores measure the relative maximum frequency-domain gain of each subsystem when subsystems with lower scores are excluded, as illustrated in Figure 1. LAST prunes insignificant subsystems to achieve a desired compression level, reducing unnecessary computational and memory costs while bounding the output distortion by the â„‹âˆsubscriptâ„‹\mathcal{H}_{\infty}caligraphic_H start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norms of the pruned subsystems. Figure 1: Illustration of LAST for two layers. Matrices are divided by lines on a per-state basis, and subsystems are sorted in descending order by their â„‹âˆsubscriptâ„‹\mathcal{H}_{\infty}caligraphic_H start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norms. LAST scores are obtained by normalizing each â„‹âˆsubscriptâ„‹\mathcal{H}_{\infty}caligraphic_H start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norm by the sum of all â„‹âˆsubscriptâ„‹\mathcal{H}_{\infty}caligraphic_H start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norms in a layer when the states with lower â„‹âˆsubscriptâ„‹\mathcal{H}_{\infty}caligraphic_H start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norms are excluded. Since LAST scores correlate with model-level output energy loss, we prune all parameters corresponding to states with low LAST scores. We validate the insignificant state identification performance of LAST on long-range sequences, including Long Range Arena (LRA) (Tay et al., 2021) and Speech Command (Warden, 2018) benchmarks. Our results present that previous SSMs have great compressibility, demonstrating that pruning 33% (26.25%) of the trained states resulted in only 0.52% (0.32%) of accuracy loss in MIMO models (in multi-SISO models) on average, including the non-compressible cases."
https://arxiv.org/html/2411.02636v1,Straintronic magnetic tunnel junctions for analog computation: A perspective,"The straintronic magnetic tunnel junction (s-MTJ) is an MTJ whose resistance state can be changed continuously or gradually from high to low with a gate voltage that generates strain the magnetostrictive soft layer. This unusual feature, not usually available in MTJs that are switched abruptly with spin transfer torque, spin-orbit torque or voltage-controlled-magnetic-anisotropy, enables many analog applications where the typically low tunneling magneto-resistance ratio of MTJs (on/off ratio of the switch) and the relatively large switching error rate are not serious impediments unlike in digital logic or memory. More importantly, the transfer characteristic of a s-MTJ (conductance versus gate voltage) always sports a linear region that can be exploited to implement analog arithmetic, vector matrix multiplication and linear synapses in deep learning networks very effectively. In these applications, the s-MTJ is actually superior to the better known memristors and domain wall synapses which do not exhibit the linearity and/or the analog behavior. 111Invited perspective","Magnetic tunnel junctions (MTJs) are the quintessential spin-to-charge converters which convert magnetic information into electrical information. They are widely used in magnetic memory and logic as a binary switch. The archetypal MTJ is shown in Fig. 1(a) and consists of an insulating spacer layer sandwiched between two ferromagnetic layers of different material composition or different thickness. All layers are slightly elliptical in cross-section, so that the magnetization of both ferromagnetic layers has an easy axis along the major axis of the ellipse and hence will tend to align along one of the two directions parallel to the major axis. One of the ferromagnetic layers has a fixed magnetization (along the major axis) and is referred to as the â€œhard layerâ€, while the otherâ€™s magnetization can be changed by an external agent, such as a current or a voltage. It is referred to as the â€œsoft layerâ€. The resistance of the MTJ, Rğ‘…Ritalic_R, measured between the two ferromagnetic layers, is determined by spin-dependent tunneling of electrons between the two ferromagnetic layers, through the insulating spacer layer, and therefore depends on the relative magnetization orientations of the two ferromagnetic layers. The resistance is given approximately by the expression Râ¢(Î¸)=RP+RAâ¢Pâˆ’RP2â¢[1âˆ’câ¢oâ¢sâ¢Î¸],ğ‘…ğœƒsubscriptğ‘…ğ‘ƒsubscriptğ‘…ğ´ğ‘ƒsubscriptğ‘…ğ‘ƒ2delimited-[]1ğ‘ğ‘œğ‘ ğœƒR(\theta)=R_{P}+{{R_{AP}-R_{P}}\over{2}}[1-cos\theta],italic_R ( italic_Î¸ ) = italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT + divide start_ARG italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT - italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG [ 1 - italic_c italic_o italic_s italic_Î¸ ] , (1) where Î¸ğœƒ\thetaitalic_Î¸ is the angle between the magnetizations of the hard and the soft layers, RPsubscriptğ‘…ğ‘ƒR_{P}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT is the resistance when the two magnetizations are mutually parallel and RAâ¢Psubscriptğ‘…ğ´ğ‘ƒR_{AP}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT is the resistance when the two magnetizations are mutually antiparallel. Usually, RAâ¢P>RPsubscriptğ‘…ğ´ğ‘ƒsubscriptğ‘…ğ‘ƒR_{AP}>R_{P}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT > italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT. The above expression for the resistance shows that one can change it by changing the angle Î¸ğœƒ\thetaitalic_Î¸ with an external agent. The external agent can be a current passed through the MTJ which will tend to make the magnetizations parallel or antiparallel depending on the polarity of the current via the mechanism of spin transfer torque (STT) [1, 2]. Another technique is to place the MTJ on a strip of heavy metal (HM) like Pt or Î²ğ›½\betaitalic_Î²-Ta (with the soft layer in contact with the strip) and pass a current through the strip which exerts a spin-orbit torque (SOT) on the soft layerâ€™s magnetization to rotate it [3], making it either parallel or antiparallel to the magnetization of the hard layer and thereby switching the MTJâ€™s resistance from RPsubscriptğ‘…ğ‘ƒR_{P}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT to RAâ¢Psubscriptğ‘…ğ´ğ‘ƒR_{AP}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT. or vice versa. The heavy metal can be replaced with a topological insulator (TI) [4] or Weyl semi-metal [5] which also exerts a spin-orbit torque to rotate the soft layerâ€™s magnetization to effect the switching. Yet another technique is to use a voltage instead of a current and it is best suited to ferromagnetic materials whose stable magnetization orientations are out of plane (â€œupâ€ and â€œdownâ€) owing to perpendicular magnetic anisotropy. The voltage changes the anisotropy of the soft layer from perpendicular to in-plane and hence rotates the magnetization by 90âˆ˜, bringing it from out-of-plane to in-plane. An in-plane magnetic field then causes the magnetization to precess further around it and the voltage pulse is shaped such that it reaches zero when the precession has caused another 90âˆ˜ rotation to complete a full 180âˆ˜ rotation of the magnetization. This is referred to as voltage controlled magnetic anisotropy (VCMA) based switching [6]. Figure 1: (a) A magnetic tunnel junction (MTJ). (b) Switching the MTJ resistance with spin transfer torque (STT). (c) Switching the resistance with spin-orbit torque (SOT). (d) Switching the resistance with voltage-controlled-magnetic-anisotropy (VCMA). (d) Straintronic switching. The diagrams are not to scale. The fourth methodology is relatively uncommon and will work only when the soft layer is made of a magnetostrictive material that is placed in elastic contact with a piezoelectric layer. A voltage applied across the piezoelectric generates biaxial strain underneath the MTJ [7] and strains the soft layer whose magnetization rotates from the major axis toward the minor axis depending on the sign of the strain (compressive or tensile) [8]. If the strain is withdrawn as soon as the magnetization has rotated through 90âˆ˜ in the soft layerâ€™s plane, then a residual torque on the magnetization associated with the out-of-plane excursion of the magnetization during rotation will result in another 90âˆ˜ rotation in-plane to complete a full 180âˆ˜ rotation [9]. There are other (more complicated) ways of flipping the magnetization with strain which do not require precise timing of the voltage pulse (similar to VCMA), but they require applying voltage pulses sequentially using different pairs of gates [10]. The modality of switching magnetization with strain (where the strain can be generated electrically with the aid of a piezolectric layer placed underneath the soft layer) is referred to as â€œstraintronicsâ€ [11]. The two voltage-controlled mechanisms â€“- VCMA and straintronics â€“- are more energy-efficient than the two current controlled mechanisms (STT and SOT), but also much more error-prone. There is seemingly always a trade-off between energy cost and reliability in binary switching which is almost universal [12]. The four magnetization flipping mechanisms that were discussed are shown in Figs. 1(b) - (e). Figure 2: The net effective magnetic field experienced by the soft layer of a straintronic magnetic tunnel junction in the presence of: (1) strain of the appropriate sign and (2) dipole coupling with the hard layer. ğ‡ssubscriptğ‡ğ‘ {\bf H}_{s}bold_H start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is the effective magnetic field due to strain and ğ‡dsubscriptğ‡ğ‘‘{\bf H}_{d}bold_H start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT is the effective magnetic field due to dipole coupling with the hard layer. By varying ğ‡ssubscriptğ‡ğ‘ {\bf H}_{s}bold_H start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT continuously with a gate voltage, one can vary Ï•italic-Ï•\phiitalic_Ï• and hence the resistance of the s-MTJ continuously between RPsubscriptğ‘…ğ‘ƒR_{P}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT and RAâ¢Psubscriptğ‘…ğ´ğ‘ƒR_{AP}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT. One very important distinction between straintronics and the other three switching mechanisms is that the other three usually can rotate the magnetization through 180âˆ˜ and not any arbitrary angle. This will allow the MTJ resistance to assume only two values, RPsubscriptğ‘…ğ‘ƒR_{P}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT and RAâ¢Psubscriptğ‘…ğ´ğ‘ƒR_{AP}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT. Hence those three mechanisms can implement a binary switch good for digital applications. Straintronics can also implement a binary switch, but additionally it is possible to rotate the magnetization of the soft layer through any arbitrary angle Ï•italic-Ï•\phiitalic_Ï• between 0âˆ˜ and 90âˆ˜ and orient the magnetization anywhere between the easy and hard axes, i.e, anywhere between the major and minor axes [13] as long as the voltage generating the strain is kept on. Any intermediate orientation is very stable against thermal noise since the potential well that forms around that state is very deep (several tens of kT) [14]. This will allow the MTJ resistance to assume arbitrary values between RPsubscriptğ‘…ğ‘ƒR_{P}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT and RAâ¢Psubscriptğ‘…ğ´ğ‘ƒR_{AP}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT, which lends itself to analog applications that are far less explored than digital applications of MTJs and yet can be more rewarding since they are usually more error-tolerant. The above feature that allows a straintronic MTJâ€™s resistance to assume any arbitrary value between RPsubscriptğ‘…ğ‘ƒR_{P}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT and RAâ¢Psubscriptğ‘…ğ´ğ‘ƒR_{AP}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT requires some amount of dipole coupling between the hard and the soft layers of the MTJ. Dipole coupling acts like an effective magnetic field ğ‡dsubscriptğ‡ğ‘‘{\bf H}_{d}bold_H start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT along the easy (major) axis of the soft layer whose direction is antiparallel to the magnetization of the hard layer [13]. The strain, on the other hand, acts like an effective magnetic field ğ‡ssubscriptğ‡ğ‘ {\bf H}_{s}bold_H start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT along the hard axis (minor axis) of the elliptical soft layer. The resultant effective magnetic field ğ‡ğ‡{\bf H}bold_H will point along the vector sum of the two fields as shown in Fig. 2. By varying the strain continuously with an analog voltage, we can continuously vary ğ‡ssubscriptğ‡ğ‘ {\bf H}_{s}bold_H start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and hence change the direction of ğ‡ğ‡{\bf H}bold_H (or the angle Ï•italic-Ï•\phiitalic_Ï•). Since the magnetization will ultimately align along ğ‡ğ‡{\bf H}bold_H, we can rotate the magnetization of the soft layer through any angle Ï•italic-Ï•\phiitalic_Ï• between 0âˆ˜ and 90âˆ˜, and hence change the resistance of the MTJ continuously in an analog fashion. Figure 3: (a) A domain wall synapse consisting of a magnetic tunnel junction placed over a heavy metal layer. Successive current pulses passed through the heavy metal exerts successive spin-orbit torques on the soft layer that is in contact with the heavy metal. These pulses move the domain wall in steps thereby changing the fraction of the soft layer whose magnetization is parallel (antiparallel) to that of the hard layer. The resistance of the MTJ is the parallel combination of three resistances consisting of the fraction where the magnetizations of the hard and soft layer are parallel, the fraction where they are antiparallel and the remaining fraction involving the domain wall. (b) A grooved soft layer where the domain wall position is stabilized against â€œcreepâ€ which is back-and-forth motion of the domain wall due to thermal noise. There are other quasi-continuous ways of varying an MTJâ€™s resistance that do not involve straintronics. One is by propagating a domain wall through a rectangular soft layer in steps [15, 16, 17]. Successive current pulses applied through a heavy-metal or topological insulator underlayer (see Fig. 1 (c)) will generate successive spin orbit torque pulses on the soft layer and move a domain wall along the length of the soft layer in steps of xisubscriptğ‘¥ğ‘–x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. This changes the fraction of the soft layer whose magnetization is parallel (antiparallel) to that of the hard layer in steps and hence changes the resistance of the MTJ in steps (see Fig. 3(a); RNsubscriptğ‘…ğ‘R_{N}italic_R start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT is the resistance of the MTJ after Nğ‘Nitalic_N current pulses). This, of course, does not enable truly continuous change (since it is in discrete steps), but allows the MTJ resistance to assume (discrete) intermediate values between RPsubscriptğ‘…ğ‘ƒR_{P}italic_R start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT and RAâ¢Psubscriptğ‘…ğ´ğ‘ƒR_{AP}italic_R start_POSTSUBSCRIPT italic_A italic_P end_POSTSUBSCRIPT. However, such an approach is easier said than done. Once the domain wall has moved to a location after the current pulse subsides, it does not necessarily stay there, but can move backwards or forwards owing to thermal noise, which makes the MTJ resistance unstable. There are some proposed approaches to counter this effect, such as by making the soft layer grooved as shown in Fig. 3(b), but it is not 100% reliable. The shape of the grooves, the depth of the grooves and the separation between neighboring grooves â€“- all have to be carefully designed and controlled to minimize undesired domain wall movement or â€œcreepâ€ due to thermal noise. Even then, stability of the MTJ resistance could be elusive. There are many other ways of changing an MTJâ€™s resistance in a quasi-analog fashion, such as by gradually changing the magnetization of a ferromagnetic soft layer exchange coupled to an antiferromagnetic layer [18] via pulsed current flow through the antiferromagnetic layer, or by switching the grains of a granular soft layer one after another [19], or by nucleating an increased number of skyrmions in the soft layer [20]. These are difficult-to-control techniques and they are only quasi-continuous, not completely continuous. In contrast, the method involving straintronics to change an MTJâ€™s resistance can be truly continuous and has other desirable properties such as: (1) any intermediate state between the 0âˆ˜ and 90âˆ˜ (0âˆ˜ â‰¤\leqâ‰¤ Ï•italic-Ï•\phiitalic_Ï• â‰¤\leqâ‰¤ 90âˆ˜) is extremely stable against thermal noise [14], and (2) the conductance of the MTJ, Gsâˆ’Mâ¢Tâ¢Jsubscriptğºğ‘ ğ‘€ğ‘‡ğ½G_{s-MTJ}italic_G start_POSTSUBSCRIPT italic_s - italic_M italic_T italic_J end_POSTSUBSCRIPT, can be varied linearly with a gate voltage VGsubscriptğ‘‰ğºV_{G}italic_V start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT. The linearity is extremely valuable since it is generally elusive. Many applications in analog computing will require and benefit from this linearity."
https://arxiv.org/html/2411.02447v1,qGDP:Quantum Legalization andDetailedPlacement for Superconducting Quantum Computers,"Noisy Intermediate-Scale Quantum (NISQ) computers are currently limited by their qubit numbers, which hampers progress towards fault-tolerant quantum computing. A major challenge in scaling these systems is crosstalk, which arises from unwanted interactions among neighboring components such as qubits and resonators. An innovative placement strategy tailored for superconducting quantum computers can systematically address crosstalk within the constraints of limited substrate areas.Legalization is a crucial stage in placement process, refining post-global-placement configurations to satisfy design constraints and enhance layout quality. However, existing legalizers are not supported to legalize quantum placements. We aim to address this gap with qGDP, developed to meticulously legalize quantum components by adhering to quantum spatial constraints and reducing resonator crossing to alleviate various crosstalk effects.Our results indicate that qGDP effectively legalizes and fine-tunes the layout, addressing the quantum-specific spatial constraints inherent in various device topologies. By evaluating diverse NISQ benchmarks. qGDP consistently outperforms state-of-the-art legalization engines, delivering substantial improvements in fidelity and reducing spatial violation, with average gains of 34.4Ã—\timesÃ— and 16.9Ã—\timesÃ—, respectively.","The rapid scaling of superconducting quantum computers (QCs) brings formidable challenges, notably managing crosstalk due to unintended electromagnetic interactions among quantum chip components [1, 2, 3]. Such interactions can significantly degrade computational fidelity by affecting gate operations when components with resonant frequencies are closely positioned [3, 4, 5, 6, 7, 8]. Furthermore, larger substrate sizes in superconducting qubits intensify electromagnetic coupling, leading to spurious modes that shorten coherence times and worsen crosstalk [9, 10, 11]. A novel placement strategy, likening quantum device components to charged particles, offers a promising solution by achieving effective spatial and frequency isolation, while also addressing the challenges of substrate and inter-component crosstalk, thereby enhancing the scalability and fidelity of QCs [12]. Figure 1: Impact of placement optimization stages on layout quality. Placement stages in sequence is global placement (GP, gray), legalization (LG, blue), and detailed placement (DP, light blue). The blue and red lines underscore the critical role of legalization. Despite its brief runtime, legalization considerably affects layout quality. Improper legalization can undermine the outcomes from GP, and these issues are often irreparable during DP. Despite advancements in quantum system placement, existing method primarily concentrates on the global placement (GP) stage, which merely determines rough locations for components. However, as quantum systems grow in complexity and scale, the subsequent stages of legalization (LG) and detailed placement (DP) become increasingly crucial. The objective of these stages is to resolve design rule violations and incrementally enhance the overall solution quality [13]. Additionally, each component should be as close as possible to its original position determined during the GP to preserving the GP quality. Figure 1 demonstrates the relationship between layout quality versus placement optimization stages. The LG stage, despite its brief runtime, considerably affects layout quality. Traditional legalizers, illustrated by the red line and designed for classical systems, are inadequate for quantum placements as they fail to address quantum-specific challenges like crosstalk, focusing instead on eliminating overlaps and boundary issues. Such an improper legalization can undermine GP outcomes, and these deficiencies are typically irreparable during DP. Moreover, the partitioning of resonators is a promising technique to enhance the flexibility of the placement design [12]. However, the challenge is reintegrating these wire blocks without causing excessive crosses. Scattered wire blocks lead to numerous crossovers in resonators, necessitating the use of many airbridges [14]. This is problematic as airbridges reduce the fidelity of resonators [15]. Therefore, quantum-aware legalizers, as represented by the blue line in Figure 1, are needed to resolve quantum spatial constraints and resonator crossings. To address above problems and further improve the fidelity of quantum layout without sacrifice of area utilization, we present qGDP, a legalization and detail placement engine tailored for superconducting QCs. qGDP organizes legalization into two phases. Initially, it focuses on qubit legalization by ignoring the resonators, ensuring minimum spacing between qubits and minimal displacement to maintain the quality of GP. Following qubit fixation, qGDP transitions to resonator legalization, focusing on the aforementioned integrity problem. It ensures that wire blocks of each resonator are proximate to at least one other block of the same resonator, again with minimal displacement. Subsequently, qGDP applies a window zoom to areas where theoretical crosstalk exists or resonator crossings occur, extracting and re-placing resonators to reduce crosstalk and resolve crossing points. All above processes are centralized around meeting specific quantum spatial constraints. The contributions of this work are summarized as follows: â€¢ To our knowledge, this research is the first to comprehensively address the challenges of legalization and detailed placement in quantum layout design, significantly enhancing quantum system fidelity and scalability. â€¢ We introduce qGDP, a framework designed to meticulously legalize the quantum layout with adhering quantum spatial constraints, minimizing the resonator crosses and fine-tuning layout details to mitigate various crosstalk. â€¢ To achieve this, qGDP systematically organizes the legalization process into stages for qubits and resonators. followed by the deployment of a detailed placer that identifies and refines regions with spatial violations."
https://arxiv.org/html/2411.02248v1,"Advancing Cyber-Attack Detection in Power Systems: A Comparative Study of Machine Learning and Graph Neural Network Approachesâ€ â€ thanks:This work was supported by the U.S. Department of Energyâ€™s (DOE) Office of Cybersecurity, Energy Security, and Emergency Response (CESER) and performed at the Pacific Northwest National Laboratory (PNNL), operated for the U.S. DOE by Battelle Memorial Institute under Contract No. DE-AC05-76RL01830.","This paper explores the detection and localization of cyber-attacks on time-series measurements data in power systems, focusing on comparing conventional machine learning (ML) like k-means, deep learning method like autoencoder, and graph neural network (GNN)-based techniques. We assess the detection accuracy of these approaches and their potential to pinpoint the locations of specific sensor measurements under attack. Given the demonstrated success of GNNs in other time-series anomaly detection applications, we aim to evaluate their performance within the context of power systems cyber-attacks on sensor measurements. Utilizing the IEEE 68-bus system, we simulated four types of false data attacks, including scaling attacks, additive attacks, and their combinations, to test the selected approaches. Our results indicate that GNN-based methods outperform k-means and autoencoder in detection. Additionally, GNNs show promise in accurately localizing attacks for simple scenarios, although they still face challenges in more complex cases, especially ones that involve combinations of scaling and additive attacks.","Cyber-attacks on power systems can have devastating effects, disrupting essential services and causing significant economic losses [1]. As power systems become increasingly interconnected and digitized, they become more vulnerable to sophisticated cyber-attacks, as demonstrated by real cyber-attacks [2, 3]. Detecting cyber-attacks on power systems in a timely manner is of paramount importance because it allows for swift mitigation measures, minimizing the impact of the attack and ensuring the resilience and reliability of the power grid [4, 5]. Increasing cloud-based communication and control systems, fast-evolving ransomware threats, and the convergence of information technology (IT) and operations technology (OT) systems were identified as among the emerging cybersecurity challenges faced by the power grid in a recent report by the US Department of Energy [6]. Most existing grid cybersecurity solutions either focus only on IT-based intrusion detection [7, 8], or a hybrid (combined IT/OT) intrusion detection but with static, pre-determined, OT rules [9, 10]. While rule-based cyber intrusion detection engines rely on operatorsâ€™ OT knowledge to establish a â€˜baselineâ€™, these suffer from a lack of adaptability, especially as new energy resources with evolving controls and communications are integrated into the grid at an increasing rate [11]. There has been various research focusing on OT-based automated cyber-attack detection for power systems using different algorithmic (non-rule-based) methods, including model-based, machine learning, and deep learning methods. A broad class of the cyber-detection methods use physics-based models with an outlier detection algorithm [12, 13, 14]. Different machine learning-based methods, on the other hand, have deployed time-series modeling approach [15], decision trees [16], ensemble methods [17], or unsupervised k-nearest neighbor approach [18]. More recently, various deep learning-based intrusion detection approaches are being investigated, e.g., the Convolutional Neural Network (CNN) approach [19], deep reinforcement learning [20]. In this research, we study cyber-attacks that are injected into the sensor measurements of voltage angles [21]. We look specifically at the cases when attacks are introduced right after a grid event, where the attacks can be hidden behind transient disturbances. This study explores the application of several machine learning (ML) techniques to detect cyber-attacks on power systems, not only identifying the occurrence of an attack but also investigating the specific buses that are under attack. Traditional ML methods like k-means clustering [22] and deep learning-based methods such as autoencoders [23] combined with k-means were tested in this study. However, the complex and interconnected nature of power systems suggests that graph neural network (GNN) approaches, including Graph Attention Networks (GAT) [24] and Graph Deviation Networks (GDN) [25], might offer superior detection capabilities. This paper details our investigation into these methods, comparing their effectiveness in the simulated environment of the IEEE 68-bus system."
https://arxiv.org/html/2411.02135v1,AI-Ready Energy Modelling for Next Generation RANâ€ â€ thanks:The work of K.Â Sthankiya was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/V519935/1 for the University of Queen Mary University of London Centre for Doctoral Training in Data-Centric Engineering.,"Recent sustainability drives place energy-consumption metrics in centre-stage for the design of future radio access networks (RAN). At the same time, optimising the trade-off between performance and system energy usage by machine-learning (ML) is an approach that requires large amounts of granular RAN data to train models, and to adapt in near realtime. In this paper, we present extensions to the system-level discrete-event AIMM (AI-enabled Massive MIMO) Simulator, generating realistic figures for throughput and energy efficiency (EE) towards digital twin network modelling. We further investigate the trade-off between maximising either EE or spectrum efficiency (SE). To this end, we have run extensive simulations of a typical macrocell network deployment under various transmit power-reduction scenarios with a range of difference of 43 dBm. Our results demonstrate that the EE and SE objectives often require different power settings in different scenarios. Importantly, low mean user CPU execution times of 2.17 Â±plus-or-minus\pmÂ± 0.05 seconds (2 s.d.) demonstrate that the AIMM Simulator is a powerful tool for quick prototyping of scalable system models which can interface with ML frameworks, and thus support future research in energy-efficient next generation networks.","The significance of mobile telecommunications cannot be overstated, with global subscribers expected to reach 100 billion by 2030 [1]. Compared to earlier systems, 5G radio access networks (RAN) have more flexibility to become energy efficient using various techniques [1]. Meanwhile, energy usage remains considerable, with up to 70% of this concentrated at the base stations (BS) [2]. However, exactly how to continuously optimise next generation networks to achieve a trade-off between energy usage and performance is an open question. Artificial Intelligence (AI) and, more specifically, machine learning (ML) techniques are promising for this optimisation, but their training cannot take place on a real network. To enable ML training and testing without the risk of catastrophic network outages, virtual replicas of operational networks mirror events from a live network [3] satisfying the need for vast amounts of real-world data. This paper implements and assesses an extension to estimate energy use using a system-level wireless simulator, AIMM (AI-enabled Massive MIMO) Simulator [4]. With our extensions, the AIMM Simulator meets two key needs for using AI to optimise energy usage: rapid scenario analysis for large numbers of training rounds to run and faithful, real-world, network key performance indicator (KPI) tracking; e.g. Channel Quality Index (CQI) and Modulation and Coding Scheme (MCS). The simulator is designed to simulate tens of BSs, including multi-tier deployments with tens of User Equipment (UE) devices attached to each. For the scenarios presented in this paper, a single run generally takes less than 3 seconds on consumer hardware. Emerging Open RAN [5] architecture positions the RAN Intelligence Controller (RIC) as an interface to optimise the network with AI and ML models. By design, the AIMM Simulator offers users an interface to write code in line with the RIC functionality. In other words, integration with the RIC is an abstraction for future development. In this work, we created a function to calculate and monitor the power and energy consumption of base stations in an idealised urban 5G deployment and study scenarios of reduced transmit power. Three-dimensional positions (i.e. xğ‘¥xitalic_x, yğ‘¦yitalic_y, and zğ‘§zitalic_z spatial coordinates) of UEs in the simulation area and their CQI and resulting throughput are calculated, allowing a look at realistic estimates for energy efficiency (EE) and spectral efficiency (SE). Advancing technology crucially requires dynamic optimisation for resource allocation in rapidly fluctuating network and traffic conditions, where traditional static EE and SE points quickly become obsolete. Our work accentuates this by demonstrating the trade-off between SE and EE and the possibility of major savings by adapting models to the current situation in the network. Minimising the time required to obtain these results and the potential benefits of increasing EE or SE make dynamic optimisation targets crucial for efficient and adaptable network resource utilisation. The contribution of this paper is presenting the AIMM Simulator and demonstrating its use for measuring energy consumption in different scenarios. The simulator is extremely promising for AI and ML use cases as it can quickly produce a detailed estimate of BS energy consumption in scenarios with tens of BSs and several hundred users. This will enable the deployment of both traditional optimisation techniques and AI training for optimisation techniques that look at, for example, varying power levels and sleep modes. We show that considerable savings can be made in this way, by adjusting only three of the nineteen BS in our scenarios we can increase the network energy efficiency by up to 14.8%."
https://arxiv.org/html/2411.02098v1,"Low-Rank Tensors for Multi-Dimensional Markov Modelsâ€ â€ thanks:This work was partially supported by the NSF under award CCF-2340481, the Spanish AEI PID2022-136887NB-I00 and the Community of Madrid via the ELLIS Madrid Unit. Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-17-S-0002. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Army or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.
Emails:nav@rice.edu,s.rozada.2019@alumnos.urjc.es,antonio.garcia.marques@urjc.es,segarra@rice.edu","This work presents a low-rank tensor model for multi-dimensional Markov chains. A common approach to simplify the dynamical behavior of a Markov chain is to impose low-rankness on the transition probability matrix. Inspired by the success of these matrix techniques, we present low-rank tensors for representing transition probabilities on multi-dimensional state spaces. Through tensor decomposition, we provide a connection between our method and classical probabilistic models. Moreover, our proposed model yields a parsimonious representation with fewer parameters than matrix-based approaches. Unlike these methods, which impose low-rankness uniformly across all states, our tensor method accounts for the multi-dimensionality of the state space. We also propose an optimization-based approach to estimate a Markov model as a low-rank tensor. Our optimization problem can be solved by the alternating direction method of multipliers (ADMM), which enjoys convergence to a stationary solution. We empirically demonstrate that our tensor model estimates Markov chains more efficiently than conventional techniques, requiring both fewer samples and parameters. We perform numerical simulations for both a synthetic low-rank Markov chain and a real-world example with New York City taxi data, showcasing the advantages of multi-dimensionality for modeling state spaces.","Dynamic processes provide a mathematical framework for modeling the evolution of complex systems [1, 2]. When the system and its dynamics are known, control theory has a long history of successful applications [3, 4]. However, the dynamics are not always available and oftentimes probabilistic, and they must instead be estimated from sampled trajectories. Learning the dynamics of complex systems is challenging due to non-linear behavior, the presence of uncertainties, high dimensionality, and data scarcity [5]. Assuming specific properties of the system can render the problem tractable. In this work, we rely on two assumptions: Markovianity, where future behavior relies only on the current state of the system, and low-rankness [6], where the system dynamics can be represented by a small set of hidden variables [7]. Low-rank methods are widely used for learning the transition probabilities of Markov systems, which can be represented via probability transition matrices. When the full matrix is known, low-rank techniques enable compression for lower-dimensional models [8, 9]. Moreover, low-rankness has shown success in estimating an unknown probability matrix from samples, a classical problem in estimation theory [10, 11]. Markov chains pose a challenging case, where samples within a trajectory of observed states are typically correlated. Many works propose low-rank Markov transition matrix estimation for a simpler problem that is feasible for limited samples [12, 13, 14]. Beyond practical considerations, the concept of low-rankness arises naturally when transitions occur in a low-dimensional hidden space, such as for hidden Markov chains [15, 16, 17]. However, past literature does not fully exploit the ubiquitous multi-dimensionality of state spaces. Tensors provide natural models for transition probabilities between multi-dimensional states, where tensor dimensions correspond to those of the state space [18, 19, 20]. By considering the tensor to be low-rank, we postulate a parsimonious model that also deals with the curse of dimensionality. We thus leverage the notion of low-rankness for tensors to formulate an interpretable model of the transition probabilities, both for the entire state space and for each dimension. Previous works exploit tensor low-rankness for learning Markov models via higher-order moments [21, 22, 23], for locally-interacting Markov chains [24, 25, 26, 27], and for value functions of multi-dimensional Markov decision processes [28, 29, 30, 31, 32]. However, this is the first work to use low-rank tensors for modeling transitions in such spaces. Moreover, we consider the practical task of estimating a transition tensor from trajectories via a provably convergent optimization approach. Our contributions are as follows. C1 We propose a tensor-based probabilistic model for Markov chains on multi-dimensional state spaces, which provides a link between probabilistic models and low-rank tensor decompositions. C2 We present an optimization-based approach to estimate a low-rank Markov transition tensor from trajectories, which enjoys convergence when solved with alternating minimization. C3 We demonstrate the advantages of tensors for modeling Markovian systems and the viability of their estimation through synthetic and real-world simulations."
https://arxiv.org/html/2411.02089v1,Bidding and Dispatch Strategies with Flexibility Quantification and Pricing for Electric Vehicle Aggregator in Joint Energy-Regulation Market,"Managing and unlocking the flexibility hidden in electric vehicles (EVs) has emerged as a critical yet challenging task towards low-carbon power and energy systems. This paper focuses on the online bidding and dispatch strategies for an EV aggregator (EVA) in a joint energy-regulation market, considering EVsâ€™ flexibility contributions and compensations. A method for quantifying EV flexibility as a tradable commodity is proposed, allowing the EVA to set flexibility prices based on bid-in supply curves. An EVA bidding model in the joint market incorporate flexibility procurement is formulated. The stochastic model predictive control technique is employed to solve the bidding problem online and address the uncertainties from the electricity markets and the EVs. A power dispatch protocol that ensures a profitable and feasible allocation based on EV flexibility contribution is proposed. An affine mapping control strategy can be derived based on parametric linear programming, enables online indexing of optimal solutions given the regulation signals to avoid repeatedly solving the problem. Numerical experiments show the effectiveness of the proposed scheme, and the solution methodology can be applied in real-time.","I-A Motivation Efforts to decarbonize the power system and electrify transportation are driving the growth of renewable energy sources (RESs) and electric vehicles (EVs) [1, 2]. However, this growth creates a paradox: The integration of uncoordinated charging load, coupled with the uncertain and volatile nature of RESs, can exacerbate power fluctuations and voltage deviation, threatening the power system reliability [3]. Conversely, the elastic nature of EVs with the advancements in Vehicle-to-Grid (V2G) technology offers promising solutions to mitigate these negative impacts through coordinated EV charging [4]. A key question is how to effectively coordinate the rising number of EVs to enhance power system reliability. EV flexibility can be leveraged by shifting charging demand or discharging within their sojourn times according to power system needs. Direct control of a fleet of EVs by the independent system operator (ISO) is impractical due to the market participation thresholds and privacy concerns. This challenge has led to the emergence of the EV aggregator (EVA), which serves as an intermediary between EVs and ISO. The profit-driven EVA typically participates in energy arbitrage and is increasingly involved in ancillary markets, such as reserve [5, 6], frequency regulation [7, 8, 9] and demand response [10, 11] markets. This shift has created a win-win solution, enhancing both EVA profitability and power system reliability. The frequency regulation market stands out as particularly lucrative among these ancillary markets. Hence, a fair and efficient EVA energy-regulation joint bidding model that utilizes EV flexibility and the subsequent power dispatch model in response to regulation signals are of great importance. I-B Related Works This section explores existing literature on three key aspects relevant to EVA operations: EV flexibility quantification and pricing, the incorporation of flexibility procurement in EVA bidding within the joint energy-regulation market, and the dispatch of EVs in response to regulation signals. Quantifying EV usersâ€™ flexibility contribution and compensating them appropriately is crucial. Several studies have explored demand-side flexibility quantification, generally categorizing approaches into region-based [12, 13] and metric-based approaches [14, 15]. The region-based method generally uses the power-energy feasible region of the EV charging problem. Authors in [13] proposed an optimization problem to characterize flexibility region as the area between upper and lower power trajectories. However, this region-based method describes the flexibility potential rather than flexibility contribution, which overlooks the actual flexibility that EVAs could harness from EVs, and market income is not allocated based on the actual flexibility contribution of EVs. Metric-based methods, on the other hand, measure flexibility in terms of capacity or power envelopes. Authors in [14] proposed three indices to quantify EV usersâ€™ flexibility contribution based on upper energy boundary and actual energy level. While these indices provide a framework for understanding flexibility, they often restrict EVs from fully offering their potential and do not account for regulation contributions. For the EVA bidding problem, one challenges is how to manage uncertainties in electricity markets and EV behaviors. Typical techniques to manage uncertainties include stochastic optimization [7, 16, 11, 5], robust optimization [17, 8, 6], and risk-averse method [8, 9, 16]. However, these approaches lack adaptability, as they do not update in real-time, resulting in a static response to dynamic market conditions. In contrast, the model predictive control (MPC) technique can address problems online and provides a certain degree of robustness to uncertainties through its receding-horizon implementation as in [18]. Nevertheless, its deterministic formulation typically limits its effectiveness in systematically managing uncertainties [19]. Besides, due to the EVAâ€™s intrinsic for-profit nature, the bidding strategies mentioned above [7, 10, 16, 8, 11, 5, 6, 17, 9, 18] aim to maximize profits across multiple electricity markets, benefiting both the EVA and the power system. However, these strategies often overlook the interests of EV users, which can lead to their resistance to market participation. Moreover, the flexibility procurement and bidding problems are inherently intertwined. Thus, how to quantify EV usersâ€™ flexibility contribution and compensate them accordingly within the bidding problem remains an open question. Besides the bidding problem, the subsequent power dispatch problem to determine the (dis)charging power of each connected EV within the EVA, is non-trivial and is not well studied, especially in the context of the regulation market. One challenge is how to ensure the fairness and feasibility of power allocation for each EV given the regulation signals. Proportional allocation [20, 6] is the most common allocation protocol. Authors in [6] proposed a polytope-based EV aggregation and disaggregation method to bid in the energy and reserve market. Authors in [7] used the Lagrange multipliers of the energy update constraints in the bidding problem as the allocation protocol, ensuring that the EVAâ€™s profit maximization is still guaranteed. However, these methods either overlook EVAâ€™s profit maximization or EV usersâ€™ preferences and fairness. Another significant challenge is adhering to the regulation signals issued by the ISO every few seconds. Authors in [7, 20] formulated the dispatch problem as a Mixed-Integer Linear Programming problem (MILP), solved each time a signal was issued. However, this method is computationally expensive and impractical, as solving the MILP can take several seconds. I-C Contributions and Organizations To address the aforementioned issues, this paper proposes online energy-regulation joint bidding and power dispatch models for EVA, incorporating EV flexibility quantification and pricing. Our key contributions are as follows: 1) An EV flexibility quantification method that considers regulation provisions is proposed. By treating EV flexibility as a commodity, the EVA can set flexibility prices and procure them based on the bid-in flexibility supply curves of EVs. 2) An EVA bidding model in the joint energy-regulation market that incorporates flexibility procurement is proposed, aiming to minimize the EVAâ€™s total cost while respecting EVsâ€™ charging demand. The bidding problem is then reformulated using stochastic MPC technique, enabling real-time bidding. 3) An optimal power dispatch protocol that ensures EVA profitability while respecting EVsâ€™ flexibility preferences and committed quantities is proposed. An affine mapping control strategy for the dispatch problem can be derived using parametric linear programming (pLP) prior to the operating hour, allowing for online indexing of optimal solutions every time a regulation signal is issued. The remainder of the paper is organized as follows: Section II provides an overview of the problem and market mechanisms. Section III details the quantification and pricing of EV flexibility. Section IV describes the proposed EVA bidding and dispatch models. Section V explains the model reformulation and solution methodology. Section VI presents simulation results, and Section VII concludes the paper."
https://arxiv.org/html/2411.01944v1,Kernel-based predictive control allocationfor a class of thrust vectoring systems with singular points,"This paper considers a class of thrust vectoring systems, which are nonlinear, overactuated, and time-invariant. We assume that the system is composed of two subsystems and there exist singular points around which the linearized system is uncontrollable. Furthermore, we assume that the system is stabilizable through a two-level control allocation. In this particular setting, we cannot do much with the linearized system, and a direct nonlinear control approach must be used to analyze the system stability. Under adequate assumptions and a suitable nonlinear continuous control-allocation law, we can prove uniform asymptotic convergence of the points of equilibrium using Lyapunov input-to-state stability and the small gain theorem. This control allocation, however, requires the design of an allocated mapping and introduces two exogenous inputs. In particular, the closed-loop system is cascaded, and the output of one subsystem is the disturbance of the other, and vice versa. In general, it is difficult to find a closed-form solution for the allocated mapping; it needs to satisfy restrictive conditions, among which Lipschitz continuity to ensure that the disturbances eventually vanish. Additionally, this mapping is in general nontrivial and non-unique. In this paper, we propose a new kernel-based predictive control allocation to substitute the need for designing an analytic mapping, and assess if it can produce a meaningful mapping â€œon-the-flyâ€ by solving online an optimization problem. The simulations include three examples, which are the manipulation of an object through an unmanned aerial vehicle in two and three dimensions, and the control of a surface vessel actuated by two azimuthal thrusters.","In the last decade, we have seen an increase of the use of unmanned aerial vehicles (UAVs) in a range of military [21] and civilian applications [23], [27]. As those flying objects are getting increasingly more capable, we have thought to operate them as robots to physically interact with the environment [1], [15]. In these particular applications, the UAVs can be seen as effective orientable thrusters [18], which can produce a force in all directions. For example, we can control the UAV attitude through its four propellers and modify the thrust direction to transport [25] and manipulate objects [17]. In such applications, as a general statement, we can say that the primary control objective is to create the required forces and torques to achieve the overall desired motion. However, in numerous aerospace [4], [22] and marine [9], [34] applications, the system is usually overactuated with a redundant set of actuators. A possible solution is to follow a control-allocation strategy [11]. It is based on a hierarchical motion-control approach and introduces one or more secondary objectives. Note that adding more control objectives besides the primary objective is not a disadvantage per se, as it offers the designer extra degrees of freedom to minimize, for example, the power or fuel consumption of the system [32]. In the control-allocation framework, the primary control objective is achieved through a high-level motion control unit, which commands the overall forces and torques applied to the system. A lower-level control-allocation unit is added to coordinate the different actuators such that they produce the desired forces and torques. This can be achieved by setting up the secondary objectives. In this paper, we apply the control-allocation philosophy to a particular class of nonlinear, time-invariant, overactuated, and linearized-uncontrollable systems, which consist of two subsystems. Similarly to [30] and [31], we consider the actuator dynamics and thoroughly analyze the system stability through nonlinear control tools. In particular, we show that we can stabilize the system by breaking down the overall control architecture into two separate control units for each subsystem. Specifically, assuming that the other subsystem dynamics do not interfere, each control unit is designed such that the subsystem is stabilized. Next, to achieve the primary control objective, we connect both subsystems through an allocated mapping. This mapping is designed by properly formulating the secondary objective such that the closed-loop system guarantees that the desired forces and torques are generated. This control architecture uses a cascaded structure and generates two disturbances entering each subsystem. Under adequate assumptions, we can formally prove the system stability using Lyapunov input-to-state stability and the small gain theorem [14]. However, as the system is overactuated, this allocated mapping is in general nontrivial and not unique. Additionally, to guarantee that the disturbances eventually vanish and, thus, asymptotic stability, Lipschitz continuity of the mapping must be ensured. Depending on the system complexity, finding such a closed-form mapping can be very hard. The generalized (Moore-Penrose or pseudo) inverse [12] solution is the most commonly used method to compute the allocated mapping from the generalized force. This inverse, which is suggested in [32] for control allocation, can either be performed on the matrix BğµBitalic_B of the linearized system [5], [6], [8] or on the Jacobian of the nonlinear system [24], [26]. Since overactuated systems possess infinite mappings and the generalized inverse always exists and is unique, this method is certainly appealing. However, our paper considers a special class of systems where the generalized inverse leads to oscillatory behaviors. In fact, due to the system overactuation and the presence of scleronomic constraints, there exists a set of â€œlinearized-uncontrollableâ€ points that must be stabilized. To ensure that all equilibrium points are asymptotically stable, the allocated mapping must be Lipschitz continuous. Because the generalized-inverse method does not guarantee this condition to be true in general, applying it to the systems considered in this paper leads to oscillations. For these reasons, this paper proposes a numerical solution to alleviate the difficulty of finding such an analytic Lipschitz-continuous mapping. The main contribution of this paper is to propose the novel nonlinear kernel-based predictive control allocation (KPCA). KPCA introduces in the cost function a new term, which penalizes the deviation of the mapping from the kernel space. By doing so, we are able to locally â€œsmooth outâ€ the allocated mapping in the vicinity of the kernel space such that the system stability is preserved. We investigate through three relevant numerical examples whether KPCA is able to produce a meaningful mapping â€œon-the-flyâ€ by solving online an optimization problem. Note that a vaguely similar idea of combining nullspace techniques with quadratic programming (QP) but with linearization technique is suggested in [28]. Although the authors tackle the similar problem of infinite mappings due to the system overactuation, they still use the generalized-inverse method to derive the mapping and the purpose of the nullspace is totally different from our paper. In particular, the authors first use the force-decomposition technique [13], which transforms the nonlinear allocation problem into a linear one. Next, to eliminate the linear approximation errors, the authors use the nullspace projection technique. However, the authors clearly state that they are unable to deal with the singularities, which are the core problem of this paper. In fact, [28] avoids the singularities by enforcing that the angular changes are sufficiently small. There are two fundamental issues with this technique: i) since the mapping is derived from the generalized-inverse method, Lipschitz continuity of the allocated mapping is not guaranteed, and ii) any linearization technique would fail at the singular points. Therefore, it is necessary to directly tackle this problem through nonlinear control techniques. Another contribution of this paper is the extension of past results [19], [17], [18] to more general systems other than aerospace. As pointed out in [11], cross-disciplinary research should be encouraged. We thus attempt to give a more general formulation so that the proposed scheme can be modified and applied to other systems. In particular, we demonstrate the effectiveness of KPCA by applying it to a surface vessel actuated by two azimuthal thrusters, where the analytic continuous stabilizing control law and the allocated mapping are a-priori unknown."
https://arxiv.org/html/2411.01908v1,Frequency-based Design Method for Model-Free Controllers,"Model-Free Control (MFC) has been applied to a wide variety of systems in which it has shown its performance. MFC offers â€œmodel-free operationâ€, but the controller design requires some information from the nominal plant. This paper introduces a new design method for model-free controllers that uses minimal data about the system and retrieves a set of stable controller configurations. This method is specifically developed for first-order model-free controllers, but can be extended to second-order controllers, and it relies in a frequency analysis of the controller and the plant. The main feature of the design method is decoupling the design of the main control parameter alpha from the rest, providing specific values for it. The efficacy of the proposed method will be showcased with some relevant application examples.","I INTRODUCTION Model-Free Control (MFC) [1] has been applied to a wide variety of systems that can be complex, time-varying or non-linear, and it has shown its performance and robustness against plant changes (see [2, 3, 4, 5, 6, 7, 8] as examples of the MFC capabilities). It has been shown that MFC offers â€œmodel-free operationâ€, however, the controller design requires some information from the nominal plant. Recent works [9, 10, 11] propose different solutions for the design problem that rely on time response information or a nominal model of the plant. In this paper, a new design procedure for regulators under the MFC paradigm that uses minimal data about the system is presented. This algorithm relies on the frequency analysis of the controller and the regulated system, and is specifically developed for first-order model-free controllers, but can be extended to second-order controllers. The main feature of the proposed algorithm is decoupling the design of the parameter Î±ğ›¼\alphaitalic_Î±, which has been shown to be related to the aggressiveness [12], [13] and robustness [14], from the design of the usual PD gains in the MFC architecture, for which it provides a region that contains the stable parameter configurations. To evaluate the potential of the frequency-based MFC design method, the longitudinal control of an autonomous car is studied, applying a cascade model-free control structure and simulating the performance of the controller in a realistic vehicle simulator. The rest of the paper is structured as follows. A brief introduction to Model-Free Control is presented in Section II. Section III details the proposed design method for the parameter Î±ğ›¼\alphaitalic_Î±. Section IV explains the proposed method for designing the PD parameters. A simplified version of the design method is shown in Section V. Two illustrative examples and the results from simulation tests are presented in Sections VI and VII. Finally, concluding remarks and references can be found in the last section."
https://arxiv.org/html/2411.01872v1,Backstepping Design for Incremental Input-to-State Stabilization of Unknown Systems,"Incremental stability of dynamical systems ensures the convergence of trajectories from different initial conditions towards each other rather than a fixed trajectory or equilibrium point. Here, we introduce and characterize a novel class of incremental Lyapunov functions, an incremental stability notion known as Incremental Input-to-State practical Stability (Î´ğ›¿\deltaitalic_Î´-ISpS). Using Gaussian Process, we learn the unknown dynamics of a class of control systems. We then present a backstepping control design scheme that provides state-feedback controllers that render the partially unknown control system Î´ğ›¿\deltaitalic_Î´-ISpS. To show the effectiveness of the proposed controller, we implement it in two case studies.","A stronger stability property of non-linear systems called Incremental Stability ensures the convergence of trajectories towards each other rather than to a specific trajectory or equilibrium point. This notion has recently been extensively studied due to its applicability in the synchronization of cyclic feedback systems [1], complex networks [2] and interconnected oscillators [3], modelling of nonlinear analog circuits [4], and symbolic model construction for nonlinear control systems [5, 6, 7, 8, 9]. Incremental Input-to-State Stability (Î´ğ›¿\deltaitalic_Î´-ISS), a particular class of incremental stability, has been extensively studied and characterized by Lyapunov functions [10, 11, 12]. In addition, state feedback controllers for rendering a class of control systems Î´ğ›¿\deltaitalic_Î´-ISS have been designed. Examples include works on unstable non-smooth control systems [13], stochastic systems [14] and backstepping approach [15, 16]. While controller synthesis for Î´ğ›¿\deltaitalic_Î´-ISS stabilization has been studied extensively, to the best of the authorsâ€™ knowledge, there is no work on controller synthesis for an unknown system. We aim to address this problem by learning the unknown system dynamics by using the Gaussian Process and developing a backstepping control design scheme based on the learned system model. We consider a class of partially unknown control systems represented in the strict feedback form. Gaussian process (GP) has been used for system identification in various works in the literature due to its ability to approximate unknown nonlinear dynamics while providing a measure of the model fidelity [17]. It has been used in works on tracking control [18], feedback linearization [19], control Lyapunov approach [20], and control barrier functions [21]. Since GP-based system models are just approximations, it is not possible to ensure strict Î´ğ›¿\deltaitalic_Î´-ISS by using the learned models. But, it is possible to ensure relaxation of the property called incremental Input-to-State practical Stability (Î´ğ›¿\deltaitalic_Î´-ISpS). We define and characterize this notion for the first time in this paper based on the notion of Input-to-State practical Stability introduced in [22]. We then use a backstepping control design scheme to synthesize controllers that ensure Î´ğ›¿\deltaitalic_Î´-ISpS property. In this paper, we present the definition and characterization of Incremental Input-to-Space practical Stability. We use Gaussian Process to learn the unknown dynamics of the partially unknown system, given in strict-feedback form, using the methodology introduced in [23]. We then provide a backstepping control design scheme along with the corresponding Î´ğ›¿\deltaitalic_Î´-ISpS Lyapunov functions based on a filtered command backstepping approach that synthesizes controllers for rendering the system Î´ğ›¿\deltaitalic_Î´-ISpS. To the best of the authorsâ€™ knowledge, this is the first work that synthesizes a controller for guaranteed (probabilistic) incremental stabilization of a class of partially unknown systems given in a strict feedback form. To show the practicality of the approach, we implement the controllers synthesized based on the proposed design scheme in two case studies to show that the trajectories of the system do indeed converge."
https://arxiv.org/html/2411.01863v2,Space-Time Decoupled Information Metasurface,"Information metasurface is a type of metasurface device capable of rapidly altering its EM characteristics to generate specific information. Past research presents various strategies for embedding information into ambient EM waves through the superposition of each unitâ€™s quantified properties. Despite their capabilities, these approaches alone are insufficient for independently managing the creation of information and the allocation of spatial energy. Here, we propose a general theory of space-time decoupled metasurface (STD-Metasurface) to obtain the completely independent manipulation of information generating and spatial filtering. As proof-of-concept demonstration, we design a single-diode small-signal-modulation based prototype, capable of generating precise arbitrary waveforms while maintaining unaltered beam patterns. We exhibit how the superiority of the STD-Metasurface enhance its practicality and facilitate its application as a reconfigurable backscatter transmitter and an dynamic Doppler-spoofing reflection tag.","The essence of information is defined as the variations in objects relative to a subject. In wireless electromagnetic (EM) systems (such as wireless communications and radar), EM waves serve as the primary medium for information transmission. The information is embedded in the variations carried by the waves, whereas their spatial distribution or propagation path constitutes the information channel, also known as the communication channel. Recently, metasurface, comprised of 2D arrays of sub-wavelength reconfigurable units, has attracted great attention due to their simple structures without radio frequency (RF) components and their remarkable capabilities in manipulating EM wave and even producing information. With the advancement of EM metamaterials, a new technology has emerged that generates information based on EM metasurfaces, referred to as information metasurfaces. In 1898, J.C. Bose first discovered that a helical coil could alter the polarization direction of EM waves, marking the beginning of human understanding that artificial materials could modify the properties of EM waves [1]. In 1952, Schelkunoff and Friis identified that artificial materials exhibited artificial magnetism, a property not found in nature [2]. In 1999, Rodger Walser introduced the term ""metamaterials"" at a DARPA symposium, describing them as materials capable of entirely novel responses not seen in the natural world [3]. In 2003, Sievenpiper and colleagues first proposed the concept and prototype of reconfigurable metasurfaces, stating that ""the periodic surface impedance distribution can alter the surfaceâ€™s EM characteristics"" [4]. In 2014, Cui and his team in China introduced the concept and prototype of digital-coding metasurfaces, â€™bridging the physical and digital worldsâ€™ [5]. Using field-programmable array devices to dynamically adjust the EM characteristics of the metasurface, they significantly enhanced the reconfigurability and real-time control of EM wave manipulation. In 2017, Cui et al. proposed the concept of information metasurfaces, where the rapid, periodic switching of preset EM characteristic distributions allows these surfaces to ""not only control EM waves, but also generate information"" [6]. Since then, EM metasurfaces have entered the era of information metasurfaces. Recent research on how to generate information using metasurfaces can be categorized into three main methods: 1) spatial coding, 2) temporal coding, and 3) space-time coding. Spatial coding involves designing the EM response of metasurface units in a 2D distribution, altering the radiation characteristics of EM waves, and encoding information in the spatial domain. The receiver extracts the information through spatial sampling. This method has been applied to areas such as spatial energy distribution [5] (beamforming), spatial phase distribution [7] (holographic imaging), spatial polarization distribution [8, 9], etc. However, its limitation is that it remains fixed in the time domain. Temporal coding addresses this issue by introducing periodically varying digital control signals that encode information in the time or frequency domains, generating specific time-frequency information. This approach has been utilized for independent control of harmonic amplitude and phase [10], efficient frequency conversion [11], multi-polarization conversion [12], etc. In contract, its drawback lies in the inability to simultaneously control the spatial distribution of EM waves. To overcome these limitations, a digital metasurface-based information generation framework known as space-time coding (STC) was proposed [6, 13]. Its principle is the joint design of the time sequences of the 2D EM response distribution, enabling dynamic control over both the spatial and temporal characteristics of the EM waves. This coding scheme allows information and energy to be encoded at specific locations, enhancing the signal-to-noise ratio (SNR) at the receiver and even improving physical layer security. However, a key feature within the framework of space-time coding is the coupling between time and space. In this framework, the time domain baseband signal is generated by sequentially switching a predesigned beam sequence, where each beam is meticulously crafted based on the position of the receiver. The time domain signal depends on the spatial sampling points, meaning that the normalized Escatterâ¢(Ï„;ğ«ğŸ)subscriptğ¸scatterğœsubscriptğ«1E_{\text{scatter}}(\tau;\mathbf{r_{1}})italic_E start_POSTSUBSCRIPT scatter end_POSTSUBSCRIPT ( italic_Ï„ ; bold_r start_POSTSUBSCRIPT bold_1 end_POSTSUBSCRIPT ) and Escatterâ¢(Ï„;ğ«ğŸ)subscriptğ¸scatterğœsubscriptğ«2E_{\text{scatter}}(\tau;\mathbf{r_{2}})italic_E start_POSTSUBSCRIPT scatter end_POSTSUBSCRIPT ( italic_Ï„ ; bold_r start_POSTSUBSCRIPT bold_2 end_POSTSUBSCRIPT ) exhibit different temporal variations. Although space-time coupling can offer advantages in specific applications, it also imposes significant limitations on the broader applicability of information metasurfaces, leading to three key challenges that are difficult to address: â€¢ Precise generation of arbitrary signal: The discretization of EM characteristics and control signal make it difficult to generate continuous arbitrary waveforms and spectra; â€¢ Complex spatial environments: When the signal is encoded at specific spatial locations, changes or uncertainties in the environment hinder accurate transmission; â€¢ Beam variation pattern leakage: Signal generation relies on changes in spatial distribution, making variation patterns in the side lobes susceptible to interception and detection. The challenges in addressing the three aforementioned issues stem from the following: (1) a lack of continuous signal control over metasurface units, preventing the generation of continuous electromagnetic properties; (2) the inability to truly modulate the baseband signal onto a radio frequency (RF) carrier, instead relying on the superposition principle to control the amplitude and phase parameters of the electromagnetic field at specific spatial sampling points; (3) The failure to decouple the variations in the spatial domain from those in the time domain. Fundamentally, this coupling effect arises from insufficient degrees of freedom to control the electromagnetic characteristics of each individual unit. To eliminate this coupling effect, we aim to achieve: Esâ¢câ¢aâ¢tâ¢tâ¢eâ¢râ¢(Ï„,ğ«)=E~sâ¢câ¢aâ¢tâ¢tâ¢eâ¢râ¢(Ï„)â¢EÂ¯sâ¢câ¢aâ¢tâ¢tâ¢eâ¢râ¢(ğ«),subscriptğ¸ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğœğ«subscript~ğ¸ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘ŸğœsubscriptÂ¯ğ¸ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğ«E_{scatter}(\tau,\mathbf{r})=\tilde{E}_{scatter}(\tau)\bar{E}_{scatter}(% \mathbf{r}),italic_E start_POSTSUBSCRIPT italic_s italic_c italic_a italic_t italic_t italic_e italic_r end_POSTSUBSCRIPT ( italic_Ï„ , bold_r ) = over~ start_ARG italic_E end_ARG start_POSTSUBSCRIPT italic_s italic_c italic_a italic_t italic_t italic_e italic_r end_POSTSUBSCRIPT ( italic_Ï„ ) overÂ¯ start_ARG italic_E end_ARG start_POSTSUBSCRIPT italic_s italic_c italic_a italic_t italic_t italic_e italic_r end_POSTSUBSCRIPT ( bold_r ) , (1) Figure 1: Conceptual illustration of decoupling of scattering EM waves in terms of space and time domain base on STD-Metasurface. By independently controlling the modulation factor and the beam pattern factor of each unit, the sampling in time domain represents the information modulated by STD-Metasurface, and spatial energy distribution in space domain. Here we illustrate that the signal generator modulate the continuous baseband signal onto the air-fed source, meanwhile the varying discrete codebook sequence aligns the main-lobe of the beam to the moving receiver. . where E~sâ¢câ¢aâ¢tâ¢tâ¢eâ¢râ¢(Ï„)subscript~ğ¸ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğœ\tilde{E}_{scatter}(\tau)over~ start_ARG italic_E end_ARG start_POSTSUBSCRIPT italic_s italic_c italic_a italic_t italic_t italic_e italic_r end_POSTSUBSCRIPT ( italic_Ï„ ) and EÂ¯sâ¢câ¢aâ¢tâ¢tâ¢eâ¢râ¢(ğ«)subscriptÂ¯ğ¸ğ‘ ğ‘ğ‘ğ‘¡ğ‘¡ğ‘’ğ‘Ÿğ«\bar{E}_{scatter}(\mathbf{r})overÂ¯ start_ARG italic_E end_ARG start_POSTSUBSCRIPT italic_s italic_c italic_a italic_t italic_t italic_e italic_r end_POSTSUBSCRIPT ( bold_r ) represent the temporal and spatial components of the scattered electric field of the metasurface, respectively. The former carries the information varying over time, while the latter manages the spatial energy distribution. Fortunately, this functional separation aligns with the logic of traditional multi-antenna transmitters. In RF links, a mixer shifts the spectrum of the baseband signal produced by a digital-to-analog converter (DAC) to near the carrier frequency, and then a phase shifter adjusts the beam direction. In this work, we propose for the first time the concept of a space-time decoupled metasurface (STD-Metasurface), where the spatial properties E~scatterâ¢(Ï„)subscript~ğ¸scatterğœ\tilde{E}_{\text{scatter}}(\tau)over~ start_ARG italic_E end_ARG start_POSTSUBSCRIPT scatter end_POSTSUBSCRIPT ( italic_Ï„ ) and temporal properties EÂ¯scatterâ¢(r)subscriptÂ¯ğ¸scatterğ‘Ÿ\bar{E}_{\text{scatter}}(r)overÂ¯ start_ARG italic_E end_ARG start_POSTSUBSCRIPT scatter end_POSTSUBSCRIPT ( italic_r ) of the scattered electric field can be independently controlled. We also designed and fabricated a space-time decoupled metasurface based on small-signal modulation using a single diode, where each unit cell employs just one PIN diode to independently achieve both beamforming and continuous signal modulation. This allows for the generation of arbitrary waveforms and spectra while maintaining a fixed spatial energy distribution. At the application level, we built a prototype system based on the STD-Metasurface. By utilizing beamforming and designing arbitrary symbol waveforms, the system adapts to complex channels, enhancing the signal-to-noise ratio (SNR) after matched filtering at the receiver. This enables backscatter communication with passive beamforming functionality. Additionally, as a scatterer radar tag, the system can control the spatial energy distribution to reduce radar cross-section (RCS), and it can generate arbitrary micro-Doppler signatures with low interception probability."
https://arxiv.org/html/2411.01794v2,Revisiting Game-Theoretic Control in Socio-Technical Networks: Emerging Design Frameworks and Contemporary Applications,"Socio-technical networks represent emerging cyber-physical infrastructures that are tightly interwoven with human networks. The coupling between human and technical networks presents significant challenges in managing, controlling, and securing these complex, interdependent systems. This paper investigates game-theoretic frameworks for the design and control of socio-technical networks, with a focus on critical applications such as misinformation management, infrastructure optimization, and resilience in socio-cyber-physical systems (SCPS). Core methodologies, including Stackelberg games, mechanism design, and dynamic game theory, are examined as powerful tools for modeling interactions in hierarchical, multi-agent environments. Key challenges addressed include mitigating human-driven vulnerabilities, managing large-scale system dynamics, and countering adversarial threats. By bridging individual agent behaviors with overarching system goals, this work illustrates how the integration of game theory and control theory can lead to robust, resilient, and adaptive socio-technical networks. This paper highlights the potential of these frameworks to dynamically align decentralized agent actions with system-wide objectives of stability, security, and efficiency.","Game theory addresses strategic interactions among decision-makers, often referred to as players or agents [1]. Each player has a distinct objective functionâ€”either a utility to maximize or a cost to minimizeâ€”which encapsulates their preferences across available alternatives. However, these preferences are interdependent, shaped by the choices made by other players, creating a need for game theory as a framework to model these strategic dynamics [2]. In non-cooperative games, where players act independently, achieving an equilibrium is a central focus. The Nash equilibrium, a key concept developed by John Nash, represents a stable state where no player can unilaterally adjust their strategy for a better outcome [3]. This equilibrium concept enables analysts to identify stable states in competitive environments. When hierarchical decision-making exists, other solution concepts like the Stackelberg equilibrium are more suitable, especially in scenarios where leaders act first and followers respond. This framework is particularly applicable in control scenarios where independent agents must coordinate within shared constraints [4]. The formalization of game theory is largely attributed to John von Neumann and Oskar Morgensternâ€™s Theory of Games and Economic Behavior [1], a pioneering text that established the fieldâ€™s foundations and generated broad, interdisciplinary interest. Nashâ€™s work on equilibrium expanded the field significantly, enabling stable solutions for non-cooperative games [3]. Additional advancements by Richard Bellman, who introduced dynamic programming [5], and Rufus Isaacs, known for differential games [6], extended game theory to dynamic and multi-stage decision-making, integrating it more deeply into control and optimization contexts. A defining moment for game theory was its integration into optimal control and decision processes, especially through the efforts of researchers at the RAND Corporation, including Bellman, Nash, and Isaacs. At RAND, these theorists developed models intersecting military strategy, economics, and control systems, catalyzing breakthroughs in multi-agent decision-making processes. Isaacsâ€™ differential games, for instance, established a framework for continuous-time strategic interactionsâ€™ foundational for adversarial scenarios like pursuit-evasion games [6]. This interdisciplinary evolution of game theory alongside advances in optimal control provided the groundwork for its application in modern networked systems, where it remains central to understanding strategic interdependencies and designing resilient control mechanisms. I-A Game Theory and Socio-Technical Systems Todayâ€™s interconnected systemsâ€”ranging from telecommunications and social networks to critical infrastructureâ€”face unique challenges as they are populated by autonomous agents, each pursuing individual objectives, but interconnected through their actions and information exchanges. Game theory is a critical tool in such environments, especially where decentralized decision-making is needed. In multi-agent systems, every agentâ€™s actions affect the networkâ€™s overall state, often creating complex dynamics that are difficult to predict and control. Game-theoretic analysis equips researchers and designers with the methods to anticipate these interactions, predict system behavior, and develop strategies that enhance stability, efficiency, and resilience across the network [7]. Moreover, these networks are often socio-technical systems, where human behavior directly influences their performance, efficiency, and resilience. Human decisions and interactions shape the functioning of many networked systems, such as transportation, energy grids, and public health infrastructure [8, 9, 10]. For example, transportation networks must account for the flexible and sometimes unpredictable nature of human routing decisions. In such settings, infrastructure planning cannot be isolated from human behavior. The well-known Braess paradox illustrates that adding roads to a network may lead to increased congestion, as drivers individually optimize their routes, often at the expense of overall efficiency [11]. Game-theoretic models help planners anticipate these outcomes, enabling the design of transportation networks that mitigate unintended consequences and improve flow [12]. In smart energy systems, where energy prosumers (both consumers and producers) decide when to buy or sell energy, individual behaviors impact the gridâ€™s supply-demand balance. To maintain grid stability, game-theoretic mechanisms can incentivize prosumers to make decisions that align personal economic interests with the systemâ€™s operational needs [13]. These control mechanisms foster a resilient and efficient energy network by balancing incentives for prosumers in ways that enhance resource allocation and minimize the risk of outages [14]. Figure 1 illustrates the nature of the control of socio-technical systems. The technical system is coupled with the human networks, and the designer can influence the coupled system through different control paradigms, including information, incentives, and network structures. Public health further exemplifies the socio-technical nature of modern systems, as seen during the COVID-19 pandemic. Individual choices, such as decisions about vaccination, mask-wearing, and social interactions, had substantial effects on the spread of the virus [15, 16, 17, 18]. In such interconnected populations, the communityâ€™s health state depends on the aggregation of personal decisions. Game-theoretic design principles offer powerful tools for crafting behavioral incentives and nudges that guide individuals toward compliance with public health measures [19]. By structuring these incentives effectively, game theory helps manage collective health outcomes, particularly during health crises, and underscores the importance of considering socio-technical dynamics in system design. By integrating game-theoretic frameworks into socio-technical systems, designers gain the ability to understand and anticipate human-driven impacts on system dynamics. Game theory provides structured approaches for designing incentives, controlling information flows, and implementing adaptive mechanisms that foster desirable behaviors. These strategies are essential for ensuring that the interactions of autonomous agentsâ€”whether they are people, machines, or a mix of bothâ€”contribute positively to network performance, resilience, and societal benefit [20]. Fig. 1: A Game-Theoretic Control Paradigm for Socio-Technical Systems: Socio-technical networks are composed of interconnected human and technical networks. Human agents interact both with one another and with technical infrastructures, including power grids, transportation systems, and cyber networks. The control of these networks can be achieved through strategic designs in information flow, network structure, and incentive mechanisms. Information design guides how agents access and process data, while network design shapes the connectivity and interaction pathways within the system. Incentive design, on the other hand, motivates desired behaviors by aligning agent actions with system-wide objectives, ensuring that human and technical interactions are coordinated to achieve resilience, efficiency, and security across the socio-technical network. I-B Game-Theoretic Control Design Game theory offers not only a framework for modeling, performance evaluation, and risk assessment but also a robust design methodology for creating decentralized agents. A key strength of game-theoretic design lies in its decentralized approach, which provides a foundational structure for building and managing complex, large-scale networks [21, 22]. In these decentralized networks, individual agents act based on personal incentives, often with limited or no knowledge of the networkâ€™s overall state. This bottom-up approach mirrors real-world systems, where centralized control may be impractical or ineffective. For human agents, game-theoretic design allows for behavior modification to align with system goals. For machine agents, it enables programming diverse agents to follow a coordinated protocol. These agents, whether human or machine, can work collectively to achieve desired outcomes associated with metrics such as efficiency, robustness, resilience, and security. By embedding game-theoretic strategies, designers can anticipate and guide agent interactions, facilitating cooperative behavior even in environments with limited information sharing or direct coordination. Figure 2 presents an agent-based perspective on socio-technical systems illustrated in Figure 1. Within this framework, human agents within human networks engage with machine agents in technical networks, while also interacting with other agents in their respective networks. Each human agent operates as a coupled system, integrating individual belief processes with action processes. Similarly, each machine agent functions as a coupled system, linking control processes with physical processes. Designers can influence various agents through targeted levers across different system components, aiming to optimize system-level performance. The design of agents is closely linked to control theory, specifically the design of controllers that manage dynamical systems to achieve desirable properties like stability and optimality. While control theory traditionally focuses on governing centralized control systems, game-theoretic agent design introduces a complementary approach that is particularly suited to large-scale socio-technical networks. In these networks, the goal is often to achieve outcomes such as optimal social welfare or collective efficiency, which align with the objectives of control theory. Game-theoretic design operates from the bottom up, creating decentralized agents that make decisions based on local information and personal incentives. This bottom-up approach enables scalability, making it ideal for vast, complex networks typical of socio-technical systems, where centralized control may be impractical. By designing agents to act independently yet cohesively, game-theoretic design facilitates adaptable, resilient, and efficient network behaviors, even in highly dynamic and large-scale environments. Modeling of the Agents For the socio-technical system illustrated in Figure 1, game theory can model diverse interactions within socio-technical networks from the ground up. These interactions can be categorized into several key types. First, interactions occur between agents within the same network, such as those between human agents in human networks or machine agents within technical networks. These intra-network interactions capture the dynamics among similar types of agents and can reveal emergent patterns within isolated sub-systems. Second, interactions take place between agents across different networks. For instance, human agents in the socio-network interact with machine agents in the technical network, bridging the socio-technical divide. These cross-network interactions are crucial for understanding how human and machine agents jointly influence system outcomes. A third category involves interactions with adversarial agents. Adversarial agents are specifically introduced to evaluate the security, robustness, and resilience of the network. These adversarial entities may be real participants within the network or artificial agents created to assess risk. By engaging human or technical agents with adversaries designed with specific intentions and capabilities, we can measure local security and resilience properties more accurately. Finally, interactions occur between agents and a designer. Here, a designer exerts influence over agents in a controlled way to guide their behavior toward achieving network-wide objectives. This interaction serves as a means of designing and controlling agent actions within the network to align with broader system goals. Each of these interactions takes on distinct forms, and the various games representing them are ultimately composed into a larger framework, referred to as a â€œmeta-game.â€ This meta-game governs the design and control of the entire socio-technical network, enabling a holistic approach to understanding and managing complex interactions within the system. Control of the Agents Agents can be controlled in various ways, depending on their nature and function, and these controls can be categorized into three primary paradigms. The first is physical control, which involves managing physical attributes like speed, direction, and other measurable quantities, as seen in robotic agents [23]. The second is cyber control, where the focus is on controlling the information received by agents, such as news broadcasts for human agents or sensor data for autonomous vehicles. The third paradigm is human control, where the objective is to influence perceptions and incentives to guide human behavior in desired directions. Across these paradigms, network structure and information design are fundamental. How agents communicate, physically interact, and gather information from observations and perceptions are critical components of effective system design [24]. A key connection between control theory and game-theoretic design emerges through the use of dynamic game frameworks to model and guide agent behavior in evolving environments [25, 26]. In dynamic games [27], agents interact over time within changing environments and face uncertainties. Agent behaviors are characterized by adaptive feedback loops, where decisions continuously adjust based on environmental conditions. Information flow becomes particularly crucial in these scenarios, as agents make real-time decisions with limited or noisy information about othersâ€™ actions. The flow and structure of information directly shape agentsâ€™ strategic choices, influencing the overall systemâ€™s resilience and robustness. Bridging control and game-theoretic design achieves a unified approach to achieving individual dynamic agents and ensuring the stability and efficiency of the entire system. On the individual level, agents must operate effectively within their local environments, maintaining stability in response to changing conditions and achieving their own performance goals. At the system level, however, the design must prioritize overall stability, resilience, and system-wide metrics [28, 29]. Fig. 2: Illustration of Interaction Between a Social Agent in the Human Network and a Machine Agent in the Technical Network: A social agent interacts with the human network and a machine agent within the technical network. Each agent is also connected to other agents within its own network. The machine agent provides specific services to the social agent, while the social agent impacts the machine agent and its network through behaviors such as consumption, usage, or demand patterns. The designer can strategically influence both networks using tools like information design and incentive structures. Information design shapes the structure of information between agents, while incentive design aligns agent actions with broader system goals, creating a coordinated and adaptive socio-technical system. I-C The Underlying Philosophy of Agent-Based Game-Theoretic Design in Socio-Technical Networks The agent-based game-theoretic design of socio-technical networks embodies a dual philosophy [30, 31]: reductionistic design and holistic control. On one side, game-theoretic design takes a reductionist approach, where the whole system is decomposed into modular components or agents. By breaking down complex, large-scale networks into manageable agents, this approach allows designers to handle intricate interdependencies and diverse functions within the system. On the other side, the design aims to achieve high-level system objectivesâ€”such as efficiency, security, and resilienceâ€”which are often prescribed at a system-wide level. The reductionist design of individual agents must, therefore, be aligned with these holistic goals, ensuring coherence between component-level actions and overall system performance. Establishing such coherence is fundamental to the principles guiding game-theoretic design in socio-technical networks. Achieving coherence between agent-level design and system-level objectives requires a framework to bridge them. Designers need to assess how individual agent behaviors impact system-wide metrics, making it essential to monitor the alignment of component actions with system goals. Game-theoretic analysis provides this bridge by offering a structured framework to predict system-level behaviors through equilibrium concepts. The equilibrium, depending on the application and structure of the network, enables designers to forecast the outcomes of individual actions within the larger system. Various solution concepts within game theory offer tools to assess and develop performance metrics. For instance, in a security context, equilibrium analysis between a defender agent and an attacker can yield risk metrics, while in robustness analysis, saddle-point equilibria between the system and external disturbances inform robustness metrics. Game theory serves as a bridge that enables reductionist designers to account for the holistic impact of individual agent designs on the systemâ€™s objectives. Meanwhile, holistic system designers must shape the architecture, including hierarchies, network structures, and resource allocations, to ensure that agent-level designs contribute to the systemâ€™s high-level goals. Frameworks such as Stackelberg games, equilibrium-constrained optimization, and mechanism design theory play a central role in achieving this alignment. Holistic designers must understand how agents respond to these structures at equilibrium and ensure that top-down control strategies foster the intended system-wide behavior. Ensuring coherence becomes more challenging under conditions of uncertainty, adaptive requirements, and emergent properties such as resilience and security. These complex requirements demand clear, quantifiable metrics to guide system and agent design. Despite these challenges, game theoryâ€”with its rich array of tools and methodologiesâ€”provides a means to develop advanced techniques that foster coherence in dynamic, complex systems. For instance, game-theoretic tools can incorporate learning and adaptation, enabling agent designs to evolve in response to an uncertain environment while staying aligned with system goals. This coherence between reductionism and holistic control, illustrated in Figure 3, is where game theory and control theory intersect, together forming the foundation for a new system design paradigm. Designing socio-technical networks requires this paradigm shift and the convergence between control and game theory to address the unique demands of these complex, interconnected systems. For example, in a smart grid, it is insufficient to simply control each subsystem, such as energy generation or distribution, in isolation. The system must account for the interplay between independent agents (e.g., consumers, generators, and grid operators) who each respond to incentives, environmental conditions, and their own objectives. By integrating game-theoretic strategies, designers can predict how these agents will behave collectively, while control theory enables the coordination of these actions to maintain grid stability, efficiency, and resilience. Fig. 3: The holistic control design must align consistently with the reductionist behaviors of individual agents. Game theory, inherently a reductionist approach, focuses on designing and analyzing individual agent behaviors, while control theory provides a holistic framework to achieve overarching system goals. Game-theoretic control offers a cohesive approach that bridges these two perspectives, integrating the detailed evaluation and synthesis tools of reductionist models with the coordination and control mechanisms of holistic design. This combined framework ensures that individual agent actions are aligned with the broader system objectives, creating a unified and adaptive socio-technical system. I-D Organization of the Letter This letter provides an overview of game-theoretic design approaches. In Section II, we explore foundational frameworks used in agent design, focusing on Stackelberg-type game frameworks and mechanism design theory, which has been widely applied to settings like auctions and market structures. Section III examines the challenges of designing socio-technical systems, addressing issues such as human behavioral dynamics, uncertainty quantification, and scalability. In Section IV, we present emerging paradigms in game-theoretic design, including mean-field design, learning-based design, population-based design, and adversarial design. These approaches are applied to critical areas such as misinformation management in social networks, resilience in industrial control systems, and congestion control in infrastructure networks. We close the letter with the concluding remarks of Section V."
https://arxiv.org/html/2411.01506v2,Degradation-Infused Energy Portfolio Allocation Framework: Risk-Averse Fair Storage Participation,"This work proposes a novel degradation-infused energy portfolio allocation (DI-EPA) framework for enabling the participation of battery energy storage systems in multi-service electricity markets. The proposed framework attempts to address the challenge of including the rainflow algorithm for cycle counting by directly developing a closed-form of marginal degradation as a function of dispatch decisions. Further, this closed-form degradation profile is embedded into an energy portfolio allocation (EPA) problem designed for making the optimal dispatch decisions for all the batteries together, in a shared economy manner. We term the entity taking these decisions as â€˜facilitatorâ€™ which works as a link between storage units and market operators. The proposed EPA formulation is quipped with a conditional-value-at-risk (CVaR)-based mechanism to bring risk-averseness against uncertainty in market prices. The proposed DI-EPA problem introduces fairness by dividing the profits into various units using the idea of marginal contribution. Simulation results regarding the accuracy of the closed-form of degradation, effectiveness of CVaR in handling uncertainty within the EPA problem, and fairness in the context of degradation awareness are discussed. Numerical results indicate that the DI-EPA framework improves the net profit of the storage units by considering the effect of degradation in optimal market participation.","The electricity markets over the world are making regulatory changes in order to encourage participation of energy storage systems (ESS) [1]. The ESS offer services essential for managing the intermittency associated with renewable energy and for lowering peak demand [2]. The ESS can perform energy arbitrage in electricity markets by charging at lower prices and discharging at higher prices. Thus, its revenue depends on the price gap which varies depending on the energy supply and demand in the network. Consequently, the revenue is subject to market risks due to price volatility. An energy storage aggregator is a coalition of multiple energy storage systems (ESS) that collectively participates in electricity markets on their behalf. Forming such a coalition offers several advantages, including optimized resource utilization, enhanced market power, and effective risk hedging [3]. Additionally, it enables smaller units to access electricity markets that they might otherwise be excluded from due to minimum battery size requirements. The market participation strategy of an ESS is essentially an optimization problem subject to the batteryâ€™s operational and physical limits as constraints [4, 5]. A large number of these dispatch studies do not consider degradation in the optimization problem [6, 7, 8]. According to some standards, once the battery reaches a 20% loss in its capacity, it is deemed unfit [9]. Therefore, storage systems with higher degradation rate or with a higher replacement cost may adjust the bidding strategies to account for the higher degradation costs [10, 11]. The degradation model helps optimize dispatch to balance revenue generation with long life. Also, a degradation-aware dispatch can help achieve a circular economy with batteries [12]. There are multiple approaches to estimate the loss in capacity due to cycling. The stress factor-based capacity degradation models are widely used to estimate the incremental degradation due to battery cycling [13]. The stress factor models are semi-empirical models that express the loss in battery capacity as a function of battery states such as the depth-of-discharge (DoD), temperature, state-of-charge (SoC), etc., in each equivalent charge-discharge cycle [14]. The equivalent cycles are obtained by the rainflow cycle counting algorithm which utilizes the SoC profile traversed by the battery over a time horizon to extract the equivalent half or full cycles of charge and discharge [15]. Particularly in a multi-market environment where the ESS goes through partial charge-discharge cycles, a cycle counting algorithm is needed for efficient estimation of capacity loss. However, the rainflow algorithm does not possess an analytical functional form as it is a set of rule-based logical statements, and hence, it is challenging to incorporate it into an optimization framework. Specially designed methods have been explored in [16, 17, 18, 19] to incorporate cycle counting based storage degradation into battery dispatch frameworks. Authors in [16] have utilized rainflow algorithm-based battery degradation cost in the training process of its reinforcement learning-based ESS scheduling model. The authors in [20] have proposed a graph-based linear mapping of SoC profile to charge-discharge half-cycle depths which exploits the rainflow algorithm. In [17], a stress factor-based degradation model is proposed which counts degradation only during battery discharging and thus, removing the need for a cycle counting algorithm in the optimization framework. However, it may lead to over- or under-estimation of the incremental degradation. The authors in [18] have developed an online control policy for optimal battery energy dispatch problem considering the rainflow algorithm-based degradation cost. The control policy is developed assuming known penalty parameters for a certain market horizon. Further, [18] develops a sub-gradient method based iterative procedure for solving the optimization problem where the optimality is guaranteed under some mild conditions [18, Sec. III-D and Theorem 3]. However, none of the above works focus on developing a closed-form expression for estimating battery capacity degradation which can be directly incorporated into the energy storage optimization problem and helps solving it in a single-level using the off-the-shelf solvers, without demanding parameter tuning or additional iterations. Another contribution of this work is the creation of a bidding strategy for the participation of a storage coalition in electricity markets. Various studies explore the integration of sharing economy frameworks into battery participation in markets [21, 22, 23, 24]. A comprehensive review of different approaches to solving shared storage problems can be obtained from [25]. These works highlight the importance of coalition based approaches for market participation. However, the challenge of participation with volatile market prices of future instances remains a challenge not addressed in these works. There have been attempts to employ the portfolio theory to solve the participation under uncertainty [26, 27, 28]. Mean-variance optimization has also been used for storage scheduling [29]. Importantly, due to uncertainties in price information, the distribution of coalition profit maximization [23] does not follow a specific probability distribution like the normal distribution and is not symmetric. Therefore, the Markowitz model or mean-variance optimization [29, 30] fails to capture the skewness in the EPA objective distribution [31]. Further, the variance is not monotonic, and thus, lacks the coherency property as a risk measure [31]. These issues highlight the need to develop a shared economic participation framework for energy storage systems that can handle price uncertainties and incorporate degradation in dispatch decisions. In this paper, we propose a degradation-infused energy portfolio allocation (DI-EPA) framework that provides optimal procurement of services from electricity markets, including energy, reserve, and regulation. Additionally, considering the complexities of existing degradation estimation methods, in this work, we express the incremental degradation as a function of dispatch and utilize the Gaussian process (GP) [32] to learn its closed-form [33], thus, eliminating the need for a cycle-counting algorithm. This closed-form expression of battery degradation is incorporated into the proposed DI-EPA framework that also considers the conditional value-at-risk (CVaR) approach to ensure a minimum profit guarantee under uncertain market clearing prices. This CVaR-based constraint mitigates the risk of ending a given scheduling period with financial loss. As such, this work approaches the DI-EPA problem from the optimization modellingâ€™s point of view. Moreover, we do not attempt to propose a new market model but provide a fair energy storage participation framework within existing market structures. The main contributions of this work are summarized below. â€¢ Proposing a novel risk-averse and degradation-infused energy storage coalition framework for participating in a multi-service electricity market based on a shared-economy model. The proposed framework improves the economic utilization of batteries over their lifetime by strategically dispatching them according to their degradation profiles. â€¢ Developing a Gaussian Process-based degradation and dispatch (D&D) profiling to model the cost of degradation (COD) of each battery unit as a function of its dispatch. This profiling infuses the degradation awareness into decision-making for shared-economy operations. â€¢ Formulating and solving an EPA problem with consideration of CVaR for market price and uncertainties associated with degradation profiling, aimed at introducing risk aversion into the bidding strategy. Additionally, we devise a fair mechanism for profit distribution by drawing upon concepts from marginal contributions within the Shapley value framework to equitably distribute overall profits. Remark. The battery/storage participation via an aggregator has been studied in [34, 23]. The majority of these works, both cooperative and comparative participation, model the aggregator as an independent entity that has its own goalsâ€“ profit making for itself in general. Studies have also been carried out on how not to let the aggregator tilt the balance in its favor by altering prices [34]. However, in the proposed framework, the entity designed for market participation does not pursue its own profit making goals and only provides some â€˜facilitiesâ€™ to the battery owners. These facilities or services include the participation of batteries in electricity markets and fair distribution of profit at the end of participation. In return, the battery operators can work on a fixed fee or profit share model to compensate for the service. Therefore, to distinguish the workings from a profit-seeking aggregator, we term the entity providing these facilities as â€˜Facilitatorâ€™. The services provided by the facilitator can also include the remaining life estimation and second life prediction, etc. In the following, we use small bold letters to indicate vectors of appropriate dimension, e.g., ğ=[d1,d2,â€¦]âŠ¤ğsuperscriptsubscriptğ‘‘1subscriptğ‘‘2â€¦top\mathbf{d}=[d_{1},d_{2},\ldots]^{\top}bold_d = [ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ ] start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT as dispatch vector; ğŸ1\mathbf{1}bold_1 is the vector of ones of appropriate dimension; and N!ğ‘N!italic_N ! represents the factorial of the whole number Nğ‘Nitalic_N."
https://arxiv.org/html/2411.01451v1,Deep Reinforcement Learning for Optimizing Inverter Control: Fixed and Adaptive Gain Tuning Strategies for Power System Stability,"This paper presents novel methods for tuning inverter controller gains using deep reinforcement learning (DRL). A Simulink-developed inverter model is converted into a dynamic link library (DLL) and integrated with a Python-based RL environment, leveraging the multi-core deployment and accelerated computing to significantly reduce RL training time. A neural network-based mechanism is developed to transform the cascaded PI controller into an actor network, allowing optimized gain tuning by an RL agent to mitigate scenarios such as subsynchronous oscillations (SSO) and initial transients. Two distinct tuning approaches are demonstrated: a fixed gain strategy, where controller gains are represented as RL policy (actor network) weights, and an adaptive gain strategy, where gains are dynamically generated as RL policy (actor network) outputs. A comparative analysis of these methods is provided, showcasing their effectiveness in stabilizing the transient performance of grid-forming and grid-following converters and deployment challenges in hardware. Experimental results are presented, demonstrating the enhanced robustness and practical applicability of the RL-tuned controller gains in real-world systems.","Although artificial intelligence has revolutionized the tech industry, its adoption in the power systems domain has been relatively slow. In particular, AI technique, such as deep reinforcement learning (DRL), has seen limited exploration for power system control, especially in addressing stability issues in power inverters. Ref. [1] presents a comprehensive review of published research in this area from 2005 to 2023. The data shows a sharp rise in RL-based power system stability papers starting in 2019, peaking at 80 publications in 2022. Growing interest highlights the potential of RL to tackle complex challenges in power system stability, though further research is necessary to broaden its applications in this field. Ref. [2, 3, 4, 5, 6] explored using RL agents to replace traditional PI controllers in various inverter-based applications. However, this approach presents several key challenges: (1) The DRL model generates control signals directly, which is difficult to implement in hardware due to the high computational demand of the neural network based policy, making it impractical for embedded systems. (2) Additionally, the policy may respond unpredictably to scenarios, not encountered during training, potentially leading to system instability under untested conditions. In contrast, ref. [7, 8, 9] used RL agents to tune PI controllers by adjusting adaptive gains. This method enhances the approach of replacing the PI controller with a DRL agent by retaining the PI controller and enabling adaptive tuning. However, it still faces hardware deployment limitations, as the RL policy requires a fully connected neural network, which is computationally intensive and challenging to implement in real-time systems. One of the prominent challenges of EMT simulation is the requirement for very small time steps to accurately solve the network dynamics. For instance, Simulink, a widely used software for EMT simulation, requires approximately 20,000 computational steps per second, which demands significant computational resources to ensure accurate results. On the other hand, the computation methods for RL value functions are divided into Monte Carlo (MC) and Temporal Difference (TD) methods [10]. MC method estimate value function by averaging the returns from complete episode, providing higher accuracy due to their reliance on the full trajectories of data. However, it is not always feasible to obtain complete sample data for every episode, especially in the field of power electronic converters, where the training process may be interrupted by unsafe operating conditions. Also, it is desirable to update the value function and control policy in real-time during the training process. From the existing literature, it can be observed that currently in the field of power electronics, there is a greater emphasis on the application of RL algorithms based on TD methods [10]. Therefore, the RL algorithm needs to run 20,000 computation per second together with the inverter based power system environment. If the model runs for 5s for each episode, it requires 100,000 computation for each episode. Our experiments indicate that the RL agent requires a significant number of training episodes, ranging from 50 to 2,000, to achieve reasonable convergence. Figure 1: GFL inverter RL agent training pipeline where(a) represents the PPO Agent, (b) GFL inverter environment setup with the grid (GFM, load and transformer), (c) Trained gains / RL policy is transported into hardware setup TABLE I: State-of-art DRL training environment for IBR controller tuning Environment Name Process Advantages Disadvantages Ref Native MATLAB Environment EMT modeling in Simulink and RL agent in MATLAB Low build time Higher training time [11] Python Environment Model inverter and RL agent in Python High flexibility Difficult building EMT model of IBR in Python [12] MATLAB + Python Hybrid Environment IBR model in Simulink and RL Agent in Python. Use communication to transfer data Rich Python RL library, easy EMT modeling in Simulink High communication time and high training time [8] Native Python Environment (DLL-based) Model the IBR in Simulink, convert into multiple DLLs, Build the RL agent and environment in Python Lower training time. Able to parallel compute the EMT model and GPU acceleration of RL agent training. High flexibility Initial challenge of interfacing IBR DLL into Python environment. Developed a Python library to overcome this challenge [13] Various EMT simulation platforms, such as MATLAB, offer integration capabilities for developing reinforcement learning (RL) agents, allowing easy integration with simulation environments [14]. While these platforms can significantly reduce development time, they may result in longer training durations compared to other programming environments. For instance, previous studies developed electric motor EMT models and RL agents in Python-based environments, demonstrating computational efficiency, though these approaches often require more time to develop the EMT models [12, 15]. Another approach in the literature involves developing the RL agent in a programming environment like Python while using an EMT simulation platform such as MATLAB for modeling the system [8]. However, this method introduces higher communication latency, leading to longer training times. A comparison of these approaches is presented in Table I. In this research, we addressed these key challenges of IBR controller tuning using DRL method. While DRL-based automatic controller tuning holds promise for optimizing gains in non-linear systems like IBRs, two major obstacles persist: (1) the high training time for such models in EMT simulation software (MATLAB environment) and (2) the impracticality of replacing traditional PI controllers with RL agents or deploying RL agents for adaptive gain tuning due to computational limitations in microcontrollers. To overcome these challenges, we propose a hybrid approach that leverages the strengths of both Simulink and Python. The EMT model is developed in Simulink and converted into a DLL, enabling integration with a Python-based RL environment using the PyTorch library. This approach allows us to utilize the best of both worlds; Simulinkâ€™s advanced modeling capabilities and Pythonâ€™s computational power and rich RL libraries offering a more efficient and practical solution for controller tuning in IBR-based power system. Finally, contributions of this research are as follows: â€¢ A novel neural network-based mechanism is developed to convert the inverter cascaded PI controller into an actor network, enabling gain optimization by an RL agent under scenarios such as transients, subsynchronous oscillation (SSO) and faults. The tuned gains can be directly deployed into inverters, ensuring robust performance and seamless integration with existing systems. â€¢ Two approaches are developed for tuning inverter control gains: a fixed gain method, where controller gains are embedded as weights of actor network, and an adaptive gain method, where gains are generated dynamically as actor network outputs. A comparison is provided, highlighting the effectiveness of both strategies in stabilizing the transient performance of grid-forming and grid-following converters. â€¢ A pipeline is proposed for IBR controller tuning, where the inverter model developed in an EMT simulation platform (e.g., Simulink) is converted into a DLL and integrated into a programming environment (e.g., Python) for reinforcement learning (RL). This approach utilizes multi-core deployment and accelerated computing to optimize the model, significantly reducing training time. To the best of our knowledge, this is the first application of a DLL-based IBR controller tuning mechanism employing deep reinforcement learning (DRL), setting a new precedent in the field. â€¢ Presented experimental results showcasing the improved performance of RL-tuned controller gains, highlighting their practical application and impact. Figure 2: Simulink to DLL generation for parallel computing of the RL environment The organization of this paper is as follows: section II outlines the mechanism for building the inverter environment and developing the RL agent, section III presents the performance of the DRL-based controller tuning for the GFL inverter and compares the two methods explored in this research, section IV covers the applicability, advantages, and disadvantages of both methods, while section V offers the final remarks."
https://arxiv.org/html/2411.01412v1,Near-Optimal Emission-Aware Online Ride Assignment Algorithm for Peak Demand Hours,"Ridesharing has experienced significant global growth over the past decade and is becoming integral to future transportation networks. These services offer alternative mobility options in many urban areas, promoting car-light or car-free lifestyles, with their market share rapidly expanding due to the convenience they offer. However, alongside these benefits, concerns have arisen about the environmental impact of ridesharing, particularly its contribution to carbon emissions. A major source of these emissions is â€œdeadhead milesâ€ that are driven without passengers between trips. This issue is especially pronounced during high-demand periods when the number of ride requests exceeds platform capacity, leading to longer deadhead miles and higher emissions. While reducing these unproductive miles can lower emissions, it may also result in longer wait times for passengers as they wait for a nearby driver, potentially diminishing the overall user experience. As ridesharing platforms continue to grow, solutions that reduce carbon emissions must strike a balance between environmental objectives and maintaining service quality, particularly the wait times of the rider.In this paper, we propose LARA, an online algorithm for rider-to-driver assignment that dynamically adjusts the maximum allowed deadhead miles for drivers and assigns ride requests accordingly. While LARA can be applied under any conditions, it is particularly more effective during high-demand hours, aiming to reduce both carbon emissions and rider wait times. We prove that LARA achieves near-optimal performance in online settings compared to the optimal offline algorithm. Furthermore, we evaluate LARA using both synthetic and real-world datasets, demonstrating up to 34.2%percent34.234.2\%34.2 % reduction in emissions and up to 42.9%percent42.942.9\%42.9 % reduction in rider wait times compared to state-of-the-art algorithms. While recent studies have introduced the problem of emission-aware ride assignment, LARA is the first algorithm to provide both theoretical and empirical guarantees on performance.","In 2022, the transportation sector accounted for 28.4% of total greenhouse gas emissions. This has been a consistent trend, as transportation has remained the leading source of emissions in the U.S. since 2017. The dominance of transportation in emissions is largely due to the widespread use of vehicles powered by gasoline and diesel, which are major sources of carbon dioxide and other greenhouse gases. As the population grows, economies develop, and urbanization continues, the demand for urban transport is expected to increase. This increase is likely to counteract any CO2 reductions from low- and zero-carbon technologies. By 2050, projections indicate that motorized urban mobility will nearly double, leading to a 26% rise in CO2 emissions from urban transport [14]. Given these environmental challenges, there is a growing need to investigate and develop transportation systems that focus on sustainable practices. With growing populations and advancing technology, ridesharing services have transformed travel by offering on-demand mobility through individual or shared vehicles, leading to global adoption [35]. According to Statista, the global ridesharing market is expected to experience substantial growth, with revenue anticipated to reach $167.60currency-dollar167.60\$167.60$ 167.60 billion by 2024 and $212.80currency-dollar212.80\$212.80$ 212.80 billion by 2029 [31]. The number of users is also projected to increase, reaching 2.31 billion by 2029, with user penetration rising from 23.1% in 2024 to 28.6% by 2029. Studies initially highlighted the substantial benefits of ridesharing services for both the environment and the economy, including reduced pollution such as CO2 emissions and noise, decreased traffic congestion, and lower energy consumption [4]. Despite these promising advantages, concerns about climate change, traffic congestion, and oil dependency have emerged as ridesharing services have gained popularity [7]. For example, pickups and drop-offs from ridesharing services have increasingly disrupted traffic flow, particularly in urban areas that are already congested [11]. Additionally, other studies indicate that ridesharing services can raise energy consumption through deadhead miles driven without passengers which can negate any potential energy savings and exacerbate traffic congestion. In fact, a rideshared car ride generates 47% more CO2 emissions on average compared to an equivalent private car ride [18, 30]. During high-demand hours, ridesharing platforms face distinct challenges. The imbalance between available drivers and ride requests often leads to driver selectiveness, with drivers rejecting trips to less popular destinations or passengers with specific needs. This selectiveness can drive up trip cancellation rates, as platforms may prioritize less desirable trips in the interest of fairness. However, this puts pressure on drivers, who risk deactivation if they cancel too many rides, potentially exacerbating driver shortages as they leave the platform [22]. Even in centralized ridesharing systems where algorithms handle rider-driver assignment, online assignment systems often struggle to optimize objectives like minimizing wait times and reducing carbon emissionsâ€”particularly when driver availability is limited. In such cases, a simple assignment algorithm that assigns riders to the first available driver offers little flexibility in optimizing these factors. Consequently, much of the riderâ€™s wait time is spent in the assignment queue, awaiting a match. Under these conditions, first-come, first-served strategies can result in extended deadhead miles and longer wait times, degrading the user experience for both riders and drivers. While some studies have explored methods to reduce deadhead miles and their associated emissions [18], and others have aimed to improve service quality by minimizing wait times [29, 30], many of these approaches lack performance guarantees across varying conditions. This issue becomes especially critical during high-demand periods when driver availability is low. In such cases, online algorithms without performance guarantees can fall significantly short compared to the optimal offline algorithms, which benefit from knowing future inputs. This underscores the need for new strategies that ensure environmentally sustainable ridesharing systems with reliable emissions reductions, even during peak demand, while also maintaining high service quality with respect to wait times. Developing such systems and strategies presents significant challenges. Providing theoretical guarantees in ridesharing optimization is inherently complex, as it requires managing dynamic decision-making processes involving both current and future drivers and riders. A major challenge lies in the uncertainty of future ride requestsâ€™ timing and locations. Assigning a driver to a rider not only affects the current ride but also determines when and where that driver will be available again, influencing the deadhead distance and emissions of subsequent rides. This creates a feedback loop, potentially causing long-term changes in the actions taken by the online algorithm and its overall performance. Furthermore, designing an online algorithm that balances the platformâ€™s goalsâ€”such as emission reductionâ€”with rider preferences, like minimizing wait times, adds another layer of complexity. Real-world factors, such as fluctuating urban traffic conditions, further complicate efforts to ensure consistent, optimal performance in practice. In this paper, we propose a Lyapunov-based Algorithm for Ride Assignment (LARA), designed to reduce carbon emissions in ridesharing platforms while minimizing rider wait times. While LARA is applicable at all times, its benefits over previous state-of-the-art algorithms are especially pronounced during high-demand hours, when a naive assignment approach could lead to long queues of unassigned requests and long waiting times accordingly. To address this, LARA dynamically adjusts the upper bound on deadhead distances for assigned drivers based on real-time conditions. The algorithmâ€™s decisions are influenced by the number of ride requests in the assignment queue (i.e., unassigned requests waiting to be dispatched). We present a theoretical performance analysis of LARA and empirically evaluate it against state-of-the-art algorithms using both synthetic and real-world datasets. Our contributions are summarized as follows: â€¢ We conduct a case study on the impact of limiting deadhead distances on both the carbon emissions of trips and rider waiting times. Based on this study, we propose a new approach to control the trade-off between emission and waiting times, framing it as a deadhead control problem (DCP). We model the objective of DCP as a weighted sum of two factors: the expected carbon reduction and the rate of ride assignment within the platform. â€¢ We introduce LARA, a near-optimal online algorithm for solving DCP that does not require future input predictions. LARA adjusts the upper bound on deadhead distances based on the number of ride requests in the assignment queue. It uses a tunable hyperparameter that allows for performance optimization under different conditions. â€¢ We provide a theoretical performance analysis of LARA, showing that its objective value for DCP is within a bounded distance from the optimal solution (Theorem 1). We also show that this bound approaches zero when LARA is not constrained by the assignment queue length. â€¢ We conduct a comprehensive experimental analysis to evaluate LARA using both real-world and synthetic datasets, comparing its performance against the existing emission-aware ride assignment algorithm TORA [30] as well as additional heuristic methods. Our results indicate significant reductions in both emissions and waiting times during high-demand periods. For instance, compared to classic approaches, LARA achieves up to 34.2%percent34.234.2\%34.2 % reduction in average emissions on the synthetic dataset (Figure LABEL:fig:synthetic_batchDur) and up to 13.9%percent13.913.9\%13.9 % on the real-world dataset (Figure 4), consistently outperforming competing algorithms across various scenarios. Furthermore, our analysis of LARA reveals a trade-off between reducing emissions and ensuring fair ride assignments among different drivers."
https://arxiv.org/html/2411.01361v1,Control Node Placement and Structural Controllability of Water Quality Dynamics in Drinking Networks,"Chlorine, the most widely used disinfectant, needs to be adequately distributed in water distribution networks (WDNs) to maintain consistent residual levels and ensure water safety. This is performed through control node injections at the treatment plant via booster stations scattered in WDNs. While previous studies have applied various optimization metrics for booster station placement, many have failed to consider the coverage of the station injections and the dynamic nature of WDNs. In particular, variations in hydraulics and demand significantly impact the reachability and efficacy of chlorine injections which then impact optimal placement of booster stations. This study introduces a novel formulation that combines control- and graph-theoretic approaches to solve the booster station placement problem. Unlike traditional methods, our approach emphasizes maximizing the systemâ€™s ability to control disinfectant levels with minimal energy, taking into account the time-varying hydraulic profiles that lead to different optimal station placements. We propose a simple weighting technique to determine the placements by assessing the structural controllability of each configuration, based on the networkâ€™s topology and independent of specific parameters like decay rates or pipe roughness. This method ensures effective chlorine coverage across the network. Our approach is validated on different networks, demonstrating its operational effectiveness, scalability, and practicality.","Disinfection is a key process in the practice of maintaining water quality against bacterial growth and contamination spread in urban water systems. Operators of water distribution networks (WDNs) maintain sufficient disinfectant residualsâ€”specifically chlorine in our studyâ€”across the network. This is achieved through (i) controlled chlorine injections at the treatment plant and (ii) the strategic placement of booster stations at a number of nodes within the network. The placement of booster stations is constrained by the accessibility of network nodes and the associated costs, limiting the number of stations that can be installed. The traditional approach to solving the chlorine booster station placement (CBSP) problem is to determine the geographic locations considering various objectives: minimizing the chlorine injection masses, maintaining residual levels, minimizing costs, reducing byproduct formation, and/or ensuring timely response to uncertain contamination events. However, WDNs are complex systems where real-time operations and varying consumer demands lead to changing flow rates and directions, directly influencing the reachability and capability of chlorine injections. Consequently, the CBSP problem yields different optimal locations for each hydraulic scenario. The conventional method for selecting final locations for fixed stations typically involves choosing the nodes that appear most frequently across various scenarios. To that end, the majority of studies in the CBSP literature overlook critical theoretical and practical considerations. They often fail to thoroughly assess the coverage and effectiveness of booster station injections, where coverage refers to the ability of booster stations to distribute chlorine effectively across the entire network. Additionally, these studies do not adequately examine how this coverage changes under varying hydraulic scenarios within the WDN. Furthermore, they lack a systematic approach for determining optimal station placements that ensures sufficient coverage across multiple scenarios while adapting to the networkâ€™s dynamic nature. In this paper, we introduce a novel control engineering-based CBSP formulation, aimed at maximizing system controllability. In this context, controllability is defined as the ability to effectively steer, regulate, and maintain disinfectant levels within the network to consistently meet the established water health standards. The placement strategy is designed to achieve this objective with the application of minimal and efficiently distributed control input energy (i.e., chlorine injections). In addition, we determine the final station placements by assessing the resulting system structural controllabilityâ€”a concept that links system controllability to network topology, independent of specific underlying parameters (e.g., decay and reaction rates and pipe roughness coefficients). This is achieved through a graphical assessment of the network and its connectivity, thereby accounting for the unique layout of each network to maximize the coverage of chlorine injections over the network components. Moreover, we extend this approach to provide WDN operators with insights into backup locations for chlorine injection in cases of booster station malfunctions or insufficient capacity. The approach is applied and tested on various network sizes and scenarios, with considerations for scalability also discussed. To the best of our knowledge, this is the first attempt to tackle the CBSP from a coupled control- and graph-theoretic perspective while also considering the operational aspects and practical considerations of WDNs. In the following sections, we review the existing literature and highlight the gaps that this study aims to fill. I-A Literature Review The topic of CBSP in the water engineering field has a rich body of literature, which we cover in this section. To introduce our novel approach to solving this engineering problem, we survey the literature on the following: (i) previous state-of-the-art methodologies to solve this problem and their limitations, (ii) the concept of controllability and its application to place actuators (i.e., the controllers, which are the booster stations in our study), and (iii) how this concept can be integrated with graph theory to perform a structural controllability analysis that considers the systemâ€™s actual structure and identifies the most effective configuration of stations. This review provides the reader with both general and specific insights into how CBSP has been employed in the water engineering field, alongside the principles of general and structural controllability in control theory, and how these concepts can exploited and applied to this specific application for WDNs. Throughout this literature survey, we highlight the main gaps and drawbacks that this paper aims to address, leading to the presentation of its main contributions listed in Section I-B. Chlorine Booster Stations Placement. The study [1] has introduced the concept of placing booster stations along the network instead on relying on the station at the very start of the network. Since then, several studies have explored various methodologies to optimally allocate these booster stations, each contributing unique approaches but also presenting limitations. These methodologies differ in the formulation of the optimization problem and the techniques used to determine the locations of booster stations. The authors in [2] determine these locations by formulating and solving a mixed-integer linear program that minimizes the chlorine mass consumed by the population. The same approach is adopted by the authors in [3] for optimal locations and scheduling of booster chlorination in a real water supply network located in Al-Khobar, Saudi Arabia. On the other hand, Propato et al. [4] have proposed a mixed-integer quadratic programming model to locate booster stations and determine their dosage schedules, optimizing the systemâ€™s residual spatiotemporal distribution. The study [5] has investigated the problem of optimal placement and operation of valves and chlorine boosters by minimizing the average zone pressure while maintaining target chlorine concentrations. This problem is solved via a convex heuristic to generate candidate locations and evaluate configurations. Meng et al. [6] optimize booster station locations from a hydraulic perspective using the particle backtracking algorithm, allowing placement at critical upstream nodes to meet disinfectant needs. Subramaniam et al. [7] have introduced the chlorine covering set theory, which guides selecting optimal booster locationsâ€”ensuring chlorine needs are met for all nodes. Genetic algorithm (GA) have also been utilized to solve the CBSP problem by: (i) minimizing the difference between chlorine concentration and the residual chlorine upper bound [8], (ii) taking into consideration the dispersion process in networkâ€™s dead-ends [9], (iii) coupling the GA with a multi-species WQ model to optimize booster locations and dosage to minimize exposure to Escherichia coli (E. coli) [10], (iv) linking the GA with EPANET-MSX to minimize the overall costs of booster stations placement, construction, and operation while delivering the water with acceptable residual chlorine and TTHM concentrations (i.e., disinfectant by-products) [11], or (v) integrating GA with particle swarm optimization techniques to optimize booster stations locations and scheduling to ensure regulatory compliance and minimize environmental impacts [12]. Additionally, Behzadian et al. [13] have presented a two-phase multi-objective optimization approach for booster disinfection. The first phase determines booster locations by maximizing volumetric discharge with appropriate disinfectant levels and minimizing the total disinfectant mass. The second phase refines these locations to minimize discharge avoiding THM limits and maximize discharge with standard disinfectant levels. The primary limitation in the aforementioned studies is the lack of coverage guarantees for booster station injections. Additionally, the final placement locations are typically determined by considering either a single dominant hydraulic scenario or multiple scenarios. In the latter case, the final placement is often based on selecting the most picked locations across these different hydraulic scenarios.Our paper addresses the first limitation by formulating a placement problem that ensures submodularityâ€”a control-theoretic concept discussed in subsequent sectionsâ€”and effective distribution of the control energy. In other words, this CBSP problem is designed to choose locations that would result on maximizing the controllability with the application of minimal control input energy (i.e., chlorine injections). The second issue is resolved by evaluating numerous possible hydraulic scenarios and determining the placement under these conditions. We employ a structural controllability-based weighting technique to ensure and assess not only the control energy required and the most apparent locations but also the maximum coverage of the injections. Following, we summarize the literature on these topics and explain how they are applicable to the control and operation of drinking water networks. Controllability-Driven Actuators Placement. In dynamic systems and control engineering, efforts have been put forth to tackle the actuator or control node placement problems by leveraging the concept of controllability [14]. Systemâ€™s controllability can be quantified using different metrics that reflect the size of the controllable subspace and the control energy stored [15]. That is, the actuator placement problem can be formulated by incorporating these metrics to obtain the optimal placements that result in maximizing the controllability over the system. Some of those metrics, satisfy an important property that is referred to as submodularity [16]. Submodularity reflects the principle of diminishing returns. In terms of controllability, submodularity suggests that adding an actuator to a smaller subset of actuators typically results in a greater marginal increase in system controllability than adding the same actuator to a larger subset. The submodular nature of certain controllability metrics and formulating the actuator placement problem as set function optimization, allow for the use of greedy algorithms. These algorithms iteratively select the actuator that provides the most significant improvement in controllability, ensuring that the resulting configuration is within at least 63% of the optimal placement [17, 16]. In the context of WDNs, exploiting this concept is particularly beneficial for overcoming the complexity associated with formulating and solving optimization problems with large dimensionality after discretizing the WQ dynamics, making finding the absolute optimal solution computationally infeasible for many scenarios and conditions. The other layer of complexity that our paper tackles is caused by the changing hydraulic settings in WDNs as flow rates and directions change due to varying consumer demands and other factors. These changes directly influence the system controllability and consequently the placement of booster stations [18]. In our paper, we propose a weighting technique that is based on both graph and control theory to determine the final booster station configuration. In this technique, we assess the systemâ€™s structural controllability, which determines whether the systemâ€™s graphical structure and connectivity allow for overall control of WQ dynamics through control input manipulation (i.e., chlorine injections), regardless of specific parameter values (e.g., pipe roughness coefficients, and decay and reaction coefficients) [19, 20]. As a result, this technique favors booster station locations that maximize chlorine controllability and coverage for this specific network topology, components and characteristics across different hydraulic scenarios. This graph-based approach recognizes that each WDN has its own distinct topology (e.g., branched, tree, looped, or composite networks), unique characteristics, and varying operational scenarios. I-B Paper Contributions The objective of this paper is to provide a control- and network-theoretic approach that determines the optimal geographic placements of chlorine booster stations. The detailed methodological (Mâ€“KC) and practical (Pâ€“KC) key contributions presented by our work are as follows. Mâ€“KC Mâ€“KCâ–¶â–¶\displaystyle{\color[rgb]{0.87,0.7675,0.7675}\definecolor[named]{% pgfstrokecolor}{rgb}{0.87,0.7675,0.7675}\blacktriangleright}â–¶ This paper addresses the booster station placement problem based on a control- and graphical-theoretic approach. The CBSP problem is formulated to maximize the WQ controllability and minimize the energy required by the chlorine controlled inputs. This is achieved by incorporating controllability metrics and formulating the problem as a set function optimizationâ€”the problem variables are the sets of booster station locations. These metrics are selected for their important property, submodularity, which allows us to solve the set function optimization problem using a forward greedy algorithm that provides near-optimal placement with a guarantee of achieving a minimum percentage of the optimal solution. M&Pâ€“KC M&Pâ€“KCâ–¶â–¶\displaystyle{\color[rgb]{0.966,0.8861,0.83}\definecolor[named]{pgfstrokecolor% }{rgb}{0.966,0.8861,0.83}\blacktriangleright}â–¶ We propose a dynamic and scenario-based booster station placement approach. We introduce a practical weighting technique that is based on system structural controllability and evaluates multiple common and realistic hydraulic scenarios and the resulting near-optimal placements. Pâ€“KC Pâ€“KCâ–¶â–¶\displaystyle{\color[rgb]{1,1,0.75}\definecolor[named]{pgfstrokecolor}{rgb}{% 1,1,0.75}\pgfsys@color@cmyk@stroke{0}{0}{0.25}{0}\pgfsys@color@cmyk@fill{0}{0}% {0.25}{0}\blacktriangleright}â–¶ The developed approach is tested on various network sizes, characteristics, topologies, and operational conditions. In addition, a scalability-driven framework is developed to expand this approach for large-scale networks and is tested on the C-town network. Furthermore, to bridge the gap between the findings of the theoretical algorithms and operational aspects, we provide actionable insights for operators, including utilizing our approach to determine backup locations for chlorine injections in scenarios where fixed stations fail or have insufficient capacity. Tab. I: WQ models for different WDNs components Component WQ Model Eq. Reservoir ciRâ¢(t+Î”â¢tWQ)=1Ã—ciRâ¢(t)+1Ã—ciBRâ¢(t+Î”â¢tWQ)superscriptsubscriptğ‘ğ‘–Rğ‘¡Î”subscriptğ‘¡WQ1superscriptsubscriptğ‘ğ‘–Rğ‘¡1superscriptsubscriptğ‘ğ‘–subscriptBRğ‘¡Î”subscriptğ‘¡WQ\displaystyle{\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{% rgb}{0.48,0.07,0.07}c_{i}^{\mathrm{R}}(t+\Delta t_{\mathrm{WQ}})}={\color[rgb]% {0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}1}\times{\color[rgb]{% 0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{0.48,0.07,0.07}c_{i}^{% \mathrm{R}}(t)}+{\color[rgb]{0.0,0.47,0.44}\definecolor[named]{pgfstrokecolor}% {rgb}{0.0,0.47,0.44}1}\times{\color[rgb]{0.8,0.33,0.0}\definecolor[named]{% pgfstrokecolor}{rgb}{0.8,0.33,0.0}c_{i}^{\mathrm{B}_{\mathrm{R}}}(t+\Delta t_{% \mathrm{WQ}})}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_R end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) = 1 Ã— italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_R end_POSTSUPERSCRIPT ( italic_t ) + 1 Ã— italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_B start_POSTSUBSCRIPT roman_R end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) (1) Tank ViTKâ¢(t+Î”â¢tWQ)â¢ciTKâ¢(t+Î”â¢tWQ)=ViTKâ¢(t)â¢ciTKâ¢(t)+âˆ‘jâˆˆLinqinjâ¢(t)â¢Î”â¢tWQâ¢cinjâ¢(t)+ViBTKâ¢(t+Î”â¢tWQ)â¢ciBTKâ¢(t+Î”â¢tWQ)âˆ’âˆ‘kâˆˆLoutqoutkâ¢(t)â¢Î”â¢tWQâ¢ciTKâ¢(t)+ViTKâ¢(t)â¢Î”â¢tWQâ¢RTKâ¢(ciTKâ¢(t))missing-subexpressionsuperscriptsubscriptğ‘‰ğ‘–TKğ‘¡Î”subscriptğ‘¡WQsuperscriptsubscriptğ‘ğ‘–TKğ‘¡Î”subscriptğ‘¡WQsuperscriptsubscriptğ‘‰ğ‘–TKğ‘¡superscriptsubscriptğ‘ğ‘–TKğ‘¡subscriptğ‘—subscriptğ¿insubscriptsuperscriptğ‘ğ‘—inğ‘¡Î”subscriptğ‘¡WQsubscriptsuperscriptğ‘ğ‘—inğ‘¡missing-subexpressionsubscriptsuperscriptğ‘‰subscriptBTKğ‘–ğ‘¡Î”subscriptğ‘¡WQsubscriptsuperscriptğ‘subscriptBTKğ‘–ğ‘¡Î”subscriptğ‘¡WQsubscriptğ‘˜subscriptğ¿outsubscriptsuperscriptğ‘ğ‘˜outğ‘¡Î”subscriptğ‘¡WQsuperscriptsubscriptğ‘ğ‘–TKğ‘¡superscriptsubscriptğ‘‰ğ‘–TKğ‘¡Î”subscriptğ‘¡WQsuperscriptğ‘…TKsuperscriptsubscriptğ‘ğ‘–TKğ‘¡\displaystyle\begin{aligned} &{\color[rgb]{0,0,1}\definecolor[named]{% pgfstrokecolor}{rgb}{0,0,1}V_{i}^{\mathrm{TK}}(t+\Delta t_{\mathrm{WQ}})}{% \color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.48,0.07,0.07}c_{i}^{\mathrm{TK}}(t+\Delta t_{\mathrm{WQ}})}={\color[rgb]{% 0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}V_{i}^{\mathrm{TK}}(t)}{% \color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.48,0.07,0.07}c_{i}^{\mathrm{TK}}(t)}+\sum_{j\in L_{\mathrm{in}}}{\color[rgb]% {0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}q^{j}_{\mathrm{in}}(t)% \Delta t_{\mathrm{WQ}}}{\color[rgb]{0.48,0.07,0.07}\definecolor[named]{% pgfstrokecolor}{rgb}{0.48,0.07,0.07}c^{j}_{\mathrm{in}}(t)}\\ &+{\color[rgb]{0.0,0.47,0.44}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.0,0.47,0.44}V^{\mathrm{B_{\mathrm{TK}}}}_{i}(t+\Delta t_{\mathrm{WQ}})}{% \color[rgb]{0.8,0.33,0.0}\definecolor[named]{pgfstrokecolor}{rgb}{0.8,0.33,0.0% }c^{\mathrm{B_{\mathrm{TK}}}}_{i}(t+\Delta t_{\mathrm{WQ}})}-\sum_{k\in L_{% \mathrm{out}}}{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0,1}q^{k}_{\mathrm{out}}(t)\Delta t_{\mathrm{WQ}}}{\color[rgb]{% 0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{0.48,0.07,0.07}c_{i}^{% \mathrm{TK}}(t)}+{\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{% 0,0,1}V_{i}^{\mathrm{TK}}(t)\Delta t_{\mathrm{WQ}}}R^{\mathrm{TK}}({\color[rgb% ]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{0.48,0.07,0.07}c_{i}% ^{\mathrm{TK}}(t)})\end{aligned}start_ROW start_CELL end_CELL start_CELL italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) = italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_t ) italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_t ) + âˆ‘ start_POSTSUBSCRIPT italic_j âˆˆ italic_L start_POSTSUBSCRIPT roman_in end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_q start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_in end_POSTSUBSCRIPT ( italic_t ) roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_in end_POSTSUBSCRIPT ( italic_t ) end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL + italic_V start_POSTSUPERSCRIPT roman_B start_POSTSUBSCRIPT roman_TK end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) italic_c start_POSTSUPERSCRIPT roman_B start_POSTSUBSCRIPT roman_TK end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) - âˆ‘ start_POSTSUBSCRIPT italic_k âˆˆ italic_L start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_q start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT ( italic_t ) roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_t ) + italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_t ) roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT ( italic_t ) ) end_CELL end_ROW (2) Junction ciJâ¢(t)=âˆ‘jâˆˆLinqinjâ¢(t)â¢cinjâ¢(t)+qiBJâ¢(t)â¢ciBJâ¢(t)qiDJâ¢(t)+âˆ‘kâˆˆLoutqoutkâ¢(t)superscriptsubscriptğ‘ğ‘–Jğ‘¡subscriptğ‘—subscriptğ¿insuperscriptsubscriptğ‘inğ‘—ğ‘¡superscriptsubscriptğ‘inğ‘—ğ‘¡subscriptsuperscriptğ‘subscriptBJğ‘–ğ‘¡subscriptsuperscriptğ‘subscriptBJğ‘–ğ‘¡subscriptsuperscriptğ‘subscriptDJğ‘–ğ‘¡subscriptğ‘˜subscriptğ¿outsuperscriptsubscriptğ‘outğ‘˜ğ‘¡\displaystyle{\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{% rgb}{0.48,0.07,0.07}c_{i}^{\mathrm{J}}(t)}=\frac{\sum_{j\in L_{\mathrm{in}}}{% \color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}q_{\mathrm{in% }}^{j}(t)}{\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}% {0.48,0.07,0.07}c_{\mathrm{in}}^{j}(t)}+{\color[rgb]{0.0,0.47,0.44}% \definecolor[named]{pgfstrokecolor}{rgb}{0.0,0.47,0.44}q^{\mathrm{B_{\mathrm{J% }}}}_{i}(t)}{\color[rgb]{0.8,0.33,0.0}\definecolor[named]{pgfstrokecolor}{rgb}% {0.8,0.33,0.0}c^{\mathrm{B_{\mathrm{J}}}}_{i}(t)}}{{\color[rgb]{0,0,1}% \definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}q^{\mathrm{D}_{\mathrm{J}}}_{i}% (t)}+\sum_{k\in L_{\mathrm{out}}}{\color[rgb]{0,0,1}\definecolor[named]{% pgfstrokecolor}{rgb}{0,0,1}q_{\mathrm{out}}^{k}(t)}}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_J end_POSTSUPERSCRIPT ( italic_t ) = divide start_ARG âˆ‘ start_POSTSUBSCRIPT italic_j âˆˆ italic_L start_POSTSUBSCRIPT roman_in end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT roman_in end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) italic_c start_POSTSUBSCRIPT roman_in end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ( italic_t ) + italic_q start_POSTSUPERSCRIPT roman_B start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) italic_c start_POSTSUPERSCRIPT roman_B start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) end_ARG start_ARG italic_q start_POSTSUPERSCRIPT roman_D start_POSTSUBSCRIPT roman_J end_POSTSUBSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) + âˆ‘ start_POSTSUBSCRIPT italic_k âˆˆ italic_L start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT roman_out end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_t ) end_ARG (3) Pump ciMâ¢(t+Î”â¢tWQ)=1Ã—ckâ‹…â¢(t+Î”â¢tWQ)superscriptsubscriptğ‘ğ‘–Mğ‘¡Î”subscriptğ‘¡WQ1superscriptsubscriptğ‘ğ‘˜bold-â‹…ğ‘¡Î”subscriptğ‘¡WQ\displaystyle{\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{% rgb}{0.48,0.07,0.07}c_{i}^{\mathrm{M}}(t+\Delta t_{\mathrm{WQ}})}={\color[rgb]% {0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}1}\times{\color[rgb]{% 0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{0.48,0.07,0.07}c_{k}^{% \bm{\cdot}}(t+\Delta t_{\mathrm{WQ}})}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_M end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) = 1 Ã— italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_â‹… end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) (4) Valve ciVâ¢(t+Î”â¢tWQ)=1Ã—ckâ‹…â¢(t+Î”â¢tWQ)superscriptsubscriptğ‘ğ‘–Vğ‘¡Î”subscriptğ‘¡WQ1superscriptsubscriptğ‘ğ‘˜bold-â‹…ğ‘¡Î”subscriptğ‘¡WQ\displaystyle{\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{% rgb}{0.48,0.07,0.07}c_{i}^{\mathrm{V}}(t+\Delta t_{\mathrm{WQ}})}={\color[rgb]% {0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}1}\times{\color[rgb]{% 0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{0.48,0.07,0.07}c_{k}^{% \bm{\cdot}}(t+\Delta t_{\mathrm{WQ}})}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_V end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) = 1 Ã— italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT bold_â‹… end_POSTSUPERSCRIPT ( italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) (5) Pipe [ciPâ¢(1,t+Î”â¢tWQ)ciPâ¢(2,t+Î”â¢tWQ)â‹®ciPâ¢(lâˆ’1,t+Î”â¢tWQ)ciPâ¢(l,t+Î”â¢tWQ)]=(1âˆ’Î»iâ¢(t))â¢[ciPâ¢(1,t)ciPâ¢(2,t)â‹®ciPâ¢(lâˆ’1,t)ciPâ¢(l,t)]+Î»iâ¢(t)â¢[cjJâ¢(t)ciPâ¢(1,t)â‹®ciPâ¢(lâˆ’2,t)ciPâ¢(lâˆ’1,t)]+Î”â¢tWQâ¢[RPâ¢(ciPâ¢(1,t))RPâ¢(ciPâ¢(2,t))â‹®RPâ¢(ciPâ¢(lâˆ’1,t))RPâ¢(ciPâ¢(l,t))]matrixsubscriptsuperscriptğ‘Pğ‘–1ğ‘¡Î”subscriptğ‘¡WQsubscriptsuperscriptğ‘Pğ‘–2ğ‘¡Î”subscriptğ‘¡WQâ‹®subscriptsuperscriptğ‘Pğ‘–ğ‘™1ğ‘¡Î”subscriptğ‘¡WQsubscriptsuperscriptğ‘Pğ‘–ğ‘™ğ‘¡Î”subscriptğ‘¡WQ1subscriptğœ†ğ‘–ğ‘¡matrixsubscriptsuperscriptğ‘Pğ‘–1ğ‘¡subscriptsuperscriptğ‘Pğ‘–2ğ‘¡â‹®subscriptsuperscriptğ‘Pğ‘–ğ‘™1ğ‘¡subscriptsuperscriptğ‘Pğ‘–ğ‘™ğ‘¡subscriptğœ†ğ‘–ğ‘¡matrixsubscriptsuperscriptğ‘Jğ‘—ğ‘¡subscriptsuperscriptğ‘Pğ‘–1ğ‘¡â‹®subscriptsuperscriptğ‘Pğ‘–ğ‘™2ğ‘¡subscriptsuperscriptğ‘Pğ‘–ğ‘™1ğ‘¡Î”subscriptğ‘¡WQmatrixsuperscriptğ‘…Psubscriptsuperscriptğ‘Pğ‘–1ğ‘¡superscriptğ‘…Psubscriptsuperscriptğ‘Pğ‘–2ğ‘¡â‹®superscriptğ‘…Psubscriptsuperscriptğ‘Pğ‘–ğ‘™1ğ‘¡superscriptğ‘…Psubscriptsuperscriptğ‘Pğ‘–ğ‘™ğ‘¡\displaystyle\begin{aligned} {\color[rgb]{0.48,0.07,0.07}\definecolor[named]{% pgfstrokecolor}{rgb}{0.48,0.07,0.07}\begin{bmatrix}c^{\mathrm{P}}_{i}(1,t+% \Delta t_{\mathrm{WQ}})\\ c^{\mathrm{P}}_{i}(2,t+\Delta t_{\mathrm{WQ}})\\ \vdots\\ c^{\mathrm{P}}_{i}(l-1,t+\Delta t_{\mathrm{WQ}})\\ c^{\mathrm{P}}_{i}(l,t+\Delta t_{\mathrm{WQ}})\end{bmatrix}}={\color[rgb]{% 0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}(1-{\lambda}_{i}(t))}{% \color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.48,0.07,0.07}\begin{bmatrix}c^{\mathrm{P}}_{i}(1,t)\\ c^{\mathrm{P}}_{i}(2,t)\\ \vdots\\ c^{\mathrm{P}}_{i}(l-1,t)\\ c^{\mathrm{P}}_{i}(l,t)\end{bmatrix}}+{\color[rgb]{0,0,1}\definecolor[named]{% pgfstrokecolor}{rgb}{0,0,1}{\lambda}_{i}(t)}{\color[rgb]{0.48,0.07,0.07}% \definecolor[named]{pgfstrokecolor}{rgb}{0.48,0.07,0.07}\begin{bmatrix}c^{% \mathrm{J}}_{j}(t)\\ c^{\mathrm{P}}_{i}(1,t)\\ \vdots\\ c^{\mathrm{P}}_{i}(l-2,t)\\ c^{\mathrm{P}}_{i}(l-1,t)\end{bmatrix}}+{\color[rgb]{0,0,1}\definecolor[named]% {pgfstrokecolor}{rgb}{0,0,1}\Delta t_{\mathrm{WQ}}}\begin{bmatrix}R^{\mathrm{P% }}({\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}{rgb}{% 0.48,0.07,0.07}c^{\mathrm{P}}_{i}(1,t)})\\ R^{\mathrm{P}}({\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}% {rgb}{0.48,0.07,0.07}c^{\mathrm{P}}_{i}(2,t)})\\ \vdots\\ R^{\mathrm{P}}({\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}% {rgb}{0.48,0.07,0.07}c^{\mathrm{P}}_{i}(l-1,t)})\\ R^{\mathrm{P}}({\color[rgb]{0.48,0.07,0.07}\definecolor[named]{pgfstrokecolor}% {rgb}{0.48,0.07,0.07}c^{\mathrm{P}}_{i}(l,t)})\end{bmatrix}\end{aligned}start_ROW start_CELL [ start_ARG start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 , italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 2 , italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL â‹® end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l - 1 , italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l , italic_t + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT ) end_CELL end_ROW end_ARG ] = ( 1 - italic_Î» start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) ) [ start_ARG start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 , italic_t ) end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 2 , italic_t ) end_CELL end_ROW start_ROW start_CELL â‹® end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l - 1 , italic_t ) end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l , italic_t ) end_CELL end_ROW end_ARG ] + italic_Î» start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) [ start_ARG start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_J end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 , italic_t ) end_CELL end_ROW start_ROW start_CELL â‹® end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l - 2 , italic_t ) end_CELL end_ROW start_ROW start_CELL italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l - 1 , italic_t ) end_CELL end_ROW end_ARG ] + roman_Î” italic_t start_POSTSUBSCRIPT roman_WQ end_POSTSUBSCRIPT [ start_ARG start_ROW start_CELL italic_R start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT ( italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 1 , italic_t ) ) end_CELL end_ROW start_ROW start_CELL italic_R start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT ( italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( 2 , italic_t ) ) end_CELL end_ROW start_ROW start_CELL â‹® end_CELL end_ROW start_ROW start_CELL italic_R start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT ( italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l - 1 , italic_t ) ) end_CELL end_ROW start_ROW start_CELL italic_R start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT ( italic_c start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_l , italic_t ) ) end_CELL end_ROW end_ARG ] end_CELL end_ROW (6) Paper Organization. The remainder of this paper is organized as follows: Section II presents the WQ dynamics model and its state-space representation. Based on this representation, the notion of WQ controllability and its Gramian and metrics are introduced in Section III. Following this, Section IV formulates the booster station placement problem, which is then validated through several case studies encompassing various scales, layouts, and scenarios in Section V. Finally, Section VI provides conclusions, discusses the studyâ€™s limitations, and recommends directions for future research. Notation. Italicized, boldface upper and lower case characters represent matrices and column vectors: ağ‘\displaystyle aitalic_a is a scalar, ğšğš\displaystyle\bm{\mathrm{a}}bold_a is a vector, and ğ‘¨ğ‘¨\displaystyle\bm{A}bold_italic_A is a matrix. The notation â„nsuperscriptâ„ğ‘›\displaystyle\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT denotes the sets of column vectors with nğ‘›\displaystyle nitalic_n real numbers, while â„nÃ—msuperscriptâ„ğ‘›ğ‘š\displaystyle\mathbb{R}^{n\times m}blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_m end_POSTSUPERSCRIPT denotes the sets of matrices with nğ‘›\displaystyle nitalic_n rows and mğ‘š\displaystyle mitalic_m columns. The variables with upper case characters â‹…J,â‹…R,â‹…TK,â‹…P,â‹…M,superscriptbold-â‹…Jsuperscriptbold-â‹…Rsuperscriptbold-â‹…TKsuperscriptbold-â‹…Psuperscriptbold-â‹…M\displaystyle\bm{\cdot}^{\mathrm{J}},\bm{\cdot}^{\mathrm{R}},\bm{\cdot}^{% \mathrm{TK}},\bm{\cdot}^{\mathrm{P}},\bm{\cdot}^{\mathrm{M}},bold_â‹… start_POSTSUPERSCRIPT roman_J end_POSTSUPERSCRIPT , bold_â‹… start_POSTSUPERSCRIPT roman_R end_POSTSUPERSCRIPT , bold_â‹… start_POSTSUPERSCRIPT roman_TK end_POSTSUPERSCRIPT , bold_â‹… start_POSTSUPERSCRIPT roman_P end_POSTSUPERSCRIPT , bold_â‹… start_POSTSUPERSCRIPT roman_M end_POSTSUPERSCRIPT , and â‹…Vsuperscriptbold-â‹…V\displaystyle\bm{\cdot}^{\mathrm{V}}bold_â‹… start_POSTSUPERSCRIPT roman_V end_POSTSUPERSCRIPT represent the variables related to junctions, reservoirs, tanks, pipes, pumps, and valves."
https://arxiv.org/html/2411.01336v1,Distributed Tracing for Cascading Changes of Objects in the Kubernetes Control Plane,"Kubernetes is a container orchestration system that employs a declarative configuration management approach. In Kubernetes, each desired and actual state is represented by an â€œobjectâ€, and multiple controllers autonomously monitor related objects and update their objects towards the desired state in the control plane. Because of this design, changes to one object propagate to other objects in a chain. The cluster operators need to know the time required for these cascading changes to complete, as it directly affects the quality of service of applications running on the cluster. However, there is no practical way to observe this kind of cascading change, including breakdown of the time taken by each change. Distributed tracing techniques are commonly used in the microservices architecture to monitor application performance, but they are not directly applicable to the control plane of Kubernetes; the microservices architecture relies on explicitly calling APIs on other services, but in Kubernetes the controllers just monitor objects to know when to start processing, and never call functions on other controllers directly. In this paper, we propose a system that automatically traces changes to objects in the control plane. Our method adds one identifier, a Change Propagation ID (CPID), to the metadata of an object, and the controller that observes an object change propagates its CPID to the objects that the controller is updated. When multiple changes need to be merged on an object, a new CPID is generated, and the relationship between the original CPID and the new CPID is sent to the external trace server. We confirmed that change propagation can be visualized and the required time measured. We also showed that this systemâ€™s overhead is not significant.","A declarative configuration management system is a system that defines and inputs a desired state and automatically updates configurations to to maintain the actual state matches with the desired state. This architecture is widely used in controllers for large scale systems, and Kubernetes (The Kubernetes Authors, 2014)(Verma et al., 2015) is a de facto container (Bernstein, 2014) management system that employs this approach. In Kubernetes, operators configure resources and their objects by defining only the desired state. An â€œobjectâ€ in Kubernetes represents the desired and current state of each cluster function, and a resource is a set of objects of the same function. In the control plane of Kubernetes, many components called controllers controllers observe the current state of the objects that each controller is in charge of, and constantly perform a process (reconciliation loop) to get them into the desired state. This mechanism allows the controller to detect when the current state changes due to failures, updates by the operators, or other reasons, and the controller tries to maintain the state of the system in the desired state autonomously. Since some of these controllers control their resource objects by observing the state of objects in other resources, changes occur in a chain among these resource controllers and objects until status of all related objects are updated. One of the key metrics in a Kubernetes cluster is the time required to update all related objects when the desired state of one object is changed. This time directly affects the quality of service of applications running on the cluster. For example, when requests to an application is suddenly increased, the operators will start more container replicas by updating the number of replicas to handle the requests, but the application cannot handle all the requests until all related objects are updated and additional replicas are launched. In order to identify where bottlenecks are, we need a mechanism to make cascading changes among related resources visible to operators. However, there are two challenges for observing these cascading changes. First, because the updates of each object is processed autonomously in each controller in the control plane, it is not clear to which controller or object a change is propagated next. The operator does not specify the sequence of updating each object, but rather, once an object is changed, the various objects are updated autonomously to reflect the change of the object. Therefore, it is impossible to know (or define) when the change process for all related objects has been completed. In addition, the objects to be affected by a change of a specific object are different depending on the system, such as the use of custom resources and plugins installed by the operators. Second, it is difficult to know when a change in one object will cause a change in other objects. Since each controller is not invoked directly from other controllers but autonomously monitors the current object status and processes it to the desired status, the timing to start the update process of the objects depends on each controller; that is, the updates may be processed one by one just after other objects are updated, or several updates may be processed in batch. K-Bench (VMware, 2020) and ClusterLoader (Kubernetes, 2018) are tools to measure the time to handle some predefined types of changes, but they do not have a mechanism for tracing cascading changes between objects in general. Conventional methods that require manual logging by the operators (Ehira et al., 2023) can trace all resources, but they are costly because they require a deep knowledge of the controller design. The Kubernetes community has discussed extending the control planeâ€™s functionality to allow to measure cascading changes (Naser, 2020)(Li, 2021). However, these are still in the discussion because of a problem in handling tracing information to be placed on the object. Distributed tracing techniques (Sigelman et al., 2010)(Fonseca et al., 2007)(Chen et al., 2002)(Gschwind et al., 2002) are commonly used in the microservices architecture to monitor application performance, but they are not directly applicable to the control plane of Kubernetes; the microservices architecture is based on RPCs (Thurlow, 2009) and clearly identify the start and the completion of the requests, but in Kubernetes the controllers just monitor objects to know when to start processing, and never call functions on other controllers directly. We propose a distributed tracing method in the Kubernetes control plane to facilitate observation of change propagation. We add change tracing identifiers to object metadata called Change Propagation ID (CPID). CPID is assigned to objects when processing objects in each controller, and CPID in the objects is propagated to other objects that the controller updates. CPIDs are newly assigned when multiple changes are merged on an object, that is, the object is updated according to multiple objects having different CPIDs. All logs related to the handling of CPIDs are sent to the external trace server, and the trace server analyses the logs to show the current status of the cascading changes. Our method incorporates the change propagation logic at the time of implementation of each controller, and the cluster operator can understand relationships between resources and controllersâ€™ behavior better to trace cascading changes of objects. We confirmed that our system can easily trace change propagation and that the performance impact of the system on clusters is not significant. The contributions of this study are as follows. â€¢ We summarize the challenges and requirements for tracing cascading changes specific to the Kubernetes control plane. â€¢ This is the first distributed tracing system applied to the Kubernetes control plane, by the idea of combining updating CPIDs in the objects and analysing logs of how CPIDs are updated. â€¢ We implement the proposed system and show that changes can be traced in the control plane and that the overhead is acceptable."
https://arxiv.org/html/2411.01297v1,Receding Hamiltonian-Informed Optimal Neural Control and State Estimation for Closed-Loop Dynamical Systems,"This paper formalizes Hamiltonian-Informed Optimal Neural (Hion) controllers, a novel class of neural network-based controllers for dynamical systems and explicit non-linear model predictive control. Hion controllers estimate future states and compute optimal control inputs using Pontryaginâ€™s Maximum Principle. The proposed framework allows for customization of transient behavior, addressing limitations of existing methods. The Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano) architecture facilitates training and ensures accurate state estimation. Optimal control strategies are demonstrated for both linear and non-linear dynamical systems.","Optimal control problems often involve designing controllers for systems with complex, chaotic, and/or non-linear dynamics. These problems are crucial in sectors such as unmanned aerial vehicles (UAVs) flight controllers, robotics, and nuclear power plants Salzmann et al. (2023); Katayama et al. (2023); Naimi et al. (2022). Various methods have been developed to address these problems. Solutions include dynamic programming, bang-bang controllers, proportional-integral-derivative (PID) controllers, linear-quadratic regulators (LQR), reinforcement learning (RL), and many variants of model predictive control (MPC). However, these methods often encounter challenges in delivering solutions that are both optimally effective and practical. Some methods react to deviations without considering the optimality of the control, while others can be expensive to operate in practice Schwenzer et al. (2021); Bemporad et al. (2002). Neural network approaches also grapple with their own unique challenges to generate solutions that consider accurate system dynamics. The quality of the control is often contingent on the quality of the training data Zheng et al. (2023). Among the developed methods, MPC is intriguing as it considers the effects of current control actions on future states. Nonetheless, many methods fail to address optimality conditions or computational efficiency when real-time optimization of the control is required Bemporad et al. (2002). To address these challenges, this chapter introduces a new class of neural network-based controllers for dynamical systems: Hamiltonian-Informed Optimal Neural (Hion) controllers, along with a novel architecture, the Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano). Hion controllers are a type of explicit MPC neural network-based models that map an observed and desired state to a continuous control strategy and expected future states. The objective is to optimize the parameters of the controller to provide state estimation and control that not only adheres to a system dynamics but also follows a given transient response profile. The model is intended to operate in a closed-loop system, where it can cope with delays in receiving state information by predicting the systemâ€™s expected future behavior. Hion controllers offer a new alternative to RL methods and other MPC-based approaches for controlling dynamical systems such as UAVs and other robots. 1.1 Background Model-predictive control (MPC) defines a set of algorithms that utilize future state estimation to generate control strategies for a system Schwenzer et al. (2021). They often involve iteratively solving control optimization problems for a receding horizon in a closed-loop environment. Classical MPCs repeatedly linearize the plantâ€™s dynamics at each iteration. These are used for state prediction and to solve for a zero-order hold control strategies via dynamic programming. The linearization-based approach enables computationally effective state estimation, albeit at the expense of some dynamic accuracy. Non-linear MPCs (NMPCs) are a later attempt to incorporate the dynamics non-linearity by using surrogate models to reduce the computational burden of state predictions. Explicit MPCs form an overlapping category that solve the control optimization in advance and reuse the previously obtained solution when tested Schwenzer et al. (2021); Bemporad et al. (2002). Our proposed controller falls under the category of an explicit NMPC. Artificial neural networks with MPC (ANN-MPC) are a subset of MPC models that most often involve utilizing neural networks as the prediction models in a MPC-based control loop Wang et al. (2021); Pang et al. (2023); Hewing et al. (2020); Cavagnari et al. (1999). Commonly, they consist of training a neural network model to predict that expected future state of system after a given amount of time, relying on trajectories data collected in simulation or test environments. The advantage of these models is that they can reduce significantly the cost of classical computationally expensive prediction model with the approximation provided by the neural network. Hence, less time is needed before the next state of the system can be sampled and an action could be taken. Recurrent neural networks are used in a subset of these algorithms Jordanou et al. (2021); Ren et al. (2022). They involve feeding previously sampled state to the model during inference and passing its knowledge to future generation to improve its predictive capability. Although a significant number of strategies involve replacing the predictive component, a subset of these works attempt to replace the MPC controller entirely by training them to be surrogate using collected trajectories and the corresponding control observed by a larger computationally-expensive MPC model Rivera et al. (2024); Hertneck et al. (2018); Ã…kesson and Toivonen (2006). However, the optimality, dynamics accuracy, and out-of-distribution performance can be impacted when using neural networks as surrogate models. Physics-informed neural network with MPC (PINN-MPC) seeks to improve even further the capability of the predictive models in the MPC loop. At their core, PINN-MPCs integrate information about the dynamics that result in improvement to the accuracy and precision of the future state estimation and the information passed to the control optimization Antonelo et al. (2024); Faria et al. (2024); Arnold and King (2021); Zheng et al. (2023). Physics-Informed Neural Nets for Control (PINC), introduced in Antonelo et al. (2024), was one of the first method proposed to use PINNs as the prediction model in a MPC strategy. It consist of training a PINN to predict an continuous estimation of future states for a fixed horizon given a constant control signal. With this method, more reliable prediction exists that can guide a control optimization. Additionally, due to the continuous state prediction, distinct control optimization strategies that rely on distinct sampling rate can be implemented or tested with a single PINN model. Limitations exist with these approaches. Although, a continuous set of future states of the system are estimated, PINN-MPCs often rely on a single predicted state to guide the control, due to the model reliance on zero-order hold control optimization. Few works exist that consider neural networks as physics-informed controller in dynamical systems. Fewer works exist that allow for the adjustment of the environment transient characteristic. Ours and newer approaches reinvents the idea of a controller when a neural network model is involved. Schiassi et al. (2022) illustrate the feasibility of training neural controllers using Bellman optimality principle, and how they can be extended using X-TFC for different initial and final conditions. Schiassi et al. (2021). Dâ€™ambrosio et al. (2021); Barry-Straume et al. (2022); Chi (2024) are recent attempts to establish neural network models that encourages PMP optimality. The works demonstrated that PMP can be used to train a neural network model to predict an optimal trajectory with desired transient properties. However, several limitations exist. One such limitation is that each model was only demonstrated to solve TVBNP for a single predefined initial and final state. This makes them impractical for closed-loop systems control where they will vary. Any new boundary condition would require fine-tuning the model (e.g., using X-TFC). Building on top of these projects, our model provides a closed-loop MPC neural network-based controller and state estimator that generalizes for variable inputs. It also theoretically defines a set of dynamical systems for which the model may be utilized. Hion removes the classical control optimization step conducted by classical MPC models, ANN-MPC, and PINN-MPC, and use a single neural network as both the prediction and control model. Fig. 1 illustrates a comparison between the conceptual behavior of different model predictive controllers. (a) Classical MPC. (b) ANN-MPC. (c) PINN-MPC. (d) Hion (Our). Figure 1: Conceptual behaviors of closed-loop model predictive controllers. 1.2 Contributions Our research formalizes a novel class of neural network controllers, termed Hion controllers, designed to optimize control strategies within closed-loop dynamical systems. These controllers function as model predictive controllers, enabling predictive decision-making. Our contributions include: 1. Establishing a theoretical framework for Hion controllers. 2. Proposing a novel neural network architecture specifically tailored for state estimation and control of dynamical systems. 3. Developing algorithms for training Hion controllers that are aligned with Pontryaginâ€™s Maximum/Minimum Principle and encourage optimal control. 1.3 Outline Following the introduction, the first section formally defines the problem of interest we aim to address. We then present our proposed methodology and the underlying theoretical principles that support it. The experimental results section demonstrates the effectiveness and capabilities of our architecture through its application to various dynamical systems. Finally, we conclude by highlighting the advantages and limitations of the method along with potential future directions of our work."
https://arxiv.org/html/2411.01286v1,Mixed-Integer MPC-Based Motion Planning Using Hybrid Zonotopes with Tight Relaxations,"Autonomous vehicle (AV) motion planning problems often involve non-convex constraints, which present a major barrier to applying model predictive control (MPC) in real time on embedded hardware. This paper presents an approach for efficiently solving mixed-integer MPC motion planning problems using a hybrid zonotope representation of the obstacle-free space. The MPC optimization problem is formulated as a multi-stage mixed-integer quadratic program (MIQP) using a hybrid zonotope representation of the non-convex constraints. Risk-aware planning is supported by assigning costs to different regions of the obstacle-free space within the MPC cost function. A multi-stage MIQP solver is presented that exploits the structure of the hybrid zonotope constraints. For some hybrid zonotope representations, it is shown that the convex relaxation is tight, i.e., equal to the convex hull. In conjunction with logical constraints derived from the AV motion planning context, this property is leveraged to generate tight quadratic program (QP) sub-problems within a branch-and-bound mixed-integer solver. The hybrid zonotope structure is further leveraged to reduce the number of matrix factorizations that need to be computed within the QP sub-problems. Simulation studies are presented for obstacle-avoidance and risk-aware motion planning problems using polytopic maps and occupancy grids. In most cases, the proposed solver finds the optimal solution an order of magnitude faster than a state-of-the-art commercial solver. Processor-in-the-loop studies demonstrate the utility of the solver for real-time implementations on embedded hardware.","Motion planning is a foundational task in vehicle autonomy. Often, motion planning algorithms serve as an intermediate autonomy level between high-level decision making and/or path-planning algorithms and low-level path-following controllers [1]. Given some local description of the environment, which is in general non-convex [2], these algorithms must construct collision-free trajectories and respect constraints on the vehicleâ€™s motion [3]. Additional considerations may factor into the motion plan such as smoothness or comfort [3], and uncertainty mitigation or risk reduction [4, 5]. Motion planning is generally performed online, which makes computational efficiency a necessity. I-A Gaps in the Literature MPC is a widely used technique in AV motion planning. Trajectories generated by MPC-based motion planners have numerous features including dynamic feasibility, constraint satisfaction, and optimality in a receding horizon sense. A significant barrier to the widespread adoption of MPC-based AV motion planners is the computational challenge of solving MPC problems of sufficient expressiveness in real-time on embedded hardware [3]. Many real-time implementable approaches for MPC-based motion planning rely on approximate or local solutions. Examples include sequential convex programming [6, 7, 8], model predictive path integral control (MPPI) [9, 10], and model predictive contouring control [11]. A limitation of these approaches is that they are not guaranteed to find the globally optimal solution when there are non-convex constraints, as in the case of obstacle avoidance [2]. Several MPC formulations have been proposed to account for non-convexity in motion planning problems. For example, branching and scenario MPC account for non-convexity via enumerated scenarios and scenario trees [12, 13, 14]. In [15], collision avoidance is enforced in MPC via a dual formulation. Ref. [16] presents a mixed-integer MPC formulation for AV motion planning which is subsequently validated using high fidelity traffic simulations. A review of mixed-integer programming formulations for motion planning is given in [2]. MPC problems formulated as mixed-integer convex programs (e.g., [16, 17]) can be solved to global optimality using branch-and-bound methods [18]. Mixed-integer programs are often solved using general-purpose mixed-integer solvers, such as Gurobi [19] and MOSEK [20]. Specialized techniques for solving mixed-integer programs have also been proposed. In [17], a custom MIQP solver for motion planning is proposed and evaluated in robotics experiments. When compared to commercial mixed-integer solvers, the solver proposed in that study was found to be approximately 5-6 times faster than MOSEK and 1.5-2.5 times slower than Gurobi. Follow-on work by the same authors used a neural network to predict the optimal integer variables for the MIQPs, thus reducing the MIQPs to convex QPs at the expense of sub-optimality [21]. Similar neural network-based approaches were proposed in [22]. Advanced set representations have been leveraged for use in motion planning problem formulations. Zonotopes are used to reduce the complexity of a hyperplane arrangement description of the obstacle avoidance constraints in [23]. The obstacle-free space is described exactly using polynomial zonotopes in a nonlinear MPC formulation in [24], and using hybrid zonotopes in a mixed-integer formulation in [25]. In both [24] and [25], the advanced set representations resulted in reductions in optimization times when compared to a hyperplane arrangement description of the obstacle-free space. Both of these papers used general-purpose solvers rather than specialized motion planning solvers as in [17, 21]. Many of the aforementioned problem formulations (e.g., [17, 15, 16]) are based on a description of the obstacles and do not explicitly use a free space description. Formulating motion planning problems in terms of the obstacle-free space enables many useful problem specifications [26, 2], such as the ability to formulate the motion planning problem using an occupancy grid map (OGM). OGMs are widely used in robotics to fuse diverse sources of uncertainty within a unified, probabilistic description of the environment [27, 28, 29]. Occupancy probabilities can be generalized to risks or costs as described in [28]. Despite their ubiquity, most existing approaches for motion planning over OGMs cast the problem in terms of a binary formulation where occupancy probabilities are not directly utilized. For instance, chance constraints are used to construct a binary OGM in a stochastic MPC formulation in [30]. Clothoid tentacles are used for motion planning over binary OGMs in [31]. In [32], a barrier function is used with nonlinear MPC to prohibit an AV from entering cells above a certain occupancy probability. In [33], time-varying OGMs are used to calculate the feasible space for a sampling-based motion planner. For situations with significant uncertainty or for which there is no feasible way for the AV to avoid entering a cell with elevated occupancy probability, these approaches can be limiting. Ref. [34] directly incorporates occupancy probabilities into a sampling-based motion planner that uses trajectory parameterizations. To the best of the authorsâ€™ knowledge, there are no prior publications showing MPC-based motion planning where cell occupancy probabilities or costs are directly used in the MPC cost function. I-B Contributions This article presents an approach to formulating and solving MIQPs for MPC-based motion planning. A hybrid zonotope set representation is used to represent the obstacle-free space, and its structure is exploited within the MIQP solver. The obstacle-free space is efficiently represented either as a general polytopic map or an OGM. Occupancy probabilities or costs associated with obstacle-free regions are incorporated into the MPC formulation. This article builds upon our previous work [35], which presented a branch-and-bound algorithm based on a notion of reachability between regions of the obstacle-free space and showed how the hybrid zonotope structure can be exploited within the QP sub-problems. The new contributions in this paper can be categorized as follows; (1) Convex relaxations: We show that certain hybrid zonotopes constructed from general polytopes and from OGMs have the property that their convex relaxation is their convex hull. We use this property in combination with â€œreachabilityâ€ constraints to tighten the QP sub-problems such that convergence can be reached with fewer branch-and-bound iterations. (2) Algorithm development: The branch-and-bound solver is modified to support multi-threading and warm-starting, and nodes are generated in such a way as to reduce the sensitivity of solution times to map complexity. The MPC formulation and MIQP solver are implemented in C++. (3) Region-dependent costs: In order to perform risk-aware planning, region-dependent costs are added to the MPC formulation. The obstacle avoidance-based branch-and-bound logic is modified accordingly. (4) Numerical results: The proposed approach is evaluated in desktop computer simulations and real-time processor-in-the-loop testing. When compared to a conventional mixed-integer constraint representation (unions of halfspace representation polytopes using the Big-M method [2]) and the state-of-the-art mixed-integer solver Gurobi [19], the proposed approach often finds the optimal solution one to two orders of magnitude faster."
https://arxiv.org/html/2411.01198v1,"Stability analysis of distributed Kalman filtering
algorithm for stochastic regression model","In this paper, a distributed Kalman filtering (DKF) algorithm is proposed based on a diffusion strategy, which is used to track an unknown signal process in sensor networks cooperatively. Unlike the centralized algorithms, no fusion center is need here, which implies that the DKF algorithm is more robust and scalable. Moreover, the stability of the DKF algorithm is established under non-independent and non-stationary signal conditions. The cooperative information condition used in the paper shows that even if any sensor cannot track the unknown signal individually, the DKF algorithm can be utilized to fulfill the estimation task in a cooperative way. Finally, we illustrate the cooperative property of the DKF algorithm by using a simulation example.","Nowadays, more and more data can be collected through sensor networks, and estimating or tracking an unknown signal process of interest based on the collected data has attracted a lot of research attention. Basically, there are two different ways to process the data, i.e., the centralized and distributed method. For the centralized processing method, measurements or estimates from all sensors over the network need to be transferred to a fusion center, which may not be feasible due to limited communication capabilities, energy consumptions, packet losses or privacy considerations. Moreover, this method lacks robustness, since whenever the fusion center fails the whole network collapses. Because of these drawbacks, the distributed processing approach arises, where each sensor utilizes the local observations and the information derived from its neighbors to estimate the unknown parameters, which is more robust and scalable compared with the centralized case. Moreover, distributed estimation algorithms may achieve the same performance with the centralized case by optimizing the adjacency matrix. Note that different kinds of distributed estimation algorithms can be obtained by combining different cooperative strategies and different estimation algorithms. For examples, incremental LMS [1, 2], consensus LMS [3, 4], diffusion LMS [5, 6, 7, 8, 9, 10], incremental LS [11, 12], consensus LS [13, 14, 15], diffusion LS [16, 17, 18, 19, 20, 21, 22], and distributed KF [23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]. In our recent work (see e.g. [3, 4, 5]), we have established the stability and performance results for the distributed LMS and LS filters, without imposing the usual independence and stationarity assumptions for the system signals. Since the KF algorithm would be optimal when the noise and the parameter variation are white Gaussian noises, here we focus on the KF algorithm in this work. Another reason for us to study this problem is that the existing convergence theory in the literature is far from satisfactory since it can hardly be applied to non-independent and non-stationary signals coming from practical complex systems where feedback loops inevitably exist, and much effort has been devoted to the investigation of distributed KF where the observation matrices of the system are deterministic. For examples, [23] studied a distributed KF based on consensus strategies, and [24] introduced a scalable suboptimal Kalman-Consensus filter and provided a formal stability and performance analysis. Moreover, [25] proposed a distributed a distributed KF algorithm based on covariance intersection method, and analyzed the stability properties, and [26] designed the optimal consensus and innovation gain matrices yielding distributed estimates with minimized mean-squared error. A quantized gossip-based interactive Kalman Filtering (QGIKF) algorithm for deterministic fixed observation matrices was studied in [27], together with the weak convergence. In addition, [28] developed a Kalman filter type consensus + innovations distributed linear estimator, and designed the optimal consensus and innovation gain matrices yielding distributed estimates with minimized mean-squared error, and [29] proposed a gossip-based distributed Kalman filter (GDKF) for deterministic time-varying observation matrices, and provided the error reduction rate. Furthermore, [30] and [31] considered Kalman-consensus filter for linear time-invariant systems, where the communication links are subject to random failures. A distributed Kalman filtering algorithm of a linear time-invariant discrete-time system in the presence of data packet drops was studied in [32], and a distributed Kalman filtering for deterministic time-varying observation matrices with mild assumption on communication topology and local observability was studied in [33]. Moreover, [35] studied the performance of partial diffusion Kalman filtering (PDKF) algorithm for the networks with noisy links, and [36] designed a distributed Kalman filtering algorithm, where the communication links of the sensor networks are subject to bounded time-varying transmission delays. Furthermore, [34] established the boundedness of the error covariance matrix and the exponentially asymptotic unbiasedness of the state estimate for deterministic time-varying observation matrices. To the best of our knowledge, the first step to consider distributed KF algorithms for the dynamical system with general random coefficients is made in [37], where each estimator shares local innovation pairs with its neighbors to collectively finish the estimation task. However, the proposed distributed KF algorithm requires to exchange a lot of information since it needs to diffuse Lğ¿Litalic_L times for each time iteration, where Lğ¿Litalic_L is not smaller than the diameter of the network topology which increases as the network grows. In this paper, we will consider a well-known distributed time-varying stochastic linear regression model, and provide a theoretical analysis for a distributed KF algorithm of diffusion type [25, 26, 30, 34] where the diffusion strategy is designed via the so called covariance intersection fusion rule. Each node is only allowed to communicate with its neighbors, and both the estimates of the unknown parameter and the inverse of the covariance matrices are diffused between neighboring nodes in such a diffusion strategy. Note also that it only needs to diffuse one time for each time iteration, which greatly reduces the communication complexity compared with [37]. The main contributions of the paper contain the following aspects: 1) The stability of the proposed distributed KF algorithm can be obtained without relying on the assumptions of the independency and stationarity of the regression signals, which makes it possible for applications to the stochastic feedback system. 2) The stability result of the proposed distributed KF algorithm is established under a cooperative excitation condition, which is a natural extension of the single sensor case, and implies that the whole sensor network can accomplish the estimation task cooperatively, even if none of the sensors can do it individually due to lack of sufficient information. In the rest of the paper, we will present the graph theory, observation model, and the distributed KF algorithm in Section II. The error equations, mathematical definitions, and assumptions are stated in Section III. The main results and proofs are given in Sections IV and V, respectively. Section VI gives a simulation result and Section VII concludes the paper and discusses related future problems. Basic notations: In the sequel, a vector Xâˆˆâ„nğ‘‹superscriptâ„ğ‘›X\in\mathbb{R}^{n}italic_X âˆˆ roman_â„ start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is viewed as an nğ‘›nitalic_n-dimensional column real vector and Aâˆˆâ„mÃ—nğ´superscriptâ„ğ‘šğ‘›A\in\mathbb{R}^{m\times n}italic_A âˆˆ roman_â„ start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT is viewed as an mÃ—nğ‘šğ‘›m\times nitalic_m Ã— italic_n-dimensional real matrix throughout the paper. Let Aâˆˆâ„nÃ—nğ´superscriptâ„ğ‘›ğ‘›A\in\mathbb{R}^{n\times n}italic_A âˆˆ roman_â„ start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT and Bâˆˆâ„nÃ—nğµsuperscriptâ„ğ‘›ğ‘›B\in\mathbb{R}^{n\times n}italic_B âˆˆ roman_â„ start_POSTSUPERSCRIPT italic_n Ã— italic_n end_POSTSUPERSCRIPT be two symmetric matrices, then Aâ‰¥Bğ´ğµA\geq Bitalic_A â‰¥ italic_B means Aâˆ’Bğ´ğµA-Bitalic_A - italic_B is a positive semidefinite matrix, and A>Bğ´ğµA>Bitalic_A > italic_B means Aâˆ’Bğ´ğµA-Bitalic_A - italic_B is a positive definite matrix. Let also Î»mâ¢aâ¢xâ¢{â‹…}subscriptğœ†ğ‘šğ‘ğ‘¥â‹…\lambda_{max}\{\cdot\}italic_Î» start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT { â‹… } and Î»mâ¢iâ¢nâ¢{â‹…}subscriptğœ†ğ‘šğ‘–ğ‘›â‹…\lambda_{min}\{\cdot\}italic_Î» start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT { â‹… } denote the largest and the smallest eigenvalues of the matrix, respectively. For any matrix Xâˆˆâ„mÃ—nğ‘‹superscriptâ„ğ‘šğ‘›X\in\mathbb{R}^{m\times n}italic_X âˆˆ roman_â„ start_POSTSUPERSCRIPT italic_m Ã— italic_n end_POSTSUPERSCRIPT, the Euclidean norm is defined as â€–Xâ€–=(Î»mâ¢aâ¢xâ¢{Xâ¢XâŠ¤})12normğ‘‹superscriptsubscriptğœ†ğ‘šğ‘ğ‘¥ğ‘‹superscriptğ‘‹top12\parallel X\parallel=(\lambda_{max}\{XX^{\top}\})^{\frac{1}{2}}âˆ¥ italic_X âˆ¥ = ( italic_Î» start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT { italic_X italic_X start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT } ) start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT, where (â‹…)âŠ¤superscriptâ‹…top(\cdot)^{\top}( â‹… ) start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT denotes the transpose operator. We use ğ”¼â¢[â‹…]ğ”¼delimited-[]â‹…\mathbb{E}[\cdot]roman_ğ”¼ [ â‹… ] to denote the mathematical expectation operator, and ğ”¼[â‹…|â„±k]\mathbb{E}[\cdot|\mathcal{F}_{k}]roman_ğ”¼ [ â‹… | caligraphic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ] to denote the conditional mathematical expectation operator, where {â„±k}subscriptâ„±ğ‘˜\{\mathcal{F}_{k}\}{ caligraphic_F start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } is a sequence of nondecreasing Ïƒğœ\sigmaitalic_Ïƒ-algebras[38]. Here we use logâ¡(â‹…)â‹…\log(\cdot)roman_log ( â‹… ) to denote the logarithmic operator based on natural number eğ‘’eitalic_e, Trâ¢(â‹…)Trâ‹…\text{Tr}(\cdot)Tr ( â‹… ) and |â‹…||\cdot|| â‹… | to denote the trace and determinant of the matrix, respectively. Note that |â‹…||\cdot|| â‹… | should not be confused with the absolute value of a scalar from the context."
https://arxiv.org/html/2411.01194v1,Relay Satellite Assisted LEO Constellation NOMA Communication System,"This paper proposes a relay satellite assisted low earth orbit (LEO) constellation non-orthogonal multiple access combined beamforming (R-NOMA-BF) communication system, where multiple antenna LEO satellites deliver information to ground non-orthogonal users. To measure the service quality, we formulate a resource allocation problem to minimize the second-order difference between the achievable capacity and user request traffic. Based on the above problem, joint optimization for LEO satellite-cell assignment factor, NOMA power and BF vector is taken into account. The optimization variables are analyzed with respect to feasibility and non-convexity. Additionally, we provide a pair of effective algorithms, i.e., doppler shift LEO satellite-cell assisted monotonic programming of NOMA with BF vector (D-mNOMA-BF) and ant colony pathfinding based NOMA exponential cone programming with BF vector (A-eNOMA-BF). Two compromise algorithms regarding the above are also presented. Numerical results show that: 1) D-mNOMA-BF and A-eNOMA-BF algorithms are superior to that of orthogonal multiple access based BF (OMA-BF) and polarization multiplexing schemes; 2) With the increasing number of antennas and single satellite power, R-NOMA-BF system is able to expand users satisfaction; and 3) By comparing various imperfect successive interference cancellation, the performance of A-mNOMA-BF algorithm exceeds D-mNOMA-BF.","With the increase in the types and numbers of communicating spacecraft, the tasks for tracking and controlling mobile equipments are becoming more prominent. The amount of satellite data transmission was also increasing, which was difficult to be performed by ground measurement and control stations alone[1]. For the urgent demand with wide area connectivity and global access, the terrestrial cellular and satellite communication networks faced great challenges to continue their independent development[2]. The integrated air-space-ground communication systems were the development trend of future communication networks to achieve efficient resource scheduling[3]. Satellite constellations utilized synergistic capabilities for global communications, navigation, environmental monitoring and other missions[4]. Low earth orbit (LEO) satellite constellation enabled any location on earth to be covered by satellites at any moment[5]. Moreover, relay satellites was able to provide measurement data and control services for satellites with other orbital altitudes[6]. To obtain more warning time in response to major natural disasters[7], the relay satellites have greatly improved the efficiency of using various types as satellites. Up to now, non-orthogonal multiple access (NOMA) technology has been applied in a variety of star-earth domains. The NOMA has shown a stronger resource allocation capability compared to orthogonal multiple access (OMA)[8, 9]. LEO constellations applying NOMA is able to the lower latency of multi-tasking services types in fifth-generation-advance networks [10]. Based on the above study, this paper further surveyed that the different requirements of terrestrial users can be satisfied by NOMA based LEO constellation. The authors of [11] investigated the information delivery rate maximization limited by data queuing and power allocation in the satellite-based Internet of Things. In addition, the authors of [12] designed the bandwidth compression of satellite terrestrial NOMA networks to guarantee usersâ€™ fairness. Both user fairness and rate maximization are difficult to realize user traffic fitting from different directions, which encourages the optimal satellite resource allocation in various beams via NOMA. Multiple-beam NOMA with different architectures were discussed in [13, 14], where the ground users were imperfectly orthogonal in the more distant space. The authors of [15] considered NOMA assisted multi-antenna satellite systems with imperfect successive interference cancelation (ipSIC), where the NOMA scheme was verified to obtain higher system rates. In [16], the ipSIC scheme suffered from the nasty effects of inter-cell interference and similar user channel gains. In practice, there will be interleaving among the satellite beams, and it is intuitive to encourage multiple users with different conical areas performing the same NOMA codebook. Compared to traditional terrestrial mobile networks, cooperative satellite communications have provided multi-access connectivity and flexible mobility[17, 18]. The theoretical capacity requirement was proved in [17] by the optimization algorithm of three-dimensional LEO satellites. As a further advance, the authors of [18] integrated the LEO satellite into the fifth generation system, where the dynamic satellite link regime was more effective to reduce the transmission delay. Multi-satellite assisted terrestrial networks were discussed in [19], which effectively improves the overall system throughput. However, the user requirement and satellite orbit information were unstable and limited in [20]. Due to the complex atmospheric conditions, the authors of [21] showed that the deployment location of ground stations severely affect the resource utilization of LEO satellites. In [22], the relay satellite brilliantly planned the trajectory of a large-scale LEO satellites and increased the transmission rate through a time extension scheme. In addition, the authors of [23] discussed spectral coexistence interference for the geostationary earth orbit (GEO) and Ku-band LEO satellite communications. In [24], LEO and GEO satellites coexisting communication scenarios were considered, where the poor high temperature effects of GEO satellite users hardly were ignored. For illustration purposes, the sharp decline of user satisfaction caused by GEO satellite gateway interference was verified in [25]. As a result, GEO satellites should provide more reasonable auxiliary functions for large-scale LEO satellite constellation networks. The relay satellite was able to cover LEO satellites on a large scale and to carry out unified planning based on measurement information [26], which is capable of avoiding the waste of satellite orbit resources. High-orbit relay satellite enhanced the system capacity of LEO satellite communications, where the grand users were virtually free from interference [27]. To flexibly accomplish beam alignment for LEO satellite [28], a practical user-accurate positioning scheme was proposed by exploiting the internal beamforming (BF) design and external beam scheduling. Unlike high orbit satellite communications, large-scale LEO satellites more easily solved complex user-base station association matrices with BF [29], where the shaped beams were designed arbitrarily. Moreover, the authors of [30, 31] proposed a robust BF scheme to overcome the interference generated for multi-beam LEO satellite networks. To serve many users within limited beams, the LEO satellites based packet BF technique made the beam center more concentrated in the userâ€™s area [32]. From the perspective of conserving satellite resources, the authors of [33] studied the power constrained LEO satellites communications by iterating the weight vectors of the beams. In general, LEO satellites with beam-hopping techniques were employed serve users of different specific regions in discrete time slots [34], whereas BF enables the continuous service of users at arbitrary locations. The successively different directional beams still satisfy different types of terminals and mitigate inter-user interference. I-A Motivations and Contributions The aforementioned investigation results have provided the basis of superior analysis on satellite networks with NOMA and BF. However, the NOMA conveniently serves multiple mission types of user access to meet complex traffic demands in LEO constellation communications. In parallel, BF formed by the number of existing LEO satellite antennas is not sufficient for full orthogonality among users. Hence, this paper investigates whether the combination of NOMA and BF further improves the satisfaction of users. Since ground stations are limited by geographical factors, LEO constellations lack the flexibility and comprehensiveness to obtain orbital and usersâ€™ information, etc. Legacy GEO satellite produces the effects of undesirable high temperatures and interference for LEO satellite communications. As a consequence, relay satellite is considered to track and measure the status information of LEO satellites anytime, and plays an important role in resolving the Doppler shift and path coordination. To the best of our knowledge, there are no related works to consider the resource optimization of the relay satellite assisted LEO constellation NOMA combined BF (R-NOMA-BF) system, which motivates us to elaborate it. The thesis iterates and transforms non-convexity in the optimization problem, and proposes two associated joint optimization algorithms. In general, resource optimization of R-NOMA-BF system usually leads to complex joint optimization problems. In general, resource optimization of R-NOMA-BF system usually leads to complex joint optimization problems. The optimization may not be achievable for large-scale instances owing to unaffordable complexity and time. For difficult solving mixed integer non-convex programming (MINCP) problems, the optimal solution is probably unknown. Consequently, important motivations for exploiting the algorithms are: 1) Determining the degree of difficulty of the resource optimization problem; 2) Providing a reasonable interval for the optimal value; and 3) Performing proper benchmarks of the approximate suboptimal solution. The basic contributions of the thesis are summarized as follows: 1. We formulate a resource allocation problem to minimize the second-order difference between the achievable capacity and user request traffic in R-NOMA-BF system. We jointly optimize power, BF vector and LEO satellite-cell alignment factor to obtain usersâ€™ satisfaction. In the power-constrained case, we give constraints on the NOMA factor and BF vector. Further, we analyze the feasibility to match LEO satellite-cell and NP-hard. 2. We design the Doppler shift LEO satellite-cell assisted monotonic programming of NOMA with BF vector (D-mNOMA-BF) algorithm. More specifically, the travel angles between relay satellite and LEO satellites are effectively measured as Doppler shifts, where users and cells are matched based on the measured results. BF vector optimization utilizes the singular value decomposition (SVD) algorithm to first isolate the interference at the cell level. Then, we design the monotone approximation optimization scheme to deal with the non-convexity about the objective function for NOMA power variables. 3. We design an ant colony pathfinding based NOMA exponential cone programming with BF vector (A-eNOMA-BF) algorithm. Relay satellite is able to cover and deal with large-scale LEO satellites with cell matching data in real time by the ant colony algorithm. Compared with D-mNOMA-BF, this algorithm enables global planning of complex dynamic path information of LEO satellites. Furthermore, the NOMA-based exponential programming scheme has higher accuracy and lower complexity. 4. We further simulate the proposed NOMA-based algorithms superior to OMA for traffic fitting in R-NOMA-BF system. It is shown that A-eNOMA-BF algorithm satisfies the demand of users better than D-mNOMA-BF. We also verified two compromise algorithms, i.e., D-eNOMA-BF and A-mNOMA-BF. We compare the performance of different polarization and single-beam schemes. The impact of usersâ€™ satisfaction on ipSIC is taken into consideration. I-B Organization and Notations The rest of this paper is organized as follows. In Section II, R-NOMA-BF system is presented, where the ground users exist in cell aligned LEO satellite. In Section III, we formulate an optimization problem for power-constrained and LEO satellite-cell matching and discuss NP-hard. In Section IV, we provide two effective algorithms in R-NOMA-BF system. Section V presents numerical results to verify the superiority of proposed algorithms, and concluded in Section VI. The key symbols in this paper are elaborated as follows: The operator |â‹…|\left|\cdot\right|| â‹… | represents the absolute value of a complex number. âˆ¥â‹…âˆ¥\left\|{\cdot}\right\|âˆ¥ â‹… âˆ¥ indicates the square of the norm. (â‹…)Hsuperscriptâ‹…ğ»{\left(\cdot\right)^{H}}( â‹… ) start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT means the conjugate transpose of the matrix; (â‹…)Tsuperscriptâ‹…ğ‘‡{\left(\cdot\right)^{T}}( â‹… ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT denotes transpose operations."
https://arxiv.org/html/2411.01055v1,Combining Physics-based and Data-driven Modeling for Building Energy Systems,"Building energy modeling plays a vital role in optimizing the operation of building energy systems by providing accurate predictions of the buildingâ€™s real-world conditions. In this context, various techniques have been explored, ranging from traditional physics-based models to data-driven models. Recently, researchers are combining physics-based and data-driven models into hybrid approaches. This includes using the physics-based model output as additional data-driven input, learning the residual between physics-based model and real data, learning a surrogate of the physics-based model, or fine-tuning a surrogate model with real data. However, a comprehensive comparison of the inherent advantages of these hybrid approaches is still missing. The primary objective of this work is to evaluate four predominant hybrid approaches in building energy modeling through a real-world case study, with focus on indoor temperature dynamics. To achieve this, we devise three scenarios reflecting common levels of building documentation and sensor availability, assess their performance, and analyse their explainability using hierarchical Shapley values. The real-world study reveals three notable findings. First, greater building documentation and sensor availability lead to higher prediction accuracy for hybrid approaches. Second, the performance of hybrid approaches depend on the type of building room, but the residual approach using a Feedforward Neural Network as data-driven sub-model performs best on average across all rooms. This hybrid approach also demonstrates a superior ability to leverage the physics-based simulation from the physics-based sub-model. Third, hierarchical Shapley values prove to be an effective tool for explaining and improving hybrid models while accounting for input correlations.","In the last decade, the operation of buildings accounted for approximately 30% of global energy consumption and 26% of CO2 emissions [18]. Over half of this consumption stems from the operation of building energy systems such as Heating, Ventilation and Air Conditioning (HVAC), as well as the electrical systems. This situation underscores the urgent need for measures to reduce energy consumption in the operation these systems. Fortunately, advancements in digitalization and sensor deployment provide a foundation for using analytical and machine learning tools to optimize the performance of building energy systems. On this basis, Building Energy Models (BEM) can be developed to simulate a buildingâ€™s real-world condition and predict future behaviour thereby enabling the recommendation of optimal control actions. However, a major challenge remains: the availability of data, particularly in terms of comprehensive building documentation, sensor coverage, and the length of recorded data. This lack of data complicates the reliable and accurate construction of BEMs but also hinders the widespread adoption of energy optimization strategies in buildings. The Building Energy Model is a digital representation of a building designed for simulating energy and temperature dynamics. It is constructed based on various characteristics of the building, including building geometry, material properties, installed energy systems, and operational inputs such as weather conditions, HVAC operation, and occupancy schedules [11]. To accurately reflect real-world conditions, the BEM is initially calibrated with sensor data to ensure faithful representation of the building and its energy systems. Once calibrated, the BEM can predict building operational performance and indoor temperature evolution across different time scales such as quarter-hourly or hourly resolution. Its comprehensive predictive capabilities make the BEM a versatile tool for various applications in building planning and operations. In retrofitting studies, it serves as an evaluation tool to assess potential measures for improving energy efficiency [36]. In building operations, it provides detailed temperature predictions as basis for control algorithms, enabling the optimization of energy system operations [20]. Classical BEMs are physics-based models that use a system of differential equations to represent all building subsystems and their interactions. This includes equations for indoor heat balance, HVAC dynamics, and incident solar radiation. These models, often referred to as physics-based models, provide insights into the underlying physical phenomena governing building performance [11]. However, the accuracy of these simulations can be compromised by the unavailability of detailed building information. Additionally, setting up a BEM can be a time-consuming process that requires significant expertise. A widely used physics-based modeling software in both academia and industry is EnergyPlus [39], which enables dynamic thermal simulations and supports the energy-efficient design and operation of buildings. In case of simplified building descriptions or the requirement to lower computational complexity, reduced-order physics-based models may be employed. These models consist of simplified physical descriptions in the form of differential equations paired with data-driven identification of model coefficients, such as thermal Resistance-Capacitance (RC) models [11]. More recent approaches of building energy modeling focus on data-driven methods, which utilize statistical and machine learning techniques such as Autoregressive Integrated Moving Average (ARIMA), Feedforward Neural Network (FFNN), Long-Short Term Memory (LSTM) and Convolutional Neural Network (CNN) [11]. These methods rely on sensor measurements to establish relationships between defined input and output variables. Commonly modeled relationships include those between building operation data â€“ such as energy consumption or temperature â€“ and the corresponding sensor readings. Because these models are directly applied to data, the underlying mechanisms are not easily accessible or interpretable, leading them to be classified as data-driven models. Unlike physics-based models, data-driven approaches do not require detailed physical building information or the calibration of physical parameters, making them easier to set up as digital twins. However, their performance is highly dependent on the quantity and quality of the available measurement data. Despite this dependency, data-driven models have demonstrated high accuracy in numerous studies and are particularly well-suited for modelling buildings at an urban scale, thanks to their reduced configuration time [29]. An emerging category of approaches for building energy modeling is known as hybrid or physics-induced modeling, which combines elements from physics-based and data-driven methods. These approaches offer significant advantages, particularly in scenarios where sensor data or detailed documentation is lacking. Several research studies aim to combine reduced-order physics-based models with data-driven models through two general strategies. The first strategy involves formulating a loss function based on the reduced-order model to train the data-driven model. A prominent example is the use of Physics-informed Neural Network (PINN), where a physics-informed loss function is devised based on RC model temperature dynamics in order to train a FFNN [17, 26, 8]. The second strategy consists of incorporating reduced-order model elements into a data-driven model architecture. A number of works model building dynamics with a State-Space model framework and parameterize state and/or observation equation with FFNN [14], Neural Ordinary Differential Equations [38] or Graph Neural Networks [47]. However, ensuring physical consistency still remains a challenge in aforementioned strategies [12]. Another line of work researches the combination of high-fidelity physics-based models with data-driven models. We identify four distinct approaches in the context of our research: assistant, residual, surrogate, and augmentation. In the assistant strategy, the output from a physics-based model is used as an additional input to the data-driven model [6, 2] or conversely, the data-driven model can provide inputs or corrections to the physics-based model [42, 4, 21]. This additional input may provide valuable context information, but also increases the number of input features. The residual strategy involves using a data-driven model to learn the residuals between the output of a physics-based model and actual observed data [37, 10, 13, 25]. This approach aims to capture unmodeled physical phenomena and variations in the data that the physics-based model may not fully account for. It is particularly useful when certain inputs are not reliably represented in the data or when incorporating domain knowledge with a physical reference input is necessary. In the surrogate strategy, a data-driven model is trained to replace the physics-based model by using the same inputs, with the physics-based modelâ€™s simulations serving as the target outputs [45, 44, 15, 41, 35]. The primary motivation for this approach is to reduce computation time, allowing the data-driven model to perform simulations that would otherwise be time-consuming with the physics-based model. Nevertheless, it relies on a sufficiently accurate simulator. The augmentation strategy augments the real data with simulated data from a physics-based model, training the data-driven model on this augmented dataset and subsequently adapting it to real-world situations [27, 9, 1, 40]. This augmentation strategy is particularly advantageous when little to no real-world data is available, as it allows the model to leverage simulated data to improve its accuracy and generalization capabilities. However, this approach may suffer from performance degradation if there is a significant disparity between simulated and real data. Several studies investigate the dependency of hybrid models on documentation and sensor measurements. For example, various levels of building documentation are explored in [6] using an assistant methodology that combines IDA-ICE software and Gradient Boosting Regression Trees (GBRT). In scenarios with limited sensor data, [24] employs a Surrogate-FFNN model based on EnergyPlus simulations to assess performance with varying data availability â€“ 100%, 30% and 20%, or even less [7]. Additionally, [19] explores the impact of seasonally limited data using an Augmentation-FFNN-CNN model implemented through the MATLAB toolbox CARNOT [46]. Few research studies address the explainability of hybrid models. For example, [22] evaluates a Surrogate-Random Forest model trained on simulated data from Energyplus using Pearson correlation and Gini importance scores to assess feature relevance. Similarly, [5] enhances the interpretability of a Surrogate-GBRT model trained on simulated data from Energyplus by incorporating a causal inference framework, thereby making the hybrid model inherently more transparent. Although hybrid approaches in BEM are gaining increasing attention, three significant research gaps remain. First, most recent studies focus on developing new hybrid methods within specific data contexts, yet there is a lack of a comprehensive research comparing the advantages and disadvantages of different hybrid approaches. A systematic evaluation of these methods is still missing, which limits our understanding of their relative strengths and weaknesses. Second, there are only few works that compare scenarios with limited building documentation or sensor data. Third, the majority of existing literature primarily evaluates hybrid models in terms of accuracy, with less attention given to the explainability of these approaches. Greater emphasis on explainability is crucial for understanding the hybrid models general behaviour, uncovering model biases of the physics-based sub-model and building trust for real-world application. These research gaps highlight the need for more comparative studies and a deeper exploration of how hybrid BEMs perform under varying levels of data availability and quantity. In our study, we focus on the thermal modeling aspect of the BEM to predict indoor temperature dynamics, as understanding these dynamics forms the foundation for further analysis. Furthermore, we concentrate on a high-fidelity physics-based model for the construction of hybrid models. This paper tries to address the previously mentioned research gaps by making the following three main contributions: â€¢ We enhance the understanding of hybrid building energy models by investigating and comparing four predominant hybrid approaches across three challenging real-world scenarios, each characterized by varying levels of building documentation and sensor data availability. â€¢ We apply a hierarchical Shapley value framework to an agglomerative clustering analysis using Pearsonâ€™s distance metric, providing valuable insights into the nature of hybrid models while accounting for the correlations. This also allows to investigate potential model biases of the physics-based part such as a bias at higher outdoor temperatures. â€¢ We examine and compare performance of the four hybrid approaches in a limited training data setting, offering a detailed analysis of their dependency on data quantity and their robustness under constrained conditions. The remainder of this paper is organized as follows: Section 2 outlines the methodology used in this study. Section 3 introduces the data set-up and implementation details of the hybrid approaches. Section 4 conducts case studies across various documentation and sensor scenarios and analyzes the results. Finally, Section 5 draws conclusions and provides an outlook on future research directions."
https://arxiv.org/html/2411.00995v1,Safe Imitation Learning-based Optimal Energy Storage Systems Dispatch in Distribution Networks,"The integration of distributed energy resources (DER) has escalated the challenge of voltage magnitude regulation in distribution networks. Traditional model-based approaches, which rely on complex sequential mathematical formulations, struggle to meet real-time operational demands. Deep reinforcement learning (DRL) offers a promising alternative by enabling offline training with distribution network simulators, followed by real-time execution. However, DRL algorithms tend to converge to local optima due to limited exploration efficiency. Additionally, DRL algorithms can not enforce voltage magnitude constraints, leading to potential operational violations when implemented in the distribution network operation. This study addresses these challenges by proposing a novel safe imitation reinforcement learning (IRL) framework that combines IRL and a designed safety layer, aiming to optimize the operation of Energy Storage Systems (ESSs) in active distribution networks. The proposed safe IRL framework comprises two phases: offline training and online execution. During the offline phase, optimal state-action pairs are collected using an NLP solver, guiding the IRL policy iteration. In the online phase, the trained IRL policyâ€™s decisions are adjusted by the safety layer to maintain safety and constraint compliance. Simulation results demonstrate the efficacy of Safe IRL in balancing operational efficiency and safety, eliminating voltage violations, and maintaining low operation cost errors across various network sizes, while meeting real-time execution requirements.","The penetration of renewable energies has pressed emerging challenges to distribution network operators (DSOs) due to the lag in distribution network upgrades, particularly evident in the Netherlands, where the severity of voltage magnitude problems has escalated [1]. This bottleneck in infrastructure modernization has significantly promoted energy investors to deploy energy storage systems (ESSs) into distribution networks, offering a viable pathway to mitigate voltage magnitude instabilities and enhance the resilience of the distribution network [2]. In this context, optimizing ESSs dispatch is crucial to ensure voltage regulation while also aiming to minimize operational costs amidst the constraints of an aging network [3]. However, fluctuating prices, varying electricity demands, and uncertainty in renewable generation bring significant challenges in defining the dynamic and sequential optimal operation decisions. Traditional model-based approaches, which rely on predefined forecasts or complex probability functions to manage uncertainties, often struggle with real-time decision-making [4]. As these methods require extensive computational resources, they can be inefficient in adapting to the fast-paced and variable nature of the optimal ESSs dispatch problem. Deep Reinforcement Learning (DRL) emerges as a promising alternative to traditional model-based approaches, offering a model-free solution that excels in fast-paced, sequential decision-making scenarios [5]. DRL has been successfully applied in diverse fields such as game playing, robotics control, and industrial systems, where it transforms operational sequences into Markov Decision Processes (MDPs) [6]. In the context of energy systems tasks, DRL has demonstrated the potential to optimize complex tasks, such as voltage control [7] and energy management [8], by enabling the DRL algorithms to learn directly from interactions with the built energy system simulator. This capability allows DRL to handle the complexities and uncertainties inherent in distribution networks more effectively [8]. One of the primary challenges associated with DRL algorithms is low exploration efficiency. The agent requires substantial time to learn due to the need for extensive exploration of the action space [9]. This inefficiency is particularly problematic in scenarios with high-dimensional action spaces, such as controlling multiple ESSs in a distribution network [10]. This low exploration efficiency consequently leads DRL algorithms to converge prematurely to suboptimal solutions, as fully exploring all possible actions becomes increasingly difficult. For instance, previous research [10] has shown that DRL algorithms often focus on leveraging only a single ESS that is highly sensitive to voltage magnitude fluctuations, while neglecting the potential flexibility offered by other ESSs. This behavior results in suboptimal performance and prevents the system from fully utilizing the flexibility of multiple ESSs [11]. Imitation Learning (IL) offers a complementary approach that can enhance the data efficiency of DRL algorithms [12]. IL is a strategy where the learning agent aims to mimic the behavior of an expert by learning from optimal state-action pairs [13]. In the context of ESSs dispatch, expert decisions can be derived from solving daily scenarios using commercial solvers, which derive optimal state-action pairs under various scenarios. These pairs provide a high-quality dataset that the RL agent can use to learn desired behaviors without needing to engage in inefficient online exploration [14]. By incorporating IL, the learning process of DRL algorithms is significantly accelerated, as the RL agent starts with a base of optimal actions in different states, thereby reducing the exploration space and focusing on refining strategies that have already proven to be effective. For instance, [15] integrated expert demonstrations into the training phase of DRL for real-time dispatch of generation units. Results showed DRL algorithms can achieve faster convergence and improve 2.2% performance compared to the model-based solution in real-time dispatch tasks. The work in [16] applied a Mixed-Integer Linear Programming (MILP)-based IL approach to Heating, Ventilation, and Air Conditioning (HVAC) control. By using IL, a control policy can be trained by imitating the optimal MILP-based decisions, enabling efficient real-time HVAC control without the need for solving complex optimization problems in real-time. In [17], IL is leveraged to accelerate DRL algorithms training for building HVAC control. Results demonstrated DRL algorithms could achieve better control efficiency and effectiveness in managing building HVAC systems. In [18], an IL-based approach is proposed for online optimal power scheduling of microgrids. The IL-based controller can rapidly adjust power scheduling in real-time, ensuring optimal operation of microgrids under varying conditions by learning from optimal scheduling policies derived from offline optimization models. Previous studies have shown that IL or an offline trained IL followed by online DRL fine-tuning can improve the training efficiency and the performance of dispatch policies. However, this combination presents several challenges. First, purely imitation learning-based approaches are highly sensitive to the training dataset, leading to poor generalizability and potentially suboptimal behavior in scenarios that were not part of the training data [12]. Second, although online fine-tuning can mitigate this problem, it may also cause a performance collapse due to the state-action distribution shift, where the DRL agentâ€™s exploration leads to actions and states that deviate significantly from those seen during the imitation learning phase [19]. Third, both of these previous approaches struggle to guarantee the feasibility of the decisions or enforce operational constraints, as they do not explicitly account for feasibility during the imitation learning process [20]. In light of these challenges, our contributions are threefold: â€¢ We introduce a framework that combines the strengths of DRL algorithms and IL to enhance the training efficiency and dispatch performance of trained algorithms. Moreover, the framework can rigorously enforce operational constraints in distribution networks during the dispatch. This innovative approach addresses the limitations previously identified in these areas. â€¢ During the offline training phase, we employ a dual-gradient strategy utilizing both the IL policy and the critic network. This approach stabilizes the training process and expedites learning, effectively overcoming standard DRL algorithmsâ€™ computational and exploration challenges. â€¢ To guarantee the feasibility of dispatch decisions, the safe layer proposed in our previous paper [21] is extended to the framework during the online operation. This layer filters out unsafe actions, redirecting them into safer alternatives, thus ensuring the operational feasibility of decisions in scenarios not covered by expert data."
https://arxiv.org/html/2411.02333v1,Discrete the solving model of time-variant standard Sylvester-conjugate matrix equations using Euler-forward formula: An analysis of the differences between sampling discretion errors and space compressive approximation errors in optimizing neural dynamics,"Time-variant standard Sylvester-conjugate matrix equations are presented as early time-variant versions of the complex conjugate matrix equations. Current solving methods include Con-CZND1 and Con-CZND2 models, both of which use ode45 for continuous model. Given practical computational considerations, discrete these models is also important. Based on Euler-forward formula discretion, Con-DZND1-2i model and Con-DZND2-2i model are proposed. Numerical experiments using step sizes of 0.1 and 0.001. The above experiments show that Con-DZND1-2i model and Con-DZND2-2i model exhibit different neural dynamics compared to their continuous counterparts, such as trajectory correction in Con-DZND2-2i model and the swallowing phenomenon in Con-DZND1-2i model, with convergence affected by step size. These experiments highlight the differences between optimizing sampling discretion errors and space compressive approximation errors in neural dynamics.","Standard Sylvester-conjugate matrix equations (SSCME) [1] are the earliest version of complex conjugate matrix equations (CCME) [2]. And SSCME is time-invariant. In recent years of studies, Wu et al. provided matrix algebraic formula methods [3] and iterative solving methods [2] based on approximation theory. The essence of the iterative methods is to gradually approach the theoretical solution using multi-step computations. And time-variant standard Sylvester-conjugate matrix equations (TVSSCME) [4] is the time-variant extension of SSCME. TVSSCME is supplemented by the differences between differential algebra and linear algebra operations [5, 6]. The difference between SSCME and TVSSCME solutions is shown in Fig. 1. Unless otherwise specified, let Oğ‘‚Oitalic_O represent â€œnull matrix"", and only consider the unique theoretical solution Xâˆ—â¢(Ï„)superscriptğ‘‹âˆ—ğœX^{\ast}(\tau)italic_X start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_Ï„ ), same as below. (a)Xâ¢(Ï„)â¢Fâˆ’Aâ¢Xâ¢(Ï„)âˆ’C=Oğ‘‹ğœğ¹ğ´ğ‘‹ğœğ¶ğ‘‚X(\tau)F-AX(\tau)-C=Oitalic_X ( italic_Ï„ ) italic_F - italic_A italic_X ( italic_Ï„ ) - italic_C = italic_O, where Ï„â†’+âˆâ†’ğœ\tau\to+\inftyitalic_Ï„ â†’ + âˆ, Xâ¢(Ï„)â†’Xâˆ—â¢(Ï„).â†’ğ‘‹ğœsuperscriptğ‘‹âˆ—ğœX(\tau)\to X^{\ast}(\tau).italic_X ( italic_Ï„ ) â†’ italic_X start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_Ï„ ) .(b)Xâ¢(Ï„)â¢Fâ¢(Ï„)âˆ’Aâ¢(Ï„)â¢Xâ¢(Ï„)âˆ’Câ¢(Ï„)=Oğ‘‹ğœğ¹ğœğ´ğœğ‘‹ğœğ¶ğœğ‘‚X(\tau)F(\tau)-A(\tau)X(\tau)-C(\tau)=Oitalic_X ( italic_Ï„ ) italic_F ( italic_Ï„ ) - italic_A ( italic_Ï„ ) italic_X ( italic_Ï„ ) - italic_C ( italic_Ï„ ) = italic_O, where Ï„â†’+âˆâ†’ğœ\tau\to+\inftyitalic_Ï„ â†’ + âˆ, Xâ¢(Ï„)â†’Xâˆ—â¢(Ï„).â†’ğ‘‹ğœsuperscriptğ‘‹âˆ—ğœX(\tau)\to X^{\ast}(\tau).italic_X ( italic_Ï„ ) â†’ italic_X start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_Ï„ ) . Figure 1: Differences between SSCME(a) and TVSSCME(b). TVSSCME is currently primarily solved using zeroing neural dynamics (ZND) models Con-CZND1 [4] and Con-CZND2 [4]. Above two models structure can be seen in Fig. 2. Random InputRandom InputRandom InputRandom Inputâ‹®â‹®\vdotsâ‹®n{â‹®â‹®\vdotsâ‹®n{â‹®â‹®\vdotsâ‹®n{â‹®â‹®\vdotsâ‹®â€¦â‹®â‹®\vdotsâ‹®â‹®â‹®\vdotsâ‹®OutputOutputOutputOutputâ‹®â‹®\vdotsâ‹®n{â‹®â‹®\vdotsâ‹®n{ Hidden Complex Layer1 Input Real Layer Hidden Complex Layer2 Hidden Real Layer1 Output Real Layer Random InputRandom InputRandom InputRandom Inputâ‹®â‹®\vdotsâ‹®n{â‹®â‹®\vdotsâ‹®n{â€¦â‹®â‹®\vdotsâ‹®â‹®â‹®\vdotsâ‹®OutputOutputOutputOutputâ‹®â‹®\vdotsâ‹®n{â‹®â‹®\vdotsâ‹®n{ Input Real Layer Hidden Real Layer1 Output Real Layer Figure 2: Different between Con-CZND1 [4] model and Con-CZND2 [4] model. 2 Con-CZND1 model. 2 Con-CZND2 model. However, Con-CZND1 model essentially approximates using the complex field error, while Con-CZND2 model approximates using the real field error. In ode45 [7] solver, Con-CZND2 model does not perform as well as Con-CZND1 model. Discrete neural dynamics is validated in previous studies to reduce the error between theoretical and numerical solutions [8]. Zhang et al. continued to develop discretion in the real field, progressing from Euler-forward formula [9, 10] to an 11-point sampling discretion [11, 12, 13]. However, there is no exploration of neural models for solving TVSSCME using sampling discretion in the existing literature. According to the known studies, the two continuous solution models, Con-CZND1 and Con-CZND2, show significant differences due to the approximation effects of the internal ode45 [14] solver. Additionally, Con-CZND1 model exhibits space compressive approximation phenomenon. Therefore, it is essential to rigorously establish a discrete neural dynamics model for TVSSCME. The rest of this article is organized as follows: Section 2 provides the definition of TVSSCME and supplementary knowledge. Section 3 defines the Con-DZND1-2i discrete solving model over the complex field and the Con-DZND2-2i discrete solving model over the real field. Section 4 presents simulations that validate the effectiveness of each model and compares their strengths and weaknesses. Sections 5 and 6 summarizes this article and suggests future directions. Before proceeding to the next section, the main contributions of this article are as follows: (1) Con-DZND1-2i model, which directly defines complex field error, and Con-DZND2-2i model, which maps to real field error, are proposed for solving TVSSCME. (2) Based on Euler-forward formula, both discrete models, Con-DZND1-2i and Con-DZND2-2i, which use different step sizes, can ultimately approximate the theoretical solution. (3) Con-DZND1-2i model defines complex field error, while Con-DZND2-2i model maps to real field error. These models highlight a significant difference between optimizing space compressive approximation errors and optimizing sampling discretion errors in neural network optimization. Both aspects should be considered from different perspectives."
https://arxiv.org/html/2411.02292v1,ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence,"Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can process data without the limitation of time intervals. They have advantages in learning and understanding the evolution of complex real dynamics. Many previous works have focused on NODEs in concise forms, while numerous physical systems taking straightforward forms, in fact, belong to their more complex quasi-classes, thus appealing to a class of general NODEs with high scalability and flexibility to model those systems. This, however, may result in intricate nonlinear properties. In this paper, we introduce ControlSynth Neural ODEs (CSODEs). We show that despite their highly nonlinear nature, convergence can be guaranteed via tractable linear inequalities. In the composition of CSODEs, we introduce an extra control term for learning the potential simultaneous capture of dynamics at different scales, which could be particularly useful for partial differential equation-formulated systems. Finally, we compare several representative NNs with CSODEs on important physical dynamics under the inductive biases of CSODEs, and illustrate that CSODEs have better learning and predictive abilities in these settings.","Neural ODEs (NODEs) [4] were developed from the limiting cases of continuous recurrent networks and residual networks and exhibit non-negligible advantages in data incorporation and modeling unknown dynamics of complex systems, for instance. Their continuous nature makes them particularly beneficial to learning and predicting the dynamical behavior of complex physical systems, which are often difficult to realize due to sophisticated internal and external factors for the systems. Starting from the introduction of NODEs, many types of variants of NODEs have been studied (see, e.g., [6, 19, 17, 15]). Nevertheless, there are rare studies concerning highly scalable and flexible dynamics that also present complex nonlinear natures, bringing difficulties in their modeling and analyses. No in-detail research attention has been paid to the scalability of depth and structure in NODEs despite numerous physical systems in the real world having inscrutable dynamics and compositions. To fill in this gap, we propose ControlSynth Neural ODEs (CSODEs), in whose structure another sub-network is also incorporated for enlarging the dexterity of the composition and controlling the evolution of the state. Different from most of the existing methods and experiments, we focus on widely investigated physical models, with known state evolution under necessary conditions. Subsequently, we will show that the proposed NODEs are effective in learning and understanding those models. Our contributions are mainly composed of the novel structure of CSODEs, their convergence guarantees, and the comparative experiments among CSODEs, several representative NODEs, and their divisions, illustrating the beneficiality of CSODEs in the setting of prediction. The convergence conditions provide tractable solutions for constraining the learned model to a convergent one. The preliminary experiment demonstrates that our CSODEs can learn the evolution of the dynamics faster and more precisely. Also, we show that introducing sub-networks into CSODE does not impact the overall computational performance more than the other comparable NNs. Finally, we compare NODE, Augmented Neural ODE (ANODE), Second Order Neural ODE (SONODE), CSODE, and its variant in real dynamical systems. The experimental results indicate that our CSODE is beneficial to more accurate time series prediction in the systems. Our code is available online at https://github.com/ContinuumCoder/ControlSynth-Neural-ODE."
https://arxiv.org/html/2411.02230v1,Energy-Aware Coverage Planning for Heterogeneous Multi-Robot System,"We propose a distributed control law for a heterogeneous multi-robot coverage problem, where the robots could have different energy characteristics, such as capacity and depletion rates, due to their varying sizes, speeds, capabilities, and payloads. Existing energy-aware coverage control laws consider capacity differences but assume the battery depletion rate to be the same for all robots. In realistic scenarios, however, some robots can consume energy much faster than other robots; for instance, UAVs hover at different altitudes, and these changes could be dynamically updated based on their assigned tasks. Robotsâ€™ energy capacities and depletion rates need to be considered to maximize the performance of a multi-robot system. To this end, we propose a new energy-aware controller based on Lloydâ€™s algorithm to adapt the weights of the robots based on their energy dynamics and divide the area of interest among the robots accordingly. The controller is theoretically analyzed and extensively evaluated through simulations and real-world demonstrations in multiple realistic scenarios and compared with three baseline control laws to validate its performance and efficacy.","There has been an increase in the use of autonomous robots in recent years, particularly for the purpose of surveillance and monitoring environments. The collected data from the robots can be used to make further decisions. For example, in a precision agriculture application, robot-collected hyper-spectral images might be used for weed localization and treating the affected areas with herbicides [32]. For such applications, sensor (or spatial) coverage is an important computational problem to consider. In a multi-robot sensor coverage problem, the objective is to distribute the robots (sensors) in a manner that optimally monitors the workspace covering the spatial region (environment) with at least one robotâ€™s sensor footprint. This objective is different from and is not to be confused with the area coverage problem, where the objective is that the robot(s) should visit each and every part of the environment. A promising technique for achieving the sensor coverage goal is to divide the environment into regions using Lloydâ€™s algorithm, also known as Voronoi partitions, which can be optimized based on constraints such as uncertainties in sensing and sensor health, among others [28, 31, 24, 1]. A heterogeneous group of robots (e.g., a group of UGVs and UAVs) allows the system to develop proficiency in different areas of the task despite their inherent limitations on a specific capability, such as sensing or mobility [27, 2]. UAVs, for instance, can cover more terrain and are less susceptible to obstacles than UGVs, which, on the other hand, generally have higher battery capacities and deplete energy at a slower rate than UAVs [38]. We posit that the robotâ€™s energy depletion rate is an important factor that needs to be incorporated into their controllers to increase the overall lifetime of the multi-robot system and optimize the mission objective. Figure 1: Regions assigned by standard Voronoi partitioning (left) and the proposed energy-aware controller (EAC) (right). The robots have the same initial battery level. However, robot 3â€™s depletes its energy three times faster than the other robots. Therefore, robot 3â€™s EAC-assigned region area (area in m2superscriptğ‘š2m^{2}italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT in parentheses) is less than other robots by adapting the weights (wisubscriptğ‘¤ğ‘–w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) based on the ratio of energy depletion rate between the robots. The rate of energy depletion could vary significantly between robots due to their sensor payloads (e.g., some robots may be equipped with high-power thermal cameras or power drill tools in search and rescue applications), as well as their velocities (e.g., UAVs operating at higher altitude will deplete energy faster than the ones at lower altitudes) [23, 19, 36]. Moreover, the energy depletion rate of a robot can change dynamically over time (e.g., activating or deactivating a payload, increasing the velocity, etc.), and the robots need to adjust their coverage loads accordingly. Fig. 1 illustrates such a scenario. Here, the energy-aware controller optimally assigns a lower area to robot 3, which had almost four times higher energy depletion rate compared to the other robots, which will limit its ability to cover more area. Assigning a larger area to a robot with more energy or a lower depletion rate and smaller areas to other robots will distribute the workload among robots and ensure effective coverage and timely execution of tasks. Energy availability has significant implications for coverage planning. Several works proposed in the literature [11, 12, 6, 8] focusing on this problem consider limiting the robot velocity and/or partitioning the regions based on the current energy levels. However, this will result in poor coverage quality when robots have different energy depletion rates. Moreover, as discussed earlier, real-world applications might require a heterogeneous group of robots with different and dynamic energy characteristics, where robots consume energy differently.Moreover, restricting the robotsâ€™ velocities like performed in [12, 8] could conflict with the low-level, time-limited tasks the robots are assigned to. To this end, we present a novel distributed multi-robot controller that holistically considers the robotâ€™s energy capacity and depletion characteristics of other robots for optimal energy-aware coverage planning. We extensively validate the controller in simulations (scaling up to 100100100100 robots) and with real robots within an in-house swarm robotics testbed (see the video). The main contributions of our paper are as follows. â€¢ To the best of our knowledge, this is the first study that considers differences in the energy capability of the robots with heterogeneous energy depletion rates for effective multi-robot coverage planning. â€¢ Unlike prior studies, we consider that the robots might have time-varying energy depletion rates during a mission. Compared to the state-of-the-art relevant algorithms, our proposed Energy-Aware Controller (EAC) significantly reduces coverage cost and achieves energy-balancing coverage objectives."
https://arxiv.org/html/2411.02186v1,Limiting Kinetic Energy through Control Barrier Functions: Analysis and Experimental Validation,"In the context of safety-critical control, we propose and analyse the use of Control Barrier Functions (CBFs) to limit the kinetic energy of torque-controlled robots. The proposed scheme is able to modify a nominal control action in a minimally invasive manner to achieve the desired kinetic energy limit. We show how this safety condition is achieved by appropriately injecting damping in the underlying robot dynamics independently of the nominal controller structure. We present an extensive experimental validation of the approach on a 7-Degree of Freedom (DoF) Franka Emika Panda robot. The results demonstrate that this approach provides an effective, minimally invasive safety layer that is straightforward to implement and is robust in real experiments. A video of the experiments can be found here","Collaborative robots, sometimes called cobots, are gaining traction across a wide range of industries, including logistics, service robotics, and manufacturing [1, 2]. Safety is a critical control objective when these robots share space with humans [3, 4]. The recent rise of learning-based controllers, which typically only provide probabilistic safety guarantees, underscores the need for safety-critical approaches [5]. ISO standards [6] attempt to formalise the safety hazards in this setting, and their mitigation is an active research area. Some works prevent interaction, by enforcing a speed-dependent separation distance between the robot and operator, assuming reliable detection methods [7, 8]. Other works limit long-duration interaction power and force by implementing, e.g., impedance control [9, 10, 11]. Yet other approaches explore various dynamic human-robot impact scenarios and relate the impact velocity to the risk of injury [12]. In this work safety is addressed by bounding the kinetic energy that could potentially be transferred to a human operator, in order to prevent injury in collision scenarios. The importance of this choice is backed by numerous publication which relate directly relevant safety metrics to the energy flow generated from the interaction [13, 4]. Furthermore, the power and force limiting (PFL) conditions in the ISO/TS 15066 [6], which are the only collaborative conditions in which contact between humans and robots are considered, are addressed through energetic constraints. We propose a method that takes the form of a safety filter, enforcing a bound on maximum kinetic energy while minimally altering a desired control input. We make use of Control Barrier Functions (CBFs), a safety-critical control algorithm able to constrain the robot to a region of its state space representing safe operating conditions [14]. Most CBF implementations in robotics apply to safety-critical kinematic control (i.e., tasks in which the safety constraint represents obstacle avoidance conditions) and rely on lower-level controllers to handle system dynamics [15, 16, 17]. Instead, we investigate the use of energy-based CBFs and, different from previous works such as [18, 19], utilize them to directly limit the kinetic energy of a torque-controlled robot. We recognise relevant related works proposing schemes to limit the kinetic or total energy of torque-controlled manipulators for safety objectives. These works are motivated by energy-aware and passivity arguments [20, 21, 22], stressing the fact that safety measures are closely related to energy- and power-based metrics. The recent work [23] presents a control algorithm that is able to limit the kinetic energy, achieved by using higher-order CBFs in a system augmented with energy tanks [24], used to enforce passivity of the overall scheme. Other approaches attempt to limit kinetic energy [25] and total energy [26] of controlled robots, also using energy-tank based arguments to recover passivity. In this work, we present a novel approach that avoids considering passivity as a strict constraint to be achieved at design phase. Instead, we achieve the kinetic energy bound directly through the proposed CBF-based algorithm. We analytically and experimentally show that the proposed CBF operates solely by injecting damping into the system, ensuring that the safety-critical control action inherently preserves the passivity of any nominal passive closed-loop system. This eliminates the need for supplementary tools such as energy tanks, making the proposed scheme significantly simpler than most of the state-of-the-art solutions. The main contributions of this paper are: 1. A kinetic energy-limiting CBF-based safety filter and analysis of its energetic properties. 2. Extensive experimental validation on a 7-DoF robot manipulator of the proposed safety-critical control system. The remainder of this paper is outlined as follows. Section 2 contains the mathematical background and analysis involving the specific CBF used in this work. We present extensive experimental results in four scenarios in Section 3. Finally, Sections 4 and 5 discuss the results and conclude the paper."
https://arxiv.org/html/2411.02158v1,Learning Multiple Initial Solutions to Optimization Problems,"Sequentially solving similar optimization problems under strict runtime constraints is essential for many applications, such as robot control, autonomous driving, and portfolio management. The performance of local optimization methods in these settings is sensitive to the initial solution: poor initialization can lead to slow convergence or suboptimal solutions. To address this challenge, we propose learning to predict multiple diverse initial solutions given parameters that define the problem instance. We introduce two strategies for utilizing multiple initial solutions: (i) a single-optimizer approach, where the most promising initial solution is chosen using a selection function, and (ii) a multiple-optimizers approach, where several optimizers, potentially run in parallel, are each initialized with a different solution, with the best solution chosen afterward. We validate our method on three optimal control benchmark tasks: cart-pole, reacher, and autonomous driving, using different optimizers: DDP, MPPI, and iLQR. We find significant and consistent improvement with our method across all evaluation settings and demonstrate that it efficiently scales with the number of initial solutions required. The code is available at https://github.com/EladSharony/miso.","Many applications, ranging from trajectory optimization in robotics and autonomous driving to portfolio management in finance, require solving similar optimization problems sequentially under tight runtime constraints (Paden et al., 2016; Ye et al., 2020; Mugel et al., 2022). The performance of local optimizers in these contexts is often highly sensitive to the initial solution provided, where poor initialization can result in suboptimal solutions or failure to converge within the allowed time (Michalska & Mayne, 1993; Scokaert et al., 1999). The ability to consistently generate high-quality initial solutions is, therefore, essential for ensuring both performance and safety guarantees. Conventional methods for selecting these initial solutions typically rely on heuristics or warm-starting, where the solution from a previously solved, related problem instance is reused. More recently, learning-based solutions have also been proposed, where neural networks are used to predict an initial solution. However, in more challenging cases, where the optimization landscape is highly non-convex or when consecutive problem instances rapidly change, predicting a single good initial solution is inherently difficult. To this end, we propose Learning Multiple Initial Solutions (MISO) (Figure 1), in which we train a neural network to predict multiple initial solutions. Our approach facilitates two key settings: (i) a single-optimizer method, where a selection function leverages prior knowledge of the problem instance to identify the most promising initial solution, which is then supplied to the optimizer; and (ii) a multiple-optimizers method, where multiple initial solutions are generated jointly to support the execution of several optimizers, potentially running in parallel, with the best solution chosen afterward. More specifically, our neural network receives a parameter vector that characterizes the problem instance and outputs Kğ¾Kitalic_K candidate initial solutions. The network is trained on a dataset of problem instances paired with (near-)optimal solutions and is evaluated on previously unseen instances. Crucially, the network is designed not only to predict good initial solutionsâ€”those close to the optimalâ€”but also to ensure that these solutions are sufficiently diverse, potentially spanning all underlying modes of the problem in hand. To actively encourage this multimodality, we implement training strategies such as a winner-takes-all loss that penalizes only the candidate with the lowest loss, a dispersion-based loss term to promote dispersion among solutions, and a combination of both. We evaluate MISO across three distinct local optimization algorithms applied to separate robot control tasks: First-order Box Differential Dynamic Programming (DDP), which utilizes first-order linearization for the cart-pole swing-up task; Model Predictive Path Integral (MPPI) control, a sampling-based method, for the reacher task; and the Iterative Linear Quadratic Regulator (iLQR), a trajectory optimization algorithm, for an autonomous driving task. Our results show that MISO significantly outperforms existing initialization methods that rely on heuristics, learn to predict a single initial solution or use ensembles of independently learned models. In summary, our key contributions are as follows: 1. We present a novel framework for predicting multiple initial solutions for optimizers. 2. We introduce two distinct strategies for utilizing the predicted initial solutions: (i) single-optimizer, where the most promising solution is chosen based on a selection function, and (ii) multiple-optimizers, where multiple optimizers are initialized, potentially in parallel, with the best solution chosen afterward. 3. We design and implement specific training objectives to prevent mode collapse and ensure that the predicted solutions remain multimodal. 4. We apply our framework to three distinct sequential optimization tasks and perform extensive evaluation. Figure 1: As opposed to previous works that predict a single initial solution, MISO trains a single neural network to predict multiple initial solutions. We use them to either initialize a single optimizer or jointly initialize multiple optimizers."
https://arxiv.org/html/2411.02086v1,Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism,"Railway Turnout Machines (RTMs) are mission-critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. Due to frequent operations and exposure to harsh environments, RTMs are susceptible to failures and can potentially pose significant safety hazards. For safety assurance applications, especially in early-warning scenarios, RTM faults are expected to be detected as early as possible on a continuous 7x24 basis. However, limited emphasis has been placed on distributed model inference frameworks that can meet the inference latency and reliability requirements of such mission-critical fault diagnosis systems, as well as the adaptation of diagnosis models within distributed architectures. This has hindered the practical application of current AI-driven RTM monitoring solutions in industrial settings, where single points of failure can render the entire service unavailable due to standalone deployment, and inference time can exceed acceptable limits when dealing with complex models or high data volumes. In this paper, an edge-cloud collaborative early-warning system is proposed to enable real-time and downtime-tolerant fault diagnosis of RTMs, providing a new paradigm for the deployment of models in safety-critical scenarios. Firstly, a modular fault diagnosis model is designed specifically for distributed deployment, which utilizes a hierarchical architecture consisting of the prior knowledge module, subordinate classifiers, and a fusion layer for enhanced accuracy and parallelism. Then, a cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is developed to minimize the overhead resulting from distributed task execution and context exchange by strategically partitioning and offloading model components across cloud and edge. Additionally, an election consensus mechanism is implemented within CEC-PA to ensure system robustness during coordinator node downtime. Comparative experiments and ablation studies are conducted to validate the effectiveness of the proposed distributed fault diagnosis approach. Our ensemble-based fault diagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset collected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA demonstrates superior recovery proficiency during node disruptions and speed-up ranging from 1.98x to 7.93x in total inference time compared to its counterparts.","Railway transportation offers a high-capacity, cost-effective, and environmentally friendly solution for long-distance travel, making it a popular choice for passenger and freight services in Europe, Asia, and North America. According to M&M market research [1], the global railway system was valued at $25.1 billion in 2022 and is estimated to reach $30.9 billion by 2027. The Railway Turnout Machines (RTMs), also known as the Railway Point Machines (RPMs), are critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. However, RTMs are prone to failures due to wearing caused by frequent operations and exposure to harsh outdoor environments. Statistical analysis reveals RTMs as one of railside equipment that experience the highest failure rates, accounting for 18% of all documented railway system failures occurring between 2011 and 2017 [2]. The malfunction of RTMs can lead to catastrophic accidents such as collisions and train derailments, resulting in severe casualties and property losses. This typically involves the concept of preventive maintenance [3], which calls for regularly scheduled inspections and repairs targeting at the prevention of failures before they occur. For a long time, such condition-based maintenance mainly depends on the expert knowledge and experience of railway workers and thus can be time-consuming and labor-intensive. Therefore, an unsupervised, resilient, and responsive RTM fault early-warning system for train drivers and maintenance groups has raised lots of concern in the industry. With the advent of information technology, Railside Monitoring Units (RMUs) are deployed to collect runtime data during the operation of RTMs. Numerous fault diagnosis methods have been developed utilizing the collected data on vibration [4], current [5, 6, 7], torque and acoustic signals [8], etc. Previous endeavors have been primarily dedicated to enhancing model accuracy, while paying little attention to the performance and reliability issues caused by inappropriate deployment methods [9]. For safety assurance applications, especially in early-warning scenarios, we expect faults to be detected as early as possible to provide drivers and maintenance groups with more response time. The high computational overhead and complex procedures of these fault diagnosis models can make real-time inference challenging on resource-constrained devices such as Personal Computers (PCs). The traditional standalone deployment [10], where all the model components are deployed on a single device or platform, is also susceptible to system-wide unavailability in case of any software or hardware malfunctions on that centralized node [11]. Cloud computing has then become a common approach to wide range of fault diagnostic applications in Industry 4.0 [12], micro-electromechanical systems (MEMS) [13], Cloud Native [14], etc. However, the data gathered must be sent to the cloud to harness the high-performance and elastic advantages of cloud computing. In addition to privacy concerns [15] stemming from the sensitive nature of sensor data (e.g., route schedules and geographical locations), the transmission of data in railway environments like underground tunnels, inevitably leads to data loss and network latency issues [16]. These factors significantly impair the real-time capabilities of cloud-based solutions and hinder their effectiveness in monitoring mission-critical infrastructure [17]. In the past decade, academic interest has grown in combining edge computing with fault detection for model deployment, also known as Edge Intelligence (EI) [18]. This novel approach shifts computation from centralized cloud servers to the network edge, offering latency [19], energy consumption [20], Quality of Service (QoS) [21] and mobility [22] enhanced solutions. Federated Learning (FL) [23] has emerged as a potent approach for preserving privacy during model training, which enable each distributed client to train a local replica of the global model with its own dataset before sending updates to aggregate the shared global model. However, limited emphasis has been placed on distributed model inference frameworks that can meet the latency and reliability requirements of the fault diagnosis model deployment, or on tailoring the diagnosis models to perform optimally within distributed architectures. The inherent complementarity of cloud and edge computing has fostered the concept of cloud-edge collaboration [24], a paradigm that dynamically allocates and coordinates computational tasks across cloud and edge. This collaborative approach has inspired new paradigms for AI-driven real-time and downtime-tolerant monitoring tasks in mission-critical industrial applications [25] , where such systems benefit from the high availability characteristic of modern cloud computing infrastructure and the low-latency capabilities afforded by edge computing deployments. Therefore, a RTM fault diagnosis model optimized for distributed deployment, coupled with its edge-cloud collaboration empowered model inference framework is proposed in this paper, where model components are strategically partitioned and offloaded jointly across cloud and edge rather than relying solely on cloud or local to facilitate reliability and faster response. The main contributions of this paper can be summarized as: â€¢ A parallel-optimized RTM fault diagnosis model is developed with model integration technique. The model incorporates an enhanced three-stage segmentation scheme as prior knowledge and the outputs of multiple sub-classifiers are fused by a fuzzy-based ensemble mechanism to form the final classification result. â€¢ A cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is proposed to address the real-time and robustness challenges of distributed fault diagnosis. CEC-PA partitions the integrated model components into pipelines and intelligently schedules them across all worker nodes. Additionally, a downtime-tolerant mechanism is proposed to ensure system robustness. â€¢ Extensive experiments are conduced to evaluate the effectiveness of the proposed fault detection model and CEC-PA framework. Results showcase our ensemble-based fault diagnosis model produce accurate predictions across all fault types and CEC-PA outperform other approaches in terms of real-time performance and reliability. The rest of this paper is organized as follows: Section II discusses previous works on parallelization techniques in distributed AI. Section III presents the preliminary discussion on the working principle and current pattern analysis of three-stage turnouts. Section IV establishes the time consumption model and multi-objective optimization problem of the proposed cloud-edge RTM fault early-warning system. Section V implements the parallel-optimized turnout fault diagnosis scheme and provides a detailed description of the interactions between each module. Section VI presents the design details of CEC-PA. Section VII demonstrates the effectiveness of the fault diagnosis model and CEC-PA through comparative experiments. Finally, Section VIII draws a conclusion of this paper and highlights its future research directions. TABLE I: Comparison of Different Parallelization Strategies Key Characteristics Data Parallelization Model Parallelization Pipeline Parallelization Applicable scenarios Large datasets with smaller models Extremely large models Long pipelines Proof of convergence âœ“âœ“\checkmarkâœ“ Ã—\timesÃ— âœ“âœ“\checkmarkâœ“ Heterogeneous cluster support âœ“âœ“\checkmarkâœ“ Ã—\timesÃ— âœ“âœ“\checkmarkâœ“ Load balance Ã—\timesÃ— âœ“âœ“\checkmarkâœ“ âœ“âœ“\checkmarkâœ“ Communication overhead High Low Moderate Implementation difficulty Low High Moderate Scalability High Moderate High"
https://arxiv.org/html/2411.01826v1,An online optimization algorithm for tracking a linearly varying optimal point with zero steady-state error,"In this paper, we develop an online optimization algorithm for solving a class of nonconvex optimization problems with a linearly varying optimal point. The global convergence of the algorithm is guaranteed using the circle criterion for the class of functions whose gradient is bounded within a sector. Also, we show that the corresponding LurÃ©-type nonlinear system involves a double integrator, which demonstrates its ability to track a linearly varying optimal point with zero steady-state error. The algorithm is applied to solving a time-of-arrival based localization problem with constant velocity and the results show that the algorithm is able to estimate the source location with zero steady-state error.","I INTRODUCTION Online optimization [1] has emerged as a pivotal area of research with significant implications across various fields, including real-time control systems, signal processing, and machine learning. An online optimization problem typically involves making a sequence of decisions in real-time, where the objective function changes over time. The primary challenge lies in solving an optimization problem with an objective function that shifts as time progresses, while ensuring that the time-varying solution converges with minimal steady-state error, thereby maintaining high accuracy and stability over time. Recently, the design of algorithms to solve such problems has been investigated in [2], employing tools from robust control theory, especially in solving convex problems. The paper [2] designed control-based online optimization algorithms based on the internal model principle, which states that for a control system to eliminate steady-state error, the controller must incorporate a model of corresponding disturbances and input signals [3]. This improves the accuracy and performance of the solutions. In this paper, we consider a special case of the problems considered in [2] in which the optimal point of the objective function varies linearly with time. Then, as opposed to the approach of [2], we design an algorithm which minimizes the root convergence rate [4] along with ensuring zero steady-state error; see also [5, 6, 7]. This leads to a simple closed form formula for the resulting algorithm. We consider an unconstrained optimization problem: xâˆ—â¢(t)=argâ¡minâ¡fâ¢(x,t),superscriptğ‘¥ğ‘¡ğ‘“ğ‘¥ğ‘¡x^{*}(t)=\arg\min f(x,t),italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) = roman_arg roman_min italic_f ( italic_x , italic_t ) , (1) where f:â„n+1Ã—n+1â†’â„:ğ‘“â†’superscriptâ„ğ‘›1ğ‘›1â„f:\mathbb{R}^{n+1\times n+1}\rightarrow\mathbb{R}italic_f : blackboard_R start_POSTSUPERSCRIPT italic_n + 1 Ã— italic_n + 1 end_POSTSUPERSCRIPT â†’ blackboard_R is the cost function that attains its minimum at xâˆ—â¢(t)superscriptğ‘¥ğ‘¡x^{*}(t)italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ). In the case which fâ¢(x,t)ğ‘“ğ‘¥ğ‘¡f(x,t)italic_f ( italic_x , italic_t ) is independent of tğ‘¡titalic_t, [5] proposes an approach to this problem through an algorithm which we generalize to the following form: xâ¢(t+1)ğ‘¥ğ‘¡1\displaystyle x(t+1)italic_x ( italic_t + 1 ) =xâ¢(t)âˆ’Î±â¢âˆ‡xfâ¢(xâ¢(t),t)+Î³â¢âˆ‡xfâ¢(xâ¢(tâˆ’1),t)absentğ‘¥ğ‘¡ğ›¼subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡ğ‘¡ğ›¾subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡1ğ‘¡\displaystyle=x(t)-\alpha\nabla_{x}f(x(t),t)+\gamma\nabla_{x}f(x(t-1),t)= italic_x ( italic_t ) - italic_Î± âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x ( italic_t ) , italic_t ) + italic_Î³ âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x ( italic_t - 1 ) , italic_t ) +Î²â¢(xâ¢(t)âˆ’xâ¢(tâˆ’1)),ğ›½ğ‘¥ğ‘¡ğ‘¥ğ‘¡1\displaystyle\quad+\beta(x(t)-x(t-1)),+ italic_Î² ( italic_x ( italic_t ) - italic_x ( italic_t - 1 ) ) , (2) where xâ¢(t)âˆˆâ„nğ‘¥ğ‘¡superscriptâ„ğ‘›x(t)\in\mathbb{R}^{n}italic_x ( italic_t ) âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is the current estimate, tğ‘¡titalic_t is the iteration index, Î±ğ›¼\alphaitalic_Î± is the step size, Î²ğ›½\betaitalic_Î² is a momentum parameter, Î³ğ›¾\gammaitalic_Î³ is an additional parameter, and âˆ‡xfâ¢(x,t)subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡\nabla_{x}f(x,t)âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x , italic_t ) is the gradient of the cost function. In order to obtain zero steady-state error with a linearly varying optimal point, we introduce an extra integration into the algorithm. This implies we restrict our attention to the special case of (I) where Î²=1ğ›½1\beta=1italic_Î² = 1. Then the recursion (I) becomes xâ¢(t+1)ğ‘¥ğ‘¡1\displaystyle x(t+1)italic_x ( italic_t + 1 ) =xâ¢(t)âˆ’Î±â¢âˆ‡xfâ¢(xâ¢(t),t)+Î³â¢âˆ‡xfâ¢(xâ¢(tâˆ’1),t)absentğ‘¥ğ‘¡ğ›¼subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡ğ‘¡ğ›¾subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡1ğ‘¡\displaystyle=x(t)-\alpha\nabla_{x}f(x(t),t)+\gamma\nabla_{x}f(x(t-1),t)= italic_x ( italic_t ) - italic_Î± âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x ( italic_t ) , italic_t ) + italic_Î³ âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x ( italic_t - 1 ) , italic_t ) +xâ¢(t)âˆ’xâ¢(tâˆ’1).ğ‘¥ğ‘¡ğ‘¥ğ‘¡1\displaystyle\quad+x(t)-x(t-1).+ italic_x ( italic_t ) - italic_x ( italic_t - 1 ) . (3) Also, we consider the class of possibly nonconvex cost functions as defined in [5]. Definition 1 Given Lâ‰¥m>0ğ¿ğ‘š0L\geq m>0italic_L â‰¥ italic_m > 0, let â„±m,Lsubscriptâ„±ğ‘šğ¿\mathscr{F}_{m,L}script_F start_POSTSUBSCRIPT italic_m , italic_L end_POSTSUBSCRIPT denote the class of cost functions fâ¢(x,t)ğ‘“ğ‘¥ğ‘¡f(x,t)italic_f ( italic_x , italic_t ) with the following property: The function fâ¢(x,t)ğ‘“ğ‘¥ğ‘¡f(x,t)italic_f ( italic_x , italic_t ) is twice differentiable such that fâ¢(x,t)=f~â¢(xâ¢(t)âˆ’xâˆ—â¢(0)âˆ’aâ¢t)ğ‘“ğ‘¥ğ‘¡~ğ‘“ğ‘¥ğ‘¡superscriptğ‘¥0ğ‘ğ‘¡f(x,t)=\tilde{f}\left(x(t)-x^{*}(0)-at\right)italic_f ( italic_x , italic_t ) = over~ start_ARG italic_f end_ARG ( italic_x ( italic_t ) - italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( 0 ) - italic_a italic_t ) and for every xâ¢(t)âˆˆâ„nğ‘¥ğ‘¡superscriptâ„ğ‘›x(t)\in\mathbb{R}^{n}italic_x ( italic_t ) âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, (mâ¢(xâ¢(t)âˆ’xâˆ—â¢(t))âˆ’âˆ‡xfâ¢(xâ¢(t),t))Tsuperscriptğ‘šğ‘¥ğ‘¡superscriptğ‘¥ğ‘¡subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡ğ‘¡ğ‘‡\displaystyle\left(m\left(x(t)-x^{*}(t)\right)-\nabla_{x}f(x(t),t)\right)^{T}( italic_m ( italic_x ( italic_t ) - italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ) - âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x ( italic_t ) , italic_t ) ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT Ã—(Lâ¢(xâ¢(t)âˆ’xâˆ—â¢(t))âˆ’âˆ‡xfâ¢(xâ¢(t),t))absentğ¿ğ‘¥ğ‘¡superscriptğ‘¥ğ‘¡subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡ğ‘¡\displaystyle\times\left(L\left(x(t)-x^{*}(t)\right)-\nabla_{x}f(x(t),t)\right)Ã— ( italic_L ( italic_x ( italic_t ) - italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) ) - âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x ( italic_t ) , italic_t ) ) â‰¤0,for alltâ‰¥0.formulae-sequenceabsent0for allğ‘¡0\displaystyle\leq 0,\quad\text{for all}\quad t\geq 0.â‰¤ 0 , for all italic_t â‰¥ 0 . (4) Here, xâˆ—â¢(t)=xâˆ—â¢(0)+aâ¢tsuperscriptğ‘¥ğ‘¡superscriptğ‘¥0ğ‘ğ‘¡x^{*}(t)=x^{*}(0)+atitalic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) = italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( 0 ) + italic_a italic_t is the linearly varying optimal point of fâ¢(x,t)ğ‘“ğ‘¥ğ‘¡f(x,t)italic_f ( italic_x , italic_t ), where ağ‘aitalic_a is a constant vector. Note that for twice differentiable functions, (1) implies that mâ¢Iâ‰¤âˆ‡x2fâ¢(xâˆ—â¢(t),t)â‰¤Lâ¢I,for alltâ‰¥0.formulae-sequenceğ‘šğ¼superscriptsubscriptâˆ‡ğ‘¥2ğ‘“superscriptğ‘¥ğ‘¡ğ‘¡ğ¿ğ¼for allğ‘¡0mI\leq\nabla_{x}^{2}f(x^{*}(t),t)\leq LI,\quad\text{for all}\quad t\geq 0.italic_m italic_I â‰¤ âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f ( italic_x start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT ( italic_t ) , italic_t ) â‰¤ italic_L italic_I , for all italic_t â‰¥ 0 . (5) The inequality (1) is a multivariate analogue of a sector bound on the gradient of a scalar function, where the gradient âˆ‡xfâ¢(x,t)subscriptâˆ‡ğ‘¥ğ‘“ğ‘¥ğ‘¡\nabla_{x}f(x,t)âˆ‡ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_f ( italic_x , italic_t ) belongs to the sector [m,L]ğ‘šğ¿[m,L][ italic_m , italic_L ] [8]. In this paper, we develop an online optimization algorithm of the form (I) to solve a class of optimization problems with cost functions fâ¢(x,t)âˆˆâ„±m,Lğ‘“ğ‘¥ğ‘¡subscriptâ„±ğ‘šğ¿f(x,t)\in\mathscr{F}_{m,L}italic_f ( italic_x , italic_t ) âˆˆ script_F start_POSTSUBSCRIPT italic_m , italic_L end_POSTSUBSCRIPT. We write the algorithm (I) as a LurÃ©-type nonlinear system where the corresponding transfer function incorporates a double integrator. The system is recognized as a Type 2 system, in which the system will have zero steady-state error in response to a ramp input [9, Table 4-4, p.200]. Moreover, the absolute stability of the corresponding system is established by employing the circle criterion, which guarantees the global convergence of the algorithm. We specifically consider using the root convergence factor [4, p. 288] as a measure of the rate of convergence. We also apply the algorithm to a time-of-arrival (TOA) based localization problem, demonstrating that the algorithm is able to estimate the linearly varying source location with zero steady-state error. This paper is organized as follows: Section II presents background material and preliminary results. In Section III, we describe and prove our main result. Section IV illustrates our algorithm with an example of a TOA based localization problem. Finally, the paper is concluded in Section V."
https://arxiv.org/html/2411.01428v1,Distributionally Robust Resource Allocation with Trust-aided Parametric Information Fusion,"Reference information plays an essential role for making decisions under uncertainty, yet may vary across multiple data sources. In this paper, we study resource allocation in stochastic dynamic environments, where we perform information fusion based on trust of different data sources, to design an ambiguity set for attaining distributionally robust resource allocation solutions. We dynamically update the trust parameter to simulate the decision makerâ€™s trust change based on losses caused by mis-specified reference information. We show an equivalent tractable linear programming reformulation of the distributionally robust optimization model and demonstrate the performance in a wildfire suppression application, where we use drone and satellite data to estimate the needs of resources in different regions. We demonstrate how our methods can improve trust and decision accuracy. The computational time grows linearly in the number of data sources and problem sizes.","In the realms of stochastic optimization and system control, decision-making challenges involve information uncertainty, which often stems from limited data and imprecise measurements. Over the last decade, distributionally robust optimization (DRO) has been widely used for attaining the best worst-case performance under ambiguously known uncertain parameters [1, 2, 3]. In broader resource allocation problems, [4] formulates a multi-period DRO model to dynamically optimize pre-positioning of emergency supplies under demand uncertainty; [5] uses DRO to optimize resource allocation in disaster relief; [6] compares a stochastic programming (SP) approach with DRO for epidemic control resource allocation under stochastic spatiotemporal demand, and shows that the DRO approach can limit the number of unvaccinated or untested people by paying higher cost. Building on the emergence of DRO models, in this paper, we consider stochastic resource allocation with unknown demand whose distribution can be inferred from multiple data sources via data fusion [7]. The existing techniques for fusing data can be classified into two main categories: probability-based methods [8, 9, 10] and Artificial Intelligence (AI)-based methods [11, 12, 13, 14], but not all are adaptable for characterizing the ambiguity set in DRO models. In our approach, we use trust [15] as weights to fuse information from different sources, and its dynamic nature [16, 17] allows us to update trust on different sources over time. Our work complements the existing studies in stochastic resource allocation using DRO with trust being the ambiguously known uncertain parameter and we take multi-source reference information into consideration. The main contributions of the paper are three-fold: â€¢ We develop a multi-reference distributionally robust optimization (MR-DRO) model for stochastic resource allocation. We combine predicted information from multiple sources to construct the ambiguity set. â€¢ We design a trust update process to simulate trust variation over time once more data becomes available. We utilize historical data and outcomes to select proper trust that adapts to the relative prediction errors. â€¢ We show, via extensive computational results, that the MR-DRO model yields better results than the ones obtained by fully trusting a single source. The remainder of the paper is organized as follows. In Section II, we formulate the MR-DRO model by constructing the trust-aided ambiguity set and derive its tractable reformulation. In Section III, we propose a trust update process to simulate trust change over time. Section IV includes detailed numerical experiments and result analysis. In Section V, we conclude the paper and state future research directions."
https://arxiv.org/html/2411.01425v1,Learning Hidden Subgoals under Temporal Ordering Constraints in Reinforcement Learning,"In real-world applications, the success of completing a task is often determined by multiple key steps which are distant in time steps and have to be achieved in a fixed time order. For example, the key steps listed on the cooking recipe should be achieved one-by-one in the right time order. These key steps can be regarded as subgoals of the task and their time orderings are described as temporal ordering constraints. However, in many real-world problems, subgoals or key states are often hidden in the state space and their temporal ordering constraints are also unknown, which make it challenging for previous RL algorithms to solve this kind of tasks. In order to address this issue, in this work we propose a novel RL algorithm for learning hidden subgoals under temporal ordering constraints (LSTOC). We propose a new contrastive learning objective which can effectively learn hidden subgoals (key states) and their temporal orderings at the same time, based on first-occupancy representation and temporal geometric sampling. In addition, we propose a sample-efficient learning strategy to discover subgoals one-by-one following their temporal order constraints by building a subgoal tree to represent discovered subgoals and their temporal ordering relationships. Specifically, this tree can be used to improve the sample efficiency of trajectory collection, fasten the task solving and generalize to unseen tasks. The LSTOC framework is evaluated on several environments with image-based observations, showing its significant improvement over baseline methods. 111Source codes will be released upon acceptance","In real life, successfully completing a task often involves multiple temporally extended key steps, where these key steps have to be achieved in specified time orders. For instance, in the process of making chemicals, different operations have to be strictly performed in the right time order, e.g., sulfuric acid must be added after water. Otherwise, the right chemical reaction can never occur or even the safety will be threatened. These key steps are necessary for the success of completing the given task and skipping any of them or doing them in the wrong time order will lead to failure of the task. In this work, these key steps are regarded as subgoals. Tasks consisting of multiple subgoals with temporal ordering constraints are common in many real-world applications, such as the temporal logic tasks in control systems and robotics (Baier and Katoen 2008). Since these tasks may have long-time horizon and sparse reward, the knowledge of subgoals and their temporal orderings are necessary for modern RL algorithms to solve these tasks efficiently. However, these subgoals can be hidden and unknown in many real-world scenarios. For instance, due to the userâ€™s lack of knowledge, these subgoals may be missing when specifying the task. Alternatively, due to the partial observability of environment, the agent does not know subgoals and their temporal orderings in advance. Motivating example. For example, consider a service robot tasked to collect the diamond in limited time steps, as shown in Figure 1(a). Due to the limited power and the blockage of river, the agent has to first go to the charger to get charged, then pick up wheel or board to go across the river, and finally get the diamond. If the agent first picks up the wheel or board and then goes to the charger, the task cannot be finished in the required time steps. The temporal dependencies of these subgoals can be described by the finite state machine (FSM) in Figure 1(b). The temporal logic language for describing these dependencies is c;(bâˆ¨\veeâˆ¨w);d. However, since the agent can only observe things around him, it is not aware of the river and does not know that charger, wheel and board are subgoals, i.e., subgoals are hidden to the agent. (a) Layout c;(bâˆ¨w);dcbwğ‘‘\text{c};(\text{b}\vee\text{w});dc ; ( b âˆ¨ w ) ; italic_d b w d v0subscriptğ‘£0v_{0}italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT vTsubscriptğ‘£ğ‘‡v_{T}italic_v start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT c (b) FSM Figure 1: (a) Example task. (b) The FSM for temporal dependencies of subgoals. Letters â€câ€, â€bâ€, â€wâ€ and â€dâ€ are short for charger, board, wheel and diamond, respectively. When solving tasks with hidden subgoals, the binary label at the end of the episode, indicating the task is accomplished successfully or not, is the only reward information for the agent to leverage to solve the task. This existing situation can be challenging for modern RL algorithms which use Bellman equation to propagate value estimates back to the earlier key steps (Sutton and Barto 2018). These algorithms suffer from slow convergence and expensive learning complexity, which are going to be verified by empirical experiments. Therefore, it is necessary to develop new RL algorithms to solve the task which contains multiple hidden subgoals with unknown temporal ordering constraints. To the best knowledge, this is the first work which investigates this problem. In this work, we propose a novel framework for Learning hidden Subgoals under Temporal Ordering Constraints in RL (LSTOC). It consists of the learning subgoal and the labeling components. In learning subgoals, the proposed framework efficiently discovers states or observations corresponding to hidden subgoals and learns their temporal dependencies by using contrastive learning, which iteratively builds a subgoal tree (denoted as ğ’¯ğ’¯\mathcal{T}caligraphic_T and defined in Section 3.2) by discovered subgoals to represent learned temporal ordering constraints. Specifically, in ğ’¯ğ’¯\mathcal{T}caligraphic_T, nodes are labeled with discovered key states of subgoals and edges represent their temporal ordering relationships. This tree ğ’¯ğ’¯\mathcal{T}caligraphic_T is used to guide the trajectory collection, ground the semantic meaning of learned subgoals (labeling component), accelerate the task solving and help the generalization. Subgoal Learning. In order to improve the sample efficiency, we propose a new learning method which discovers hidden subgoals one-by-one and builds ğ’¯ğ’¯\mathcal{T}caligraphic_T iteratively representing discovered subgoals and their temporal ordering relationships. The trajectory collection is guided by ğ’¯ğ’¯\mathcal{T}caligraphic_T to focus more on the working node vwsubscriptğ‘£ğ‘¤v_{w}italic_v start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT which is a node in ğ’¯ğ’¯\mathcal{T}caligraphic_T not fully explored yet. In every iteration of building ğ’¯ğ’¯\mathcal{T}caligraphic_T, by using a novel contrastive learning method, the agent only discovers the next subgoal which is next to the subgoal of working node vwsubscriptğ‘£ğ‘¤v_{w}italic_v start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT under temporal ordering constraints, and expands ğ’¯ğ’¯\mathcal{T}caligraphic_T by adding this newly discovered subgoal as a new child to vwsubscriptğ‘£ğ‘¤v_{w}italic_v start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT. Then, this new child node will be used as working node, initiating the next iteration of tree expansion. This iterating process will stop whenever the success of every collected trajectory about task completion can be explained by the constructed ğ’¯ğ’¯\mathcal{T}caligraphic_T, meaning that ğ’¯ğ’¯\mathcal{T}caligraphic_T is fully constructed, i.e., all the hidden subgoals and temporal orderings have been learned in ğ’¯ğ’¯\mathcal{T}caligraphic_T. Contrastive Learning. In order to discovery subgoals next to working node vwsubscriptğ‘£ğ‘¤v_{w}italic_v start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, we propose a new contrastive learning method. In this case, since only the first visit to the next subgoal is meaningful, based on pre-processed trajectories, the proposed method first computes the first-occupancy representation (FR) (Moskovitz, Wilson, and Sahani 2021) of trajectories by removing repetitive states. Then, we will use contrastive learning to detect subgoals. However, since conventional contrastive learning could detect multiple subgoals without giving any temporal distance information and the next subgoal is temporally closest to vwsubscriptğ‘£ğ‘¤v_{w}italic_v start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT among detected ones, the real next subgoal we need to discover could be missed by using conventional methods. Therefore, it is necessary to learn the temporal distances (i.e., temporal orderings) of detected subgoals and then select the temporally closest one as next subgoal. Therefore, we propose a new contrastive learning objective which can detect key states for subgoals and learn their temporal orderings at the same time. It formulates the contrastive learning objective by using temporal geometric sampling to sample one positive state from the FR of a processed positive trajectory and several negative states from the FR of a batch of processed negative trajectories. To be best of our knowledge, we are the first to propose a contrastive learning method which can detects key states and learn their temporal distances at the same time. Labeling. In the labeling part, if the specification representing the temporal dependencies of subgoal semantic symbols is given, we formulate an integer linear programming (ILP) problem to determine the mapping from the discovered key states of subgoals to subgoal semantic symbols, making every path of the constructed subgoal tree ğ’¯ğ’¯\mathcal{T}caligraphic_T satisfy the given specification. When this ILP problem is solved, the labeling function is obtained, essentially giving semantic meaning to every learned subgoal. We evaluate LSTOC on 9999 tasks in three environments, including Letter, Office, and Crafter domains. In these environments, with image-based observations, the agent needs to visit different objects under temporal ordering constraints. Our evaluations show that LSTOC can outperform baselines on learning subgoals and efficiency of solving given tasks. The generalizability of LSTOC is also empirically verified. The limitation of LSTOC is discussed finally."
https://arxiv.org/html/2411.01416v1,Sequential Charging Station Location Optimization under Uncertain Charging Behavior and User Growth,"Charging station availability is crucial for a thriving electric vehicle market. Due to budget constraints, locating these stations usually proceeds in phases, which calls for careful consideration of the (random) charging demand growth throughout the planning horizon. This paper integrates user choice behavior into two-stage and multi-stage stochastic programming models for intracity charging station planning under demand uncertainty. We derive a second-order conic representation for the nonlinear, nonconvex formulation by taking advantage of the binary nature of location variables and propose subgradient inequalities to accelerate computation. Numerical results demonstrate the value of employing multi-stage models, particularly in scenarios of high demand fluctuations, increased demand dispersion, and high user sensitivity to the distance-to-recharge.","I INTRODUCTION A transition from conventional fuel vehicles to electric vehicles (EVs) is underway to meet carbon emission targets [1]. Although policy efforts and market trends are supporting a promising outlook, wider EV adoption is still hindered by insufficient availability of charging facilities [2]. Thus, strategic location planning is essential for supporting an expanding EV market. Given the high cost of charger procurement and power grid expansion [3], the planning is typically long-term and sequential. In each phase of time, only a small batch of stations is constructed to achieve a balance between market coverage and budget limitations of the investors. In sequential charging station location, it is imperative to not only consider the current demand but also prepare for future charging needs. In a dynamic and evolving market, future EV charging demand grows in a stochastic manner [1]. Although there have been extensive analyses on multi-year planning of intracity stations, research incorporating stochastic demand growth remains limited. Most studies are built on deterministic scenarios where future demand is known [4, 5] or calculated using external data such as surveys [6] and historical charging records [7]. Existing stochastic extensions either consider daily power load fluctuations rather than long-term user growth [8], or only employ two-stage models without extending to a multi-stage decision framework [3]. Furthermore, the aforementioned studies rarely address the user choice among different stations or the competition between home charging and public charging options. To fill in the aforementioned gaps, we depict the choice of EV users patronizing charging stations using a multinomial logit (MNL) model. Then, we integrate MNL into a multi-stage stochastic program considering random demand growth and sequential location of charging stations. This yields a fractional integer programming formulation, which is nonlinear and nonconvex. Nevertheless, we derive a second-order conic representation by taking advantage of the binary nature of the location variables. As compared to the state-of-the-art, this drastically reduces the formulation size and accelerates its computation. Furthermore, we conduct numerical experiments based on real EV demand data to demonstrate the value of our model and the effectiveness of our solution approach. We summarize the main contributions as follows. 1. We propose the first multi-stage stochastic programming models for sequentially locating charging stations, incorporating random demand growth and user choice behavior. 2. We derive a second-order conic representation, accompanied by subgradient inequalities, to tackle the ensuing fractional integer programming formulation. 3. We conduct extensive numerical experiments to demonstrate the value of considering a multi-stage model as opposed to a two-stage one, as well as the effectiveness of the proposed solution approach. The remainder of this paper is organized as follows. Section II reviews the relevant literature, Section III describes our models, and Section IV presents the solution approach. Experiment results are reported in Section V and conclusions are drawn in Section VI."
https://arxiv.org/html/2411.01321v1,Control Strategies for Pursuit-Evasion Under Occlusion Using Visibility and Safety Barrier Functions,"This paper develops a control strategy for pursuit-evasion problems in environments with occlusions. We address the challenge of a mobile pursuer keeping a mobile evader within its field of view (FoV) despite line-of-sight obstructions. The signed distance function (SDF) of the FoV is used to formulate visibility as a control barrier function (CBF) constraint on the pursuerâ€™s control inputs. Similarly, obstacle avoidance is formulated as a CBF constraint based on the SDF of the obstacle set. While the visibility and safety CBFs are Lipschitz continuous, they are not differentiable everywhere, necessitating the use of generalized gradients. To achieve non-myopic pursuit, we generate reference control trajectories leading to evader visibility using a sampling-based kinodynamic planner. The pursuer then tracks this reference via convex optimization under the CBF constraints. We validate our approach in CARLA simulations and real-world robot experiments, demonstrating successful visibility maintenance using only onboard sensing, even under severe occlusions and dynamic evader movements.","I INTRODUCTION Pursuit-evasion problems [1] are studied in computational geometry, control theory, and robotics, motivated by applications in search and rescue [2], security and surveillance [3], and environmental monitoring [4]. Introducing visibility constraints leads to the art gallery problem [5], which has elegant solutions with static pursuers (guards) in 2D polygonal environments but becomes challenging with mobile pursuers in 3D environments. Prior works tackled pursuit-evasion with visibility constraints by graph-theoretic [6] or game-theoretic approaches [7, 8, 9]. This work proposes a novel control design for pursuit-evasion with field-of-view (FoV) constraints, using control barrier function (CBF) techniques. Originating from the pioneering works of Wieland and AllgÃ¶wer [10] and Ames et al. [11, 12], CBFs have become a key tool for safety-critical applications such as adaptive cruise control [13], robot manipulation [14], robot locomotion [15], and robot flight [16]. CBFs are defined so that the positivity of a barrier function implies forward invariance of a corresponding superlevel set for the trajectories of a dynamical system. A key observation is that, for control-affine dynamical systems, CBF constraints are linear in the control input. This allows control synthesis via quadratic programming (QP) subject to linear CBF constraints [12]. Figure 1: A mobile pursuer aims to keep a mobile evader within its field of view despite occlusions and without colliding with obstacles. The inset shows the evader in the pursuerâ€™s camera view. A challenge for CBF methods is to determine a function whose positivity is equivalent to the desired safe behavior. Previous works have demonstrated ways to validate candidate CBFs for polynomial control systems [17] subject to multiple state and input constraints [18], or even to learn CBFs from expert demonstrations [19]. A key observation in this paper is that visibility under occlusion can also be formulated as a CBF condition using the signed distance function (SDF) of the sensor FoV. An SDF provides the (signed) distance from any point in space to a given setâ€™s boundary. SDFs have been used to model shapes and surfaces in vision and robotics tasks such as object shape reconstruction [20] and robot mapping [21]. In this paper, we introduce a novel visibility CBF constraint that enforces negativity of the SDF from an evader to the occluded FoV of the pursuer. While early works on CBFs primarily focused on continuous systems subject to differentiable CBF constraints, recent advances [22, 23, 18] have studied the regularity of CBF-based controllers and have expanded their applicability to non-smooth dynamics and non-smooth constraints. These method have been used in [24] to enable safe multi-robot navigation with polytope-shaped robots. In our case, the visibility CBF is Lipschitz continuous but not differentiable everywhere. It is also time-varying due to the pursuerâ€™s and evaderâ€™s motion. Hence, it is necessary to use a non-smooth time-varying formulation of the visibility CBF constraint involving its generalized gradient. Maintaining visibility is a requirement in various target tracking problems. A visibility-aware planner proposed in [25] generates sensor trajectories with maximal visibility to a moving target. A vision-based controller for a quadrotor landing on a ground vehicle, while preventing camera occlusions, was developed in [26]. Leader-follower formations for maintaining visibility and safety were developed using dipolar vector fields in a known environment in [27]. Similarly, in the presence of FoV constraints, dipolar vector fields have been used in [28] to formulate a model predictive control scheme for differential-drive robot navigation, maintaining visibility of a static landmark. Closely related to our work, Gao et al. [29] formulate a probabilistic notion of visibility under occlusion, which is maintained using an extended Kalman filter [30]. The authors develop a real-time non-myopic trajectory planner for visibility-aware and safe target tracking in the presence of uncertainty. This work is complementary to ours in that it focuses on high-level non-myopic visibility planning using nonlinear optimization, while we focus on low-level myopic visibility control to track a planned reference using convex programming. The contributions of our work are summarized as follows. â€¢ We prove Lipschitz continuity of the signed distance to an occluded FoV to justify its use as a non-smooth time-varying CBF for visibility maintenance. â€¢ We achieve non-myopic visibility maintenance by coupling a kinodynamic planner that generates reference controls leading to evader visibility, with a controller that tracks the planned reference using convex optimization subject to visibility and safety CBF constraints. â€¢ We demonstrate successful visibility maintenance in real robot experiments using only onboard sensing, even under severe occlusions and dynamic evader motion."
https://arxiv.org/html/2411.01271v1,"Interacting Large Language Model Agents.
Interpretable Models and Social Learning.","This paper develops theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making by interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and stochastic control algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors.This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under two settings: (a) centrally controlled LLMAs and (b) autonomous LLMAs with incentives. Throughout the paper, we numerically demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like Llama and Mistral and closed-source models like ChatGPT. The main takeaway of this paper, based on substantial empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting. Traditionally, such models are used in economics to study interacting human decision-makers.","This paper develops theory and algorithms for interacting Large Language Model Agents (LLMAs) by leveraging techniques from signal processing and microeconomics. Specifically, we focus on developing interpretable models and stochastic control algorithms for LLMAs, enabling them to interact sequentially for Bayesian inference. We construct interpretable models of LLMAs at two levels of abstraction, as outlined in Figure 1. First, we model an individual LLMA as a rationally inattentive Bayesian utility maximizer, capturing the agentâ€™s decision-making process under limited attention. Second, we extend this approach to a sequence of LLMAs engaging in Bayesian social learning, where each agent acts as a Bayesian utility maximizer. Our models are inspired by the self-attention mechanism in large language models (LLMs) and observed challenges, such as model collapse, that can arise during LLM training. Rationally Inattentive Bayesian Utility Maximizer LLM AgentLLM Agent nText observation Action (Estimate) â‹¯â‹¯\displaystyle\cdotsâ‹¯Abstraction 1Abstraction 2LLM Agent 1Text observation Action (Estimate) Sequence of LLM Agents perform Bayesian social learningLLM Agents are rationally inattentive Bayesian utility maximizers Figure 1: Summary of the proposed contributions: We discuss the different black-box models for LLMA and how LLMAs can be used as a sensing mechanism to perform Bayesian inference. Part 1 models the LLMAs as a rationally inattentive Bayesian utility maximizer and numerically establishes the behavior in applications of product quality identification and hate speech classification. Part 2 discusses how Bayesian social learning in a sequence of LLMAs can be used for sequential state estimation. However, in Part 3, we show that the agents can perform the same incorrect action due to herding. We then discuss a stochastic control approach to delay herding when LLMAs are centrally controlled and when they are autonomous but are incentivized. Furthermore, motivated by the observed bias in the behavior of interacting LLMAs, we demonstrate that a sequence of LLMAs engaging in Bayesian social learning converge to identical estimates, or â€herdâ€. To address this phenomenon, we propose a stochastic control approach, formulating an optimal stopping problem to balance the trade-off between privacy and herding, to detect the failure state. Our approach is designed for two scenarios: (a) when the LLMAs are centrally controlled, and (b) when they operate autonomously. Our goal is to demonstrate that concepts from controlled sensing and microeconomics, traditionally applied to human decision-making, can be used to both understand and synthesize the behavior of interacting LLMAs [12, 7, 13, 39, 40]. We support our theoretical findings with numerical experiments using advanced LLMs for Bayesian inference on real-world data. This paper is crafted to engage a broad readership, highlighting applications of Bayesian agents in diverse fields, including financial news analysis, e-commerce review evaluation, and online toxicity detection. These examples underscore the flexibility of our methodologies for cross-disciplinary applications. The reproducible code for our experiments is publicly accessible at github.com/aditj/sociallearningllm. I-A Motivation LLM agents (LLMAs) are being rapidly deployed for different applications and to quote Sam Altman, CEO of OpenAI (creators of ChatGPT, a popular LLM which has 200 million weekly active users): â€œ2025 is when (AI) agents will workâ€. LLMAs use a large language model (LLM) to parse the input and have additional agency to perform tasks. LLMs (such as ChatGPT and Llama) are neural networks with billions of parameters trained on trillions of tokens of textual data to parse long texts for summarizing, compiling key facts, and generating new text. The key technical improvement that leads to the efficient deployment of LLMs is the transformer architecture [75]. The effectiveness of LLMs on textual texts has made their deployment and adoption widespread [54]. Many applications have been proposed in healthcare, online platform moderation, and finance, where these LLMs are used to parse the textual observations and suggest decisions based on their outputs [48]. LLM Bayesian Engine LLMA as a Sensing MechanismLLMARational Inattention Cost Bayesian UtilityLLMALLMALLMALLMA Bayesian Social Learning and Stochastic Control LLMA Engineer Interpretable & Robust Bayesian Inference Text Observation State Estimate Reconstructed Interpretable Model using Bayesian Revealed Preferences Figure 2: Engineering with large language model agents: We propose engineer with LLMs on three different levels: a) First, we propose constructing an LLMA with an LLM attached to the Bayesian engine. The LLM acts as a sensor for the text input and outputs interpretable low-dimensional outputs, which are used by the Bayesian engine to produce a state estimate. b) We formulate the necessary and sufficient conditions for a LLMAs to be a rationally inattentive Bayesian utility maximizer (RIBUM). We also present algorithms to reconstruct feasible utilities and rational inattention costs if the LLMA is indeed a RIBUM, attributing the LLMA with an interpretable microeconomic model. c) We show how a sequence of LLMAs can efficiently perform sequential Bayesian social learning by controlling their outputs to delay herding optimally. Our Bayesian social learning models can be extended to study Bayesian social learning in a network of LLMAs. In many tasks, the outputs of the LLMs are often part of a more extensive pipeline; for example, the output of the LLMs, either in a specified format or as embeddings, is frequently used as inputs to other Bayesian entities, including classifiers [55]. The Bayesian framework also becomes essential in applications where the LLMs have to output decisions and need to provide confidence in the decision output. Thus, it is of interest to study a single Bayesian agent that uses the LLM to parse text observations, update its Bayesian belief, and take action. This paper studies such entities and refers to them as Large Language Models Agents (LLMAs). Constructing interpretable models for LLMAs is crucial to understanding and controlling their interaction. I-A1 Interacting large language model agents It is predicted that by 2025, 90% of web content will be generated by large language models (LLMs) [1]. In recent practical implementations, individual LLMs are part of a bigger system, referred to as LLMAs, and interact with the content generated by other LLMAs and the external environment [86]. Furthermore, recent research has shown how generative models are trained on the data generated by other generative models can collapse [67]. Therefore naturally, LLMAs interact with each other either implicitly or explicitly. Hence, controlling the dynamics of interacting LLMAs is essential to improve the accuracy and trustworthiness of decisions by LLMAs. To the best of our knowledge, only a few recent works systematically study the behaviors of LLMAs using tools from microeconomics and signal processing [30]. This study aims to bridge this gap by systematically reviewing LLMAs and the different mathematical frameworks by studying Bayesian social learning in a sequence of LLMAs to achieve Bayesian inference. I-A2 Interpretable Engineering of LLMAs Many different third-party services have already started providing various kinds of LLMAs as a service, including Agentforce by Salesforce and IBM AI agents [66]. The underlying intelligence engine of these third-party agents is an LLM or a vision language model (VLM). The LLMAs are used in personal applications for coding, shopping, and scraping data and in enterprise applications for getting insights on user activity and automating industrial workflows. Therefore, it becomes imperative to study interpretable models for these agents since many of the proposed applications these agents involve sensitive information (like personal records, financial information, bio-medical data, and personal preferences). By interpretable, we refer to models that facilitate a transparent understanding of complex models through clear and explainable representations of their decision-making processes. The workflows of the AI agents also include making decisions, and the interpretability and reliability of these agents become vital for them to be trustworthy. Therefore, mathematical models are needed to aid in engineering and deploying LLMAs. To this end, we propose a LLMA composed of an LLM and a Bayesian engine, which by construction is interpretable. Further, we use Bayesian revealed preferences 111The framework of Bayesian revealed preferences is also referred to as inverse optimization or inverse reinforcement learning. to reconstruct a Bayesian utility function for both our constructed LLMA and for off-the-shelf LLMAs. I-A3 Bayesian Inference from Multi-Modal Data Stream In various applications, like online e-commerce platforms, video streaming platforms, and social networks, there is a rich stream of multimodal data available using text, images, and videos. Different inference tasks involve fusing information from various data streams to get actionable insights. With the recent progress in deep learning, many of the traditional signal processing methods are being replaced with contemporary methods that use LLMs and VLMs. However, just using static models is not sufficient to model the dynamics of real-life settings, e.g. on online platforms, and underlying dynamics are better modeled in a Bayesian framework. Therefore, motivated by practical applications, we propose the construction of LLMAs which can perform Bayesian inference sequentially on a data stream. This complements continual learning, which deals with continually learning new tasks without forgetting what was learned previously [78]. I-B Main Results Sec. 3: LLM Agent as a SensorSec. 4: LLM Agent as a Rationally Inattentive Bayesian Utility Maximizer (RIBUM)Part 1Indiviual LLM Agent Part 2Network of LLM Agents Sec. 5: Sequential Bayesian Social LearningSec. 6: Word-of-MouthBayesian Social LearningSec. 6: AsynchronousBayesian Social LearningPart 3Stochastic Control for delaying Herding in LLM Agents Sec. 7: Optimal Stopping for Centrally Controlled LLM AgentsSec. 8: Optimal Stopping for Incentivized Autonomous LLMAsSec. 9: Stochastic Approximation for Optimal Policy Sec. 10: Numerical Results on Bayesian inference Product Quality and Hate Speech Peddler Identification Figure 3: Organization of the paper: The paper is divided into three parts. Part 1 deals with interpretable models for an individual LLM agent. Part 2 extends the models to a social learning setting where LLM agents interact with each other to perform Bayesian inference. Part 3 proposes stochastic control methods to delay herding in a sequence of LLM agents. This paper builds on tools from Bayesian revealed preferences from microeconomics (inverse reinforcement learning), sequential Bayesian estimation (from signal processing), and structured stochastic control (from control theory) to construct interpretable models and synthesize interaction of LLMAs. The impact of our results on more efficient, systematic, and interpretable engineering of LLMAs is summarized in Figure 2. The main contributions of this paper are: 1. We propose constructing a LLMA as a composition of a large language model (LLM) sensor, which acts a low-dimensional map from the text space and a Bayesian engine, which uses the measurement from the LLM to update the posterior and act optimally. We show how this model is useful for interpretable Bayesian inference with applications in sequential data on online platforms. 2. To obtain an interpretable utility function for a LLMA, we provide necessary and sufficient conditions in Theorem 1 for a LLMA to be a rationally inattentive Bayesian utility maximizer (RIBUM). For a LLMA who is a RIBUM, we propose Algorithm 2 and Algorithm 3 to reconstruct the max-margin and sparsest utility estimate, respectively. Our methods are applicable both our LLMA and off-the-shelf LLMAs. 3. We study Bayesian social learning in a LLMAs, sequentially estimating a state given text observations and in Theorem 2 show that such a sequence of LLMAs form an information cascade and herd in their actions. We show that this is true for both when no private observations are shared and when a finite number of private observations are shared. Further, we provide a detailed analysis of the effect of the quality of results from LLM of the LLMA and the number of private observations. 4. To delay herding in a sequence of LLMAs, we formulate an optimal stopping problem for two regimes: a) when the LLMAs are centrally controlled by an entity b) when the LLMAs are autonomous but are incentivizes by an entity. We show in Theorem 3 and Theorem 4 that under certain assumptions on the observation matrix and cost functions, the optimal policy for the partially observed Markov decision process of both the optimal stopping problems has a threshold structure. We then propose a stochastic approximation algorithm in Algorithm 7, which exploits the structural results to estimate the optimal policy parameters. The algorithm does not need access to the system parameters, is computationally efficient, and can track changes in the system. 5. We finally present several numerical experiments to demonstrate the efficacy of our proposed methods. We show how our constructed LLMA can be used for interpretable Bayesian inference for analyzing financial data. We show how the Bayesian revealed preferences framework can estimate the utility of an off-the-shelf LLM when used for hate-speech detection. Finally we show numerical studies on two examples of sequential Bayesian inference: hate speech peddler identification and product quality analysis, to demonstrate herding of LLMAs, and applicability of our structural results. To summarize, this paper attempts to answer the following questions with respect to interacting LLM Agents, 1. How can LLMAs be constructed so that they can be used for sequential Bayesian inference such that the observation and outputs are interpretable? 2. What is a principled approach to analyze whether a LLMA is a Bayesian utility maximizer and also reconstructs its utility function given only black-box access? 3. How does one systematically study Bayesian social learning in multiple interacting LLMAs to explain observed behaviours such as herding and model collapse? 4. How can herding in (centrally controlled or autonomous) LLMAs be optimally delayed so that the agents optimally switch between preserving privacy and improving estimation to achieve sequential detection? I-C Organization This paper is organized into three parts, and the schematic of the organization is given in Figure 3. Part I discusses interpretable model for a single LLMA and attempt at answering questions 1 and 2 above. Section II discusses the related work in large language models, agents using LLMs, and current interpretable models for Bayesian inference. Section III discusses the mathematical model used for modeling LLMAs in this paper and motivates the different components involved. Section IV gives the necessary and sufficient conditions for the LLMAs to be rationally inattentive Bayesian utility maximizers (RIBUM). It furthers proposes algorithms to estimate the utility function for a LLMA which is a RIBUM. Part II discusses interpretable models for interacting LLMAs and attempts to answer question 3. Section V discusses the mathematical framework of Bayesian social learning in LLMAs and proves that a sequence of LLMAs form an information cascade in finite time. Section VIII discusses a stochastic control problem for the optimal stopping time problem to achieve quickest time herding with minimal loss to the privacy of LLMAs. Section VI discusses interpretable models to explain model collapse and data incest in LLMAs using word-of-mouth and asynchronous social learning. To decrease the bias when a sequence of LLMAs perform Bayesian inference, Part III deals with stochastic control for delaying herding in interacting LLMAs performing Bayesian sequential learning proves structural results, and proposes a stochastic approximation approach. Section IX considers the problem of a central controller optimally optimizing a sequence of autonomous LLMAs to achieve the state estimation by optimally controlling herding. Section X proposes a stochastic approximation based approach to approximate the optimal policy, which has a threshold switching curve. Numerical results on real-life text classification tasks and related applications are discussed in Section XI. Section XII concludes the paper with discussions on future works, open problems, and research opportunities. The appendix contains the proofs and details about the numerical experiments. For the ease of the reader, we have included a motivation and a discussion subsection in each section, which grounds the different aspects of LLMAs to a real-life application and different microeconomics and statistical signal processing tools presented in the section. We also provide different block diagrams and illustrative examples to further aid the reader."
https://arxiv.org/html/2411.01000v1,Enhancing Model-Based Step Adaptation for Push Recovery through Reinforcement Learning of Step Timing and Region,"This paper introduces a new approach to enhance the robustness of humanoid walking under strong perturbations, such as substantial pushes. Effective recovery from external disturbances requires bipedal robots to dynamically adjust their stepping strategies, including footstep positions and timing. Unlike most advanced walking controllers that restrict footstep locations to a predefined convex region, substantially limiting recoverable disturbances, our method leverages reinforcement learning to dynamically adjust the permissible footstep region, expanding it to a larger, effectively non-convex area and allowing cross-over stepping, which is crucial for counteracting large lateral pushes. Additionally, our method adapts footstep timing in real time to further extend the range of recoverable disturbances. Based on these adjustments, feasible footstep positions and DCM trajectory are planned by solving a QP. Finally, we employ a DCM controller and an inverse dynamics whole-body control framework to ensure the robot effectively follows the trajectory.","I INTRODUCTION In humanoid robot locomotion, Divergent Component of Motion (DCM) trajectory generation [1] has become a widely used method for generating walking motions. Current research is evolving from focusing on motion generation to addressing the challenges of robustness and adaptability in the face of external disturbances. Model-based push recovery approaches often involve analytical calculation of footstep adjustments [2, 3] or optimization-based methods [4] to adjust timing and position of footsteps [5, 6]. Data-driven approaches have also been applied to generate push recovery strategies. Recent works [7, 8] used end-to-end Reinforcement Learning (RL) to acquire a variety of push recovery and balancing behaviors, such as ankle, hip, and stepping strategies, which are similar to those seen in humans [9]. While these methods adapt well to various disturbances, they often require extensive training data and significant computational resources. Model-based approaches typically use distinct control components, such as a low-level whole-body controller and a high-level trajectory planner to precisely follow pre-planned motions [10]. In contrast, data-driven approaches often employ end-to-end learning of joint positions [11, 12, 13, 14]. These methods tend to generalize better to unknown environments due to extensive domain randomization. For instance, [15] showcased robust blind walking on stairs using terrain randomization techniques. Moreover, [16] demonstrates navigation over difficult stepping-stone patterns effectively using vision input. Nevertheless, the success of end-to-end learning approaches largely depends on the precise design of the reward function. Figure 1: Simulation of the robot kangaroo during a lateral push force of 800N for 0.1 seconds. Our method allows the robot to quickly recover from such a large disturbance through a leg cross-over and simultaneous adjustment in step timing. Hybrid approaches that combine model-based and data-driven methods have emerged as promising solutions, aiming to take advantage of both [17, 18]. Duan et al. [19] propose integrating robot system knowledge into reinforcement learning to train bipedal locomotion policies directly in task space, improving sample efficiency and demonstrating the approach in simulation and on the real robot Cassie. Castillo et al. [20] achieved a lightweight network structure and sample efficiency through an RL framework, where actions parameterize desired joint trajectories rather than direct actuator inputs. In [21], the same authors propose a hierarchical approach that uses RL at the high level to train policies for task space commands and employs a model-based low-level controller to track these trajectories. Both policies demonstrate robust performance against various disturbance forces applied to the torso. From a model-based perspective, we can effectively find optimal solutions for previewed footstep placements using a QP approach. However, some drawbacks remain. The step time appears nonlinear in the solution of the DCM dynamics. Thus, iterative solutions or a general nonlinear optimization are required. Additionally, using a QP restricts possible footstep regions to convex shapes, thereby reducing the solution space and excluding options such as cross-over stepping. Recent works have addressed this problem by decomposing the non-convex regions into convex subregions by a set of rules [22] or by evaluating the feasibility of the resulting QP problem [23]. The contribution of this work lies in enhancing model-based trajectory planning by integrating RL to address the limitations of the QP approach. This is achieved by using an RL agent to dynamically adjust key parameters within the model-based control framework. The selected parameters are the step frequency, single support percentage, and rotation angle of a convex step area around the current stance foot. Thanks to the combination of RL with the model-based framework, our method significantly improves the maximal recoverable external disturbances, enhancing the robustness of the robotâ€™s walking. Additionally, the division of tasks between RL and model-based controllers improves the learning efficiency of RL, enabling the training of a single environment in just a few hours."
https://arxiv.org/html/2411.00889v1,MESS+: Energy-Optimal Inferencing in Language Model Zoos with Service Level Guarantees,"Open-weight large language model (LLM) zoos allow users to quickly integrate state-of-the-art models into systems. Despite increasing availability, selecting the most appropriate model for a given task still largely relies on public benchmark leaderboards and educated guesses. This can be unsatisfactory for both inference service providers and end users, where the providers usually prioritize cost efficiency, while the end users usually prioritize model output quality for their inference requests. In commercial settings, these two priorities are often brought together in Service Level Agreements (SLA). We present MESS+, an online stochastic optimization algorithm for energy-optimal model selection from a model zoo, which works on a per-inference-request basis. For a given SLA that requires high accuracy, we are up to 2.5Ã—2.5\times2.5 Ã— more energy efficient with MESS+ than with randomly selecting an LLM from the zoo while maintaining SLA quality constraints.","As the number of open-weight large language models (LLMs), such as Llama [3], Mistral/Mixtral [8], and Granite [6], is increasing rapidly, deep learning infrastructure providers and end users are confronted with an abundance of models (model zoo) for their language modeling tasks. This leaves many users questioning what model is best to choose and whether highly regarded benchmark results apply to their specific problem [13]. Currently, the best way to approach model selection is educated guessing. Since working with LLMs can be expensive [17], minimizing costs is an equally high priority for end users and inference endpoint operators. This leaves us with a tri-fold problem: End-users primarily care about a correct model output. When inquiring about text information, e.g., by asking questions or requesting language translation, end users are mostly interested in obtaining factually correct and sufficiently clear language output [20]. Additionally, many users are unfamiliar with the technical details of LLMs, making it challenging for them to select the right model for the job, i.e., their primary references are domain-specific benchmark rankings [4, 5]. However, there is no intuitive method to compare the complexity of individual requests with benchmark tasks. Thus, we require an automatic method to select the most appropriate LLM for any given request. Inference endpoint providers prioritize low operating costs. Operating infrastructure that can run state-of-the-art LLMs can be costly. Microsoft has announced it will acquire a stake in the Three Mile Island nuclear power plant in the United States to satisfy the energy demand of its planned data center in Pennsylvania, which has two reasons: consistent energy delivery and low energy cost [16]. In times of globally rising energy costs, this underpins the necessity of energy-optimal service operations. Currently, inference service providers only allow their users to query specific models on serverless endpoints or to deploy individual models on dedicated hardware. To further optimize operating costs and improve user experience, we require a method that can choose the best model for any given request while minimizing energy consumption. Enterprise use-cases require a consistently high-quality model inference output while keeping costs in check. Enterprise users unite the requirement for high-quality model outputs and price sensitivity. Thus, commercial players typically rely on service-level agreements (SLAs) when sourcing services for their own products. This creates a legal basis for holding the model operator responsible for delivering high quality and performance. Such SLAs typically come with various levels where, in the case of model outputs, the primary quality metric is accuracy. Thus, we require a method for formalizing and quantifying the SLA requirements. In summary, we ask the question: Can we select appropriate models from the model zoo to ensure energy efficiency while satisfying SLAs? In this paper, we address this question by using a stochastic optimization [14] framework to develop an optimal control algorithm, which enables end users to query an inference service and automatically select the most appropriate Model with Energy-optimal Service-level GuaranteeS (MESS+). Our work is related to two major research directions: dynamic inference and inference request scheduling. A broad overview is provided in [24]. Dynamic inference. Typically, dynamic inference approaches involve early exit strategies within a single model and require changes to the original model architecture. These changes are usually additional layers that decide whether a request continues to subsequent layers or is evicted [12, 22]. While these approaches can save inference costs, they require additional training of the decision layers. Further, increasing the capabilities of dynamic inference models is difficult as it requires changing the model architecture and re-training. MESS+ removes the need for altering the architecture of readily available pre-trained models and automatically selects the most appropriate model with regard to a given SLA for each request. Inference request scheduling. To this end, scheduling work primarily focuses on reducing latency during an inference call. Here, the main idea is to group [10], arrange [7], or early evict [21] inference requests for faster processing. These approaches focus exclusively on scenarios with a single fixed model for all incoming requests and a priority for latency. In contrast, MESS+ is a decision-making method to route requests for energy-optimal processing while maintaining a minimum accuracy over time. As such, our approach contributes to establishing minimum quality guarantees but does not consider latency directly. However, there is a relationship between latency and energy efficiency since smaller models are typically faster to execute an inference call [17]. Since MESS+ routes inference requests to different models, it can build on top of existing scheduling techniques. Taken together, we enable dynamic inference across readily available pre-trained LLMs with service level guarantees, while offering compatibility with existing inference load scheduling techniques. MESS+ can reduce the energy consumption of a given task by up to 4.6Ã—4.6\times4.6 Ã— compared to a fixed selection of a single model from a model zoo."
https://arxiv.org/html/2411.00703v1,Set-Theoretic Direct Data-driven Predictive Control,"Designing the terminal ingredients of direct data-driven predictive control presents challenges due to its reliance on an implicit, non-minimal input-output data-driven representation. By considering the class of constrained LTI systems with unknown time delays, we propose a set-theoretic direct data-driven predictive controller that does not require a terminal cost to provide closed-loop guarantees. In particular, first, starting from input/output data series, we propose a sample-based method to build N-step input output backward reachable sets. Then, we leverage the constructed family of backward reachable sets to derive a data-driven control law. The proposed method guarantees finite-time convergence and recursive feasibility, independent of objective function tuning. It requires neither explicit state estimation nor an explicit prediction model, relying solely on input-output measurements; therefore, unmodeled dynamics can be avoided. Finally, a numerical example highlights the effectiveness of the proposed method in stabilizing the system, whereas direct data-driven predictive control without terminal ingredients fails under the same conditions.","Terminal costs and constraints, often termed terminal ingredients, have been proposed in Model Predictive Controllers (MPC) to approximate the gap between finite-time and infinite-time predictions, thereby ensuring closed-loop guarantees such as stability and recursive feasibility. In the absence of terminal ingredients, the stability of Data-Driven Predictive Control (DDPC), similar to MPC, depends on the prediction horizon and tuning of the objective function, see [1, Ch. 12, E.g. 12.2]. On the other hand, introducing conservative choice of terminal ingredients, such as the equilibrium point, may significantly decrease the Region of Attraction (RoA). Designing such ingredients for DDPC differs from MPC because only input-output measurements and a Hankel-based matrix representation are available. Current solutions for the terminal constraint are limited to the equilibrium point of the system [2], artificial set points [3], or require the systemâ€™s lag to be known [4]. For a comprehensive discussion on this issue, refer to [5, Sec 3.1], which highlights that designing the terminal ingredients in this framework is still an open question. In response to these limitations, we leverage reachability analysis as a systematic approach for designing terminal ingredients. A prevalent approach for designing and analyzing constraint control systems is reachability analysis, which systematically explores all potential solutions to prevent constraint violations. Forward Reachable Sets (FRS) and Backward Reachable Sets (BRS) serve as fundamental tools in this context, enabling the calculation of states that can be reached from specified initial conditions or directed towards a target set over finite (or infinite) time. Several methodologies currently exist for computing these sets, including sampling-based methods, simulation-based techniques, set propagation, and Hamilton-Jacobi analysis, as detailed in [6, 7, 8, 9]. Each methodology presents trade-offs between computational complexity and the accuracy of the resulting set representations. Most reachability analysis techniques rely on pre-specified state-space models, necessitating state estimation or full state measurement, which can pose challenges for practical implementation when often output measurements are available. Requiring input-state data and exact knowledge of the systemâ€™s order, recursive matrix zonotopes are proposed in [10] to calculate data-driven forward reachable sets. This approach allows for the utilization of a set of models instead of relying on a potentially inaccurate single model, effectively capturing the true dynamics of the system. Similarly, matrix zonotopes are employed in [11] to calculate data-driven backward reachable sets. Using the data-driven over-approximated FRS and under-approximated BRS, two predictive controllers are introduced in [12] and [11]. Note that any model assumptions made during the identification process, including the systemâ€™s order, may result in model mismatch, even in LTI systems [13]. In contrast, the input-output data-driven framework introduced by J. C. Willems in [14] remains unaffected by unmodeled dynamics and unstructured uncertainty, as it defines the system using input-output data without relying on explicit representations. Limited research has focused on reachability analysis when only input-output data is available and the number of states is unknown. In [15], input-output safe sets are computed for iterative tasks, which require optimizing performance for a specific initial condition and objective function. In [16], two online and offline methods are proposed to safely expand the input-output safe set of a short-sighted safety filter. Additionally, [17] calculates maximal admissible sets for a constant input to design a data-driven reference governor. To the best of our knowledge, no method presently exists for calculating N-step Input-Output Backward Reachable Sets (N-IOBRS) from an implicit input-output data-driven representation, nor for employing these sets within a direct data-driven predictive control framework. In this paper, first, we estimate N-IOBRS using a data-driven safety filter. Next, we utilize the resulting family of nested N-IOBRS to develop a direct data-driven predictive controller. The proposed method extends the state-feedback set-theoretic controller [11] into the input-output framework while maintaining finite-time convergence and recursive feasibility properties. In contrast to [11], our method employs input-output multi-step prediction and attains a large region of attraction without requiring the exact system order, full state measurements, and an explicit system representation. It is important to emphasize that the proposed method systematically addresses input delays by over-approximating the systemâ€™s lag, as described in [16]. To extend the proposed method to noisy measurements and input disturbances settings, the N-IOBRS must be appropriately tightened based on the noise and disturbance levels, which is beyond the scope of this paper. Figure 1 illustrates a flowchart depicting the overall process of the proposed method. Figure 1: Flowchart of the overall process: (A) a single experiment generating an input-output dataset; (B) the sample-based method for computing N-IOBRS; and (C) the ST-DDPC developed from the sample-based N-IOBRS. The remainder of the paper is organized as follows: Section II explains the preliminary materials and the problem statement. Section III presents the sample-based method to compute N-IOBRS. Section IV provides the proposed controller, along with a proof of recursive feasibility and convergence. Numerical results and discussion are presented in Section V. Finally, Section VI provides concluding remarks."
https://arxiv.org/html/2411.00668v1,Model Predictive Contouring Control with Barrier and Lyapunov Functions for Stable Path-Following in UAV systems,"In this study, we propose a novel method that integrates Nonlinear Model Predictive Contour Control (NMPCC) with an Exponentially Stabilizing Control Lyapunov Function (ES-CLF) and Exponential Higher-Order Control Barrier Functions to achieve stable path-following and obstacle avoidance in UAV systems. This framework enables unmanned aerial vehicles (UAVs) to safely navigate around both static and dynamic obstacles while strictly adhering to desired paths. The quaternion-based formulation ensures precise orientation and attitude control, while a robust optimization solver enforces the constraints imposed by the Control Lyapunov Function (CLF) and Control Barrier Functions (CBF), ensuring reliable real-time performance. The method was validated in a Model-in-the-Loop (MiL) environment, demonstrating effective path tracking and obstacle avoidance. The results highlight the frameworkâ€™s ability to minimize both orthogonal and tangential errors, ensuring stability and safety in complex environments.","The increasing complexity of modern aerial systems has driven substantial advancements in control methodologies capable of managing the non-linear dynamics and safety-critical constraints of Unmanned Aerial Vehicles (UAVs) [1]. UAVs are required to operate in dynamic and uncertain environments where they must not only follow predefined paths but also respond adaptively to the presence of obstacles and interactions with other vehicles [2]. These scenarios place significant demands on control systems, which require simultaneous optimization of trajectory accuracy, safety, and real-time adaptability [3]. NMPCC has emerged as a promising control framework to address these challenges [4, 5, 6]. Unlike traditional non-linear model predictive control (NMPC), which focuses on minimizing time-indexed tracking errors, NMPCC emphasizes minimizing contouring and lag errors relative to a desired path [7]. This path-following optimization provides greater flexibility for UAVs to adapt their trajectories in response to environmental changes, such as the sudden appearance of obstacles, while still maintaining overall mission objectives [8]. This adaptability makes NMPCC particularly well suited for complex tasks that require real-time responsiveness and high levels of autonomy [9]. The integration of NMPCC in UAV systems introduces additional challenges when operating in complex airspaces, requiring avoidance of both static and dynamic obstacles [10]. Ensuring safe, collision-free navigation under these conditions requires advanced control strategies that can dynamically adjust trajectories while maintaining the required stability and safety standards [11]. Traditional control methods, though effective in simpler environments, often lack the robustness and flexibility to handle the complexity of multi-UAV systems in real-world applications [12, 13, 14]. To address these limitations, we propose integrating NMPCC with CLF and CBF frameworks. CLFs offer a systematic numerical approach to ensuring system stability by enforcing the decrease of a Lyapunov function over time [15, 16, 17, 18]. On the other hand, CBFs enforce safety constraints, ensuring that UAVs operate within safe boundaries and avoid collisions [19, 20]. The combination of these control frameworks within the NMPCC structure results in a comprehensive approach that addresses both performance and safety, enabling UAVs to navigate complex and dynamic environments autonomously. In addition to path-following and safety considerations, three-dimensional orientation is crucial for UAV control, particularly in dynamic environments where precise maneuvers are required [21]. To address this, we incorporate quaternion-based formulations into the control design. Quaternions offer several advantages over traditional Euler angles, such as avoiding gimbal lock and ensuring smooth, continuous rotation [22]. By leveraging the mathematical properties of quaternions, particularly their ability to map to the tangent space using the logarithmic map (Log) [23, 24, 25], we can effectively manage the rotational dynamics of UAVs, enhancing the robustness and accuracy of the overall control system. This quaternion-based approach is rooted in Lie theory and manifold principles, providing a mathematically rigorous foundation for handling the rotational behavior of UAVs. Implementing this integrated framework, which combines NMPCC, CLFs, CBFs, and quaternion-based formulations, introduces considerable computational challenges, especially for real-time applications. To mitigate these challenges, we utilize ACADOS [26], an open-source software package optimized for solving optimal control problems with high computational efficiency, alongside CasADi [27], a symbolic framework for automatic differentiation and numerical optimization. These frameworks are particularly well-suited for UAV control tasks that demand rapid and precise decision-making, offering flexibility that enables seamless integration with the proposed control strategy. For the numerical integration of rotational dynamics, a fourth-order Runge-Kutta method is applied to ensure precise state updates. In this study, we validate the proposed control strategy through extensive Model-in-the-Loop (MiL) simulations. These simulations demonstrate the practicality and robustness of the approach in complex environments. The results highlight the benefits of integrating NMPCC with advanced control techniques, offering a unified solution for safe and reliable UAV navigation in challenging operational settings. In summary, the contributions of this paper are threefold: â€¢ We present a novel integration of NMPCC with Control ES-CLF and higher-order CBF, providing a unified approach to dynamic obstacle avoidance in environments with multiple obstacles. â€¢ We demonstrate the effectiveness of quaternion-based formulations, using the Log operator to map to the tangent space according to Lie theory and manifolds, improving the robustness and precision of UAV attitude control by ensuring smooth transitions and accurate orientation representation. I-A Outline This paper is structured as follows. Section II introduces the dynamic model of the UAV system, providing the foundation for the control strategy. Section III presents the CLF used to ensure stability in the proposed framework. Section IV focuses on the CBF, which enforce safety constraints in the system. Section V explores the NMPCC approach, detailing its application to path-following and obstacle avoidance. Section VI provides an in-depth analysis of the experiments and results, demonstrating the performance of the proposed control strategy. Finally, Section VII concludes the paper, summarizing the key findings and outlining potential directions for future research."
https://arxiv.org/html/2411.00656v1,Identification of Analytic Nonlinear Dynamical Systems with Non-asymptotic Guarantees,"This paper focuses on the system identification of an important class of nonlinear systems: linearly parameterized nonlinear systems, which enjoys wide applications in robotics and other mechanical systems. We consider two system identification methods: least-squares estimation (LSE), which is a point estimation method; and set-membership estimation (SME), which estimates an uncertainty set that contains the true parameters. We provide non-asymptotic convergence rates for LSE and SME under i.i.d. control inputs and control policies with i.i.d. random perturbations, both of which are considered as non-active-exploration inputs. Compared with the counter-example based on piecewise-affine systems in the literature, the success of non-active exploration in our setting relies on a key assumption on the system dynamics: we require the system functions to be real-analytic. Our results, together with the piecewise-affine counter-example, reveal the importance of differentiability in nonlinear system identification through non-active exploration. Lastly, we numerically compare our theoretical bounds with the empirical performance of LSE and SME on a pendulum example and a quadrotor example.","Learning control-dynamical systems with statistical methodology has received significant attention in the past decade (Sarker et al., 2023; Li et al., 2023b; Chen and Hazan, 2021; Simchowitz and Foster, 2020; Wagenmaker and Jamieson, 2020; Simchowitz et al., 2018; Dean et al., 2018; Abbasi-Yadkori and SzepesvÃ¡ri, 2011; Li et al., 2021b). In particular, the estimation of linear dynamical systems, e.g. xt+1=Aâˆ—â¢xt+Bâˆ—â¢ut+wtsubscriptğ‘¥ğ‘¡1superscriptğ´subscriptğ‘¥ğ‘¡superscriptğµsubscriptğ‘¢ğ‘¡subscriptğ‘¤ğ‘¡x_{t+1}=A^{*}x_{t}+B^{*}u_{t}+w_{t}italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_A start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_B start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, is relatively well-studied: it has been shown that non-active exploration by i.i.d. noises on control inputs utsubscriptğ‘¢ğ‘¡u_{t}italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and system disturbances wtsubscriptğ‘¤ğ‘¡w_{t}italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are already enough for accurate system identification, and least square estimation (LSE) can achieve the optimal estimation convergence rate (Simchowitz and Foster, 2020; Simchowitz et al., 2018). However, nonlinear control systems are ubiquitous in real-world applications, e.g. robotics (Siciliano et al., 2010; Alaimo et al., 2013), power systems (Simpson-Porco et al., 2016), transportation (Kong et al., 2015), etc. Motivated by this, there has been a lot of attention on learning nonlinear systems recently. One natural and popular direction to study nonlinear system identification is on learning linearly parameterized nonlinear systems as defined below, which is a straightforward extension from the standard linear systems (Mania et al., 2022; Khosravi, 2023; Foster et al., 2020) xt+1=Î¸âˆ—â¢Ï•â¢(xt,ut)+wtsubscriptğ‘¥ğ‘¡1subscriptğœƒitalic-Ï•subscriptğ‘¥ğ‘¡subscriptğ‘¢ğ‘¡subscriptğ‘¤ğ‘¡x_{t+1}=\theta_{*}\phi(x_{t},u_{t})+w_{t}italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_Î¸ start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT italic_Ï• ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT where Î¸âˆ—subscriptğœƒ\theta_{*}italic_Î¸ start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT is a vector of unknown parameters and Ï•â¢(xt,ut)italic-Ï•subscriptğ‘¥ğ‘¡subscriptğ‘¢ğ‘¡\phi(x_{t},u_{t})italic_Ï• ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is a known vector of nonlinear features. On the one hand, some classes of these systems are shown to enjoy similar benefits of linear systems. For example, bilinear systems can also be estimated by LSE under non-active exploration with i.i.d. noises (Sattar et al., 2022), as well as linear systems with randomly perturbed nonlinear policies (Li et al., 2023b). On the other hand, it is also known that non-active exploration is insufficient for general linearly parameterized nonlinear systems. In particular, (Mania et al., 2022) provides a counter example showing that non-active exploration is insufficient to learn accurate models under piece-wise affine feature functions. This motivates a sequence of follow-up work on the design of active exploration for nonlinear system estimation, which is largely motivated by the non-smooth feature functions such as ReLu in neural networks (Mania et al., 2022; Kowshik et al., 2021; Khosravi, 2023). However, there is a big gap between bilinear systems, which is infinitely differentiable, and the counter example by non-smooth systems. A natural question is: to what extent can non-active exploration still work for linearly parameterized nonlinear systems? Contributions. One major contribution of this paper is showing that LSE with non-active i.i.d. noises can efficiently learn any linearly parameterized nonlinear systems with real-analytic feature functions and provide a non-asymptotic convergence rate. Notice that real-analytic feature functions are common in physical systems. For example, polynomial systems satisfy this requirement and have wide applications in power systems (Simpson-Porco et al., 2016), fluid dynamics (Noack et al., 2003), etc. Further, trigonometric functions also satisfy the real-analytic property so a large range of robotics and mechanical systems also satisfy this requirement (Siciliano et al., 2010; Alaimo et al., 2013). A side product of our LSE convergence rate analysis is the convergence rate for another commonly used uncertainty quantification method in control: set membership estimation (SME). Numerically, we test our theoretical results in pendulum and quadrotor systems. Simulations show that LSE and SME can indeed efficiently explore the system and converge to the true parameter under non-active exploration noises. Technically, the key step in our proof is establishing the block-martingale-small-ball condition (BMSB) for general analytic feature functions, which greatly generalizes the bilinear feature function in Sattar et al. (2022). Our result is built on an intuition inspired by the counter example in (Mania et al., 2022): the counter example in (Mania et al., 2022) requires that some feature function is zero in a certain region, so nothing can be learned about its parameter if the states stay in this region. However, analytic functions cannot be a constant zero in a positive-measure region unless it is a constant zero everywhere. Therefore, the counter example does not work, and non-active exploration around any states can provide some useful information. Our proof formalizes this intuition by utilizing the Paley-Zygmund Petrov inequality (Petrov, 2007). Related work. Inspired by neural network parameterization, nonlinear systems of the form xt+1=Ï•â¢(Aâˆ—â¢xt)+wtsubscriptğ‘¥ğ‘¡1italic-Ï•subscriptğ´subscriptğ‘¥ğ‘¡subscriptğ‘¤ğ‘¡x_{t+1}=\phi(A_{*}x_{t})+w_{t}italic_x start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = italic_Ï• ( italic_A start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is also studied in the literature, where Ï•â¢(â‹…)italic-Ï•â‹…\phi(\cdot)italic_Ï• ( â‹… ) is a known nonlinear link function and Aâˆ—subscriptğ´A_{*}italic_A start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT is unknown. The least square cost is no longer quadratic or even convex in this case and various optimization methods have been proposed to learn this type of systems (Kowshik et al., 2021; Sattar et al., 2022; Foster et al., 2020). Another related line of research focuses on nonlinear regression with dependent data (Ziemann and Tu, 2022; Ziemann et al., 2023, 2024),111yt=fâˆ—â¢(xt)+wtsubscriptğ‘¦ğ‘¡subscriptğ‘“subscriptğ‘¥ğ‘¡subscriptğ‘¤ğ‘¡y_{t}=f_{*}(x_{t})+w_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT âˆ— end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is considered, where xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and ytsubscriptğ‘¦ğ‘¡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT correlate with the historical data. which can be applied to nonlinear system identification. The nonlinear regression in (Ziemann and Tu, 2022; Ziemann et al., 2023, 2024) is based on non-parametric LSE and its variants, and their convergence rates under different scenarios have been analyzed. It is interesting to note that this line of work usually assumes certain persistent excitation assumptions,222For example, (Ziemann and Tu, 2022) assumes hyper-contractivity, and (Ziemann et al., 2024) assumes the empirical covariance of the {xt}tâ‰¥0subscriptsubscriptğ‘¥ğ‘¡ğ‘¡0\{x_{t}\}_{t\geq 0}{ italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t â‰¥ 0 end_POSTSUBSCRIPT process is invertible with high probability (Corollary 3.2). whereas our paper demonstrates that persistent excitation holds by establishing the BMSB condition for linearly parameterized and real-analytic nonlinear control systems. Uncertainty set estimation is crucial for robust control under model uncertainties Lu and Cannon (2023); Lorenzen et al. (2019); Li et al. (2021a). SME is a widely adopted uncertainty set estimation method in robust adaptive control (Lorenzen et al., 2019; Lu and Cannon, 2023; Bertsekas, 1971; Bai et al., 1995). Recently, there is an emerging interest in analyzing SMEâ€™s convergence and convergence rate for dynamical systems (Li et al., 2024; Lu et al., 2019; Xu and Li, 2024), because previous analysis focus more on the linear regression problem (e.g. (AkÃ§ay, 2004; Bai et al., 1998)). There are also recent applications of SME to online control Yu et al. (2023), power systems Yeh et al. (2024), and computer vision Gao et al. (2024); Tang et al. (2024). Notation. The set of non-negative real numbers is denoted by â„â‰¥0subscriptâ„absent0\mathbb{R}_{\geq 0}blackboard_R start_POSTSUBSCRIPT â‰¥ 0 end_POSTSUBSCRIPT. The notation âŒˆâ‹…âŒ‰â‹…\lceil\cdot\rceilâŒˆ â‹… âŒ‰ stands for the ceiling function. For a real vector zâˆˆâ„nğ‘§superscriptâ„ğ‘›z\in\mathbb{R}^{n}italic_z âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, â€–zâ€–2subscriptnormğ‘§2\|z\|_{2}âˆ¥ italic_z âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT represents its â„“2subscriptâ„“2\ell_{2}roman_â„“ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT norm, â€–zâ€–âˆsubscriptnormğ‘§\|z\|_{\infty}âˆ¥ italic_z âˆ¥ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT represents its â„“âˆsubscriptâ„“\ell_{\infty}roman_â„“ start_POSTSUBSCRIPT âˆ end_POSTSUBSCRIPT norm, and zisuperscriptğ‘§ğ‘–z^{i}italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT represents its iğ‘–iitalic_i-th component with i=1â¢â‹¯â¢nğ‘–1â‹¯ğ‘›i=1\cdots nitalic_i = 1 â‹¯ italic_n. The set of real symmetric matrices is denoted by ğ•Šnsuperscriptğ•Šğ‘›\mathbb{S}^{n}blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. For a real matrix Zğ‘Zitalic_Z, ZâŠºsuperscriptğ‘âŠºZ^{\intercal}italic_Z start_POSTSUPERSCRIPT âŠº end_POSTSUPERSCRIPT represents its transpose, â€–Zâ€–2subscriptnormğ‘2\|Z\|_{2}âˆ¥ italic_Z âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT its maximum singular value, â€–Zâ€–Fsubscriptnormğ‘ğ¹\|Z\|_{F}âˆ¥ italic_Z âˆ¥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT its Frobenius norm, Ïƒminâ¢(Z)subscriptğœğ‘\sigma_{\min}(Z)italic_Ïƒ start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT ( italic_Z ) its minimum singular value, vecâ¢(Z)vecğ‘\hbox{vec}(Z)vec ( italic_Z ) its vectorization obtained by stacking its columns, and for a real square matrix Zğ‘Zitalic_Z, trâ¢(Z)trğ‘\hbox{tr}(Z)tr ( italic_Z ) represents its trace. For a real symmetric matrix Zğ‘Zitalic_Z, Zâ‰»0succeedsğ‘0Z\succ 0italic_Z â‰» 0 and Zâª°0succeeds-or-equalsğ‘0Z\succeq 0italic_Z âª° 0 indicate that Zğ‘Zitalic_Z is positive definite and positive semi-definite, respectively. For a measurable set â„°âŠ‚â„nâ„°superscriptâ„ğ‘›\mathcal{E}\subset\mathbb{R}^{n}caligraphic_E âŠ‚ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, Î»nâ¢(â„°)superscriptğœ†ğ‘›â„°\lambda^{n}(\mathcal{E})italic_Î» start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( caligraphic_E ) represents its Lebesgue measure in â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT and â„°csuperscriptâ„°ğ‘\mathcal{E}^{c}caligraphic_E start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT represents its complement in â„nsuperscriptâ„ğ‘›\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT. The notation âˆ…\emptysetâˆ… stands for the empty set. For a set ğ’¯ğ’¯\mathcal{T}caligraphic_T of matrices Î¸âˆˆâ„nÃ—mğœƒsuperscriptâ„ğ‘›ğ‘š\theta\in\mathbb{R}^{n\times m}italic_Î¸ âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_n Ã— italic_m end_POSTSUPERSCRIPT, diamâ¢(ğ’¯)diamğ’¯\hbox{diam}(\mathcal{T})diam ( caligraphic_T ) denotes its diameter and it is defined as diamâ¢(ğ’¯)=supÎ¸,Î¸â€²âˆˆğ’¯â€–Î¸âˆ’Î¸â€²â€–Fdiamğ’¯subscriptsupremumğœƒsuperscriptğœƒâ€²ğ’¯subscriptnormğœƒsuperscriptğœƒâ€²ğ¹\hbox{diam}(\mathcal{T})=\sup_{\theta,\theta^{\prime}\in\mathcal{T}}\|\theta-% \theta^{\prime}\|_{F}diam ( caligraphic_T ) = roman_sup start_POSTSUBSCRIPT italic_Î¸ , italic_Î¸ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆˆ caligraphic_T end_POSTSUBSCRIPT âˆ¥ italic_Î¸ - italic_Î¸ start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT âˆ¥ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT. For ziâˆˆâ„subscriptğ‘§ğ‘–â„z_{i}\in\mathbb{R}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆˆ blackboard_R with i=1,â‹¯,â„“ğ‘–1â‹¯â„“i=1,\cdots,\ellitalic_i = 1 , â‹¯ , roman_â„“, the notation diagâ¢(z1,â‹¯,zâ„“)diagsubscriptğ‘§1â‹¯subscriptğ‘§â„“\hbox{diag}(z_{1},\cdots,z_{\ell})diag ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , â‹¯ , italic_z start_POSTSUBSCRIPT roman_â„“ end_POSTSUBSCRIPT ) denotes a matrix in â„â„“Ã—â„“superscriptâ„â„“â„“\mathbb{R}^{\ell\times\ell}blackboard_R start_POSTSUPERSCRIPT roman_â„“ Ã— roman_â„“ end_POSTSUPERSCRIPT with diagonal entries of zisubscriptğ‘§ğ‘–z_{i}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. This paper uses truncated-Gaussianâ¢(0,Ïƒw,[âˆ’wmax,wmax])truncated-Gaussian0subscriptğœğ‘¤subscriptğ‘¤subscriptğ‘¤\texttt{truncated-Gaussian}(0,\sigma_{w},[-w_{\max},w_{\max}])truncated-Gaussian ( 0 , italic_Ïƒ start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT , [ - italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ] ) to refer to the truncated Gaussian distribution generated by Gaussian distribution with zero mean and Ïƒw2superscriptsubscriptğœğ‘¤2\sigma_{w}^{2}italic_Ïƒ start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT variance with truncated range [âˆ’wmax,wmax]subscriptğ‘¤subscriptğ‘¤[-w_{\max},w_{\max}][ - italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ]. The same applies to multi-variate truncated Gaussian distributions."
https://arxiv.org/html/2411.00579v1,Constraint-Driven Multi-USV Coverage Path Generation for Aquatic Environmental Monitoring,"In this article, we address aquatic environmental monitoring using a fleet of unmanned surface vehicles (USVs). Specifically, we develop an online path generator that provides either of circular or elliptic paths based on the real-time feedback so that the USVs efficiently sample the sensor data over given aquatic environment. To this end, we begin by formulating a novel online path generation problem for a group of Dubins vehicles in the form of cost minimization based on the formulation of persistent coverage control. We then transform the cost minimization into a constraint-based specification so that a prescribed performance level is certified. An online coverage path generator is then designed based on the so-called constraint-based control in order to meet the performance certificate together with additional constraints inherent in the parameters that specify the paths. It is also shown there that the present constraint-based approach allows one to drastically reduce the computational complexity stemming from combinations of binary variables corresponding to the turning directions of the USVs. The present coverage path generator is finally demonstrated through simulations and experiments on an original testbed of multiple USVs.","Aquatic environmental monitoring with unmanned surface vehicles (USVs) or underwater vehicles equipped with external sensors is interesting for a variety of applications where different types of variables need to be monitored [1]. The literature includes heterogeneous examples such as the estimation of the thickness of lava eruptions by performing near-bottom magnetic surveys at the seafloor [2], the detection of plankton-rich waters by measuring chlorophyll density [3], the detection of pollution sources [4], and the generation of environmental maps of variables of interest (e.g., pH, temperature, dissolved oxygen, turbidity, etc.) [5, 6]. See also [7] for a specific survey on marine environmental applications that include the monitoring of multiple physical, biochemical, and ecosystem features. Given the large extension of the areas that need to be covered, it is common to rely on a fleet of vehicles, which needs to be coordinated to maximize the efficiency of the operations. Typically, this involves the optimization of some type of criterion, e.g., the quality of the state estimation [8], the performance of a control system with robots in the loop [9], metrics related to the entropy of the information gathered [10], or some type of utility function encoding optimal coverage and sensing policies, as in coverage control [11]. As pointed out by [12], which surveys source localization methods, the strategies followed by these multi-agent systems have strong commonalities with those used by nature, e.g., in chemotaxis and infotaxis processes. Moreover, some of the methods proposed are directly inspired in biology, e.g., the optimal coverage of dynamic pollutant profiles by mimicking bacterial swarms [13]. From methods available in the literature, we are especially interested in coverage control, which distributes a group of mobile sensors over the environment for efficient data sampling by following the gradient over an objective function [11]. From this basic setup, some variations have been proposed. For example, persistent coverage control stimulates the mobile sensors to continuously patrol the environment rather than forming a stationary configuration [14, 15, 16]. Other recent advances in coverage control include data-driven policies and dealing with unknown event density functions [17, 18, 19]. Finally, a significant topic in this context is that of constraints, which can also be accounted for by means of constraint-based control [20]. In this way, the coverage control policy can be aware of issues such as the battery levels â€”essential the for long-duration autonomy of the robots [21]â€”, obstacles, and performance guarantees [15, 22, 23]. Despite these advances, most existing solutions assume a fully actuated kinematic model for the mobile robots, and cannot be applied to water vehicles having various motion constraints. The coverage control for robots with motion constraints similar to USVs has been investigated in the literature. The papers [24, 25, 26] considered a unicycle model with a nonholonomic constraint such that the lateral linear body velocity is constrained to be zero. While the linear body velocity in the forward direction was assumed to be fully controllable in these publications, most USVs do not have braking systems and can only generate thrust in the forward direction. In view of these hardware constraints, assuming a constant forward speed would be more realistic in many cases. To deal with this issue, some authors consider simplified models such as that of the Dubins vehicle, where constant forward speed is assumed [25, 27]. Coverage-like problems with motion constraints have also been investigated within the framework of the traveling salesman problem [28] and coverage path planning [29]. Additionally, the kinematic vehicle models assumed in the above papers allow instantaneous velocity changes, but USVs may find problems to follow the velocities generated by coverage controllers due to their greater inertia. Another issue is that water vehicles may suffer significant disturbances due to currents, waves, and wind. For these reasons, in marine craft control it is common to employ a hierarchical architecture composed of control layers for guidance, navigation, and control [30]. Indeed, multiple works follow a backstepping approach [31, 32], where the overall control problem is decoupled into a high-level problem that generates ideal paths to follow and a low-level problem that makes USVs follow the path while rejecting disturbances. That is, the fleet trajectory design is decoupled from the individual trajectory tracking problem, as happens, for example, in [33]. With this in mind, it is not surprising that most results on coverage control with USVs focus on coverage path planning [34, 35, 36], leaving path following to low-level controllers. Nevertheless, even in these simpler setups, there are challenges to solve. For example, it is difficult to update the paths flexibly under real situations because path planning is executed in a different layer. In addition, the path-planning approach generates only a finite-length path, which may not be suitable for persistent monitoring of the aquatic environment. In this article, we consider an online coverage path generation with real-time feedback on the states of the USVs. Specifically, we deal with circular and elliptic paths as the most basic path shapes. We then formulate a novel online optimal path generation problem corresponding to each path shape, which is based on the formulation of persistent coverage control [14, 15, 16]. To guarantee coverage performance for the path, we transform the specification for minimizing costs into a constraint-based specification. It is then pointed out that the problem may suffer from a combinatorial explosion associated with binary variables corresponding to the turning directions of the USVs. To address this issue, we present a more conservative scheme to reduce computational complexity, which is also shown to provide a partially distributed structure. We then present a partially distributed constraint-based control as in [15, 21, 22, 23] to meet the performance constraint together with constraints that parameters of the path are inherently required to satisfy. The proposed online coverage path generator is then demonstrated with an ideal mathematical model over a wide area free from the space constraints. We finally implement a hierarchical control architecture including the present coverage path generator on an experimental testbed, and demonstrate that the path generator works even in the presence of various uncertainties in the real physical world. In summary, the contributions of this article are: 1. A novel online coverage path generation problem is presented, which generates persistently patrolling behavior, differently from offline coverage path planning. 2. To the best of the authorsâ€™ knowledge, this article is the first to apply the concept of constraint-based control to the high-level path generation. Indeed, the constraint-based control is generally used as low-level safe control. 3. It is revealed that the possible computational explosion stemming from combinations of binary decision variables can be avoided by giving the specification as a constraint rather than cost minimization. 4. The proposed path generator is demonstrated through simulations and also experiments on a real testbed. Finally, notice that a very preliminary version of this work was presented in conference [37]. The current article presents a refined version of the proposed framework and also original simulations on elliptic paths and experiments."
https://arxiv.org/html/2411.00506v1,Weighted Null Space Fitting (WNSF): A Link between The Prediction Error Method and Subspace Identification,"Subspace identification method (SIM) has been proven to be very useful and numerically robust for estimating state-space models. However, it is in general not believed to be as accurate as prediction error method (PEM). Conversely, PEM, although more accurate, comes with non-convex optimization problems and requires local non-linear optimization algorithms and good initialization points. This contribution proposes a weighted null space fitting (WNSF) method to identify a state-space model, combining some advantages of the two mainstream approaches aforementioned. It starts with the estimate of a non-parametric model using least-squares, and then the reduction to a state-space model in the observer canonical form is a multi-step least-squares procedure where each step consists of the solution of a quadratic optimization problem. Unlike SIM, which focuses on the range space of the extended observability matrix, WNSF estimates its null space, avoiding the need for singular value decomposition. Moreover, the statistically optimal weighting for the null space fitting problem is derived. It is conjectured that WNSF is asymptotically efficient, which is supported by a simulation study.","Prediction error method (PEM) and subspace identification method (SIM) are the two mainstream approaches in system identification. Originating from the maximum likelihood (ML) estimator [1], PEM minimizes a cost function based on prediction errors, the differences between observed outputs and their predictions based on the model and past data. When noise is Gaussian, PEM with a quadratic cost function is equivalent to ML estimation. In particular, its asymptotic covariance is the inverse of the Fisher information matrix, which makes PEM an asymptotically efficient estimator, attaining the smallest variance for consistent estimators, as defined by the CramÃ©r-Rao lower bound (CRLB) [2, 3]. PEM is widely used as a benchmark in system identification [4], however, there are two issues that may hinder its successful application. The first one is the risk of converging to a local minimum rather than a global minimum of the cost function, which is generally non-convex. Addressing this issue requires local non-linear optimization algorithms and good initial estimates. The second challenge arises in multivariable systems, where PEM typically requires a large number of parameters to describe them, making it complex and less convenient, especially when building models in the state-space form. On the other hand, originating from the celebrated Ho-Kalman algorithm [5], subspace identification method (SIM) is known for its numerical robustness and convenient parameterization for MIMO systems. Although there exist many variants, including but not limited to [6, 7, 8, 9, 10, 11], most SIMs can be unified into a common framework which typically involves three steps [12]: First, high-order models containing the systemâ€™s Markov parameters are estimated using projection or least-squares regression. Second, these high-order models are reduced to a low-dimensional subspace using singular value decomposition (SVD), where the extended observability matrix could be found. Third, a balanced realization of the state-space matrices is obtained from the extended observability matrices and linear regressions. Despite the tremendous success of SIMs both in theory and practice [13], some drawbacks should be emphasized. First, although it has been proved that most SIMs are consistent using open-loop data [14, 15], and some of them are consistent using closed-loop data [16], the question of whether there are subspace methods that are asymptotically efficient under general settings is still unresolved some 50 years after this family of methods was introduced. It is generally believed that SIMs are not as accurate as PEM [17]. Second, it is difficult to incorporate structural information in SIMs, often resulting in black-box models. Some exceptions to this can be found in [18, 19]. The primary motivation of this work is to introduce a new method for identifying linear time-invariant (LTI) systems in the state-space form. As indicated in the title, our method serves as a bridge between PEM and SIM, with the bridging criteria being twofold: First, it should offer performance comparable to PEM. Second, it should be numerically robust, leveraging the key strengths of SIM. Our method builds upon the foundation of existing approaches that aim to address one or more of the aforementioned drawbacks of PEM and SIM. We will not attempt to fully review this vast field, but we highlight some of the milestones. I-A Related Work Instrumental variable methods (IVMs) [20] could ensure consistency in many settings without encountering non-convexity issues. Although asymptotic efficiency can be achieved for certain problems via iterative algorithms [21], [22], IVMs cannot attain CRLB using closed-loop data. Some methods fix certain parameters in the cost function, transforming it into a quadratic optimization problem solvable via (weighted) least-squares. In each iteration, the fixed coefficients are updated with estimates from the previous step during weighting or filtering. This approach gives rise to iterative least-squares methods, like iterative quadratic maximum likelihood (IQML) method [23, 24], the Steiglitz-McBride method [25], and the Box-Jenkins Steiglitz-McBride (BJSM) algorithm [26]. While these methods avoid non-convex optimization, their consistency and asymptotic efficiency are only guaranteed under specific conditions, such as white noise and open-loop data. Moreover, optimal accuracy requires an infinite number of iterations. In addition to iterative least-squares methods, multi-step least-squares techniques use a finite number of steps to achieve estimates with certain asymptotic properties. These methods involve solving convex optimization problems or numerically reliable procedures at each step. An important feature of these methods is that a more flexible model is often estimated in an intermediate step, followed by a model reduction to obtain the model of interest. Asymptotic efficiency requires that the intermediate model acts as a sufficient statistics, with model reduction done statistically soundly. Some of the representative methods are indirect PEM [27], Durbinâ€™s first and second methods [28, 29], and the weighted null space fitting (WNSF) method [30]. These methods have been applied to several structured models, such as output-error (OE), auto-regressive moving-average with exogenous inputs (ARMAX), and Box-Jenkins (BJ) models, but not to state-space models, which this work addresses. Since the publication of the Ho-Kalman algorithm [5], significant efforts have been made to improve SIMs. Key developments include direct estimation of the Hankel matrix [6, 7, 8], parallel estimation of several high-order ARX models [9, 11], and addressing bias in closed-loop settings [31, 10, 32, 33, 16]. While most SIMs focus on the range space of the extended observability matrix, some approaches have shifted attention to the null space [34, 35], where an optimal estimate of the observability matrixâ€™s null space is obtained using a two-step least-squares method. Null space fitting offers the advantage of deriving optimal weighting, making it an important heuristic for our method. However, to fully parameterize the null space, certain canonical forms are required, which makes this approach less convenient for MIMO systems. Additionally, estimating the null space still requires an SVD step to explicitly obtain the observability matrix. Recent work [36, 37] has reformulated the least-squares realization of autonomous LTI systems as an eigenvalue problem, solved using block Macaulay matrices. This perspective sheds some new light in understanding the identification of a state-space model. However, it demands large-scale numerical algorithms when dealing with extensive data sets. In terms of performance, it is demonstrated through asymptotic tools that SIMs are generally consistent with open-loop data, and some methods are asymptotically equivalent [38]. In particular, the canonical variate analysis (CVA) [6] method achieves the optimal accuracy among available weighting choices of the SVD step when the measured inputs are white [39], however, simulation studies indicate that it is not asymptotically efficient [11]. Currently, the quest for an asymptotically efficient SIM is still open [17, 38]. I-B Contributions This work extends the WNSF method, originally proposed in [30], to state-space models. It uses two features of the methods aforementioned. The first feature is starting with an estimate of a high-order non-parametric model, which contains the systemâ€™s Markov parameters, similar to the pre-estimation step of SSARX [10]. This non-parametric model captures the behavior of the true system with sufficient accuracy and serves as a sufficient statistics. Subsequently, model reduction is performed to obtain a state-space model in the observer canonical form. Unlike methods that explicitly minimize the model-reduction cost function, such as the indirect PEM, the model reduction of WNSF involves solving a weighted least-squares problem. The optimal weighting able to achieve asymptotic efficiency depends on the estimated model parameters. To facilitate this, an additional least-squares step is introduced to provide an initial estimate of these parameters. The WNSF method, consisting of multiple least-squares steps, offers favorable computational properties compared to methods like PEM. Another interesting feature of WNSF is that it estimates the null space of the extended observability matrix, parameterized by the coefficients of the systemâ€™s characteristic polynomial, rather than the range space typically estimated by most SIMs using SVD. This approach eliminates the need for a SVD step and weighting matrices associated to it, and avoids the tuning of past and future horizons, making it more straightforward and easier to implement. In summary, the key contributions of this work are as follows: It derives a novel method to identify state-space models, which combines some features of PEM and SIM simultaneously, appearing to be numerically robust. In addition we believe that it can be shown that WNSF is asymptotically efficient, though this remains to be shown formally. I-C Structure The disposition of the paper is as follows: Following the Introduction, in Section II, we present preliminaries, including models and assumptions relevant to WNSF, and a brief overview of SIM and PEM. In Section III, we introduce the WNSF method with SISO systems. In Section IV, we discuss how WNSF is related to PEM and SIM. In Section V, we demonstrate the effectiveness of WNSF using a numerical example. Finally, it is concluded in Section VI. I-D Notations 1. For a matrix Xğ‘‹Xitalic_X with appropriate dimensions, XâŠ¤superscriptğ‘‹topX^{\top}italic_X start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT, Xâˆ’1superscriptğ‘‹1X^{-1}italic_X start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, Xâ€ superscriptğ‘‹â€ X^{\dagger}italic_X start_POSTSUPERSCRIPT â€  end_POSTSUPERSCRIPT, Ïâ¢(X)ğœŒğ‘‹\rho(X)italic_Ï ( italic_X ), rankâ¢(X)rankğ‘‹{\text{rank}}(X)rank ( italic_X ), Nullâ¢(X)Nullğ‘‹{\text{Null}}(X)Null ( italic_X ) and dimâ¢(Nullâ¢(X))dimNullğ‘‹\text{dim}\left(\text{Null}(X)\right)dim ( Null ( italic_X ) ) denote its transpose, inverse, Moore-Penrose pseudo-inverse, spectral radius, rank, null space and dimension of the null space, respectively. Moreover, Iğ¼Iitalic_I and 00 are the identity and zero matrices of appropriate dimensions. 2. ğ”¼â¢xtğ”¼subscriptğ‘¥ğ‘¡{\mathbb{E}{x_{t}}}blackboard_E italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the expectation of a random vector xtsubscriptğ‘¥ğ‘¡x_{t}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and ğ”¼Â¯â¢xÂ¯ğ”¼ğ‘¥\bar{\mathbb{E}}xoverÂ¯ start_ARG blackboard_E end_ARG italic_x is defined by ğ”¼Â¯â¢x:=limNâ†’âˆ1Nâ¢âˆ‘t=1Nğ”¼â¢xtassignÂ¯ğ”¼ğ‘¥subscriptâ†’ğ‘1ğ‘superscriptsubscriptğ‘¡1ğ‘ğ”¼subscriptğ‘¥ğ‘¡\bar{\mathbb{E}}x:=\mathop{\lim}\limits_{N\to\infty}\frac{1}{N}\sum\limits_{t=% 1}^{N}{\mathbb{E}{x_{t}}}overÂ¯ start_ARG blackboard_E end_ARG italic_x := roman_lim start_POSTSUBSCRIPT italic_N â†’ âˆ end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG âˆ‘ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT blackboard_E italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. 3. xâˆ¼ğ’©â¢(Î¼,Î£)similar-toğ‘¥ğ’©ğœ‡Î£x\sim\mathcal{N}(\mu,\Sigma)italic_x âˆ¼ caligraphic_N ( italic_Î¼ , roman_Î£ ) means that a random vector xğ‘¥xitalic_x is normally distributed with mean Î¼ğœ‡\muitalic_Î¼ and covariance Î£Î£\Sigmaroman_Î£, and xNâˆ¼Asâ¢ğ’©â¢(Î¼,Î£)similar-tosubscriptğ‘¥ğ‘Asğ’©ğœ‡Î£x_{N}\sim{\text{As}}\mathcal{N}(\mu,\Sigma)italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT âˆ¼ As caligraphic_N ( italic_Î¼ , roman_Î£ ) means that xNsubscriptğ‘¥ğ‘x_{N}italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT converges in distribution to ğ’©â¢(Î¼,Î£)ğ’©ğœ‡Î£\mathcal{N}(\mu,\Sigma)caligraphic_N ( italic_Î¼ , roman_Î£ ) as Nâ†’âˆâ†’ğ‘N\to\inftyitalic_N â†’ âˆ w.p.1, where Nâ†’âˆâ†’ğ‘N\to\inftyitalic_N â†’ âˆ w.p.1 means Nğ‘Nitalic_N tends to infinity with probability one."
https://arxiv.org/html/2411.00417v1,"Closed-LoopStability of aLyapunov-BasedSwitching Attitude Controller forEnergy-EfficientTorque-Input-SelectionDuring Flightâ€ â€ thanks:This work was supported by the Joint Center for Aerospace Technology Innovation (JCATI) throughAward172172172172, the Washington State University (WSU) Foundation and the Palouse Club through a Cougar Cage Award toN.â€‰O.â€‰PÃ©rez-Arancibia, and the WSU Voiland College of Engineering and Architecture through astart-upfund toN.â€‰O.â€‰PÃ©rez-Arancibia.â€ â€ thanks:F.â€‰M.â€‰F.â€‰R.â€‰GonÃ§alvesandN.â€‰O.â€‰PÃ©rez-Arancibiaare with the School of Mechanical and Materials Engineering, Washington State University (WSU), Pullman,WA99164991649916499164-2920292029202920, USA.R.â€‰M.â€‰Benais with the Department of Mechanical and Civil Engineering, California Institute of Technology (Caltech), Pasadena,CA91125911259112591125-2100210021002100, USA. Corresponding authorsâ€™ Email:francisco.goncalves@wsu.edu(F.â€‰M.â€‰F.â€‰R.â€‰G.);n.perezarancibia@wsu.edu(N.â€‰O.â€‰P.-A.)","We present a new Lyapunov-based switching attitude controller for energy-efficient real-time selection of the torque inputted to an uncrewed aerial vehicle (UAV) during flight. The proposed method, using quaternions to describe the attitude of the controlled UAV, interchanges the stability properties of the two fixed pointsâ€”one locally asymptotically stable and another unstableâ€”of the resulting closed-loop (CL) switching dynamics of the system. In this approach, the switching events are triggered by the value of a compound energy-based function. To analyze and ensure the stability of the CL switching dynamics, we use classical nonlinear Lyapunov techniques, in combination with switching-systems theory. For this purpose, we introduce a new compound Lyapunov function (LF) that not only enables us to derive the conditions for CL asymptotic and exponential stability, but also provides us with an estimate of the CL systemâ€™s region of attraction. This new estimate is considerably larger than those previously reported for systems of the type considered in this paper. To test and demonstrate the functionality, suitability, and performance of the proposed method, we present and discuss experimental data obtained using a ğŸ‘ğŸ31\boldsymbol{31}bold_31-g quadrotor during the execution of high-speed yaw-tracking maneuvers. Also, we provide empirical evidence indicating that all the initial conditions chosen for these maneuvers, as estimated, lie inside the systemâ€™s region of attraction. Last, experimental data obtained through these flight tests show that the proposed switching controller reduces the control effort by about ğŸ“ğŸ‘53\boldsymbol{53}bold_53 %, on average, with respect to that corresponding to a commonly used benchmark control scheme, when executing a particular type of high-speed yaw-tracking maneuvers.","We envision a future in which swarms of robotic flying insects are deployed in unstructured environments to perform tasks useful for society; for example, biological research that requires the physical and visual tracking of natural insects. The execution of these types of maneuvers necessitates the use of high-performance, robust, and efficient controllers capable of following high-speed trajectories while maintaining stable flight. We can imagine, for instance, a robotic insect flying embedded in a bee colony; then, while tracking a particular specimen the robot is commanded to track a different bee and must promptly rotate in order to follow the new reference. Intuitively, it seems that we should always apply the control torque in the direction of the shorter rotational path; however, depending on the instantaneous attitude and angular-velocity errors of the flyer, it might be advantageous to apply the control torque in the direction of the longer rotational path, according to a user-defined performance figure of merit (PFM) [1, 2, 3, 4, 5]. In this paper, we use quaternions to represent the attitude kinematics and dynamics of the controlled uncrewed aerial vehicle (UAV) in space, a method widely used in aerial robotics and spacecraft control due to its numerical robustness and suitability to avoid issues caused by kinematic singularities [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1, 15, 16, 2, 17, 18, 19, 20, 21, 22, 3, 23]. Unfortunately, as discussed in [4, 5, 6, 7], when quaternions are used to define attitude controllers of the type presented in [7], the resulting closed-loop (CL) dynamics exhibit two fixed points corresponding to the same attitude kinematics but with opposite stability propertiesâ€”one is locally asymptotically stable and the other is unstable. This issue brings challenges regarding flight performance; however, the stability properties of the two CL systemâ€™s fixed points can be interchanged in real time by simply changing the sign of a term in the control lawâ€”equivalent to reversing the direction of the torque input corresponding to that termâ€”which is beneficial from a performance perspective in some cases. Specifically, a switching scheme of this type can be implemented to prevent unwinding behavior, defined as large rotations (>Ï€absentğœ‹>\pi> italic_Ï€ rad) that return the flyer to its original orientation and are caused by representational ambiguities [16]. The most common technique used to avoid unwinding is to multiply the term proportional to the vector part of the attitude-error quaternion (AEQ) by the sign of its scalar part in the definition of the control law that specifies the total torque inputted to the UAVâ€™s open-loop dynamics [10, 11, 12, 13, 14, 15, 7, 6, 9, 8]. This scheme ensures that the direction of the proportional torque is aligned with that of the shorter rotational path, which is not always the best decision from an energy perspective. To address this energy-motivated decision problem, [1] presents a heuristic method based on the a-priori specification of a set of rules extracted from one-thousand simulations with random initial conditions for orientation and angular velocity; [2] presents two hybrid controllers based on backstepping and the definition of an energy-like Lyapunov function (LF), respectively; [3] presents a hybrid controller that accounts for both the AEQ and angular-velocity error in the definition of a switching law; and, [4] presents a model-predictive method to select the most cost-efficient direction of the proportional torque input according to a user-defined PFM. More recently, in [5], we introduced a Lyapunov-based switching attitude controller that accounts for both the AEQ and the angular-velocity error to select, between two options, the torque law used for feedback control during flight; however, the chosen LF allows only for a very conservative estimation of the region of attraction of the stable CL fixed state, and we did not show that the unstable equilibrium is a saddle pointâ€”a topic thoroughly discussed in this paper. Here, we present a new Lyapunov-based switching attitude controller that selects the torque inputted to the controlled UAV, according to a criterion of energy efficiency, in real time. Also, we present stability analyses of the CL dynamics resulting from using the proposed switching scheme. Furthermore, by using a modified version of the LF in [5], we show that one of the two fixed states of the CL system is exponentially stable under a set of conditions, and provide an estimate of the corresponding region of attraction. To test the proposed approach, we implemented the new switching controller on a 31313131-g quadrotor to execute high-speed yaw-tracking maneuvers. The obtained experimental data compellingly demonstrate the suitability, functionality, and performance of the proposed approach. Figure 1: UAV platform used in the real-time flight experiments, the Crazyflie 2.12.1\boldsymbol{2.1}bold_2.1. Here, ğ“‘={ğ’ƒ1,ğ’ƒ2,ğ’ƒ3}ğ“‘subscriptğ’ƒ1subscriptğ’ƒ2subscriptğ’ƒ3\boldsymbol{\mathcal{B}}=\left\{\boldsymbol{b}_{1},\boldsymbol{b}_{2},% \boldsymbol{b}_{3}\right\}bold_caligraphic_B = { bold_italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_italic_b start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT }, with its origin coinciding with the UAVâ€™s center of mass, denotes the body-fixed frame of reference; ğ“={ğ’1,ğ’2,ğ’3}ğ“subscriptğ’1subscriptğ’2subscriptğ’3\boldsymbol{\mathcal{N}}=\left\{\boldsymbol{n}_{1},\boldsymbol{n}_{2},% \boldsymbol{n}_{3}\right\}bold_caligraphic_N = { bold_italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_italic_n start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT } denotes the inertial frame of reference. The rest of the paper is organized as follows. Section II reviews the main topics regarding the attitude dynamics of the controlled UAV and briefly describes a commonly-used quaternion-based continuous controller that we employed as the starting point of the presented research. Section III discusses the derivation of the two fixed points of the CL system resulting from using the continuous controller and their stability properties; this section also formulates and explains the performance problem associated with the implementation of quaternion-based attitude control laws of the type considered here. Section IV describes the switching control scheme introduced in this paper, derives the two equilibrium pointsâ€”one stable and another unstableâ€”of the resulting CL system, analyzes their stability, and provides an estimate for the region of attraction of the stable CL fixed point. Section V presents and analyzes experimental results. Last, Section VI states some conclusions. Notation: 1. â„â„\mathbb{R}blackboard_R and â„3superscriptâ„3\mathbb{R}^{3}blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT denote the sets of real numbers and triplets, respectively. 2. ğ’®3superscriptğ’®3\mathcal{S}^{3}caligraphic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT denotes the set of unit quaternions. 3. Italic lowercase symbols denote scalars, e.g., pğ‘pitalic_p; bold lowercase symbols denote vectors, e.g., ğ’‘ğ’‘\boldsymbol{p}bold_italic_p; bold uppercase symbols denote matrices, e.g., ğ‘·ğ‘·\boldsymbol{P}bold_italic_P; and bold crossed lowercase symbols denote quaternions, e.g., Â¯â¢ğ’‘â¢ğ¦ğ¢ğ¬ğ¬ğ¢ğ§ğ Â¯ğ’‘ğ¦ğ¢ğ¬ğ¬ğ¢ğ§ğ \boldsymbol{\mathrlap{\mathchoice{\raisebox{-6.65pt}[0.0pt][0.0pt]{$% \displaystyle{\mathchar 22\relax\mkern-9.0mu}$}}{\raisebox{-6.65pt}[0.0pt][0.0% pt]{$\textstyle{\mathchar 22\relax\mkern-9.0mu}$}}{\raisebox{-6.65pt}[0.0pt][0% .0pt]{$\scriptstyle{\mathchar 22\relax\mkern-9.0mu}$}}{\raisebox{-6.65pt}[0.0% pt][0.0pt]{$\scriptscriptstyle{\mathchar 22\relax\mkern-9.0mu}$}}}pmissing}start_ARG Â¯ end_ARG bold_italic_p bold_missing. 4. The symbols Ã—\timesÃ— and âŠ—tensor-product\otimesâŠ— denote the vector cross-product and quaternion product, respectively. 5. The operator âˆ¥â‹…âˆ¥2\|\cdot\|_{2}âˆ¥ â‹… âˆ¥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT computes the 2222-norm of a vector. 6. The operator sgnâ¢{â‹…}sgnâ‹…\text{sgn}\left\{\,\cdot\,\right\}sgn { â‹… } extracts the sign of a real scalar. 7. The symbols >>>, <<<, â‰¥\geqâ‰¥, and â‰¤\leqâ‰¤ denote ordering or definiteness relationships when used with scalars or matrices, respectively. 8. The symbol ğ‘°ğ‘°\boldsymbol{I}bold_italic_I denotes any identity matrix of adequate dimensions; the symbol ğŸ0\boldsymbol{0}bold_0 denotes any block of zeros of adequate dimensions."
https://arxiv.org/html/2411.00337v2,Coherent Hierarchical Probabilistic Forecasting of Electric Vehicle Charging Demand,"The growing penetration of electric vehicles (EVs) significantly changes typical load curves in smart grids. With the development of fast charging technology, the volatility of EV charging demand is increasing, which requires additional flexibility for real-time power balance. The forecasting of EV charging demand involves probabilistic modeling of high dimensional time series dynamics across diverse electric vehicle charging stations (EVCSs). This paper studies the forecasting problem of multiple EVCS in a hierarchical probabilistic manner. For each charging station, a deep learning model based on a partial input convex neural network (PICNN) is trained to predict the day-ahead charging demandâ€™s conditional distribution, preventing the common quantile crossing problem in traditional quantile regression models. Then, differentiable convex optimization layers (DCLs) are used to reconcile the scenarios sampled from the distributions to yield coherent scenarios that satisfy the hierarchical constraint. It learns a better weight matrix for adjusting the forecasting results of different targets in a machine-learning approach compared to traditional optimization-based hierarchical reconciling methods. Numerical experiments based on real-world EV charging data are conducted to demonstrate the efficacy of the proposed method.","The number of electric vehicles (EVs) is increasing rapidly worldwide in recent years. According to the Global EV Outlook, there were over 16.5 million EVs on the road in 2021, which was tripled in three years [1]. The estimated electricity demand from EVs in 2030 would exceed 1400 TWh in the net zero emissions by 2050 scenario. While the EV electricity demand in China takes up 0.5% of the countryâ€™s final electricity demand in 2021, a conservative estimate of the number is expected to go beyond 3% in 2030. Although there are discussions on whether fast charging or battery swapping should be the major pattern of future EV refueling solution [2], fast charging is currently in a dominant position, and many obstacles need to be overcome for wider adoption of battery swapping mode. Thus, EV charging demand would undoubtedly be a significant component of the electricity load, with its special shape and volatility due to fast charging technologies. EV charging station (EVCS) operators are important stakeholders in the era of EV, as they provide charging piles and services to general EV users in addition to their home chargers. An EVCS operator may own multiple geographically-distributed charging stations and operate them hierarchically in coordination with the grid [3]. On many occasions, it may also be responsible for electricity purchase, ancillary service provision, and customer interaction [4, 5, 6]. Due to the stochastic nature of EV users, almost all the operating decisions of EVCS operators are made under uncertainty. Therefore, it is essential for EVCS operators to model the stochasticity and perform probabilistic forecasting of EV charging demand, generally in a hierarchical manner. I-A Related Work in Hierarchical Forecasting Hierarchical forecasting refers to the forecasting of multiple time series in hierarchy, i.e., some of the time series are the aggregation of others. Such hierarchical relationship of the time series would naturally yield a hierarchical constraint (which will be further explained in Section II-B) for the values of the time series at any specific time point. However, individual forecasting of them cannot guarantee the hierarchical constraint, leading to conflict of the results, reducing in accuracy, and further trouble in forecasting-guided operation and optimization. At early stage, the hierarchical constraint is enforced based on top-down or bottom-up forecasting [7, 8], i.e., distributing the high-level results to low-level or summing up the low-level results to high-level. However, such methods suffer from loss of information from individual series dynamics. Hyndman et al. proposed to use the reconciliation-based forecasting method to solve the hierarchical forecasting problem [9, 10]. This method, a.k.a., coherent forecasting, solves an optimization problem of adjusting the base forecasting results of different levels so that the adjusted results satisfy the hierarchical constraint. There are some emerging research related to hierarchical probabilistic forecasting. Taieb et al. [11, 12] proposed a bottom-up probabilistic forecast aggregation method based on copula theory. After aggregation, mean forecast combination and reconciliation are used to further ensure the coherency in the mean values. Hierarchical probabilistic load forecasting became the topic of the Global energy forecasting competition 2017 (GEFCom2017) [13]. Although several teams utilized the hierarchy information, few team discussed the problem of coherency at that time. Roach [14] proposed an XGBoost-based reconciled forecasting model for GEFCom2017. However, the reconciliation mainly focused on the quantile forecasting results instead of the probablisitic distribution. The forecasting results obtained from coherent forecasting has good statistical characteristics. According to the empirical study of [15], coherent forecasting usually improves the forecasting accuracy. It also ensures consistency of forecasted time series. Such advantages can further help the EVCS operator in consistent electricity purchase, efficient energy storage system operation, and effective customer interaction. However, there are still some open problems in coherent hierarchical forecasting, e.g., how to extend it to probabilistic forecasting [16], and how to implement it in a typical machine learning framework. I-B Related Work in EV Demand Forecasting Although EV charging demand forecasting is a relatively new topic compared to traditional energy forecasting problems, e.g., load forecasting and renewable generation forecasting, there has been some noticeable work in recent years. As for deterministic forecasting, Arias et al. [17] proposed a decision tree-based model to forecast the charging demand, utilizing historical traffic and weather data of the same region. Saputra et al. [18] used a deep neural network (DNN) to forecast the energy demand of a certain area covering multiple charging stations. A federated learning approach is adopted to address the communication overhead and privacy issues. Li et al. [19] proposed a sophisticated federated learning framework for EVCS demand forecasting. The forecasting model is based on convolutional neural network (CNN), bi-directional long short-term memory (BiLSTM), and attention mechanism. Charging piles are divided into clusters, and inter-cluster and inner-cluster federated learning are used to train the base layer and the personalized layer of the model, respectively. Dabbaghjamanesh et al. [20] proposed a Q-learning based method for load forecasting of EVCS. Q-learning is a reinforcement learning technique that can generate more accurate forecasting based on conventional models of recurrent neural network (RNN) and artificial neural network (ANN). Qiao et al. [21] forecasted the number of occupied charging piles at one certain EVCS using XGBoost. As for probabilistic forecasting, Huber et al. [22] focused on performing quantile forecasts of EV parking duration and energy demand from its upcoming trip distance. A multi-layer perception-based quantile regression model and multivariate conditional kernel density estimators are applied. Buzna et al. [23] proposed an ensemble learning-based forecasting framework for hierarchical probabilistic EV load. Quantile regression-based methods such as linear quantile regression (LQR) and quantile regression forests are used to generate forecasting results at low levels, and an â„“1subscriptâ„“1\ell_{1}roman_â„“ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-penalized LQR model is used to ensemble the results and obtain the high-level results. Wu et al. [24] used a parametric approach to model the arrival time and driven distances of EVs with normal and log-normal distributions. Hu et al. [25] combined the self-attention layers with the framework of machine theory of mind to perform quantile forecast. Li et al. [26] used LSTM for point forecasting and Gaussian distribution for uncertainty modeling. A Markov decision process solved by a proximal policy optimization algorithm from reinforcement learning is used to forecast the variance of the Gaussian distribution. Several research gaps arise from the aforementioned analysis. The limited number of papers on probabilistic forecasting usually focus on the ultra-short-term forecast, e.g., 15-min ahead in [25] and 1-hour ahead in [26], which is exactly one time interval ahead depending on data granularity. Existing methods use parametric approaches (e.g., the Gaussian distribution) or the classical quantile regression to model the conditional distribution of EVCS demand. Parametric approaches assuming certain forms of potential distributions may not be able to capture the strong stochasticity in EV usersâ€™ charging behaviors. In practice, classical quantile regression methods usually suffer from the annoying quantile crossing problem. Moreover, few literature looked into the problem in a hierarchical way, which means the forecasting results do not satisfy the hierarchical constraint in general. I-C Contribution To address the research gaps in probabilistic modeling of EV charging demand and forecasting consistency among different multiple EVCSs, this paper proposes a novel deep learning-based forecasting framework. It adopts two types of neural network layers for convex learning. The first one is based on partial input convex neural network (PICNN) [27, 28], which learns a strictly convex function mapping the partial input to the output. The PICNN can be used to parameterize an invertible model for universal density approximation without quantile crossing. It is used to construct a multi-horizon loss function and learn multi-variate distributions in forecasting problems [29]. The second one is based on differentiable convex optimization layers (DCL) [30], which learns the mapping of parameters to optimal values of a certain type of convex optimization problems called disciplined parameterized programs. DCL can be adopted efficiently as a layer in back-propagation neural networks [31]. We use PICNN to model the conditional distribution of EVCS demand in probabilistic forecasting, and DCL to further convert stochastic scenarios to coherent scenarios satisfying the hierarchical constraint. The proposed method does not rely on any distributional assumptions of the target series. The contribution of this paper is three fold: 1. A novel framework based on deep learning for EV charging demand probabilistic forecasting is proposed. It solves practical issues of multi-variate stochasticity modeling and hierarchical coherency. 2. PICNN is adopted to model the joint probabilistic distribution of multi-horizon EVCS demand as the gradient of a convex function w.r.t. quantile levels, which avoids potential problem of quantile crossing. 3. DCL is introduced to further map individual forecasting results of different EVCS to coherent scenarios. The weight matrix for adjustment in hierarchical reconciliation can be learned in a deep learning manner. Note that discussion or innovation on the forecasting engines (e.g., ANN, RNN, LSTM, etc.) is beyond the scope of this paper. The forecasting engine used in the case study of this paper is based on the widely-adopted LSTM modules, which can be easily changed to other forecasting engines. I-D Paper Structure The rest of this paper is structured as follows. Section II presents some preliminary knowledge of the hierarchical and probabilistic forecasting problem. Section III introduces the proposed EVCS demand forecasting framework. Section IV details the methodology of the adopted convex learning layers. The case study is conducted in Section V. Finally, Section VI draws the conclusion."
https://arxiv.org/html/2411.00223v1,Learning Optimal Interaction Weights in Multi-Agents Systems,"This paper presents a spatio-temporal inverse optimal control framework for understanding interactions in multi-agent systems (MAS). We employ a graph representation approach and model the dynamics of interactions between agents as state-dependent edge weights in a consensus algorithm, incorporating both spatial and temporal dynamics. Our method learns these edge weights from trajectory observations, such as provided by expert demonstrations, which allows us to capture the complexity of nonlinear, distributed interaction behaviors. We derive necessary and sufficient conditions for the optimality of these interaction weights, explaining how the network topology affects MAS coordination. The proposed method is demonstrated on a multi-agent formation control problem, where we show its effectiveness in recovering the interaction weights and coordination patterns from sample trajectory data.","Multi-agent systems (MAS) represent collections of autonomous agents working together to accomplish a task, with applications such as crowd navigation [1], human-robot (swarm) interaction [2], and multi-robot systems [3]. Graph representations provide a natural framework to model inter-agent coordination, where nodes represent agents, edges define their interactions, and edge weights quantify the strength of interactions. Understanding these interaction patterns is crucial, as agents must adapt their behaviors based on the environmental context and the states of neighboring agents. The topology of interactions evolves over time as agents move, and communicate (Fig. 1). Identifying neighborhood (adjacency) relationships between agents and how strongly agents influence each other (i.e., encoded as interaction weights) are essential for understanding the complex network dynamics, such as pedestrian movements in crowds. Classical methods such as graph signal processing (GSP) [4] and optimization-based approaches [5], primarily focused on identifying graph topology or estimating the Laplacian matrix from data. However, these approaches often assume uniform edge weights across all edges, limiting their ability to capture complex interactions. Recent advances such as self-attention encoders [6] have sought to recover edge weights from data but generally assume time-invariant weights. While graph neural networks (GNNs) [7, 8, 9] offer increased flexibility in learning interaction patterns, they can be difficult to interpret for performance analysis. Figure 1: A network of agents in a MAS. The topology of interactions evolves dynamically as agents move and share information, with interaction strengths represented by edge weights. The goal is to learn the edge weight policy from observed trajectories. Here, we adopt a control perspective, leveraging weighted consensus algorithms to characterize distributed coordination in MAS [10]. These algorithms explore how network topology influences performance and convergence, leveraging notions from algebraic graph theory [11]. Weighted consensus provides a foundation for advanced coordination behaviors such as formation control, where constraints like collision avoidance can be enforced to MAS [12]. Desired formation patterns are embedded in the coordination graph through carefully designed edge weights, often utilizing distance-based artificial potential functions (APFs) [11], where the function minimum corresponds to the target formation pattern. However, designing energy functions for complex topologies remains challenging, as local convexity does not guarantee global convergence in non-convex networks. We propose an inverse optimal control (IOC) framework to learn the interaction behavior directly from data. Existing IOC [13] and inverse reinforcement learning (IRL) methods aim to recover reward functions that characterize MAS behavior [14], but often pose restrictive assumptions about system linearity, and time-invariance. In contrast, this study employs a weighted consensus algorithm to model the interactions between agents, focusing on learning optimal, state-dependent edge weights directly from expert demonstrations via an IOC approach. Unlike previous methods, our framework does not rely on parametric energy functions (i.e., with known functional forms), system linearity, or time-invariance. By deriving optimality conditions, we provide a geometric interpretation of how interaction weights shape agent dynamics, demonstrating the potential to learn coordination behaviors. The remainder of this paper is organized as follows: Section II outlines key concepts for formulating the MAS coordination problem. In Section III, we formalize the IOC framework, explaining how interaction weights are learned from expert demonstrations and presenting the conditions for optimality. Section IV presents a simulation example, showcasing the effectiveness of the proposed method in recovering edge weights from MAS trajectory data. Finally, Section V concludes the paper with key findings, offers insights for future research, and discusses the broader applicability of the framework to complex, dynamic MAS."
https://arxiv.org/html/2411.00555v1,An exact column generation algorithm for load balancing in capacity sharing networks,"Capacity sharing networks are typical heterogeneous communication networks widely applied in information and communications technology (ICT) field. In such networks, resources like bandwidth, spectrum, computation and storage are shared among various communication services. Meanwhile, the issue of network congestion is always a prominent challenge. To handle network congestion essentially needs to solve the load balancing of networks. In this paper, for capacity sharing networks, we formulate their load balancing problem as a maximum multi-commodity flow problem. For such a problem, always a large-scale linear programming, the column generation algorithm is a commonly used and crucial method to solve it. In each iteration, this algorithm involves solving a linear programming subproblem and determining whether to terminate or generate a new column for inclusion in the subproblem. This iterative procedure of solving and checking continues throughout the algorithm. Nevertheless, since the checking subproblem is NP-hard, its solution significantly impacts the overall efficiency of the algorithm. In this paper, we innovatively convert the checking subproblem into a single-constrained shortest path (SCSP) subproblem. By exactly solving the SCSP subproblem, we can obtain the optimal solution to the checking subproblem with same or less computing time. Experimental results demonstrate that our algorithm achieves computational efficiency comparable to heuristic algorithms while outperforming other state-of-the-art algorithms by at least an order of magnitude.","Network technology is a pivotal force in the evolution of modern information and communications technology (ICT), crucial to applications like 6G communication, the Internet of Things (IoT), and the metaverse. As IoT, cloud computing, big data, and artificial intelligence rapidly advance, network technology undergoes continuous upgrades and enhancements, providing a robust foundation for ICT development. Looking ahead, network technology will remain essential in addressing growing communication demands, driving innovation, and facilitating breakthroughs in the ICT field. In the ICT field, achieving high reliability and low latency is vital for effective information transmission. However, network congestion has become a major challenge. It occurs when excessive data flows through specific nodes or links, increasing transmission delays from source to destination and degrading network performance. Additionally, congestion affects network throughput and can lead to packet loss. Therefore, tackling network congestion is essential for improving overall network performance. In particular, the issue of load balancing in capacity sharing networks is more prominent. Capacity sharing networks have wide applications in the ICT field, including smart home networks [1]-[2], wireless communication networks [3, 4], the link-computing networks and data center networks [5]-[6]. For example, various transmission technologies like ZigBee, Wi-Fi, Ethernet, Z-Wave, and Bluetooth coexist in smart home networks. Devices such as computers, smartphones, and smart TVs connected to the same transmission technology must share the available radio band. This sharing limits the capacity available to each device, and when multiple devices transmit the message simultaneously, they compete for the available capacity. This shared capacity can lead to network congestion, negatively affecting overall network performance and user experience. To alleviate network congestion, numerous studies have explored various approaches. Heuristic algorithms and software-defined networking technologies are often employed to tackle these issues. Additionally, this type of problems can be formulated as a multi-commodity flow problem, allowing for the use of mathematical optimization methods to find solutions. In the first type of studies, researchers tackled network congestion using heuristic algorithms and innovative flow management frameworks, specifically designed to satisfy the practical requirements of capacity sharing networks and achieve efficient solutions. Some researchers have also proposed dynamic multi-technology network intelligent management strategies [7, 8]. These strategies employ cross-technology management frameworks and algorithms to improve network throughput and adapt to device mobility and real-time demands. Moreover, researchers in [3] proposed node capacity constraints in order to optimize flow allocation in the network. A lightweight and scalable defense framework based on software-defined networking was utilized to efficiently manage network resources [9]. Meanwhile, some traffic monitoring techniques are employed to tackle the load balancing challenge in capacity sharing networks and their performance is validated in practical applications [10]-[11]. For instance, a heuristic load balancing algorithm, the video load balancing solution (ViLBaS), is proposed in [11]. This algorithm introduces a utility function composed of two components: the traffic load on the communication link and the delay to the flowâ€™s destination and it aims to minimize this utility function by selecting the shortest path with the lowest utility value for rerouting the traffic. The second category of research provides theoretical guarantees for proposed algorithms and results, utilizing mathematical programming principles. For instance, the authors in [12] introduced a pipe model based on the multi-commodity flow model to optimize flow allocation across multiple services, taking into account service requirements and link capacity constraints. However, the fluctuation of network traffic demands can impose challenges in predicting service flow demand [13]. To overcome this uncertainty, a robust optimization model that incorporates flow fluctuation was proposed, enhancing the networkâ€™s ability to handle the uncertain flow and mitigate congestion [14]. In [15], the authors quantified the congestion level in multi-commodity flow models using a convex and differentiable congestion function. A flow deviation method that replaces the traditional gradient concept with shortest path flow was proposed in [16], particularly effective for problems with convex differentiable objective functions. In different scenarios, furthermore, the transmission of service flow also needs to satisfy specific requirements, such as flow indivisibility, shortest path routing and k-routing flow [17]-[18]. In reference [19], researchers formulated the load balancing problem as a maximum multi-commodity flow model and employed the column generation algorithm to solve this problem. This method can optimize the quality of service and mobility management in capacity sharing networks, enabling real-time response to network changes. In this paper, our goal is to identify the optimal solution to the load balancing problem more efficiently. We propose an exact algorithm that not only provides theoretical guarantees but also achieves the maximum network throughput while satisfying load balancing requirements. The main contributions of this paper are: â€¢ We propose an exact column generation algorithm to solve the load balancing problem in capacity sharing networks. Utilizing the reduced cost vector from linear programming theory, we perform the optimality condition of this algorithm, ensuring the attainment of the optimal solution. By exactly achieving maximal network throughput, our proposed algorithm significantly improves overall network performance and stability. â€¢ In our proposed column generation algorithm, we innovatively transform the optimality checking subproblem into a single-constrained shortest path (SCSP) subproblem, solving it exactly. Since the checking subproblem is NP-hard, its solution directly impacts the efficiency of the entire algorithm. We prove that this checking subproblem is equivalent to a SCSP problem, which can be solved exactly. Furthermore, by employing exact algorithms that take advantage of the Lagrange duality method to exclude most feasible solutions, we significantly enhance the algorithmâ€™s computational efficiency. â€¢ The proposed column generation algorithm achieves the optimal solution within the same or less computing time compared to existing algorithms. Extensive numerical experiments demonstrate that our algorithm not only maximizes network throughput while satisfying load balancing requirements but also exhibits computational efficiency on par with the heuristic algorithm ViLBaS, outperforming other algorithms by at least an order of magnitude. Furthermore, this algorithm can serve as a benchmark for evaluating the network throughput improvement performance of other algorithms. The rest of this paper is organized as follows. In Section II, we introduce the detailed formulation of the load balancing problem model and provide the relevant mathematical descriptions. Section III presents the proposed column generation algorithm. In Section IV, we analyze the algorithmâ€™s complexity and convergence. Section V details extensive numerical experiments and presents the results obtained from various algorithms. Finally, Section VI offers conclusions on this paper."
https://arxiv.org/html/2411.00461v1,A Multi-Granularity Supervised Contrastive Framework for Remaining Useful Life Prediction of Aero-engines,"Accurate remaining useful life (RUL) predictions are critical to the safe operation of aero-engines. Currently, the RUL prediction task is mainly a regression paradigm with only mean square error as the loss function and lacks research on feature space structure, the latter of which has shown excellent performance in a large number of studies. This paper develops a multi-granularity supervised contrastive (MGSC) framework from plain intuition that samples with the same RUL label should be aligned in the feature space, and address the problems of too large minibatch size and unbalanced samples in the implementation. The RUL prediction with MGSC is implemented on using the proposed multi-phase training strategy. This paper also demonstrates a simple and scalable basic network structure and validates the proposed MGSC strategy on the CMPASS dataset using a convolutional long short-term memory network as a baseline, which effectively improves the accuracy of RUL prediction.","Aero-engines gradually experience fatigue and degradation over long periods, leading to a decline in performance and reliability. Therefore, accurate remaining useful life (RUL) predictions are important for timely maintenance and safety improvement. The current RUL prediction approaches can be categorized into two main groups: physical model-based approaches [1] and data-driven approaches [2]. Physical model-based approaches use the understanding and modeling of the physical behavior of a system to predict the RUL of a component or device. However, as the complexity of devices increases, it becomes increasingly difficult to build physical models. With the development of technologies such as sensors and the Internet of Things, it has become easier to acquire and save data, which has led to the growing popularity of data-driven RUL prediction approaches. Figure 1: Encoder feature space of four test engines from FD001 visualized by t-SNE. (a) is the feature space for regression training only, and the features with the same RUL label between different engines are far away from each other. (b) is the feature space trained by multi-granularity supervised comparisons with close feature distances between different engines with the same RUL label. Data-driven approaches extract valid features directly from data and use them for RUL prediction without physical knowledge. These can be further categorized into shallow machine learning and deep learning. The former requires manual feature selection and feeding them into predictors such as support vector regressior [3] to infer RUL. On the contrary, the deep learning approaches adopt an end-to-end training strategy, which simultaneously realizes feature extraction and RUL reasoning by optimizing the loss function through stochastic gradient descent, and has great versatility and flexibility. The best-known deep learning approaches are convolutional neural networks (CNN) [4] and long short-term memory neural networks (LSTM) [5]. However, most of the current deep learning-based RUL prediction approaches use only the mean square error (MSE) as the loss function, a metric that focuses only on the magnitude of the error without considering the underlying structural features, as shown in Fig 1a. Contrast learning is an unsupervised learning method that uses contrast loss (e.g., infoNCE [6] and triplet loss [7]) to pull similar embeddings together and push dissimilar embeddings apart in feature space. This helps to improve the performance of downstream tasks [8]. For labeled samples, supervised contrastive learning can be used. It extends a single positive sample in minibatch into multiple positive samples by the idea of class label-based aggregation, which results in tighter embedding of different classes [9]. However, for RUL prediction tasks with dense labels, a huge batch size is required to balance positive and negative samples in supervised contrastive learning using RUL labels directly, which is usually limited by hardware. To solve this problem, this paper proposes a multi-granularity supervised contrastive (MGSC) framework that incorporates two contrastive strategies, coarse-grained and fine-grained, to balance the samples while avoiding large batch sizes through a large classification scale label of health status (HS). The intuition of the framework is that the features used as input to the RUL regression layer should be aligned by RUL labels. Based on this, a simple scalable network structure was designed for validation in II-C. Moreover, to integrate the MGSC and regression tasks, a multi-phase training strategy is proposed. The proposed framework exceeds the accuracy of the baselines on the CMAPSS dataset, as shown in Fig 1b. The main contributions of this paper are as follows: 1. An MGSC strategy is proposed to regularize the feature space and also to solve the problems of large minibatch size and imbalance of positive and negative samples. 2. A multi-phase training strategy is proposed to progressively achieve the alignment of embeddings of the same RUL samples in the feature space and the regression prediction of RUL. 3. The proposed MGSC framework effectively improves the prediction accuracy in the baseline. The paper is organized as follows. Section II describes the proposed MGSC framework, including contrast strategy, training strategy, and network structure. Section III introduces the data preprocessing and model construction. Section IV verifies the validity of the MGSC framework through experimental comparisons. The conclusion is presented in Section V."
https://arxiv.org/html/2411.00413v1,Multi-Uncertainty Aware Autonomous Cooperative Planning,"Autonomous cooperative planning (ACP) is a promising technique to improve the efficiency and safety of multi-vehicle interactions for future intelligent transportation systems. However, realizing robust ACP is a challenge due to the aggregation of perception, motion, and communication uncertainties. This paper proposes a novel multi-uncertainty aware ACP (MUACP) framework that simultaneously accounts for multiple types of uncertainties via regularized cooperative model predictive control (RC-MPC). The regularizers and constraints for perception, motion, and communication are constructed according to the confidence levels, weather conditions, and outage probabilities, respectively. The effectiveness of the proposed method is evaluated in the Car Learning to Act (CARLA) simulation platform. Results demonstrate that the proposed MUACP efficiently performs cooperative formation in real time and outperforms other benchmark approaches in various scenarios under imperfect knowledge of the environment.","Multi-vehicle systems can significantly accelerate task completion, e.g., platoon formation and collaborative logistics, via communications and interactions among previously isolated vehicles [1, 2, 3, 4]. The key to realizing these systems and tasks lies in achieving high-performance and computationally-efficient autonomous cooperative planning (ACP), which is a high-dimensional system with nonholonomic motion and collision avoidance constraints [5]. However, ACP may suffer from various uncertainties. First, in an autonomous driving (AD) functional pipeline, the downstream trajectory planning is based on the output of upstream environmental perception [6]. Therefore, errors of the learning-based perception, also known as perception uncertainty (as shown in Fig. 1a), will propagate to the model-based planning. In addition, there exists inevitable mismatch between the planned and actual trajectories [7]. Such motion uncertainty becomes even larger in some adversarial conditions, e.g., bad weather. Last but not the least, by shifting from single- to multi-vehicle perception, the perception uncertainty can be significantly reduced in the ACP (Fig. 1b). However, imperfect channel state information could result in communication outage, which may jeopardize the information fusion [8, 9]. Under a high communication uncertainty, the case of Fig. 1b would shift back to Fig. 1a, as the ego-vehicle cannot receive the views of other-vehicles. Existing uncertainty-aware planning approaches treat perception, motion, communication uncertainties separately. Moreover, they mostly focus on single-vehicle AD instead of multi-vehicle ACP. There also exist other vehicle platooning works [10, 11, 12], but none of them consider uncertainty issues. To fill this gap, this paper proposes a multi-uncertainty aware ACP (MUACP) framework, that incorporates perception, motion, communication uncertainties into a unified optimization formulation, thereby automatically allowing for their aggregated effects. (a) Perception uncertainty (b) Multi-vehicle perception Figure 1: Perception uncertainty and multi-vehicle perception. Specifically, our solution chooses the lidar sensor as an illustration for computing the perception uncertainty, due to its ability to provide direct, dense, active, accurate depth measurements of environments [13, 14]. Motion uncertainty is measured according to the wheel feedbacks and weather conditions [7]. Communication uncertainty is built based on the wireless channel distribution and the outage probability [8]. Based on these models, the MUACP problem is formulated as a regularized cooperative model predictive control (RC-MPC) problem, where the regularizers for motion uncertainties and the constraints for perception-communication uncertainties are constructed according to the aforementioned methodologies. Finally, we implement the MUACP approach in the Car Learning to Act (CARLA) simulation platform [15]. To enlarge the sensing ranges and improve the detection accuracies of individual vehicles, we also implement the late-fusion cooperative perception module based on [16, 17] and bridge this module with the MUACP, forming a even more robust ACP system. Results demonstrate the superiority of the proposed MUACP in various scenarios. To the best of our knowledge, this is the first work to consider multiple uncertainties in ACP system. The main contributions are summarized below: â€¢ We design an efficient ACP strategy based on MPC with full-shape collision avoidance constraints; â€¢ We incorporate motion, perception, communication uncertainties into MPC as regularizers and constraints; â€¢ We evaluate the performance of the proposed scheme in the CARLA with extensive comparisons."
https://arxiv.org/html/2411.00107v1,"First, Learn What You Donâ€™t Know:Active Information Gathering for Driving at the Limits of Handling","Combining data-driven models that adapt online and model predictive control (MPC) has enabled effective control of nonlinear systems. However, when deployed on unstable systems, online adaptation may not be fast enough to ensure reliable simultaneous learning and control. For example, controllers on a vehicle executing highly dynamic maneuvers may push the tires to their friction limits, destabilizing the vehicle and allowing modeling errors to quickly compound and cause a loss of control. In this work, we present a Bayesian meta-learning MPC framework. We propose an expressive vehicle dynamics model that leverages Bayesian last-layer meta-learning to enable rapid online adaptation. The modelâ€™s uncertainty estimates are used to guide informative data collection and quickly improve the model prior to deployment. Experiments on a Toyota Supra show that (i) the framework enables reliable control in dynamic drifting maneuvers, (ii) online adaptation alone may not suffice for zero-shot control of a vehicle at the edge of stability, and (iii) active data collection helps achieve reliable performance.","Controlling unstable nonlinear systems remains challenging [1]. For example, guiding a vehicle through dynamic and unstable drifting maneuvers is a difficult task where slight decision-making mistakes can lead to spin outs or crashes. In recent years, advanced control methods have been developed with the goal of giving autonomous vehicles the ability to leverage their full handling potential [2, 3, 4, 5, 6], with possible applications to the future design of advanced driver-assistance systems and autonomous driving safety systems. In this setting, data-driven model predictive control (MPC) methods have gained wide popularity [7] due to their high performance, capability to use expressive learned models, and ability to account for constraints such as actuator limits and obstacle avoidance. In particular, data-driven models that adapt using online data are able to reduce model errors and sim-to-real discrepancies at deployment time [8, 9, 10, 11]. However, simultaneously controlling and learning unstable systems is notoriously difficult. For instance, successfully executing a drifting maneuver requires a sufficiently-accurate initial model to precisely initiate the drift, and online adaptation alone may not enable identifying a model fast-enough to execute the maneuver, see Section V-C. Autonomous driving at the limits, therefore, presents a challenge for data-driven MPC with online adaptation, due to its fast-paced nature and the inherent instability of the executed maneuvers. These difficulties raise the following questions: How can we effectively identify uncertain nonlinear dynamics to ensure that subsequent control is reliable? How should informative data be collected efficiently and safely to minimize time-intensive and potentially expensive testing on hardware? Figure 1: We propose an active information gathering framework that guides data collection to quickly learn a model in a controlled (i.e., safe) setting. Adapting on this information-rich data enables reliable control in dynamic drifting trajectories. In contrast, directly using the model without adapting prior to deployment may lead to unreliable control performance such as spinning out. Contributions: We present an active information gathering framework to quickly identify nonlinear dynamics for reliable subsequent deployment in MPC. Specifically: â€¢ We present a data-driven vehicle dynamics model that combines a physics-based model and last-layer Bayesian meta-learning model. By learning expressive neural network features and only adapting the last layer, the model is conducive to rapid online adaptation and offers a structured and interpretable uncertainty representation. â€¢ We present an active information gathering method to identify an accurate model as fast as possible while satisfying constraints during data collection. â€¢ We extensively validate the proposed approach on a Toyota Supra in dynamic drifting maneuvers. In particular, results show that online adaptation of the meta-trained model may not be sufficient to reliably initiate a drift, and that active information gathering significantly helps quickly identifying a model and enabling robust drifting maneuvers. From a vehicle control perspective, our results demonstrate that executing drifting maneuvers is possible by adapting a model on autonomously-generated non-drifting trajectories. Specifically, information-rich trajectories do not necessarily involve drifting, and adapting on such data aids in robust execution of challenging drifting maneuvers."
