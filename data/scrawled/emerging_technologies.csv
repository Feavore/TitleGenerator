URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04629v1,Pushing Boundaries: Quantum-Enhanced Leader Election and the Limits of Consensus,"This work addresses the complexities involved in designing distributed quantum algorithms, highlighting that quantum entanglement does not bypass the Fischer-Lynch-Paterson (FLP) impossibility theorem in asynchronous networks. Although quantum resources such as entanglement offer potential speedups, the inherent constraints of classical communication remain. We develop a leader election algorithm as a proof of concept, demonstrating how entanglement can enhance efficiency while still contending with asynchronous delays. This algorithm serves as a foundation for a broader blueprint for future distributed quantum algorithms, providing insights into both the real performance gains and the limitations that entanglement offers in a distributed setting.","Reducing the processing time for hard problems is one of the most critical issues for computing. This is achievable in two complementary ways: by designing efficient algorithms with better time complexity, and by increasing the computing power of the system that runs the algorithms. This article focuses on the latter in the context of quantum computing. Quantum computing increases computational power through quantum advantage: in comparison with a traditional computer, it offers a considerable speedup of processing time for selected problems. Quantum computing is now a reality, with the advent of practical quantum computers based on different technological platforms like superconducting systems [1], trapped ions [2] and photonic systems [3]. The efficiency of these quantum computers is evolving fast, with the expectation that they will become commercially useful in the near future. As happened in the past with classical computers, however, a quantum computer is perceived as a stand-alone unit for the centralized execution of an algorithm. Another way of increasing computational power is to distribute computations over multiple processors. Every processor handles an independent portion of the computation concurrently, and shares its partial results with all the other processors so they can form the complete results together. Remote processors must reach consensus in order to achieve a consistent view of results, for instance to agree on the value of shared data, or to dispatch tasks among processors. Distributed quantum computing is a combination of both paradigms: a set of quantum computers cooperate over a network to increase their common computing power. The literature offers several distributed versions of quantum algorithms [4, 5, 6, 7]. However, all of these remain theoretical; they overlook critical distributed computing constraints. Chief among these is that exchanges of data between processors introduce delays, which are an obstacle to consensus. This work contributes two significant results that address these challenges at the intersection of distributed and quantum computing. It addresses the complexities of designing distributed quantum algorithms, demonstrating that quantum entanglement does not circumvent the limitations posed by the FLP impossibility in asynchronous networks. As a proof of concept, we develop a quantum-enhanced leader election algorithm that leverages GHZ (Greenberger–Horne–Zeilinger) states to improve efficiency while factoring in communication delays. This algorithm serves as a foundation for a blueprint, which we present for future distributed quantum algorithms, outlining practical design principles that account for asynchrony, classical communication constraints, and entanglement advantages. Our findings illustrate both the potential gains and the limitations of entanglement in distributed quantum systems, providing insights into the conditions for efficient operation and the enduring challenges of achieving consensus in quantum-enhanced architectures. The remainder of this paper is organized as follows. Section II provides an overview of foundational concepts in distributed systems, setting the stage for the challenges of distributed quantum computing. Section III examines classical leader election algorithms to illustrate the constraints that distributed quantum algorithms must address. Section IV defines the hardware and operational requirements for integrating quantum components in a classical network. Section V introduces our quantum-enhanced leader election algorithm, describing its use of GHZ states to improve efficiency while factoring in communication delays. In Section VI, we provide a blueprint for designing future distributed quantum algorithms. Finally, Section VII summarizes our contributions and draws potential directions for future research."
https://arxiv.org/html/2411.04354v1,Impact of white noise in artificial neural networks trained for classification: performance and noise mitigation strategies,"In recent years, the hardware implementation of neural networks, leveraging physical coupling and analog neurons has substantially increased in relevance. Such nonlinear and complex physical networks provide significant advantages in speed and energy efficiency, but are potentially susceptible to internal noise when compared to digital emulations of such networks. In this work, we consider how additive and multiplicative Gaussian white noise on the neuronal level can affect the accuracy of the network when applied for specific tasks and including a softmax function in the readout layer. We adapt several noise reduction techniques to the essential setting of classification tasks, which represent a large fraction of neural network computing. We find that these adjusted concepts are highly effective in mitigating the detrimental impact of noise.","Artificial neural networks (ANNs) play an important role in various fields such as pattern recognition, data analysis, industrial control and complex problem solving. They can be trained on a large amount of data and take reasonably balanced decision based on this information. With these capabilities, they are successfully used in medicine Amato et al. (2013); Salahuddin et al. (2022); Sarvamangala and Kulkarni (2022); Celard et al. (2023), finance Lazcano, Herrera, and Monge (2023); Li, Wang, and Yang (2023), technology Mortaza Aghbashlo and Mujumdar (2015); Almonacid et al. (2017); Yang, Cui, and Gu (2023), biology Marabini and Carazo (1994); Suzuki (2011); Samborska et al. (2014), and for predicting the behavior of complex, chaotic systems Wang et al. (2024). The use of ANNs makes it possible to accelerate and improve decision-making processes, which significantly increases the efficiency and accuracy of systems. However, substantial challenges with energy efficiency, speed and scalability of ANNs are some of the main limitations of their application in various fields Marković et al. (2020). For example, complex tasks such as large-volume or real-time data processing require significant computing resources and hence energy. In addition, augmenting ANNs to find complex patterns can lead to problems with scalability and computing resource management Christensen et al. (2022). Motivated by these limitations, there is a growing amount of research into developing more energy-efficient ANN architectures, optimizing computation and developing specialized hardware to perform ANN operations, called hardware neural networks Seiffert (2004); Misra and Saha (2010); Bouvier et al. (2019). These efforts aim at improving the efficiency and performance of ANNs and open the prospect of their wider application in various fields. In hardware ANNs, also called in memory computing, the artificial neurons and connections between them are based on physical principles such as optical Wang et al. (2022); Ma et al. (2023), memristive Tuma et al. (2016); Lin et al. (2018); Xia and Yang (2019), spin-torque Tor (2017), Mach–Zehnder interferometer Shen et al. (2017); Cem et al. (2023), photoelectronic Chen et al. (2023) or coherent silicon photonics Mourgias-Alexandris et al. (2022) effects. A major consequence of such networks in comparison to digital network emulations is that analogue hardware is always prone to a certain level of noise. Noise in hardware ANNs can arise from a variety of sources. Regardless of its origin, such noise can have a negative impact on the accuracy of the ANN, as it can cause errors in the transmission and transformation of information. There is a substantial amount of literature on mitigation strategies for noise in the input signal of ANNs Maas et al. (2012); Burger, Schuler, and Harmeling (2012); Seltzer, Yu, and Wang (2013); Yue et al. (2022), but in the case of hardware ANNs the general context changes as here noise arises internally of the network. There are several papers that describe the properties of various internal noise sources in hardware ANNs Dolenko and Card (1993); Dibazar et al. (2006); Soriano et al. (2015); Janke and Anderson (2020); Nurlybayeva et al. (2022); Ma et al. (2023). In our previous papers Semenova et al. (2019); Semenova, Larger, and Brunner (2022), we have studied the impact of white Gaussian internal noise on simplified and trained neural networks with linear Semenova et al. (2019) and nonlinear Semenova, Larger, and Brunner (2022) activation functions. Moreover, we developed an analytical description for predicting the noise level in the output of ANN with internal noise. Further, in Ref. Semenova and Brunner (2022), we proposed several techniques how to reduce different types of internal noise on untrained networks. However, this does not include the essential step of generalizing these techniques towards application scenarios. This essential development we report here, and focus on the hardware ANN’s performance in the context of classifications tasks, with the commonly applied softmax function in the final layer. Again, we consider a wide range of different noise types, yet the impact of thresholding for classification, as well as the probabilistic transformation through the softmax demand substantial innovation with regards to analysis as well as noise mitigation techniques. This paper starts explaining the considered trained deep ANN (Sect. II.1). We consider the impact of several cases of white Gaussian noise in the deep hardware ANN trained for digit recognition, and we evaluate the noise impact in terms of accuracy degradation (Sect. III). Noise types are additive and multiplicative, correlated and uncorrelated (they are explained in details in Sect. II.2). We then apply two noise mitigation techniques to different cases of internal noise and explain how these techniques can be realized schematically and in terms of connection matrices for already trained network (Sect. IV, V). In Supplementary materials we suggest how the connection matrices can be modified according to these techniques using Python code as an example. The main difference from our previous work is that here we consider different noise intensities rather than one set, which allows us to increase the range of application of the results to different types of hardware ANNs. In addition, we previously considered the effect of noise from the point of view of only the signal-to-noise ratio (SNR) of the output signal. In the case of classifying ANNs, this is not entirely correct, since the last layer of such networks often uses the softmax function, for which it is not the output signal itself that is important, but the sequence number of the neuron with the maximum output signal. In this article, we consider the impact of various noises of different intensities on the accuracy of the classification network. Therefore, the conclusions proposed in this article are fundamentally new comparing to all our previous works."
https://arxiv.org/html/2411.04224v1,WiFlexFormer: Efficient WiFi-Based Person-Centric Sensing,"We propose WiFlexFormer, a highly efficient Transformer-based architecture designed for WiFi Channel State Information (CSI)-based person-centric sensing. We benchmark WiFlexFormer against state-of-the-art vision and specialized architectures for processing radio frequency data and demonstrate that it achieves comparable Human Activity Recognition (HAR) performance while offering a significantly lower parameter count and faster inference times. With an inference time of just 10 ms on an Nvidia Jetson Orin Nano, WiFlexFormer is optimized for real-time inference. Additionally, its low parameter count contributes to improved cross-domain generalization, where it often outperforms larger models. Our comprehensive evaluation shows that WiFlexFormer is a potential solution for efficient, scalable WiFi-based sensing applications. The PyTorch implementation of WiFlexFormer is publicly available at: https://github.com/StrohmayerJ/WiFlexFormer.","WiFi has emerged as a promising modality in person-centric sensing due to its advantages over optical approaches, including cost-effectiveness, unobtrusiveness, visual privacy protection, and the ability to perform long-range sensing through walls [15, 4]. Together, these characteristics enable efficient, contactless monitoring of human activities in confined indoor environments without the need for per-room sensor deployment, representing a significant economic advantage [18]. Channel State Information (CSI) serves as the foundation for modern WiFi-based person-centric sensing. CSI is a metric obtained in the Orthogonal Frequency-Division Multiplexing (OFDM) scheme, which subdivides a WiFi channel into multiple sub-channels with different carrier frequencies (subcarriers) [7]. This subdivision allows for fast, parallel transmission of data, while CSI provides detailed information about how each subcarrier is affected by the environment, enabling the correction of environment-induced noise at the receiver on a per-subcarrier basis, and through correlating the distinctive patterns of amplitude attenuation and phase shifts in CSI caused by specific human movements, applications such as Human Activity Recognition (HAR) [12]. While existing approaches to CSI-based HAR often rely on generic CNN-based vision architectures, they are not optimal due to their focus on local dependencies and shift-invariance [25]. To effectively leverage the unique properties of CSI, specialized architectures have been developed [2, 11, 25]. However, they tend to suffer from other problems such as overly complex designs and reliance on computationally expensive features, resulting in high inference times and limited practicality for real-time applications. Contributions. To address these challenges, we make the following contributions: (I) We propose WiFlexFormer, a Transformer-based architecture that achieves similar HAR performance with significantly lower parameter count and inference time, making it highly efficient and well-suited for WiFi-based real-time person-centric sensing applications. (II) We conduct comprehensive evaluations of WiFlexFormer on publicly available WiFi datasets, assessing its HAR performance using amplitude and Doppler Frequency Shift (DFS) features and comparing it against existing state-of-the-art architectures. (III) We investigate the effectiveness of various subcarrier sub-sampling strategies to further optimize inference speed while maintaining model performance."
https://arxiv.org/html/2411.03859v1,UniTraj: Universal Human Trajectory Modeling from Billion-Scale Worldwide Traces,"Human trajectory modeling is essential for deciphering movement patterns and supporting advanced applications across various domains. However, existing methods are often tailored to specific tasks and regions, resulting in limitations related to task specificity, regional dependency, and data quality sensitivity. Addressing these challenges requires a universal human trajectory foundation model capable of generalizing and scaling across diverse tasks and geographic contexts. To this end, we propose UniTraj, a Universal human Trajectory foundation model that is task-adaptive, region-independent, and highly generalizable. To further enhance performance, we construct WorldTrace, the first large-scale, high-quality, globally distributed dataset sourced from open web platforms, encompassing 2.45 million trajectories with billions of points across 70 countries. Through multiple resampling and masking strategies designed for pre-training, UniTraj effectively overcomes geographic and task constraints, adapting to heterogeneous data quality. Extensive experiments across multiple trajectory analysis tasks and real-world datasets demonstrate that UniTraj consistently outperforms existing approaches in terms of scalability and adaptability. These results underscore the potential of UniTraj as a versatile, robust solution for a wide range of trajectory analysis applications, with WorldTrace serving as an ideal but non-exclusive foundation for training.","Human trajectory data, which captures the movement paths of individuals or groups over time, has become increasingly significant in various domains such as transportation management (Luca et al., 2021), logistics optimization (Guo et al., 2018), and web-based services (Zheng, 2015). With the widespread adoption of GPS-enabled devices and the integration of positioning technologies into numerous applications, vast amounts of trajectory data are generated daily from vehicles and mobile devices connected to the Internet (Wang et al., 2022; Lan et al., 2022; Liu et al., 2024). This type of data permits us an unprecedented opportunity to analyze movement patterns, traffic flow, and user mobility behaviors, supporting a range of applications from real-time traffic updates to location-based services and personalized content recommendations (Chen et al., 2024). \Description [] Figure 1. Overview of this work, we propose a trajectory foundation model and also collect a worldwide trajectory dataset. The pre-trained UniTraj can be used as a backbone while adapters are trained for different regions and tasks. To effectively harness this wealth of data, robust human trajectory modeling techniques are essential to extract meaningful insights. Modeling trajectories allows us to convert raw location data into actionable information, uncovering human mobility patterns across spatial and temporal dimensions to enable advanced applications in diverse fields (Chang et al., 2023). From a task-oriented perspective, existing methods often utilize various statistical and machine learning techniques (e.g., CNNs and RNNs) to capture detailed spatio-temporal features (Li et al., 2024). These models are typically optimized for specific tasks, with algorithms and architectures tailored to address distinct challenges such as trajectory prediction, anomaly detection, and activity recognition (Lin et al., 2024b). On the data side, researchers employ a wide array of trajectory datasets gathered from sources like vehicles, mobile devices, and other GPS-enabled equipment (Chen et al., 2024; Luca et al., 2021). These datasets vary significantly in size, geographic coverage, and quality, providing essential support for model development and evaluation. Collectively, these task-specific modeling efforts and diverse data sources have propelled advancements in human trajectory analysis, deepening our understanding of mobility behavior. Despite these advancements, existing methods face significant limitations that impede their generalizability and practical applicability: Despite these advancements, existing methods face (i) Task Specificity: Current approaches are typically designed and optimized for specific tasks, lacking the flexibility to adapt across different applications without extensive modifications. This task-centric focus restricts their reusability across a range of trajectory-related problems, including prediction, classification, and anomaly detection. (ii) Regional Dependency: Many models are developed and trained on data from specific geographic regions, limiting their effectiveness when applied to trajectories from diverse locations. Variations in infrastructure, traffic patterns, and behaviors across regions mean that models confined on narrow geographic data often fail to capture the diversity essential for global trajectory datasets, thus struggling to generalize to new environments. (iii) Data Quality Sensitivity: Real-world trajectory data is inherently heterogeneous, with variability in sampling rates, noise levels, and occasional missing data due to differences in data collection criteria and device capabilities. Existing models are typically sensitive to these inconsistencies, leading to degraded performance when faced with noisy or incomplete data. This sensitivity requires extensive data preprocessing and cleaning, which may not always be practical, reducing the robustness of these models in real-world scenarios. What measures can be taken to overcome these limitations? Empirically, developing a task-adaptive, region-independent, and scalable foundation model for universal trajectory modeling is both an emerging necessity and a promising trend (Zhang et al., 2024). As shown in Figure 1, such a model can generalize across various tasks without requiring specialized models for each application, thereby enhancing scalability and efficiency. Additionally, a foundation model can effectively handle diverse data qualities, making it adaptable to real-world scenarios where data variability is the norm. However, constructing a universal trajectory foundation model presents two primary challenges: • Data preparation: Constructing a foundation model requires the collection and integration of vast amounts of high-quality trajectory data, covering different geographic regions, sampling rates, and user behaviors. However, most existing datasets are primarily held by a limited number of companies or organizations with proprietary rights or restrictive access policies, hindering widespread usage and collaborative research. Furthermore, the labor and financial costs associated with data collection make obtaining large-scale, high-quality trajectory datasets particularly difficult. As a result, available datasets (such as GeoLife (Zheng et al., 2009) and Porto (Meghan O’Connell, 2015)) are often restricted to specific regions or cities, reducing their generalizability and constraining research aimed at broader, global applications. • Model Design: A universal trajectory foundation model must be equipped several capabilities that current approaches lack. First, the model should be capable of generalizing across diverse spatio-temporal contexts, enabling it to serve as a backbone that can be adapted to a wide range of tasks without extensive modifications. Second, it must maintain robust representation capabilities to handle data with varying qualities, demonstrating resilience to noise, missing values, and inconsistent sampling rates. Finally, the model should balance complexity and computational efficiency, avoiding overfitting to specific data patterns while remaining scalable for large datasets. With these challenges in mind, we introduce WorldTrace, the first large-scale, high-quality, globally distributed trajectory dataset sourced from open platforms. Spanning 2.45 million trajectories with billions of points across 70 countries, WorldTrace overcomes the limitations of existing datasets by offering extensive geographic coverage, diverse sampling rates, and accessible data, thereby calling widespread use and collaboration. Meanwhile, we present UniTraj, a Universal human Trajectory foundation model designed to be task-adaptive, region-independent, and resilient to varying data quality. UniTraj can serve as a versatile backbone capable of supporting diverse trajectory analysis tasks without dependence on a specific dataset, though it achieves optimal performance when trained on high-quality, diverse data like WorldTrace. In addition, our approach employs advanced pre-training techniques, including multiple resampling and masking strategies, which enable UniTraj to capture complex spatio-temporal dependencies and adapt to heterogeneous data characteristics across regions and sampling frequencies. This design promotes robust generalization across tasks and regions, offering a scalable and efficient solution for a wide range of trajectory analysis applications. In summary, the contributions of our research are as follows: • We construct the first large-scale, high-quality, globally distributed trajectory dataset, called WorldTrace. This dataset overcomes the limitations of existing datasets by offering accessible data for widespread use and collaboration, facilitating research with a broader global perspective, and supporting the development of universal trajectory models. • We propose UniTraj, a universal human trajectory foundation model that leverages advanced techniques such as multiple resampling and masking strategies. UniTraj can serve as a backbone to captures complex spatio-temporal dependencies and adapts to heterogeneous data characteristics across different regions and sampling rates. • We conduct extensive experiments across diverse trajectory analysis tasks and real-world datasets, demonstrating the scalability and adaptability of UniTraj. Additionally, we validate the unique advantages of WorldTrace, highlighting its potential as an ideal dataset for building robust and generalizable trajectory models."
https://arxiv.org/html/2411.03557v1,Shem: A Hardware-Aware Optimization Framework for Analog Computing Systems,"As the demand for efficient data processing escalates, reconfigurable analog hardware which implements novel analog compute paradigms, is promising for energy-efficient computing at the sensing and actuation boundaries. These analog computing platforms embed information in physical properties and then use the physics of materials, devices, and circuits to perform computation. These hardware platforms are more sensitive to nonidealities, such as noise and fabrication variations, than their digital counterparts and accrue high resource costs when programmable elements are introduced. Identifying resource-efficient analog system designs that mitigate these nonidealities is done manually today.While design optimization frameworks have been enormously successful in other fields, such as photonics, they typically either target linear dynamical systems that have closed-form solutions or target a specific differential equation system and then derive the solution through hand analysis. In both cases, time-domain simulation is no longer needed to predict hardware behavior. In contrast, described analog hardware platforms have nonlinear time-evolving dynamics that vary substantially from design to design, lack closed-form solutions, and require the optimizer to consider time explicitly. We present Shem, an optimization framework for analog systems. Shem leverages differentiation methods recently popularized to train neural ODEs to enable the optimization of analog systems that exhibit nonlinear dynamics, noise and mismatch, and discrete behavior. We evaluate Shem on oscillator-based pattern recognizer, CNN edge detector, and transmission-line security primitive design case studies and demonstrate it can improve designs. To our knowledge, the latter two design problems have not been optimized with automated methods before.","There has been an emergence of new workloads that place extreme power constraints on the hardware and require processing near the sensing and actuation interfaces (lequepeys2021overcoming, ; outeiral2021prospects, ; bayerstadler2021industry, ; irimia2012green, ). Analog computing systems are a promising class of hardware that can perform processing directly on analog signals domain and often at very low energy, enabling processing large amounts of analog data with little digitization (Decadal, ; murmann2020a2i, ). Analog computing systems encode information in physical signals (e.g., voltage) and then leverage the dynamics of materials, devices, and circuits to perform computation. Modern analog computing systems are reconfigurable and implement novel analog computational paradigms, such as oscillator-based computing and cellular nonlinear networks, which are inherently nonlinear and often use non-standard physical properties (ryynanen2001dual, ; gangopadhyay2014compressed, ; mehonic2020memristors, ; konatham2020real, ; sebastian2020memory, ). Ordinary differential equations capture the semantics of the analog compute paradigm and executing computations involves solving or simulating the differential equation system. Design Challenges. Identifying a resource-efficient, system-level design for this class of analog hardware that delivers acceptable fidelity remains a significant challenge and is primarily performed manually. First, analog hardware is sensitive to nonidealities that affect the fidelity of the computation, such as noise, fabrication-induced parameter variations, and environmental sensitivities. Second, the digital interface circuitry used to program the hardware and perform digital/analog conversion increases resource usage, especially with increasing precision (cowan2005vlsi, ; huang2017hybrid, ; tsividis2018analog-computer, ; guo2016hybrid-computer, ; achour2020Legno, ). Reducing the precision of these digital elements and the degree of programmability significantly reduces the complexity of the design. Enabling automated optimization of these analog systems would enable identification of useful design. 1.1. Existing Design Optimization Methods Design optimization tools find design parameterizations that minimize some cost function, which captures the end-to-end system-level property the designer desires. Gradient-based optimization methods have been extensively used in photonics (hughes2018adjoint, ; su2020nanophotonic, ; molesky2018inverse, ; li2022physics, ; li2023lightridge, ; molesky2018inverse, ). Because these optimizers use gradient information to drive the search intelligently and are built on heavily accelerated ML frameworks, designs of tens or even hundreds of thousands of design variables can be targeted (kang2024large, ; piggott2020inverse, ). Gradient-based optimizers have also been used to optimize analog circuits to a lesser degree; these methods optimize SPICE circuits to minimize specific circuit metrics (e.g., delay) and typically leverage optimization techniques that specifically work with circuit schematics (rohrer1967fully, ; director1969generalized, ; conn1998jiffytune, ; conn1999gradient, ; visweswariah2000noise, ; joshi2017analog, ; hu2020adjoint, ; li2023circuit, ). Limitations. The gradient-based methods devised in these prior works are insufficient for optimizing analog computing systems. To be able to use these methods, the gradient of the cost function must be taken. Previously developed optimizers primarily target linear dynamical systems that have closed-form solutions or target a specific differential equation system that has a hand-derived solution. In both cases, time-domain simulation is no longer needed to predict hardware behavior, and the gradient is relatively straightforward to compute. In contrast, this class of analog systems has nonlinear dynamics that rarely have analytic solutions, so gradient needs to be taken over the time domain simulation of the system. Analog systems also experience nonidealities, such as fabrication-induced errors and noise that introduce stochasticity into the system’s dynamics. This stochastic behavior is not inherently differentiable and interacts with the system’s nonlinearities, producing complex behaviors. In addition, digital logic that exists at the programming and measurement interfaces of the analog system is inherently discrete and, therefore, also not readily differentiable. These behaviors make the system even more challenging to differentiate. 1.2. Optimization of Analog Systems with Shem We present Shem, an optimization framework for analog systems that directly optimizes over time-domain differential equation models with nonlinear dynamics. Shem leverages the adjoint method, a differentiation method recently popularized to train neural ODEs, to directly differentiate over ordinary differential equation simulations and find the gradient of the cost function (kidger2022neural, ; chen2018neural, ; li2020scalable, ). Shem deploys a translation pass that makes noise, mismatch, and digital logic differentiable so the gradient may be taken. We build Shem on the JAX machine learning framework, which supports auto-differentiation and offers highly optimized backends for auto-parallelization and hardware acceleration, enabling scalable design optimization (jax2018github, ). Shem inherits the programming conveniences offered by JAX, enabling Shem to target a range of analog system designs and optimize complex cost functions that evaluate system-level properties such as end-to-end error. 1.2.1. Contributions • We introduce techniques that enable time-domain design optimization of analog systems and differentiation over noise, fabrication variations, and digital logic. • We present Shem, a framework built on JAX that supports auto-differentiation and optimization of nonlinear time-domain analog systems. • We evaluate Shem on oscillator-based pattern recognizer, cellular nonlinear network edge detector, and transmission-line security primitive design case studies and demonstrate it can improve designs. To our knowledge, the latter two design problems have not been optimized with automated methods before."
https://arxiv.org/html/2411.03892v1,Two Sides of the Same Coin: Large-scale Measurements of Builder and Rollup after EIP-4844,"Web3 is reshaping decentralized ecosystems through innovations like Ethereum. Recently, EIP-4844 is implemented in Ethereum to support its Layer-2 scaling solutions, which introduces a new 128 KB data structure called blob. This upgrade incorporates type-3 transactions with blobs to verify data availability and reduce gas costs for rollups, significantly affecting the strategies of both builders and rollups. In this paper, we present an in-depth study of emerging strategies in builder and rollup markets after EIP-4844, containing hundred million transactions. We find that the efficiency of builder and rollup strategies is interdependent, akin to two sides of the same coin—both cannot be optimized simultaneously. That is, when builders operate efficiently, rollups tend to overpay in fees, conversely, when rollups optimize their costs, builders may incur losses in inefficient transaction selection. From the side of builders, our results show that 29.48% of these blocks have been constructed inefficiently, which does not produce sufficient profits for builders. Through our evaluation from the side of rollups, we find that over 72.53% of type-3 transactions pay unnecessary fees, leading to notable economic costs of rollups. Our work provides critical insights into optimizing block construction and transaction strategies, advancing the economic efficiency and data scalability of Web3 infrastructures, yet, much like balancing a seesaw, the efficiency of builders and rollups cannot be optimized concurrently.","Web3 leverages decentralized technologies such as blockchain to enable peer-to-peer interactions without intermediaries. A groundbreaking and transformative application of decentralized technology is Ethereum, an open and decentralized blockchain platform that facilitates the development and execution of smart contracts and decentralized applications (Gilbert, 2022; Wang et al., 2022b). In the Ethereum mainnet, the Proposer Builder Separation (PBS) mechanism was proposed to separate the role of the miner by introducing the builder and the proposer (Buterin, 2021a). The task of builders is to construct the block, where they sort the transactions to maximize the extractable value and submit the block to proposers (Daian et al., 2020; Heimbach and Wattenhofer, 2022). Each transaction within the block originates from two sources: the public mempool, which is accessible to everyone, and private channels that deliver transactions directly to specific builders (Qin et al., 2022). Ethereum Layer-2 refers to scaling solutions built on top of the Ethereum mainnet to enhance its performance and throughput, with rollups being one of the most prominent approaches that bundle multiple transactions into a single batch before submitting them to the mainnet. Ethereum has implemented EIP-4844 in the Dencun upgrade to improve network scalability and lower transaction costs (Ethereum.org, 2024a). This upgrade presents blob, a new data format of 128 KB designed to facilitate rollups on Ethereum’s Layer-2, alongside blob-carrying transactions known as type-3 transactions (Vitalik Buterin, 2023). By offloading data storage to blobs, the gas fee is significantly reduced for rollups. Type-3 transactions are much larger and therefore take more space in blocks (Park et al., 2024; Gomez, 2024b). Blocks with large size take longer to propagate through the network. On the one side, builders need to balance the block size with the profit they can obtain (Wahrstätter, 2024b; Gomez, 2024a). They also need to decide whether to choose type-3 transactions in their blocks. This results in different behaviors among builders in response to type-3 transactions. On the other side, rollups need to issue their blobs into type-3 transactions and wait for being included in blocks. Type-3 transactions always carry the different number of blobs and different fees, which indicate various strategies of rollups (Cui, 2024). Evidently, the strategies among rollups are also complicated. We evaluate the strategies of builders and rollups after EIP-4844. We find that the efficiency of their strategies is like the two sides of the same coin. When the builders strategy is efficient, rollups may need to pay more fees. Oppositely, when rollups adopt the efficient strategy and offer lower fees, builders may face block profits lower than potential profits. In the side of builders, we first examine the new strategy that builders employed after EIP-4844 (Section 4). Although the average total block size has been increasing to nearly 400 KB, the size of transactions from public mempool in the block has decreased from 150 KB before the upgrade to about 30 KB. Builders have taken to shrinking the size of the rest of the block to contain type-3 transactions, which prompts us to delve deeper into the efficiency of this strategy. We recognize that the builder strategy is not efficient enough after EIP-4844. In the side of rollups, they need to pay higher fees to ensure inclusion in efficient blocks. However, we find that most type-3 transactions issued by rollups pay too high fees (Section 5). Our primary contributions are: (1) We build a large-scale dataset to quantify builders and rollups market in Ethereum from after EIP-4844 to August, which includes 319,529,950 transactions with 1,336,822 type-3 transactions. To the best of our knowledge, we are the first to illuminate different strategies and efficiencies of builders and rollups in the network after EIP-4844. Our dataset provides the most complete labelling of builders and rollups so far. (2) We identify the strategies that different builders adopt after EIP-4844. By examining the size of transactions versus the profit paid to builders, we provide a model for measuring efficient blocks for builders. We find that 29.48% of the blocks containing type-3 transactions do not give the builder enough profits to compensate for the loss of excluding other transactions. This will assist builders in improving their strategies for constructing blocks for higher profit margin. (3) We explore the sending patterns of different rollups for blob and type-3 transactions and find that the behavior within the market is distinctive. We measure efficient type-3 transactions for rollups and observe that 72.53% of type-3 transactions in the market are inefficient. This work contributes to how rollups can optimize their pricing of type-3 transactions to achieve cost savings. (4) Furthermore, we evaluate the losses of other flawed strategies in the market and reveal significant losses of 186.92 ETH for rollups. We also characterize the impact of one misuse of blobs. It induces a delay of over 19 seconds in the type-3 transactions of rollups."
https://arxiv.org/html/2411.02945v1,Instant Resonance: Dual Strategy Enhances the Data Consensus Success Rate of Blockchain Threshold Signature Oracles,"With the rapid development of Decentralized Finance (DeFi) and Real-World Assets (RWA), the importance of blockchain oracles in real-time data acquisition has become increasingly prominent. Using cryptographic techniques, threshold signature oracles can achieve consensus on data from multiple nodes and provide corresponding proofs to ensure the credibility and security of the information. However, in real-time data acquisition, threshold signature methods face challenges such as data inconsistency and low success rates in heterogeneous environments, which limit their practical application potential. To address these issues, this paper proposes an innovative dual-strategy approach to enhance the success rate of data consensus in blockchain threshold signature oracles. Firstly, we introduce a Representative Enhanced Aggregation Strategy (REP-AG) that improves the representativeness of data submitted by nodes, ensuring consistency with data from other nodes, and thereby enhancing the usability of threshold signatures. Additionally, we present a Timing Optimization Strategy (TIM-OPT) that dynamically adjusts the timing of nodes’ access to data sources to maximize consensus success rates. Experimental results indicate that REP-AG improves the aggregation success rate by approximately 56.6% compared to the optimal baseline, while the implementation of TIM-OPT leads to an average increase of approximately 32.9% in consensus success rates across all scenarios.","As a core component of blockchain data interoperability, blockchain oracles play a crucial role in the acquisition and transmission of off-chain data, significantly advancing the development of blockchain applications [1, 2]. In recent years, the rapid growth of fields such as Decentralized Finance (DeFi) [3, 4] and Real World Assets (RWA) [5, 6] has led to increasing demand for external real-time data, such as exchange rates and price information [7]. Furthermore, other blockchain applications, including supply chain management [8, 9, 10], the Internet of Things (IoT) [11, 12, 13], and smart cities [14], also rely on real-time data such as location and traffic flow to improve efficiency, transparency, and collaboration among stakeholders. In this context, oracles provide accurate and timely data, laying the foundation for the seamless integration of smart contracts in heterogeneous data environments while significantly enhancing the reliability and functionality of blockchain applications. To ensure data credibility, blockchain oracles are typically composed of multiple distributed nodes that gather information from various data sources. Initially, the nodes collect data from multiple sources and perform preliminary aggregation using common methods such as median [15, 16, 17, 18], majority voting [19], and weighted averaging [20, 21]. These aggregation techniques aim to improve the representativeness of the data and mitigate the influence of malicious data sources. Subsequently, the distributed nodes engage in a further consensus on the aggregated data to ensure the reliability of the result. In this process, threshold signatures serve as a robust data consensus mechanism, ensuring that a valid signature is generated only when a predetermined number of nodes reach agreement. It effectively prevents the freeloading problem and provides strong cryptographic security guarantees, allowing the consensus results to be reliably verified. Consequently, threshold signatures have been widely adopted in various commercial projects, such as Chainlink [22] and the DOS Network [23], becoming a critical technology for ensuring data credibility and security. Figure 1: Threshold signature fails consensus when obtaining real-time data. However, the timing of data retrieval by distributed heterogeneous nodes from distributed data sources is inconsistent, leading to heterogeneity in the acquired real-time data, which complicates the consensus requirements of threshold signatures. As illustrated in Figure 1, different nodes may access information from the same data source at different times, resulting in discrepancies in both the timestamps and content of the data. This inconsistency significantly reduces the success rate of consensus in threshold signatures, affecting their usability in real-time data acquisition applications. Therefore, improving the consistency of aggregated data among nodes in real-time data retrieval tasks to ensure the usability of threshold signatures is the primary research objective of this paper. In this paper, we propose two innovative strategies to improve the data consensus success rate of blockchain threshold signature oracles. First, we introduce a novel data aggregation method called the Representative Enhanced Aggregation Strategy (REP-AG), which aims to improve the representativeness of the data aggregated by nodes, ensuring consistency with the aggregated data of other nodes and thereby enhancing the usability of threshold signatures. Second, we design a Timing Optimization Strategy (TIM-OPT) that adjusts the timing of node access to data sources and data distribution, thus increasing the success rate of data consensus. The main contributions of this paper are as follows: 1. We propose a novel data aggregation method, REP-AG, which models the data aggregation process of nodes under incomplete information as a Bayesian game and solves it. This approach significantly improves the consistency of aggregation results among nodes, thereby enhancing the success rate of data consensus. 2. We design a Timing Optimization Strategy TIM-OPT that introduces an appropriate waiting time before nodes access data sources, utilizing Bayesian game methods. This strategy effectively increases the concentration of data among nodes, thereby improving the success rate of data consensus. 3. Experiments show that under the same environmental assumptions, REP-AG improves the consensus success rate by approximately 56.6 % compared with the optimal baseline, and the consensus success rate of all schemes after the application of TIM-OPT increases by approximately 32.9 % on average. The remainder of this paper is structured as follows: Section 2 introduces the related works and existing challenges. Section 3 presents the system workflow and details of the proposed approach. Section 4 provides the experimental results and analysis. Finally, Section 5 concludes the paper and outlines future research directions."
https://arxiv.org/html/2411.02684v1,"Towards Intelligent Augmented Reality (iAR): A Taxonomy of Context, an Architecture for iAR, and an Empirical Study","Recent advancements in Augmented Reality (AR) research have highlighted the critical role of context awareness in enhancing interface effectiveness and user experience. This underscores the need for intelligent AR (iAR) interfaces that dynamically adapt across various contexts to provide optimal experiences. In this paper, we (a) propose a comprehensive framework for context-aware inference and adaptation in iAR, (b) introduce a taxonomy that describes context through quantifiable input data, and (c) present an architecture that outlines the implementation of our proposed framework and taxonomy within iAR. Additionally, we present an empirical AR experiment to observe user behavior and record user performance, context, and user-specified adaptations to the AR interfaces within a context-switching scenario. We (d) explore the nuanced relationships between context and user adaptations in this scenario and discuss the significance of our framework in identifying these patterns. This experiment emphasizes the significance of context-awareness in iAR and provides a preliminary training dataset for this specific Scenario.","1 Related Work In today’s life, the growing dependence on personal computing devices such as mobile phones for decision-making and task performance has led to challenges, such as disruptions to social interactions [65, 20, 14, 1, 39]. This has driven Ubiquitous computing’s goal of seamlessly embedding near-constant, yet unobtrusive, digital information into daily life [70, 69]. Research indicates that by integrating the information into the real world (RW), AR can enhance efficiency and memory retention and reduce such challenges [67, 18]. However, AR effectiveness depends on presenting the right information at the right time and form [9, 28, 36]. This work focuses on developing iAR systems that enhance user experience and efficiency by ensuring timely and appropriate information delivery in any context. Various studies, such as ARWin and ARBrowse, have demonstrated the benefits of AR for providing on-demand information, assistance, and entertainment, spanning from domestic use to workplace collaboration and productivity [43, 22, 45]. However, the effectiveness of AR interfaces is influenced by factors such as information overload, visual clutter, obtrusiveness, and distractions, all of which can negatively affect situational awareness, cognitive load, and performance [44, 7, 25, 35, 66]. For instance, in social contexts, poorly designed AR interfaces can obstruct communication by occluding facial expressions, leading to social isolation and privacy concerns [38, 37, 3, 33]. This has led to research on AR design principles for non-invasive AR and view management that provide socially relevant information while maintaining visual focus on interlocutors [41, 57, 19]. The RW Spatial setting and occlusion also significantly impact AR effectiveness [50, 48, 15, 24]. Extensive research has proposed AR design principles to address occlusion between RW and virtual objects [47, 63, 72, 8, 55]. These principles often emphasize view management techniques that ensure non-occlusive yet visible placement of virtual content [6, 4, 34, 54], as well as peripheral placement approaches [11, 53, 51]. Other strategies prioritize the real world, activating virtual content only when necessary through user-triggered adaptations, such as transparency, layout, or level of detail (LoD) [27, 26]. This work examines the underexplored impact of RW spatial settings and occlusions on AR effectiveness, focusing on user-specified AR adaptations in context-switching scenarios. An effective AR interface must include considerations such as positioning, transparency, and LoD for timely and appropriate information presentation in a given context. Pervasive AR envisions “continuous, universal, and omnipresent"" integration of AR content into various daily tasks [36]. Non-adaptive AR interfaces, which fail to adjust to user context, risk presenting information inappropriately. Achieving Pervasive AR requires a deep understanding of the user’s changing context, task, and environment to inform interface design. AR devices, equipped with sensors, offer unique opportunities for context detection, as seen in research assisting visually impaired users [56], highlighting AR’s potential to meet everyday information needs. Context-aware interfaces, such as ARWin and HoloDoc, propose design principles to optimize content presentation and enhance user experiences in specialized contexts using contextual data like user attention, spatial settings, lighting, and environmental cues [46, 68, 71]. However, an interface suitable in one scenario may be ineffective in another, as user priorities shift across contexts, requiring the interface to adapt accordingly [40, 23]. For instance, while providing relevant information to the user’s conversation can enhance user experience [65, 19], the user’s cognitive load may require low levels of information if they are engaged in a high-level collaborative task [59, 10]. User-triggered, context-aware interfaces allow manual selection of contextually-customized AR content presentation, offering predictability and control but increasing user effort [61, 60]. Recent research has focused on automatic adaptations to AR content without manual intervention, demonstrating the integration of unobtrusive context-aware AR into daily life [49, 45, 13]. To enable Pervasive AR, the interface must automatically detect and respond to contextual changes. However, current context-aware interfaces are limited in their adaptation scope, detect only a narrow range of contextual components, such as fatigue or social interactions, and use predefined design principles tailored to specific scenarios [12, 19]. This work investigates iAR interfaces capable of optimal adaptations in unfamiliar contexts without prior knowledge. Enabling iAR requires a framework that supports all potential AR adaptations, the detection and representation of the context, and its delivery to the decision-making process [58, 21]. Previous work identifies a design space of the potential adaptations to an AR interface [17]. Various approaches have been proposed for representing context, such as the 5 Ws (Who, What, Where, When, Why), or using categories like location, identity, time, and activity [58, 2, 62]. Others study specific sub-contexts such as physical environment [34, 13], objects [71, 42], background [52], and depth perception [30]. However, these taxonomies often overlook components essential to AR interface effectiveness, incorporate implicit elements that cannot be automatically detected or quantified by an iAR system, or lack clear frameworks for identifying implicit information essential for AR design. In this work, we reviewed existing research on context characterization and taxonomies and, through iterative refinement, identified key contextual information that influences AR performance. We propose a comprehensive taxonomy of quantifiable contextual components and a framework for iAR systems that use these components to infer the impact of various AR adaptations and make optimal adjustments in real-time."
https://arxiv.org/html/2411.02282v2,A Comprehensive Simulation Framework for CXL Disaggregated Memory,"Compute eXpress Link (CXL) has emerged as a key enabler of memory disaggregation for future heterogeneous computing systems to expand memory on-demand and improve resource utilization. However, CXL is still in its infancy stage and lacks commodity products on the market, thus necessitating a reliable system-level simulation tool for research and development. In this paper, we propose CXL-DMSim111Open sourced at https://github.com/ferry-hhh/CXL-DMSim., an open-source full-system simulator to simulate CXL disaggregated memory systems with high fidelity at a gem5-comparable simulation speed. CXL-DMSim incorporates a flexible CXL memory expander model along with its associated device driver, and CXL protocol support with CXL.io and CXL.mem. It can operate in both app-managed mode and kernel-managed mode, with the latter using a dedicated NUMA-compatible mechanism. The simulator has been rigorously verified against a real hardware testbed with both FPGA-based and ASIC-based CXL memory prototypes, which demonstrates the qualification of CXL-DMSim in simulating the characteristics of various CXL memory devices at an average simulation error of 4.1%. The experimental results using LMbench and STREAM benchmarks suggest that the CXL-FPGA memory exhibits a ∼similar-to\sim∼2.88×\times× higher latency than local DDR while the CXL-ASIC latency is ∼similar-to\sim∼2.18×\times×; CXL-FPGA achieves 45-69% of local DDR memory bandwidth, whereas the number for CXL-ASIC is 82-83%. We observed that the performance of CXL memory is 3×\times× more sensitive to Rd/Wr patterns than local DDR, with the max. bandwidth at 74%:26% rather than 50%:50% due to the current compromised CXL+DDR controller design. The study also reveals that CXL memory can significantly enhance the performance of memory-intensive applications, improved by 23×\times× at most with limited local memory for Viper and approximately 16% in bandwidth-sensitive scenarios such as MERCI. Moreover, the simulator’s observability and expandability are showcased with detailed case-studies, highlighting its great potential for research on future CXL-interconnected hybrid memory pool.","With the prevalence of massive data-driven applications such as AI/ML and big data analytics, the demand for larger memory is ever-increasing in today’s heterogeneous parallel computing systems. Over the past two decades, the CPU performance has been boosted dramatically thanks to Moore’s law and multi/many-core scaling. However, the memory capacity and bandwidth per core have been decreasing, which apparently poses a bottleneck for system performance [1]. In modern datacenters, the deployment unit is a monolithic server which contains closely-coupled computing and memory resources. This monolithic architecture for many years is always CPU-biased, leading to memory over-provision across the whole system. It has been observed that more than 50% of the aggregated memory is unused most of the time in production clusters at Google and Facebook [2]. Considering the rising DRAM chip prices in recent years, the under-utilization of memory resources becomes prohibitively expensive, which greatly boosts the TCO of datacenters [3]. With the advent of memory disaggregation technologies, new solutions can be explored to tackle the memory wall and memory under-utilization challenges [4]. Memory disaggregation technologies decouple memory resources from CPUs, providing a feasible option for memory pooling. Conventionally, Remote Direct Memory Access (RDMA) technology is exploited to realize memory disaggregation [2, 5, 6, 7, 8]. But RDMA is based on networking IO semantics which requires specialized NICs and software intervention, leading to a latency multiple orders of magnitude longer than that of local memory access. In recent years, several low-latency and high-bandwidth memory-coherent interconnect protocols arise in industry, which have shown advantages over RDMA for memory disaggregation [9, 10, 11, 12, 13, 14, 15]. Among them, the CXL protocol [16] is very promising and embraced by an increasingly number of semiconductor vendors worldwide. With CXL, memory expansion becomes more flexible over the interconnect fabric while enabling coherent memory access via load/store instructions. Furthermore, the CXL protocol is independent on the underlying memory technology, which can be DRAM, Flash, or even emerging non-volatile memories such as MRAM and RRAM. This facilitates the construction of a unified heterogeneous memory pool for future energy- and cost-efficient computing systems. Despite its attractive features disclosed in the protocol specifications, CXL is still in its infancy stage and lacks commodity products on the market. As a result, the prior research work on CXL-based memory disaggregation is conducted based on four main methods: software-based emulation [17], software-based simulation, [18, 19, 20], hardware-based emulation [21, 22, 23], and hardware prototyping [9, 24, 25]. However, the software-based emulation such as QEMU fails to model the physical characteristics and internal micro-architecture of real CXL memory devices. The simulation endeavors of gem5-CXL and CXLMemSim are both nascent, with gem5-CXL failing to accurately model the CXL protocol behavior and provide clear access interfaces, while CXLMemSim lacks full-system simulation capabilities and cycle-accurate fidelity, resulting in limited functionality and poor usability. The hardware-based emulation such as remote NUMA lacks CXL protocol support and there is a big difference in the memory access path and performance. As for the fourth method, there are currently no market-ready prototypes of CXL-based memory-disaggregated systems; CXL commodity products are also expensive to produce and purchase. Given the above limitations, there is a clear need for an accurate, cost-effective, and flexible tool for research on CXL-based disaggregated memory systems. In this paper, we present CXL-DMSim, a full-system CXL Disaggregated Memory Simulator based on gem5 for cycle-accurate simulation, architectural exploration, and evaluation of CXL-interconnected memory systems. CXL-DMSim is as easily configurable as the original gem5 simulator and fits to a variety of CXL devices. It has been rigorously verified and calibrated by a real-world CXL1.1 testbed with both an in-house ASIC CXL memory expander and an FPGA-based CXL device prototype. To the best of our knowledge, CXL-DMSim is the first usable full-system disaggregated memory simulator. The main contributions of this paper are listed below. • A flexible device model of CXL memory expander (Type 3 device) which currently supports both DRAM and Flash as underlying storage media. • Supports for CXL.io and CXL.mem sub-protocols, which are used to enumerate, configure, and access our CXL memory device on CXL-DMSim. • A driver for the device to operate in an application-managed mode and a NUMA-aware memory management mechanism to operate in a kernel-managed mode. • An extensive evaluation that validates CXL-DMSim including performance tests, usability&fidelity tests, real-world app. tests, and observation&expandability tests; this verifies the system’s feasibility and offers guidance for appropriate usage of CXL disaggregated memory."
https://arxiv.org/html/2411.01008v1,AI-Guided Codesign Framework for Novel Material and Device Design applied to MTJ-based True Random Number Generators,"Novel devices and novel computing paradigms are key for energy-efficient, performant future computing systems. However, designing devices for new applications is often time-consuming and tedious. Here, we investigate the design and optimization of spin–orbit torque and spin transfer torque magnetic tunnel junction models as the probabilistic devices for true random number generation. We leverage reinforcement learning and evolutionary optimization to vary key device and material properties of the various device models for stochastic operation. Our AI-guided codesign methods generated different candidate devices capable of generating stochastic samples for a desired probability distribution, while also minimizing energy usage for the devices.","Figure 1: Our AI-Guided Framework for Device Discovery and Optimization for a given application. Overview of the device model, AI-guided discovery and optimization strategy, and RNG algorithm workflow. Given a target distribution, the optimization approach (b) uses a device model (a) to simulate a true random bit according to the RNG algorithm (c). The optimization algorithm designs novel device configurations (d) that must pass device checks to be viable. The viable devices are used to produce the target distribution for a given application (e). Designing devices for novel applications is oftentimes a time rigorous and resource-constrained process that requires utilizing computationally intensive simulations, device fabrication, and testing of the physical components in the application-specific environment. At the same time, customizing device characteristics to a particular application can allow for significant performance improvements. Automated codesign strategies are becoming increasingly popular with advancements in the artificial intelligence (AI) field that provide useful machine learning algorithms and frameworks [1, 2, 3, 4]. Such codesign provides new opportunities to automatically customize devices for application-specific needs to maximize performance—whether that involves a particular capability, energy usage, latency, throughput, or even combinations of metrics. The operation of emerging devices, such as magnetic tunnel junctions (MTJs) [5, 6, 7, 8], can be simulated using physics-based models that capture key behaviors based on materials and device properties. By pairing these models with AI-guided codesign, we are able to effectively optimize the device parameters for application requirements and constraints [9, 10, 11]. AI-guided methods are increasingly being adopted in electronic design automation (EDA) flows. Recently, reinforcement techniques have been used in EDA for multiple tasks including chip floor planning [12], architecture search [2], gate sizing of VLSI [13], circuit optimization [14] and analog circuit design [15]. Evolutionary algorithm (EA) approaches, on the other hand, have been used for decades to design analog circuits [16] and can be creative in the design of novel solutions to a variety of problems [17]. Both reinforcement learning (RL) and EA approaches are promising for optimization tasks, each offering unique pros and cons. In addition, recent work leverages generative AI (GAI)-based circuit characterization [18] and optimization techniques [19, 20]. In related work, physics-informed neural networks (PINNs) [21], originally designed for solving partial differential equations with informed loss functions, have been used to perform device design and optimization [22, 23, 24]. Codesign across devices, circuits, architectures, and applications for a full-stack solution is a challenge and an ongoing area of research. In previous work, we have shown initial results in leveraging RL for MTJ device codesign [10] and EA for probabilistic circuit optimization using different MTJ devices and tunnel diode device [9]. This new work presents an intelligent, automated codesign framework for emerging devices. In particular, we create a framework that is based on RL and EAs, which allows for multi-objective optimization of parameters of emerging devices for real-world applications. We showcase this framework by providing a comparison of RL and EA approaches for device design and parameter optimization and a demonstration of device parameters for energy-efficient random number generation for gamma distributions for both spin–orbit torque (SOT) and spin transfer torque (STT) MTJ devices. Though this framework is applied in the context of true random number generation using SOT and STT MTJ devices, it can be easily extended to other applications and other device types. Ultimately, our methods produce the best candidate devices and materials properties for optimizing both performance in function and energy efficiency. Generally, we see that performance is improved but energy efficiency is slightly increased, compared to the default parameters used to represent standard CoFeB MTJs. The results also show that for the SOT MTJs, a larger range of material parameters can provide good performance, and material parameters for stronger perpendicular magnetic anisotropy (PMA) are favored. In contrast, for STT MTJs there is a narrower range of parameters to achieve the performance, and weaker PMA is favored. This paper is structured as follows: in Section 2 we provide details on our application with a background on RNGs and the distribution sampling scheme we employ, Section 3 introduces the two MTJ device types we will be designing for our application, Section 4 discusses our AI-guided approach using RL and EA with Section 5 presenting our AI-guided approaches results for device discovery. Section 6 provides a discussion of the results and observations, and we finish with conclusions in Section 7. Additional supplementary information is provided in Section 9."
https://arxiv.org/html/2411.02010v1,Complex Vector Gain-Based Annealer for Minimizing XY Hamiltonians,"This paper presents the Complex Vector Gain-Based Annealer (CoVeGA), an analog computing platform designed to overcome energy barriers in XY Hamiltonians through a higher-dimensional representation. Traditional gain-based solvers utilizing optical or photonic hardware typically represent each XY spin with a single complex field. These solvers often struggle with large energy barriers in complex landscapes, leading to relaxation into excited states. CoVeGA addresses these limitations by employing two complex fields to represent each XY spin and dynamically evolving the energy landscape through time-dependent annealing. Operating in a higher-dimensional space, CoVeGA bridges energy barriers in this expanded space during the continuous phase evolution, thus avoiding entrapment in local minima. We introduce several graph structures that pose challenges for XY minimization and use them to benchmark CoVeGA against single-dimension XY solvers, highlighting the benefits of higher-dimensional operation.","The growing complexity and sheer scale of modern scientific and industrial computing tasks are pushing us to look beyond traditional von Neumann architectures for solutions to hard optimization problems. These architectures, which dominate computing today, rely on a clear separation between memory and processing and execute tasks in a step-by-step manner. While traditional computing has been reliable for decades, it’s starting to fall behind when it comes to the needs of today’s specialized applications—particularly those that demand high speed, energy efficiency, and scalability. As areas like machine learning, big data analysis, and real-time processing continue to grow, the limitations of the von Neumann approach have become more evident, creating a bottleneck that is increasingly difficult to overcome. This is where analog systems come in, offering a tailored approach to specific types of computing tasks, bypassing the constraints of conventional architectures. Physics-inspired analog machines have been proposed using various platforms such as superconducting qubits [1, 2, 3], optical parametric oscillators [4, 5, 6, 7], memristors [8], lasers [9, 10, 11], photonic systems [12, 13], trapped ions [14], polariton condensates [15, 16], photon condensates [17], and surface acoustic waves [18]. These specialized physical machines minimize programmable spin Hamiltonians, where the couplings between spins – given by the interaction matrix 𝐉𝐉\mathbf{J}bold_J – are designed such that the global minimum corresponds to the optimal solution of a combinatorial optimization problem. Problems like number partitioning, traveling salesman, graph coloring, spin glass systems, knapsack problem, binary linear programming, graph partitioning, and Max-Cut can be mapped to spin Hamiltonians [19, 20, 21]. Further applications include machine learning [22], financial markets [23], and portfolio optimization [24]. Most proposed physical analog machines use one-dimensional (‘hard’ or ‘soft’) spins as variables in the discrete binary Ising Hamiltonian HI=−∑i,jNJi⁢j⁢si⁢sjsubscript𝐻Isuperscriptsubscript𝑖𝑗𝑁subscript𝐽𝑖𝑗subscript𝑠𝑖subscript𝑠𝑗H_{\rm I}=-\sum_{i,j}^{N}J_{ij}s_{i}s_{j}italic_H start_POSTSUBSCRIPT roman_I end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT with si=±1subscript𝑠𝑖plus-or-minus1s_{i}=\pm 1italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ± 1, or the continuous XY Hamiltonian HXY=−∑i,jNJi⁢j⁢𝐬i⋅𝐬jsubscript𝐻XYsuperscriptsubscript𝑖𝑗𝑁⋅subscript𝐽𝑖𝑗subscript𝐬𝑖subscript𝐬𝑗H_{\rm XY}=-\sum_{i,j}^{N}J_{ij}{\bf s}_{i}\cdot{\bf s}_{j}italic_H start_POSTSUBSCRIPT roman_XY end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT bold_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⋅ bold_s start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT, where 𝐬i=(cos⁡θi,sin⁡θi)subscript𝐬𝑖subscript𝜃𝑖subscript𝜃𝑖{\bf s}_{i}=(\cos\theta_{i},\sin\theta_{i})bold_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( roman_cos italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , roman_sin italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), θi∈[0,2⁢π)subscript𝜃𝑖02𝜋\theta_{i}\in[0,2\pi)italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ [ 0 , 2 italic_π ), and N𝑁Nitalic_N is the number of spins. The XY model describes a system of spins constrained to rotate within a plane, each possessing a continuous degree of freedom characterized by a phase θisubscript𝜃𝑖\theta_{i}italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT relative to a fixed axis. Interactions between phases give rise to a rich array of dynamics, including vortices and Berezinski-Kosterlitz-Thouless transitions [25]. Originally developed for statistical mechanics and condensed matter physics, the XY model has found extensive applications in various physical systems where rotational symmetry is essential. Notably, it applies to superfluidity and superconductivity [26], cosmology [27], nematic liquid crystals [28], magnetic nanoparticle ensembles [29], protein folding [30], and phase retrieval problems [31]. Gain-based minimizers utilize soft-spin bifurcation dynamics via Andronov-Hopf bifurcations to minimize spin Hamiltonians [32]. The enhanced dimensionality offered by soft-spin models reduces energy barriers present in classical hard-spin Hamiltonians by representing fixed spin amplitudes as continuous variables [33]. We recently proposed the Vector Ising Spin Annealer (VISA) as an Ising minimization model capable of overcoming obstacles in solving combinatorial optimization problems [34]. By employing three soft modes to represent the vector components of an Ising spin, VISA bridges minima separated by significant energy barriers in complex energy landscapes. VISA uses real-valued soft spins, which can, for example, represent the optical parametric oscillator quadrature in coherent Ising machines. In this paper, we propose to use multiple vector components to represent the spins in networks of complex-valued fields ψisubscript𝜓𝑖\psi_{i}italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which are known to minimize XY and Ising Hamiltonians in gain-based systems [35]. The Stuart-Landau equation that governs the dynamics of one-dimensional complex oscillators ψisubscript𝜓𝑖\psi_{i}italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT in gain-based networks is given by ψ˙i=(γi−|ψi|2)⁢ψi+α⁢∑jJi⁢j⁢ψj,subscript˙𝜓𝑖subscript𝛾𝑖superscriptsubscript𝜓𝑖2subscript𝜓𝑖𝛼subscript𝑗subscript𝐽𝑖𝑗subscript𝜓𝑗\dot{\psi}_{i}=\left(\gamma_{i}-|\psi_{i}|^{2}\right)\psi_{i}+\alpha\sum_{j}J_% {ij}\psi_{j},over˙ start_ARG italic_ψ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - | italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_α ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_ψ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , (1) where we introduced a regulation parameter α>0𝛼0\alpha>0italic_α > 0 and the effective gain rate γisubscript𝛾𝑖\gamma_{i}italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is individually dynamically adjusted through feedback mechanism γ˙i=ε⁢(1−|ψi|2),subscript˙𝛾𝑖𝜀1superscriptsubscript𝜓𝑖2\dot{\gamma}_{i}=\varepsilon\left(1-|\psi_{i}|^{2}\right),over˙ start_ARG italic_γ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_ε ( 1 - | italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , (2) which can be implemented via optical delay lines for laser systems, or by spatial light modulators for polariton condensates [16]; see also Appendix A. The dynamical equation (1) can be written as ψ˙i=−∂H/∂ψi∗subscript˙𝜓𝑖𝐻superscriptsubscript𝜓𝑖\dot{\psi}_{i}=-\partial H/\partial\psi_{i}^{*}over˙ start_ARG italic_ψ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = - ∂ italic_H / ∂ italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT, which describes the process of gradient descent to the minima of the loss function H=12⁢∑i=1N(γi−|ψi|2)2−α2⁢∑i,jNJi⁢j⁢(ψi⁢ψj∗+ψi∗⁢ψj),𝐻12superscriptsubscript𝑖1𝑁superscriptsubscript𝛾𝑖superscriptsubscript𝜓𝑖22𝛼2superscriptsubscript𝑖𝑗𝑁subscript𝐽𝑖𝑗subscript𝜓𝑖superscriptsubscript𝜓𝑗superscriptsubscript𝜓𝑖subscript𝜓𝑗H=\frac{1}{2}\sum_{i=1}^{N}\left(\gamma_{i}-|\psi_{i}|^{2}\right)^{2}-\frac{% \alpha}{2}\sum_{i,j}^{N}J_{ij}\left(\psi_{i}\psi_{j}^{*}+\psi_{i}^{*}\psi_{j}% \right),italic_H = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ( italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - | italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - divide start_ARG italic_α end_ARG start_ARG 2 end_ARG ∑ start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ( italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_ψ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT + italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT italic_ψ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , (3) where γ˙i=0subscript˙𝛾𝑖0\dot{\gamma}_{i}=0over˙ start_ARG italic_γ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0 for all i𝑖iitalic_i at the threshold steady state. By representing each oscillator in its polar form as ψi=ri⁢exp⁡(i⁢θi)subscript𝜓𝑖subscript𝑟𝑖𝑖subscript𝜃𝑖\psi_{i}=r_{i}\exp(i\theta_{i})italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_exp ( italic_i italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), Eq. (1) can be decomposed into real and imaginary parts to get equations of the time evolution of the amplitude risubscript𝑟𝑖r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the phase θisubscript𝜃𝑖\theta_{i}italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as r˙isubscript˙𝑟𝑖\displaystyle\dot{r}_{i}over˙ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =γi⁢ri−ri3+α⁢∑jJi⁢j⁢rj⁢cos⁡(θi−θj),absentsubscript𝛾𝑖subscript𝑟𝑖superscriptsubscript𝑟𝑖3𝛼subscript𝑗subscript𝐽𝑖𝑗subscript𝑟𝑗subscript𝜃𝑖subscript𝜃𝑗\displaystyle=\gamma_{i}r_{i}-r_{i}^{3}+\alpha\sum_{j}J_{ij}r_{j}\cos\left(% \theta_{i}-\theta_{j}\right),= italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT + italic_α ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT roman_cos ( italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , (4) θ˙isubscript˙𝜃𝑖\displaystyle\dot{\theta}_{i}over˙ start_ARG italic_θ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =−α⁢∑jJi⁢j⁢rjri⁢sin⁡(θi−θj).absent𝛼subscript𝑗subscript𝐽𝑖𝑗subscript𝑟𝑗subscript𝑟𝑖subscript𝜃𝑖subscript𝜃𝑗\displaystyle=-\alpha\sum_{j}J_{ij}\frac{r_{j}}{r_{i}}\sin\left(\theta_{i}-% \theta_{j}\right).= - italic_α ∑ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT divide start_ARG italic_r start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG start_ARG italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG roman_sin ( italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) . (5) Starting from below the steady state threshold in the vacuum state ri=0subscript𝑟𝑖0r_{i}=0italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 0, all oscillators are pumped equally. Then, depending on the structure of 𝐉𝐉\mathbf{J}bold_J, nonzero amplitudes emerge at different rates for each oscillator as the pumping intensity increases. The feedback mechanism of Eq. (2) adjusts each oscillator so that they all reach equal amplitudes at the steady state threshold. Only under the condition of equal amplitudes at the steady state will Eq. (5) reach the minimum of the XY Hamiltonian. The sum of the steady states of Eq. (4) gives N=∑i=1Nγi+α/2⁢∑i,jJi⁢j⁢cos⁡(θi−θj)𝑁superscriptsubscript𝑖1𝑁subscript𝛾𝑖𝛼2subscript𝑖𝑗subscript𝐽𝑖𝑗subscript𝜃𝑖subscript𝜃𝑗N=\sum_{i=1}^{N}\gamma_{i}+\alpha/2\sum_{i,j}J_{ij}\cos\left(\theta_{i}-\theta% _{j}\right)italic_N = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_α / 2 ∑ start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT roman_cos ( italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ), so the global minimum of the XY model corresponds to the smallest effective injection ∑iγisubscript𝑖subscript𝛾𝑖\sum_{i}\gamma_{i}∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_γ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Close to the threshold, Eq. (5) becomes fully analogous to the Kuramoto model θ˙i=−α⁢∑j=1NJi⁢j⁢sin⁡(θi−θj).subscript˙𝜃𝑖𝛼superscriptsubscript𝑗1𝑁subscript𝐽𝑖𝑗subscript𝜃𝑖subscript𝜃𝑗\dot{\theta}_{i}=-\alpha\sum_{j=1}^{N}J_{ij}\sin\left(\theta_{i}-\theta_{j}% \right).over˙ start_ARG italic_θ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = - italic_α ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_J start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT roman_sin ( italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_θ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) . (6) Equation (1) can be adapted to minimize Ising Hamiltonians by restricting the state space of the phase, which we detail in Appendix B. Gain-based systems described by Eq. (1) can still settle in local minima during amplitude bifurcation, which limits the probability of finding the global minimum. To combat this, we introduce the complex vector gain-based annealer (CoVeGA) that exploits the advantages of extended spatial dimensions. In this model, continuous XY spins are represented as complex-valued vectors in two-dimensional vector space 𝚿i∈ℂ2subscript𝚿𝑖superscriptℂ2{\bf\Psi}_{i}\in\mathbb{C}^{2}bold_Ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_C start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. The increased dimension over typical one-dimensional spins allows to effectively overcome the barriers between minima. This paper proposes a new approach to minimizing XY Hamiltonians that utilizes the ultra-fast energy-efficient architecture of photonics-based analog machines. In Section II, we formalize CoVeGA, and provide expressions for each term in its composite Hamiltonian. In Section III, we use the Kuramoto model to investigate the difficulty of various XY minimization problems. This allows us to identify suitably hard benchmark problems to test CoVeGA and existing XY minimization algorithms, as recovering the global minimum of these problems is nontrivial. Lastly, Section IV compares the dynamics of CoVeGA to the one-dimensional Stuart-Landau network as well as other continuous-variable methods such as spin-vector Langevin and Kuramoto models. We contrast these methods by finding ground and excited state probabilities and illustrating the distribution of recovered states."
https://arxiv.org/html/2411.01924v1,Fairness-Utilization Trade-off in Wireless Networks with Explainable Kolmogorov-Arnold Networks,"The effective distribution of user transmit powers is essential for the significant advancements that the emergence of 6G wireless networks brings. In recent studies, Deep Neural Networks (DNNs) have been employed to address this challenge. However, these methods frequently encounter issues regarding fairness and computational inefficiency when making decisions, rendering them unsuitable for future dynamic services that depend heavily on the participation of each individual user. To address this gap, this paper focuses on the challenge of transmit power allocation in wireless networks, aiming to optimize α𝛼\alphaitalic_α-fairness to balance network utilization and user equity. We introduce a novel approach utilizing Kolmogorov-Arnold Networks (KANs), a class of machine learning models that offer low inference costs compared to traditional DNNs through superior explainability. The study provides a comprehensive problem formulation, establishing the NP-hardness of the power allocation problem. Then, two algorithms are proposed for dataset generation and decentralized KAN training, offering a flexible framework for achieving various fairness objectives in dynamic 6G environments. Extensive numerical simulations demonstrate the effectiveness of our approach in terms of fairness and inference cost. The results underscore the potential of KANs to overcome the limitations of existing DNN-based methods, particularly in scenarios that demand rapid adaptation and fairness.","The advent of 6G wireless networks heralds a new era of connectivity, promising to revolutionize sectors such as healthcare, education, logistics, and transportation [1]. These next-generation networks are poised to deliver unprecedented capabilities, including ultra-high data rates, massive device connectivity, and adaptive responses to highly dynamic environments [2]. Central to realizing these advancements is the efficient allocation of user transmit powers, a critical factor that directly influences network performance, user experience, and energy efficiency. In recent years, the complexity of this challenge has led researchers to explore innovative solutions leveraging Machine Learning (ML) techniques, with a particular focus on Deep Neural Networks (DNNs). Among the studies addressing the transmit power allocation problem in this rapidly evolving field, several notable approaches stand out. Nasir et al. [3] and Sheu et al. [4] employed Deep Q-Learning (DQL) to maximize the sum data rate of users, utilizing channel information as input. Li et al. [5] extended the same approach to a distributed setting. Jamous et al. [6] applied DQL to optimize transmission energy efficiency. Zhang et al. [7] innovated by using convolutional DNNs with users’ geographical information to maximize aggregate data rates. In a different approach, Zhang et al. [8] implemented Proximal Policy Optimization (PPO) with signal strength inputs to ensure predefined Signal-to-Interference-plus-Noise Ratio (SINR) thresholds. Huang et al. [9] also utilized PPO, focusing on maximizing the sum of data rates. While existing DNN-based methods have demonstrated considerable performance, they face significant challenges in two key areas: balancing network utilization with fairness, and achieving computational efficiency during inference. Most of the existing studies have primarily focused on system-wide performance indicators, such as aggregate data rates, often at the expense of equitable resource allocation among individual users. This oversight becomes particularly critical in the context of future services, where semantic-aware communication is expected, and ensuring fair participation for each user is essential to maintain the quality and diversity of outcomes, thereby mitigating potential biases from specific sources [10]. Furthermore, the DNN-based techniques prevalent in the literature are predominantly black-box models, necessitating complex computations for each inference. This computational intensity often results in prolonged inference times [11]. Such inefficiency is particularly problematic in the dynamic environments anticipated for 6G services, where rapid adaptation to changing environmental conditions is paramount. To address the gaps in existing research, this paper focuses on investigating the power allocation problem with the objective of optimizing α𝛼\alphaitalic_α-fairness. The α𝛼\alphaitalic_α-fairness metric offers a versatile framework for balancing the trade-off between fairness and utilization in resource allocation. By modulating α𝛼\alphaitalic_α, we can achieve various fairness objectives, providing a flexible approach suitable for dynamic future services. To tackle this problem, we employ a novel class of machine learning models known as Kolmogorov-Arnold Networks (KANs), which have been proposed as an alternative to conventional DNNs [12]. KANs are designed to approximate continuous multivariate functions using learnable activation functions within a relatively simple architecture, offering improved generalization capabilities. The reliance on these functions renders KANs fully explainable, significantly reducing the computational overhead typically associated with inference. This characteristic makes KANs particularly well-suited for time-sensitive and resource-constrained environments, offering an attractive solution for next-generation communication systems. The remainder of this paper is structured as follows. Section II presents the system model, provides a comprehensive problem formulation, and proves the NP-hardness of the considered problem. Section III elucidates the proposed KAN-based solution, encompassing its fundamental principles, as well as the proposed dataset generation and decentralized training algorithms. In Section IV, we present and analyze numerical results, with a particular focus on evaluating the efficiency of the proposed solution in terms of fairness and inference cost. Finally, Section V concludes the paper with a summary of our findings and closing remarks on the implications and potential future directions of this research."
https://arxiv.org/html/2411.01271v1,"Interacting Large Language Model Agents.
Interpretable Models and Social Learning.","This paper develops theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making by interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and stochastic control algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors.This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under two settings: (a) centrally controlled LLMAs and (b) autonomous LLMAs with incentives. Throughout the paper, we numerically demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like Llama and Mistral and closed-source models like ChatGPT. The main takeaway of this paper, based on substantial empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting. Traditionally, such models are used in economics to study interacting human decision-makers.","This paper develops theory and algorithms for interacting Large Language Model Agents (LLMAs) by leveraging techniques from signal processing and microeconomics. Specifically, we focus on developing interpretable models and stochastic control algorithms for LLMAs, enabling them to interact sequentially for Bayesian inference. We construct interpretable models of LLMAs at two levels of abstraction, as outlined in Figure 1. First, we model an individual LLMA as a rationally inattentive Bayesian utility maximizer, capturing the agent’s decision-making process under limited attention. Second, we extend this approach to a sequence of LLMAs engaging in Bayesian social learning, where each agent acts as a Bayesian utility maximizer. Our models are inspired by the self-attention mechanism in large language models (LLMs) and observed challenges, such as model collapse, that can arise during LLM training. Rationally Inattentive Bayesian Utility Maximizer LLM AgentLLM Agent nText observation Action (Estimate) ⋯⋯\displaystyle\cdots⋯Abstraction 1Abstraction 2LLM Agent 1Text observation Action (Estimate) Sequence of LLM Agents perform Bayesian social learningLLM Agents are rationally inattentive Bayesian utility maximizers Figure 1: Summary of the proposed contributions: We discuss the different black-box models for LLMA and how LLMAs can be used as a sensing mechanism to perform Bayesian inference. Part 1 models the LLMAs as a rationally inattentive Bayesian utility maximizer and numerically establishes the behavior in applications of product quality identification and hate speech classification. Part 2 discusses how Bayesian social learning in a sequence of LLMAs can be used for sequential state estimation. However, in Part 3, we show that the agents can perform the same incorrect action due to herding. We then discuss a stochastic control approach to delay herding when LLMAs are centrally controlled and when they are autonomous but are incentivized. Furthermore, motivated by the observed bias in the behavior of interacting LLMAs, we demonstrate that a sequence of LLMAs engaging in Bayesian social learning converge to identical estimates, or ”herd”. To address this phenomenon, we propose a stochastic control approach, formulating an optimal stopping problem to balance the trade-off between privacy and herding, to detect the failure state. Our approach is designed for two scenarios: (a) when the LLMAs are centrally controlled, and (b) when they operate autonomously. Our goal is to demonstrate that concepts from controlled sensing and microeconomics, traditionally applied to human decision-making, can be used to both understand and synthesize the behavior of interacting LLMAs [12, 7, 13, 39, 40]. We support our theoretical findings with numerical experiments using advanced LLMs for Bayesian inference on real-world data. This paper is crafted to engage a broad readership, highlighting applications of Bayesian agents in diverse fields, including financial news analysis, e-commerce review evaluation, and online toxicity detection. These examples underscore the flexibility of our methodologies for cross-disciplinary applications. The reproducible code for our experiments is publicly accessible at github.com/aditj/sociallearningllm. I-A Motivation LLM agents (LLMAs) are being rapidly deployed for different applications and to quote Sam Altman, CEO of OpenAI (creators of ChatGPT, a popular LLM which has 200 million weekly active users): “2025 is when (AI) agents will work”. LLMAs use a large language model (LLM) to parse the input and have additional agency to perform tasks. LLMs (such as ChatGPT and Llama) are neural networks with billions of parameters trained on trillions of tokens of textual data to parse long texts for summarizing, compiling key facts, and generating new text. The key technical improvement that leads to the efficient deployment of LLMs is the transformer architecture [75]. The effectiveness of LLMs on textual texts has made their deployment and adoption widespread [54]. Many applications have been proposed in healthcare, online platform moderation, and finance, where these LLMs are used to parse the textual observations and suggest decisions based on their outputs [48]. LLM Bayesian Engine LLMA as a Sensing MechanismLLMARational Inattention Cost Bayesian UtilityLLMALLMALLMALLMA Bayesian Social Learning and Stochastic Control LLMA Engineer Interpretable & Robust Bayesian Inference Text Observation State Estimate Reconstructed Interpretable Model using Bayesian Revealed Preferences Figure 2: Engineering with large language model agents: We propose engineer with LLMs on three different levels: a) First, we propose constructing an LLMA with an LLM attached to the Bayesian engine. The LLM acts as a sensor for the text input and outputs interpretable low-dimensional outputs, which are used by the Bayesian engine to produce a state estimate. b) We formulate the necessary and sufficient conditions for a LLMAs to be a rationally inattentive Bayesian utility maximizer (RIBUM). We also present algorithms to reconstruct feasible utilities and rational inattention costs if the LLMA is indeed a RIBUM, attributing the LLMA with an interpretable microeconomic model. c) We show how a sequence of LLMAs can efficiently perform sequential Bayesian social learning by controlling their outputs to delay herding optimally. Our Bayesian social learning models can be extended to study Bayesian social learning in a network of LLMAs. In many tasks, the outputs of the LLMs are often part of a more extensive pipeline; for example, the output of the LLMs, either in a specified format or as embeddings, is frequently used as inputs to other Bayesian entities, including classifiers [55]. The Bayesian framework also becomes essential in applications where the LLMs have to output decisions and need to provide confidence in the decision output. Thus, it is of interest to study a single Bayesian agent that uses the LLM to parse text observations, update its Bayesian belief, and take action. This paper studies such entities and refers to them as Large Language Models Agents (LLMAs). Constructing interpretable models for LLMAs is crucial to understanding and controlling their interaction. I-A1 Interacting large language model agents It is predicted that by 2025, 90% of web content will be generated by large language models (LLMs) [1]. In recent practical implementations, individual LLMs are part of a bigger system, referred to as LLMAs, and interact with the content generated by other LLMAs and the external environment [86]. Furthermore, recent research has shown how generative models are trained on the data generated by other generative models can collapse [67]. Therefore naturally, LLMAs interact with each other either implicitly or explicitly. Hence, controlling the dynamics of interacting LLMAs is essential to improve the accuracy and trustworthiness of decisions by LLMAs. To the best of our knowledge, only a few recent works systematically study the behaviors of LLMAs using tools from microeconomics and signal processing [30]. This study aims to bridge this gap by systematically reviewing LLMAs and the different mathematical frameworks by studying Bayesian social learning in a sequence of LLMAs to achieve Bayesian inference. I-A2 Interpretable Engineering of LLMAs Many different third-party services have already started providing various kinds of LLMAs as a service, including Agentforce by Salesforce and IBM AI agents [66]. The underlying intelligence engine of these third-party agents is an LLM or a vision language model (VLM). The LLMAs are used in personal applications for coding, shopping, and scraping data and in enterprise applications for getting insights on user activity and automating industrial workflows. Therefore, it becomes imperative to study interpretable models for these agents since many of the proposed applications these agents involve sensitive information (like personal records, financial information, bio-medical data, and personal preferences). By interpretable, we refer to models that facilitate a transparent understanding of complex models through clear and explainable representations of their decision-making processes. The workflows of the AI agents also include making decisions, and the interpretability and reliability of these agents become vital for them to be trustworthy. Therefore, mathematical models are needed to aid in engineering and deploying LLMAs. To this end, we propose a LLMA composed of an LLM and a Bayesian engine, which by construction is interpretable. Further, we use Bayesian revealed preferences 111The framework of Bayesian revealed preferences is also referred to as inverse optimization or inverse reinforcement learning. to reconstruct a Bayesian utility function for both our constructed LLMA and for off-the-shelf LLMAs. I-A3 Bayesian Inference from Multi-Modal Data Stream In various applications, like online e-commerce platforms, video streaming platforms, and social networks, there is a rich stream of multimodal data available using text, images, and videos. Different inference tasks involve fusing information from various data streams to get actionable insights. With the recent progress in deep learning, many of the traditional signal processing methods are being replaced with contemporary methods that use LLMs and VLMs. However, just using static models is not sufficient to model the dynamics of real-life settings, e.g. on online platforms, and underlying dynamics are better modeled in a Bayesian framework. Therefore, motivated by practical applications, we propose the construction of LLMAs which can perform Bayesian inference sequentially on a data stream. This complements continual learning, which deals with continually learning new tasks without forgetting what was learned previously [78]. I-B Main Results Sec. 3: LLM Agent as a SensorSec. 4: LLM Agent as a Rationally Inattentive Bayesian Utility Maximizer (RIBUM)Part 1Indiviual LLM Agent Part 2Network of LLM Agents Sec. 5: Sequential Bayesian Social LearningSec. 6: Word-of-MouthBayesian Social LearningSec. 6: AsynchronousBayesian Social LearningPart 3Stochastic Control for delaying Herding in LLM Agents Sec. 7: Optimal Stopping for Centrally Controlled LLM AgentsSec. 8: Optimal Stopping for Incentivized Autonomous LLMAsSec. 9: Stochastic Approximation for Optimal Policy Sec. 10: Numerical Results on Bayesian inference Product Quality and Hate Speech Peddler Identification Figure 3: Organization of the paper: The paper is divided into three parts. Part 1 deals with interpretable models for an individual LLM agent. Part 2 extends the models to a social learning setting where LLM agents interact with each other to perform Bayesian inference. Part 3 proposes stochastic control methods to delay herding in a sequence of LLM agents. This paper builds on tools from Bayesian revealed preferences from microeconomics (inverse reinforcement learning), sequential Bayesian estimation (from signal processing), and structured stochastic control (from control theory) to construct interpretable models and synthesize interaction of LLMAs. The impact of our results on more efficient, systematic, and interpretable engineering of LLMAs is summarized in Figure 2. The main contributions of this paper are: 1. We propose constructing a LLMA as a composition of a large language model (LLM) sensor, which acts a low-dimensional map from the text space and a Bayesian engine, which uses the measurement from the LLM to update the posterior and act optimally. We show how this model is useful for interpretable Bayesian inference with applications in sequential data on online platforms. 2. To obtain an interpretable utility function for a LLMA, we provide necessary and sufficient conditions in Theorem 1 for a LLMA to be a rationally inattentive Bayesian utility maximizer (RIBUM). For a LLMA who is a RIBUM, we propose Algorithm 2 and Algorithm 3 to reconstruct the max-margin and sparsest utility estimate, respectively. Our methods are applicable both our LLMA and off-the-shelf LLMAs. 3. We study Bayesian social learning in a LLMAs, sequentially estimating a state given text observations and in Theorem 2 show that such a sequence of LLMAs form an information cascade and herd in their actions. We show that this is true for both when no private observations are shared and when a finite number of private observations are shared. Further, we provide a detailed analysis of the effect of the quality of results from LLM of the LLMA and the number of private observations. 4. To delay herding in a sequence of LLMAs, we formulate an optimal stopping problem for two regimes: a) when the LLMAs are centrally controlled by an entity b) when the LLMAs are autonomous but are incentivizes by an entity. We show in Theorem 3 and Theorem 4 that under certain assumptions on the observation matrix and cost functions, the optimal policy for the partially observed Markov decision process of both the optimal stopping problems has a threshold structure. We then propose a stochastic approximation algorithm in Algorithm 7, which exploits the structural results to estimate the optimal policy parameters. The algorithm does not need access to the system parameters, is computationally efficient, and can track changes in the system. 5. We finally present several numerical experiments to demonstrate the efficacy of our proposed methods. We show how our constructed LLMA can be used for interpretable Bayesian inference for analyzing financial data. We show how the Bayesian revealed preferences framework can estimate the utility of an off-the-shelf LLM when used for hate-speech detection. Finally we show numerical studies on two examples of sequential Bayesian inference: hate speech peddler identification and product quality analysis, to demonstrate herding of LLMAs, and applicability of our structural results. To summarize, this paper attempts to answer the following questions with respect to interacting LLM Agents, 1. How can LLMAs be constructed so that they can be used for sequential Bayesian inference such that the observation and outputs are interpretable? 2. What is a principled approach to analyze whether a LLMA is a Bayesian utility maximizer and also reconstructs its utility function given only black-box access? 3. How does one systematically study Bayesian social learning in multiple interacting LLMAs to explain observed behaviours such as herding and model collapse? 4. How can herding in (centrally controlled or autonomous) LLMAs be optimally delayed so that the agents optimally switch between preserving privacy and improving estimation to achieve sequential detection? I-C Organization This paper is organized into three parts, and the schematic of the organization is given in Figure 3. Part I discusses interpretable model for a single LLMA and attempt at answering questions 1 and 2 above. Section II discusses the related work in large language models, agents using LLMs, and current interpretable models for Bayesian inference. Section III discusses the mathematical model used for modeling LLMAs in this paper and motivates the different components involved. Section IV gives the necessary and sufficient conditions for the LLMAs to be rationally inattentive Bayesian utility maximizers (RIBUM). It furthers proposes algorithms to estimate the utility function for a LLMA which is a RIBUM. Part II discusses interpretable models for interacting LLMAs and attempts to answer question 3. Section V discusses the mathematical framework of Bayesian social learning in LLMAs and proves that a sequence of LLMAs form an information cascade in finite time. Section VIII discusses a stochastic control problem for the optimal stopping time problem to achieve quickest time herding with minimal loss to the privacy of LLMAs. Section VI discusses interpretable models to explain model collapse and data incest in LLMAs using word-of-mouth and asynchronous social learning. To decrease the bias when a sequence of LLMAs perform Bayesian inference, Part III deals with stochastic control for delaying herding in interacting LLMAs performing Bayesian sequential learning proves structural results, and proposes a stochastic approximation approach. Section IX considers the problem of a central controller optimally optimizing a sequence of autonomous LLMAs to achieve the state estimation by optimally controlling herding. Section X proposes a stochastic approximation based approach to approximate the optimal policy, which has a threshold switching curve. Numerical results on real-life text classification tasks and related applications are discussed in Section XI. Section XII concludes the paper with discussions on future works, open problems, and research opportunities. The appendix contains the proofs and details about the numerical experiments. For the ease of the reader, we have included a motivation and a discussion subsection in each section, which grounds the different aspects of LLMAs to a real-life application and different microeconomics and statistical signal processing tools presented in the section. We also provide different block diagrams and illustrative examples to further aid the reader."
https://arxiv.org/html/2411.01049v1,Exploratory Models of Human-AI Teams: Leveraging Human Digital Twins to Investigate Trust Development,"As human-agent teaming (HAT) research continues to grow, computational methods for modeling HAT behaviors and measuring HAT effectiveness also continue to develop. One rising method involves the use of human digital twins (HDT) to approximate human behaviors and socio-emotional-cognitive reactions to AI-driven agent team members (Barricelli & Fogli, 2024). In this paper, we address three research questions relating to the use of digital twins for modeling trust in HATs. First, to address the question of how we can appropriately model and operationalize HAT trust through HDT HAT experiments, we conducted causal analytics of team communication data to understand the impact of empathy, socio-cognitive, and emotional constructs on trust formation. Additionally, we reflect on the current state of the HAT trust science to discuss characteristics of HAT trust that must be replicable by a HDT such as individual differences in trust tendencies (e.g., propensity to trust, Jessup et al., 2019), emergent trust patterns (e.g., trust violation and repair, Wildman et al., 2024), and appropriate measurement of these characteristics (e.g., growth modeling, Abramov et al., 2020). Second, to address the question of how valid measures of HDT trust are for approximating human trust in HATs, we discuss the properties of HDT trust: self-report measures, interaction-based measures, and compliance type behavioral measures. Additionally, we share results of preliminary simulations comparing different LLM models for generating HDT communications and analyze their ability to replicate human-like trust dynamics. Third, to address how HAT experimental manipulations will extend to human digital twin studies, we share experimental design focusing on propensity to trust for HDTs vs. transparency and competency-based trust for AI agents.","The integration of artificial intelligence (AI) into operational environments has become increasingly vital across diverse domains, fundamentally transforming how humans and machines collaborate to achieve shared objectives. Over the past decade, Human-AI Teaming (HAT) research has emerged as a critical field, with scholars applying cognitive science principles to understand the complexities of these novel partnerships. This growing body of research reflects the urgency of understanding how humans and AI can work together effectively, safely, and productively. As the HAT literature expands and AI capabilities advance, innovative methodologies for studying these interactions have emerged. One promising approach involves the use of human digital twins (HDTs) - computational models designed to replicate human responses and behaviors within HAT contexts. These HDTs can be configured to simulate both state-based responses (such as transient cognitive and affective reactions) and trait-based characteristics (including dispositional individual differences), offering a versatile alternative to traditional human-subjects research [1, 2]. The advantages of HDTs in HAT research are significant. Beyond addressing the practical constraints of human-subjects studies, such as cost and recruitment challenges, HDTs provide unprecedented control over experimental variables and the ability to rapidly test multiple scenarios. This capability is particularly valuable for investigating complex team phenomena, with trust emerging as a critical area of focus. Trust - the willingness to be vulnerable to another agent’s actions - serves as a fundamental determinant of HAT effectiveness and success. However, the validity of using HDTs to study trust dynamics in HATs requires careful examination. This paper addresses three crucial questions regarding the implementation of HDTs in trust research: (1) How can we effectively model and measure HAT trust using HDT-based approaches? (2) What are the essential characteristics of HAT trust that must be operationalized in HDT trust models? (3) How do experimental manipulations from traditional HAT studies translate to HDT-based research? By examining these questions, we aim to establish a framework for validating and implementing HDTs in HAT trust research, ultimately advancing our understanding of human-AI collaboration and trust development."
https://arxiv.org/html/2411.00859v1,Profiling AI Models: Towards Efficient Computation Offloading in Heterogeneous Edge AI Systems,"The rapid growth of end-user AI applications, such as computer vision and generative AI, has led to immense data and processing demands often exceeding user devices’ capabilities. Edge AI addresses this by offloading computation to the network edge, crucial for future services in 6G networks. However, it faces challenges such as limited resources during simultaneous offloads and the unrealistic assumption of homogeneous system architecture. To address these, we propose a research roadmap focused on profiling AI models, capturing data about model types, hyperparameters, and underlying hardware to predict resource utilisation and task completion time. Initial experiments with over 3,000 runs show promise in optimising resource allocation and enhancing Edge AI performance.","The rapid growth of end-user AI applications, such as real-time image recognition and generative AI, has led to high data and processing demands that often exceed device capabilities. Edge AI addresses these challenges by offloading computation to the network’s edge, where hardware-accelerated AI processing can occur [1]. This approach is integral to AI and RAN, a key component of future 6G networks as outlined by the AI-RAN Alliance111https://ai-ran.org/working-groups/. In 6G, AI integration across edge-RAN and extreme-edge devices will support efficient data distribution and distributed AI techniques, enhancing privacy and reducing latency for applications like the Metaverse and remote surgery. Despite these benefits, Edge AI faces challenges. Limited resource availability at the edge can hinder performance during simultaneous offloads. Additionally, the assumption of homogeneous system architecture in the existing literature is unrealistic, as edge devices vary widely in processor speeds and architectures (e.g., 1.5GHz vs 3.5GHz, or X86 vs ARM), impacting task processing and resource utilisation. To address these challenges, we propose a research roadmap focused on profiling AI models by analysing their execution dynamics across various bare-metal systems. Our goal is to understand how AI model types (e.g., MLP, CNN), hyperparameters (e.g., learning rate, optimiser), hardware (e.g., architecture, FLOPS), and dataset characteristics (e.g., size, batch size) affect model accuracy, resource use, and task completion time. This Profiling AI Models process allows us to predict resource needs and task completion times, enabling efficient scheduling across edge nodes. Our initial experiments, involving over 3,000 runs with varied configurations, showcase the effectiveness of our approach. Using AI techniques like XGBoost, we achieved a normalised RMSE of 0.001, a significant improvement over MLPs with over 4 million parameters. Figure 1: Research roadmap for profiling based computation offloading"
https://arxiv.org/html/2411.00499v1,Cross-modal semantic segmentation for indoor environmental perception using single-chip millimeter-wave radar raw data,"In the context of firefighting and rescue operations, a cross-modal semantic segmentation model based on a single-chip millimeter-wave (mmWave) radar for indoor environmental perception is proposed and discussed. To efficiently obtain high-quality labels, an automatic label generation method utilizing LiDAR point clouds and occupancy grid maps is introduced. The proposed segmentation model is based on U-Net. A spatial attention module is incorporated, which enhanced the performance of the mode. The results demonstrate that cross-modal semantic segmentation provides a more intuitive and accurate representation of indoor environments. Unlike traditional methods, the model’s segmentation performance is minimally affected by azimuth. Although performance declines with increasing distance, this can be mitigated by a well-designed model. Additionally, it was found that using raw ADC data as input is ineffective; compared to RA tensors, RD tensors are more suitable for the proposed model.","Since the 21st century, while society and the economy have developed rapidly, the number of fires has also increased annually [1]. Statistics show that most firefighter casualties occur during fire rescues [2]. In fire scenes, dim environments and heavy smoke severely limit the environmental perception of firefighters, negatively impacting rescue efficiency. Impaired perception can lead to disorientation and falls, causing injuries or fatalities, especially in complex indoor environments. Additionally, the air respirators for firefighters typically last only 30 minutes [3], after which they face the risk of asphyxiation or poisoning, making rescue efficiency even more critical. Therefore, enhancing the environmental perception in fire scenes is essential to reducing casualties and improving rescue efficiency. Indoor fire scene perception aims to use sensors to efficiently and accurately detect and visualize the environment, providing vital information for personnel and laying a foundation for post-processing applications. Currently, cameras [4] and LiDAR [5].are the primary tools for indoor environment perception. However, in smoke-filled and dim environments, cameras fail to properly expose and capture images, and LiDAR suffers from scattering in smoke [6], making both unsuitable for fire scenes.. In recent years, Frequency Modulated Continuous Wave (FMCW) mmWave radar emerged as a promising solution for environmental sensing in autonomous driving [7], advanced driver-assistance systems (ADAS) [8], and automatic emergency braking (AEB) [9]. Besides its low cost, its robustness in harsh conditions like fog and smoke [10] makes it particularly suitable for fire scene perception. Essentially, indoor environmental perception forms the basis for mapping [11, 12], localization [13, 14], trajectory tracking [15, 16], and navigation [17]. For fire scenes, where traditional sensors fail, mmWave radar must independently perform environment perception. However, mmWave radar is limited by hardware constraints and signal processing methods The point clouds generated by mmWave radar are significantly inferior to those of LiDAR, not only being sparse and unstable [18] but also prone to interference from multipath effects [19], particularly with single-chip mmWave radar. These limitations greatly hinder its application in indoor environment perception. Various radar data processing methods have been proposed to improve point cloud accuracy. Constant False Alarm Rate (CFAR) detectors are widely used in point cloud generation, designed to detect target signals amid noise [20]. However, CFAR algorithms, such as cell averaging (CA) CFAR [21], greatest of (GO) CFAR [22], ordered statistic (OS) CFAR [23], etc. are prone to false alarms and missed detections. Weak signals may contain rich environmental information, while strong signals could be cluttered. In complex indoor environments with multiple reflections, the detection performance of CFAR degrades significantly, and it cannot effectively eliminate multipath effects. Direction of Arrival (DOA) estimation is another critical step in point cloud generation. Super-resolution algorithms like multiple signal classifier (MUSIC) [24], iterative adaptive approach (IAA) [25], etc. offer better accuracy than traditional Fast Fourier Transform (FFT)-based azimuth estimation methods, but they have high computational complexity and limited super-resolution capabilities. Recent advances in machine learning have introduced learning-based methods for DOA estimation and radar detection. Brodeski et al. [26] a used an RCNN model for DOA estimation of single targets, and Cheng et al. [18] developed RPDNet, a radar detector based on Range-Doppler Maps (RDM), which improves point cloud accuracy and density while mitigating multipath effects. However, the precision of point clouds still lags behind LiDAR. Semantic segmentation of navigable space, instead of point cloud-based obstacle detection, is another promising approach. Orr et al. [27] proposed a cross-modal semantic segmentation model based on deep neural networks (DNNs). This model takes raw RD tensor as input and directly performs navigable space segmentation in the overlapping field of view (FoV) with camera images, demonstrating the potential of DNNs to interpret abstract mmWave radar signals and selectively filter them. Jin et al. expanded this work by adding multi-class functionality [28] and introducing a transformer structure [29, 30]. While these efforts were conducted in outdoor driving scenarios, this idea also holds great potential for indoor environmental perception. To achieve indoor fire scene perception, common indoor environment perception must first be realized, which is the focus of this paper. Point clouds are not the only solution for rescue operations. Direct segmentation of navigable space offers more intuitive guidance for personnel, eliminating the need for sparse point cloud-based obstacle detection and path planning. However, applying cross-modal semantic segmentation to indoor environment perception tasks faces several key obstacles. Previous studies primarily used high-resolution automotive radars, while the potential of the more affordable single-chip mmWave radar remains unexplored. Compared to high-resolution radars, single-chip radars have fewer antennas, providing less environmental data. Moreover, indoor environments are more complex, posing significant challenges for cross-modal models. The design of segmentation tasks and labels is crucial, as it directly affects model performance. This paper addresses these challenges and contributes as follows: • The feasibility of applying a cross-modal semantic segmentation model based on single-chip mmWave radar for indoor environment perception has been validated. A U-Net-based semantic segmentation model was proposed, demonstrating excellent performance on this task. • To reduce labor and time costs, standardize the labeling process, and improve label quality, an efficient automatic labeling method based on LiDAR point clouds and occupancy grid maps was introduced and implemented. • The point cloud-based environmental perception method and the proposed segmentation-based method were intuitively compared and evaluated. The characteristics of the proposed model and method for the segmentation task were thoroughly discussed from the perspectives of distance, azimuth, and scene complexity. A detailed analysis of their respective advantages and disadvantages was provided, along with suggestions for future optimization. Furthermore, the model’s adaptability to different types of input data was also explored. It is important to note that this paper focuses on the feasibility of using segmentation models for indoor perception, with factors related to smoke left for future exploration. The remainder of this paper is structured as follows: Section 2 introduces the dataset and LiDAR-based labeling method. Section 3 describes the semantic segmentation model, and Section 4 outlines the training details. Section 5 compares the traditional point cloud method with the proposed method, evaluating model performance and discussing characteristics and ablation experiments. Finally, Section 6 concludes the paper."
https://arxiv.org/html/2411.00140v1,ViT-LCA: A Neuromorphic Approach for Vision Transformers,"The recent success of Vision Transformers has generated significant interest in attention mechanisms and transformer architectures. Although existing methods have proposed spiking self-attention mechanisms compatible with spiking neural networks, they often face challenges in effective deployment on current neuromorphic platforms. This paper introduces a novel model that combines vision transformers with the Locally Competitive Algorithm (LCA) to facilitate efficient neuromorphic deployment. Our experiments show that ViT-LCA achieves higher accuracy on ImageNet-1K dataset while consuming significantly less energy than other spiking vision transformer counterparts. Furthermore, ViT-LCA’s neuromorphic-friendly design allows for more direct mapping onto current neuromorphic architectures.","Neuromorphic computing represents a paradigm shift in computing, characterized by its low-power processing capabilities and brain-inspired architectures [1, 2, 3, 4, 5, 6, 7]. This approach emulates biological neural networks through the use of Spiking Neural Networks (SNNs). One of the primary advantages of neuromorphic chips lies in their capacity for highly parallel and energy-efficient computations. By performing operations asynchronously and maintaining proximity between synapses and weight calculations, these systems significantly reduce data movement, thereby enhancing overall computational efficiency. These platforms integrate many-core systems capable of instantiating large populations of spiking neurons, enabling information processing that mimics the dynamics of biological neural systems. Additionally, by utilizing crossbar arrays and memristores [8, 9, 10] to store multi-bit quantities as conductance values, neuromorphic computing is particularly well-suited for efficiently evaluating matrix-vector-multiplications, which are fundamental to deep learning algorithms. A particularly interesting model in neuromorphic computing is the Locally Competitive Algorithm (LCA) [11, 12], which is a computational model and learning algorithm that iteratively updates neuron activity to achieve a sparse representation of input data. This computational model has been implemented on recent neuromorphic platforms [2, 13, 14]. The competitive mechanism inherent in LCA ensures that only a limited number of neurons become active at any given time, facilitating efficient coding of high-dimensional data. One proposal for leveraging the LCA in neuromorphic computing is the Exemplar LCA-Decoder [15]. Functioning as a single-layer encoder-decoder, this computational model iteratively updates neuron activity to identify a sparse representation of the input data (i.e, encoding) and then uses these neuron activities for classification tasks (i.e, decoding). Recently, the Transformer architecture [16] and its variants have demonstrated impressive performance across a range of tasks, including natural language processing [17, 18] and computer vision [19, 20, 21]. This success is largely due to their ability to effectively capture long-range dependencies, a capability primarily attributed to the self-attention mechanism. Given the enormous computational requirements of transformer architectures, deploying these models on devices with limited resources remains a significant challenge. As a result, integrating transformer architectures with neuromorphic computing represents a promising research avenue. In particular, the combination of transformer architectures and LCA-based learning could lead to more efficient and biologically inspired artificial intelligence systems. However, this area remains largely unexplored. This paper presents ViT-LCA, which leverages Vision Transformers (ViT) [19] to extract self-attention representations and incorporates these representations into an LCA-based SNN. This algorithm effectively addresses the challenges of deploying transformer models on energy-constrained neuromorphic platforms. The self-attention representations are extracted once and stored in non-volatile memory elements, enabling in-memory computation on neuromorphic systems that emphasize specialized operations and energy efficiency. Our approach consists of two stages. In the first stage, a transformer encoder generates self-attention representations from the input image. In the second stage, these representations are processed by a single-layer SNN that employs a LCA encoder-decoder architecture for classification tasks. In this study, we evaluate ViT-LCA on CIFAR-10 [22], CIFAR-100 [23] and ImageNet-1 [24] datasets and assess the effectiveness of integrating ViT’s self-attention representation with the efficiency of sparse coding through LCA for deployment on neuromorphic systems. By inputting self-attention representations (contextual embeddings) derived from ViT into a single-layer SNN model, we achieved high classification accuracy while ensuring low computational overhead and high energy efficiency."
https://arxiv.org/html/2411.00110v1,Lagrangian neural networks for nonholonomic mechanics,"Lagrangian Neural Networks (LNNs) are a powerful tool for addressing physical systems, particularly those governed by conservation laws. LNNs can parametrize the Lagrangian of a system to predict trajectories with nearly conserved energy. These techniques have proven effective in unconstrained systems as well as those with holonomic constraints. In this work, we adapt LNN techniques to mechanical systems with nonholonomic constraints. We test our approach on some well-known examples with nonholonomic constraints, showing that incorporating these restrictions into the neural network’s learning improves not only trajectory estimation accuracy but also ensures adherence to constraints and exhibits better energy behavior compared to the unconstrained counterpart.","The laws of motion of a Lagrangian system are determined by the principle of stationary action, also known as Hamilton’s principle. This principle states that the action is minimal (or stationary) throughout a mechanical process. From this statement, the differential equations known as Euler-Lagrange equations are derived. If the Lagrangian function of a given mechanical system is known, then Euler-Lagrange equations establish the relationship between accelerations, velocities, and positions; that is, the system dynamics are obtained from Euler-Lagrange equations. Hence, the goal of Lagrangian mechanics is to write an analytic expression for the Lagrangian function in appropriate generalized coordinates and then develop the Euler-Lagrange equations symbolically into a system of second-order differential equations whose solutions give the system’s trajectory. In many cases, even when Euler-Lagrange equations are available, the solutions are not provided in analytical or explicit forms. Therefore, we can use numerical integrators to estimate the trajectories of a mechanical system. However, numerical integrators can sometimes produce poorly behaved trajectories concerning certain physical observables, such as energy. As an alternative, geometric integrators can be employed, since they are known to preserve energy (see, for instance, [2]). However, they may not be very accurate over long periods. Even worse is the case in which an analytical expression for the Lagrangian function is unknown or difficult to work with because we do not have a system of equations to solve. In recent years, there has been an increasing interest in using neural networks to address different issues of mechanical systems (see for example [7],[8],[11],[12],[14]). In this line, Lagrangian Neural Networks were introduced in [5] as an enhancement over other types of neural networks used in mechanical systems that do not preserve physical laws, providing a tool for scenarios where, for example, equations of motion are not available to get the actual trajectory. This method assumes that the Lagrangian of a mechanical system, a scalar function, can be parametrized using a neural network and be learned directly from the system’s data. That is, the goal of LNNs is to predict the Lagrangian function of a system based on data about its positions and velocities. This approach aims to represent the system’s equations of motion with a neural network while ensuring the preservation of some specific physical properties."
