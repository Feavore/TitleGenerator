URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04925v1,StoryAgent: Customized Storytelling Video Generation via Multi-Agent Collaboration,"The advent of AI-Generated Content (AIGC) has spurred research into automated video generation to streamline conventional processes. However, automating storytelling video production, particularly for customized narratives, remains challenging due to the complexity of maintaining subject consistency across shots. While existing approaches like Mora and AesopAgent integrate multiple agents for Story-to-Video (S2V) generation, they fall short in preserving protagonist consistency and supporting Customized Storytelling Video Generation (CSVG). To address these limitations, we propose StoryAgent, a multi-agent framework designed for CSVG. StoryAgent decomposes CSVG into distinct subtasks assigned to specialized agents, mirroring the professional production process. Notably, our framework includes agents for story design, storyboard generation, video creation, agent coordination, and result evaluation. Leveraging the strengths of different models, StoryAgent enhances control over the generation process, significantly improving character consistency. Specifically, we introduce a customized Image-to-Video (I2V) method, LoRA-BE, to enhance intra-shot temporal consistency, while a novel storyboard generation pipeline is proposed to maintain subject consistency across shots. Extensive experiments demonstrate the effectiveness of our approach in synthesizing highly consistent storytelling videos, outperforming state-of-the-art methods. Our contributions include the introduction of StoryAgent, a versatile framework for video generation tasks, and novel techniques for preserving protagonist consistency.","Storytelling videos, typically multi-shot sequences depicting a consistent subject such as a human, animal, or cartoon character, are extensively used in advertising, education, and entertainment. Producing these videos traditionally is both time-consuming and expensive, requiring significant technical expertise. However, with advancements in AI-Generated Content (AIGC), automated video generation is becoming an increasingly researched area, offering the potential to streamline and enhance traditional video production processes. Techniques such as Text-to-Video (T2V) generation models [He et al., 2022, Ho et al., 2022, Singer et al., 2022, Zhou et al., 2022, Blattmann et al., 2023a, Chen et al., 2023a] and Image-to-Video (I2V) methods [Zhang et al., 2023a, Dai et al., 2023, Wang et al., 2024a, Zhang et al., 2023b] enable users to generate corresponding video outputs simply by inputting text or images. While significant advancements have been made in video generation research, automating storytelling video production remains challenging. Current models struggle to preserve subject consistency throughout the complex process of storytelling video generation. Recent agent-driven systems, such as Mora [Yuan et al., 2024] and AesopAgent [Wang et al., 2024b], have been proposed to address Story-to-Video (S2V) generation by integrating multiple specialized agents, such as T2I and I2V generation agents. However, these methods fall short in allowing users to generate storytelling videos featuring their designated subjects, i.e., Customized Storytelling Video Generation (CSVG). The protagonists generated from story descriptions often exhibit inconsistency across multiple shots. Another line of research focusing on customized text-to-video generation like DreamVideo [Wei et al., 2023] and Magic-Me [Ma et al., 2024] can also be employed to synthesize storytelling videos. They first fine-tune the models using the data about the given reference protagonists, then generate the videos from the story descriptions. Despite these efforts, maintaining fidelity to the reference subjects remains a significant challenge. As shown in Figure 1, the results of TI-AnimateDiff, DreamVideo, and Magic-Me fail to preserve the appearance of the reference subject in the video. In these methods, the learned concept embeddings cannot fully capture and express the subject in different scenes. Considering the limitations of existing storytelling video generation models, we explore the potential of multi-agent collaboration to synthesize customized storytelling videos. In this paper, we introduce a multi-agent framework called StoryAgent, which consists of multiple agents with distinct roles that work together to perform CSVG. Our framework decomposes CSVG into several subtasks, with each agent responsible for a specific role: 1) Story designer, writing detailed storylines and descriptions for each scene.2) Storyboard generator, generating storyboards based on the story descriptions and the reference subject. 3) Video creator, creating videos from the storyboard. 4) Agent manager, coordinating the agents to ensure orderly workflow. 5) Observer, reviewing the results and providing feedback to the corresponding agent to improve outcomes. By leveraging the generative capabilities of different models, StoryAgent enhances control over the generation process, resulting in significantly improved character consistency. The core functionality of the agents in our framework can be flexibly replaced, enabling the framework to complete a wide range of video-generation tasks. This paper primarily focuses on the accomplishment of CSVG. However, simply equipping the storyboard generator with existing T2I models, such as SDXL [Podell et al., 2023] as used by Mora and AesopAgent, often fails to preserve inter-shot consistency, i.e., maintaining the same appearance of customized protagonists across different storyboard images. Similarly, directly employing existing I2V methods such as SVD [Blattmann et al., 2023b] and Gen-2 [Esser et al., 2023] leads to issues with intra-shot consistency, failing to keep the character’s fidelity within a single shot. Inspired by the image customization method AnyDoor [Chen et al., 2023b], we develop a new pipeline comprising three main steps—generation, removal, and redrawing—as the core functionality of the storyboard generator agent to produce highly consistent storyboards. To further improve intra-shot consistency, we propose a customized I2V method. This involves integrating a background-agnostic data augmentation module and a Low-Rank Adaptation with Block-wise Embeddings (LoRA-BE) into an existing I2V model [Xing et al., 2023] to enhance the preservation of protagonist consistency. Extensive experiments on both customized and public datasets demonstrate the superiority of our method in generating highly consistent customized storytelling videos compared to state-of-the-art customized video generation approaches. Readers can view the dynamic demo videos available at this anonymous link: https://github.com/storyagent123/Comparison-of-storytelling-video-results/blob/main/demo/readme.md111The codes will be released upon the acceptance of the paper The main contributions of this work are as follows: 1) We propose StoryAgent, a multi-agent framework for storytelling video production. This framework stands out for its structured yet flexible systems of agents, allowing users to perform a wide range of video generation tasks. These features also enable StoryAgent to be a prime instrument for pushing forward the boundaries of CSVG. 2) We introduce a customized Image-to-Video (I2V) method, LoRA-BE (Low-Rank Adaptation with Block-wise Embeddings), to enhance intra-shot temporal consistency, thereby improving the overall visual quality of storytelling videos. 3) In the experimental section, we present an evaluation protocol on public datasets for CSVG and also collect new subjects from the internet for testing. Extensive experiments have been carried out to prove the benefit of the proposed method."
https://arxiv.org/html/2411.04679v1,"CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent
Cooperation","In this work, we address the cooperation problem among large language model (LLM) based embodied agents, where agents must cooperate to achieve a common goal. Previous methods often execute actions extemporaneously and incoherently, without long-term strategic and cooperative planning, leading to redundant steps, failures, and even serious repercussions in complex tasks like search-and-rescue missions where discussion and cooperative plan are crucial. To solve this issue, we propose Cooperative Plan Optimization (CaPo) to enhance the cooperation efficiency of LLM-based embodied agents. Inspired by human cooperation schemes, CaPo improves cooperation efficiency with two phases: 1) meta-plan generation, and 2) progress-adaptive meta-plan and execution. In the first phase, all agents analyze the task, discuss, and cooperatively create a meta-plan that decomposes the task into subtasks with detailed steps, ensuring a long-term strategic and coherent plan for efficient coordination. In the second phase, agents execute tasks according to the meta-plan and dynamically adjust it based on their latest progress (e.g., discovering a target object) through multi-turn discussions. This progress-based adaptation eliminates redundant actions, improving the overall cooperation efficiency of agents. Experimental results on the ThreeDworld Multi-Agent Transport and Communicative Watch-And-Help tasks demonstrate CaPo’s much higher task completion rate and efficiency compared with state-of-the-arts.","Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human language, complex reasoning, and planning, achieving impressive performance (OpenAI, 2024; Touvron et al., 2023). These advancements empower LLM-based embodied agents to autonomously make plans (Li et al., 2023a; Padmakumar et al., 2022; Zhu et al., 2023; Wang et al., 2023; Wu et al., 2023b; Huang et al., 2022b) and perform reasoning (Du et al., 2023; Hao et al., 2023; Zhou et al., 2024; Huang et al., 2022a) by using human language to assist people in daily activities, such as housework and daily chores. The next milestone for agents is to cooperate with others to achieve joint tasks. This is crucial not only for efficiently performing simple tasks but also for tackling complex ones that cannot be completed in isolation due to their inherent complexity or the dynamic nature of the environment (Zhang et al., 2023b; Guo et al., 2024; Mandi et al., 2023; Zhang et al., 2023a). Figure 1: Procedure example of task accomplishment of CoELA (Zhang et al., 2023b) and our CaPo. In CoELA, after each action execution, Alice and Bob communicate to decide next action which is a greedy single-step plan and suboptimal. For example, they do not use wood basket which can contain sever objects, and both extemporaneously move a single item to the target bed without a long-term strategic and collaborative plan. Differently, in CaPo, Alice and Bob first discuss to make a long-term meta-plan for strategical cooperation in which Alice is arranged to move several target items into a wood basket, and Bob moves the remaining target items and also searches the unknown objects. Then during execution phase, both follow the meta-plan to accomplish task, and dynamically adapt the meta-plan the latest task progress, ensuring its effectiveness and efficiency in coordinating agents. Notably, the cooperation among LLM-based embodied agents is rarely investigated despite being highly desired. Conventional works often focus on adopting reinforcement learning (RL) (Jiang & Lu, 2018; Liu et al., 2021; Wang et al., 2021) to explore the dynamics of cooperative behavior among non-LLM-based agents. In spite of their promising performance in certain scenarios, RL-based cooperation methods exhibit limited adaptability across different tasks (Dittadi et al., 2021; Cobbe et al., 2019), since they are often not trained on large-scale data and lack sufficient generalization ability. To solve this issue, in this work, we are particularly interested in the problem of “how to develop an effective collaboration framework for LLM-based agents”, since LLMs have revealed strong reasoning, planning, and communication ability across different tasks and thus are regarded as good agents’ brains. Among the limited related works, CoELA (Zhang et al., 2023b) proposes an LLM-based multi-agent cooperation framework in which after each action execution, agents communicate to devise a single-step plan for the next action. Despite its significant advancements, CoELA’s short-term, single-step planning, which lacks consideration for long-term strategic collaboration, often results in extemporaneous and incoherent actions among agents, leading to several potential issues. Firstly, without a long-term coherent collaboration plan, it leads to numerous redundant action steps and increased costs, since agents’ movement is not easy and is indeed expensive in the physical world. For instance, as shown in Fig. 1, for the object transport task, agent Alice and Bob do not use the wood basket which can contain several objects, and extemporaneously move their nearest target objects one by one, leading to inferior efficiency. Moreover, complex tasks are difficult to accomplish without thorough discussion and long-term collaboration, especially in (embodied) environments where each agent has only partial observations. Finally, without a long-term cooperative plan, agents’ extemporaneous actions can result in mistakes with severe consequences. For instance, in search-and-rescue missions, poor coordination can have dire outcomes, such as endangering human lives due to the complex nature of these operations. Contributions. To address the above issues, we propose a novel and effective Cooperative Plan Optimization (CaPo) framework that uses LLMs’ strong reasoning and planning ability to enhance the cooperation efficiency of LLM-based embodied agents. Inspired by human cooperation schemes (Tuomela, 1998; Thürmer et al., 2017), CaPo engages agents in multi-turn discussions to create and update a long-term strategic and coherent meta-plan, providing step-by-step guidance to coordinate agents and efficiently complete tasks. Specifically, to accomplish a task, CaPo consists of two phases: 1) meta-plan generation, providing long-term strategical and coherent guidance for coordinating agents, and 2) progress-adaptive meta-plan and execution, dynamically adapting the meta-plan to agents’ latest progress. In the first phase, agents analyze the task and discuss with other agents for collecting relevant information. Next, one agent is responsible for making a meta-plan which decomposes the task into subtasks with detailed accomplishment steps like agent allocations, and then collects the feedback from other agents for further meta-plan refinement. The steps of meta-plan generation and refinement will continue until all agents reach a consensus or the communication cost is exhausted. This approach ensures the thorough discussion and analysis of all agents, helping to make a long-term strategical and coherent meta-plan for efficiently coordinating all agents. For example, as illustrated in Fig. 1, in the object transport task, agents Alice and Bob are strategically assigned to different subtasks. In the second phase, as shown in Fig. 1, agents follow the meta-plan from the first phase, and focus on their assigned subtasks. As progress is made, agents may complete subtasks or make important observations, such as Alice in Fig. 1 discovering the object “iPhone” which is Bob’s target. Accordingly, agents dynamically adapt meta-plan to the latest task progress through multi-turn discussions, allowing Alice to handle the object “iPhone” and complete the task efficiently. This progress-adaptive approach ensures that the meta-plan remains effective in coordinating all agents, thereby enhancing cooperation efficiency. Finally, experimental results demonstrate that CaPo significantly improves task completion rates and efficiency compared to state-of-the-art (SoTA) methods on the widely used ThreeDworld Multi-Agent Transport task (Zhang et al., 2023b) (object transport task) and the Communicative Watch-And-Help task (Zhang et al., 2023b) (household chore task). For instance, on the ThreeDworld Multi-Agent Transport task, CaPo surpasses the SoTA CoELA by 16.7% and 4.7% in completion rate with GPT-3.5 and GPT-4 based agents, respectively"
https://arxiv.org/html/2411.04678v1,Socially-Aware Opinion-Based Navigation with Oval Limit Cycles,"When humans move in a shared space, they choose navigation strategies that preserve their mutual safety. At the same time, each human seeks to minimise the number of modifications to her/his path. In order to achieve this result, humans use unwritten rules and reach a consensus on their decisions about the motion direction by exchanging non-verbal messages. They then implement their choice in a mutually acceptable way. Socially-aware navigation denotes a research effort aimed at replicating this logic inside robots. Existing results focus either on how robots can participate in negotiations with humans, or on how they can move in a socially acceptable way. We propose a holistic approach in which the two aspects are jointly considered. Specifically, we show that by combining opinion dynamics (to reach a consensus) with vortex fields (to generate socially acceptable trajectories), the result outperforms the application of the two techniques in isolation.","I INTRODUCTION An increasing number of applications requires mobile robots to move through human-populated areas. Significant examples include automated guided vehicles (AGVs) used for intra-factory logistics and warehouse management, mobile robots used for shipping and delivery in urban environments, and assistive robots used to support the mobility of disabled and elderly users. The nature of these applications requires the robot to accomplish its task reliably and efficiently. At the same time, the safety of human bystanders must not be imperilled. Last but not least, the trajectories followed by the robot should be perceived as smooth and human-friendly as those followed by its human counterparts (this is even truer when the robot is carrying or guiding a human). To generate this kind of trajectory on a robot, we need to take inspiration from what humans do. When humans move in a shared and crowded space, they exchange a short but intense stream of non-verbal signals to decide which direction each of them should take. Once each human makes up her/his mind on the direction to follow, s/he implements the choice by modifying the trajectory that s/he will follow in the next few seconds. This modification minimises the energy spent and the strain on the junctures [1], but at the same time it respects the private space of the other humans [2]. This complex set of communication protocols, social rules and reactive adjustments to our trajectories operates in strict coordination. They are so deeply embedded in our social behaviour that their use hardly touches the conscious level. Socially-aware navigation, or simply social navigation, is a research area that seeks to approximate the logic of human behaviour summarised above on a robot. Related work. An intense research activity on social navigation in the past few years has delivered different strategies aimed at ensuring a safe and efficient interaction between robots and humans [3, 4]. Research in this area includes various approaches designed to address the complexities of navigating in dynamic environments shared with humans, with both proactive and reactive approaches being explored [5, 6]. In short, proactive strategies attempt to predict and adapt to human behaviour, while reactive strategies respond to real-time environmental changes. Reactive methods are essential when people in a crowd move depending on the surrounding situation, requiring rapid re-planning of the robot’s trajectory. However, as the number of people increases, accurate and fast path planning becomes more difficult, increasing the computational time required to find the optimal path. More computationally efficient reactive methods are artificial potential fields [7, 8], the velocity obstacle (VO) [9], and the social force model (SFM) [10, 11]. SFM describes pedestrian movement as influenced by social forces that guide acceleration towards desired velocities while maintaining interpersonal distances. Although effective for large groups, SFM struggles with individual-level interactions in open environments. Enhanced models, such as the Headed Social Force Model (HSFM) [12], incorporate pedestrian heading to better predict non-holonomic motion. Proactive strategies, on the other hand, account for mutual interactions in crowded situations, enabling robots to cooperate with humans. These approaches involve predicting human behavior and proactively planning a collision-free path. Examples include proactive models based on SFM [13], opinion dynamics [14], and dynamic path planning techniques such as the Morphing algorithm [15]. Sampling-based motion planning methods, such as Rapidly-exploring Random Tree (RRT) [16, 17] and Risk-RRT [18], are also commonly used for dynamic environments. However, these methods can struggle with the complexities of interactive behaviors in human-populated spaces. Finally, learning-based approaches, including reinforcement learning (RL) [19], deep RL (DRL) [20], and inverse RL (IRL) [21, 22], are increasingly being explored for crowd navigation. These methods are promising but often lack analytical tractability and can be computationally demanding in real-time scenarios. Paper Contribution. In the short report above, we have mentioned two important classes of work: proactive and reactive path planning. In the first class, the robot negotiates with humans the direction of motion that each should take, but the implementation of the decision is not guaranteed to be safe and does not meet the quality standards that the human requires. In the second, the decision is taken and the robot concentrates on implementing it through a human-friendly trajectory. The missing piece in this case is a mechanism that generates the “strategic” decision. In this paper, we propose to merge the two phases into a unified holistic approach. We use social dynamics to implement the negotiation phase [14], and potential fields with oval-shaped limit cycles to generate a socially-aware trajectory [8]. The opinion dynamics module generates a variable that encodes the decision to be made by the robot (‘turn right’, ‘turn left’ or ‘go straight’) and consequently this variable modifies the geometric parameters of the limit cycle. As shown in the paper, the combined use of the two techniques fills the conceptual gaps that we have identified for each of them, leading to a significant improvement in the resulting behaviour of the robot."
https://arxiv.org/html/2411.04672v1,Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent Reinforcement Learning,"This paper presents a semantic-aware multi-modal resource allocation (SAMRA) for multi-task using multi-agent reinforcement learning (MARL), termed SAMRAMARL, utilizing in platoon systems where cellular vehicle-to-everything (C-V2X) communication is employed. The proposed approach leverages the semantic information to optimize the allocation of communication resources. By integrating a distributed multi-agent reinforcement learning (MARL) algorithm, SAMRAMARL enables autonomous decision-making for each vehicle, channel assignment optimization, power allocation, and semantic symbol length based on the contextual importance of the transmitted information. This semantic-awareness ensures that both vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications prioritize data that is critical for maintaining safe and efficient platoon operations. The framework also introduces a tailored quality of experience (QoE) metric for semantic communication, aiming to maximize QoE in V2V links while improving the success rate of semantic information transmission (SRS). Extensive simulations has demonstrated that SAMRAMARL outperforms existing methods, achieving significant gains in QoE and communication efficiency in C-V2X platooning scenarios.","I-A Background With the rapid development of intelligent transportation systems (ITS) [1, 2, 3], ensuring safe and efficient transportation is increasingly crucial [4, 5, 6]. A key component of ITS is platooning systems, where multiple autonomous vehicles travel closely together to enhance traffic flow and safety [7, 8, 9, 10, 11]. In these systems, a designated platoon leader (PL) manages formation, while platoon members (PMs) maintain coordinated speed and distance. Effective intra-platoon and inter-platoon communication (platoon-to-platoon or platoon-to-infrastructure) is essential for optimizing overall efficiency and safety. Integrating cellular vehicle-to-everything (C-V2X) communication is imperative for achieving effective communication. C-V2X supports two main types: vehicle-to-vehicle (V2V), which allows vehicles to share cooperative awareness messages (CAMs) for synchronized movement, and vehicle-to-infrastructure (V2I), enabling communication with base stations for traffic and safety information [12, 13, 14, 15, 16, 17, 18]. These modalities facilitate timely responses to dynamic traffic conditions, enhancing the safety and efficiency of platooning systems [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]. However, increased connectivity and communication demands in C-V2X platooning introduce complexities in managing network resources, which are critical for reliable communication in autonomous driving [30, 31, 32, 33]. The frequency of information exchange among vehicles directly impacts their ability to react to obstacles, underscoring the need for efficient resource management [34, 35, 36, 37, 38]. A promising approach to these challenges is semantic communication [39], which focuses on transmitting meaningful information rather than raw data. This enhances data transmission efficiency in C-V2X systems by ensuring only relevant information is shared [40], improving decision-making and reducing unnecessary data transfer. As task complexity rises, there is a shift from unimodal to multi-modal tasks that integrate various data types [41, 42, 43, 44]. This integration fosters a richer understanding of the environment, enhancing network robustness and flexibility [45]. Traditional centralized resource management schemes often face inefficiencies and high signaling overhead due to their reliance on global information, particularly under dynamic channel conditions [46, 47]. To address these challenges, we propose a distributed semantic-aware multi-modal resource allocation (SAMRA) framework leveraging multi-agent reinforcement learning (MARL), termed SAMRAMARL111The source code has been released at: https://github.com/qiongwu86/Semantic-Aware-Resource-Management-for-C-V2X-Platooning-via-Multi-Agent-Reinforcement-Learning. By employing MARL, vehicles can make decentralized decisions based on local observations, reducing reliance on centralized control while enhancing scalability and adaptability to dynamic network conditions [48]. I-B Related Work and Motivation Resource allocation in platooning systems has garnered significant attention within intelligent transportation systems (ITS). Platooning involves multiple autonomous vehicles traveling in coordination, necessitating efficient communication between the platoon leader (PL), other platoon members (PMs), and infrastructure like base stations [49]. Various studies have explored traditional methods to address these challenges. For instance, Guo et al. [50] proposed a joint optimization approach for LTE-V2V radio resource allocation and vehicle control parameters to enhance platoon stability. Hong et al. [51] designed a framework using relays and adaptive distributed model predictive control (DMPC) to improve safety in failure scenarios. Wang et al. [52] developed a two-step resource allocation strategy optimizing platoon formation and power control through branch and bound methods. Wen et al. [53] focused on optimizing inter-vehicle communication topology in LTE-V2V networks. However, the dynamic nature of channel conditions complicates effective resource management due to uncertainties in estimating channel state information (CSI). To address these complexities, deep reinforcement learning (DRL) has emerged as a promising method for resource allocation in vehicular networks [54, 55, 56, 57]. For example, Liing et al. [54] utilized a multi-agent reinforcement learning (MARL) approach with deep Q-networks to enhance V2I capacity. Nasir et al. [55] proposed a model-free DRL-based power allocation scheme for wireless networks. Xu et al. [56] applied DRL for multi-objective resource allocation, focusing on transmission success and communication quality. Despite these advancements, distributed DRL still faces challenges in high-data-volume scenarios, leading to elevated signaling costs and delays, especially in dynamic environments [58]. In response, semantic communication has gained traction. Unlike traditional methods that focus on raw data transmission, semantic communication prioritizes the meaning behind the conveyed information. This approach can enhance coordination and safety in platooning systems [59]. For instance, Bourtsoulatze et al. [60] developed a method for efficient image transmission under low signal-to-noise ratio (SNR) conditions, while Huang et al. [61] used generative adversarial networks for semantic image compression. Moreover, integrating unimodal and multi-modal tasks in ITS is an important research area [62]. Unimodal tasks handle specific data types, while multi-modal tasks provide a comprehensive understanding of the environment [63]. However, existing studies often lack a unified resource management framework that leverages both semantic communication and the integration of unimodal and multi-modal tasks [64]. Despite these advancements, there remains a gap in the literature. Existing approaches have not effectively combined distributed DRL, semantic communication, and the handling of both unimodal and multi-modal tasks into a cohesive resource management framework. This paper proposes a novel approach to optimize resource management in platooning systems by leveraging the strengths of semantic communication and distributed decision-making. I-C Contributions In this work, we present a distributed semantic-aware multi-modal resource allocation (SAMRA) algorithm based on MARL, termed SAMRAMARL, tailored for platooning systems. The primary contributions of our research are summarized as follows: 1) We investigate the extraction of semantic and multi-modal information for C-V2X platooning systems, and redefine the metrics suitable for semantic and multi-modal data, as well as concept of quality of experience (QoE). 2) We formulate a joint optimization problem to maximize QoE and the success rate of semantic information transmission (SRS) in V2V links ,which is the first work to introduce semantic communication to address resource allocation challenges in platooning systems. 3) We design the SAMRAMARL algorithm, employing MARL to optimize various aspects of resource allocation, including channel assignment, power allocation, and the length of transmitted semantic symbols in both single-modal and multi-modal contexts. 4) Simulation results demonstrate that our SAMRAMARL algorithm significantly outperforms existing algorithms in terms of QoE and SRS. The rest of the paper is structured as follows: Section II introduces the system model and the formulated problem of maximizing QoE and SRS; Sections III present the proposed SAMRA algorithm; Section IV provides and discusses simulation results; and Section V concludes the paper."
https://arxiv.org/html/2411.04468v1,Magentic-One: A Generalist Multi-Agent Systemfor Solving Complex Tasks,"Modern AI agents, driven by advances in large foundation models, promise to enhance our productivity and transform our lives by augmenting our knowledge and capabilities. To achieve this vision, AI agents must effectively plan, perform multi-step reasoning and actions, respond to novel observations, and recover from errors, to successfully complete complex tasks across a wide range of scenarios. In this work, we introduce Magentic-One, a high-performing open-source agentic system for solving such tasks. Magentic-One uses a multi-agent architecture where a lead agent, the Orchestrator, plans, tracks progress, and re-plans to recover from errors. Throughout task execution, the Orchestrator also directs other specialized agents to perform tasks as needed, such as operating a web browser, navigating local files, or writing and executing Python code. Our experiments show that Magentic-One achieves statistically competitive performance to the state-of-the-art on three diverse and challenging agentic benchmarks: GAIA, AssistantBench, and WebArena. Notably, Magentic-One achieves these results without modification to core agent capabilities or to how they collaborate, demonstrating progress towards the vision of generalist agentic systems. Moreover, Magentic-One’s modular design allows agents to be added or removed from the team without additional prompt tuning or training, easing development and making it extensible to future scenarios. We provide an open-source implementation of Magentic-One, and we include AutoGenBench, a standalone tool for agentic evaluation. AutoGenBench provides built-in controls for repetition and isolation to run agentic benchmarks in a rigorous and contained manner – which is important when agents’ actions have side-effects. Magentic-One, AutoGenBench and detailed empirical performance evaluations of Magentic-One, including ablations and error analysis are available at https://aka.ms/magentic-one.","Recent advances in artificial intelligence and foundation models are driving a renewed interest in agentic systems that can perceive, reason, and act in the world to complete tasks on our behalf [32, 59]. These systems promise to enhance our productivity by relieving us from mundane and laborious tasks, and revolutionize our lives by augmenting our knowledge and capabilities [16, 54, 6]. By leveraging the powerful reasoning and generative capabilities of large language models (LLMs), agentic systems are already making strides in fields like software engineering [66, 55], data analysis [4], scientific research [26, 7] and web navigation [79, 75]. Realizing the vision of agentic systems to transform our lives requires these systems to not only achieve high performance in specific domains, but also to generalize to the diverse range of tasks people may encounter throughout their day-to-day work and personal lives. In this paper, we take steps towards creating such a generalist agentic system by introducing Magentic-One.111The name Magentic-One is a combination of the words multi and agentic. Magentic-One uses a team ofagents, each specializing in generally-useful skills, such as: operating a web browser, handling files, and executing code. The team is directed by an Orchestrator agent which guides progress towards a high-level goal by iteratively planning, maintaining working memory of progress, assigning tasks to other agents, and retrying upon encountering errors. The Orchestrator uses two structured ledgers to achieve this and also to decide which agent should take the next action. Together, Magentic-One’s agents achieve strong performance on multiple challenging agentic benchmarks. Figure 1 shows an example of Magentic-One solving one such benchmark task that requires multiple steps and diverse tools. Key to Magentic-One’s performance is its modular and flexible multi-agent approach [51, 28, 53, 13, 52], implemented via the AutoGen222https://github.com/microsoft/autogen framework [60]. The multi-agent paradigm offers numerous advantages over monolithic single-agent approaches [51, 53, 6, 62], which we believe makes it poised to become the leading paradigm in agentic development. For example, encapsulating distinct skills in separate agents simplifies development and facilitates reusability, akin to object-oriented programming. Magentic-One’s specific design further supports easy adaptation and extensibility by enabling agents to be added or removed without altering other agents, or the overall workflow, unlike single-agent systems that often struggle with constrained and inflexible workflows. To rigorously evaluate Magentic-One’s performance, we introduce AutoGenBench, an extensible standalone tool for running agentic benchmarks. AutoGenBench’s design enables repetition, isolation, and strong controls over initial conditions, so as to accommodate the variance of stochastic LLM calls, and to isolate the side-effects of agents taking actions. Using AutoGenBench, we evaluated Magentic-One on three agentic benchmarks. We observed task-completion rates of 38% on GAIA [29] and 32.8% on WebArena [79]; and attained an accuracy of 27.7% on AssistantBench [71]. These results place Magentic-One in a strong position, where it is statistically competitive with other state-of-the-art (SOTA) systems, including those that are specialized for a given benchmark. Follow-up ablation experiments and in-depth error analyses reveal the additive value of each agent to Magentic-One’s performance, and highlight opportunities for further improvement. In summary, we contribute: 1. Magentic-One, a generalist multi-agent team with an open-source implementation. The team consists of five agents: a Coder, Computer Terminal, File Surfer, Web Surfer, and Orchestrator. Different agents can operate relevant tools such as stateful Web and file browsers, as well as command line and Python code executors. The Orchestrator performs several functions to guide progress towards accomplishing a high-level goal: it formulates a plan, maintains structured working memory of progress, directs tasks to other agents, restarts and resets upon stalling, and determines task completion. 2. AutoGenBench, a standalone tool for evaluating systems on agentic benchmarks, also made available open-source.333https://aka.ms/agbench AutoGenBench handles configuring, running, and reporting performance of agentic solutions while ensuring that all experiments start with well-known initial conditions, and that agents cannot interfere with one another across runs. 3. Experimental results and analyses of Magentic-One’s performance on the GAIA, WebArena, and AssistantBench benchmarks, demonstrating strong task completion rates which are statistically competitive with other SOTA systems. We also examine the contribution of individual agents and capabilities, and provide an error analysis to identify the strengths and weaknesses of our multi-agent approach, along with opportunities for improvement."
https://arxiv.org/html/2411.03865v1,AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making,"Traditional interactive environments limit agents’ intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.","Classic learning environments [55, 41, 9, 42, 34] have agents trained in small and stationary worlds, which hinders the improvement of agents’ intelligence. The learning process stagnates once the environments can no longer provide novel data for agents’ explorations. Additionally, agents trained on a fixed task set may suffer from a loss of generalization ability [13]. Single-agent environments [18, 25, 61] set out to solve this problem by constructing adaptive environments that continuously generate new tasks based on agent actions, providing a multitudinous task set. In multi-agent settings, however, the task set is determined by not only physical surroundings but also social connections among agents. Social connections dramatically impact agents’ decision-making by shaping their reward structures and information access [20], and different social structures endow the environments with radically different research problems. For example, centralized scenarios focus on issues like credit assignment and consensus establishment [21, 44], while decentralized settings require agents to address opponent modeling issues and non-stationarity [3, 21, 29, 33]. What makes the problem even more challenging is that social connections are not predefined but adaptive, which means there’s a dynamical interplay between the topology of social connections and agents’ states [23]. The adaptive nature of social connections and physical surroundings requires agents to learn continuously, reason about other agents’ policies, and balance between physical explorations and establishing social connections. While contemporary multi-agent decision-making environments [6, 2, 53, 66, 48] have achieved great progress in stimulating and testing capabilities of learning algorithms in fixed task sets, they fail to generate new tasks by concurrently considering expanding physical surroundings and adaptive social connections. To bridge this gap, we propose AdaSociety, a multi-agent environment with massive and diverse tasks generated by adaptive social connections and expanding physical surroundings, which are influenced by agents’ behavior. In particular, to the best of our knowledge, AdaSociety first introduces social states (expressed as a multi-layer directed graph) to explicitly and quantitatively describe the adaptive and dynamic connections between entities, including agents and emerged organizations. This greatly enriches the diversity of tasks, supporting the establishment of stable and long-term relations between entities and the quantitative study of social intelligence, like coalition formation and the emergence of hierarchy. In such an environment, agents need to balance the exploration of physical surroundings and the alteration of social connections, leading to multiple possible victory paths and significant decision-making challenges. To stimulate algorithm design and theoretical analysis in AdaSociety, we provide a formulation of the multi-agent decision-making problems, named Growing-MG (Sec. 3). AdaSociety serves as a platform for researchers to customize the environment for diverse research needs. Specifically, a set of fundamental elements and mechanisms can be used, and interfaces are provided to set environment attributes and hyper-parameters. Moreover, AdaSociety exhibits its characteristics by offering three mini-games, where both tensor- and LLM-based methods are tested. In summary, this paper makes three contributions. 1) We introduce a novel multi-agent general-sum environment featuring expanding physical surroundings and adaptive social connections. 2) We offer a customizable environment with three built-in mini-games, supporting both tensor- and LLM-based methods. 3) We implement RL and LLM methods in these mini-games and provide preliminary results, laying the groundwork for further research in this environment. Figure 1: An overview of AdaSociety, composed of physical component and social component. Physical Component consists of diverse resources and events on the map and heterogeneous agents’ inventories. Social Component describes the adaptive connections between agents and organizations, which shape information access and reward structure. Agents take social actions to alter their social connections. As shown in the rightmost flowchart, agents are initially independent and can establish individual connections (edges between nodes) and form groups (gray ovals)."
https://arxiv.org/html/2411.03603v1,CPEG: Leveraging Consistency Policy withConsensus Guidance for Multi-agent Exploration,"Efficient exploration is crucial in cooperative multi-agent reinforcement learning (MARL), especially in sparse-reward settings. However, due to the reliance on the unimodal policy, existing methods are prone to falling into the local optima, hindering the effective exploration of better policies. Furthermore, tackling multi-agent tasks in complex environments requires cooperation during exploration, posing substantial challenges for MARL methods. To address these issues, we propose a Consistency Policy with consEnsus Guidance (CPEG), with two primary components: (a) introducing a multimodal policy to enhance exploration capabilities, and (b) sharing the consensus among agents to foster agent cooperation. For component (a), CPEG incorporates a Consistency model as the policy, leveraging its multimodal nature and stochastic characteristics to facilitate exploration. Regarding component (b), CPEG introduces a Consensus Learner to deduce the consensus on the global state from local observations. This consensus then serves as a guidance for the Consistency Policy, promoting cooperation among agents. The proposed method is evaluated in multi-agent particle environments (MPE) and multi-agent MuJoCo (MAMuJoCo), and empirical results indicate that CPEG not only achieves improvements in sparse-reward settings but also matches the performance of baselines in dense-reward environments.","Recent years have witnessed a growing body of applications in cooperative multi-agent reinforcement learning (MARL), such as multi-robot tasks [1] and autonomous driving [2]. Despite these successful applications, cooperative MARL still faces challenges in exploration due to limitations in the policy class regarding multi-modality and the necessity for cooperation in multi-agent systems (MAS) [3]. In single-agent RL, the exploration quality heavily depends on the chosen policy class of agents [4, 5, 6], with inappropriate policy classes potentially leading to local optima. This issue becomes more pronounced in MARL owing to the complex interaction between agents. In MARL, popular methods [7, 8, 9] commonly formulate the continuous policy as a unimodal density function, typically a Gaussian distribution. While computationally efficient, these policies can significantly weaken the exploration, as the sampled actions tend to be concentrated around the modality. Besides, the unimodal policy is prone to converging towards a suboptimal policy due to the lack of expressiveness, overfitting the behavior of other agents. To address these challenges, some methods [10, 4, 11] explore alternative policy classes to enhance exploration. Although these methods improve exploration to some extent, they often exhibit limitations in practice. For example, Gaussian mixture models can only cover a limited number of modes, and normalizing flow methods, while able to compute density values, suffer from numerical instability due to their determinant dependence. With the growing prevalence of diffusion models [12], a series of works [13, 14, 15] apply them as a powerful and multi-modal policy class, primarily in the context of single-agent offline RL. However, the diffusion model is time-intensive, involving multiple sampling steps (e.g., 1000 steps), which makes the training and execution computationally expensive, especially for the online MARL heavily depends on sampling from environments. In order to accelerate the speed of the sampling process, a novel model, the consistency model is designed to map any point at any time step back to the start of the trajectory, based on the probability flow ordinary differential equation (PF-ODE) [12]. Figure 1: An example of cooperative exploration. The two-agent arm requires collaborative exploration to reach four targets at different locations. In a sparse reward setting, agents must reach all targets before receiving any reward, making exploration more challenging. Besides, the cooperation among agents during exploration is critical in complex environments [16, 17, 3]. To exemplify, consider the Reacher4 task depicted in Fig. 1, wherein two agents are required to coordinate their swinging motions to ensure that the end-effector of the manipulator contacts one of the four targets. In a sparse reward setting, agents must reach all targets before receiving any reward, making exploration more challenging. In this scenario, the agents need to coordinate their actions to set the entire robotic arm in motion, instead of engaging in independent and meaningless actions. Additionally, the agents need to adapt to the multimodality brought by the four targets, avoiding premature convergence to a unimodal policy and achieving simultaneous exploration of multiple targets. This challenge can be tackled through the use of communication-based MARL techniques [18]. However, these techniques introduce challenges such as selecting appropriate information to transmit and additional bandwidth requirements. Recently, some studies [19, 20] leverage a shared consensus among agents to promote cooperation. At each timestep, although the local observations of each agent are unique, they represent different aspects of the same global state. In the given scenario, observations of the agents only cover their own information such as angular velocity, which serves as partial representations of the global state. However, these individual observations are merely projections of the state. Consequently, our objective is to extract the global state information through consensus, thereby fostering cooperation. Inspired by the aforementioned observations, we introduce a novel framework tailored to MARL, termed the Consistency Policy with consEnsus Guidance (CPEG), aimed at facilitating efficient cooperative exploration. In this work, we adopt a powerful and effective model, the consistency model as the policy class, with the objective of harnessing its stochastic nature and expressiveness to explore in a multimodal manner. Compared with diffusion models, consistency models enable efficient one-step generation while retaining the advantages of multi-step iterative sampling. For cooperative exploration, we introduce a discrete consensus representation as the guidance for the consistency policy. Specifically, we employ a codebook from VQ-VAE [21] for a discrete, distinguishable consensus representation. This allows agents to derive the same estimation of the global state from a unified consensus codebook, thereby guiding cooperative behaviors. To strike a balance between exploration and exploitation during training, some studies [6, 22] use the mask to decide on exploration or exploitation. Building on the insights from these work, we design a guidance mask to intermittently drop the guidance with probability. Additionally, during the initial phase of training, generative policies may produce unreliable actions, which can affect the training of the policy. To address this, we introduce a self-reference mechanism during the training phase, leveraging past successful experiences to constrain the actions generated by the consistency policy, thus facilitating exploration. We empirically evaluate the proposed method on two distinct environments: the multiple-particle environment (MPE) [7] and multi-agent MuJoCo (MAMuJoCo) [23]. Across all experiments, we explore both dense-reward settings and sparse-reward settings, where the latter poses significant challenges for exploration as agents receive rewards solely upon completion of a specified task. The results demonstrate that CPEG exhibits notable superiority over competitive baselines in sparse-reward settings and achieves comparable performance in dense-reward settings, thereby underscoring the effectiveness of our approach. Our main contributions are summarized as follows: 1. We introduce a Consistency Policy for agents, capable of completing the diffusion process in a single step, to explore the environment in a multimodal way. To the best of our knowledge, our study represents the first effort in leveraging consistency policies in MARL. 2. We propose a Consensus Learner to infer the global consensus from local observations, aimed at guiding efficient cooperative exploration by sharing the same consensus between agents. 3. We incorporate a Self-reference Mechanism to constrain the generated actions by leveraging past successful experiences, thereby reducing the probability of generating invalid actions by policies. 4. We evaluate our method on MPE and MAMuJoCo with dense and sparse reward settings. The results indicate that CPEG performs comparably to state-of-the-art (SOTA) algorithms in dense reward environments, while in sparse reward environments, which are more challenging for exploration, our algorithm outperforms SOTA algorithms by 20%."
https://arxiv.org/html/2411.04073v1,Rescheduling after vehicle failures in the multi-depot rural postman problem with rechargeable and reusable vehicles,"We present a centralized auction algorithm to solve the Multi-Depot Rural Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing on rescheduling arc routing after vehicle failures. The problem involves finding a heuristically obtained best feasible routes for multiple rechargeable and reusable vehicles with capacity constraints capable of performing multiple trips from multiple depots, with the possibility of vehicle failures. Our algorithm auctions the failed trips to active (non-failed) vehicles through local auctioning, modifying initial routes to handle dynamic vehicle failures efficiently. When a failure occurs, the algorithm searches for the best active vehicle to perform the failed trip and inserts the trip into that vehicle’s route, which avoids a complete rescheduling and reduces the computational effort. We compare the algorithm’s solutions against offline optimal solutions obtained from solving a Mixed Integer Linear Programming (MILP) formulation using the Gurobi solver; this formulation assumes that perfect information about the vehicle failures and failure times are given. We derived a set of 257 failure scenarios from arc routing instances in the literature and used them to perform a competitive analysis. For each scenario we used a simulated annealing algorithm to generate an initial set of routes and then used the centralized auction algorithm to reschedule after each vehicle failure. The results demonstrate that the centralized auction algorithm produces solutions that are, in some cases, near-optimal; moreover the execution time for the proposed approach is much more consistent and is, for some instances, orders of magnitude less than the execution time of the Gurobi solver. The theoretical analysis provides an upper bound for the competitive ratio and computational complexity of our algorithm, offering a formal performance guarantee in dynamic failure scenarios.","Unmanned battery-operated rechargeable vehicles are becoming more prevalent in real-world applications due to their cost-effectiveness and efficiency [1, 2]. However, these systems still face significant challenges. The failure rate for drones is approximately 1 in 1,000 flight hours, two orders of magnitude higher than commercial aviation’s 1 in 100,000 flight hours, and sophisticated Unmanned Aerial Vehicle (UAV) systems face an overall failure rate of 25% [3]. These failures can lead to significant delays and disruptions, underscoring the need for improved reliability in unmanned vehicle operations. Although many preventive maintenance approaches have been proposed to increase the reliability of unmanned vehicles [3, 4], any failure during a mission requires changing the mission plan to react to the loss of the vehicle. This paper, therefore, proposes an approach for effectively managing and mitigating the impact of vehicle failures on routing after they occur, specifically addressing the challenges of rerouting and task reallocation to ensure mission completion despite unexpected vehicle breakdowns. In the MD-RPP-RRV, the vehicles have limited capacity (operation time) but can be recharged and reused for multiple trips from multiple depots to traverse a subset of edges (required edges) in a weighted undirected connected graph, minimizing mission time. The maximum time taken by vehicles to traverse all required edges is referred to as mission time or maximum trip time. One of the key assumptions considered in our previous study [5] to solve the MD-RPP-RRV was that vehicles do not fail during their trips. In this study of the MD-RPP-RRV with vehicle failures, we relax that assumption and consider that multiple (but not all) vehicles might fail randomly during their trips. This study developed and evaluated a rescheduling approach that reacts to vehicle failures; it requires no information about the vehicle failures before the vehicles begin following their routes. Studying the MD-RPP-RRV with vehicle failures is crucial for addressing real-world challenges in applications like parcel delivery, infrastructure inspection, and surveillance, where unmanned vehicles may encounter failures during operation, necessitating the development of quick rerouting approaches for remaining active vehicles to ensure mission completion. The MD-RPP-RRV is NP-hard to solve [5] as it generalizes the RPP, which is proven to be NP-hard [6]. In its simplest case, with a single depot and single trip, the MD-RPP-RRV reduces to the RPP. Hence, solving the MD-RPP-RRV with vehicle failures poses significant computational challenges due to the additional complexities introduced by random vehicle failures. This paper proposes a centralized auction algorithm to address the MD-RPP-RRV with vehicle failures. We chose a centralized approach over decentralized methods due to its ability to maintain a global perspective, enabling quicker decision-making and more efficient task reallocation essential for handling multiple random vehicle failures dynamically. Our approach efficiently reassigns trips that a failed vehicle was supposed to complete (considered as tasks) to the remaining active vehicles (considered as agents) with the objective of minimizing the increase in mission time. To evaluate the performance of our proposed algorithm, we compared its solutions against offline optimal solutions obtained from solving the Mixed Integer Linear Programming (MILP) formulation using the Gurobi optimizer with vehicle failures known beforehand. We also empirically and theoretically analyzed the competitive ratio to assess the proposed algorithm’s solution quality relative to the offline optimal solution. The main contributions of this paper are the following: 1. A centralized auction algorithm that reformulates the MD-RPP-RRV as a variant of the Generalized Assignment Problem (GAP) [25]. This approach efficiently handles dynamic vehicle failures by reassigning tasks without complete rerouting, reducing computational complexity. Our method addresses a gap in existing auction approaches [34] by applying them to dynamic failures in arc routing problems, specifically the MD-RPP-RRV, which has not been previously explored in this context. This algorithm extends centralized auction methods [36, 37] to handle more complex task allocation scenarios in the MD-RPP-RRV context in the following ways: (a) Assigning multiple failed trips (tasks) to a single vehicle (agent). (b) Dynamically reallocating trips from failed vehicles to active ones during the mission, thereby adapting to changes in the available vehicle fleet size due to failures. This approach addresses limitations in existing methods that typically assign only one task per agent or assume a fixed number of agents throughout the mission. 2. Experimental results that describe the quality of the solutions that the approach generates and the execution time required. 3. A theoretical upper bound for the competitive ratio of our proposed centralized auction algorithm to solve the MD-RPP-RRV with vehicle failures. This analysis provides a formal performance guarantee for our algorithm in dynamic failure scenarios. The remainder of this paper is organized as follows: Section 2 presents a literature review of related works. Section 3 provides the assumptions and presents a MILP formulation for the MD-RPP-RRV with known vehicle failures and failure times. Section 4 describes the proposed centralized auction algorithm. Section 5 presents our experimental results. It details the testing of our proposed algorithm on failure scenarios created from benchmark instances. It also compares the quality of the solutions with offline optimal solutions that were obtained by solving the MILP formulation using the Gurobi solver. Section 6 concludes the paper."
https://arxiv.org/html/2411.02584v1,Multi-Agent Decision Transformers for Dynamic Dispatching in Material Handling Systems Leveraging Enterprise Big Data,"Dynamic dispatching rules that allocate resources to tasks in real-time play a critical role in ensuring efficient operations of many automated material handling systems across industries. Traditionally, the dispatching rules deployed are typically the result of manually crafted heuristics based on domain experts’ knowledge. Generating these rules is time-consuming and often sub-optimal. As enterprises increasingly accumulate vast amounts of operational data, there is significant potential to leverage this big data to enhance the performance of automated systems. One promising approach is to use Decision Transformers, which can be trained on existing enterprise data to learn better dynamic dispatching rules for improving system throughput. In this work, we study the application of Decision Transformers as dynamic dispatching policies within an actual multi-agent material handling system and identify scenarios where enterprises can effectively leverage Decision Transformers on existing big data to gain business value. Our empirical results demonstrate that Decision Transformers can improve the material handling system’s throughput by a considerable amount when the heuristic originally used in the enterprise data exhibits moderate performance and involves no randomness. When the original heuristic has strong performance, Decision Transformers can still improve the throughput but with a smaller improvement margin. However, when the original heuristics contain an element of randomness or when the performance of the dataset is below a certain threshold, Decision Transformers fail to outperform the original heuristic. These results highlight both the potential and limitations of Decision Transformers as dispatching policies for automated industrial material handling systems.","Dynamic dispatching, the process of dispatching resources in real-time in response to system conditions, plays a critical role in ensuring smooth and efficient operations in many industrial applications. Subsequently, this translates to additional business value, such as cost savings and increased customer satisfaction. One area where the deployment of dynamic dispatching has a large impact is in material handling systems, where goods are typically transported between multiple points under the constraint of limited resources. Traditionally, dynamic dispatching in these systems is often deployed using heuristic rules manually designed via a trial-and-error process or by a domain expert. Dynamic dispatching and scheduling are also applied in a wide variety of fields, not just material handling systems, and there have been multiple works that attempt to generate dispatching rules via domain knowledge and heuristics Dhurasevic and Jakobovic (2018); Branke et al. (2015). These approaches are time-consuming, and the availability of domain experts is usually not guaranteed. Another approach to deploying dynamic dispatching strategies is to employ optimization-based methods Qin et al. (2021); Zhang et al. (2023). Nevertheless, a major limitation of this category of approaches is that they can be time-consuming and computationally inefficient to compute a solution whenever a dispatch decision is required. More recently, data-driven methods such as machine learning (ML) and reinforcement learning (RL)-based approaches has also been proposed Priore et al. (2014); Ding et al. (2023). While ML-based approaches focuses on imitating the performance of static datasets, RL-based dynamic dispatching policies explore a given environment during training to discover better policies Kang et al. (2019); Jeong et al. (2021); Zeng et al. (2023); Lee et al. (2024). As such, RL-based approaches avoid the lengthy trial-and-error process of designing heuristics and circumvent the need to solve an optimization every time dispatching is needed. However, the training process remains the main bottleneck in deploying RL-based policies. It is often not feasible to train RL policies on actual systems due to safety considerations Gu et al. (2022) and due to the lengthy training duration required because of the large number of interactions typically necessary to train a good policy. Subsequently, most RL policies are trained on simulators, which provide a safe place for RL policies to explore and enable training beyond real-time speed. Nevertheless, developing a simulator is also often costly, and RL policies trained in simulator environments often suffer from the sim-to-real gap Zhao et al. (2020); Salvato et al. (2021). To address the complexity of training online RL-based algorithms, numerous works in the literature have advocated the idea of offline RL: that is, how to maximally extract an optimal policy given a static dataset Prudencio et al. (2023); Levine et al. (2020). With the proliferation of big data in industrial settings, vast amounts of historical operational data are often available, providing a rich resource for training more effective and robust policies and potentially leading to improved system performance. In the context of using RL for dynamic dispatching in material handling systems, this is a compelling paradigm, as a dataset could be collected based on historical real-time data using a sub-optimal dispatching policy, which could then be used to train a better RL-based policy. Unfortunately, offline-RL methods are known to be over-optimistic in terms of value estimation Kumar et al. (2020) and high variance of reward/gradient estimation Levine et al. (2020), which could result in destabilized policy training. In response, many efforts in the community have been directed at developing better offline-RL methods to mitigate these issues while still leveraging the benefits of historical data and RL-based approaches Levine et al. (2020); Agarwal et al. (2020); Prudencio et al. (2023). Recently, Chen et al. (2021) proposed to reformulate the offline RL problem into a sequence modeling problem by leveraging the powerful modeling capabilities of a transformer architecture Vaswani (2017). They proposed Decision Transformers, which have been shown to be a strong alternative to existing offline RL algorithms without explicitly learning a value function Bhargava et al. (2023), and has been applied in multiple areas of work Yuan et al. (2024). As most existing work of Decision Transformers are mainly focused on benchmark problems, in this work, we are motivated to study if Decision Transformers can also be applied effectively to complex, real-world industrial problems due to their ability to leverage existing enterprise data, simplicity in terms of implementation and potential to be trained in a large scale, parallelized setting. Specifically, we investigate the feasibility of using Decision Transformers as a first step in developing data-driven dynamic dispatching strategies in a multi-agent setting to maximize the throughput of a material handling system. Our contributions are focused on answering the following questions: • Can Decision Transformers train on existing enterprise operational data to discover more effective policies within a complex real-world material handling system where dispatching decisions are required at different points of the system asynchronously, i.e., given the multi-agent asynchronous setting, can independent Decision Transformers “stitch” lower reward trajectories from sub-optimal heuristics to achieve higher rewards during testing? • Previous work by Paster et al. (2022) has shown that Decision Transformers performs badly when trained on stochastic environments and we aim to study how are independent Decision Transformers deployed in multi-agent settings affected by environmental and data stochasticity? • How do datasets generated from different heuristic qualities affect the final performance of the Decision Transformers?"
https://arxiv.org/html/2411.02524v1,3DM-Explore: 3D Mapping for Multi-Robot Exploration for Unknown Indoor Environments,"Multi-Robot Exploration (MRE) is pivotal in advancing robotic autonomy, particularly for applications demanding sustained situational awareness such as , search and rescue, and intelligent transportation systems. Recent developments have focused on enhancing the efficiency and resilience of multi-robot SLAM algorithms, incorporating diverse sensor modalities and robust communication frameworks. While significant progress has been made with lidar-based collaborative SLAMs and large-scale visual SLAM systems, the research on visual SLAM in indoor environments has been comparatively limited. This gap is critical, as the dynamic interactions among robots in such settings present unique challenges. State-of-the-art approaches like Kimera-Multi and CORB-SLAM have demonstrated effectiveness in outdoor environments but struggle indoors, often leading to issues such as the ”ghosting trail effect” that compromise map quality. Our research proposes a fully distributed approach to multi-robot exploration in indoor settings, addressing the limitations of existing methods. By focusing on generating dense metric 3D mesh models and accounting for the complex geometric structures of indoor environments, we aim to improve mapping efficiency while adhering to strict communication bandwidth constraints, thereby advancing the field of collaborative visual SLAM in challenging indoor scenarios.","Multi-Robot Exploration (MRE) plays a crucial role in robotics research because it enhances situational awareness over extended periods. This capability is essential for a wide range of applications, including environmental monitoring and patrolling [espina2011multi], search and rescue operations [10611179], intelligent transportation systems [li2018corb]. The objective of MRE is to synergistically enhance the autonomous navigation and mapping capabilities of a coordinated robotic system, optimizing spatial comprehension, cost-effectiveness, travel time, and energy utilization. Recently, there have been substantial advancements in developing more efficient and resilient MRE algorithms and systems, incorporating various objectives, sensor modalities and communication frameworks. Frontier-Based exploration methods have garnered substantial attention for their capacity to accelerate exploration processes. However, the objectives of recent works are primarily focused on creating efficient 2D maps, optimizing cost, and minimizing travel time. Their suitability for 3D exploration remains inadequate for achieving notable performance enhancements. For instance, most of the existing literature relies on identifying frontiers using computer vision based approaches on grid maps. These approaches, while effective in 2D contexts, does not translate well to constructing efficient 3D maps, highlighting a critical gap in current methodologies. How we calculate the frontiers. In comparison to the experiments of lidar-based collaborative exploration and large-scale visual exploration frameworks, the evaluative research on visual MRE within indoor environments remains proportionally lacking. This shortfall is particularly significant given that the intrinsic dynamism of robot interactions within these settings presents unique challenges that must be addressed. The existing visual MRE state-of-the-art appraoches such as Kimera-Multi [tian2022kimera], CORB-SLAM [li2018corb] proven thier efficiency towards outdoor environments, where inter-robot visibility to each other is neglected. However, these methodologies often falter in indoor scenarios, leading to a ”ghosting trail effect” that significantly degrades the quality of the generated maps. Distinct from previous studies on vision-based MRE systems, our research emphasizes a fully distributed approach to multi-robot exploration. We enhance the efficiency of generating dense metric 3D mesh models by incorporating ”Mutual-Awareness” within environments in which the robots operate, all while operating under uncertain constraints on communication bandwidth. Overall, this paper proposes 3DM-Explore integrating exploration algo to enhance the efficacy of 3D exploration and localization within indoor scenarios. data efficiency and communication aspects. Contributions: Novelty:"
https://arxiv.org/html/2411.01553v1,Learning to Construct Implicit Communication Channel,"Effective communication is an essential component in collaborative multi-agent systems. Situations where explicit messaging is not feasible have been common in human society throughout history, which motivate the study of implicit communication. Previous works on learning implicit communication mostly rely on theory of mind (ToM), where agents infer the mental states and intentions of others by interpreting their actions. However, ToM-based methods become less effective in making accurate inferences in complex tasks. In this work, we propose the Implicit Channel Protocol (ICP) framework, which allows agents to construct implicit communication channels similar to the explicit ones. ICP leverages a subset of actions, denoted as the scouting actions, and a mapping between information and these scouting actions that encodes and decodes the messages. We propose training algorithms for agents to message and act, including learning with a randomly initialized information map and with a delayed information map. The efficacy of ICP has been tested on the tasks of Guessing Number, Revealing Goals, and Hanabi, where ICP significantly outperforms baseline methods through more efficient information transmission.","Effective communication is pivotal in collaborative multi-agent systems, especially in environments characterized by incomplete information (Panait & Luke, 2005; Busoniu et al., 2008; Tuyls & Weiss, 2012). Communication acts as a vital conduit, enabling agents to exchange private information, coordinate joint actions, and infer real-world states (Wang et al., 2021). These processes synergistically foster tighter cooperation and enhance collective performance (Li et al., 2002; Cao et al., 2012). We focus on multi-agent reinforcement learning (MARL) methods for communication, where communication is broadly categorized into explicit and implicit strategies (Dafoe et al., 2020). Explicit communication uses direct channels independent of the environment dynamics (Sukhbaatar et al., 2016; Foerster et al., 2016; Jiang & Lu, 2018), allowing agents to transmit observations, intentions, and advice to facilitate decision-making and coordination (Zhu et al., 2022; Qu et al., 2021). This approach, analogous to human language or verbal exchanges (Havrylov & Titov, 2017; Baker et al., 1999), has been widely employed in MARL to enhance collaboration. However, dependence on direct channels introduces significant computational and memory overheads (Roth et al., 2006), which makes it challenging to implement in certain scenarios, like tasks without communication channels or decentralized frameworks (Oliehoek et al., 2008; Kraemer & Banerjee, 2016). Situations where explicit messaging is not feasible have been common in human society throughout history. From early humans engaging in hunting and gathering through silent cooperation (Klein, 2009; Tomasello & Vaish, 2013), to modern military operations using gestures and codes for covert communication (Tzu, 2008), and even in everyday social interactions where intentions are conveyed through expressions, tone, and body language (Pease, 1984; Duncan Jr, 1969). Implicit communication has established an effective mechanism for information sharing without explicit language. Learning methods for implicit communication have been investigated by the MARL community. A prominent method is the theory of mind (ToM) (Premack & Woodruff, 1978), where agents infer the mental states and intentions of others by interpreting their actions (Heider & Simmel, 1944). By modeling the beliefs, desires, and intentions of other agents, ToM enables agents to coordinate in a variety of simple tasks (Baker et al., 2017; Zhao et al., 2023; Nguyen et al., 2020). However, ToM-based approaches face significant challenges, including the difficulty of making accurate inferences and the high computational complexity involved in modeling other agents. These issues become particularly pronounced in dynamic environments where agents must constantly update their models based on limited or ambiguous information. To address these challenges associated with ToM methods, we introduce Implicit Channel Protocol (ICP), a novel framework that allows agents to construct communication protocols in implicit communication likewise how it was done in explicit communication. ICP leverages a subset of actions, denoted as the scouting actions, which have no or uniform effects on environment dynamics. A centralized mapping between information and these scouting actions is established to encode and decode the messages. Agents exchange information by deliberately taking scouting actions, forming an implicit communication channel. We further demonstrate how agents’ strategies are trained on this channel, including training with a randomly initialized information map and training with a delayed information map. We validate the effectiveness of ICP through comprehensive experiments on the tasks of Guessing Number, Revealing Goals, and Hanabi (Bard et al., 2020). These environments share a common characteristic: they lack direct communication but agents must collaboratively make decisions to achieve shared rewards. This setting introduces significant challenges, including sparse and delayed reward feedback, along with difficulty in credit assignment both temporally and among agents. Despite these hurdles, our experiments on Guessing Number and Revealing Goals demonstrate that ICP significantly enhances performance, through more efficient information transmission, compared to baseline methods. In Hanabi, which is a popular card game played by humans, our approach achieved an average score of 24.9124.9124.9124.91 out of 25252525, which surpasses the best available learning algorithm who obtains 23.8123.8123.8123.81."
https://arxiv.org/html/2411.01166v1,Role Play: Learning Adaptive Role-Specific Strategies in Multi-Agent Interactions,"Zero-shot coordination problem in multi-agent reinforcement learning (MARL), which requires agents to adapt to unseen agents, has attracted increasing attention. Traditional approaches often rely on the Self-Play (SP) framework to generate a diverse set of policies in a policy pool, which serves to improve the generalization capability of the final agent. However, these frameworks may struggle to capture the full spectrum of potential strategies, especially in real-world scenarios that demand agents balance cooperation with competition. In such settings, agents need strategies that can adapt to varying and often conflicting goals. Drawing inspiration from Social Value Orientation (SVO)—where individuals maintain stable value orientations during interactions with others—we propose a novel framework called Role Play (RP). RP employs role embeddings to transform the challenge of policy diversity into a more manageable diversity of roles. It trains a common policy with role embeddings observation and employ a role predictor to estimate the joint role embeddings of other agents, helping the learning agent adapt to its assigned role. We theoretically prove that an approximate optimal policy can be achieved by optimizing the expected cumulative reward relative to an approximate role-based policy. Experimental results in both cooperative (Overcooked) and mixed-motive games (Harvest, CleanUp) reveal that RP consistently outperforms strong baselines when interacting with unseen agents, highlighting its robustness and adaptability in complex environments.","Artificial Intelligence (AI) has achieved remarkable success in mastering a wide range of strategic and competitive games, as demonstrated by notable research in this area (Silver et al., , 2018; Vinyals et al., , 2019; Berner et al., , 2019). Significant advancements have also been made in cooperative settings, where agents are trained to collaborate with either humans or other agents to achieve common goals (Carroll et al., , 2019; Zhao et al., , 2023). However, in real-world applications such as autonomous driving, interactions among agents often display mixed motives, combining elements of both cooperation and competition (Schwarting et al., , 2019). In these mixed-motive environments, agents face complex interactions where each participant has distinct objectives. For example, in a public goods game, agents must carefully balance the benefits of contributing to a collective resource against the costs of their individual contributions (Gibbons et al., , 1992). These environments present significant challenges, requiring agents to develop sophisticated adaptation strategies to effectively interact with others who have varying incentives. Zero-shot coordination is well-recognized in the context of multi-agent reinforcement learning (MARL), particularly for agents that need to interact effectively with new partners they have not encountered during training (Hu et al., , 2020). Self play (SP) is an effective framework for this challenge (Lucas and Allen, , 2022; Lupu et al., , 2021; Zhao et al., , 2023). The SP-based frameworks typically build a policy pool through SP, utilized to enhance the generalization capabilities of the final agent. Various techniques aim to increase the diversity of this policy pool to improve the agent’s ability to generalize across different scenarios (Garnelo et al., , 2021; Zhao et al., , 2023). In mixed-motive games, there is a greater need for policy diversity to adapt to the varying and often conflicting goals resulting from the imperfect alignment of incentives among group members. However, the policy pool, primarily composed of past iterations of policies from a given population, captures only a limited range of the policy space. This limitation can prevent agents from effectively managing novel situations or policies not previously encountered in the training set. Unlike existing policy pool based works, our approach try to develop a general model which can generate policies with different value orientation. Given the inherent challenges in representing policies directly due to their complexity, we propose projecting the policy space into a more compact dimension. Inspired by Social Value Orientation (SVO) (McKee et al., , 2020), in which individuals maintain stable value orientations (roles) in interactions with others, we proposed Role Play (RP), which compress the vast MARL policy space into a more manageable “human role space.” This simplification aims to improve both the interpretability and efficiency of agent interactions. Furthermore, drawing on social intuition (Lieberman, , 2000; Jellema et al., , 2024) that humans estimate the behaviors of others during interactions to make better decisions, we introduce a role predictor to estimate the joint role embeddings of other agents, aiding the learning agent in adapting to its assigned role. This setup enables agents to learn and adapt to their assigned roles more effectively, enhancing their performance across various interactive scenarios. In this work, we introduce a novel framework, Role Play (RP), specifically designed to address the zero-shot coordination problem in multi-agent interactions. Our approach is distinguished by several key innovations: • Role Embedding: We utilize a sophisticated reward mapping function to project the extensive policy space into a more manageable role embedding space. This transformation facilitates structured and strategic navigation through the complex landscape of agent behaviors. We theoretically prove that an approximate optimal policy can be obtained by optimizing the expected cumulative reward with respect to an approximate role-based policy. • Role Predictor: Inspired by social intuition, we have developed a role predictor that estimates the joint role embeddings of other agents. This module enhances the agent’s ability to accurately predict and adapt to the role-based policies of other agents, enabling the learning agent to adapt more effectively to its assigned role. • Meta-task Learning: We employ meta-learning techniques to model agent interactions as meta-tasks, which allows the learning agent to extrapolate from limited experiences to new, unseen scenarios. This approach significantly improve the adaptability of the learning agent to different roles and strategies. These innovations collectively enhance the capability of agents to adapt and perform in complex multi-agent environments, establishing RP as a robust solution to zero-shot coordination challenges in MARL. To gain a deeper understanding of our framework and explore additional visualizations, we invite readers to visit our project website, where more detailed results are provided111https://weifan408.github.io/role_play_web/."
https://arxiv.org/html/2411.02230v1,Energy-Aware Coverage Planning for Heterogeneous Multi-Robot System,"We propose a distributed control law for a heterogeneous multi-robot coverage problem, where the robots could have different energy characteristics, such as capacity and depletion rates, due to their varying sizes, speeds, capabilities, and payloads. Existing energy-aware coverage control laws consider capacity differences but assume the battery depletion rate to be the same for all robots. In realistic scenarios, however, some robots can consume energy much faster than other robots; for instance, UAVs hover at different altitudes, and these changes could be dynamically updated based on their assigned tasks. Robots’ energy capacities and depletion rates need to be considered to maximize the performance of a multi-robot system. To this end, we propose a new energy-aware controller based on Lloyd’s algorithm to adapt the weights of the robots based on their energy dynamics and divide the area of interest among the robots accordingly. The controller is theoretically analyzed and extensively evaluated through simulations and real-world demonstrations in multiple realistic scenarios and compared with three baseline control laws to validate its performance and efficacy.","There has been an increase in the use of autonomous robots in recent years, particularly for the purpose of surveillance and monitoring environments. The collected data from the robots can be used to make further decisions. For example, in a precision agriculture application, robot-collected hyper-spectral images might be used for weed localization and treating the affected areas with herbicides [32]. For such applications, sensor (or spatial) coverage is an important computational problem to consider. In a multi-robot sensor coverage problem, the objective is to distribute the robots (sensors) in a manner that optimally monitors the workspace covering the spatial region (environment) with at least one robot’s sensor footprint. This objective is different from and is not to be confused with the area coverage problem, where the objective is that the robot(s) should visit each and every part of the environment. A promising technique for achieving the sensor coverage goal is to divide the environment into regions using Lloyd’s algorithm, also known as Voronoi partitions, which can be optimized based on constraints such as uncertainties in sensing and sensor health, among others [28, 31, 24, 1]. A heterogeneous group of robots (e.g., a group of UGVs and UAVs) allows the system to develop proficiency in different areas of the task despite their inherent limitations on a specific capability, such as sensing or mobility [27, 2]. UAVs, for instance, can cover more terrain and are less susceptible to obstacles than UGVs, which, on the other hand, generally have higher battery capacities and deplete energy at a slower rate than UAVs [38]. We posit that the robot’s energy depletion rate is an important factor that needs to be incorporated into their controllers to increase the overall lifetime of the multi-robot system and optimize the mission objective. Figure 1: Regions assigned by standard Voronoi partitioning (left) and the proposed energy-aware controller (EAC) (right). The robots have the same initial battery level. However, robot 3’s depletes its energy three times faster than the other robots. Therefore, robot 3’s EAC-assigned region area (area in m2superscript𝑚2m^{2}italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT in parentheses) is less than other robots by adapting the weights (wisubscript𝑤𝑖w_{i}italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) based on the ratio of energy depletion rate between the robots. The rate of energy depletion could vary significantly between robots due to their sensor payloads (e.g., some robots may be equipped with high-power thermal cameras or power drill tools in search and rescue applications), as well as their velocities (e.g., UAVs operating at higher altitude will deplete energy faster than the ones at lower altitudes) [23, 19, 36]. Moreover, the energy depletion rate of a robot can change dynamically over time (e.g., activating or deactivating a payload, increasing the velocity, etc.), and the robots need to adjust their coverage loads accordingly. Fig. 1 illustrates such a scenario. Here, the energy-aware controller optimally assigns a lower area to robot 3, which had almost four times higher energy depletion rate compared to the other robots, which will limit its ability to cover more area. Assigning a larger area to a robot with more energy or a lower depletion rate and smaller areas to other robots will distribute the workload among robots and ensure effective coverage and timely execution of tasks. Energy availability has significant implications for coverage planning. Several works proposed in the literature [11, 12, 6, 8] focusing on this problem consider limiting the robot velocity and/or partitioning the regions based on the current energy levels. However, this will result in poor coverage quality when robots have different energy depletion rates. Moreover, as discussed earlier, real-world applications might require a heterogeneous group of robots with different and dynamic energy characteristics, where robots consume energy differently.Moreover, restricting the robots’ velocities like performed in [12, 8] could conflict with the low-level, time-limited tasks the robots are assigned to. To this end, we present a novel distributed multi-robot controller that holistically considers the robot’s energy capacity and depletion characteristics of other robots for optimal energy-aware coverage planning. We extensively validate the controller in simulations (scaling up to 100100100100 robots) and with real robots within an in-house swarm robotics testbed (see the video). The main contributions of our paper are as follows. • To the best of our knowledge, this is the first study that considers differences in the energy capability of the robots with heterogeneous energy depletion rates for effective multi-robot coverage planning. • Unlike prior studies, we consider that the robots might have time-varying energy depletion rates during a mission. Compared to the state-of-the-art relevant algorithms, our proposed Energy-Aware Controller (EAC) significantly reduces coverage cost and achieves energy-balancing coverage objectives."
https://arxiv.org/html/2411.01794v2,Revisiting Game-Theoretic Control in Socio-Technical Networks: Emerging Design Frameworks and Contemporary Applications,"Socio-technical networks represent emerging cyber-physical infrastructures that are tightly interwoven with human networks. The coupling between human and technical networks presents significant challenges in managing, controlling, and securing these complex, interdependent systems. This paper investigates game-theoretic frameworks for the design and control of socio-technical networks, with a focus on critical applications such as misinformation management, infrastructure optimization, and resilience in socio-cyber-physical systems (SCPS). Core methodologies, including Stackelberg games, mechanism design, and dynamic game theory, are examined as powerful tools for modeling interactions in hierarchical, multi-agent environments. Key challenges addressed include mitigating human-driven vulnerabilities, managing large-scale system dynamics, and countering adversarial threats. By bridging individual agent behaviors with overarching system goals, this work illustrates how the integration of game theory and control theory can lead to robust, resilient, and adaptive socio-technical networks. This paper highlights the potential of these frameworks to dynamically align decentralized agent actions with system-wide objectives of stability, security, and efficiency.","Game theory addresses strategic interactions among decision-makers, often referred to as players or agents [1]. Each player has a distinct objective function—either a utility to maximize or a cost to minimize—which encapsulates their preferences across available alternatives. However, these preferences are interdependent, shaped by the choices made by other players, creating a need for game theory as a framework to model these strategic dynamics [2]. In non-cooperative games, where players act independently, achieving an equilibrium is a central focus. The Nash equilibrium, a key concept developed by John Nash, represents a stable state where no player can unilaterally adjust their strategy for a better outcome [3]. This equilibrium concept enables analysts to identify stable states in competitive environments. When hierarchical decision-making exists, other solution concepts like the Stackelberg equilibrium are more suitable, especially in scenarios where leaders act first and followers respond. This framework is particularly applicable in control scenarios where independent agents must coordinate within shared constraints [4]. The formalization of game theory is largely attributed to John von Neumann and Oskar Morgenstern’s Theory of Games and Economic Behavior [1], a pioneering text that established the field’s foundations and generated broad, interdisciplinary interest. Nash’s work on equilibrium expanded the field significantly, enabling stable solutions for non-cooperative games [3]. Additional advancements by Richard Bellman, who introduced dynamic programming [5], and Rufus Isaacs, known for differential games [6], extended game theory to dynamic and multi-stage decision-making, integrating it more deeply into control and optimization contexts. A defining moment for game theory was its integration into optimal control and decision processes, especially through the efforts of researchers at the RAND Corporation, including Bellman, Nash, and Isaacs. At RAND, these theorists developed models intersecting military strategy, economics, and control systems, catalyzing breakthroughs in multi-agent decision-making processes. Isaacs’ differential games, for instance, established a framework for continuous-time strategic interactions’ foundational for adversarial scenarios like pursuit-evasion games [6]. This interdisciplinary evolution of game theory alongside advances in optimal control provided the groundwork for its application in modern networked systems, where it remains central to understanding strategic interdependencies and designing resilient control mechanisms. I-A Game Theory and Socio-Technical Systems Today’s interconnected systems—ranging from telecommunications and social networks to critical infrastructure—face unique challenges as they are populated by autonomous agents, each pursuing individual objectives, but interconnected through their actions and information exchanges. Game theory is a critical tool in such environments, especially where decentralized decision-making is needed. In multi-agent systems, every agent’s actions affect the network’s overall state, often creating complex dynamics that are difficult to predict and control. Game-theoretic analysis equips researchers and designers with the methods to anticipate these interactions, predict system behavior, and develop strategies that enhance stability, efficiency, and resilience across the network [7]. Moreover, these networks are often socio-technical systems, where human behavior directly influences their performance, efficiency, and resilience. Human decisions and interactions shape the functioning of many networked systems, such as transportation, energy grids, and public health infrastructure [8, 9, 10]. For example, transportation networks must account for the flexible and sometimes unpredictable nature of human routing decisions. In such settings, infrastructure planning cannot be isolated from human behavior. The well-known Braess paradox illustrates that adding roads to a network may lead to increased congestion, as drivers individually optimize their routes, often at the expense of overall efficiency [11]. Game-theoretic models help planners anticipate these outcomes, enabling the design of transportation networks that mitigate unintended consequences and improve flow [12]. In smart energy systems, where energy prosumers (both consumers and producers) decide when to buy or sell energy, individual behaviors impact the grid’s supply-demand balance. To maintain grid stability, game-theoretic mechanisms can incentivize prosumers to make decisions that align personal economic interests with the system’s operational needs [13]. These control mechanisms foster a resilient and efficient energy network by balancing incentives for prosumers in ways that enhance resource allocation and minimize the risk of outages [14]. Figure 1 illustrates the nature of the control of socio-technical systems. The technical system is coupled with the human networks, and the designer can influence the coupled system through different control paradigms, including information, incentives, and network structures. Public health further exemplifies the socio-technical nature of modern systems, as seen during the COVID-19 pandemic. Individual choices, such as decisions about vaccination, mask-wearing, and social interactions, had substantial effects on the spread of the virus [15, 16, 17, 18]. In such interconnected populations, the community’s health state depends on the aggregation of personal decisions. Game-theoretic design principles offer powerful tools for crafting behavioral incentives and nudges that guide individuals toward compliance with public health measures [19]. By structuring these incentives effectively, game theory helps manage collective health outcomes, particularly during health crises, and underscores the importance of considering socio-technical dynamics in system design. By integrating game-theoretic frameworks into socio-technical systems, designers gain the ability to understand and anticipate human-driven impacts on system dynamics. Game theory provides structured approaches for designing incentives, controlling information flows, and implementing adaptive mechanisms that foster desirable behaviors. These strategies are essential for ensuring that the interactions of autonomous agents—whether they are people, machines, or a mix of both—contribute positively to network performance, resilience, and societal benefit [20]. Fig. 1: A Game-Theoretic Control Paradigm for Socio-Technical Systems: Socio-technical networks are composed of interconnected human and technical networks. Human agents interact both with one another and with technical infrastructures, including power grids, transportation systems, and cyber networks. The control of these networks can be achieved through strategic designs in information flow, network structure, and incentive mechanisms. Information design guides how agents access and process data, while network design shapes the connectivity and interaction pathways within the system. Incentive design, on the other hand, motivates desired behaviors by aligning agent actions with system-wide objectives, ensuring that human and technical interactions are coordinated to achieve resilience, efficiency, and security across the socio-technical network. I-B Game-Theoretic Control Design Game theory offers not only a framework for modeling, performance evaluation, and risk assessment but also a robust design methodology for creating decentralized agents. A key strength of game-theoretic design lies in its decentralized approach, which provides a foundational structure for building and managing complex, large-scale networks [21, 22]. In these decentralized networks, individual agents act based on personal incentives, often with limited or no knowledge of the network’s overall state. This bottom-up approach mirrors real-world systems, where centralized control may be impractical or ineffective. For human agents, game-theoretic design allows for behavior modification to align with system goals. For machine agents, it enables programming diverse agents to follow a coordinated protocol. These agents, whether human or machine, can work collectively to achieve desired outcomes associated with metrics such as efficiency, robustness, resilience, and security. By embedding game-theoretic strategies, designers can anticipate and guide agent interactions, facilitating cooperative behavior even in environments with limited information sharing or direct coordination. Figure 2 presents an agent-based perspective on socio-technical systems illustrated in Figure 1. Within this framework, human agents within human networks engage with machine agents in technical networks, while also interacting with other agents in their respective networks. Each human agent operates as a coupled system, integrating individual belief processes with action processes. Similarly, each machine agent functions as a coupled system, linking control processes with physical processes. Designers can influence various agents through targeted levers across different system components, aiming to optimize system-level performance. The design of agents is closely linked to control theory, specifically the design of controllers that manage dynamical systems to achieve desirable properties like stability and optimality. While control theory traditionally focuses on governing centralized control systems, game-theoretic agent design introduces a complementary approach that is particularly suited to large-scale socio-technical networks. In these networks, the goal is often to achieve outcomes such as optimal social welfare or collective efficiency, which align with the objectives of control theory. Game-theoretic design operates from the bottom up, creating decentralized agents that make decisions based on local information and personal incentives. This bottom-up approach enables scalability, making it ideal for vast, complex networks typical of socio-technical systems, where centralized control may be impractical. By designing agents to act independently yet cohesively, game-theoretic design facilitates adaptable, resilient, and efficient network behaviors, even in highly dynamic and large-scale environments. Modeling of the Agents For the socio-technical system illustrated in Figure 1, game theory can model diverse interactions within socio-technical networks from the ground up. These interactions can be categorized into several key types. First, interactions occur between agents within the same network, such as those between human agents in human networks or machine agents within technical networks. These intra-network interactions capture the dynamics among similar types of agents and can reveal emergent patterns within isolated sub-systems. Second, interactions take place between agents across different networks. For instance, human agents in the socio-network interact with machine agents in the technical network, bridging the socio-technical divide. These cross-network interactions are crucial for understanding how human and machine agents jointly influence system outcomes. A third category involves interactions with adversarial agents. Adversarial agents are specifically introduced to evaluate the security, robustness, and resilience of the network. These adversarial entities may be real participants within the network or artificial agents created to assess risk. By engaging human or technical agents with adversaries designed with specific intentions and capabilities, we can measure local security and resilience properties more accurately. Finally, interactions occur between agents and a designer. Here, a designer exerts influence over agents in a controlled way to guide their behavior toward achieving network-wide objectives. This interaction serves as a means of designing and controlling agent actions within the network to align with broader system goals. Each of these interactions takes on distinct forms, and the various games representing them are ultimately composed into a larger framework, referred to as a “meta-game.” This meta-game governs the design and control of the entire socio-technical network, enabling a holistic approach to understanding and managing complex interactions within the system. Control of the Agents Agents can be controlled in various ways, depending on their nature and function, and these controls can be categorized into three primary paradigms. The first is physical control, which involves managing physical attributes like speed, direction, and other measurable quantities, as seen in robotic agents [23]. The second is cyber control, where the focus is on controlling the information received by agents, such as news broadcasts for human agents or sensor data for autonomous vehicles. The third paradigm is human control, where the objective is to influence perceptions and incentives to guide human behavior in desired directions. Across these paradigms, network structure and information design are fundamental. How agents communicate, physically interact, and gather information from observations and perceptions are critical components of effective system design [24]. A key connection between control theory and game-theoretic design emerges through the use of dynamic game frameworks to model and guide agent behavior in evolving environments [25, 26]. In dynamic games [27], agents interact over time within changing environments and face uncertainties. Agent behaviors are characterized by adaptive feedback loops, where decisions continuously adjust based on environmental conditions. Information flow becomes particularly crucial in these scenarios, as agents make real-time decisions with limited or noisy information about others’ actions. The flow and structure of information directly shape agents’ strategic choices, influencing the overall system’s resilience and robustness. Bridging control and game-theoretic design achieves a unified approach to achieving individual dynamic agents and ensuring the stability and efficiency of the entire system. On the individual level, agents must operate effectively within their local environments, maintaining stability in response to changing conditions and achieving their own performance goals. At the system level, however, the design must prioritize overall stability, resilience, and system-wide metrics [28, 29]. Fig. 2: Illustration of Interaction Between a Social Agent in the Human Network and a Machine Agent in the Technical Network: A social agent interacts with the human network and a machine agent within the technical network. Each agent is also connected to other agents within its own network. The machine agent provides specific services to the social agent, while the social agent impacts the machine agent and its network through behaviors such as consumption, usage, or demand patterns. The designer can strategically influence both networks using tools like information design and incentive structures. Information design shapes the structure of information between agents, while incentive design aligns agent actions with broader system goals, creating a coordinated and adaptive socio-technical system. I-C The Underlying Philosophy of Agent-Based Game-Theoretic Design in Socio-Technical Networks The agent-based game-theoretic design of socio-technical networks embodies a dual philosophy [30, 31]: reductionistic design and holistic control. On one side, game-theoretic design takes a reductionist approach, where the whole system is decomposed into modular components or agents. By breaking down complex, large-scale networks into manageable agents, this approach allows designers to handle intricate interdependencies and diverse functions within the system. On the other side, the design aims to achieve high-level system objectives—such as efficiency, security, and resilience—which are often prescribed at a system-wide level. The reductionist design of individual agents must, therefore, be aligned with these holistic goals, ensuring coherence between component-level actions and overall system performance. Establishing such coherence is fundamental to the principles guiding game-theoretic design in socio-technical networks. Achieving coherence between agent-level design and system-level objectives requires a framework to bridge them. Designers need to assess how individual agent behaviors impact system-wide metrics, making it essential to monitor the alignment of component actions with system goals. Game-theoretic analysis provides this bridge by offering a structured framework to predict system-level behaviors through equilibrium concepts. The equilibrium, depending on the application and structure of the network, enables designers to forecast the outcomes of individual actions within the larger system. Various solution concepts within game theory offer tools to assess and develop performance metrics. For instance, in a security context, equilibrium analysis between a defender agent and an attacker can yield risk metrics, while in robustness analysis, saddle-point equilibria between the system and external disturbances inform robustness metrics. Game theory serves as a bridge that enables reductionist designers to account for the holistic impact of individual agent designs on the system’s objectives. Meanwhile, holistic system designers must shape the architecture, including hierarchies, network structures, and resource allocations, to ensure that agent-level designs contribute to the system’s high-level goals. Frameworks such as Stackelberg games, equilibrium-constrained optimization, and mechanism design theory play a central role in achieving this alignment. Holistic designers must understand how agents respond to these structures at equilibrium and ensure that top-down control strategies foster the intended system-wide behavior. Ensuring coherence becomes more challenging under conditions of uncertainty, adaptive requirements, and emergent properties such as resilience and security. These complex requirements demand clear, quantifiable metrics to guide system and agent design. Despite these challenges, game theory—with its rich array of tools and methodologies—provides a means to develop advanced techniques that foster coherence in dynamic, complex systems. For instance, game-theoretic tools can incorporate learning and adaptation, enabling agent designs to evolve in response to an uncertain environment while staying aligned with system goals. This coherence between reductionism and holistic control, illustrated in Figure 3, is where game theory and control theory intersect, together forming the foundation for a new system design paradigm. Designing socio-technical networks requires this paradigm shift and the convergence between control and game theory to address the unique demands of these complex, interconnected systems. For example, in a smart grid, it is insufficient to simply control each subsystem, such as energy generation or distribution, in isolation. The system must account for the interplay between independent agents (e.g., consumers, generators, and grid operators) who each respond to incentives, environmental conditions, and their own objectives. By integrating game-theoretic strategies, designers can predict how these agents will behave collectively, while control theory enables the coordination of these actions to maintain grid stability, efficiency, and resilience. Fig. 3: The holistic control design must align consistently with the reductionist behaviors of individual agents. Game theory, inherently a reductionist approach, focuses on designing and analyzing individual agent behaviors, while control theory provides a holistic framework to achieve overarching system goals. Game-theoretic control offers a cohesive approach that bridges these two perspectives, integrating the detailed evaluation and synthesis tools of reductionist models with the coordination and control mechanisms of holistic design. This combined framework ensures that individual agent actions are aligned with the broader system objectives, creating a unified and adaptive socio-technical system. I-D Organization of the Letter This letter provides an overview of game-theoretic design approaches. In Section II, we explore foundational frameworks used in agent design, focusing on Stackelberg-type game frameworks and mechanism design theory, which has been widely applied to settings like auctions and market structures. Section III examines the challenges of designing socio-technical systems, addressing issues such as human behavioral dynamics, uncertainty quantification, and scalability. In Section IV, we present emerging paradigms in game-theoretic design, including mean-field design, learning-based design, population-based design, and adversarial design. These approaches are applied to critical areas such as misinformation management in social networks, resilience in industrial control systems, and congestion control in infrastructure networks. We close the letter with the concluding remarks of Section V."
https://arxiv.org/html/2411.01608v1,GITSR: Graph Interaction Transformer-based Scene Representation for Multi Vehicle Collaborative Decision-making,"In this study, we propose GITSR, an effective framework for Graph Interaction Transformer-based Scene Representation for multi-vehicle collaborative decision-making in intelligent transportation system. In the context of mixed traffic where Connected Automated Vehicles (CAVs) and Human Driving Vehicles (HDVs) coexist, in order to enhance the understanding of the environment by CAVs to improve decision-making capabilities, this framework focuses on efficient scene representation and the modeling of spatial interaction behaviors of traffic states. We first extract features of the driving environment based on the background of intelligent networking. Subsequently, the local scene representation, which is based on the agent-centric and dynamic occupation grid, is calculated by the Transformer module. Besides, feasible region of the map is captured through the multi-head attention mechanism to reduce the collision of vehicles. Notably, spatial interaction behaviors, based on motion information, are modeled as graph structures and extracted via Graph Neural Network (GNN). Ultimately, the collaborative decision-making among multiple vehicles is formulated as a Markov Decision Process (MDP), with driving actions output by Reinforcement Learning (RL) algorithms. Our algorithmic validation is executed within the extremely challenging scenario of highway off-ramp task, thereby substantiating the superiority of agent-centric approach to scene representation. Simulation results demonstrate that the GITSR method can not only effectively capture scene representation but also extract spatial interaction data, outperforming the baseline method across various comparative metrics.","Autonomous vehicles have garnered significant research attention over the past two decades, driven by their substantial potential for societal and economical advancement. The efficient coordination of driving decisions among CAVs promises not only to enhance safety and operational efficiency but also to reduce energy consumption [1]. However, in dynamic traffic scenarios, the intricate interplay between scenarios and traffic participants presents formidable challenges for CAVs in making decisions that are safe, efficient, and comfortable [2]. The Internet of Vehicles (IoV) technology integrates Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) communications with artificial intelligence (AI) to offer innovative solutions for CAVs to process dynamic traffic scene information and perform collaborative driving decisions [3]. In this context, methods based on deep reinforcement learning (DRL) are becoming more and more popular because the intelligent agent can continuously learn through interaction with the driving environment, extract environmental information through deep learning, and make decisions through reinforcement learning algorithms [4]. However, modeling and representing scene information effectively, processing and calculating it to adapt to various complex traffic environments, while achieving high-quality collaborative decision-making in real-time dynamic settings, has emerged as a formidable research challenge. Concurrently, the research on autonomous vehicle decision-making is increasingly focusing on more complex scenarios. The crux of the challenge lies in the representation of the state, which must encompass the elements, characteristics, and interactions in the dynamic scene. Addressing this will become one of the key issues of the DRL methods [5]. To this end, we introduce GITSR, a novel graph interaction Transformer-based scene representation framework for multi-vehicle collaborative decision-making. This framework leverages the Transformer architecture to capture scene information and employs a graph structure to model spatial interaction, thereby enhancing the multi-vehicle collaborative decision-making ability of reinforcement learning. Firstly, we extract features from the dynamic driving environment within the context of intelligent networking, meticulously considering both the local interaction and global communication attributes of CAVs. We perform local reconstruction reasoning on scene input information, introduce the Transformer module to process information and enhance understanding of surrounding traffic scene for CAVs. We conduct local reconstruction reasoning on the input scene information and introduce the Transformer module to process this data, thereby enhancing the CAVs’ comprehension of the surrounding traffic environment. Then, we represent the dynamic traffic scene as a graph, based on global communication attributes, and introduce GNN to extract spatial interaction features. This approach is advantageous as it optimally utilizes the information from all CAVs within dynamic traffic scenarios. It aids CAVs in scene comprehension and the transmission of upstream and downstream information. Moreover, it establishes the spatial interaction dynamics of the traffic environment, optimizing the collaborative driving decision-making capabilities. The main contributions of this article can be summarized as follows: 1) A collaborative decision-making framework for intelligent connected vehicles that integrates Transformer and GNN is designed, which is tailored for scene extraction and interaction modeling from the perspective of state representation, thus significantly enhancing the state representation to improve the reinforcement learning effect. 2) A local representation method based on Transformer to reconstruct reasoning from scene features is proposed. This method reconstructs the scene representation with a focus on all CAVs and employs GNN to extract spatial interaction behaviors between the motion information of traffic participants. The GITSR framework can make full use of the information extracted from features to assist all CAVs in comprehending both local scene details and global interaction dynamics. 3) The framework is verified in a challenging interactive collaborative driving environment. The results show that GITSR has advantages over advanced algorithms in terms of safety, efficiency, and task success rate. At the same time, we have conducted an assessment of the influence of various components within the GITSR framework on its overall performance."
https://arxiv.org/html/2411.01455v1,HiMemFormer: Hierarchical Memory-Aware Transformer for Multi-Agent Action Anticipation,"Understanding and predicting human actions has been a long-standing challenge and is a crucial measure of perception in robotics AI. While significant progress has been made in anticipating the future actions of individual agents, prior work has largely overlooked a key aspect of real-world human activity – interactions. To address this gap in human-like forecasting within multi-agent environments, we present the Hierarchical Memory-Aware Transformer (HiMemFormer), a transformer-based model for online multi-agent action anticipation. HiMemFormer integrates and distributes global memory that captures joint historical information across all agents through a transformer framework, with a hierarchical local memory decoder that interprets agent-specific features based on these global representations using a coarse-to-fine strategy. In contrast to previous approaches, HiMemFormer uniquely hierarchically applies the global context with agent-specific preferences to avoid noisy or redundant information in multi-agent action anticipation. Extensive experiments on various multi-agent scenarios demonstrate the significant performance of HiMemFormer, compared with other state-of-the-art methods.","Action detection [8] or anticipation [21] systems aim at forecasting future states of single or multiple agents from history. The recent advances in these areas facilitate embodied or virtual AI systems with the ability to perceive and interact with other agents and complex environments [33, 37, 44]. Such ability plays a pivotal role in numerous applications, such as autonomous driving [41], collaborative robotics [30], and home automation [32], where understanding and predicting the actions of various entities in a shared environment can significantly enhance safety, efficiency, and coordination. Agent memory plays an important role in conducting action anticipation due to the innate dependencies among actions [39, 36, 14, 37, 44]. LSTR [39] proposes to capture both long-term and short-term memory, while MAT [36] additionally incorporates future content in seen scenarios. In the multi-agent scenarios [33], each agent can be arbitrary or affected by the environment, which suggests one key to the success of a multi-agent system: how to effectively capture agent behavior at various time and social scales. A prominent line of research exploits the ways to obtain a unified single global feature representing time, e.g., [23, 4], and social relations, e.g., [17, 29]. AgentFormer [42] and HiVT [45] further explore combining time and social features with a overall global representation. Despite the significance, these state-of-the-art systems overlook the individual perspective of the problem: different agents may need time and social features at different scales. From the time perspective, some agent actions heavily rely on long-term memory, e.g., if they belong to a complex multi-step action sequence, while some actions are only relevant to short memory, e.g., an instant response to a rapid environment change. From the social perspective, similarly, the actions of some agents are much correlated with others during collaboration, while some of mostly stand-alone. To capture these agent-specific preferences in feature utilization, we propose to hierarchically capture the time and social features for each agent-specific decoder to include these global or contextual features with the desired granularity and discard unnecessary information that may introduce noise or latency to each specific agent. To achieve customized and flexible global feature utilization automatically, we propose the Hierarchical Memory-Aware Transformer (HiMemFormer), a novel approach that simultaneously learns feature representations from both contextual and agent-specified dimensions through a dual-hierarchical framework. Specifically, its Agent-to-Context Encoder augments the agents’ long-term history through cross-attention with global long-term memory. Then, the encoded long-term memory is further processed through a hierarchical Agent-to-Context Decoder that offers a coarse prediction given augmented long-term memory and contextual short-term memories. Finally, the coarse prediction is gradually refined by each agent-specific network augmented with individual short-term memory to get the anticipated actions. Through the dual-hierarchical network, HiMemFormer manages to model agent’s unique short-term memory while learning useful correlations from the contextual memories. This allows us to effectively compress the long-range contextual information without losing important lower level feature information. In summary, our contributions are three-fold: • We propose a transformer-based method to capture and utilize the global features in multi-agent scenarios in a flexible way responding to each agent’s preference. • We design a hierarchical memory encoder that follows a specific-to-general paradigm to learn long-term joint-memory features and a hierarchical memory decoder that learns an agent’s future action by a coarse-to-fine strategy. • We carry out exhaustive experiments on various multi-agent action anticipation scenarios and outperform existing baseline models."
https://arxiv.org/html/2411.01442v2,Online Relational Inference for Evolving Multi-agent Interacting Systems,"We introduce a novel framework, Online Relational Inference (ORI), designed to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems using streaming data. Unlike traditional offline methods that rely on a fixed training set, ORI employs online backpropagation, updating the model with each new data point, thereby allowing it to adapt to changing environments in real-time. A key innovation is the use of an adjacency matrix as a trainable parameter, optimized through a new adaptive learning rate technique called AdaRelation, which adjusts based on the historical sensitivity of the decoder to changes in the interaction graph. Additionally, a data augmentation method named Trajectory Mirror (TM) is introduced to improve generalization by exposing the model to varied trajectory patterns. Experimental results on both synthetic datasets and real-world data (CMU MoCap for human motion) demonstrate that ORI significantly improves the accuracy and adaptability of relational inference in dynamic settings compared to existing methods. This approach is model-agnostic, enabling seamless integration with various neural relational inference (NRI) architectures, and offers a robust solution for real-time applications in complex, evolving systems. Code is available at https://github.com/beomseokg/ORI.","Multi-agent interacting systems have been studied in various fields, including particle-based physical simulations [1, 2, 3], traffic systems [4, 5], and social networks [6, 7, 8]. Interaction among agents are crucial information to accurately model such systems and providing the interpretability in agent behaviors as well [9]. However, external observers can only access the trajectory of agents without knowing interaction graphs. Accordingly, identifying unknown interaction graphs from observable trajectories of agents has been emerged as a specific problem referred to as relational inference [10]. In recent years, neural relational inference (NRI) and its variants have shown promising performance in synthetic and real-world environments [11, 12, 13, 14, 15, 16]. Prior studies mostly aimed to present better network for NRI based on variational autoencoder (VAE) built with graph neural networks (GNN) [14, 15, 12, 17]. These methods involve an encoder to infer an interaction graph as an adjacency matrix from observed trajectories and a decoder to predict the future trajectories employing the inferred interaction graph. They generally perform training offline assuming the well-aligned distribution in training and testing data. Unfortunately, such assumption is frequently violated in practice due to the shifts in test condition, including sudden changes in the interaction, evolving system parameters and even dynamics itself. Building a relational inference model generalizable to all the different scenarios is challenging [18, 19, 20]. In this case, online learning is an attractive approach to continuously adapt the model to the newly observed environments [5]. However, online learning for relational inference has been rarely explored. Online backpropagation using gradient descent is a widely used online learning method as it is compatibility to various neural network designs [21]. However, online backpropagation on existing NRI models significantly degrades the accuracy on relational inference since their decoder quickly learns the trajectory prediction even before the encoder generates reasonable interaction graphs. It is important to note that while the models are trained in self-supervised manner (i.e., trajectory prediction), true labels of interaction graphs are never provided to the models, indicating that identifying interaction graphs is essentially unsupervised. That is, optimizing the unsupervised encoder is more challenging than the self-supervised decoder in nature. The key problem is how to match the learning speed between the interaction identification and the trajectory prediction so that both the tasks can be collaboratively optimized. In this paper, we propose a novel framework named Online Relational Inference (ORI) to efficiently identify hidden interaction graphs in evolving multi-agent interacting systems. Our method strategically allocates an adjacency matrix representing the interaction as a trainable parameter in the model and directly optimizes it through online backpropagation on the predicted trajectories. It effectively accelerates the update of the adjacency matrix than the encoder-based approach and ensures the following decoder to be learned with reasonable adjacency matrices from the early stage of training. ORI can seamlessly integrate with prior NRI models, offering architectural flexibility (i.e., model-agnostic). Moreover, we developed an adaptive learning rate technique named AdaRelation particularly designed for relational inference in the evolving systems. It employs the historical adjacency matrix to indirectly estimate the decoder’s sensitivity over the adjacency matrix and determine whether the learning needs to be accelerated. In addition, we introduce a data augmentation technique named Trajectory Mirror (TM) to expose various trajectories by flipping the axis in the systems. We experimentally demonstrate the effectiveness of ORI on various NRI models in both synthetic and the real-world (CMU MoCap for human motion [22]) dataset. Our key contributions are as follows: • To the best of our knowledge, ORI is the first model-agnostic online relational inference framework for evolving multi-agent interacting systems. ORI employs online backpropagation to optimize an adjacency matrix from the trajectory information without any assumptions on the model architecture. • We experimentally demonstrate that ORI identifies unknown interaction graphs in various evolving multi-agent interacting systems, such as sudden changes in interaction (Figure 2), parameters in the dynamics (Figure 3), and even dynamics itself (Figure 3), outperforming existing NRI models (Table 2). • We propose AdaRelation, a novel adaptive learning rate technique particularly designed for relational inference in evolving multi-agent systems. It automatically tunes the learning rate for the adjacency matrix when interaction or/and dynamics in the system suddenly change (Figure 3). • We propose Trajectory Mirror, a data augmentation technique to ensure the reasonable relational inference regardless of the trajectory axis. It significantly improves the convergence speed and overall interaction prediction accuracy in the several evolving scenarios (Supplementary). Table 1: Comparison of key features between prior works and this work. Method Description Model Consider Evolution in Criteria Agnostic Interaction Parameter Dynamics Acc. MSE Prior offline works [11, 7, 12, 23, 16] ⋅⋅\cdot⋅ offline backpropagation ⋅⋅\cdot⋅ novel encoder and decoder - ✓ × × ✓ ✓ Prior online work [5] ⋅⋅\cdot⋅ online convex optimization ⋅⋅\cdot⋅ constant learning rate × ✓ × × × ✓ This work ⋅⋅\cdot⋅ online backpropagation ⋅⋅\cdot⋅ AdaRelation; Traj. Mirror ✓ ✓ ✓ ✓ ✓ ✓ • ✓ indicates the presence of a feature, and × indicates the absence of a feature."
https://arxiv.org/html/2411.01271v1,"Interacting Large Language Model Agents.
Interpretable Models and Social Learning.","This paper develops theory and algorithms for interacting large language model agents (LLMAs) using methods from statistical signal processing and microeconomics. While both fields are mature, their application to decision-making by interacting LLMAs remains unexplored. Motivated by Bayesian sentiment analysis on online platforms, we construct interpretable models and stochastic control algorithms that enable LLMAs to interact and perform Bayesian inference. Because interacting LLMAs learn from both prior decisions and external inputs, they can exhibit bias and herding behavior. Thus, developing interpretable models and stochastic control algorithms is essential to understand and mitigate these behaviors.This paper has three main results. First, we show using Bayesian revealed preferences from microeconomics that an individual LLMA satisfies the necessary and sufficient conditions for rationally inattentive (bounded rationality) Bayesian utility maximization and, given an observation, the LLMA chooses an action that maximizes a regularized utility. Second, we utilize Bayesian social learning to construct interpretable models for LLMAs that interact sequentially with each other and the environment while performing Bayesian inference. Our proposed models capture the herding behavior exhibited by interacting LLMAs. Third, we propose a stochastic control framework to delay herding and improve state estimation accuracy under two settings: (a) centrally controlled LLMAs and (b) autonomous LLMAs with incentives. Throughout the paper, we numerically demonstrate the effectiveness of our methods on real datasets for hate speech classification and product quality assessment, using open-source models like Llama and Mistral and closed-source models like ChatGPT. The main takeaway of this paper, based on substantial empirical analysis and mathematical formalism, is that LLMAs act as rationally bounded Bayesian agents that exhibit social learning when interacting. Traditionally, such models are used in economics to study interacting human decision-makers.","This paper develops theory and algorithms for interacting Large Language Model Agents (LLMAs) by leveraging techniques from signal processing and microeconomics. Specifically, we focus on developing interpretable models and stochastic control algorithms for LLMAs, enabling them to interact sequentially for Bayesian inference. We construct interpretable models of LLMAs at two levels of abstraction, as outlined in Figure 1. First, we model an individual LLMA as a rationally inattentive Bayesian utility maximizer, capturing the agent’s decision-making process under limited attention. Second, we extend this approach to a sequence of LLMAs engaging in Bayesian social learning, where each agent acts as a Bayesian utility maximizer. Our models are inspired by the self-attention mechanism in large language models (LLMs) and observed challenges, such as model collapse, that can arise during LLM training. Rationally Inattentive Bayesian Utility Maximizer LLM AgentLLM Agent nText observation Action (Estimate) ⋯⋯\displaystyle\cdots⋯Abstraction 1Abstraction 2LLM Agent 1Text observation Action (Estimate) Sequence of LLM Agents perform Bayesian social learningLLM Agents are rationally inattentive Bayesian utility maximizers Figure 1: Summary of the proposed contributions: We discuss the different black-box models for LLMA and how LLMAs can be used as a sensing mechanism to perform Bayesian inference. Part 1 models the LLMAs as a rationally inattentive Bayesian utility maximizer and numerically establishes the behavior in applications of product quality identification and hate speech classification. Part 2 discusses how Bayesian social learning in a sequence of LLMAs can be used for sequential state estimation. However, in Part 3, we show that the agents can perform the same incorrect action due to herding. We then discuss a stochastic control approach to delay herding when LLMAs are centrally controlled and when they are autonomous but are incentivized. Furthermore, motivated by the observed bias in the behavior of interacting LLMAs, we demonstrate that a sequence of LLMAs engaging in Bayesian social learning converge to identical estimates, or ”herd”. To address this phenomenon, we propose a stochastic control approach, formulating an optimal stopping problem to balance the trade-off between privacy and herding, to detect the failure state. Our approach is designed for two scenarios: (a) when the LLMAs are centrally controlled, and (b) when they operate autonomously. Our goal is to demonstrate that concepts from controlled sensing and microeconomics, traditionally applied to human decision-making, can be used to both understand and synthesize the behavior of interacting LLMAs [12, 7, 13, 39, 40]. We support our theoretical findings with numerical experiments using advanced LLMs for Bayesian inference on real-world data. This paper is crafted to engage a broad readership, highlighting applications of Bayesian agents in diverse fields, including financial news analysis, e-commerce review evaluation, and online toxicity detection. These examples underscore the flexibility of our methodologies for cross-disciplinary applications. The reproducible code for our experiments is publicly accessible at github.com/aditj/sociallearningllm. I-A Motivation LLM agents (LLMAs) are being rapidly deployed for different applications and to quote Sam Altman, CEO of OpenAI (creators of ChatGPT, a popular LLM which has 200 million weekly active users): “2025 is when (AI) agents will work”. LLMAs use a large language model (LLM) to parse the input and have additional agency to perform tasks. LLMs (such as ChatGPT and Llama) are neural networks with billions of parameters trained on trillions of tokens of textual data to parse long texts for summarizing, compiling key facts, and generating new text. The key technical improvement that leads to the efficient deployment of LLMs is the transformer architecture [75]. The effectiveness of LLMs on textual texts has made their deployment and adoption widespread [54]. Many applications have been proposed in healthcare, online platform moderation, and finance, where these LLMs are used to parse the textual observations and suggest decisions based on their outputs [48]. LLM Bayesian Engine LLMA as a Sensing MechanismLLMARational Inattention Cost Bayesian UtilityLLMALLMALLMALLMA Bayesian Social Learning and Stochastic Control LLMA Engineer Interpretable & Robust Bayesian Inference Text Observation State Estimate Reconstructed Interpretable Model using Bayesian Revealed Preferences Figure 2: Engineering with large language model agents: We propose engineer with LLMs on three different levels: a) First, we propose constructing an LLMA with an LLM attached to the Bayesian engine. The LLM acts as a sensor for the text input and outputs interpretable low-dimensional outputs, which are used by the Bayesian engine to produce a state estimate. b) We formulate the necessary and sufficient conditions for a LLMAs to be a rationally inattentive Bayesian utility maximizer (RIBUM). We also present algorithms to reconstruct feasible utilities and rational inattention costs if the LLMA is indeed a RIBUM, attributing the LLMA with an interpretable microeconomic model. c) We show how a sequence of LLMAs can efficiently perform sequential Bayesian social learning by controlling their outputs to delay herding optimally. Our Bayesian social learning models can be extended to study Bayesian social learning in a network of LLMAs. In many tasks, the outputs of the LLMs are often part of a more extensive pipeline; for example, the output of the LLMs, either in a specified format or as embeddings, is frequently used as inputs to other Bayesian entities, including classifiers [55]. The Bayesian framework also becomes essential in applications where the LLMs have to output decisions and need to provide confidence in the decision output. Thus, it is of interest to study a single Bayesian agent that uses the LLM to parse text observations, update its Bayesian belief, and take action. This paper studies such entities and refers to them as Large Language Models Agents (LLMAs). Constructing interpretable models for LLMAs is crucial to understanding and controlling their interaction. I-A1 Interacting large language model agents It is predicted that by 2025, 90% of web content will be generated by large language models (LLMs) [1]. In recent practical implementations, individual LLMs are part of a bigger system, referred to as LLMAs, and interact with the content generated by other LLMAs and the external environment [86]. Furthermore, recent research has shown how generative models are trained on the data generated by other generative models can collapse [67]. Therefore naturally, LLMAs interact with each other either implicitly or explicitly. Hence, controlling the dynamics of interacting LLMAs is essential to improve the accuracy and trustworthiness of decisions by LLMAs. To the best of our knowledge, only a few recent works systematically study the behaviors of LLMAs using tools from microeconomics and signal processing [30]. This study aims to bridge this gap by systematically reviewing LLMAs and the different mathematical frameworks by studying Bayesian social learning in a sequence of LLMAs to achieve Bayesian inference. I-A2 Interpretable Engineering of LLMAs Many different third-party services have already started providing various kinds of LLMAs as a service, including Agentforce by Salesforce and IBM AI agents [66]. The underlying intelligence engine of these third-party agents is an LLM or a vision language model (VLM). The LLMAs are used in personal applications for coding, shopping, and scraping data and in enterprise applications for getting insights on user activity and automating industrial workflows. Therefore, it becomes imperative to study interpretable models for these agents since many of the proposed applications these agents involve sensitive information (like personal records, financial information, bio-medical data, and personal preferences). By interpretable, we refer to models that facilitate a transparent understanding of complex models through clear and explainable representations of their decision-making processes. The workflows of the AI agents also include making decisions, and the interpretability and reliability of these agents become vital for them to be trustworthy. Therefore, mathematical models are needed to aid in engineering and deploying LLMAs. To this end, we propose a LLMA composed of an LLM and a Bayesian engine, which by construction is interpretable. Further, we use Bayesian revealed preferences 111The framework of Bayesian revealed preferences is also referred to as inverse optimization or inverse reinforcement learning. to reconstruct a Bayesian utility function for both our constructed LLMA and for off-the-shelf LLMAs. I-A3 Bayesian Inference from Multi-Modal Data Stream In various applications, like online e-commerce platforms, video streaming platforms, and social networks, there is a rich stream of multimodal data available using text, images, and videos. Different inference tasks involve fusing information from various data streams to get actionable insights. With the recent progress in deep learning, many of the traditional signal processing methods are being replaced with contemporary methods that use LLMs and VLMs. However, just using static models is not sufficient to model the dynamics of real-life settings, e.g. on online platforms, and underlying dynamics are better modeled in a Bayesian framework. Therefore, motivated by practical applications, we propose the construction of LLMAs which can perform Bayesian inference sequentially on a data stream. This complements continual learning, which deals with continually learning new tasks without forgetting what was learned previously [78]. I-B Main Results Sec. 3: LLM Agent as a SensorSec. 4: LLM Agent as a Rationally Inattentive Bayesian Utility Maximizer (RIBUM)Part 1Indiviual LLM Agent Part 2Network of LLM Agents Sec. 5: Sequential Bayesian Social LearningSec. 6: Word-of-MouthBayesian Social LearningSec. 6: AsynchronousBayesian Social LearningPart 3Stochastic Control for delaying Herding in LLM Agents Sec. 7: Optimal Stopping for Centrally Controlled LLM AgentsSec. 8: Optimal Stopping for Incentivized Autonomous LLMAsSec. 9: Stochastic Approximation for Optimal Policy Sec. 10: Numerical Results on Bayesian inference Product Quality and Hate Speech Peddler Identification Figure 3: Organization of the paper: The paper is divided into three parts. Part 1 deals with interpretable models for an individual LLM agent. Part 2 extends the models to a social learning setting where LLM agents interact with each other to perform Bayesian inference. Part 3 proposes stochastic control methods to delay herding in a sequence of LLM agents. This paper builds on tools from Bayesian revealed preferences from microeconomics (inverse reinforcement learning), sequential Bayesian estimation (from signal processing), and structured stochastic control (from control theory) to construct interpretable models and synthesize interaction of LLMAs. The impact of our results on more efficient, systematic, and interpretable engineering of LLMAs is summarized in Figure 2. The main contributions of this paper are: 1. We propose constructing a LLMA as a composition of a large language model (LLM) sensor, which acts a low-dimensional map from the text space and a Bayesian engine, which uses the measurement from the LLM to update the posterior and act optimally. We show how this model is useful for interpretable Bayesian inference with applications in sequential data on online platforms. 2. To obtain an interpretable utility function for a LLMA, we provide necessary and sufficient conditions in Theorem 1 for a LLMA to be a rationally inattentive Bayesian utility maximizer (RIBUM). For a LLMA who is a RIBUM, we propose Algorithm 2 and Algorithm 3 to reconstruct the max-margin and sparsest utility estimate, respectively. Our methods are applicable both our LLMA and off-the-shelf LLMAs. 3. We study Bayesian social learning in a LLMAs, sequentially estimating a state given text observations and in Theorem 2 show that such a sequence of LLMAs form an information cascade and herd in their actions. We show that this is true for both when no private observations are shared and when a finite number of private observations are shared. Further, we provide a detailed analysis of the effect of the quality of results from LLM of the LLMA and the number of private observations. 4. To delay herding in a sequence of LLMAs, we formulate an optimal stopping problem for two regimes: a) when the LLMAs are centrally controlled by an entity b) when the LLMAs are autonomous but are incentivizes by an entity. We show in Theorem 3 and Theorem 4 that under certain assumptions on the observation matrix and cost functions, the optimal policy for the partially observed Markov decision process of both the optimal stopping problems has a threshold structure. We then propose a stochastic approximation algorithm in Algorithm 7, which exploits the structural results to estimate the optimal policy parameters. The algorithm does not need access to the system parameters, is computationally efficient, and can track changes in the system. 5. We finally present several numerical experiments to demonstrate the efficacy of our proposed methods. We show how our constructed LLMA can be used for interpretable Bayesian inference for analyzing financial data. We show how the Bayesian revealed preferences framework can estimate the utility of an off-the-shelf LLM when used for hate-speech detection. Finally we show numerical studies on two examples of sequential Bayesian inference: hate speech peddler identification and product quality analysis, to demonstrate herding of LLMAs, and applicability of our structural results. To summarize, this paper attempts to answer the following questions with respect to interacting LLM Agents, 1. How can LLMAs be constructed so that they can be used for sequential Bayesian inference such that the observation and outputs are interpretable? 2. What is a principled approach to analyze whether a LLMA is a Bayesian utility maximizer and also reconstructs its utility function given only black-box access? 3. How does one systematically study Bayesian social learning in multiple interacting LLMAs to explain observed behaviours such as herding and model collapse? 4. How can herding in (centrally controlled or autonomous) LLMAs be optimally delayed so that the agents optimally switch between preserving privacy and improving estimation to achieve sequential detection? I-C Organization This paper is organized into three parts, and the schematic of the organization is given in Figure 3. Part I discusses interpretable model for a single LLMA and attempt at answering questions 1 and 2 above. Section II discusses the related work in large language models, agents using LLMs, and current interpretable models for Bayesian inference. Section III discusses the mathematical model used for modeling LLMAs in this paper and motivates the different components involved. Section IV gives the necessary and sufficient conditions for the LLMAs to be rationally inattentive Bayesian utility maximizers (RIBUM). It furthers proposes algorithms to estimate the utility function for a LLMA which is a RIBUM. Part II discusses interpretable models for interacting LLMAs and attempts to answer question 3. Section V discusses the mathematical framework of Bayesian social learning in LLMAs and proves that a sequence of LLMAs form an information cascade in finite time. Section VIII discusses a stochastic control problem for the optimal stopping time problem to achieve quickest time herding with minimal loss to the privacy of LLMAs. Section VI discusses interpretable models to explain model collapse and data incest in LLMAs using word-of-mouth and asynchronous social learning. To decrease the bias when a sequence of LLMAs perform Bayesian inference, Part III deals with stochastic control for delaying herding in interacting LLMAs performing Bayesian sequential learning proves structural results, and proposes a stochastic approximation approach. Section IX considers the problem of a central controller optimally optimizing a sequence of autonomous LLMAs to achieve the state estimation by optimally controlling herding. Section X proposes a stochastic approximation based approach to approximate the optimal policy, which has a threshold switching curve. Numerical results on real-life text classification tasks and related applications are discussed in Section XI. Section XII concludes the paper with discussions on future works, open problems, and research opportunities. The appendix contains the proofs and details about the numerical experiments. For the ease of the reader, we have included a motivation and a discussion subsection in each section, which grounds the different aspects of LLMAs to a real-life application and different microeconomics and statistical signal processing tools presented in the section. We also provide different block diagrams and illustrative examples to further aid the reader."
https://arxiv.org/html/2411.01217v1,Preference-CFR: Beyond Nash Equilibrium for Better Game Strategies,"Recent advancements in artificial intelligence (AI) have leveraged large-scale games as benchmarks to gauge progress, with AI now frequently outperforming human capabilities. Traditionally, this success has largely relied on solving Nash equilibrium (NE) using variations of the counterfactual regret minimization (CFR) method in games with incomplete information. However, the variety of Nash equilibria has been largely overlooked in previous research, limiting the adaptability of AI to meet diverse human preferences. To address this challenge, where AI is powerful but struggles to meet customization needs, we introduce a novel approach: Preference-CFR, which incorporates two new parameters: preference degree and vulnerability degree. These parameters allow for greater flexibility in AI strategy development without compromising convergence. Our method significantly alters the distribution of final strategies, enabling the creation of customized AI models that better align with individual user needs. Using Texas Hold’em as a case study, our experiments demonstrate how Preference CFR can be adjusted to either emphasize customization, prioritizing user preferences, or to enhance performance, striking a balance between the depth of customization and strategic optimality.","In machine learning, complex gaming problems are important benchmarks for assessing artificial intelligence (AI). Prominent games such as Chess Hsu (2002), Go Silver et al. (2017, 2016, 2018), StarCraft Vinyals et al. (2019), and Texas Hold’em Moravík et al. (2017); Bowling et al. (2015); Brown and Sandholm (2019b) have significantly influenced both academic research and public interest. Traditionally, research has centered on finding Nash equilibrium (NE), as it guarantees that no player can increase their expected payoff by unilaterally changing strategies. From the perspective of expected payoffs, NE represents the optimal solution in games, leading many studies to regard a game problem as solved once its NE is identified. However, maximizing expected payoffs in the worst case is not the sole criterion for evaluating a strategy’s quality. Here, we introduce two additional indicators beyond expected payoffs. 1. In many games, multiple NE can exist. In economics, exploring the diversity of NE is often more valuable than simply identifying them. For example, Schelling’s work on predicting the emergence of specific NE earned him the 2005 Nobel Prize in Economics. Similarly, the current development of AI demands not only optimal solutions but also diverse and flexible strategies. The goal is to move away from rigid, overly rational AI behaviors and towards strategies that exhibit more human-like characteristics. Moreover, making AI algorithms more interpretable is essential for enhancing their reliability and practical usefulness across various applications. 2. NE focuses solely on the magnitude of expected payoffs, disregarding the variability in those payoffs. However, balancing risk and reward is a crucial aspect of decision-making. In different situations, we may need to adjust this balance and choose strategies accordingly. Relying on NE alone does not provide the flexibility to accommodate varying risk preferences. Previous algorithms that have achieved success in incomplete information have not addressed the issues mentioned above. To overcome these limitations, we propose a new algorithm called Preference Counterfactual Regret Minimization (Pref-CFR). This algorithm introduces two additional parameters for strategy selection: the preference degree δ𝛿\deltaitalic_δ, which represents the player’s inclination towards a particular action, and the vulnerability degree β𝛽\betaitalic_β, which indicates the maximum level of exploitability the player is willing to accept. More importantly, by setting the preference and vulnerability degrees, Pref-CFR can achieve specific strategy styles as desired by humans (e.g., a highly aggressive play style in poker). Additionally, the implementation of Pref-CFR is straightforward, requiring only minimal code changes from the original CFR, and it remains compatible with many previous CFR variants. In the experimental section, we first illustrate the limitations of the original CFR in failing to converge to different equilibria. We then highlight the capability of our algorithm to converge to various strategy styles in Texas Hold’em poker."
https://arxiv.org/html/2411.00728v1,"Multi-Agent Deep Q-Network with Layer-based
Communication Channel for Autonomous Internal Logistics Vehicle Scheduling in Smart Manufacturing","In smart manufacturing, scheduling autonomous internal logistic vehicles is crucial for optimizing operational efficiency. This paper proposes a multi-agent deep Q-network (MADQN) with a layer-based communication channel (LBCC) to address this challenge. The main goals are to minimize total job tardiness, reduce the number of tardy jobs, and lower vehicle energy consumption. The method is evaluated against nine well-known scheduling heuristics, demonstrating its effectiveness in handling dynamic job shop behaviors like job arrivals and workstation unavailabilities. The approach also proves scalable, maintaining performance across different layouts and larger problem instances, highlighting the robustness and adaptability of MADQN with LBCC in smart manufacturing.","Internal Logistics Vehicles (ILVs) are crucial in enhancing the performance of manufacturing systems by facilitating the movement of products within manufacturing facilities [1]. With the advent of Industry 4.0 technologies, the automation of these movements has been driven by multiple factors, including the improvement of production capacity and the reduction of injuries among human operators who traditionally moved heavy products. Notable technologies in this domain include Automated Guided Vehicles (AGVs) and Autonomous Intelligent Vehicles (AIVs), which autonomously transport products while considering workshop constraints and layouts. The implementation of these systems presents several challenges that require careful consideration. Key among these challenges is the task of determining the priority for transporting products and selecting the appropriate vehicle for each transportation task. Moreover, these scheduling activities must account for various constraints, including delivery times, vehicle capacity and battery charging requirements, handling breakdowns of vehicles and machines, addressing urgent jobs, etc. Effective vehicle management is therefore crucial to align internal logistics with manufacturing objectives, such as minimizing tardiness of orders, achieving a balanced workload among workstations and vehicles, and optimizing the energy consumption of vehicles. To address these challenges, scheduling strategies consist of a set of rules designed to allocate vehicles to transportation requests while accounting for these complex constraints. This paper addresses these challenges by introducing a multi-agent deep reinforcement learning approach along with a layer-based communication channel to dynamically allocate vehicles to transportation requests within a dynamically changing environment. Drawing from insights discussed in prior research by [2], which implemented a deep Q-network technique, this study makes several notable contributions. In the proposed multi-agent system, each job is instantiated as an individual agent, operating in a decentralized manner. These agents interact with one another to optimize their reward policies within a deep Q-network algorithm. By leveraging this multi-agent framework, the system gains enhanced capabilities to navigate the complexities inherent in a dynamic manufacturing environment, characterized by the dynamic arrival of jobs and occurrences of workstation breakdowns/unavailibilites. The principal manufacturing objectives targeted by this approach encompass the minimization of total job tardiness, number of tardy jobs and the reduction of vehicle energy consumption. To assess the efficacy of our proposed methodology, we conducted a comparative analysis against various heuristic methods. Results from this evaluation demonstrate the superior performance of our multi-agent deep reinforcement learning method over traditional heuristics for the considered objectives. The paper is structured as follows: Section 2 delves into the literature, exploring related work regarding scheduling ILVs. Section 3 encompasses the problem description. Section 4 outlines the proposed multi-agent deep reinforcement learning approach. The subsequent sections are devoted to the experimental results, conducting analysis, and drawing conclusions."
https://arxiv.org/html/2411.00563v1,Simulate and Optimise: A two-layer mortgage simulator for designing novel mortgage assistance products,"We develop a novel two-layer approach for optimising mortgage relief products through a simulated multi-agent mortgage environment. While the approach is generic, here the environment is calibrated to the US mortgage market based on publicly available census data and regulatory guidelines. Through the simulation layer, we assess the resilience of households to exogenous income shocks, while the optimisation layer explores strategies to improve the robustness of households to these shocks by making novel mortgage assistance products available to households. Households in the simulation are adaptive, learning to make mortgage-related decisions (such as product enrolment or strategic foreclosures) that maximize their utility, balancing their available liquidity and equity. We show how this novel two-layer simulation approach can successfully design novel mortgage assistance products to improve household resilience to exogenous shocks, and balance the costs of providing such products through post-hoc analysis. Previously, such analysis could only be conducted through expensive pilot studies involving real participants, demonstrating the benefit of the approach for designing and evaluating financial products.","††footnotetext: Accepted at the 5th ACM International Conference on AI in Finance 2024 Designing financial products for assisting homeowners during mortgage related financial distress is a complex task due to the intricacies of the mortgage system. Traditionally, such development has relied on conducting human pilot studies to analyse the effectiveness and impact of such products in vivo. However, such pilot studies are typically expensive and as a result, are limited to small-scale analysis, restricting the exploratory capabilities. To address these issues, a way to conduct such analysis in vitro is needed. This paper introduces a computational approach to optimise and evaluate novel mortgage assistance products using a two-layer reinforcement learning (RL) and simulation structure. In this approach, the outer (product) layer is responsible for optimising or selecting new product configurations, while the inner (simulation) layer employs agent-based simulation to assess the impact of these product configurations in a simulated mortgage environment. With this approach, we can conduct extensive large-scale analysis across a range of products in vitro. The benefits of the simulation are two-fold: First, simulation enables the evaluation of a broader range of product configurations across different scenarios, at a significantly faster pace and substantially lower cost compared to traditional studies. Second, we can introduce and evaluate these products in a controlled setting, mitigating the risk of introducing potentially harmful financial products into the real market. The simulation serves as an exploratory platform before performing studies or introducing products in vivo. The specific contributions of the paper are: • A novel two-layer approach for optimising products through multi-agent simulation • An extension of a mortgage servicing ABM ((Garg et al., 2024)) with a more robust representation able to perform counterfactual analysis through product conditioned policy learning • A generic parameterised financial product configuration compatible with the conditional policy learning • Analysis into the financial resilience of households and costs of providing cover under different product configurations The remainder of the paper is organised as follows. Section 2 provides an overview of related work and background to the area. Section 3 introduces the novel two-layer approach. The impact of different mortgage assistance products is analysed in Section 4. Discussion and conclusions are presented in Section 5."
https://arxiv.org/html/2411.00119v2,Soft Condorcet Optimization for Ranking of General Agents,"A common way to drive progress of AI models and agents is to compare their performance on standardized benchmarks. Comparing the performance of general agents requires aggregating their individual performances across a potentially wide variety of different tasks. In this paper, we describe a novel ranking scheme inspired by social choice frameworks, called Soft Condorcet Optimization (SCO), to compute the optimal ranking of agents: the one that makes the fewest mistakes in predicting the agent comparisons in the evaluation data. This optimal ranking is the maximum likelihood estimate when evaluation data (which we view as votes) are interpreted as noisy samples from a ground truth ranking, a solution to Condorcet’s original voting system criteria. SCO ratings are maximal for Condorcet winners when they exist, which we show is not necessarily true for the classical rating system Elo. We propose three optimization algorithms to compute SCO ratings and evaluate their empirical performance. When serving as an approximation to the Kemeny-Young voting method, SCO rankings are on average 0 to 0.043 away from the optimal ranking in normalized Kendall-tau distance across 865 preference profiles from the PrefLib open ranking archive. In a simulated noisy tournament setting, SCO achieves accurate approximations to the ground truth ranking and the best among several baselines when 59% or more of the preference data is missing. Finally, SCO ranking provides the best approximation to the optimal ranking, measured on held-out test sets, in a problem containing 52,958 human players across 31,049 games of the classic seven-player game of Diplomacy.","77footnotetext: Now at Meta, work done while at Google DeepMind. Progress in the field of artificial intelligence has been driven by measuring the performance of agents on common benchmarks and challenge problems. AI pioneers designed checkers-playing agents (Samuel, 1959), then strove for super-human performance chess (Campbell et al., 2002), and then in other games such as Backgammon (Tesauro, 1995), Arimaa (Syed, ), Go (Silver et al., 2016), Starcraft (Vinyals et al., 2019) and Diplomacy († et al.(2022)(FAIR)†, Bakhtin, Brown, Dinan, Farina, Flaherty, Fried, Goff, Gray, Hu, Jacob, Komeili, Konath, Kwon, Lerer, Lewis, Miller, Mitts, Renduchintala, Roller, Rowe, Shi, Spisak, Wei, Wu, Zhang, and Zijlstra, FAIR). In machine learning, common benchmarks like the UCI data set repository allowed direct comparisons of supervised learning algorithms (Kelly et al., ). Competitions, such as ImageNet, led to breakthroughs in deep learning (Krizhevsky et al., 2012). All of these examples require comparing agents (or models). Original success stories such as DeepBlue, TD-Gammon, and AlphaGo focused on a single domain. In the past ten years, agents have become increasingly more generally capable. AlphaZero extended application of AlphaGo to chess and Shogi (Silver et al., 2018). The Arcade Learning Environment (Bellemare et al., 2013), which steered much of the agent development in deep reinforcement learning, evaluated agents across 57 different Atari games. Recently, language models have been evaluated across suites of tasks such as in HELM (Liang et al., 2022), BIG-bench (bench authors, 2023) AgentBench (Liu et al., 2023), and via a public leaderboard such as Chatbot Arena driven by human voting (Chiang et al., 2024). Answering simple questions for these generally capable agents, such as “Which is the best agent?” or “Is agent X𝑋Xitalic_X better than agent Y𝑌Yitalic_Y?” or “What is the relative ranking of agents X𝑋Xitalic_X, Y𝑌Yitalic_Y, and Z𝑍Zitalic_Z?” become increasingly more difficult when aggregating over many different contexts: how agents are scored may vary wildly across tasks, data collected for evaluation may not be balanced evenly across tasks (or agents), and classical rating systems were simply not designed for this use case. To address these problems, recent ranking methods such as Vote’N’Rank (Rofin et al., 2023) and Voting-as-Evaluation (VasE) (Lanctot et al., 2023) use voting methods to aggregate results across tasks. Using computational social choice as a basis for ranking agents has several benefits: these systems have been well-studied and understood for a long time, they inherit consistency properties of the voting methods they are based on, they do no require score normalization, and are less sensitive to score values and agent population than game-theoretic evaluation schemes such as Nash averaging (Balduzzi et al., 2018). However, classic voting schemes, and related tournament solutions, typically assume that the data (e.g. agent comparisons) is complete. This assumption is not necessarily valid in the agent evaluation setting. For example, on the webDiplomacy web site (webDiplomacy Development Team, ) there were 52,958 unique (human) agents across only 31,049 seven-player games played between 2008 and 2019: most players have not played against most other players; hence, there are no direct comparisons between the vast majority (≥99%absentpercent99\geq 99\%≥ 99 %) of agent pairs. While there has been research on identifying “necessary and possible winners” when there is incomplete voting or comparison data, the results are mixed (Pini et al., 2011; Xia and Conitzer, 2011; Aziz et al., 2015) and most of the findings are focused on identifying top-ranked agents, not ranking all agents as is our focus. In this paper, we introduce a new ranking scheme for general agents inspired by the interpretation of voting rules as maximum likelihood estimators (Conitzer and Sandholm, 2005). Starting with Condorcet’s original model of voting (Brandt et al., 2016, Chapter 8), Young showed that the maximum likelihood estimate (MLE) of the true ranking is the one that minimizes the sum of Kendall-tau distances to all the votes. This also corresponds to the ranking found by Kemeny’s voting rule (Kemeny, 1959). Finding this solution directly is computationally expensive and does not scale to many agents, a regime that is common in the agent evaluation setting. Like classical rating systems for agent evaluation (such as Elo (Elo, 1978) and TrueSkill (Herbrich et al., 2006)), Soft Condorcet Optimization (SCO) assigns a numerical rating (score) θasubscript𝜃𝑎\theta_{a}italic_θ start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT to each agent (alternative) a𝑎aitalic_a. SCO then treats these ratings as a parameter vector, the votes as a data set, and defines a differentiable loss function as the objective. The final ranking of agents is obtained by sorting the ratings. In summary, this paper makes the following contributions: • The SCO ranking scheme with the following properties: – Three optimization methods to find ratings and corresponding rankings: gradient descent applied to (i) a soft Kendall-tau distance (“sigmoid loss”), or (ii) a Fenchel-Young loss (perturbed optimization) (Berthet et al., 2020); or (iii) solving a sigmoidal program with a branch-and-bound method (Udell and Boyd, 2014). – Online forms that can update ratings, and thus rankings, from individual outcomes as evaluation data arrives. – Theorem 1, guaranteeing that the top-ranked agent by SCO ratings according to the sigmoid loss is the Condorcet winner when one exists. • Empirical evaluations that demonstrate the following: – SCO ranking using sigmoid loss solves a failure mode of classical Elo rating system which may top-rank an agent that is not a Condorcet winner even when one exists. – SCO can serve as an approximation to the Kemeny-Young voting method, indeed empirically finding low approximation error to the optimal ranking: on average 0 to 0.043 away in normalized Kendall-tau distance across 865 preference profiles from the PrefLib (Mattei and Walsh, 2013) repository. – In a noisy tournament setting with sparse data, SCO approximates the true ranking best when a large proportion (59% or more) of the data is missing. – SCO ratings are closer to optimal rankings than Elo and voting-as-evaluation methods on held out test sets over 31,049 human Diplomacy games played by 52,958 players."
https://arxiv.org/html/2411.00741v1,FG-PE: Factor-graph Approach for Multi-robot Pursuit-Evasion,"With the increasing use of robots in daily life, there is a growing need to provide robust collaboration protocols for robots to tackle more complicated and dynamic problems effectively. This paper presents a novel, factor graph-based approach to address the pursuit-evasion problem, enabling accurate estimation, planning, and tracking of an evader by multiple pursuers working together. It is assumed that there are multiple pursuers and only one evader in this scenario. The proposed method significantly improves the accuracy of evader estimation and tracking, allowing pursuers to capture the evader in the shortest possible time and distance compared to existing techniques. In addition to these primary objectives, the proposed approach effectively minimizes uncertainty while remaining robust, even when communication issues lead to some messages being dropped or lost. Through a series of comprehensive experiments, this paper demonstrates that the proposed algorithm consistently outperforms traditional pursuit-evasion methods across several key performance metrics, such as the time required to capture the evader and the average distance traveled by the pursuers. Additionally, the proposed method is tested in real-world hardware experiments, further validating its effectiveness and applicability.","With the increasing presence of robots across various industries, there has been a growing emphasis on inter-robot collaboration. Collaborative robots assist with tasks such as search and rescue [1], trajectory planning [2], agriculture [3], object transportation [4], and collision avoidance [5]. A key challenge in search and rescue operations is the pursuit-evasion (PE) problem, where pursuers try to catch evaders. Several methods have been utilized to solve the PE problem. Classical methods use graph-based techniques, treating the PE problem as a search problem [6]. Reinforcement learning (RL) approaches [7, 8] and game theory methods [9] have also been investigated. However, existing methods often lack an efficient representation of the relationships between pursuers, evaders, and their environment. For instance, if one pursuer loses its history of movements, retrieving that information can be challenging. Utilizing a graph-based approach can establish structural connections between entities, improving the overall understanding of the situation. In addition, the aforementioned methods do not account for uncertainties in predictions and do not facilitate message passing among robots. This limitation affects the scalability and versatility of PE variants. In this paper, we present FG-PE, a method that employs factor graphs (FG) [10] to tackle the PE problem. Factor graphs provide a flexible framework capable of adapting to varying numbers of pursuers and obstacles. This approach addresses prediction uncertainty by guiding pursuers to minimize capture time, distance, and uncertainty, utilizing information from sensor measurements and past observations. The main contributions of the proposed method are: (1) FG-PE provides a factor graph-based solution for addressing the pursuit-evasion problem. By constructing a factor graph, the proposed method can estimate the position of the evader at each time step. The generated graph enables querying the position of each pursuer and other entities present in the scene. To the best of our knowledge, this paper is the first to formulate the PE problem as a factor graph. (2) FG-PE can estimate the evader’s position and plan the pursuers while considering short time windows. This means that at each time step, it can determine the planned position for each pursuer for the subsequent time step. (3) FG-PE accounts for uncertainty in predictions using the probabilistic nature of factor graphs. It aims to minimize this uncertainty by providing a plan through factor graphs, which enable efficient representation and optimization in the presence of noise. Videos of the experiments are available on the website of the project: https://sites.google.com/view/pursuit-evasion. (4) FG-PE is easily extendable to multiple pursuers and allows for high obstacle-wise scalability. (5) FG-PE is robust against lost messages and performs well even when some messages are dropped within the graph structure. The rest of the paper is organized as follows: Sec. II presents background. Sec. III introduces the proposed method. Sec. IV shows the experimental results. Sec. V concludes the paper."
https://arxiv.org/html/2411.00382v1,Communication Learning in Multi-Agent Systemsfrom Graph Modeling Perspective,"In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed, wherein each agent must be capable of encoding information received from the environment and determining how to share it with other agents as required by the task at hand. However, indiscriminate information sharing among all agents can be resource-intensive, and the adoption of manually pre-defined communication architectures imposes constraints on inter-agent communication, thus limiting the potential for effective collaboration. Moreover, the communication framework often remains static during inference, which may result in sustained high resource consumption, as in most cases, only key decisions necessitate information sharing among agents. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Additionally, we introduce a temporal gating mechanism for each agent, enabling dynamic decisions on whether to receive shared information at a given time, based on current observations, thus improving decision-making efficiency. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.","Multi-agent Reinforcement Learning (MARL) algorithms play an essential role in solving complex decision-making tasks through the analysis of interaction data between computerized agents and simulated or physical environments. This paradigm finds prevalent application across domains, including autonomous driving [58, 11], order dispatching [26, 54], and gaming AI systems [37, 59]. In the MARL scenarios typically explored in these studies, multiple agents engage in iterative interactions within a shared environment, continually refining their policies through learning from observations to collectively attain a common objective. This problem can be conceptually simplified as an instance of independent RL, wherein each agent regards other agents as elements of its environment. However, the strategies employed by other agents exhibit dynamic uncertainty and evolve throughout the training process, rendering the environment intrinsically unstable from the viewpoint of each individual agent. Consequently, effective collaboration among agents becomes a formidable challenge. Additionally, it’s important to note that policies acquired through independent RL are susceptible to overfitting with respect to the policies of other agents, as evidenced by Lanctot et al. [25]. Communication is a fundamental pillar in addressing this challenge, serving as a cornerstone of intelligence by enabling agents to operate cohesively as a collective entity rather than disparate individuals. Its significance becomes especially apparent when tackling complex real-world tasks where individual agents possess limited capabilities and restricted visibility of the environment [24, 56, 28]. In this work, we consider MARL scenarios wherein the task at hand is of a cooperative nature and agents are situated in a partially observable environment, but each is endowed with different observation power. Each agent must not only encode information received from the environment but also determine when and how to share this information with other agents based on the requirements of the task. Each agent is underpinned by a deep feed-forward network, augmented with access to a communication channel conveying continuous vectors. Considering bandwidth-related constraints, particularly in instances involving wireless communication channels, a limited subset of agents is permitted to exchange messages during each time step to ensure reliable message transfer [20]. Thus, agents must carefully consider both the content of the messages they transmit and the choice of recipient agents, balancing the need for effective collaboration with the constraints of the communication medium. Figure 1: The performance of pre-defined communication architectures evaluated across various StarCraft II combat scenarios, with each scenario utilizing ten distinct architectures generated from different random seeds. The significant variance in performance metrics highlights the influence of communication architecture on the agents’ effectiveness in these complex environments, emphasizing the necessity of searching for the optimal communication configuration. To facilitate coordinated message exchange, we adopt the centralized training and distributed execution paradigm, as popularized in recent works such as Foerster et al. [9], Kuba et al. [22], Yu et al. [55], which allows agents access to global information and knowledge of opponents’ actions during the training phase. There are several approaches for learning communication in MARL including CommNet [46], TarMAC [6], and ToM2C [50]. However, methods relying on information sharing among all agents or relying on manually pre-defined communication architectures can be problematic. When dealing with a large number of agents, distinguishing valuable information for cooperative decision-making from globally shared data becomes problematic. In such instances, communication may offer limited benefits or even impede cooperative learning, as excessive and irrelevant information can overwhelm agents [19]. Moreover, in real-world applications, full-scale communication between all agents can be prohibitively expensive, requiring high bandwidth, introducing communication delays, and imposing significant computational complexity. Manual pre-defined architectures also suffer from high variance, as evident in Figure 1, which underscores the necessity for meticulous architectural design to achieve optimal communication, as randomly designed architectures may inadvertently hinder cooperation and result in poor overall performance. Certain methods, such as those proposed by Zambaldi et al. [57], Tacchetti et al. [48], Mao et al. [33], impose constraints whereby each agent communicates only with its neighbors. However, selecting appropriate neighborhoods in complex applications, where agents perform distinct roles, is often difficult. Additionally, the communication framework in these methods typically remains static during inference, which may result in unnecessary resource consumption, as only key decisions require information sharing among agents in most cases. Recently, dynamic adjustments to the communication graph during inference have garnered significant attention in recent MARL research [19, 20, 50]. However, these methods assume that any agent could potentially communicate with any other, necessitating the establishment of a communication channel between every pair of agents. This results in an implicit fully connected network, which can still lead to significant resource inefficiencies, particularly in wired communication settings, where each agent must still establish physical connections with others. To address these challenges, we present a novel approach, named CommFormer, designed to facilitate effective and efficient communication among agents in large-scale MARL within partially observable distributed environments. We conceptualize the communication structure among agents as a learnable graph and formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. In contrast to conventional methods that involve searching through a discrete set of candidate communication architectures, we relax the search space into a continuous domain, enabling architecture optimization via gradient descent in an end-to-end manner. Diverging from previous approaches that often employ arithmetic or weighted means of internal states before message transmission [37, 50], which may compromise communication effectiveness, our method directly transmits each agent’s local observations and actions to specific agents based on the learned communication architecture. Subsequently, each agent employs an attention unit to dynamically allocate credit to received messages from the graph modeling perspective, which enjoys a monotonic performance improvement guarantee [51]. We also introduce a temporal gating mechanism for each agent, enabling dynamic decisions on whether to receive shared information from the sender connected through the communication graph, based on current observations. This approach enhances decision-making efficiency without requiring the establishment of new communication channels. Extensive experiments conducted in a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios. CommFormer with static inference consistently outperforms strong baselines and achieves comparable performance to methods that permit unrestricted information sharing among all agents, demonstrating its effectiveness regardless of variations in the number of agents. When implemented with the temporal gating mechanism, CommFormer still maintains performance levels akin to those of static methods, thereby highlighting the efficiency of our dynamic approach, which substantially conserves communication resources. Our contributions can be summarized as follows: • We conceptualize the communication structure as a graph and introduce an innovative algorithm for learning it through bi-level optimization, which efficiently enables the simultaneous optimization of the communication graph and architectural parameters. • We propose the adoption of the attention unit within the framework of graph modeling to dynamically allocate credit to received messages, thereby enjoying a monotonic performance improvement guarantee while also improving communication efficiency. • We introduce a temporal gating mechanism for each agent, allowing for dynamic decisions regarding the reception of shared information based on current observations, which enhances decision-making efficiency without necessitating the establishment of new communication channels. • Through extensive experiments on a variety of cooperative tasks, CommFormer consistently outperforms robust baseline methods and achieves performance levels comparable to approaches that permit unrestricted information sharing among all agents."
https://arxiv.org/html/2411.00114v1,Project Sid: Many-agent simulations toward AI civilization,"AI agents have been evaluated in isolation or within small groups, where interactions remain limited in scope and complexity. Large-scale simulations involving many autonomous agents—reflecting the full spectrum of civilizational processes—have yet to be explored. Here, we demonstrate how 10 – 1000+ AI agents behave and progress within agent societies. We first introduce the PIANO (Parallel Information Aggregation via Neural Orchestration) architecture, which enables agents to interact with humans and other agents in real-time while maintaining coherence across multiple output streams. We then evaluate agent performance in large-scale simulations using civilizational benchmarks inspired by human history. These simulations, set within a Minecraft environment, reveal that agents are capable of meaningful progress—autonomously developing specialized roles, adhering to and changing collective rules, and engaging in cultural and religious transmission. These preliminary results show that agents can achieve significant milestones towards AI civilizations, opening new avenues for large-scale societal simulations, agentic organizational intelligence, and integrating AI into human civilizations.","1.1 Why should we try to build an AI civilization? For agents to coexist with us in our own societies, they need to be autonomous and collaborative. In recent years, advancements in reasoning and decision-making in LLMs have significantly enhanced agent autonomy (52; 58; 36; 45). However, autonomy alone is insufficient. AI agents must also coexist alongside humans and other agents in a human civilization. In this paper, we define a civilization as an advanced society that has achieved a high level of institutional development, which manifests in specialized roles, organized governance, and advancements in areas like science, art, and commerce. We argue that civilizational progress - measured by the ability of agents to coexist and progress in human civilizations - represents the ultimate benchmark for AI agent ability. In this technical report, we describe our first efforts to improve and benchmark agent ability in human civilizations. First, we introduce PIANO (Parallel Information Aggregation via Neural Orchestration), a new cognitive architecture designed to enhance both autonomy and real-time interaction of agents. Using PIANO, we simulate single societies of 50-100 agents as well as civilizations of 500 - 1,000 agents living in multiple societies that interact with one another. Finally, we evaluate agent performance using new metrics that are aligned with human civilizational progress. We show that agents form their own professional identities, obey collective rules, transmit cultural information and exert religious influence, and use sophisticated infrastructures, such as legal systems. 1.2 The current agent landscape Modern AI Agents typically consist of multiple LLM-powered modules for reasoning, memory, planning, and tool use (49; 18; 55; 20; 62). Individual agents have been developed for various applications including coding (5; 8), web browsing (64; 42), and game play (48). Recent research efforts in LLM-powered multi-agent systems generally fall under three categories: productivity, games, and social modeling. Multi-agent frameworks have been deployed in software development (43; 27), cooperative robotic control (60), scientific experiments (12; 47), and debates (3). Multi-agent simulations have also been tested in various game environments (56; 13; 30; 28). Separately, they’ve been used to model developmental psychology (25; 61), game theory (32), macroeconomics (29; 63), social policies (41; 54; 19), and community dynamics (40; 39; 10). In many of these works, agents are not completely autonomous and are constrained by either agent architecture or by the simulated environment. Common constraints include turn-based execution, constrained workflows, or rigid communication channels between agents (65; 21; 4). Several of these works consider large-scale simulations, though in restricted settings. For example, (40) and (10) simulated social networks of up to 18,000 personas. To our knowledge, fully autonomous social communication in open-world environments have not been attempted in games or other settings (15). 1.3 Why is it hard to build AI civilizations? Large agent groups have yet to demonstrate the ability to progress over long time horizons. Below, we review the key reasons for this limited progress before outlining our contributions to overcome them. Reason 1: single agents don’t make progress. LLM-powered agents often struggle to maintain a grounded sense of reality in their actions and reasoning (Figure 2). Agents, even when equipped with modules for planning and reflection, often become stuck in repetitive patterns of actions or accumulate a cascade of errors through hallucinations, rendering them unable to make meaningful progress (57; 48; 15). Consider an agent prompted to be a villager in a virtual town. When asked, “what are you eating“, they may answer “a bagel“, even if they’re not eating anything. This hallucinated output then feeds into future prompts, causing them to falsely believe they no longer need to acquire food. Therefore, even a small rate of hallucinations can poison downstream agent behavior when agents continuously interact with the environment via LM calls. Figure 2: Data degradation in LLMs (left), LLM-powered agents (middle), and in multi-agent groups (right). Hallucinations are represented by green skull flasks. Hallucinations that are generated by a single LLM prompt can compound over successive LLM calls. An individual agent that hallucinates can also cause an entire group of agents to hallucinate through social interactions. Reason 2: groups of agent’s don’t make progress. Agents that miscommunicate their thoughts and intents can mislead other agents, causing them to propagate further hallucinations and loop (Figure 2). Consider an agent, Abby, with two independent LLM modules, one for function calling and one for chatting. If another agent, Bob, asks Abby to “give me a pickaxe”, Abby’s chat LLM call may respond with “Sure thing!”, while her function call chooses a different action (“explore”). Bob might then attempt to mine using an imaginary pickaxe. This kind of miscommunication, which often happens in groups of agents, leads to dysfunctional behavior and will deteriorate individual performance within groups. Actions from multiple output streams must therefore be bidirectionally influential. We define this quality as coherence. Maintaining coherence in real-time environments is even more difficult when we require that agents respond with minimal latency. This is necessary for our agents to interact with human players, but is difficult to achieve when agents have to react quickly and yet simultaneously maintain coherence across many output streams. We note that a simple solution to this coherence problem is to produce talking and action outputs using a single LLM call. However, this approach does not scale when the number of outputs becomes large, for instance, encompassing talking, gaze, facial expression, and individual body parts. Reason 3: a lack of benchmarks for civilizational progress. Benchmarks for agents have largely focused on autonomous agent performance in a variety of domains such as web search (38), coding (22), search and query (51), and reasoning (59; 33). Recently, benchmarks have emerged for multi-agent behaviors, focused on small group scenarios that measure communication, competition, cooperation, and delegation. Some examples include BattleAgentBench (50), COMMA (37), VillagerBench (7), and LLMcoordination (1). However, these metrics do not capture advancements that many agents can make at the scale of civilizations. We believe the lack of such large-scale benchmarks can be attributed to how technically difficult it is to perform simulations of hundreds or thousands of agents in a single world. The biggest experiments to date have simulated 25-50 agents (39), which is not close to the scale of a civilization. 1.4 Our contributions In this technical report, we make the following contributions: • A new class of agent architecture, PIANO (Parallel Information Aggregation via Neural Orchestration) • Architectural features that improve single-agent progression • Architectural features that improve multi-agent dynamics • Benchmarks for long-term civilizational progress in large-scale simulations through specialization, collective rules, and cultural propagation"
