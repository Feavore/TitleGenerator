URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04686v1,Precision-Aware Iterative Algorithms Based on Group-Shared Exponents of Floating-Point Numbers,"Iterative solvers are frequently used in scientific applications and engineering computations. However, the memory-bound Sparse Matrix-Vector (SpMV) kernel computation hinders the efficiency of iterative algorithms. As modern hardware increasingly supports low-precision computation, the mixed-precision optimization of iterative algorithms has garnered widespread attention. Nevertheless, existing mixed-precision methods pose challenges, including format conversion overhead, tight coupling between storage and computation representation, and the need to store multiple precision copies of data. This paper proposes a floating-point representation based on the group-shared exponent and segmented storage of the mantissa, enabling higher bit utilization of the representation vector and fast switches between different precisions without needing multiple data copies. Furthermore, a stepped mixed-precision iterative algorithm is proposed. Our experimental results demonstrate that, compared with existing floating-point formats, our approach significantly improves iterative algorithms‚Äô performance and convergence residuals.","Numerical simulations of many practical problems often rely on solving the linear equation system ùë®‚Å¢ùíô=ùíÉùë®ùíôùíÉ\bm{A}\bm{x}=\bm{b}bold_italic_A bold_italic_x = bold_italic_b, where ùë®ùë®\bm{A}bold_italic_A is an m√ónùëöùëõm\times nitalic_m √ó italic_n sparse matrix, and ùíôùíô\bm{x}bold_italic_x and ùíÉùíÉ\bm{b}bold_italic_b are dense vectors of sizes nùëõnitalic_n and mùëömitalic_m, respectively. Iterative solvers such as conjugate gradient (CG) and generalized minimal residual (GMRES) methods are commonly used in practice. Sparse matrix-vector multiplication (SpMV) is a frequently used computational kernel in these algorithms, but it is also their primary performance bottleneck. The computational efficiency of SpMV is constrained by the memory access efficiency, making it a typical memory-bound kernel. With the continuous development of artificial intelligence technology, many computer hardware manufacturers have begun to support multiple precision data representations in their latest products. For example, NVIDIA‚Äôs latest H100 supports TF32, BF16, FP16, FP8 (E4M3 and E5M2), among others. This has inspired researchers to use mixed-precision techniques to optimize iterative algorithms. Compared to traditional iterative algorithms, which typically use a single floating-point format like FP64 or FP32, mixed-precision optimization introduces multiple-precision floating-point representations in iterative algorithms, reducing memory access and computational overhead, thereby improving the efficiency of iterative solving algorithms. As a key kernel of iterative algorithms, mixed-precision optimization of SpMV has attracted the attention of researchers [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The main idea behind these efforts is to select different-precision floating-point representations for non-zero elements (non-zeros) based on their magnitudes. For example, smaller values are stored in the single-precision floating-point format, while larger values are stored in the double-precision format, with both computations performed in double precision. In terms of mixed-precision optimization for iterative algorithms, current work mainly focuses on using different-precision floating-point representations in different statements or iterations [11, 12, 13, 14, 15]. Existing mixed-precision algorithms have improved the computational efficiency of iterative algorithms. However, using general-purpose floating-point representations introduces several challenges: (1) Lower utilization of binary bits: FP64 and FP32 have 11 and 8 exponent bits, respectively, and are two widely used floating-point representations. However, in specific applications, floating-point data may have particular numerical distributions, such as a small distribution range or most data being distributed around some points. In this case, the exponent field of the existing floating-point representations has a low binary bits utilization. (2) Tight coupling between storage and computation: In systems based on traditional floating-point formats, the precision used for storage is tightly coupled with the precision for computation, which sacrifices flexibility and limits the mixed-precision optimization of iterative algorithms. (a) Information entropy (b) Top-1 (c) Top-2 (d) Top-4 (e) Top-8 (f) Top-16 (g) Top-32 (h) Top-64 Figure 1: Non-zeros value distribution of sparse matrices. (a) Information entropy of non-zeros values, exponents, and mantissa. (b)-(h) Ratios of non-zeros corresponding to the top-kùëòkitalic_k exponents. Based on the observations above and the observation that most non-zeros in a sparse matrix exhibit close or same exponents, this paper introduces an innovative mixed-precision iterative algorithm based on group-shared exponents. The core idea lies in the extraction and separate storage of shared exponents for a collection of floating-point numbers, thus freeing exponent bits for additional mantissa storage. This method boosts computational efficiency and improves the convergence of iterative computations with lower precision. This approach is practical because the separately stored shared exponents can be represented with more binary bits, preventing data overflow in computations. Moreover, by dedicating all bits, aside from the sign bit, to the mantissa representation, this approach improves the precision of data representation under bit-width constraints. The primary contributions of this paper include: ‚Ä¢ A new floating-point format is proposed, based on group-shared exponents and segmented storage of the tail, which eliminates the need for storing multiple copies of the same data in different precisions. ‚Ä¢ A stepped mixed-precision iterative algorithm optimization method based on the group-shared exponents‚Äô floating-point format is proposed, which maintains the convergence property of the iterative algorithm while reducing the SpMV memory footprint overhead. ‚Ä¢ The optimization method for the mixed-precision iterative algorithm introduced in this paper has been validated on a GPU platform. Experimental results demonstrate that our proposed method improves the solving efficiency of the iterative algorithm and achieves similar residuals to high-precision iterative solvers. The remainder of this paper is organized as follows. Section II analyzes the numerical distribution of sparse matrices used in iterative algorithms. Section III describes our proposed new floating-point format and stepped mixed-precision iterative algorithm. Section IV evaluates the performance of the proposed approach. Section V discusses related work, and Section VI concludes the paper."
https://arxiv.org/html/2411.04561v1,Joint wireless and computing resource management with optimalslice selection in in-network-edge metaverse system,"This paper presents an approach to joint wireless and computing resource management in slice-enabled metaverse networks, addressing the challenges of inter-slice and intra-slice resource allocation in the presence of in-network computing. We formulate the problem as a mixed-integer nonlinear programming (MINLP) problem and derive an optimal solution using standard optimization techniques. Through extensive simulations, we demonstrate that our proposed method significantly improves system performance by effectively balancing the allocation of radio and computing resources across multiple slices. Our approach outperforms existing benchmarks, particularly in scenarios with high user demand and varying computational tasks.","As technology advances, the new wave of networks is blending virtual and augmented realities with our real world. This creates a whole new space called the metaverse, where the digital and physical worlds come together [1]. Creating and maintaining the Metaverse requires enormous resources, including computing resources for intensive data processing to support the Extended Reality, storage resources, and communication resources for maintaining ultra-high-speed and low-latency connections [2]. Given these demands for resources, there is a need for a solution that efficiently manages and allocates these diverse types of resources to power the applications and functionalities within the Metaverse. Network slicing, where multiple end-to-end networks are created on shared physical infrastructure, solves this problem. Different network slices can be used for specific applications or services [3]. They can also scale up and down according to the service requirements, and network resources to the metaverse can be allocated according to the demand to facilitate the recommended QoE to the user while optimizing the resource usage in the network. The metaverse consists of a large-scale number of virtual worlds that require interoperability with each other. These virtual worlds must be rendered in real-time and synchronized with the physical world. The MEC provides a distributed infrastructure located near the user, however it fails to meet concurrent user demands, which results in high delay and severely affects the full realization of the metaverse[4]. According to Internet Research Task Force (IRTF) [5], the Computing in the network (COIN) research group, 6G edge nodes acting as task executors are not only purpose-built servers. They encompass any edge node augmented with computing resources. This offers a promising solution, utilizing untapped network resources for task execution, thereby diminishing latency and fulfilling quality of experience (QoE) requirements. However, augmenting computing resources or enabling COIN increases power consumption [6]. Effectively allocating COIN resources in real-time to adapt to dynamic user demands while ensuring overall system availability still presents a critical challenge. Despite the advancement in resource management problems, challenges persist. Network slicing and in-network computing have been extensively investigated in the literature to address such challenges. Previous approaches assume each application is dynamically assigned to a specific slice with a static resource pool based on workload and SLA requirements [7]. However, dynamic assignment leads to mixed workloads in slices and consequently demands flexibility in managing these resources. Jo≈°ilo et al. [8] tackled the resource management and dynamic assignment of slices, by solving complex algorithms in an edge computing system, but with the presence of in-network computing, where multiple nodes act as potential task executors, efficiently managing these resources still remains a challenge. Unlike the above works, the authors in [9] used a Water-Filling-based heuristic algorithm to address joint network slicing and in-network computing resource allocation. However, their focus was solely on managing resources between slices (inter-slice) without considering the resource management issues within the slices (intra-slice). In this paper, we address the joint network slicing, inter-slice radio, intra-slice radio and in-network resource management problem and make the following key contributions: ‚Ä¢ We formulated the problem as a mixed-integer non-linear programming problem (MINLP) and achieved the optimal solution through a standard optimization solver. ‚Ä¢ We performed an extensive evaluation under different load and task settings and compared it against some benchmark solutions."
https://arxiv.org/html/2411.04538v1,Analysis of Blockchain Assisted Energy Sharing Algorithms with Realistic Data Across Microgrids,"With escalating energy demands, innovative solutions have emerged to supply energy affordably and sustainably. Energy sharing has also been proposed as a solution, addressing affordability issues while reducing consumers‚Äô greed. In this paper, we analyse the feasibility of two energy sharing algorithms, centralized and peer-to-peer, within two scenarios, between microgrids within a county, and between microgrids across counties. In addition, we propose a new sharing algorithm named Selfish Sharing, where prosumers take advantage of consumers‚Äô batteries in return for letting them consume part of the shared energy. The results for sharing between microgrids across counties show that the dependency on the grid could be reduced by approximately 5.72%, 6.12%, and 5.93% using the centralized, peer-to-peer and selfish sharing algorithms respectively, compared to trading only. The scenario of sharing between microgrids within a county has an average decrease in dependency on the grid by 5.66%, 6.0%, and 5.80% using the centralized, peer-to-peer and selfish algorithms respectively, compared to trading without sharing. We found that trading with batteries and the proposed sharing algorithms prove to be beneficial in the sharing between microgrids case. More specifically, the case of trading and sharing energy between microgrids across counties outperforms sharing within a county, with P2P sharing appearing to be superior.","The increasing energy need and the rising environmental issues that raising from utility grids, led more consumers to install energy generation tools, such as solar panels, to enable them to use energy in a more clean and affordable way. In Europe, the usage of renewable energy is targeted to be about 45% in 2030 [1]. In another interesting study, the number of homes with solar panels in the United States is expected to be 16.8 millions in 2032, compared to only 3.6 millions in 2022 [2]. Those consumers might still need energy and might generate energy more than what they need. In the case of generating energy more than needed, the consumers become energy providers, or what is called prosumers. With the spread of prosumers that are considered as distributed energy sources, microgrid term appeared. A microgrid brings together distributed power sources, loads, energy storage devices, and control devices to create a unified and manageable power supply system [3]. Figure 1 illustrates how the microgrid controller coordinates between the distributed energy resources, loads, and utility grid. Microgrid controller decides when to sell or buy from the utility grid, charges/discharges the energy storage, utilizes the renewable and distributed energy resources efficiently, and sends energy to loads when needed. Figure 1: Microgrid coordinating between distributed energy sources, loads, and utility grid. Peer-to-peer (P2P) energy trading between prosumers and consumers within or between microgrids was introduced to facilitate the transfer of energy between peers in more affordable way [4]. Some solutions utilized blockchains and smart contracts to provide the trading service in a decentralized and more reliable manners such as [5], where blockchain could be considered as an immutable, decentralized, distributed, auditable and transparent ledger that is managed by a consensus protocol [6]. Smart contracts on the other hand are programs that are hosted on the blockchain, where a group of people trusts the program to do some operations for them in a decentralized and transparent way [7]. Although P2P energy trading aims to offer the energy at a more affordable price, there are still some consumers or even prosumers not able to generate enough energy for their need, and not able to afford it as well. For instance, in Spain during the year 2021, energy poverty was 9.3% of the households, meaning approximately tenth of the households consumed energy less than the national median [8]. Decreasing energy poverty and enabling consumers to access energy when they need it and not being able to afford it have been addressed in studies such as [9], [10], and [11]. In this paper, we examine the effectiveness of previously proposed battery-based sharing algorithms between microgrids in two different scenarios. In addition, we propose a new sharing algorithm, namely Selfish Sharing. The main contributions are as follows: 1. An examination of the efficacy of previously proposed battery-based sharing algorithms for inter-microgrid sharing, investigated within two distinct scenarios: between counties and within a county settings. 2. Proposal of a novel sharing algorithm called Selfish Sharing, and assessment of its effectiveness within the aforementioned scenarios. In the proposed selfish sharing algorithm, prosumers share energy if they need it in the future where a percentage of this shared energy would be consumed by the receiving consumer, and the left shared amount would be stored on the consumer‚Äôs energy storage to be used by the sharing prosumer within a specific amount of time. By using blockchain, the proposed system enables the transactions to be handled between the peers directly without third party involvement, along with matching between the offers, trading requests, and sharing requests in a decentralized manner using smart contracts. 3. After analysing the simulation results of the proposed algorithms, we found that energy trading and sharing between microgrids across counties performs better than energy trading and sharing within a county. 4. Demonstration of the advantageous nature of incorporating energy storage systems to accumulate surplus energy for subsequent discharge during periods of demand, providing tangible benefits to prosumers. 5. Affirmation of the beneficial impact of both centralized and P2P sharing algorithms in facilitating energy sharing between microgrids. 6. From a monetary point of view, empirical evidence supports the superiority of the selfish sharing algorithm over P2P and centralized algorithms in the within a county sharing scenario. It also shows a comparative advantage over the centralized sharing algorithm in the between-counties sharing scenario, particularly in terms of decreasing dependency on the grid. In this paper, the term ‚Äùenergy trading‚Äù refers to transferring energy for direct monetary return, while ‚Äùenergy sharing‚Äù refers to transferring energy in return of a non-monetary form. The paper is organized as follows. Section II provides the literature review and Section III demonstrates the materials and methods. Section IV describes the experimental analysis, and Section V discusses the obtained results. Finally, Section VI states the conclusions."
https://arxiv.org/html/2411.04318v1,Intersections of Web3 and AI ‚Äì View in 2024,"This paper summarises the intersection of Web3 and AI technologies, synergies between these technologies, and gaps that we suggest exist in the conception of the possible integrations of these technologies. The summary is informed by a comprehensive literature review of current academic and industry papers, analyst reports, and Ethereum research community blogposts. We focus our contribution on the perceived gaps and detail some novel approaches that would benefit the blockchain/Web3 ecosystem. We believe that the overview presented in this paper will help guide researchers interested in the intersection of Web3 and AI technologies.","We first need to address why we are interested in Artificial Intelligence (AI) at all in the context of Web3 and how general industry trends might affect our industry. Recent advancements in AI, particularly in Large Language Models (LLMs) such as GPT-4, have significant implications for businesses, and especially those in the software development sector. Studies have highlighted a shift in the anticipated impact of AI on labour markets and underscore the increasing relevance of AI in information processing and higher-skilled occupations. It is important to note that LLMs do not constitute the entire field of AI. AI is a complicated and nuanced field with many possible subcategorisations. At a high level, one might consider the subfield of machine learning to include deep neural networks (also called deep learning), supervised and unsupervised learning as well as many other techniques. The subfield of natural language processing (NLP) focuses on content extraction, classification, machine translation, question answering and text generation, with LLMs being the most modern and successful NLP techniques. Many other subfields exist, including expert systems, machine vision and machine speech systems, planning, and of course robotics. One important reason why it is so hard to categorise AI approaches is the rapidity with which they are combining with each other. McKinsey & Company has suggested that AI adoption within corporations should arguably be ‚Äúdomain focused‚Äù labour [58]. That is, a software development organisation should consider the software development domain and look for opportunities to use AI to assist software developers to perform their jobs more effectively. Importantly to software development organisations, McKinsey also found, ‚ÄúAbout 75 percent of the value that generative AI use cases could deliver falls across four areas: Customer operations, marketing and sales, software engineering, and R&D‚Äù [33]. In other words, business operations and software development activities make up the majority of LLM impacts on the labour market, which will naturally affect software development organisations disproportionally. It seems that companies related to science, technology, engineering and mathematics (STEM) that embrace the use of AI are likely to outperform those that do not [45]. The nature of work in the information economy is also anticipated to change drastically. ‚ÄúCurrent generative AI and other technologies have the potential to automate work activities that absorb 60 to 70 percent of employees‚Äô time today‚Äù [23]. Thus, we can reasonably expect that jobs in the information economy will look very different in coming decades to what they do today. For software development companies this means a transformation in how software is created, tested, and maintained, with AI taking over routine coding tasks, enhancing productivity, and enabling developers to focus on more complex problem-solving and innovative work. 1.1 The Impact of AI on the Economy How will AI impact the economy and on what timescale? Although we cannot provide complete answers, we can provide some insights and intuitions. Before LLMs came onto the market, the Organisation for Economic Co-operation and Development (OECD) discussed the risks of automation in the context of robotics and the automation of manual tasks [5]. They concluded that 9% of jobs across 21 OECD countries were at risk of automation and emphasised that low-skilled workers were more vulnerable. That is, higher education levels were generally seen as protective against automation. The authors suggested that technological advancements would lead to changes in task structures rather than wholesale job losses, with a need for workforce retraining to mitigate the effects of automation on low-skilled jobs [5]. However, the release of advanced LLMs into the market introduced changes due to their natural language capabilities. By 2023, McKinsey Global Institute noted that the scope of AI extended beyond manual tasks to include complex information processing and creative tasks traditionally performed by more highly-educated workers [33]. The report estimated that nearly half of all work activities could be automated by current technologies, affecting jobs across various sectors. Subsequent academic studies have estimated similarly, with [23] finding 49% of jobs are likely to be impacted. McKinsey [33] estimated, ‚ÄúGenerative AI could increase labor productivity by 0.1 percent to 0.6 percent annually over the next 10 to 20 years. When combined with other technologies, automation overall could contribute 0.5 to 3.4 percent annually to productivity growth, assuming labor is redeployed at today‚Äôs productivity levels and not including general equilibrium effects.‚Äù The impact on productivity would therefore seem to be a key driver of differentiation between those companies that embrace the technology and those who choose not to. Prudent application of these percentages would provide a way to estimate the appropriate level of spending on AI tooling: in order to gain a net value from AI tooling, an organisation needs to spend less on tooling than the productivity gains realised. However, these comparisons are inadequate when AI is further leveraged within an organisation to provide less quantifiable advantages, such as improved customer service, enhanced and novel product development utilising the syngergies of AI and Web3. Cautions are beginning to emerge from the research community on those promising productivity increases. A recent study by Cui et al noted that Microsoft Copilot was well adopted by both senior and junior developers, but the senior developers showed substantially less benefit from using the product [19]. The same phenomenon occurred when evaluating seniority by tenure at Microsoft. Worryingly, a 2024 study by Uplevel Labs noted, ‚ÄúDevelopers with Copilot access saw a significantly higher bug rate while their issue throughput remained consistent.‚Äù [66]. These concerns clearly indicate that the relationship of LLMs to productivity is not as clear as initially thought. To understand the possible time scales for mainstream economic adoption of AI we need to briefly discuss the nature of innovation, how it works and how it unfolds. The biologist Stuart Kauffman developed a ‚Äútheory of organization‚Äù to explain biological evolution that he called the ‚Äúadjacent possible‚Äù [37]. An organism builds itself and interacts with its environment but is limited to what it finds in that environment. That is, what is possible is limited by what is adjacent to an organism. Further, each new development adds to the possibilities present in the environment. W. Brian Arthur later noted that the same phenomenon occurs in technical innovation [6]. This is important to understand because our current observations of LLM orchestration with existing systems is very much an active exploration of the adjacent possible. Steven Johnson, looking into the nature of innovation, defined the 10/10 rule as ‚ÄúA decade to build the new platform, and a decade for it to find a mass audience.‚Äù [35] He noted that many technologies have followed this pattern, from hardware to software and including the Web. In contrast, he noted that some technologies skip that process and seem to arrive fully formed in very little time. His work explores the differences between those technologies that follow the 10/10 rule and those that manage to proceed faster. The question naturally arises, ‚ÄúAre LLMs a fast or slow technical process?‚Äù One might consider that the original attention mechanism paper was first published in preprint in 2014 [7]111First publishing in 2014 and last updated in 2016. Google‚Äôs ‚ÄúAttention is All You Need‚Äù paper followed three years later [67]222First publishing in 2017 and last updated in 2023. Here we are in 2024, fully 10 years later. What may seem like a rapid development has been in train for the last decade. LLMs seem, therefore, to be following the 10/10 rule and not the 1/1 step function that has often been inferred in breathless media reporting. We most likely will spend the better part of the next decade absorbing and integrating the capabilities brought by LLMs. 1.2 Unintended Consequences There will almost certainly be unintended consequences as AI feeds back on human use of the technologies. For example, human skills in new generations of users may encounter developmental limitations for problems where LLMs and other AI forms cannot help. Examples might include new feature development where the desired features have no representation in training sets and/or is not extractable from training. Such eventualities seem historically precedented and might slow development for certain categories of features and retain the reliance on a few highly-capable individuals who can creatively develop features impossible with AI. National governments other than those developing new AI systems are also wary of reliance on technology owned, controlled and accessed from countries such as the United States or China. We have seen similar concerns raised regarding reliance on, for example, social media, messaging systems, navigation systems, cloud computing and Internet governance. Smaller countries have become reliant at all levels from households to governments on foreign technical infrastructure. For example, the Reserve Bank of Australia recently noted that ‚Äúfinancial institutions‚Äô dependence on ‚Äòa small number of AI‚Äô could ‚Äòcreate vulnerabilities due to a single point of failure‚Äù ‚Äô [60]. 1.3 Structure of this Paper We begin this paper by providing a comprehensive literature review covering the intersection between Web3 and AI technologies in section 2, Literature Review. We focus especially on how these technologies could complement each other. We discuss the landscape of the intersection of Web3 and AI technologies (Web3xAI) and identify the key thought leaders. We follow by identifying the key interlinking technologies, and cover various proposals for the application of AI to Web3. We then categorise the various proposals for Web3xAI integration: ‚Ä¢ Sustainability ‚Ä¢ Non-LLM-based AI with Web3 utility ‚Ä¢ Ownership and copyright applications ‚Ä¢ Philosophical and ethical implications ‚Ä¢ Future prospects and research directions identified by others In section 3, Research Gap Analysis, we identify new potential research directions not found in the literature review. In section 4, Risks and Mitigations we identify a series of common risks when working with AI in general and LLMs in particular. In section 5, Research Directions, we suggest research directions based on our gap analysis and lessons learned so far. Finally, we conclude by summarising the key points discovered to date."
https://arxiv.org/html/2411.04271v1,OpenFLAME: Building a large scale federated localization and mapping servicehttps://openflam.github.io,"The widespread availability of maps has enabled the development of numerous location-based applications, including navigation, ride-sharing, fitness tracking, gaming, robotics, and augmented reality. Today, the maps that power these services are predominantly controlled by a few large corporations and mostly cover outdoor spaces. As the use of these applications expands and indoor localization technologies advance, we are seeing the need for a scalable, federated location management system that can extend into private spaces.We introduce OpenFLAME111FLAME is short for Federated Localization and Mapping Engine., the first federated and decentralized localization service. OpenFLAME links servers that handle localization for specific regions, providing applications with a seamless global view. Creating a federated localization system poses challenges, such as discovering the appropriate servers for a region and integrating services managed by independent providers. To address these issues and ensure scalability, we leverage Domain Name System (DNS) for service discovery and implement map abstractions to retrieve and merge locations across different maps. Our trace-driven study demonstrates that federated localization across remote servers is feasible with acceptable query latencies. To highlight the potential of the system, we developed an augmented reality navigation application for a large indoor space, showing that OpenFLAME can successfully power location-based applications.","The proliferation of digital maps has facilitated the emergence of a wide range of location-based applications, including, but not limited to, navigation, ride-sharing, gaming, first-responder support and robotics. At present, the mapping infrastructure that supports these services is hosted by a limited number of corporations (e.g., Google and Apple) and is primarily focused on outdoor environments driven by wide-area positioning systems like GPS. However, we are now seeing accurate localization moving indoors with the maturation of approaches such as machine learning-based image matching and Time-of-Flight ranging (e.g., compact LiDAR and UWB), both of which are becoming standard on smartphones [37, 36, 61, 56]. This shift indoors opens the potential for new capabilities like persistent Augmented Reality (AR) content, while also naturally increasing the coverage of existing location-based services. Unfortunately, this shift indoors brings new challenges in terms of scale and privacy that completely disrupt our traditional centrally managed mapping infrastructure. The storage and cartography effort required for indoor spaces far outweighs that of outdoor maps which tend to be relatively sparse in terms of landmarks and features. Outdoor maps are generated using satellite and aerial photographs that are refined by ground-based vehicles equipped with GPS. Some estimate that there could be more than 100 billion square feet (and growing) of indoor space in the world [10]. Surveying this space will require a huge effort and it would be extremely difficult to align landmarks in a global reference frame, such as the latitude and longitude of the GPS. As technology evolves, we will see a mixture of maps created at different quality levels and resolutions. Most importantly, there is an increased sensitivity in terms of privacy when it comes to indoor spaces. Many organizations, such as stores, would benefit from accurate localization for applications such as product search [8, 39], but would not be willing to publicly host detailed maps. This becomes even more of an issue in more private homes or office spaces. Indoor maps contain sensitive information that needs to be owned and controlled by the owner of the physical space. We have already begun to see existing commercial solutions, such as Google‚Äôs Geospatial API [24], Niantic‚Äôs Lightship VPS [44], and Apple‚Äôs GeoTracking [6], attempt to scale image-based localization to a global level with significant drawbacks. These solutions require collecting location imagery of the environments beforehand by company affiliates (e.g., Google Street View cars [26], Niantic surveyors, and developers [44]). A 3D map of scanned locations is constructed using the collected data. These maps are then used to localize devices based on their query images. An application built on one of these services is tied to the availability of their particular infrastructure. For example, it is not possible for an AR application built on Niantic Lightship VPS to extend its user base to a city not supported by Niantic, even if it is supported by Google‚Äôs Geospatial API. These applications are also limited to the technology used by the service provider. For example, they cannot take advantage of the rapid improvement in localization techniques that is happening in academia until the service provider implements them. In this paper, we present OpenFLAME, the first design of a federated localization and mapping service. OpenFLAME is organized into ‚Äòmap servers‚Äô ‚Äì independent localization services deployed by potentially disparate parties. OpenFLAME provides the means to discover and tie these services together so that applications get a unified view of the world. ‚ÄòMaps‚Äô in OpenFLAME capture both true-to-scale spatial data that can be used for localization (e.g. dense point clouds used for visual positioning systems) and the more traditional symbolic relationships between landmarks (¬ß 5). Spatial data are used for positioning, while symbolic data can be used as a context for applications. Building a federated localization service comes with several challenges. Unlike centralized services where the application knows which localization service to contact, OpenFLAME must discover map servers that provide a localization service for the region where the device is located. Rather than relying on a central spatial database, we repurpose the Domain Name System (DNS) to discover map servers. The device‚Äôs coarse location information is converted to a list of DNS-compatible domain names called geo-domains. We query the DNS with these geo-domains to get a list of map servers. Our system uses the existing DNS infrastructure without changes in implementation. As a result, OpenFLAME can be easily adopted. Organizing the localization service into independent maps introduces the problem of consistency between maps. A straightforward technique to ensure consistency is to enforce all maps to conform to the global coordinate system of latitudes and longitudes. However, aligning an indoor map accurately with the geographic coordinate system is a notoriously difficult problem [19, 45]. Aligning indoor maps with centimeter-level accuracy, where commodity GPS receivers have an error of 10-20 m, needs expensive survey equipment such as RTK GNSS [59] and total stations [60]. Therefore, in OpenFLAME we do not enforce such constraints. All maps operate in their own local coordinate systems. We use a simple map abstraction of ‚Äòwaypoints‚Äô to implicitly align overlapping maps. This abstraction enables applications built on top of OpenFLAME to consistently transition between map boundaries. In summary, our contributions are as follows. ‚Ä¢ We present the first design of a federated and scalable mapping and localization service (¬ß 3). ‚Ä¢ We show how a spherical indexing system (S2 [28]) can be used to repurpose the DNS into a spatial database. We use it to discover map servers hosted in a region (¬ß 4). ‚Ä¢ We introduce a simple map abstraction that removes the need for maps to be aligned with each other in a geographic coordinate system (¬ß 5) while still maintaining consistency across map boundaries. To show that OpenFLAME can power future location-based applications, we built an Augmented Reality (AR) indoor application on top of it. Our application guides users to their destination by rendering arrows on a smart phone‚Äôs camera feed. We evaluate the core metrics of our service in ¬ß 8. 222We demo some tools built for OpenFLAME on https://openflam.github.io. We highly encourage readers to try them out."
https://arxiv.org/html/2411.04159v1,Cooperation and Personalization on a Seesaw: Choice-based FL for Safe Cooperation in Wireless Networks,"Federated learning (FL) is an innovative distributed artificial intelligence (AI) technique. It has been used for interdisciplinary studies in different fields such as healthcare, marketing and finance. However the application of FL in wireless networks is still in its infancy. In this work, we first overview benefits and concerns when applying FL to wireless networks. Next, we provide a new perspective on existing personalized FL frameworks by analyzing the relationship between cooperation and personalization in these frameworks. Additionally, we discuss the possibility of tuning the cooperation level with a choice-based approach. Our choice-based FL approach is a flexible and safe FL framework that allows participants to lower the level of cooperation when they feel unsafe or unable to benefit from the cooperation. In this way, the choice-based FL framework aims to address the safety and fairness concerns in FL and protect participants from malicious attacks.","The growing scale and heterogeneity of wireless networks have boosted the applications of data-driven artificial intelligence (AI) algorithms in wireless networks. Federated learning (FL) is an innovative distributed AI technique that allows participants to train models cooperatively without sharing data. It has been rapidly promoted in recent years by the research community with the benefits of privacy preservation and the utilization of on-device intelligence [zhang2023device]. However, the practical application of FL in wireless networks is still in its infancy. It has not yet gained widespread acceptance in the industry, mainly due to the security concerns and inherent problems of the cooperative mechanism. The distributed and open system architecture introduces new threats and makes FL models more vulnerable to poisoning attacks compared with centralized machine learning models [zhang2023distributed]. Several existing studies explore how to defend against vulnerabilities and enhance the security of FL models. Some commonly used defense schemes include anomaly detection, trusted execution authentication, and robust model aggregation. Nevertheless, these state-of-the-art defense techniques have adopted a sporadic approach, incorporating add-on techniques into the FL framework in a fix-and-patch manner. In addition, the attackers are becoming intelligent and can learn to adapt to existing defense schemes [li2022learning]. This makes defenses more difficult, especially without knowing the type and principle of attacks. Therefore, there is still great potential to improve and promote the security level of FL in wireless communications. Figure 1: The examples of tunable cooperation in FL and their applications to FL-based wireless networks. The left part shows the applications and vulnerabilities of FL in wireless networks. The right part shows examples of tunable cooperation in existing personalized FL techniques. By changing the parameters related to the cooperation degree, given personalized FL techniques will convert between fully cooperative FL and fully personalized independent learning. Personalized FL schemes have been proposed by academia as effective solutions to the problem of local data and problem-setting heterogeneity in FL. A thorough survey is presented in [tan2022towards] on recent advances in personalized FL and gives a taxonomy of personalized FL techniques in terms of personalization strategies. However, most existing studies focus on describing detailed implementations of different personalized FL approaches, with limited exploration of the commonalities in these designs. After an exhaustive investigation of the personalized FL, we propose a new understanding of the essence of existing personalized FL frameworks. To the best of our knowledge, this is the first time that the tunable cooperation concept in personalized FL is proposed and discussed. In our view, the traditional FL schemes create a fixed cooperative paradigm for multiple independent participants. Participants must be fully committed to the cooperation and share the same cooperation output. On this basis, personalized FL methods create more flexible cooperative paradigms by introducing parameters that regulate the closeness degree of cooperation between participants. Therefore, we conclude that cooperation and personalization in FL are two ends of a seesaw. By controlling the parameters representing the degree of personalization and cooperation, it is possible to control which side of the seesaw is higher according to the problem and environment settings, thus realizing tunable cooperation in FL. Motivated by the above ideas, we introduce a flexible choice-based FL framework that dynamically adjusts the degree of cooperation and personalization of participants during the use of FL. It is also a more equitable and safer framework that allows participants to not cooperate when they feel unsafe or unable to benefit from cooperation. Note that, our choice-based FL can not only be used to solve the heterogeneity problem of traditional FL but also can be applied to noise and security concerns. When the system is under attack, participants will be more inclined to personalized training to avoid interference from attacks and noise. Through these structural changes, FL is given a higher degree of flexibility and can be more widely used in wireless networks. The contributions of this work are three-fold: First, we present an up-to-date survey of FL in wireless networks and illustrate the benefits and concerns of FL. Second, we provide a new perspective on the commonalities of existing personalized FL algorithms and propose the concept of tunable cooperation. Based on this concept, we introduce a flexible choice-based FL framework that solves major concerns of FL, including heterogeneity, fairness, and poisoning attacks. Third, we provide a case study of implementing a choice-based FL framework with clustering and knowledge distillation scheme under a cell sleep control scenario. By testing the framework with heterogeneous data and intelligent attacks, we prove the superiority of choice-based FL in terms of robustness and security. The remainder of this work is organized as follows. Section II explains the benefits and vulnerabilities of FL in wireless networks. Section III presents the tunable cooperation concept in personalized FL. Section IV introduces the choice-based FL framework and shows how it solves the vulnerabilities of FL. Section V demonstrates our case study and Section VI concludes the paper."
https://arxiv.org/html/2411.03999v1,ParaGAN: A Scalable Distributed Training Framework for Generative Adversarial Networks,"Recent advances in Generative Artificial Intelligence have fueled numerous applications, particularly those involving Generative Adversarial Networks (GANs), which are essential for synthesizing realistic photos and videos. However, efficiently training GANs remains a critical challenge due to their computationally intensive and numerically unstable nature. Existing methods often require days or even weeks for training, posing significant resource and time constraints.In this work, we introduce ParaGAN, a scalable distributed GAN training framework that leverages asynchronous training and an asymmetric optimization policy to accelerate GAN training. ParaGAN employs a congestion-aware data pipeline and hardware-aware layout transformation to enhance accelerator utilization, resulting in over 30% improvements in throughput. With ParaGAN, we reduce the training time of BigGAN from 15 days to 14 hours while achieving 91% scaling efficiency. Additionally, ParaGAN enables unprecedented high-resolution image generation using BigGAN.","The last decade has witnessed the success of Generative Adversarial Networks (Goodfellow et al., 2014), which has a wide range of applications including image super-resolution (Ledig et al., 2017), image translation (Isola et al., 2017; Zhu et al., 2017), photo inpainting (Yu et al., 2018; Demir and Unal, 2018). However, training GAN at scale remains challenging because of the computational demands and optimization difficulties. Unlike other conventional neural networks where optimization is straightforward by taking gradient descents, there are two sub-networks to optimize in GAN, namely generator and discriminator. The generator samples from the noise and produces a fake sample as close to the real sample as possible, and the discriminator evaluates the generated sample. The generator aims to fool the discriminator, and the discriminator will try to identify the fake images from the real ones. Since the two components are optimized for two contradicting goals, it has been observed that GANs are difficult to converge. Therefore, to speed up the GAN training at large scale, we need a framework optimized on both system and numerical perspectives. Table 1. A summary of reported training time and model size for GANs trained on ImageNet dataset. GANs Hardware Time # Params. SNGAN (Miyato et al., 2018) 8 V100 GPUs 3d 13.6h 81.44M ProgressiveGAN (Karras et al., 2017) 8 V100 GPUs 4d 43.2M ContraGAN (Kang and Park, 2020) 8 V100 GPUs 5d 3.5h 160.78M SAGAN (Zhang et al., 2019a) 8 V100 GPUs 10d 18.7h 81.47M BigGAN (Brock et al., 2019) 8 V100 GPUs 15 d 158.42M Figure 1. ParaGAN scales to 1024 TPU accelerators at 91% scaling efficiency. Due to the difficulty of optimizing GAN, many state-of-the-art GAN models take days or even weeks to train. For instance, BigGAN (Brock et al., 2019) took 15 days on 8√ó8\times8 √ó V100 GPUs to converge. Table 1 summarizes the reported training time of some of the state-of-the-art GAN models. This makes it difficult to quickly reproduce, evaluate, and iterate GAN experiments. Also, current GAN frameworks usually only support training on a small number of nodes (Kang and Park, 2020; Contributors, 2021; Lee and Town, 2020). We argue that training speed is an important yet often ignored factor in the current GAN training landscape, and we propose to accelerate it with distributed training. However, distributed GAN training has several challenges. First of all, most data centers have storage nodes and compute nodes separated for elasticity, but network congestion can happen from time to time, which prolongs the latency between nodes and affects training throughput. Secondly, there are usually different types of accelerators in the data center, but each of them has its architectural design and preferred data layout. If ignored, it could lead to under-utilization of accelerators. Last but not least, training GAN at scale may cause a convergence problem, in which the GAN loss does not converge to a stable equilibrium. Therefore, this framework has to consider both system and numerical perspectives. In this work, we present ParaGAN, the first distributed training framework that supports large-scale distributed training for high-resolution GAN. We identify the performance bottlenecks when training at scale and optimize them for efficiency. ParaGAN has a simple interface for building new GAN architecture, and it supports CPU, GPU, and TPU. The main contributions of ParaGAN include: ‚Ä¢ We design and implement the first scalable distributed training framework for GAN with optimizations on both system and numerical perspectives. With ParaGAN, the training time of BigGAN can be shortened from 15 days to 14 hours with 1024 TPU accelerators at 91% scaling efficiency, as shown in Fig. 1. ParaGAN also enables direct photo-realistic image generation at unprecedented 1024√ó1024102410241024\times 10241024 √ó 1024 resolution, which is 4√ó4\times4 √ó higher than the original BigGAN model. ‚Ä¢ From the system optimization perspective, we use congestion-aware data pipeline and hardware-aware layout transformation to improve the accelerator utilization, and low-precision training to alleviate the memory stress. They contribute to 30-40% throughput improvements over baseline. ‚Ä¢ From the numerical optimization perspective, we show that the generator and discriminator can be optimized independently, and present an asynchronous update scheme together with an asymmetric optimization policy. The paper is organized in the following manner: we discuss the motivation and requirement for large-scale GAN training in Section 2; in Section 3, we will explain our design for ParaGAN, and how those architectural considerations can address the requirements; in Section 4 and Section 5, we will cover the system-level and numerical-level optimizations for scalable training in ParaGAN respectively; in Section 6, we present our scalability evaluation of ParaGAN and study the effect of different optimization techniques; a brief review of related work on GAN and large-scale distributed training is presented in Section 7; we conclude this study of GAN in Section 8."
https://arxiv.org/html/2411.03902v1,Almost Time-Optimal Loosely-Stabilizing Leader Election on Arbitrary Graphs Without Identifiers in Population Protocols,"The population protocol model is a computational model for passive mobile agents. We address the leader election problem, which determines a unique leader on arbitrary communication graphs starting from any configuration. Unfortunately, self-stabilizing leader election is impossible to be solved without knowing the exact number of agents; thus, we consider loosely-stabilizing leader election, which converges to safe configurations in a relatively short time, and holds the specification (maintains a unique leader) for a relatively long time. When agents have unique identifiers, Sudo et al.(2019) proposed a protocol that, given an upper bound NùëÅNitalic_N for the number of agents nùëõnitalic_n, converges in O‚Å¢(m‚Å¢N‚Å¢log‚Å°n)ùëÇùëöùëÅùëõO(mN\log n)italic_O ( italic_m italic_N roman_log italic_n ) expected steps, where mùëömitalic_m is the number of edges. When unique identifiers are not required, they also proposed a protocol that, using random numbers and given NùëÅNitalic_N, converges in O‚Å¢(m‚Å¢N2‚Å¢log‚Å°N)ùëÇùëösuperscriptùëÅ2ùëÅO(mN^{2}\log{N})italic_O ( italic_m italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_N ) expected steps. Both protocols have a holding time of Œ©‚Å¢(e2‚Å¢N)Œ©superscriptùëí2ùëÅ\Omega(e^{2N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) expected steps and use O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) bits of memory. They also showed that the lower bound of the convergence time is Œ©‚Å¢(m‚Å¢N)Œ©ùëöùëÅ\Omega(mN)roman_Œ© ( italic_m italic_N ) expected steps for protocols with a holding time of Œ©‚Å¢(eN)Œ©superscriptùëíùëÅ\Omega(e^{N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) expected steps given NùëÅNitalic_N.In this paper, we propose protocols that do not require unique identifiers. These protocols achieve convergence times close to the lower bound with increasing memory usage. Specifically, given NùëÅNitalic_N and an upper bound ŒîŒî\Deltaroman_Œî for the maximum degree, we propose two protocols whose convergence times are O‚Å¢(m‚Å¢N‚Å¢log‚Å°n)ùëÇùëöùëÅùëõO(mN\log n)italic_O ( italic_m italic_N roman_log italic_n ) and O‚Å¢(m‚Å¢N‚Å¢log‚Å°N)ùëÇùëöùëÅùëÅO(mN\log N)italic_O ( italic_m italic_N roman_log italic_N ) both in expectation and with high probability. The former protocol uses random numbers, while the latter does not require them. Both protocols utilize O‚Å¢(Œî‚Å¢log‚Å°N)ùëÇŒîùëÅO(\Delta\log N)italic_O ( roman_Œî roman_log italic_N ) bits of memory and hold the specification for Œ©‚Å¢(e2‚Å¢N)Œ©superscriptùëí2ùëÅ\Omega(e^{2N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) expected steps.","The population protocol model, introduced by Angluin et al.[3], is a computational model widely recognized in distributed computing and applicable to passive mobile sensor networks, chemical reaction systems, and molecular calculations, etc. This model comprises nùëõnitalic_n finite state machines (called agents), which form a network (called a population). Agents‚Äô states are updated through communication (called interaction) among a pair of agents. A simple connected digraph G=(V,E)ùê∫ùëâùê∏G=(V,E)italic_G = ( italic_V , italic_E ) (called a communication graph) determines the possibility of interaction among the agents. In this model, only one pair of agents interacts at each step. The interactions are determined by a uniform random scheduler. The leader election problem is one of the most studied problems in population protocols. This problem involves agents electing a unique leader agent from the population and maintaining this unique leader forever. Angluin et al.[3] first studied this problem for complete graphs with designated common initial state. Under this assumption, many studies have been conducted [3, 6, 11, 17], and a time and space optimal protocol [6] has already been proposed. Several studies also exist for arbitrary graphs [1, 2], and a time-optimal protocol [2] has already been proposed. The self-stabilizing leader election problem requires that agents start from any configuration, elect a unique leader agent, and maintain this unique leader forever. It is known that there is no self-stabilizing leader election protocol for arbitrary graphs [4] and complete graphs [8]. In the context of complete or arbitrary graphs, researchers have explored the problem in three ways. The first approach involves assuming that all agents initially know the exact number nùëõnitalic_n of agents [7, 8, 22]. The second approach introduces an oracle that informs agents about the existence of leaders [5, 9, 10]. The third approach relaxes the requirement of maintaining a unique leader forever, introducing a loosely-stabilizing leader election problem, where agents start from any configuration, elect a unique leader within a short time, and maintain this leader for a long time. Sudo et al.[16] first addressed this problem on complete graphs. Subsequent studies have continued to explore this problem on complete graphs [12, 15, 21], and on arbitrary graphs [18, 19, 20] as follows and summarized in Table 1. Sudo, Ooshita, Kakugawa, and Masuzawa [18] first addressed this problem for arbitrary graphs, and it is significantly improved by Sudo, Ooshita, Kakugawa, Masuzawa, Datta, and Lawrence [20] introducing a novel concept of Same Speed Timer. They proposed two protocols. The first protocol, ùí´ID2subscriptùí´ID2{\mathcal{P}_{\mathrm{ID2}}}caligraphic_P start_POSTSUBSCRIPT ID2 end_POSTSUBSCRIPT, assumes that agents have unique identifiers and are given NùëÅNitalic_N as initial knowledge. ùí´ID2subscriptùí´ID2{\mathcal{P}_{\mathrm{ID2}}}caligraphic_P start_POSTSUBSCRIPT ID2 end_POSTSUBSCRIPT converges within O‚Å¢(m‚Å¢N‚Å¢log‚Å°n)ùëÇùëöùëÅùëõO(mN\log{n})italic_O ( italic_m italic_N roman_log italic_n ) expected steps and holds the unique leader with Œ©‚Å¢(N‚Å¢e2‚Å¢N)Œ©ùëÅsuperscriptùëí2ùëÅ\Omega(Ne^{2N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) expected steps, using O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) bits of memory. The second protocol, ùí´RD2subscriptùí´RD2{\mathcal{P}_{\mathrm{RD2}}}caligraphic_P start_POSTSUBSCRIPT RD2 end_POSTSUBSCRIPT, assumes that agents can make randomized transitions and is given NùëÅNitalic_N as initial knowledge. ùí´RD2subscriptùí´RD2{\mathcal{P}_{\mathrm{RD2}}}caligraphic_P start_POSTSUBSCRIPT RD2 end_POSTSUBSCRIPT converges within O‚Å¢(m‚Å¢N2‚Å¢log‚Å°N)ùëÇùëösuperscriptùëÅ2ùëÅO(mN^{2}\log{N})italic_O ( italic_m italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_N ) expected steps and holds the unique leader with Œ©‚Å¢(N‚Å¢e2‚Å¢N)Œ©ùëÅsuperscriptùëí2ùëÅ\Omega(Ne^{2N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) expected steps using O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) bits of memory. Sudo et al.also demonstrated that the lower bound of the convergence time is Œ©‚Å¢(m‚Å¢N)Œ©ùëöùëÅ\Omega(mN)roman_Œ© ( italic_m italic_N ) steps for any loosely-stabilizing protocols with holding a unique leader Œ©‚Å¢(eN)Œ©superscriptùëíùëÅ\Omega(e^{N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) expected steps. Loosely-stabilizing leader election protocols without requiring unique identifiers or random numbers were proposed [19] and then improved [22]. The protocol, ùí´ARsubscriptùí´AR\mathcal{P}_{\mathrm{AR}}caligraphic_P start_POSTSUBSCRIPT roman_AR end_POSTSUBSCRIPT, given NùëÅNitalic_N and ŒîŒî\Deltaroman_Œî as initial knowledge, converges within O‚Å¢(m‚Å¢n‚Å¢D‚Å¢log‚Å°n+m‚Å¢N‚Å¢Œî2‚Å¢log‚Å°N)ùëÇùëöùëõùê∑ùëõùëöùëÅsuperscriptŒî2ùëÅO(mnD\log{n}+mN\Delta^{2}\log{N})italic_O ( italic_m italic_n italic_D roman_log italic_n + italic_m italic_N roman_Œî start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_N ) expected steps and holds with Œ©‚Å¢(N‚Å¢eN)Œ©ùëÅsuperscriptùëíùëÅ\Omega(Ne^{N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) expected steps using O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) bits of memory [22]. Table 1: Convergence and Holding Times for Loosely-Stabilizing Leader Election Protocols with Exponential Holding Times. nùëõnitalic_n denotes the number of agents, NùëÅNitalic_N denotes the upper bound of nùëõnitalic_n, mùëömitalic_m denotes the number of edges of the communication graph, Dùê∑Ditalic_D denotes the diameter of the communication graph, and ŒîŒî\Deltaroman_Œî denotes the upper bound of the maximum degree of the communication graph. All protocols are given NùëÅNitalic_N as initial knowledge. Protocols with ‚àó‚àó\ast‚àó are also given ŒîŒî\Deltaroman_Œî. The symbol ‚Ä†‚Ä†\dagger‚Ä† represents lower bounds of convergence time or memory usage for protocols with holding time of Œ©‚Å¢(eN)Œ©superscriptùëíùëÅ\Omega(e^{N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ). Graph Convergence Holding Memory Requisite [16] complete O‚Å¢(n‚Å¢N‚Å¢log‚Å°n)ùëÇùëõùëÅùëõO(nN\log{n})italic_O ( italic_n italic_N roman_log italic_n ) Œ©‚Å¢(N‚Å¢eN)Œ©ùëÅsuperscriptùëíùëÅ\Omega(Ne^{N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) - [12] complete O‚Å¢(n‚Å¢N)ùëÇùëõùëÅO(nN)italic_O ( italic_n italic_N ) Œ©‚Å¢(eN)Œ©superscriptùëíùëÅ\Omega(e^{N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) - [12]‚Ä†‚Ä†\dagger‚Ä† complete Œ©‚Å¢(n‚Å¢N)Œ©ùëõùëÅ\Omega(nN)roman_Œ© ( italic_n italic_N ) Œ©‚Å¢(eN)Œ©superscriptùëíùëÅ\Omega(e^{N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) - - [12]‚Ä†‚Ä†\dagger‚Ä† complete - Œ©‚Å¢(eN)Œ©superscriptùëíùëÅ\Omega(e^{N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) Œ©‚Å¢(log‚Å°N)Œ©ùëÅ\Omega(\log{N})roman_Œ© ( roman_log italic_N ) - [18]‚àó‚àó\ast‚àó arbitrary O‚Å¢(m‚Å¢Œî‚Å¢N‚Å¢log‚Å°n)ùëÇùëöŒîùëÅùëõO(m\Delta N\log{n})italic_O ( italic_m roman_Œî italic_N roman_log italic_n ) Œ©‚Å¢(N‚Å¢eN)Œ©ùëÅsuperscriptùëíùëÅ\Omega(Ne^{N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) agent identifiers [18]‚àó‚àó\ast‚àó arbitrary O‚Å¢(m‚Å¢Œî2‚Å¢N3‚Å¢log‚Å°N)ùëÇùëösuperscriptŒî2superscriptùëÅ3ùëÅO(m\Delta^{2}N^{3}\log{N})italic_O ( italic_m roman_Œî start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_N start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT roman_log italic_N ) Œ©‚Å¢(N‚Å¢eN)Œ©ùëÅsuperscriptùëíùëÅ\Omega(Ne^{N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) random numbers [20] arbitrary O‚Å¢(m‚Å¢N‚Å¢log‚Å°n)ùëÇùëöùëÅùëõO(mN\log{n})italic_O ( italic_m italic_N roman_log italic_n ) Œ©‚Å¢(N‚Å¢e2‚Å¢N)Œ©ùëÅsuperscriptùëí2ùëÅ\Omega(Ne^{2N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) agent identifiers [20] arbitrary O‚Å¢(m‚Å¢N2‚Å¢log‚Å°N)ùëÇùëösuperscriptùëÅ2ùëÅO(mN^{2}\log{N})italic_O ( italic_m italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_N ) Œ©‚Å¢(N‚Å¢e2‚Å¢N)Œ©ùëÅsuperscriptùëí2ùëÅ\Omega(Ne^{2N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) random numbers [20]‚Ä†‚Ä†\dagger‚Ä† arbitrary Œ©‚Å¢(m‚Å¢N)Œ©ùëöùëÅ\Omega(mN)roman_Œ© ( italic_m italic_N ) Œ©‚Å¢(eN)Œ©superscriptùëíùëÅ\Omega(e^{N})roman_Œ© ( italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) - - [19]‚àó‚àó\ast‚àó arbitrary O‚Å¢(m‚Å¢n‚Å¢D‚Å¢log‚Å°n+m‚Å¢N‚Å¢Œî2‚Å¢log‚Å°N)ùëÇùëöùëõùê∑ùëõùëöùëÅsuperscriptŒî2ùëÅO(mnD\log{n}+mN\Delta^{2}\log{N})italic_O ( italic_m italic_n italic_D roman_log italic_n + italic_m italic_N roman_Œî start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_N ) Œ©‚Å¢(N‚Å¢eN)Œ©ùëÅsuperscriptùëíùëÅ\Omega(Ne^{N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ) O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) - This‚àó‚àó\ast‚àó arbitrary O‚Å¢(m‚Å¢N‚Å¢log‚Å°n)ùëÇùëöùëÅùëõO(mN\log{n})italic_O ( italic_m italic_N roman_log italic_n ) Œ©‚Å¢(N‚Å¢e2‚Å¢N)Œ©ùëÅsuperscriptùëí2ùëÅ\Omega(Ne^{2N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) O‚Å¢(Œî‚Å¢log‚Å°N)ùëÇŒîùëÅO(\Delta\log{N})italic_O ( roman_Œî roman_log italic_N ) random numbers This‚àó‚àó\ast‚àó arbitrary O‚Å¢(m‚Å¢N‚Å¢log‚Å°N)ùëÇùëöùëÅùëÅO(mN\log{N})italic_O ( italic_m italic_N roman_log italic_N ) Œ©‚Å¢(N‚Å¢e2‚Å¢N)Œ©ùëÅsuperscriptùëí2ùëÅ\Omega(Ne^{2N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) O‚Å¢(Œî‚Å¢log‚Å°N)ùëÇŒîùëÅO(\Delta\log{N})italic_O ( roman_Œî roman_log italic_N ) - 1.1 Our Contribution In this paper, we propose a protocol ùí´BCsubscriptùí´BC{\mathcal{P}_{\mathrm{BC}}}caligraphic_P start_POSTSUBSCRIPT roman_BC end_POSTSUBSCRIPT whose convergence time is nearly optimal on anonymous (without unique identifiers) arbitrary graphs, as supported by the lower bound [20]. The lower bound for the complete graph [12] is Œ©‚Å¢(n‚Å¢N)Œ©ùëõùëÅ\Omega(nN)roman_Œ© ( italic_n italic_N ), but this is a special case, as the lower bound [20] is known to hold even when m=Œò‚Å¢(n2)ùëöŒòsuperscriptùëõ2m=\Theta(n^{2})italic_m = roman_Œò ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Given NùëÅNitalic_N and ŒîŒî\Deltaroman_Œî, ùí´BCsubscriptùí´BC{\mathcal{P}_{\mathrm{BC}}}caligraphic_P start_POSTSUBSCRIPT roman_BC end_POSTSUBSCRIPT converges within O‚Å¢(m‚Å¢N‚Å¢log‚Å°n)ùëÇùëöùëÅùëõO(mN\log n)italic_O ( italic_m italic_N roman_log italic_n ) steps if the transition is randomized, and O‚Å¢(m‚Å¢N‚Å¢log‚Å°N)ùëÇùëöùëÅùëÅO(mN\log N)italic_O ( italic_m italic_N roman_log italic_N ) steps if the transition is deterministic111 Transition is said to be deterministic if it does not require random numbers in the transition. Though the transition is deterministic, we allow the protocol to exploit the randomness with which initiator and responder roles are chosen. , both in expectations and with high probability. The protocol holds the unique leader with Œ©‚Å¢(N‚Å¢e2‚Å¢N)Œ©ùëÅsuperscriptùëí2ùëÅ\Omega(Ne^{2N})roman_Œ© ( italic_N italic_e start_POSTSUPERSCRIPT 2 italic_N end_POSTSUPERSCRIPT ) expected steps and utilizes O‚Å¢(Œî‚Å¢log‚Å°N)ùëÇŒîùëÅO(\Delta\log N)italic_O ( roman_Œî roman_log italic_N ) bits of memory. The proposed ùí´BCsubscriptùí´BC{\mathcal{P}_{\mathrm{BC}}}caligraphic_P start_POSTSUBSCRIPT roman_BC end_POSTSUBSCRIPT has better convergence time than SOTA self-stabilizing leader election protocol [22] which converges with O‚Å¢(m‚Å¢n2‚Å¢D‚Å¢log‚Å°n)ùëÇùëösuperscriptùëõ2ùê∑ùëõO(mn^{2}D\log{n})italic_O ( italic_m italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_D roman_log italic_n ) steps with requiring the knowledge of nùëõnitalic_n. To achieve the convergence time of ùí´BCsubscriptùí´BC{\mathcal{P}_{\mathrm{BC}}}caligraphic_P start_POSTSUBSCRIPT roman_BC end_POSTSUBSCRIPT, we utilize the Same Speed Timer proposed in ùí´RD2subscriptùí´RD2{\mathcal{P}_{\mathrm{RD2}}}caligraphic_P start_POSTSUBSCRIPT RD2 end_POSTSUBSCRIPT [20], which requires two-hop coloring. The self-stabilizing two-hop coloring protocol was first studied by Angluin et al.[4], and further explored by Sudo et al.[20] (see Table 2). In this paper, we propose two new self-stabilizing two-hop coloring protocols; ùí´LRUsubscriptùí´LRU{\mathcal{P}_{\mathrm{LRU}}}caligraphic_P start_POSTSUBSCRIPT roman_LRU end_POSTSUBSCRIPT with randomized transitions, and ùí´‚Ä≤LRUsubscriptsuperscriptùí´‚Ä≤LRU{\mathcal{P^{\prime}}_{\mathrm{LRU}}}caligraphic_P start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_LRU end_POSTSUBSCRIPT with deterministic transitions. Both protocols require NùëÅNitalic_N and ŒîŒî\Deltaroman_Œî as initial knowledge. ùí´LRUsubscriptùí´LRU{\mathcal{P}_{\mathrm{LRU}}}caligraphic_P start_POSTSUBSCRIPT roman_LRU end_POSTSUBSCRIPT converges within O‚Å¢(m‚Å¢n)ùëÇùëöùëõO(mn)italic_O ( italic_m italic_n ) steps, both in expectation and with high probability, and uses O‚Å¢(Œî‚Å¢log‚Å°N)ùëÇŒîùëÅO(\Delta\log{N})italic_O ( roman_Œî roman_log italic_N ) bits of memory. ùí´‚Ä≤LRUsubscriptsuperscriptùí´‚Ä≤LRU{\mathcal{P^{\prime}}_{\mathrm{LRU}}}caligraphic_P start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_LRU end_POSTSUBSCRIPT converges within O‚Å¢(m‚Å¢(n+Œî‚Å¢log‚Å°N))ùëÇùëöùëõŒîùëÅO(m(n+\Delta\log{N}))italic_O ( italic_m ( italic_n + roman_Œî roman_log italic_N ) ) steps, both in expectation and with high probability, and also uses O‚Å¢(Œî‚Å¢log‚Å°N)ùëÇŒîùëÅO(\Delta\log{N})italic_O ( roman_Œî roman_log italic_N ) bits of memory. In ùí´‚Ä≤LRUsubscriptsuperscriptùí´‚Ä≤LRU{\mathcal{P^{\prime}}_{\mathrm{LRU}}}caligraphic_P start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_LRU end_POSTSUBSCRIPT, agents generate random numbers independently from interactions among themselves. To ensure the independence among random numbers, we employ the self-stabilizing normal coloring protocol ùí´NCsubscriptùí´NC{\mathcal{P}_{\mathrm{NC}}}caligraphic_P start_POSTSUBSCRIPT roman_NC end_POSTSUBSCRIPT to assign superiority or inferiority between adjacent agents. When interacting, only the superior agent uses the interaction to generate random numbers. ùí´NCsubscriptùí´NC{\mathcal{P}_{\mathrm{NC}}}caligraphic_P start_POSTSUBSCRIPT roman_NC end_POSTSUBSCRIPT converges within O‚Å¢(m‚Å¢n‚Å¢log‚Å°n)ùëÇùëöùëõùëõO(mn\log{n})italic_O ( italic_m italic_n roman_log italic_n ) steps, both in expectation and with high probability, and utilizes O‚Å¢(log‚Å°N)ùëÇùëÅO(\log{N})italic_O ( roman_log italic_N ) bits of memory."
https://arxiv.org/html/2411.03892v1,Two Sides of the Same Coin: Large-scale Measurements of Builder and Rollup after EIP-4844,"Web3 is reshaping decentralized ecosystems through innovations like Ethereum. Recently, EIP-4844 is implemented in Ethereum to support its Layer-2 scaling solutions, which introduces a new 128 KB data structure called blob. This upgrade incorporates type-3 transactions with blobs to verify data availability and reduce gas costs for rollups, significantly affecting the strategies of both builders and rollups. In this paper, we present an in-depth study of emerging strategies in builder and rollup markets after EIP-4844, containing hundred million transactions. We find that the efficiency of builder and rollup strategies is interdependent, akin to two sides of the same coin‚Äîboth cannot be optimized simultaneously. That is, when builders operate efficiently, rollups tend to overpay in fees, conversely, when rollups optimize their costs, builders may incur losses in inefficient transaction selection. From the side of builders, our results show that 29.48% of these blocks have been constructed inefficiently, which does not produce sufficient profits for builders. Through our evaluation from the side of rollups, we find that over 72.53% of type-3 transactions pay unnecessary fees, leading to notable economic costs of rollups. Our work provides critical insights into optimizing block construction and transaction strategies, advancing the economic efficiency and data scalability of Web3 infrastructures, yet, much like balancing a seesaw, the efficiency of builders and rollups cannot be optimized concurrently.","Web3 leverages decentralized technologies such as blockchain to enable peer-to-peer interactions without intermediaries. A groundbreaking and transformative application of decentralized technology is Ethereum, an open and decentralized blockchain platform that facilitates the development and execution of smart contracts and decentralized applications (Gilbert, 2022; Wang et al., 2022b). In the Ethereum mainnet, the Proposer Builder Separation (PBS) mechanism was proposed to separate the role of the miner by introducing the builder and the proposer (Buterin, 2021a). The task of builders is to construct the block, where they sort the transactions to maximize the extractable value and submit the block to proposers (Daian et al., 2020; Heimbach and Wattenhofer, 2022). Each transaction within the block originates from two sources: the public mempool, which is accessible to everyone, and private channels that deliver transactions directly to specific builders (Qin et al., 2022). Ethereum Layer-2 refers to scaling solutions built on top of the Ethereum mainnet to enhance its performance and throughput, with rollups being one of the most prominent approaches that bundle multiple transactions into a single batch before submitting them to the mainnet. Ethereum has implemented EIP-4844 in the Dencun upgrade to improve network scalability and lower transaction costs (Ethereum.org, 2024a). This upgrade presents blob, a new data format of 128 KB designed to facilitate rollups on Ethereum‚Äôs Layer-2, alongside blob-carrying transactions known as type-3 transactions (Vitalik Buterin, 2023). By offloading data storage to blobs, the gas fee is significantly reduced for rollups. Type-3 transactions are much larger and therefore take more space in blocks (Park et al., 2024; Gomez, 2024b). Blocks with large size take longer to propagate through the network. On the one side, builders need to balance the block size with the profit they can obtain (Wahrst√§tter, 2024b; Gomez, 2024a). They also need to decide whether to choose type-3 transactions in their blocks. This results in different behaviors among builders in response to type-3 transactions. On the other side, rollups need to issue their blobs into type-3 transactions and wait for being included in blocks. Type-3 transactions always carry the different number of blobs and different fees, which indicate various strategies of rollups (Cui, 2024). Evidently, the strategies among rollups are also complicated. We evaluate the strategies of builders and rollups after EIP-4844. We find that the efficiency of their strategies is like the two sides of the same coin. When the builders strategy is efficient, rollups may need to pay more fees. Oppositely, when rollups adopt the efficient strategy and offer lower fees, builders may face block profits lower than potential profits. In the side of builders, we first examine the new strategy that builders employed after EIP-4844 (Section 4). Although the average total block size has been increasing to nearly 400 KB, the size of transactions from public mempool in the block has decreased from 150 KB before the upgrade to about 30 KB. Builders have taken to shrinking the size of the rest of the block to contain type-3 transactions, which prompts us to delve deeper into the efficiency of this strategy. We recognize that the builder strategy is not efficient enough after EIP-4844. In the side of rollups, they need to pay higher fees to ensure inclusion in efficient blocks. However, we find that most type-3 transactions issued by rollups pay too high fees (Section 5). Our primary contributions are: (1) We build a large-scale dataset to quantify builders and rollups market in Ethereum from after EIP-4844 to August, which includes 319,529,950 transactions with 1,336,822 type-3 transactions. To the best of our knowledge, we are the first to illuminate different strategies and efficiencies of builders and rollups in the network after EIP-4844. Our dataset provides the most complete labelling of builders and rollups so far. (2) We identify the strategies that different builders adopt after EIP-4844. By examining the size of transactions versus the profit paid to builders, we provide a model for measuring efficient blocks for builders. We find that 29.48% of the blocks containing type-3 transactions do not give the builder enough profits to compensate for the loss of excluding other transactions. This will assist builders in improving their strategies for constructing blocks for higher profit margin. (3) We explore the sending patterns of different rollups for blob and type-3 transactions and find that the behavior within the market is distinctive. We measure efficient type-3 transactions for rollups and observe that 72.53% of type-3 transactions in the market are inefficient. This work contributes to how rollups can optimize their pricing of type-3 transactions to achieve cost savings. (4) Furthermore, we evaluate the losses of other flawed strategies in the market and reveal significant losses of 186.92 ETH for rollups. We also characterize the impact of one misuse of blobs. It induces a delay of over 19 seconds in the type-3 transactions of rollups."
https://arxiv.org/html/2411.03533v1,Shared Memory-Aware Latency-Sensitive Message Aggregation for Fine-Grained Communication,"Message aggregation is often used with a goal to reduce communication cost in HPC applications. The difference in the order of overhead of sending a message and cost of per byte transferred motivates the need for message aggregation, for several irregular fine-grained messaging applications like graph algorithms and parallel discrete event simulation (PDES). While message aggregation is frequently utilized in ‚ÄúMPI-everywhere‚Äù model, to coalesce messages between processes mapped to cores, such aggregation across threads in a process, say in MPI+X models or Charm++ SMP (Shared Memory Parallelism) mode, is often avoided. Within-process coalescing is likely to require synchronization across threads and lead to performance issues from contention. However, as a result, SMP-unaware aggregation mechanisms may not fully utilize aggregation opportunities available to applications in SMP mode. Additionally, while the benefit of message aggregation is often analyzed in terms of reducing the overhead, specifically the per message cost, we also analyze different schemes that can aid in reducing the message latency, ie. the time from when a message is sent to the time when it is received. Message latency can affect several applications like PDES with speculative execution where reducing message latency could result in fewer rollbacks. To address these challenges, in our work, we demonstrate the effectiveness of shared memory-aware message aggregation schemes for a range of proxy applications with respect to messaging overhead and latency.","Message aggregation libraries aim to provide an interface for applications for consolidating messages to reduce communication cost associated with processing many messages. Previous schemes for aggregation like TRAM [1] in Charm++ and streaming library Active Pebbles [2] use topology-aware routing schemes but these are less beneficial for modern topologies like fat-trees. Modern machines have many tens of cores per physical node (also called ‚Äúhost‚Äù). Applications tend to use them in two modes: in one mode there is a separate process bound to each core that is used by the application; this is called MPI-everywhere for MPI applications, and non-SMP mode in Charm++ applications. The other mode uses multiple processes per node, but each process owns multiple cores, and facilitates shared memory parallelism (SMP) across cores within a process. For MPI applications, parallelism within a process and across cores is managed by one of the thread-parallel libraries such as OpenMP or Kokkos. Charm++ does not require a separate programming model for within-process communication, but utilizes the shared memory for optimizing inter-object communication and cooperation. It is this SMP mode that is the focus of our work. This SMP scenario amplifies the need for message aggregation further. For the MPI-everywhere mode, systems like YGM [3] have recently looked into implementing node-aware schemes for message aggregation. YGM‚Äôs mechanism for coalescing messages at node level uses a phase of node-local exchange which relies on underlying MPI implementation‚Äôs method of achieving cheaper within node communication (eg. xpmem or cma). But the context we aim at is where worker threads divide the application‚Äôs work among cores, but need to exchange many short messages with other workers, in a shared memory setting. The SMP mode, in Charm++ as well as MPI, presents several potential advantages to the applications. Worker threads can share and divide work more efficiently, for better load balance (using dynamic schedules in OPENMP or stealable tasks and object migration in Charm++. Large read-only data structures can be shared among workers without making multiple copies. Even dynamic data structures, such as software particle (or tree) caches in astronomy codes can save duplicate external (across-node) communications. These orthogonal benefits may make SMP mode attractive independent of message-aggregation considerations. The questions we explore in this paper are: Given that an application wants to operate in SMP mode, what options are there for organzining message aggregation and how do they compare with each other? Does the SMP mode aggregation by itself provides benefits over non-SMP aggregation that makes SMP mode attractive even for applications that don‚Äôt particularly require SMP mode? Finally, what metrics are important for such comparisons? The main metric that has motivated research on message aggregation has been the overhead. The cost associated with sending a message over network can be computed using the alpha-beta communication model as Œ±+N‚Å¢Œ≤ùõºùëÅùõΩ\alpha+N\betaitalic_Œ± + italic_N italic_Œ≤, where N is the number of bytes. Here Œ±ùõº\alphaitalic_Œ± is the latency per message and Œ≤ùõΩ\betaitalic_Œ≤ (inverse of bandwidth) is the cost per byte transferred. To illustrate the costs, we measure the time taken to send a message (roundtrip time/2) between two physical nodes on the Delta supercomputer using a ping-pong benchmark. As we can see in Figure 1, the time for small message sizes is dominated by latency cost (Œ±ùõº\alphaitalic_Œ±) and is in the order of microseconds. The cost of sending an additional byte of data (Œ≤ùõΩ\betaitalic_Œ≤), on the other hand, is about 0.1 nanosecond (indicating a bandwidth of about 12 GB/s). The primary motivating factor for message aggregation is this discrepancy in the order of Œ±ùõº\alphaitalic_Œ± and Œ≤ùõΩ\betaitalic_Œ≤ values. While Œ±ùõº\alphaitalic_Œ± is in microseconds, Œ≤ùõΩ\betaitalic_Œ≤ is less than a nanosecond per byte, motivating the need for fine-grained messaging applications to aggregate many small messages into fewer large messages. Figure 1: Time to send a message does not change for small byte count, suggesting time is dominated by latency Œ±ùõº\alphaitalic_Œ± of Œºùúá\muitalic_Œºseconds For clarity in usage of terms, we will use ‚Äúitems‚Äù to refer to short messages the application wishes to send, and reserve ‚Äúmessage‚Äù to refer to aggregated data that the aggregation libraries send via the underlying communication mechanism. The use-cases for message aggregation range from all-to-all communication in MPI, where every rank wishes to send a relatively small number of items to every other rank, to streaming scenarios encountered in graph algorithms or in the well-known random-access benchmark from the HPC Challenge suite, where each worker continually generates a stream of items to others. While in the former case, one can anticipate an end point where the application signifies the contribution from a worker has ended, in the latter case, although such an end point may exist at some time, one must design the library for continuous flow of information. The distinction between the two cases is only quantitative, but may require different aggregation techniques. In contrast to both these, consider a parallel discrete event simulation with an optimistic (or speculative) protocol: here, items are events with a payload and a time-stamp associated with them. Destination (logical processes or agents) that receive items in time-stamp order are efficient, but out-of-order delivery leads to cascading rollbacks with high overhead. Observing the behavior of many such applications, and by virtue of the co-design of our new aggregation library with a graph algorithm, we inferred that, aside from effects on communication cost using message aggregation, a key metric that is influenced by message aggregation is latency. Different applications categories are differently affected by latency. We define latency here as the time from when an item is generated by the application to when it is delivered to the application on another processor. While aggregation reduces the cost of sending items, it increases the latency associated with each item. As items are buffered, latency is higher for messages than without any aggregation. In extreme cases, for applications that are highly latency sensitive, message aggregation may have to be avoided. But in practice, the large Œ±ùõº\alphaitalic_Œ± cost of short messages means that we have to deal with the dual goals of reducing latency while simultaneously reducing the overhead. The paper presents multiple schemes for aggregation in the context of SMP applications (where cores on a node are organized into multiple processes with multiple cores assigned to each), and compares them on the dual metrics of overhead and latency, for multiple applications represented by benchmarks. The schemes vary based on the level at which the aggregation is done: core/worker, or process, on both sending and receiving side. We analyze the benefits of SMP-aware message aggregation with different schemes, for different types of applications. The contributions of this paper are as follows. ‚Ä¢ We analyze message aggregation which is SMP-aware, with one buffer per destination node (or process), or one buffer per source node or both. ‚Ä¢ We analyze the above schemes and how buffer sizes affect latency and overhead ‚Ä¢ We describe a series of features that are engendered by application-co-design, which make the our library (called TramLib ) versatile and responsive to the varying needs of the applications. We characterize latency and overhead of the different schemes considered using the following: We use two common short benchmarks, namely histogramming and index-gather, which allow isolated measurements of overhead and latency. We also demonstrate usefulness of our library on irregular applications including a graph applications and PDES, via benchmarks representative of those."
https://arxiv.org/html/2411.03376v1,An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services,"This paper presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud AI services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud AI services‚Äô trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.","Artificial Intelligence (AI) as a service is a rapidly expanding technology paradigm. The AI market is expected to grow as more companies adopt AI technology to remain competitive. Major cloud providers, including Amazon Web Services [1], Microsoft Azure [2], Google Cloud Platform [3], Alibaba Cloud [4], Oracle Cloud, IBM Cloud, and Salesforce Service Cloud, offer AI as a service. This enables customers to develop and deploy AI models using cloud-based platforms. Cloud AI services commonly achieve learning accuracy by using standard metrics such as precision, recall, and F1 score [1, 2, 3]. However, certain services such as Alibaba Cloud [4] may not explicitly provide these performance metrics. Additionally, a recent study [5] conducts a detailed investigation into the maintenance of AI services, focusing on computer vision. This research uncovers inconsistencies and evolution risks in AI services. The explainable AI (XAI) aims to develop models and methods that enable human users to comprehend, trust, and manage AI models [6]. A study [7] discusses the role of XAI in computer vision-based decisions. They emphasize that XAI can promote trust in AI computer vision systems through improved understanding and prediction. Another work [8] presents the XAI criterion that refines the functionality objectives for XAI methods. One criterion involves the analysis of feature influence and feature causality. XAI is increasingly adopted in applications that require transparency, fairness, and trustworthiness in decision-making, particularly in sensitive domains [9]. The limitation of the XAI practices is the existing XAI techniques developed are often tailored to specific types of models or cases [10], which makes the XAI practices less reusable and versatile to other applications. Meanwhile, numerous cloud AI services have been provided by cloud platforms to support general applications across domains [1, 2, 3, 4]. Together, the trend underscores the need to integrate XAI methods with cloud AI services to foster trust in cloud-based AI applications. XAI demands that activities conducted during an XAI process be traceable and reproducible [11]. Ensuring that the data utilized in the AI models and XAI methods are trustworthy becomes important. Addressing data provenance in XAI operations is essential for guaranteeing that the generated explanations are reliable, verifiable, and consistently reproducible across diverse settings. Despite the numerous XAI frameworks available, a noticeable gap exists among essential components [10] including data processing, methods configuration, and evaluation metrics. These components collectively form complex pipelines. This observation motivates our proposal: a design by a cloud-native paradigm based on the microservice architecture. This architectural style benefits from its capacity for the independent deployment of diverse components, streamlined communication via RESTful APIs, and a built-in adaptability that accommodates the introduction of new XAI methods, substitutions of AI models, and adjustments in pipeline configurations. Furthermore, our proposed architecture offers precise record capabilities. This ensures that the provenance of every XAI operation is transparent, facilitating the reproduction of XAI tasks or entire workflows. For black-box models or cloud-based AI services, the task of revealing the internal structures of AI models becomes unfeasible, especially for those AI services that encapsulate models behind standard RESTful APIs. We address this challenge by drawing inspiration from XAI methods that focus on feature influence and causality, for example, SHAP [12]. Besides, we propose a method that approximates the black-box AI model with a custom-built model and computes the feature contribution values, providing interpretable insights even from opaque AI models. Confronted by these challenges, our work is directed towards the following research questions. ‚Ä¢ RQ1: How to obtain and evaluate XAI results without unfolding the cloud AI service model structure? We investigate the cloud AI services and XAI methods in Section II-A and II-B. This enables us to understand the communication between cloud AI services and the specific requirements for XAI methods. Subsequently, we briefly summarize the applicable XAI methods in the taxonomy, Section IV-A. We also seek the packaged XAI frameworks listed in Section II-C. However, most frameworks are not explicitly compatible with cloud AI services. Therefore, we propose workflow as Figure 2 that integrates Cloud AI with Post hoc XAI, expressed in Section IV-B. Ultimately, scenarios one and two in Section VII-C and VII-B compute and evaluate the XAI results from integrating three major cloud AI services. ‚Ä¢ RQ2: What are the essential components required for XAI service architecture to deliver feature contribution explanations for models? To implement XAI within a service-oriented framework, Section V delineates the key architectural components critical for integration with existing cloud-based AI services. Following this, Section VII presents four illustrative scenarios using the designed XAI service architecture to explore typical discovery situations. ‚Ä¢ RQ3: How to collect XAI provenance data from operations to ensure traceability within the XAI service? Referring to the related works in Section II-D, we notice that the provenance data is necessary for XAI operations. Referring to the key components in XAI operations, we provide a graph format design for the XAI provenance data. Section VI introduces how to automatically collect the provenance data from various XAI operations within the XAI service. By retrieving the provenance data, we can identify differences and edit configurations to the XAI operations. In section VII-D, scenario three, we showcase a scenario that optimizes the model by modifying and executing reproduction. This scenario leads to improvements in both model performance and the XAI evaluation metrics. With the operations traceable and reproducible, we present the cloud-agnostic reproduction in scenario four, section VII-E. In this work, we propose an innovative XAI service architecture specifically designed to feature contribution explanations, illustrated through a showcase scenario drawn from computer vision cloud AI services. This method involves the utilization of approximation models to generate images, emphasizing the most contributing features. These masked images then act as inputs to create the AI services‚Äô predictions. We calculate the prediction changes value between the original and masked images. Leveraging these prediction changes, we compute a comprehensive explanation summary for the AI services, providing a transparent overview. The main contributions are summarized as follows: ‚Ä¢ Design cloud-platform-independent XAI service framework. The open API architecture is independent of the cloud-specific AI service. The architecture accommodates first-class entities in the XAI process as unified micro-services. The communication is open API-based, thus encapsulating the variance of models, XAI methods, and inputs and outputs from feature engineering. ‚Ä¢ Provide explanations across multiple cloud AI services. Based on the definition of the XAI consistency metric, we derive an explanation summary cross-validated on multiple clouds to observe both the learning performance of AI services and data augmentation effects. ‚Ä¢ Reproduce XAI operations through configure-and-rerun. The configuration of services is the receipt of composing an end-to-end explanation workflow. By reserving the configuration of each service given a workflow definition, we accumulate the provenance of how each explanation is produced. Through the coordination center of the XAI framework, we can rerun the XAI workflow to reproduce the explanation. We demonstrate the XAI service architecture with four discovery scenarios in Section VII, including (1) Cloud AI performance evaluation, (2) XAI consistency evaluation, (3) Probing of data augmentation effect, (4) Cloud-agnostic reproduction on three major cloud service platforms includes Azure Cognitive Service [2], Google Cloud Vertex AI [3], and Amazon Web Services Rekognition [1]. Our study employs consistency metric [8] to assess the explanations derived from multiple cloud AI services. The experimental results help us observe and discover that data augmentation techniques not only enhance all cloud AI service learning performance but also improve evaluation results from the different XAI methods. The adoption of XAI frameworks is designed for data science and machine learning engineers, effectively functioning as a tool for assessment in the development of complex AI systems. A recent study [13] proposes a multi-level governance pattern that integrates team-level XAI practices with organization-level ethical standards, thereby organizing ethical principles. This work introduces an XAI service framework for AI service practitioners, ensuring alignment with ethical guidelines and organizational values. The remaining sections are structured as follows: Section II explores related works on cloud-based AI services and their explainability challenges. Section III summarizes the employed background knowledge. In Section IV, we delve into post-hoc XAI methods and their integration into cloud services. Section V presents our microservices-based XAI architecture. Section VI emphasizes the tracing and reproducibility aspects of XAI operations using provenance data. Section VII presents the setup and results of the experiment. Section VIII evaluates the XAI service from the system aspect. The paper concludes by summarizing our findings in Section IX."
https://arxiv.org/html/2411.04112v1,Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For Autonomous Visual Robot Navigation,"Centralized learning requires data to be aggregated at a central server, which poses significant challenges in terms of data privacy and bandwidth consumption. Federated learning presents a compelling alternative, however, vanilla federated learning methods deployed in robotics aim to learn a single global model across robots that works ideally for all. But in practice one model may not be well suited for robots deployed in various environments. This paper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated learning framework that is deployed with vision based autonomous robot navigation in diverse outdoor environments. The framework addresses the key federated learning challenge of deteriorating model performance of a single global model due to the presence of non-IID data across real-world robots. Extensive real-world experiments validate that Fed-EC reduces the communication size by 23x for each robot while matching the performance of centralized learning for goal-oriented navigation and outperforms local learning. Fed-EC can transfer previously learnt models to new robots that join the cluster.","Poor availability of high-speed internet is limiting outdoor robots from realizing their full potential. In today‚Äôs world, robots are seamlessly deployed in diverse conditions all over the world, from bustling urban landscapes to rugged terrains in the wild. Many of these robots are using visually guided autonomy architectures powered by machine learning and self-supervision. Recent works [1], [2], [3], [4], [5] have shown that with access to large amounts of data, robots can achieve state-of-the-art navigation performance and can be deployed in various scenarios with minimum human intervention needed. These and other such methods are driving tremendous progress in self-driving cars [6], [7], robots navigation in indoor [8], [9] and outdoor environments [10], [11], [12]. However, in practice, traditional learning approaches require access to all of the data in one place, uploaded to a central server for model training requiring high speed internet. Furthermore, robots operating in the world experience diverse and varied environments requiring continuous upload of large amounts of data to the central server. While effective in controlled environments with high bandwidths, uploading big chunks of data can be a challenge for robots in environments where high-speed internet is not available, is intermittent, outright denied or even leads to significant battery power consumption. Federated learning (FL) [13] reduces the bandwidth requirement while enabling these robots to collectively enhance their learning by sharing model updates. With recent advancements in the capabilities of edge devices, federated learning takes advantage of edge computation to train models locally and shares model parameters instead of raw data with the server to learn a shared global model. FL also allows robots to send updates at intervals, rather than continuously streaming data reducing bandwidth usage. Further, through federated learning, there is hope that robots can gather insights from their respective environments, while also contributing to a global pool of knowledge to learn adaptable models for varied environments on the go. Traditionally, FL learns a single model that tries to minimize the average loss across robots. However, local data on deployed robots is highly non-IID due to different usage and operating locations. During FL, the divergence of the local datasets due to their non-IID nature leads to slower convergence and worsening learning performance when the models are aggregated. In such cases, a singular global model suffers and may perform worse than local models for some robots. With non-IID data, it is improbable that there exists a single global model that fits the needs for all robots. The global model can be biased and unfair. Current robotic systems that use federated learning frameworks do so in simulation [14] or in structured indoor environments [15] and do not account for heterogeneity that arises in the real-world deployment of robots. One way to avoid biased global models is to learn personalized models by clustering robots with similar local data distributions and training one aggregate model for each cluster. As a result, robots collaborate with only robots with similar experiences avoiding biases and negative performance. Previous clustered FL methods compare local model weights or gradients that rely on indirect information of the data distribution. [16] and [17] cluster the clients and learn individual cluster models but incur a high communication cost in doing so. In this paper, we highlight the first clustering-based system, Federated-EmbedCluster(Fed-EC) for self-supervised visually guided autonomous navigation which overcomes the need for high bandwidth speeds. Fed-EC is deployed on two different visual navigation models to showcase its modularity. To overcome the negative affect of non-IID data on model performance, Fed-EC groups the aggregation of local models by looking at similarity between the local datasets. Within each cluster group, the data is similar and mimics an IID set up ensuring that model aggregation does not degrade performance. Unlike previous methods where multiple rounds are needed [16] or multiple models are communicated [17], in each communication round the mean embedding vector which does not incur any additional communication cost is shared along with the local model. Fed-EC does not know the cluster identities beforehand and hence simultaneously identifies clusters within participating robots and learns individual cluster models in the federated setting. Figure 1: Workflow of Fed-EC. (a) Participating robots learn and communicates a mean embedding vector and local model weight to the server. The server clusters the robots using the mean embedding vector and aggregates local models in each cluster to learn a model which is shared with the robots based on their cluster identity. (b) The robots navigate to the a given GPS goal using the learnt model which takes as input RGB and depth images from the front facing camera. (c) If a new robot is deployed it computes a mean embedding and shares it with the server. The server assigns a cluster to the robot and sends the respective cluster model to the robot to use. In this paper, we consider robots in the wild that are constantly deployed with limited hardware on board, limited communication bandwidth, and battery power. The main contributions of our papers are as follows: ‚Ä¢ We propose a clustering-based personalized FL strategy Fed-EC, to overcome the problems generated by the heterogeneous nature of robotic operations. ‚Ä¢ We implement and test the framework of Federated learning in the robotics settings, in particular on real robots using two different navigation models to navigate to a given GPS point. ‚Ä¢ We validate through real-world robot experiments in diverse outdoor terrains that Fed-EC can perform as well as the centralized framework while reducing communication size and is better than just local training. We also show that learning a personalized FL model for each cluster is better than learning a singular global FL model over all robots. ‚Ä¢ We also show the transferability properties of our system to new robots that join the network."
https://arxiv.org/html/2411.03832v1,DART-PIM: DNA read mApping acceleRaTor Using Processing-In-Memory,"Genome analysis has revolutionized fields such as personalized medicine and forensics. Modern sequencing machines generate vast amounts of fragmented strings of genome data called reads. The alignment of these reads into a complete DNA sequence of an organism (the read mapping process) requires extensive data transfer between processing units and memory, leading to execution bottlenecks. Prior studies have primarily focused on accelerating specific stages of the read-mapping task. Conversely, this paper introduces a holistic framework called DART-PIM that accelerates the entire read-mapping process. DART-PIM facilitates digital processing-in-memory (PIM) for an end-to-end acceleration of the entire read-mapping process, from indexing using a unique data organization schema to filtering and read alignment with an optimized Wagner‚ÄìFischer algorithm. A comprehensive performance evaluation with real genomic data shows that DART-PIM achieves a 5.7√ó\boldsymbol{5.7\times}bold_5.7 bold_√ó and ùüêùüìùüï√ó\boldsymbol{257\times}bold_257 bold_√ó improvement in throughput and a ùüóùüê√ó\boldsymbol{92\times}bold_92 bold_√ó and ùüêùüï√ó\boldsymbol{27\times}bold_27 bold_√ó energy efficiency enhancement compared to state-of-the-art GPU and PIM implementations, respectively.","Genome analysis serves as the cornerstone of numerous scientific and medical breakthroughs, playing a vital role in personalized medicine [33, 23, 11, 41] and advanced forensics [116, 18]. Genome sequencing machines [51, 52, 50, 86] produce vast volumes of short random fragmented strings of bases (A, C, G, T in DNA), commonly referred to as reads, which are extracted from the longer original DNA [4]. The process of reconstructing the original DNA sequence begins by approximating the location of each read in the overall genome and then using the reads at each location to deduce the most likely reference base. This procedure involves complex algorithms with high memory requirements that constitute a bottleneck to the entire genome analysis process. Sequence alignment is a popular approach that localizes read fragments based on their similarity to a corresponding reference genome of the target species. This approach is feasible thanks to the high degree (above 99%percent9999\%99 %) of resemblance between genomes of different specimens within the same species. This computational process, called read mapping [4, 21], involves offline indexing of the reference genome (utilizing short representative fragments known as minimizers [93]), followed by three consecutive online steps (illustrated in Figure 1): (1) Seeding of potential locations (PLs) in the reference genome based on read and reference minimizer similarity; (2) pre-alignment filtering to eliminate false PLs; and (3) read alignment that determines the most probable PL according to a similarity metric. This latter stage is the most computationally intensive task, invoking string matching algorithms [40, 105, 80] for each read and PL within the complete genome. Figure 1: Genome sequence alignment process. The genomic samples are input into a sequencing machine that fragments them into small segments. The sequencing machine generates short strings known as reads, which are then processed during the read-mapping procedure. Read mapping involves an offline indexing stage followed by the online seeding, pre-alignment filtering, and read alignment stages. State-of-the-art read alignment techniques predominantly employ a dynamic programming (DP)-based approach, such as the Needleman-Wunsch or Smith-Waterman algorithms [68, 81, 97]. The complexity of these algorithms follows quadratic scaling in execution time and memory usage, thereby generating a significant latency and energy bottleneck [6, 4, 48, 79, 104]. This read-mapping bottleneck has been exacerbated by the recent enhancements of read generation rates on the one hand and insufficient corresponding growth in computational power on the other [4, 42]. To bridge the gap between read volumes and compute capabilities, prior work focused mainly on accelerating each read-mapping stage separately. This effort often prioritizes optimizing read alignment due to its significant computational demands, which can consume over 70% of the read-mapping execution time [19, 36, 79, 26, 60, 61, 20]. Read mapping, however, involves not only the processing of huge datasets, but also the frequent movements of huge volumes of data between the processor(s) and memory units, which increase both execution time and energy consumption [77, 4]. Since the data being transferred throughout the read-mapping process is roughly 100√ó100\times100 √ó larger than the original input reads, the acceleration of computational tasks across the process is still bottlenecked by the data transfer between the processes. Numerous studies have proposed alternative approaches to efficiently process large amounts of reads [19, 36, 79, 26, 60, 61, 20]. For instance, the state-of-the-art read mapper SeGraM [20] leverages near-memory computing [91] within a 3D-stacked memory architecture to mitigate costly chip-to-chip data transfers. Consequently, SeGraM‚Äôs read alignment process alone achieves a 144√ó144\times144 √ó speedup over existing CPU-executed software read mappers, alongside a 4.4√ó4.4\times4.4 √ó power consumption reduction. Nevertheless, when handling the entire read-mapping process, SeGraM‚Äôs performance demonstrates only a 2.5√ó2.5\times2.5 √ó speedup for short reads over the existing CPU-based read mappers. A promising solution to overcoming the data transfer bottleneck involves conducting read-mapping computations directly within the memory. This approach utilizes digital processing-in-memory (PIM) techniques to enable high parallelism, eliminating the need for frequent data transfer between the memory and the processor [78]. This paper presents DART-PIM, a DNA read mApping acceleRaTor using Processing-In-Memory, which is a comprehensive framework for accelerating the entire read-mapping process. The uniqueness of DART-PIM lies in its ability to execute all read-mapping stages for a given read and PL inside a single memory crossbar array. The majority of computations are performed using the memory cells, thus circumventing the data transfer bottleneck. A small number of complementary operations are executed in adjacent RISC-V cores. A key element for efficient computation in DART-PIM is careful organization of the reads and the reference genome within memory instances to minimize data transfer along all steps of read mapping. Within these instances, the data allocation is optimized for maximal utilization of the inherent massive parallelism offered by PIM. Consequently, DART-PIM greatly enhances the performance and energy efficiency of the entire end-to-end read-mapping process. This paper makes the following main contributions: 1. We propose a novel end-to-end accelerator architecture for the entire read-mapping process. 2. We present a data organization technique for accelerated indexing and seeding of a reference genome. 3. We develop a high-performance, memory efficient, in-memory pre-alignment filtering mechanism based on the linear Wagner-Fischer algorithm [107]. 4. We improved read alignment in-memory performance by enhancing the Wagner‚ÄìFischer algorithm with an affine-gap penalty and traceback capability. 5. We build system-level simulators that use real genome datasets to ensure precise evaluation of DART-PIM‚Äôs accuracy and performance, and compare it to state-of-the-art implementations (NVIDIA Parbricks [85] and SeGraM [20]), demonstrating faster execution time (5.7√ó5.7\times5.7 √ó and 257√ó257\times257 √ó improvement, respectively) and improved energy efficiency (92√ó92\times92 √ó and 27√ó27\times27 √ó, respectively)."
https://arxiv.org/html/2411.03742v1,Adaptive Consensus Gradients Aggregation for Scaled Distributed Training,"Distributed machine learning has recently become a critical paradigm for training large models on vast datasets. We examine the stochastic optimization problem for deep learning within synchronous parallel computing environments under communication constraints. While averaging distributed gradients is the most widely used method for gradient estimation, whether this is the optimal strategy remains an open question. In this work, we analyze the distributed gradient aggregation process through the lens of subspace optimization. By formulating the aggregation problem as an objective-aware subspace optimization problem, we derive an efficient weighting scheme for gradients, guided by subspace coefficients. We further introduce subspace momentum to accelerate convergence while maintaining statistical unbiasedness in the aggregation. Our method demonstrates improved performance over the ubiquitous gradient averaging on multiple MLPerf tasks while remaining extremely efficient in both communicational and computational complexity. A sample implementation of the method is available at https://github.com/yoniLc/AdaCons.","Distributed optimization is essential for training modern deep neural networks on large-scale datasets. Distributed environments can utilize different methods to parallelize computations, such as data parallelism [4] and model (e.g., tensor, pipeline) parallelism [58], each offering distinct benefits and suited to various applications [23, 8]. This work focuses on the challenge of efficient gradient aggregation within synchronous data parallelism where each worker processes a different subset of data. Synchronous data parallelism evenly distributes subsets of the dataset among several compute nodes/workers. Each node computes its local gradient before their aggregation into a central (master) model. While this aggregation is generally done at every iteration, model averaging [75, 69], which averages individual models trained over parallel workers, was developed to reduce communication overhead. The most ubiquitous aggregation methods remain linear combinations of the descent directions, such as averaging [50] and its proximal variants [71]. These aggregation methods are generally efficiently implemented using modern all-reduce strategies [10]. However, finding optimal aggregation remains an open problem since distributed systems are vulnerable to computing errors from the workers [5] or to out-of-distribution data samples inducing bad local gradients. Recently, researchers have begun wondering whether model averaging is the best strategy in a distributed setting [66], and have started exploring more elaborate aggregation schemes [60, 27, 34]. In particular, given multiple workers‚Äô directions, the aggregation remains an ill-posed problem that must be solved according to a prior metric of interest. In this work, we propose an efficient linear subspace optimization perspective to the problem of gradient aggregation. Beyond the conceptual novelty, we make three technical contributions: (i) We first formulate the aggregation problem as a subspace optimization problem created upon the original optimization objective. (ii) We propose a first-order approximation of the solution allowing the efficient closed-form formulation of the model update. (iii) We further extend the solution with a momentum-based unbiased estimator based on the statistics of the previous subspace coefficients. The benefits of the method are as follows. (i) The method outperforms the standard averaging technique by substantial margins on multiple training tasks while remaining scalable with respect to the number of workers. (ii) The method requires only low communication overhead and negligible computational cost. (iii) The method does not require hyper-parameter tuning (e.g., learning rate) or modification of the standard data-parallel setting."
https://arxiv.org/html/2411.03398v1,DP-HLS: A High-Level Synthesis Framework for Accelerating Dynamic Programming Algorithms in Bioinformatics,"Dynamic programming (DP) based algorithms are essential yet compute-intensive parts of numerous bioinformatics pipelines, which typically involve populating a 2-D scoring matrix based on a recursive formula, optionally followed by a traceback step to get the optimal alignment path. DP algorithms are used in a wide spectrum of bioinformatics tasks, including read assembly, homology search, gene annotation, basecalling, and phylogenetic inference. So far, specialized hardware like ASICs and FPGAs have provided massive speedup for these algorithms. However, these solutions usually represent a single design point in the DP algorithmic space and typically require months of manual effort to implement using low-level hardware description languages (HDLs). This paper introduces DP-HLS, a novel framework based on High-Level Synthesis (HLS) that simplifies and accelerates the development of a broad set of bioinformatically relevant DP algorithms in hardware. DP-HLS features an easy-to-use template with integrated HLS directives, enabling efficient hardware solutions without requiring hardware design knowledge. In our experience, DP-HLS significantly reduced the development time of new kernels (months to days) and produced designs with comparable resource utilization to open-source hand-coded HDL-based implementations and performance within 7.7‚Äì16.8% margin. DP-HLS is compatible with AWS¬Æ EC2 F1 FPGA instances. To demonstrate the versatility of the DP-HLS framework, we implemented 15 diverse DP kernels, achieving 1.3‚Äì32√ó\times√ó improved throughput over state-of-the-art GPU and CPU baselines and providing the first open-source FPGA implementation for several of them. The DP-HLS codebase is available freely under the MIT license at https://github.com/TurakhiaLab/DP-HLS and its detailed wiki at https://turakhia.ucsd.edu/DP-HLS/.","Genomic data is one of the fastest-growing data types globally, far outpacing Moore‚Äôs law in terms of data generation [1]. To meet the rising computational demands of analyzing and interpreting this data, several efforts have focused on accelerating bioinformatics applications on hardwares like GPUs, FPGAs, and ASICs [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]. While a lot of these accelerators are custom solutions to target a narrow application in bioinformatics, they also share notable similarities. For example, many of these solutions accelerate an algorithm based on dynamic programming (DP) [26]. This is unsurprising, as DP provides an efficient framework for comparing biological sequences‚Äîsuch as DNA, RNA, proteins, or even electrical signals from sequencing instruments‚Äîwhich is fundamental to many bioinformatics tasks, such as local pairwise alignments [27, 28], multiple sequence alignment [29, 30], homology searches [31, 32], whole-genome alignments [33], basecalling [34], and variant calling [35]. DP algorithms are computationally intensive, and therefore, they often dominate the runtime of these applications [36]. Recognizing their importance to bioinformatics, NVIDIA¬Æ recently introduced a specialized instruction, DPX, specifically to accelerate DP algorithms on GPUs [37]. Another key characteristic, particularly in FPGA and ASIC solutions [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], is the use of a hardware primitive‚Äîlinear systolic arrays‚Äîthat has been recognized since the 1980s for its efficiency in accelerating DP algorithms [38]. Most of these solutions focus on a specific or narrow set of 2-D DP algorithms and are typically designed at the Register Transfer Level (RTL) using low-level Hardware Description Languages (HDLs) like Verilog or VHDL, which makes them difficult to design and modify. A similar observation was made by a recent work, GenDP [39], which proposed a linear systolic array with software-programmable processing elements to accelerate a broad range of DP algorithms. However, software-programmable solutions introduce significant overhead on circuit-programmable FPGAs, which are the primary target of our work since they have already found commercial applications in bioinformatics [40, 41, 42]. Fig. 1. Common variations in 2-D DP algorithms in Bioinformatics. Kernels are indexed using #‚Äôs based on Table 1. In this paper, we present DP-HLS, a novel framework based on High-Level Synthesis (HLS) for accelerating broad DP kernels on FPGAs. A key feature of the framework is the separation of back-end optimizations from the front-end interface, enabling users to define new kernels in C++ without needing expertise in HLS or digital design. DP-HLS makes the following key contributions: (1) We developed DP-HLS, an HLS framework that, by separating front-end and back-end, introduces a layer of abstraction in the HLS flow to significantly enhance the productivity in deploying DP kernels on FPGAs. Specifically, the front-end provides a high degree of flexibility to specify new DP kernels in C++ without needing to add or modify HLS directives, while the back-end uses fixed HLS directives to efficiently translate these specifications into optimized RTL implementations with systolic arrays. Our experiments confirm that the generated RTL code exhibits the expected linear systolic array behavior. (2) Using the DP-HLS framework, we implemented 15 bioinformatically relevant DP kernels on FPGAs, covering a wide range of applications from basecalling to protein sequence alignment. All kernels are functionally verified and deployed on AWS¬Æ EC2 F1 instances for broad accessibility. For most kernels, no open-source FPGA implementations are available. (3) For the three DP kernels with available hand-optimized RTL implementations, DP-HLS achieved throughput within 7.7-16.8% and comparable resource utilization. However, DP-HLS significantly boosts design productivity, from months to days, compared to manual RTL design. (4) For several DP kernels implemented in state-of-the-art CPU and GPU libraries, DP-HLS delivered a 1.3‚Äì32√ó\times√ó improvement in throughput over CPU- and GPU-optimized AWS¬Æ EC2 instances of the same cost. (5) We demonstrate, through an example, that recently proposed tiling heuristics [11] are compatible with DP-HLS and can be used for performing both short and long sequence alignments on the FPGA. (6) We have made the entire framework publicly available, including all 15 DP kernels as case studies at https://github.com/TurakhiaLab/DP-HLS. Additionally, we provide a comprehensive wiki to assist new users at https://turakhia.ucsd.edu/DP-HLS/."
https://arxiv.org/html/2411.03357v1,PipeLLM: Fast and Confidential Large Language Model Services with Speculative Pipelined Encryption,"Confidential computing on GPUs, like NVIDIA H100, mitigates the security risks of outsourced Large Language Models (LLMs) by implementing strong isolation and data encryption. Nonetheless, this encryption incurs a significant performance overhead, reaching up to 52.8% and 88.2% throughput drop when serving OPT-30B and OPT-66B, respectively. To address this challenge, we introduce PipeLLM, a user-transparent runtime system. PipeLLM removes the overhead by overlapping the encryption and GPU computation through pipelining‚Äîan idea inspired by the CPU instruction pipelining‚Äîthereby effectively concealing the latency increase caused by encryption. The primary technical challenge is that, unlike CPUs, the encryption module lacks prior knowledge of the specific data needing encryption until it is requested by the GPUs. To this end, we propose speculative pipelined encryption to predict the data requiring encryption by analyzing the serving patterns of LLMs. Further, we have developed an efficient, low-cost pipeline relinquishing approach for instances of incorrect predictions. Our experiments on NVIDIA H100 GPU show that compared with vanilla systems without confidential computing (e.g., vLLM, PEFT, and FlexGen), PipeLLM incurs modest overhead (<<<19.6% in throughput) across various LLM sizes, from 13B to 175B.","Large Language Models (LLMs) are increasingly used across various applications (zheng2023judging, ; copilot, ). With the growth of open-source LLMs (touvron2023llama, ; jiang2024mixtral, ; zhang2022opt, ), companies are integrating and fine-tuning these models into their business operations. Due to LLM‚Äôs reliance on high-end GPUs, many businesses opt for outsourced services, such as cloud, attracted by their high availability and flexible pay-as-you-go models. However, these cloud infrastructures, often complex in nature, encompass a large Trusted Computing Base (TCB), which may contain vulnerabilities, both publicly reported and zero-day (li2021twinvisor, ; chen2023security, ). This poses security risks for LLMs, which are usually fine-tuned with proprietary data, and user prompts that contain sensitive business information. Thus, any data breach could expose critical business secrets. To mitigate these security threats, people introduce confidential computing. Confidential computing is designed to safeguard tenants‚Äô code and data against untrusted privileged software and rogue employees of cloud providers. The confidential virtual machine (CVM), supported by technologies such as Intel TDX (tdxmodule, ), AMD SEV (sevsnp, ), and ARM CCA (armcca, ), serves as a prime example of this. Any software external to a CVM is unable to access the code and data within it. Regarding machine learning workloads, people develop GPU enclaves to enhance security measures within GPUs (volos2018graviton, ; h100cc, ). A notable implementation of this is the NVIDIA H100 GPU (h100cc, ), which supports confidential computing inside the GPU to protect sensitive data and models from unauthorized access. Moreover, the data communication between the CVM and the GPU enclave is encrypted, further reinforcing the security of I/O operations. Although GPU confidential computing effectively enhances security for traditional small-scale AI models, it significantly undermines the performance of LLMs in throughput and latency. Our comprehensive experiments on NVIDIA H100 GPUs reveal that the GPU enclave can incur up to a 52.8% latency overhead on serving OPT-30B, a 36.2% throughput drop on fine-tuning OPT-30B, and an 88.2% throughput drop on serving OPT-66B (¬ß3). This overhead is largely due to a combination of memory swapping plus encryption. The swapping happens because LLMs consume a huge amount of GPU memory. For example, the OPT-66B model needs approximately 132GB of memory to store all its parameters, surpassing the 80GB memory of H100 GPUs. Moreover, runtime states such as the Key-Value cache (KV cache) (pope2022kv, ) during LLM inferences and activation during LLM training also consume significant GPU memory. Owing to the limited GPU memory, a GPU enclave has to dynamically swap out inactive parameters and/or runtime states to the main memory. This process requires encrypting the data transferred out of the GPU enclave. Correspondingly, the CPU cores must decrypt data received from the GPU, and re-encrypt it before sending it back to the GPU (¬ß2.2). However, the encryption and decryption pose a severe bottleneck due to the limited computational capability. This bottleneck significantly harms the overall performance, particularly in the context of LLMs. This paper introduces PipeLLM, a system designed to eliminate the performance overhead associated with GPU confidential computing for LLMs. Importantly, PipeLLM achieves this without requiring any changes to the existing LLM systems or the hardware, while still upholding the same level of security. The underlying principle of PipeLLM is straightforward yet effective: it decouples encryption tasks from the critical path of the memory swapping mechanism, by leveraging speculative pipelined encryption (¬ß4.3), a technique we proposed. Drawing inspiration from the concept of speculative execution in CPUs, PipeLLM anticipates which data blocks will be required by the GPU and pre-encrypts them. By doing so, PipeLLM significantly reduces the overhead of the GPU confidential computing by integrating predictions, encryptions, and data transfers into a pipeline. However, akin to CPU pipelining, an incorrect prediction could not only waste an individual pre-encrypted data but also invalidate the entire pipeline of subsequent pre-encrypted data. This is a consequence of the encryption scheme used by GPU enclaves, designed to prevent replay attacks (replayattack, ). In the H100‚Äôs confidential computing, data is encrypted using a private key in conjunction with a unique integer known as the Initialization Vector (IV). The IV is synchronized between the CPU and GPU, and increments by one with each encryption. Consequently, if an incorrect piece of data is encrypted in a pipeline, all subsequent IVs in the pipeline could become invalid, requiring re-encryption of the subsequent data with the correct IVs. We will elaborate on the encryption mechanism and IVs in ¬ß5.3. To address the challenge, we observe that LLM systems are highly predictable in swapping, allowing PipeLLM to accurately determine the sequence of data being swapped using heuristics. For instance, FlexGen (flexgen-paper, ) and PEFT (peft, ) compute the LLM through a layer-by-layer process, enabling PipeLLM to efficiently swap in layer parameters in their respective order. Similarly, vLLM (vllm-paper, ) uses simple swapping policies like FIFO (First-In, First-Out) and LIFO (Last-In, First-Out). PipeLLM can recognize these swapping policies and use them to predict the future swapping sequence. Moreover, PipeLLM incorporates several techniques to accelerate the pipeline and mitigate the cost of prediction errors. First, PipeLLM develops an efficient validation scheme to verify the correctness of a pre-defined ciphertext (¬ß5.2). Second, PipeLLM introduces request re-ordering and NOP padding to handle IV mismatches without relinquishing the entire pipeline (¬ß5.3). Finally, PipeLLM provides asynchronous decryption to accelerate data transfer (¬ß5.4). We implement PipeLLM with approximately 1K lines of code in C++. We conducted our performance experiments on an Intel server equipped with an NVIDIA H100-SXM GPU. The evaluation results show that PipeLLM significantly reduces the overhead associated with GPU confidential computing for LLM serving and fine-tuning, cutting it from as much as 88.2% to <<<19.6% in throughput, across various LLM sizes, ranging from 13 billion to 175 billion parameters. In summary, this paper makes the following contributions: ‚Ä¢ We conduct a comprehensive performance analysis of NVIDIA Confidential Computing on an H100 GPU enclave with LLM workloads. ‚Ä¢ We propose speculative pipelined encryption, an approach to greatly reduce the swapping overhead of confidential computing. It works well for LLM serving and fine-tuning. ‚Ä¢ We have built a system PipeLLM and evaluated its performance on multiple state-of-the-art LLM systems."
https://arxiv.org/html/2411.02945v1,Instant Resonance: Dual Strategy Enhances the Data Consensus Success Rate of Blockchain Threshold Signature Oracles,"With the rapid development of Decentralized Finance (DeFi) and Real-World Assets (RWA), the importance of blockchain oracles in real-time data acquisition has become increasingly prominent. Using cryptographic techniques, threshold signature oracles can achieve consensus on data from multiple nodes and provide corresponding proofs to ensure the credibility and security of the information. However, in real-time data acquisition, threshold signature methods face challenges such as data inconsistency and low success rates in heterogeneous environments, which limit their practical application potential. To address these issues, this paper proposes an innovative dual-strategy approach to enhance the success rate of data consensus in blockchain threshold signature oracles. Firstly, we introduce a Representative Enhanced Aggregation Strategy (REP-AG) that improves the representativeness of data submitted by nodes, ensuring consistency with data from other nodes, and thereby enhancing the usability of threshold signatures. Additionally, we present a Timing Optimization Strategy (TIM-OPT) that dynamically adjusts the timing of nodes‚Äô access to data sources to maximize consensus success rates. Experimental results indicate that REP-AG improves the aggregation success rate by approximately 56.6% compared to the optimal baseline, while the implementation of TIM-OPT leads to an average increase of approximately 32.9% in consensus success rates across all scenarios.","As a core component of blockchain data interoperability, blockchain oracles play a crucial role in the acquisition and transmission of off-chain data, significantly advancing the development of blockchain applications [1, 2]. In recent years, the rapid growth of fields such as Decentralized Finance (DeFi) [3, 4] and Real World Assets (RWA) [5, 6] has led to increasing demand for external real-time data, such as exchange rates and price information [7]. Furthermore, other blockchain applications, including supply chain management [8, 9, 10], the Internet of Things (IoT) [11, 12, 13], and smart cities [14], also rely on real-time data such as location and traffic flow to improve efficiency, transparency, and collaboration among stakeholders. In this context, oracles provide accurate and timely data, laying the foundation for the seamless integration of smart contracts in heterogeneous data environments while significantly enhancing the reliability and functionality of blockchain applications. To ensure data credibility, blockchain oracles are typically composed of multiple distributed nodes that gather information from various data sources. Initially, the nodes collect data from multiple sources and perform preliminary aggregation using common methods such as median [15, 16, 17, 18], majority voting [19], and weighted averaging [20, 21]. These aggregation techniques aim to improve the representativeness of the data and mitigate the influence of malicious data sources. Subsequently, the distributed nodes engage in a further consensus on the aggregated data to ensure the reliability of the result. In this process, threshold signatures serve as a robust data consensus mechanism, ensuring that a valid signature is generated only when a predetermined number of nodes reach agreement. It effectively prevents the freeloading problem and provides strong cryptographic security guarantees, allowing the consensus results to be reliably verified. Consequently, threshold signatures have been widely adopted in various commercial projects, such as Chainlink [22] and the DOS Network [23], becoming a critical technology for ensuring data credibility and security. Figure 1: Threshold signature fails consensus when obtaining real-time data. However, the timing of data retrieval by distributed heterogeneous nodes from distributed data sources is inconsistent, leading to heterogeneity in the acquired real-time data, which complicates the consensus requirements of threshold signatures. As illustrated in Figure 1, different nodes may access information from the same data source at different times, resulting in discrepancies in both the timestamps and content of the data. This inconsistency significantly reduces the success rate of consensus in threshold signatures, affecting their usability in real-time data acquisition applications. Therefore, improving the consistency of aggregated data among nodes in real-time data retrieval tasks to ensure the usability of threshold signatures is the primary research objective of this paper. In this paper, we propose two innovative strategies to improve the data consensus success rate of blockchain threshold signature oracles. First, we introduce a novel data aggregation method called the Representative Enhanced Aggregation Strategy (REP-AG), which aims to improve the representativeness of the data aggregated by nodes, ensuring consistency with the aggregated data of other nodes and thereby enhancing the usability of threshold signatures. Second, we design a Timing Optimization Strategy (TIM-OPT) that adjusts the timing of node access to data sources and data distribution, thus increasing the success rate of data consensus. The main contributions of this paper are as follows: 1. We propose a novel data aggregation method, REP-AG, which models the data aggregation process of nodes under incomplete information as a Bayesian game and solves it. This approach significantly improves the consistency of aggregation results among nodes, thereby enhancing the success rate of data consensus. 2. We design a Timing Optimization Strategy TIM-OPT that introduces an appropriate waiting time before nodes access data sources, utilizing Bayesian game methods. This strategy effectively increases the concentration of data among nodes, thereby improving the success rate of data consensus. 3. Experiments show that under the same environmental assumptions, REP-AG improves the consensus success rate by approximately 56.6 % compared with the optimal baseline, and the consensus success rate of all schemes after the application of TIM-OPT increases by approximately 32.9 % on average. The remainder of this paper is structured as follows: Section 2 introduces the related works and existing challenges. Section 3 presents the system workflow and details of the proposed approach. Section 4 provides the experimental results and analysis. Finally, Section 5 concludes the paper and outlines future research directions."
https://arxiv.org/html/2411.02868v1,iAnomaly: A Toolkit for Generating Performance Anomaly Datasets in Edge-Cloud Integrated Computing Environments,"Microservice architectures are increasingly used to modularize IoT applications and deploy them in distributed and heterogeneous edge computing environments. Over time, these microservice-based IoT applications are susceptible to performance anomalies caused by resource hogging (e.g., CPU or memory), resource contention, etc., which can negatively impact their Quality of Service and violate their Service Level Agreements. Existing research on performance anomaly detection in edge computing environments is limited primarily due to the absence of publicly available edge performance anomaly datasets or due to the lack of accessibility of real edge setups to generate necessary data. To address this gap, we propose iAnomaly: a full-system emulator equipped with open-source tools and fully automated dataset generation capabilities to generate labeled normal and anomaly data based on user-defined configurations. We also release a performance anomaly dataset generated using iAnomaly, which captures performance data for several microservice-based IoT applications with heterogeneous QoS and resource requirements while introducing a variety of anomalies. This dataset effectively represents the characteristics found in real edge environments, and the anomalous data in the dataset adheres to the required standards of a high-quality performance anomaly dataset.","Edge-cloud integrated environments consist of devices with heterogeneous computing, storage, and networking capabilities. Microservice architectures are increasingly used to modularize IoT applications and deploy them in these distributed environments to meet the Quality of Service (QoS) requirements of each module while optimizing resource usage [1, 2]. Over time, these microservice-based IoT applications are susceptible to performance anomalies caused by resource hogging (e.g., CPU or memory) and resource contention, which can negatively impact their QoS and violate their Service Level Agreements [3, 4, 5]. Therefore, it is crucial to conduct performance anomaly detection on microservice-based IoT applications in edge computing environments and eventually mitigate such anomalies. Currently, there is limited research on performance anomaly detection in edge computing environments. One of the main reasons for this is the absence of publicly available edge performance anomaly datasets, which are crucial for training and evaluating algorithms proposed in such research. The few existing studies rely on cloud datasets [6, 7] or data collected from private edge setups [3, 8, 4] to evaluate their proposed approaches. The cloud datasets have been collected from applications (mostly web applications) deployed on cloud servers. However, these cloud servers lack the heterogeneity found in edge devices in terms of computing, storage, and networking capabilities. Additionally, the microservices in cloud applications do not demonstrate the same diversity in terms of QoS and resource requirements as those in an IoT application. As a result, cloud datasets fail to capture characteristics inherent to real edge environments. On the other hand, private edge setups have not been publicly released and lack detailed information, which makes it difficult to replicate their environments, generate the necessary data, and reproduce the results of the anomaly detection experiments. It also hinders research in the field because not everyone has access to a real edge-cloud deployment for data collection purposes. Hence, relying on cloud datasets and private edge setups does not facilitate performance anomaly detection research in edge computing environments, thus posing a challenge to the progression of the field. Therefore, there is an opportunity to create a performance anomaly dataset that reflects the characteristics of edge computing environments and release the setup used for dataset generation. Edge computing emulators are a suitable platform to generate performance anomaly datasets. They are more representative of real edge environments when compared to simulators, and are more easily accessible and cost-effective when compared to real edge deployments. The main aim of existing edge computing emulators is to create a staging environment that achieves compute and network realism similar to a real edge environment and facilitate testing of IoT applications before deploying them into production [9, 10, 11, 12, 13, 14]. However, these general-purpose emulators do not incorporate in their design, tools and mechanisms required to autonomously and transparently generate large-scale performance anomaly datasets useful for model training and evaluation. For example, they lack adequate monitoring tools to collect performance and system-level metrics, workload generation tools to generate and capture normal performance data, and chaos engineering mechanisms to inject performance anomalies into applications. This work addresses this gap by presenting the iAnomaly framework, a performance anomaly-enabled full-system emulator that accurately models an edge computing environment hosting microservice-based IoT applications. iAnomaly is designed with open-source tools and provides fully automated dataset generation capabilities to generate labeled normal (data collected under normal conditions without anomalies) and anomaly (data collected under anomalous conditions) data based on user-defined configurations. In addition, we present a performance anomaly dataset generated using the proposed framework. The dataset captures performance data for several microservice-based IoT applications with heterogeneous QoS and resource requirements across a wide range of domains, software architectures, service composition patterns, and communication protocols by introducing a variety of client/sensor-side as well as server-side anomalies. To the best of our knowledge, this multivariate dataset is the first open-source edge performance anomaly dataset. The analysis of the dataset showed that the microservices within it vary in terms of their QoS and resource usage during regular operation, thus successfully capturing the characteristics of a real edge dataset. Further analysis confirmed that the anomalous data in the dataset meets the necessary standards for a high-quality performance anomaly dataset. This includes having an anomaly ratio comparable to other standard anomaly datasets and the dataset‚Äôs non-triviality. The rest of the paper is organized as follows: Section II reviews the existing related works. Section III presents the architecture of the iAnomaly toolkit, while section IV discusses the implementation aspects of iAnomaly. Section V provides details of the generated performance anomaly dataset followed by an analysis of the dataset. Section VI concludes the paper and draws future research directions."
https://arxiv.org/html/2411.02829v1,CE-CoLLM: Efficient and Adaptive Large Language ModelsThrough Cloud-Edge Collaboration,"Large Language Models (LLMs) have achieved remarkable success in serving end-users with human-like intelligence. However, LLMs demand high computational resources, making it challenging to deploy them to satisfy various performance objectives, such as meeting the resource constraints on edge devices close to end-users or achieving high accuracy with ample resources. In this paper, we introduce CE-CoLLM, a novel cloud-edge collaboration framework that supports efficient and adaptive LLM inference for end-users at the edge with two modes, (1) low-latency edge standalone inference and (2) highly accurate cloud-edge collaborative inference. First, we show that the inherent high communication costs for transmitting LLM contextual information between the edge and cloud dominate the overall latency, making it inefficient and costly to deploy LLMs using cloud-edge collaboration. Second, we propose several critical techniques to address this challenge, including early-exit mechanism, cloud context manager, and quantization in cloud-edge collaboration to enable not only low-latency standalone edge inference but also efficient and adaptive cloud-edge collaborative inference for LLMs. Third, we perform comprehensive experimental analysis, which demonstrates that CE-CoLLM significantly reduces inference time by up to 13.81% and cloud computation costs by up to 84.55% compared to the popular cloud-based LLM deployment, while maintaining comparable model accuracy. The proposed approach effectively shifts the computational load to the edge, reduces the communication overhead, scales efficiently with multiple edge clients, and provides reliable LLM deployment using cloud-edge collaboration.","1 Introduction Large Language Models (LLMs) have shown tremendous utility across diverse fields, from Natural Language Processing (NLP) to critical decision-making tasks [55, 38, 6]. However, the common practice of deploying LLMs to the cloud suffers from several critical issues, including (1) high latency due to the inherent data transmission delay between the cloud and edge [10, 17, 64], (2) unstable responsiveness potentially attributed to unstable network connections, especially for WiFi [10], and (3) privacy leakage risks caused by transmitting private user data over the public Internet [51, 15, 8]. While deploying deep learning models solely on the edge benefits from several advantages, including eliminating communication latency, preserving privacy, providing personalized services, and stable performance, deploying full-sized LLMs solely on edge devices also presents significant challenges. Due to the high complexity and high resource requirements of LLMs, resource-constrained edge devices lack the computational resources to fully support large-scale LLMs [59, 60, 63, 45]. The existing practices of deploying LLMs to the edge mainly include (1) compressing LLMs, such as through distillation, quantization, or pruning [20, 26, 34, 31], which sacrifice model accuracy, (2) deploying small language models at the edge and deferring complex tasks to the cloud-based full LLM [17], which introduces duplicated inference costs and inefficient resource usage, (3) partitioning LLMs into two partitions for edge and cloud separate deployment [64], which incurs high communication overhead, dominating the overall LLM inference costs and resulting in slow inference. Hence, it remains a critical challenge to leverage cloud-edge collaboration to provide efficient, fast, accurate, and adaptive on-device inference for LLMs. In this paper, we propose a novel cloud-edge collaborative framework for LLMs to address this challenge. We aim to achieve three key performance objectives. (1) The edge standalone mode can perform LLM inference independently at the edge without compromising LLM accuracy. (2) The cloud-edge collaborative LLM deployment provides adaptive LLM inference, where the edge client will determine the need for cloud-based LLM support based on the token prediction confidence. The high-confidence token predictions will be produced directly at the edge using an early-exit mechanism, while low-confidence tokens will be sent to the cloud to continue the full LLM inference. (3) The cloud-side LLM scales well by supporting multiple edge clients with low communication costs and efficient context management for each client in LLM cloud-edge co-inference. We make three original contributions in this study. First, we show the significant communication overhead in implying splitting LLM for cloud-edge deployment. Second, we introduce an early-exit mechanism and a suite of optimization techniques, which significantly reduce the communication cost between the cloud server and edge devices and provide adaptive cloud-edge co-inference as well as edge standalone inference. Third, we conduct comprehensive experiments, which demonstrate that our approach effectively delivers efficient and adaptive LLM inference through cloud-edge collaboration."
https://arxiv.org/html/2411.02581v1,Configurable Non-uniform All-to-all Algorithms,"MPI_Alltoallv generalizes the uniform all-to-all communication (MPI_Alltoall) by enabling the exchange of data blocks of varied sizes among processes. This function plays a crucial role in many applications, such as FFT computation and relational algebra operations. Popular MPI libraries, such as MPICH and OpenMPI, implement MPI_Alltoall using a combination of linear and logarithmic algorithms. However, MPI_Alltoallv typically relies only on variations of linear algorithms, missing the benefits of logarithmic approaches. Furthermore, current algorithms also overlook the intricacies of modern HPC system architectures, such as the significant performance gap between intra-node (local) and inter-node (global) communication. This paper introduces a set of Tunable Non-uniform All-to-all algorithms, denoted TuNAlgsuperscriptsubscriptTuNAlg\text{\emph{TuNA}}_{\text{\emph{\scriptsize l}}}^{\text{\emph{\scriptsize g}}}TuNA start_POSTSUBSCRIPT l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT g end_POSTSUPERSCRIPT, where g and l refer to global (inter-node) and local (intra-node) communication hierarchies. These algorithms consider key factors such as the hierarchical architecture of HPC systems, network congestion, the number of data exchange rounds, and the communication burst size. The algorithm efficiently addresses the trade-off between bandwidth maximization and latency minimization that existing implementations struggle to optimize. We show a performance improvement over the state-of-the-art implementations by factors of 42424242x and 138138138138x on Polaris and Fugaku, respectively.","Optimizing data movement remains a critical challenge in the era of exascale. Collective communication that involves data exchange among (almost) all processes is an important class of data movement. Owing to its global scope, collectives are typically difficult to scale, and can consume a substantial portion of the overall execution time in applications, often accounting for between 25%percent2525\%25 % and 50%percent5050\%50 %, or more [1]. Machine learning (ML) applications, in particular, depend heavily on all-reduce and all-to-all (both uniform and non-uniform) collectives, which are crucial in efficiently shuffling data and synchronizing parameters during the parallel training process [2, 3]. Beyond ML, various HPC workloads heavily utilize non-uniform all-to-all communication. These include graph algorithms like PageRank [4], Fast Fourier Transform (FFT) computations [5], quantum computer simulations [6], and certain advanced preconditioners and solvers [7]. State-of-the-art implementations of non-uniform all-to-all communication typically rely on linear-time algorithms. In contrast, uniform all-to-all collective implementations utilize either linear-time algorithms (e.g., the scattered algorithm [8]) or logarithmic-time algorithms (e.g., the Bruck algorithm [9]), depending on the message sizes. Adapting logarithmic-time algorithms for non-uniform all-to-all communication is a challenging task and has only recently been explored [10]. However, while being logarithmic, this work only works for a fixed base of 2 and is agnostic of the architecture hierarchy of HPC systems, leaving substantial room for further optimization. Our research focuses on improving the efficiency of all-to-all communication for non-uniform data distributions, addressing the limitations of existing approaches. We introduce a novel algorithm called TuNA (tunable-radix non-uniform all-to-all), specifically designed for non-uniform all-to-all workloads. TuNA‚Äôs key innovation lies in its adjustable radix, which determines the base of the logarithmic complexity. This radix can be set anywhere between 2 and the total number of processes (PùëÉPitalic_P). By allowing fine-grained control over the radix, TuNA enables users to optimize the trade-off between the number of communication rounds and the size of data exchanges, leading to improved performance scalability. Furthermore, the recent increase in CPU cores and NUMA domains requires some HPC workloads to be executed in an m√ónùëöùëõm\times nitalic_m √ó italic_n configuration per compute node (w.r.t MPI ranks and OpenMP threads) [11] to utilize the available resources effectively. To address the communication needs of these workloads, we have extended TuNA with hierarchical variants, referred to as TuNAlgsuperscriptsubscriptTuNAlg\text{\emph{TuNA}}_{\text{\emph{\scriptsize l}}}^{\text{\emph{\scriptsize g}}}TuNA start_POSTSUBSCRIPT l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT g end_POSTSUPERSCRIPT. This extension separates the all-to-all communication into two distinct components: intra-node (local) and inter-node (global). By doing so, TuNAlgsuperscriptsubscriptTuNAlg\text{\emph{TuNA}}_{\text{\emph{\scriptsize l}}}^{\text{\emph{\scriptsize g}}}TuNA start_POSTSUBSCRIPT l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT g end_POSTSUPERSCRIPT can better adapt to the multi-level structure of contemporary HPC systems, potentially improving overall communication efficiency. In summary, our paper makes the following contributions: 1. We develop TuNA, capable of adjusting the radix (rùëüritalic_r) from 2222 to PùëÉPitalic_P. Evaluation of TuNA reveals: small radices work for small messages, a radix close to PùëÉ\sqrt{P}square-root start_ARG italic_P end_ARG improves mid-sized communication, and large radices work for large messages. 2. We develop hierarchical TuNA and its variants that decouple the communication into local intra-node and global inter-node data exchange phases, to improve performance further. 3. We perform a detailed evaluation of our techniques using scaling studies (up to 16161616k processes) on Fugaku [12] and Polaris [13]. Our algorithms exhibit a performance improvement of 60.60√ó60.60\times60.60 √ó (TuNA), 138.59√ó138.59\times138.59 √ó (TuNAlgsuperscriptsubscriptTuNAlg\text{\emph{TuNA}}_{\text{\emph{\scriptsize l}}}^{\text{\emph{\scriptsize g}}}TuNA start_POSTSUBSCRIPT l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT g end_POSTSUPERSCRIPT) over the vendor implementation of MPI_Alltoallv."
https://arxiv.org/html/2411.02560v1,Fast and Robust Information Spreading in the Noisy\pull\pull\pullModel,"Understanding how information can efficiently spread in distributed systems under stochastic and noisy communication conditions is a fundamental question in both biological research and artificial system design. When the communication pattern is stable, allowing agents to control whom they interact with, noise in communication can often be mitigated through redundancy or more sophisticated coding techniques. In contrast, previous work has shown that noisy communication has fundamentally different consequences on well-mixed systems. Specifically, Boczkowski et al. (2018) considered the noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model, in which in each (parallel) round, each agent passively receives observations of the messages held by h‚Ñéhitalic_h randomly chosen agents, where each message can be viewed as any other message in the alphabet Œ£Œ£\Sigmaroman_Œ£ with probability Œ¥ùõø\deltaitalic_Œ¥. The authors proved that in this model, the basic task of propagating a bit value from a single source to the whole population requires Œ©‚Å¢(n‚Å¢Œ¥h‚Å¢(1‚àíŒ¥‚Å¢|Œ£|)2)Œ©ùëõùõø‚Ñésuperscript1ùõøŒ£2\Omega(\frac{n\delta}{h(1-\delta|\Sigma|)^{2}})roman_Œ© ( divide start_ARG italic_n italic_Œ¥ end_ARG start_ARG italic_h ( 1 - italic_Œ¥ | roman_Œ£ | ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) rounds. For example, for one-to-one interactions (h=1‚Ñé1h=1italic_h = 1) and constant Œ¥ùõø\deltaitalic_Œ¥, this lower bound is exponentially higher than the time required to reliably spread information over a stable complete network, thus exemplifying how the loss of structure in communication can undermine the system‚Äôs ability to effectively counteract noise.The current work shows that the aforementioned lower bound is almost tight. In particular, in the extreme case where each agent observes all other agents in each round, which can be related to scenarios where each agent senses the average tendency of the system, information spreading can reliably be achieved in ùí™‚Å¢(log‚Å°n)ùí™ùëõ\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) time, assuming constant noise. We present two simple and highly efficient protocols, thus suggesting their applicability to real-life scenarios. Notably, they also work in the presence of multiple conflicting sources and efficiently converge to their plurality opinion. The first protocol we present uses 1-bit messages but relies on a simultaneous wake-up assumption. By increasing the message size to 2 bits and removing the speedup in the information spreading time that may result from having multiple sources, we also present a simple and highly efficient self-stabilizing protocol that avoids the simultaneous wake-up requirement.Overall, our results demonstrate how, under stochastic communication, increasing the sample size can compensate for the lack of communication structure by linearly accelerating information spreading time.","1.1 Context and Motivation This work falls within the research area of ‚Äúnatural algorithms‚Äù, investigating biologically inspired settings through an algorithmic perspective [1, 2, 3, 4]. By taking a higher-level, abstract approach, this research has the potential to provide fundamental computational insights into the underlying biological processes, which may be challenging to derive using more conventional computational techniques such as differential equations and simulations [5, 6, 7, 8, 9]. Such abstraction can help uncover computational constraints shaped by environmental factors and, alternatively, identify adaptable, efficient problem-solving strategies relevant across various biological constraints [10, 5, 7, 11]. At the same time, this perspective can inspire applications that harness these insights for artificial systems. A motivating scenario: Cooperative transport by ‚Äúcrazy ants‚Äù. This study is motivated by the fascinating phenomenon of cooperative transport exhibited by Paratrechina longicornis ants, commonly known as ‚Äúcrazy ants‚Äù. This process involves a group of ants working together to physically transport a large food load to their nest [12, 13, 14, 6]. Researchers typically assume that each ant carrying the load senses the combined force exerted by all the carrying ants through the object being transported. After obtaining noisy measurements of this sum, each ant decides whether, and to what extent, it should align its force with this measurement. It is also believed that the ants physically transporting the load have limited information about the direction to the nest and that occasionally, a new knowledgeable ant enters the carrying group and helps navigate the load toward the nest [12, 13]. However, the directional signal from a knowledgeable ant might be diluted in the noise generated by the many other ants holding the load. This raises a natural question: How can a single informed ant convey directional information to the rest of the group when communication is limited to the inherently noisy perception of the load‚Äôs movement? This question was explored in [12], where the authors used statistical mechanics methods to show that, in principle, if the ants balance their response to the load‚Äôs movement with their individual directional preferences in a specific way, a single ant can, eventually (i.e., in a steady state), influence the entire group and steer the load in its preferred direction. However, another key question remains: Can such a process occur quickly? Since it is known that disseminating information through pairwise noisy interactions requires time that is linear in the group size [8], a positive answer would depend on whether sensing the average opinion could lead to fundamentally different scenarios, where convergence is significantly sped up compared to pairwise interactions. This motivated us to study convergence time as a function of sample size. 1.2 Background Disseminating information under randomized or unpredictable communication is a fundamental building block in multiple multi-agent systems, and has thus been extensively studied in various communities, including computer science [15, 16, 17, 18, 19, 20], physics [21, 22], and biology [8, 23, 24, 25]. In particular, in the biological world, the nature of computations performed by multi-agent systems under noisy communication varies across different organisms. Some systems effectively share information despite communication noise [26, 27], while others avoid gathering social information, thereby missing out on its potential benefits [28]. The key features that reduce unreliability in stochastic multi-agent systems are generally not well understood, nor are the circumstances in which these strategies might fail. Advancing our understanding in this area could deepen our insight into the constraints that shape the evolution of cooperative biological systems. Following this approach, we study the basic information spreading problem (also called broadcast, or rumor spreading) in a well-mixed population, where messages are noisy [18, 17, 19, 8, 29]. More specifically, we consider a population of nùëõnitalic_n agents, each having an opinion, which, for simplicity, is assumed to be binary, i.e., either 0 or 1. One opinion corresponds to the ground truth and is called correct. A few agents, called sources, know which opinion is correct and are moreover aware of being sources [23, 24]. The goal of all agents is to have their opinions coincide with the correct opinion as quickly as possible. We also consider the more general case of having conflicting sources, that is, scenarios with sources that do not necessarily agree on which opinion is correct. In this case, we require that all agents converge on the majority opinion among sources. Similarly to [8, 19], we consider the noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model of communication, in which each agent receives noisy observations from h‚Ñéhitalic_h randomly chosen agents in each (parallel) round. Each agent uùë¢uitalic_u holds a (public) message œÉùúé\sigmaitalic_œÉ in some alphabet Œ£Œ£\Sigmaroman_Œ£ and whenever another agent observes uùë¢uitalic_u, it receives a noisy version œÉ‚Ä≤superscriptùúé‚Ä≤\sigma^{\prime}italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT of œÉùúé\sigmaitalic_œÉ, where noise is independent among observations. It is convenient to think of the noise such that the message observed œÉ‚Ä≤superscriptùúé‚Ä≤\sigma^{\prime}italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT has a bias to be the original message œÉùúé\sigmaitalic_œÉ, but there is also some (e.g., constant) probability Œ¥ùõø\deltaitalic_Œ¥ that each other possible message in the alphabet Œ£Œ£\Sigmaroman_Œ£ is received as œÉ‚Ä≤superscriptùúé‚Ä≤\sigma^{\prime}italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT. The authors in [8] showed that for a constant Œ¥ùõø\deltaitalic_Œ¥ and a constant number of sources (all supporting the correct opinion), information spreading requires Œ©‚Å¢(nh)Œ©ùëõ‚Ñé\Omega\left(\frac{n}{h}\right)roman_Œ© ( divide start_ARG italic_n end_ARG start_ARG italic_h end_ARG ) rounds, no matter which strategy is employed by the agents. For a constant sample size h‚Ñéhitalic_h, such as in the case of pairwise communication (where h=1‚Ñé1h=1italic_h = 1), this lower bound is exponentially greater than the time needed to reliably disseminate information in a stable, complete network, highlighting how the loss of communication structure can severely weaken the system‚Äôs ability to effectively counteract noise. However, the lower bound formula also leaves open the possibility of achieving a speedup as sample size increases. Indeed, at first glance, there is reason to believe that increasing the sample size could dramatically improve the convergence time because agents obtain more information about the system‚Äôs configuration in each round. On the other hand, these extra observations come from a single configuration (at round tùë°titalic_t), and it is perhaps required to monitor the evolution of the system over many rounds in order to determine the opinion of the source, as in [20]. Under this intuition, it is not even clear whether letting each agent observe all other agents in each round (albeit through noisy observations) could break the Œ©‚Å¢(n)Œ©ùëõ\Omega(n)roman_Œ© ( italic_n ) lower bound. Indeed, in this case, a logarithmic number of rounds would suffice for an agent uùë¢uitalic_u to extract the correct opinion if it could reliably detect which of the samples it received in each round originated from the source. However, this detection capability is not readily available to uùë¢uitalic_u. For instance, the sources cannot safely signal the fact that they are sources using a designated bit, since this bit itself would be noisy. Consequently, since the sources are few, most of the received samples would falsely appear to come from the source, even though they originate from non-sources. Conversely, without the ability to identify the source, non-source agents must somehow extract information from multiple samples, most of which are unreliable. Indeed, in any given round, all agents are likely to have roughly the same quality of information about the correct opinion. Enhancing this quality, particularly in the initial stages, when it is very poor, appears challenging in the presence of noisy interactions. These considerations suggest that understanding which opinion is correct in a logarithmic or even poly-logarithmic number of rounds, may not be a trivial task. 1.3 Problem Definition Consider a set I={1,‚Ä¶,n}ùêº1‚Ä¶ùëõI=\{1,\dots,n\}italic_I = { 1 , ‚Ä¶ , italic_n } of agents, where each agent iùëñiitalic_i holds a binary opinion Y(i)‚àà{0,1}superscriptùëåùëñ01Y^{(i)}\in\{0,1\}italic_Y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ‚àà { 0 , 1 }. We also consider a subset of the agents, referred to as source agents, where each source agent is initially given a preference in {0,1}01\{0,1\}{ 0 , 1 } (which does not prevent it from later adopting a different opinion). We let s1subscriptùë†1s_{1}italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT (resp. s0subscriptùë†0s_{0}italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT) denote the number of source agents with preference 1111 (resp. 00), and make the mild assumption that s0,s1‚â§n4subscriptùë†0subscriptùë†1ùëõ4s_{0},s_{1}\leq\frac{n}{4}italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚â§ divide start_ARG italic_n end_ARG start_ARG 4 end_ARG. We further define the bias as s:=|s1‚àís0|assignùë†subscriptùë†1subscriptùë†0s:=|s_{1}-s_{0}|italic_s := | italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT |, and require that s‚â•1ùë†1s\geq 1italic_s ‚â• 1. The preference supported by the strict majority of sources is referred to as the correct opinion. The goal is for all agents, including sources, to eventually adopt the correct opinion as fast as possible. The Noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model. Agents employ communication using an alphabet of messages, termed Œ£Œ£\Sigmaroman_Œ£, that may differ from the opinion set {0,1}01\{0,1\}{ 0 , 1 }. Time proceeds in discrete rounds, where in every round the following happens. 1. Each agent (source or non-source) chooses a message œÉ‚ààŒ£ùúéŒ£\sigma\in\Sigmaitalic_œÉ ‚àà roman_Œ£ to display. 2. Every agent samples h‚Ñéhitalic_h agents in IùêºIitalic_I, uniformly at random with replacement. In particular, Agent iùëñiitalic_i may sample twice the same agent, and may also sample itself. 3. Every agent receives noisy versions of the messages displayed by the sampled agents. Specifically, we consider a stochastic111A matrix is stochastic if its coefficients are non-negative, with each row summing to 1 (see Definition 9). matrix N‚àà‚Ñù|Œ£|√ó|Œ£|ùëÅsuperscript‚ÑùŒ£Œ£N\in\operatorname{\mathbb{R}}^{|\Sigma|\times|\Sigma|}italic_N ‚àà roman_‚Ñù start_POSTSUPERSCRIPT | roman_Œ£ | √ó | roman_Œ£ | end_POSTSUPERSCRIPT, called noise matrix, so that if Agent iùëñiitalic_i samples Agent jùëójitalic_j, displaying a message œÉ‚ààŒ£ùúéŒ£\sigma\in\Sigmaitalic_œÉ ‚àà roman_Œ£, then Agent iùëñiitalic_i observes message œÉ‚Ä≤superscriptùúé‚Ä≤\sigma^{\prime}italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT with probability NœÉ,œÉ‚Ä≤subscriptùëÅùúésuperscriptùúé‚Ä≤N_{\sigma,\sigma^{\prime}}italic_N start_POSTSUBSCRIPT italic_œÉ , italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT. 4. Every Agent iùëñiitalic_i may update its opinion Y(i)superscriptùëåùëñY^{(i)}italic_Y start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT, and its internal state (that may include counters, and other variables). The decisions of an agent may depend on all the messages that it received so far, and possibly on the agent‚Äôs coin tosses (in the case that the agent‚Äôs protocol is probabilistic). We assume that the random events involved in sampling, noise, and agents‚Äô coin tosses are all independent (across both rounds and agents). As a standard convention, we say that an event Aùê¥Aitalic_A happens with high probability (w.h.p.) if ‚Ñô‚Å¢(A)=1‚àíùí™‚Å¢(1/n2)‚Ñôùê¥1ùí™1superscriptùëõ2\mathbb{P}(A)=1-\mathcal{O}(1/n^{2})roman_‚Ñô ( italic_A ) = 1 - caligraphic_O ( 1 / italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). Definition 1. For Œ¥‚àà[0,1/|Œ£|]ùõø01Œ£\delta\in[0,1/|\Sigma|]italic_Œ¥ ‚àà [ 0 , 1 / | roman_Œ£ | ] and a (stochastic) noise matrix NùëÅNitalic_N, we say that ‚Ä¢ NùëÅNitalic_N is Œ¥ùõø\deltaitalic_Œ¥-lower bounded if NœÉ,œÉ‚Ä≤‚â•Œ¥subscriptùëÅùúésuperscriptùúé‚Ä≤ùõøN_{\sigma,\sigma^{\prime}}\geq\deltaitalic_N start_POSTSUBSCRIPT italic_œÉ , italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ‚â• italic_Œ¥ for every œÉ,œÉ‚Ä≤‚ààŒ£ùúésuperscriptùúé‚Ä≤Œ£\sigma,\sigma^{\prime}\in\Sigmaitalic_œÉ , italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚àà roman_Œ£. ‚Ä¢ NùëÅNitalic_N is Œ¥ùõø\deltaitalic_Œ¥-upper bounded if {NœÉ,œÉ‚â•1‚àí(|Œ£|‚àí1)‚Å¢Œ¥for every ‚Å¢œÉ‚ààŒ£,NœÉ,œÉ‚Ä≤‚â§Œ¥for every ‚Å¢œÉ‚â†œÉ‚Ä≤‚ààŒ£.casessubscriptùëÅùúéùúé1Œ£1ùõøfor every ùúéŒ£subscriptùëÅùúésuperscriptùúé‚Ä≤ùõøfor every ùúésuperscriptùúé‚Ä≤Œ£\begin{cases}N_{\sigma,\sigma}\geq 1-(|\Sigma|-1)\delta&\text{for every }% \sigma\in\Sigma,\\ N_{\sigma,\sigma^{\prime}}\leq\delta&\text{for every }\sigma\neq\sigma^{\prime% }\in\Sigma.\end{cases}{ start_ROW start_CELL italic_N start_POSTSUBSCRIPT italic_œÉ , italic_œÉ end_POSTSUBSCRIPT ‚â• 1 - ( | roman_Œ£ | - 1 ) italic_Œ¥ end_CELL start_CELL for every italic_œÉ ‚àà roman_Œ£ , end_CELL end_ROW start_ROW start_CELL italic_N start_POSTSUBSCRIPT italic_œÉ , italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ‚â§ italic_Œ¥ end_CELL start_CELL for every italic_œÉ ‚â† italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚àà roman_Œ£ . end_CELL end_ROW (1) ‚Ä¢ NùëÅNitalic_N is Œ¥ùõø\deltaitalic_Œ¥-uniform if there is equality in Eq. 1, i.e., {NœÉ,œÉ=1‚àí(|Œ£|‚àí1)‚Å¢Œ¥for every ‚Å¢œÉ‚ààŒ£,NœÉ,œÉ‚Ä≤=Œ¥for every ‚Å¢œÉ‚â†œÉ‚Ä≤.casessubscriptùëÅùúéùúé1Œ£1ùõøfor every ùúéŒ£subscriptùëÅùúésuperscriptùúé‚Ä≤ùõøfor every ùúésuperscriptùúé‚Ä≤\begin{cases}N_{\sigma,\sigma}=1-\left(|\Sigma|-1\right)\delta&\text{for every% }\sigma\in\Sigma,\\ N_{\sigma,\sigma^{\prime}}=\delta&\text{for every }\sigma\neq\sigma^{\prime}.% \end{cases}{ start_ROW start_CELL italic_N start_POSTSUBSCRIPT italic_œÉ , italic_œÉ end_POSTSUBSCRIPT = 1 - ( | roman_Œ£ | - 1 ) italic_Œ¥ end_CELL start_CELL for every italic_œÉ ‚àà roman_Œ£ , end_CELL end_ROW start_ROW start_CELL italic_N start_POSTSUBSCRIPT italic_œÉ , italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_Œ¥ end_CELL start_CELL for every italic_œÉ ‚â† italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT . end_CELL end_ROW Self-stabilizing setting. In order to capture the lack of ability to synchronize clocks, and in particular, the inability to know when the process starts, we consider a somewhat relaxed version of the classical notion of self-stabilization in distributed computing [30, 31]. That is, we assume that at time 00 (unknown to the agents) an adversary can manipulate and possibly corrupt the initial configuration, however, there are some constraints on such a manipulation. Specifically, we assume that the adversary first chooses the set of sources and their preferences. As before, the preference that is more frequent among sources is referred to as the correct opinion. Subsequently, the adversary chooses the internal states of agents, for example, by including in their memory fake samples presumably gathered earlier, or by corrupting their counters or clocks if they have any. However, we assume that agents know both the number of agents nùëõnitalic_n and the noise matrix NùëÅNitalic_N, and whether or not they are sources with a given preference; this information is not corrupted by the adversary. Definition 2 (Convergence). We say that a protocol solves the noisy information spreading problem in time ùíØùíØ\mathcal{T}caligraphic_T w.h.p if the system reaches consensus on the correct opinion in at most ùíØùíØ\mathcal{T}caligraphic_T rounds w.h.p. Note that with this definition, even the sources whose preference is incorrect must converge on the correct opinion. We additionally say that a protocol solves the problem in time ùíØùíØ\mathcal{T}caligraphic_T in a self-stabilizing manner if, starting with any initial configuration (in the sense described above), the system reaches consensus on the correct opinion in at most ùíØùíØ\mathcal{T}caligraphic_T rounds and remains with it for a polynomial number of rounds, say n3superscriptùëõ3n^{3}italic_n start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT rounds, w.h.p. 1.4 Our Results Let us first state the lower bound theorem in [8] which merely requires correctness with probability 2/3232/32 / 3. The authors in [8] assumed that all sources agree on the correct opinion, which means, in particular, that the bias sùë†sitalic_s is the same as the number of sources. The main focus of [8] was the case of pairwise interactions, that is, h=1‚Ñé1h=1italic_h = 1, however their proof, that can be found in [32], applies for general h‚Ñéhitalic_h. Note that in the case that Œ¥ùõø\deltaitalic_Œ¥ is bounded away from 1/|Œ£|1Œ£1/|\Sigma|1 / | roman_Œ£ |, the lower bound becomes informative only when s‚â§nùë†ùëõs\leq\sqrt{n}italic_s ‚â§ square-root start_ARG italic_n end_ARG. Theorem 3 (Theorem 4 in [32].). Fix a non-source agent uùë¢uitalic_u and an integer h‚Ñéhitalic_h. Any rumor spreading protocol in the noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model with alphabet Œ£Œ£\Sigmaroman_Œ£ and Œ¥ùõø\deltaitalic_Œ¥-lower bounded noise requires Œ©‚Å¢(n‚Å¢Œ¥s2‚Å¢h‚Å¢(1‚àí|Œ£|‚Å¢Œ¥)2)Œ©ùëõùõøsuperscriptùë†2‚Ñésuperscript1Œ£ùõø2\Omega\!\left(\frac{n\delta}{s^{2}h(1-|\Sigma|\,\delta)^{2}}\right)roman_Œ© ( divide start_ARG italic_n italic_Œ¥ end_ARG start_ARG italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ( 1 - | roman_Œ£ | italic_Œ¥ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) rounds in order to guarantee that the opinion of uùë¢uitalic_u is correct with probability at least 2/3232/32 / 3. This lower bound holds even assuming that all the system‚Äôs parameters, including the number of agents nùëõnitalic_n, the noise matrix NùëÅNitalic_N, and the number of sources sùë†sitalic_s, are known to the designer of the protocol. We prove the following theorem which guarantees high probability correctness. Similarly to Theorem 3, the theorem assumes that the designer of the protocol knows all the system‚Äôs parameters. Theorem 4. Consider nùëõnitalic_n agents with bias s:=|s1‚àís0|‚â•1assignùë†subscriptùë†1subscriptùë†01s:=|s_{1}-s_{0}|\geq 1italic_s := | italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | ‚â• 1 interacting according to the noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model with alphabet Œ£={0,1}Œ£01\Sigma=\{0,1\}roman_Œ£ = { 0 , 1 } and Œ¥ùõø\deltaitalic_Œ¥-upper bounded noise. The noisy information spreading problem can be solved w.h.p in ùíØ:=ùí™‚Å¢(1h‚Å¢(n‚Å¢Œ¥min‚Å°{s2,n}‚Å¢(1‚àí2‚Å¢Œ¥)2+ns+s0+s1s2)‚ãÖlog‚Å°n+log‚Å°n)assignùíØùí™‚ãÖ1‚Ñéùëõùõøsuperscriptùë†2ùëõsuperscript12ùõø2ùëõùë†subscriptùë†0subscriptùë†1superscriptùë†2ùëõùëõ\mathcal{T}:=\mathcal{O}\!\left(\frac{1}{h}\left(\frac{n\delta}{\min\{s^{2},n% \}(1-2\delta)^{2}}+\frac{\sqrt{n}}{s}+\frac{s_{0}+s_{1}}{s^{2}}\right)\cdot% \log n+\log n\right)caligraphic_T := caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_h end_ARG ( divide start_ARG italic_n italic_Œ¥ end_ARG start_ARG roman_min { italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , italic_n } ( 1 - 2 italic_Œ¥ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG square-root start_ARG italic_n end_ARG end_ARG start_ARG italic_s end_ARG + divide start_ARG italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) ‚ãÖ roman_log italic_n + roman_log italic_n ) number of rounds, while using ùí™‚Å¢(log‚Å°ùíØ+log‚Å°h)ùí™ùíØ‚Ñé\mathcal{O}\!\left(\log\mathcal{T}+\log h\right)caligraphic_O ( roman_log caligraphic_T + roman_log italic_h ) bits of memory per agent. Remark. ‚Ä¢ Note that the theorem guarantees convergence to the correct opinion even when the bias s=1ùë†1s=1italic_s = 1. This is in contrast to multiple works in the area of population protocols that guarantee convergence to the majority opinion only when the bias towards it is Œ©‚Å¢(n‚Å¢log‚Å°n)Œ©ùëõùëõ\Omega(\sqrt{n\log n})roman_Œ© ( square-root start_ARG italic_n roman_log italic_n end_ARG ), see e.g., [33, 34, 35]. ‚Ä¢ When Œ¥>s4‚Å¢nùõøùë†4ùëõ\delta>\frac{s}{4\sqrt{n}}italic_Œ¥ > divide start_ARG italic_s end_ARG start_ARG 4 square-root start_ARG italic_n end_ARG end_ARG and the number of source agents is bounded as s0,s1‚â§nsubscriptùë†0subscriptùë†1ùëõs_{0},s_{1}\leq\sqrt{n}italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚â§ square-root start_ARG italic_n end_ARG, the upper bound in Theorem 4 becomes ùí™‚Å¢(n‚Å¢Œ¥s2‚Å¢h‚Å¢(1‚àí2‚Å¢Œ¥)2‚ãÖlog‚Å°n+log‚Å°n).ùí™‚ãÖùëõùõøsuperscriptùë†2‚Ñésuperscript12ùõø2ùëõùëõ\mathcal{O}\!\left(\frac{n\delta}{s^{2}h(1-2\delta)^{2}}\cdot\log n+\log n% \right).caligraphic_O ( divide start_ARG italic_n italic_Œ¥ end_ARG start_ARG italic_s start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_h ( 1 - 2 italic_Œ¥ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ‚ãÖ roman_log italic_n + roman_log italic_n ) . Thus, under these conditions and assuming the noise to be Œ¥ùõø\deltaitalic_Œ¥-uniform, the theorem matches the lower bound in Theorem 3 up to a logarithmic factor. ‚Ä¢ The missing log factor is likely due to the fact that Theorem 3 only requires agents to be correct with constant probability, while our notion of convergence demands high probability. Indeed, for the case where Œ¥ùõø\deltaitalic_Œ¥ is constant, the authors in [19] showed that the extra log‚Å°nùëõ\log nroman_log italic_n factor is necessary to achieve convergence w.h.p. when h=1‚Ñé1h=1italic_h = 1 (Theorem 7 in [19]), but in fact, their technique applies for general h‚Ñéhitalic_h (see also Footnote 3). Theorem 4 demonstrates how a larger sample size can linearly accelerate the information spreading time. In particular, in the extreme case where each agent observes all other agents in each parallel communication round, we show that information spreading can be reliably achieved in ùí™‚Å¢(log‚Å°n)ùí™ùëõ\mathcal{O}(\log n)caligraphic_O ( roman_log italic_n ) time, assuming sùë†sitalic_s and Œ¥>0ùõø0\delta>0italic_Œ¥ > 0 are constants. In practical contexts, this shift from linear to logarithmic time can be the difference between impracticality and feasibility. This suggests that an increased sample size can effectively compensate for the lack of structure in noisy environments (see further discussion in Section 3). To prove Theorem 4, we first reduced the general case of Œ¥ùõø\deltaitalic_Œ¥-upper bounded noise to a uniform-noise scenario by letting agents add artificial noise to their observations. A high-level overview of such a strategy is given in Section 2. The validity of the transformation follows from a general proof of the non-singularity of the noise matrix NùëÅNitalic_N (provided in Corollary 14). Although the corresponding linear algebra question is very simple to define, we could not find the answer in the literature, and instead, we provide one in Section 4. Relying on this reduction allows us to restrict attention to uniform noise. For this case, we first present a simple synchronous protocol that achieves the upper bound stated in Theorem 4. This protocol is essentially composed of three phases. During the first two phases, source agents present their preferences at all rounds while each non-source agent overall presents the same number of 0‚Äôs and 1‚Äôs. Over time, this ‚Äúneutral‚Äù behavior of non-sources allows the bias in the sources‚Äô preferences to somehow ‚Äústand-out‚Äù despite noise. Meanwhile, all agents (including sources) assemble samples from the population so that at the end of the two phases, each agent has gathered sufficiently many samples to obtain an opinion that is biased towards the correct opinion, by a very small, yet, non-negligible amount. This opinion is then presented by each agent throughout the third phase, which is dedicated to amplifying the slight bias in opinions using a majority rule. The correctness of the protocol depends on the assumption that agents begin execution simultaneously, allowing them to transition through the phases in sync. At the cost of increasing the message size to 2 bits, and giving up for the acceleration that is caused by having a large bias sùë†sitalic_s, we also present an efficient protocol that removes the simultaneous wake-up assumption to become self-stabilizing. Theorem 5. Consider nùëõnitalic_n agents with bias s:=|s1‚àís0|‚â•1assignùë†subscriptùë†1subscriptùë†01s:=|s_{1}-s_{0}|\geq 1italic_s := | italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | ‚â• 1 interacting according to the noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model with alphabet Œ£={0,1}2Œ£superscript012\Sigma=\{0,1\}^{2}roman_Œ£ = { 0 , 1 } start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and Œ¥ùõø\deltaitalic_Œ¥-upper bounded noise. The noisy information spreading problem can be solved w.h.p in ùíØ:=ùí™‚Å¢(Œ¥‚Å¢n‚Å¢log‚Å°nh‚Å¢(1‚àí4‚Å¢Œ¥)2+nh)assignùíØùí™ùõøùëõùëõ‚Ñésuperscript14ùõø2ùëõ‚Ñé\mathcal{T}:=\mathcal{O}\left(\frac{\delta n\log n}{h(1-4\delta)^{2}}+\frac{n}% {h}\right)caligraphic_T := caligraphic_O ( divide start_ARG italic_Œ¥ italic_n roman_log italic_n end_ARG start_ARG italic_h ( 1 - 4 italic_Œ¥ ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG italic_n end_ARG start_ARG italic_h end_ARG ) number of rounds, in a self-stabilizing manner, while using ùí™‚Å¢(log‚Å°ùíØ+log‚Å°h)ùí™ùíØ‚Ñé\mathcal{O}\!\left(\log\mathcal{T}+\log h\right)caligraphic_O ( roman_log caligraphic_T + roman_log italic_h ) bits of memory per agent. Recall from our definition of self-stabilization, that we assume that agents know the number of agents nùëõnitalic_n and the noise matrix NùëÅNitalic_N, and that such knowledge cannot be corrupted by the adversary. Moreover, after the adversary chooses the set of sources and their preferences, each agent knows whether it is a source or not, and this information cannot be corrupted by the adversary. In contrast to Theorem 4 (the non-self-stabilizing setting), Theorem 5 does not require agents to know the bias sùë†sitalic_s. We present our protocols and the intuition behind them in Section 2. Both are simple, and mainly rely on basic operations, namely, averaging, counting, and taking majority. 1.5 Related Works In various natural scenarios, a group must reach a consensus on a particular value determined by the environment. In these cases, agents possess varying levels of knowledge about the target value, and the system must leverage the insights of the more informed individuals [24, 36, 37, 25, 23]. The problem of propagating information from one or more sources to an entire population has been extensively studied in distributed computing under various names such as rumor spreading, information dissemination, epidemics, gossip, and broadcast, see, e.g., [38, 16, 39, 40]. The problem of converging to the most frequent opinion among sources is also known as ‚Äúzealot consensus‚Äù, ‚Äúmajority bit dissemination‚Äù and ‚Äúmajority consensus‚Äù [41, 42, 43, 44]. These tasks become particularly challenging when communication is limited and the system is vulnerable to faults. The terms ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) and ùí´‚Å¢ùí∞‚Å¢ùíÆ‚Å¢‚Ñã‚Å¢(h)ùí´ùí∞ùíÆ‚Ñã‚Ñé\mathcal{PUSH}(h)caligraphic_P caligraphic_U caligraphic_S caligraphic_H ( italic_h ) denote random meeting patterns, in which in each round, each agent uùë¢uitalic_u samples h‚Ñéhitalic_h agents uniformly at random, and may either extract information from them (ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñíùí´ùí∞‚Ñí‚Ñí\mathcal{PULL}caligraphic_P caligraphic_U caligraphic_L caligraphic_L), or inform them with a message (ùí´‚Å¢ùí∞‚Å¢ùíÆ‚Å¢‚Ñãùí´ùí∞ùíÆ‚Ñã\mathcal{PUSH}caligraphic_P caligraphic_U caligraphic_S caligraphic_H) [38, 16]. These models capture ‚Äúwell-mixed‚Äù scenarios, in which agents have little to no control over who they interact with, and are reminiscent of random meeting patterns studied in the areas of population protocols [45, 15, 46] and opinion dynamics [47, 48]. A classical information spread algorithm that works in both models is based on copying the opinions held by the knowledgable agents upon interaction [16]. This simple mechanism, however, is not self-stabilizing [30, 31] and may fail if the internal states of non-source agents are set arbitrarily. Such conditions can occur when the system dynamically changes and the agents do not share a global notion of time. Consequently, one line of work has recently focused on obtaining efficient self-stabilizing information spreading in the ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model, while specifically aiming to minimize communication and/or memory capacities [42, 49, 20, 50, 51]. While most proposed self-stabilizing algorithms are unlikely to be found in nature, some may suggest elements that could be realistically plausible [51]. In the absence of noise in communication, with some ‚Äúgrain of salt‚Äù [16], the ùí´‚Å¢ùí∞‚Å¢ùíÆ‚Å¢‚Ñãùí´ùí∞ùíÆ‚Ñã\mathcal{PUSH}caligraphic_P caligraphic_U caligraphic_S caligraphic_H and ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñíùí´ùí∞‚Ñí‚Ñí\mathcal{PULL}caligraphic_P caligraphic_U caligraphic_L caligraphic_L models are generally considered similar222Potentially, by increasing the alphabet in messages, one could imagine that the ùí´‚Å¢ùí∞‚Å¢ùíÆ‚Å¢‚Ñãùí´ùí∞ùíÆ‚Ñã\mathcal{PUSH}caligraphic_P caligraphic_U caligraphic_S caligraphic_H model could be simulated under the ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñíùí´ùí∞‚Ñí‚Ñí\mathcal{PULL}caligraphic_P caligraphic_U caligraphic_L caligraphic_L model by designating a bit in the message that signifies whether the agent is intending or not to reveal its message. However, if messages are noisy then this bit cannot be trusted.. However, when the communication is noisy, there is an exponential separation between the two models. Indeed, the authors in [8] considered the noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model (see Section 1.3), and proved Theorem 3. In particular, this theorem implies that for a single source (i.e., s=1)s=1)italic_s = 1 ), constant sample size h‚Ñéhitalic_h, and constant noise Œ¥>0ùõø0\delta>0italic_Œ¥ > 0, the information spreading time is Œ©‚Å¢(n)Œ©ùëõ\Omega(n)roman_Œ© ( italic_n ), even if correctness is guaranteed with only a constant probability per agent. The authors in [19] showed that for constant parameters as above, if convergence is required w.h.p then the techniques in [8] can be modified to yield an Œ©‚Å¢(n‚Å¢log‚Å°n)Œ©ùëõùëõ\Omega(n\log n)roman_Œ© ( italic_n roman_log italic_n ) lower bound333 Specifically, the authors in [19] showed that a protocol that solves the bit dissemination problem in the ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(1)ùí´ùí∞‚Ñí‚Ñí1\mathcal{PULL}(1)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( 1 ) model can be reduced to a so-called (m,x,Œ¥)ùëöùë•ùõø(m,x,\delta)( italic_m , italic_x , italic_Œ¥ )-Two-Party Protocol. This is a protocol that ensures that a party Aùê¥Aitalic_A can reliably receive a bit from a party BùêµBitalic_B, with a probability of success at least 1‚àíx1ùë•1-x1 - italic_x, after exchanging mùëömitalic_m messages that are affected by a Œ¥ùõø\deltaitalic_Œ¥-uniform noise. Thinking of Aùê¥Aitalic_A as the party of non-source agents, and BùêµBitalic_B as the source, they modified the lower bound for the high probability regime for the case h=1‚Ñé1h=1italic_h = 1, which was the main focus of that paper. However, since the number of messages received by party Aùê¥Aitalic_A in the ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñí‚Å¢(h)ùí´ùí∞‚Ñí‚Ñí‚Ñé\mathcal{PULL}(h)caligraphic_P caligraphic_U caligraphic_L caligraphic_L ( italic_h ) model is simply the number of rounds times h‚Ñéhitalic_h, it is possible to extend their result to apply for general h‚Ñéhitalic_h.. The authors in [19] also present a protocol that is unlikely to be found in nature but achieves information spreading in O‚Å¢(n‚Å¢log‚Å°n)ùëÇùëõùëõO(n\log n)italic_O ( italic_n roman_log italic_n ) rounds w.h.p., matching the lower bound. Standing in contrast to the linear lower bounds in [8, 19] for the case h=1‚Ñé1h=1italic_h = 1, the authors in [18] proved that in the noisy ùí´‚Å¢ùí∞‚Å¢ùíÆ‚Å¢‚Ñã‚Å¢(1)ùí´ùí∞ùíÆ‚Ñã1\mathcal{PUSH}(1)caligraphic_P caligraphic_U caligraphic_S caligraphic_H ( 1 ) model, information spreading can be solved in logarithmic time in nùëõnitalic_n. The reason behind this exponential separation is that the noisy ùí´‚Å¢ùí∞‚Å¢ùíÆ‚Å¢‚Ñãùí´ùí∞ùíÆ‚Ñã\mathcal{PUSH}caligraphic_P caligraphic_U caligraphic_S caligraphic_H model hides a reliable component in the communication: When an agent receives a message, she cannot be sure of its original content, but she can nevertheless be sure that the sender of the message ‚Äúintended‚Äù to send it. This reliability aspect is exploited in [18] to synchronize agents and control the propagation of information, to achieve fast and reliable information spreading. In this sense, communication under the noisy ùí´‚Å¢ùí∞‚Å¢ùíÆ‚Å¢‚Ñãùí´ùí∞ùíÆ‚Ñã\mathcal{PUSH}caligraphic_P caligraphic_U caligraphic_S caligraphic_H model ‚Äî but where an agent cannot be certain not only of a message‚Äôs content but also of whether the message was genuinely ‚Äúintended‚Äù for communication ‚Äî may more closely resemble the noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñíùí´ùí∞‚Ñí‚Ñí\mathcal{PULL}caligraphic_P caligraphic_U caligraphic_L caligraphic_L model. This applies, for instance, to communication through physical bumping between ants [24, 8] where bumping could be both intentional or accidental. Several scenarios in nature may be viewed as following noisy ùí´‚Å¢ùí∞‚Å¢‚Ñí‚Å¢‚Ñíùí´ùí∞‚Ñí‚Ñí\mathcal{PULL}caligraphic_P caligraphic_U caligraphic_L caligraphic_L-like communication patterns that involve relatively large sample sizes. Examples include flocks of birds, schools of fish, and bat groups, where individuals scan their surroundings to observe and respond to others‚Äô locations [22, 21, 23]. Swarming behaviors in midges [52] and firefly synchronization [53] are further cases where each individual (midge or firefly) responds to aggregated auditory or visual signals from numerous group members. In all these contexts, debates persist over the relevant communication models, particularly regarding the number and placement of observed agents [21, 53]. Additionally, during cooperative transport by longhorn ‚Äúcrazy ants‚Äù [12, 13, 54, 14, 6], carrying ants use the transported object to sense the cumulative force exerted by all participating ants. This setup allows ants to directly sense the overall directional tendency of the system, bypassing the spatial considerations in communication that appear to be more prominent in the previous examples."
https://arxiv.org/html/2411.03231v2,Formal Logic-guided Robust Federated Learning against Poisoning Attacks,"Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning. However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance. These attacks can introduce critical malfunctions, such as biased predictions or reduced accuracy, undermining the integrity and robustness of the global model. Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems. However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data. These tasks, which often involve sequential dependencies and temporal patterns, have been largely overlooked in the context of poisoning attack defenses.In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants. Based on our investigation of the effectiveness of poisoning attack defenses within the Federated Time Series (FTS) domain, we pinpoint the limitations of mainstream defenses against such attacks. Unlike traditional model-centric defenses, FLORAL leverages logic reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates. Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates. Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior. Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications. Notably, FLORAL reduced the prediction error by 93.27% in the best-case scenario compared to the second-best baseline. Our code is available at https://anonymous.4open.science/r/FLORAL-Robust-FTS.","Federated Learning (FL) has emerged as a promising solution that enables using data and computing resources from multiple clients to train a shared model under the orchestration of a central server [33]. In FL, clients use their data to train the model locally and iteratively share the local updates with the server, which then combines the contributions of the participating clients to generate a global update. The security aggregation mechanism and its distinctive distributed training mode render it highly compatible with a wide range of practical applications that have stringent privacy demands [49, 59, 40, 21]. Recently, FL has been demonstrated to be efficient in time-series related tasks [10, 48, 3] to securely share knowledge of similar expertise among different tasks and protect user privacy. Although FL has many notable characteristics and has been successful in many applications [2, 21, 46, 52, 66, 22, 41], recent studies indicate that FL is fundamentally susceptible to adversarial attacks in which malicious clients manipulate the local training process to contaminate the global model [6, 55, 44]. Based on the attack‚Äôs goal, adversarial attacks can be broadly classified into untargeted and targeted attacks. The former aims to deteriorate the performance of the global model on all test samples [9, 14]; while the latter focuses on causing the model to generate false predictions following specific objectives of the adversaries [62, 6]. Figure 1: Illustration of logical verification given by benign and malicious clients‚Äô predictions. The global property here is ‚ñ°(0,10]‚Å¢(y^‚Å¢(t)‚â§p1)‚àß‚ñ°(10,20]‚Å¢(y^‚Å¢(t)‚â§p2)‚àß‚ñ°(20,30]‚Å¢(y^‚Å¢(t)‚â§p3)‚àß‚ñ°(30,40]‚Å¢(y^‚Å¢(t)‚â§p4)subscript‚ñ°010^ùë¶ùë°subscriptùëù1subscript‚ñ°1020^ùë¶ùë°subscriptùëù2subscript‚ñ°2030^ùë¶ùë°subscriptùëù3subscript‚ñ°3040^ùë¶ùë°subscriptùëù4\square_{(0,10]}(\hat{y}(t)\leq p_{1})\wedge\square_{(10,20]}(\hat{y}(t)\leq p% _{2})\wedge\square_{(20,30]}(\hat{y}(t)\leq p_{3})\wedge\square_{(30,40]}(\hat% {y}(t)\leq p_{4})‚ñ° start_POSTSUBSCRIPT ( 0 , 10 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ‚àß ‚ñ° start_POSTSUBSCRIPT ( 10 , 20 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ‚àß ‚ñ° start_POSTSUBSCRIPT ( 20 , 30 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) ‚àß ‚ñ° start_POSTSUBSCRIPT ( 30 , 40 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ). Examples of points violating this property are marked with x. Many efforts have been devoted to dealing with existing threats in FL, which can be roughly classified into two directions: robust FL aggregation [50, 47, 69, 71] and anomaly model detection. The former aims to optimize the aggregation function to limit the effects of polluted updates caused by attackers, whereas the latter attempts to identify and remove malicious updates. For instance, Xie et al. [69] presented a certified defense mechanism based on the clipping and perturbation paradigm. Other approaches focused on new estimators such as coordinate-wise median, Œ±ùõº\alphaitalic_Œ±-trimmed mean [72], and geometric median [50] for aggregation. The main drawback of the methods mentioned above is that polluted updates remain in the global model, reducing the model‚Äôs precision while not mitigating the attack impact [43]. Several methods have been proposed to identify and remove adversarial clients from the aggregation [9, 60, 42, 54, 17, 73]. In [60], the authors proposed a defense mechanism against poisoning attacks in collaborative learning based on the KùêæKitalic_K-Means algorithm. Sattler et al. [57] proposed dividing the clients‚Äô updates into normal updates and suspicious updates based on their cosine similarities. However, most methods for identifying malicious clients proposed so far follow the majority-based paradigm in that they assume benign local model updates are a majority compared to the malicious ones; thus, polluted updates are supposed to be outliers in the distribution of all updates. Unfortunately, this hypothesis holds only if the data of the clients is IID (independent and identically distributed) and the number of malicious clients is small. Though these two approaches can mitigate poisoning attacks in FL, most of them have been evaluated primarily in the context of computer vision tasks, where image-based datasets dominate the landscape [17, 47, 44, 63]. However, FL applied to time-series data remains underexplored, particularly regarding its vulnerabilities, where adversarial attacks pose a significant threat, much like those observed in image-based datasets [23, 12, 13, 38]. Given the critical applications of time-series analysis, such as in healthcare [5, 36], financial systems [35, 34], and industrial monitoring [31, 30], ensuring the robustness and security of FL models in these scenarios is of paramount importance. Our empirical result demonstrates that these methods are not effective in the scenario of FL with time-series tasks where the data itself reflects a high level of non-iid due to the different locations where it is collected. To fill this gap, we propose FLORAL, a defense mechanism capable of mitigating poisoning attacks against Federated Time Series (FTS) under the most challenging scenarios, i.e., in the presence of heterogeneous client data and a large number of adversarial clients. Our approach is orthogonal to existing model-centric defenses. Instead, we rely on logic-based reasoning to evaluate the reliability of clients based on their behavior and resistance to poisoning attacks. This approach assesses the trustworthiness of clients by aligning their predictions with global time-series patterns. Specifically, we use symbolic reasoning to capture the logical semantics embedded in time series data, which has been shown to improve the learning process and produce more robust models for future predictions [3, 31, 29]. Our FL defense method builds on this by using symbolic reasoning to evaluate diverging intra-task logic patterns in client predictions, allowing for the detection of anomalous clients without relying solely on model similarity. This highlights the enhanced effectiveness of reasoning logic in identifying malicious behaviors in FL. The intuition behind our approach is that, after rounds of training, benign models naturally converge toward the same global objective and share consistent logical reasoning patterns, while malicious models diverge, aiming to manipulate global behavior and thereby exhibit deviant reasoning patterns. The high-level idea is visualized in Figure 1. In centralized FL, we expect the final model GTsubscriptùê∫ùëáG_{T}italic_G start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT to have the minimized error on the local data, with local models converging on a unified objective [49, 27, 24]. In light of these findings, we propose FLORAL, a logic-guided defense for FL which includes three key components. First, we extract logical reasoning properties (e.g., when training models relating to traffic and driving, a dataset measuring vehicle density over time would by expected to reach an upper extreme value of, say, 100) from clients and apply hierarchical clustering to group client updates based on the logical properties of their local models. This allows us to infer the global reasoning properties that represent the system‚Äôs clients based on clustered properties. These formal logic properties rigorous assessment of the consistency and validity of client contributions by identifying deviations from expected model behaviors. This verification-based defense substantially strengthens the security of federated learning in time-series applications, where the risk of undetected adversarial behavior is particularly high due to the sensitive nature of these tasks. By optimizing for the unique challenges of time-series data, our method enhances the robustness of FL systems, providing a more reliable safeguard compared to existing defenses. Experimental results validate the effectiveness of our approach in mitigating poisoning attacks while maintaining high model performance. In summary, our contributions are specified as follows: ‚Ä¢ We introduce FLORAL ‚Äî a novel poisoning-resistant defense for FL. It identifies and eliminates suspicious clients that distort the global model using logical reasoning property inference and verification. FLORAL is the first work that, to the best of our knowledge, thoroughly addresses poisoning attacks in FTS, even in the presence of a large number of compromised participants and complicated attack strategies. ‚Ä¢ We are the first to study the efficacy of existing robust FL defenses in the context of FTS and pinpoint their limitation when adapted to the time-series domain. ‚Ä¢ We conduct comprehensive experiments and in-depth studies on various datasets, FL settings, and attack scenarios to demonstrate the superiority of FLORAL over state-of-the-art defense techniques."
https://arxiv.org/html/2411.02597v1,Taming the Beast of User-Programmed Transactions on Blockchains: A Declarative Transaction Approach,"Blockchains are being positioned as the ‚Äùtechnology of trust‚Äù that can be used to mediate transactions between non-trusting parties without the need for a central authority. They support transaction types that are native to the blockchain platform or user-defined via user programs called smart contracts. Despite the significant flexibility in transaction programmability that smart contracts offer, they pose several usability, robustness and performance challenges.This paper proposes an alternative transaction framework that incorporates more primitives into the native set of transaction types (reducing the likelihood of requiring user-defined transaction programs often). The framework is based on the concept of declarative blockchain transactions whose strength lies in the fact that it addresses several of the limitations of smart contracts, simultaneously. A formal and implementation framework is presented and a subset of commonly occurring transaction behaviors are modeled and implemented as use cases, using an open-source blockchain database, BigchainDB as the implementation context. A performance study comparing the declarative transaction approach to equivalent smart contract transaction models reveals several advantages of the proposed approach.","Blockchains, as a technology for mediating and managing transactions between non-trusting parties, is becoming an increasingly popular concept. They are decentralized, fully replicated, append-only databases of transactions that are validated through a large, distributed consensus. These characteristics ensure that blockchain contents are tamper-proof and that no single authority controls a blockchain‚Äôs operation and contents, conferring a good degree of trust in them. Initially aimed at cryptocurrency, blockchain technology now extends to areas seeking data control and ownership decentralization, primarily for privacy and efficiency. This includes healthcare, (Agbo et al., 2019; McGhin et al., 2019), supply chain (Durach et al., 2021; Wu et al., 2019; Sund et al., 2020), decentralized finance (Defi) (Werner et al., 2021; Siyal et al., 2019), governance (Lumineau et al., 2021), web browsing, gaming, social media, and file sharing/storage (Al-Jaroodi and Mohamed, 2019). Blockchain transactions typically involve digital asset management aligned with business activities. The fundamental transaction type is asset ùñ≥ùñ±ùñ†ùñ≠ùñ≤ùñ•ùñ§ùñ±ùñ≥ùñ±ùñ†ùñ≠ùñ≤ùñ•ùñ§ùñ±\mathsf{TRANSFER}sansserif_TRANSFER between accounts, a native function in most blockchains. To address the diverse needs of modern applications, blockchains have evolved to include user-designed transactions known as smart contracts (Szabo, 1997). These contracts execute business operations and adhere to specific conditions. Examples include auction bidding and regulated patient record management. Recent survey (num, 2022) indicates the existence of over 44 million smart contracts on the Ethereum blockchain alone. Problem: Smart contracts, despite their flexibility, face adoption barriers due to several issues: (i) They require significant effort in creation and verification, offer limited reusability across platforms, and constrain automatic optimization possibilities. (ii) Vulnerable to user errors and security breaches, they pose financial risks, exemplified by the DAO attack (Mehar et al., 2019) that resulted in a loss of approximately 3.6M ETH (about $6.8B). (iii) Many transactional behaviors in smart contracts, embedded in programming structures, remain hidden on the blockchain, hindering their utility in complex data analysis. (iv) Their execution involves higher latency and costs compared to native transactions. The lack of validation semantics for these user-programmed transactions complicates concurrency conflict management, leading most platforms, including Ethereum, to adopt sequential execution, which lowers throughput. Declarative smart contracts (Chen et al., 2022), domain-specific languages (W√∂hrer and Zdun, 2020a), and smart contract templates (Hu et al., 2020) aim to ease creation and verification processes. However, they fall short in addressing performance, throughput, queryability, and other transactional model challenges in smart contracts. 1.1. Contributions: This paper investigates the feasibility and impact of lifting transactional behaviors typically found in smart contracts into the core blockchain layer as native transactions. Specifically, we propose: (1) a declarative and typed blockchain transaction model that includes the novel concept of nested blockchain transactions, as a foundation for modeling transactional behavior on blockchains. (2) concrete declarative blockchain transaction modeling of a sample transactional behavior represented in many smart contracts of the most popular blockchain application category - marketplaces. (3) an implementation framework for declarative blockchain transactions that builds on BigchainDB blockchain database‚Äôs architecture (big, [n. d.]), extending its transaction modeling and validation infrastructure. (4) a comparative performance and usability evaluation of the declarative transaction model vs. the smart contract model using Ethereum smart contracts as the baseline. The evaluation results demonstrate that the declarative transaction method significantly outperforms smart contracts, achieving improvements by a factor of 635 in latency and a minimum of 60 in throughput. The rest of the paper is organized as follows: Section 2 provides background information on blockchain native transactions, smart contracts, and BigchainDB. Section 3 introduces the formal blockchain transaction model and novel concepts of Non-nested and Nested transactions. Section 4 provides implementation details of the concepts presented in Section 3. Section 6 reviews the literature on the topic, while Section 5 reports on the comparative experiments conducted to evaluate our system and smart contract. Finally, we conclude the paper with a summary in Section 8."
https://arxiv.org/html/2411.01830v1,FaaSTube: Optimizing GPU-oriented Data Transfer for Serverless Computing,"Serverless computing has gained significant traction for machine learning inference applications, which are often deployed as serverless workflows consisting of multiple CPU and GPU functions with data dependency. However, existing data-passing solutions for serverless computing primarily reply on host memory for fast data transfer, mandating substantial data movement and resulting in salient I/O overhead. In this paper, we present FaaSTube, a GPU-efficient data passing system for serverless inference. FaaSTube manages intermediate data within a GPU memory pool to facilitate direct data exchange between GPU functions. It enables fine-grained bandwidth sharing over PCIe and NVLink, minimizing data-passing latency for both host-to-GPU and GPU-to-GPU while providing performance isolation between functions. Additionally, FaaSTube implements an elastic GPU memory pool that dynamically scales to accommodate varying data-passing demands. Evaluations on real-world applications show that FaaSTube reduces end-to-end latency by up to 90% and achieves up to 12x higher throughput compared to the state-of-the-art.","The widespread adoption of Machine Learning (ML) inference applications has heightened the demand for efficient inference service systems (Han et al., 2022; Jeong et al., 2023; Choi et al., 2022b; Zhang et al., 2023). Recent research suggests deploying inference applications on GPU-enabled serverless platforms (Yang et al., 2022; Ali et al., 2020; Romero et al., 2021; Li et al., 2022b; Wu et al., 2024; Yu et al., 2023b). With serverless computing, users package ML models as functions and leave resource provisioning and scaling to the serverless platform. This approach enables users to concentrate on the development of application logic while maintaining resource efficiency, eliminating the need for overprovisioning. In real-world scenarios, inference applications often stitch together multiple models and operations into a workflow. Table 1 presents several representative inference applications from recent research (Agarwal and Netravali, 2023; Gunasekaran et al., 2022; Shubha and Shen, 2023; Crankshaw et al., 2020; Hu et al., 2021). As a concrete example, in a traffic monitoring application (Shubha and Shen, 2023) that analyzes pedestrian and vehicle traffic (Fig. 1), video frames are first decoded and preprocessed. A detection model then extracts objects from these frames. The sub-images of detected pedestrians and vehicles are subsequently passed to two separate recognition models for further analysis of behavior and type. Inference applications in serverless computing are structured as serverless inference workflows. These workflows are hybrid, consisting of GPU functions (gFunc), CPU functions (cFunc), and their data dependencies. Figure 1. A typical traffic analysis application (Shubha and Shen, 2023; Agarwal and Netravali, 2023). Figure 2. Comparison of host-oriented inter-function data passing and our GPU-oriented inter-function data passing. Serverless inference workflows involve various types of data passing. In addition to typical cFunc-to-cFunc data passing, there are host-to-gFunc (where the host represents either a cFunc or I/O data in host memory) and gFunc-to-gFunc data passing. Unfortunately, current serverless systems rely on a Host-oriented data passing approach (Fig. 2(a)), where intermediate data is stored and exchanged via host memory (i.e., host-side external storage (Li et al., 2024; Mahgoub et al., 2021; Jia and Witchel, 2021; Yu et al., 2023a; Liu et al., 2024)). This host-oriented method overlooks the decoupling of GPU and host memory and the potential of various connections in GPU servers, resulting in substantial unnecessary overhead. For instance, in gFunc-to-gFunc data passing, this method ignores direct NVLinks between GPUs, requiring multiple sequential copies: data is first copied to host memory and then back to the target GPU, leading to large overhead (92% of the total latency in our experiments). Similarly, for host-to-gFunc data passing, this method only uses a single PCIe link, neglecting the parallel PCIe links available within GPU servers. For instance, parts of the data can be routed to neighboring GPUs via NVLink and then transferred in parallel to host memory through the neighboring GPUs‚Äô PCIe links. Our key idea is to introduce a GPU-oriented data-passing approach for serverless inference workflows. This approach allows data to be stored and exchanged directly in GPU memory, avoiding redundant transfers between GPU and host memory. It also leverages various connections within GPU servers (e.g., parallel PCIe links and NVLinks) to accelerate inter-function data passing, specifically focusing on host-to-gFunc and gFunc-to-gFunc data passing in this paper. To design an efficient GPU-oriented data-passing framework for serverless functions, the following requirements must be met: (1) Bandwidth efficiency: The available bandwidth of the various connections must be fully utilized while avoiding contention among concurrent functions. (2) Topology awareness: There are different connection topologies in modern GPU servers, requiring careful management of transfer scheduling based on the specific GPU topology during inter-function data passing. (3) GPU memory efficiency: As highlighted by existing research (Jung et al., 2023; Choi et al., 2022a), GPU memory is a limited resource; therefore, its usage should be minimized when providing GPU data storage for functions. (4) Transparent deployment: All of the above should be achieved transparently to users, simplifying the development of serverless inference workflow and hiding the complexities of various connections and GPU topologies involved. Based on these requirements, we propose FaaSTube, a GPU-efficient data transfer framework designed for serverless inference workflows. FaaSTube acts as a transparent ‚Äùtube‚Äù across GPUs, facilitating efficient data storage and transfer for functions. It comprises three key components: First, to simplify the management of various data passing in serverless inference workflows, we provide a unified data-passing interface and data index. This allows users to focus on application logic while offloading the complexities of data transfer implementation to FaaSTube. It automatically locates intermediate data within GPU servers and selects the appropriate connections (PCIe, NVLink, or network) and transfer methods (e.g., parallel or pipelined) to deliver data to the requesting function. Second, in order to fully utilize connections such as PCIe and NVLink for serverless functions, we design effective transfer scheduling mechanisms. (1) To achieve interference-free sharing of PCIe connections among concurrent functions on a shared GPU server, FaaSTube proposes fine-grained PCIe bandwidth isolation. Native GPU PCIe scheduling is not optimized for parallel transfers, and related works (Jeong et al., 2023; Fu et al., 2024) focus on exclusive inference systems while overlooking scenarios where multiple functions share a GPU server. FaaSTube manages data transfers across all PCIe connections, flexibly partitioning bandwidth based on each function‚Äôs SLO and controlling bandwidth usage during transfers. In addition, FaaSTube employs circular pinned memory buffers to enhance PCIe transfer efficiency while minimizing allocation overhead. (2) To ensure robust inter-function data transfer performance across different GPU topologies, FaaSTube implements topology-aware parallel NVLink transfer scheduling. In non-uniform topologies, many GPUs with limited direct NVLink bandwidth can severely restrict the performance of point-to-point (i.e., gFunc-to-gFunc) transfers in serverless inference workflows. Existing works (Ranganath et al., 2021; Cai et al., 2021; Wang et al., 2020) typically optimize task placement but still rely on a single direct NVLink path, failing to address this issue comprehensively. Other multi-GPU communication methods (NVIDIA Collective Communications Library NCCL, [n. d.]; Shah et al., 2023; Kim et al., 2024) focus on collective communication and often ignore point-to-point transfers, also utilizing only one NVLink path. FaaSTube identifies multiple parallel NVLink paths between bandwidth-constrained GPUs, employing contention-aware path selection to accelerate inter-function data passing. Third, to efficiently manage GPU memory and intermediate data in GPU data store for serverless functions, FaaSTube proposes an auto-scaling GPU memory pool, as function workloads and the size of intermediate data (e.g., the number of objects in a video frame) can vary dynamically. Existing GPU memory management systems (Guo et al., 2024; Paszke et al., 2019; Jung et al., 2023) are designed for long-running, exclusive GPU tasks like ML training. Therefore, they often retain a lot of idle memory blocks and lack flexible reclamation mechanisms, leading to excessive memory usage in serverless environments. FaaSTube tracks data storage requirements in real-time and caches only the necessary memory blocks in GPU data store, dynamically resizing the memory pool. Furthermore, when intermediate data accumulates in GPU memory, FaaSTube implements a smart data migration based on function request queue, migrating data to host memory and adaptively prefetching data back to the GPU, thereby effectively alleviating memory pressure without compromising performance. We implement FaaSTube on top of INFless (Yang et al., 2022), a recent serverless inference platform built on OpenFaaS. We evaluate FaaSTube on various real-world inference workflows using Azure cloud traces (Shahrad et al., 2020). The results show that FaaSTube reduces end-to-end latency by up to 90% and achieves up to 12X higher throughput compared to state-of-the-art serverless systems. In addition, we also present the scalability and effectiveness of FaaSTube on a 4-node cluster and GPU servers with various GPU topologies."
https://arxiv.org/html/2411.01791v1,Minder: Faulty Machine Detection for Large-scale Distributed Model Training,"Large-scale distributed model training requires simultaneous training on up to thousands of machines. Faulty machine detection is critical when an unexpected fault occurs in a machine. From our experience, a training task can encounter two faults per day on average, possibly leading to a halt for hours. To address the drawbacks of the time-consuming and labor-intensive manual scrutiny, we propose Minder, an automatic faulty machine detector for distributed training tasks. The key idea of Minder is to automatically and efficiently detect faulty distinctive monitoring metric patterns, which could last for a period before the entire training task comes to a halt. Minder has been deployed in our production environment for over one year, monitoring daily distributed training tasks where each involves up to thousands of machines. In our real-world fault detection scenarios, Minder can accurately and efficiently react to faults within 3.6 seconds on average, with a precision of 0.904 and F1-score of 0.893.","Recent years have witnessed a rapid increase in dataset sizes and the number of parameters in models, especially in Large Language Models (LLMs). The GPT-4 model [18], an instance of the Mixture-of-Experts (MoE) paradigm, demonstrates this growth with its 1.8T parameters. Other latest models also exhibit this trend, with parameter counts exceeding 500 billion [69, 24]. The feasibility of training such extensive models efficiently has been realized through large-scale machines and GPUs [39, 40]. It has also been accompanied by advancements in distributed model training [49, 68], high-performance collective communication[48, 65], and fault-tolerant techniques [36]. A system of such vast size and complexity involves a huge amount of computation, communication, and storage resources as well as software support for a task. Consequently, the potential for faults is high, leading to the possibility of task failure. Faulty machine detection thus has become a significant bottleneck in the maintenance of distributed tasks. In our production environment, an accidental hardware or software fault occurs twice a day on average. The entire task may be forced to stop for hours or days until fixed for retraining. The economic loss for a customer can reach more than $1700 in a 128-machine task for 40 minutes (case in 2.1). Training a GPT-2 model with 1.5 billion parameters and 40GB dataset [67], for instance, takes 200 days utilizing an NVIDIA V100 GPU [10] (or 12 days for a DGX-2H server). If the training process is frequently interrupted by such faults, operating expenses and time costs will increase significantly. However, the current manual diagnosis method is unsatisfactory. Once a halted task notification is received, the engineer needs to check the training parameters. Meanwhile, engineers from the training, physical networking, storage, and hardware teams, are also involved in diagnosis, since a fault can occur in any machine component. Examining machine logs and conducting offline performance tests on relevant hardware devices are required until the fault is detected (usually for hours). Delayed notifications, incomplete log content, and the complicated process of manual diagnosis amplify the unpredictability of time and labor costs. It‚Äôs necessary to design effective and accurate faulty machine detection methods that can quickly react to faults at runtime, not only providing better reliability but also eliminating manual efforts. Achieving such goals is challenging because a machine can fail due to various types of faults. These faults can occur in any possible component, including hardware and software, and can be intra-host or inter-host. Besides, the abnormal pattern of monitoring metrics varies from task to task, making the traditional supervised anomaly detection methods impractical, because even the same behavior might be abnormal in a task with a different workload and machine scale. Additionally, there isn‚Äôt an individual monitoring metric that necessarily signals a fault. For instance, CPU or GPU usage is the most sensitive metric for fault indication, based on our observation from real production data. However, neither one is guaranteed to identify the faulty machine for Error Correction Code (ECC) errors. If noises exist in monitoring data, the detection may be even misguided. Therefore, faulty machine detection for distributed training is challenging. Instead of creating a monolithic predictor with available monitoring data, we developed Minder by leveraging the ideas of similarity (3.1), continuity (3.2), training individual models for each monitoring metric (3.3), and metric prioritization (3.4). Minder resolves the challenges by recognizing that a machine with a fault displays an abnormal pattern in certain metric data that differs from other machines and lasts for a duration. We also train individual models for data denoising. We then track the dissimilarity between the denoised data for each machine and monitor its duration. By repeating this to individual metrics, the faulty machine is detected. To further expedite detection, we prioritize metrics to identify the most sensitive ones when a fault occurs. We designed, implemented, and deployed Minder for all the distributed training tasks. Minder operates without interrupting the running of the training machines, only requiring the pulling of monitoring data from the Data APIs for backend run-time analysis. Host metrics used by Minder cover the aspects of computation, communication, and storage. Manual labors are released from the debugging process since Minder can react within 3.6 seconds (6.1), reducing over 99% of the time of manual debugging (shorter time by 500 √ó). Minder has an overall precision and F1-score of 0.904 and 0.893. We make the following contributions. ‚Ä¢ An investigation of the fault types and their correlations with various monitoring metrics (2.3). We empirically explain why some metrics are more sensitive to faults and outline the challenges for the detection (2.4). ‚Ä¢ The ideas of similarity, continuity, denoising models, and metric prioritization for the design of Minder (3, 4). Our thorough evaluation of Minder‚Äôs implementation (5) and and ablation experiments highlight its fast reaction, high accuracy, and proper design choices (6). ‚Ä¢ Lessons we encountered when deploying Minder in practical (7). We also point out future directions."
https://arxiv.org/html/2411.01738v1,xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism,"Diffusion models are pivotal for generating high-quality images and videos. Inspired by the success of OpenAI‚Äôs Sora (sora2024, ), the backbone of diffusion models is evolving from U-Net to Transformer, known as Diffusion Transformers (DiTs). However, generating high-quality content necessitates longer sequence lengths, exponentially increasing the computation required for the attention mechanism, and escalating DiTs inference latency. Parallel inference is essential for real-time DiTs deployments, but relying on a single parallel method is impractical due to poor scalability at large scales.This paper introduces xDiT, a comprehensive parallel inference engine for DiTs. After thoroughly investigating existing DiTs parallel approaches, xDiT chooses Sequence Parallel (SP) and PipeFusion, a novel Patch-level Pipeline Parallel method, as intra-image parallel strategies, alongside CFG parallel for inter-image parallelism. xDiT can flexibly combine these parallel approaches in a hybrid manner, offering a robust and scalable solution. Experimental results on two 8√ó\times√óL40 GPUs (PCIe) nodes interconnected by Ethernet and an 8√ó\times√óA100 (NVLink) node showcase xDiT‚Äôs exceptional scalability across five state-of-the-art DiTs. Notably, we are the first to demonstrate DiTs scalability on Ethernet-connected GPU clusters. xDiT is available at https://github.com/xdit-project/xDiT.","In recent years, diffusion models have emerged as a groundbreaking technique in image (blackforestlabs2023announcement, ; esser2024scaling, ; chen2024pixart, ; peebles2023scalable, ) and video generation (sora2024, ; meta2024moviegen, ). These models create images or videos through a multi-step denoising process, leveraging a neural network model at each step. We are witnessing a transformative shift in the architecture of the denoising networks. While traditionally dominated by U-Net (ronneberger2015u, ) architectures, these networks are now evolving into Diffusion Transformers (DiTs) (peebles2023scalable, ), renowned for their superior model capacity and scalability. Images are encoded into a token sequence as input to DiTs (rombach2022high, ). The attention mechanisms of DiTs require mutual computation between tokens, leading to computation that scales quadratically with sequence length. Unlike Large Language Models (LLMs), which can generate tokens on the fly, DiTs must complete all steps to produce the final results. This poses significant challenges for real-time deployment. For example, a Sora-like video generation API (mochi1preview_guide, ; kuaishou2024, ) takes over 4 minutes to generate a fewer seconds video. Given the immense computational demands, real-time DiTs deployment inevitably necessitates parallelism across multiple computing devices. However, there is still a lack of effective methodology to scale DiTs to large scales. Although, several sequence parallelism (SP) methods (jacobs2023deepspeed, ; liu2023ring, ; fang2024unified, ; gu2024loongtrain, ) have been developed to scale long-sequence DiTs inference. Additionally, some approaches leverage input temporal redundancy (so2024frdiff, ; ma2024deepcache, ; ma2024learning, ; yuan2024ditfastattn, ; zhao2024real, ; meta2024moviegen, ), indicating a high degree of similarity in both inputs and activations across successive diffusion time steps, to design asynchronous sequence parallelism (li2023distrifusion, ) and patch-level sequence parallelism (wang2024pipefusion, ). However, using these methods in isolation fails to adapt to the underlying heterogeneous interconnects of computational devices. For instance, methods using collective communication are only suitable for high-bandwidth interconnects like NVLink, while methods using P2P (Peer-to-Peer) are more suitable for PCIe or Ethernet but do not have an advantage on NVLink. Additionally, the diversity of DiTs model architectures presents challenges for parallel implementation. Unlike LLMs, which generally have a more uniform architecture, DiTs exhibit greater variability. For instance, the method of injecting user input instructions, the connection methods of DiTs blocks, and the operators‚Äô layouts in Transformer blocks can vary significantly. This diversity means that directly applying methods designed for LLMs, such as TP and SP, may not be immediately suitable for DiTs. This paper introduces xDiT, a parallel inference system designed for DiTs. We argue that DiTs inference is analogous to the training of LLMs in that a single parallel is unlikely to scale effectively across various model architectures and network hardware, especially for those heterogeneous low-bandwidth networks. Therefore, a hybrid approach combining multiple parallel methods is necessary to achieve optimal communication efficiency. Our contributions are as follows: ‚Ä¢ We systematically investigate existing parallel methods for DiTs (including two types of sequence parallelism, DistriFusion, tensor parallelism, and PipeFusion), ‚Ä¢ We selected PipeFusion and sequence parallelism for intra-image parallelism and CFG parallelism for inter-image parallelism. We designed a method to correctly hybrid these parallel approaches, thereby scaling DiTs inference to a very large scale. ‚Ä¢ Using the aforementioned methods, we built a system called xDiT. On 16√ó\times√óL40 PCIe and 8√ó\times√óA100 NVLink devices, we studied the scalability of five image and video generation DiTs (Pixart, Stable-Diffusion3, Flux.1, HunyuanDiT, CogVideoX). xDiT is the first system to successfully scale DiTs inference to 16 GPUs."
https://arxiv.org/html/2411.01611v1,Stochastic Communication Avoidance for Recommendation Systems,"One of the major bottlenecks for efficient deployment of neural network based recommendation systems is the memory footprint of their embedding tables. Although many neural network based recommendation systems could benefit from the faster on-chip memory access and increased computational power of hardware accelerators, the large embedding tables in these models often cannot fit on the constrained memory of accelerators. Despite the pervasiveness of these models, prior methods in memory optimization and parallelism fail to address the memory and communication costs of large embedding tables on accelerators. As a result, the majority of models are trained on CPUs, while current implementations of accelerators are hindered by issues such as bottlenecks in inter-device communication and main memory lookups. In this paper, we propose a theoretical framework that analyses the communication costs of arbitrary distributed systems that use lookup tables. We use this framework to propose algorithms that maximize throughput subject to memory, computation, and communication constraints. Furthermore, we demonstrate that our method achieves strong theoretical performance across dataset distributions and memory constraints, applicable to a wide range of use cases from mobile federated learning to warehouse-scale computation. We implement our framework and algorithms in PyTorch and achieve up to 6√ó6\times6 √ó increases in training throughput on GPU systems over baselines, on the Criteo Terabytes dataset.","A significant portion of machine learning research has advanced due to better memory and computational speeds of accelerators, alongside faster interconnects and more efficient parallelization in large systems. However, accelerators often have limited memory compared to CPUs, rendering many memory-intensive algorithms infeasible for deployment. One approach to mitigate this issue is to increase memory, but this cannot keep up with the rapid growth of machine learning models. An alternative is to develop new parallelization strategies that balance memory usage and communication, as explored in strategies like those in [1, 2, 3]. Another optimization strategy involves quantization [4, 5, 6], which aims to minimize the memory footprint and computational requirements without significantly impacting accuracy. Embedding tables, which map sparse categorical features to dense vectors [7, 8], are often prime targets for quantization due to their large sizes. By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10, 11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference. These methods primarily focus on reducing the embedding size [13], while another solution is to modify the training process to save memory. Examples of this approach include reversible networks [14], which change the model‚Äôs structure, and techniques like Checkmate [15], which alter the model‚Äôs execution pattern by adding additional operations to backpropagation to decrease the number of intermediate values that need to be stored in memory. Recent algorithms have significantly increased the scale of NLP and CV models by reducing the memory demands per GPU [16, 17, 18], allowing the use of accelerators for extremely large models. However, these methods struggle with models that have large embedding tables, which are not easily managed by pipeline parallelism and remain large in parameter count. Data parallelism also falls short as it is better suited for compute-heavy tasks rather than memory-intensive embedding operations. Furthermore, techniques like recomputation or checkpointing do not suit embeddings well, as their high memory cost does not justify the modest savings from managing intermediate activations. This forces the use of model parallelism, but oftentimes the number of accelerators required to fit the large embedding tables is too great to make model parallelism a financially viable solution. These embeddings can often be terabytes large, but as observed in practice, they are not accessed uniformly at random. In real datasets, the access pattern of these embeddings varies, generally with a small portion of embeddings being accessed far more frequently than others[19]. Existing methods [20] have explored the usage of distributed communication to decrease the communication cost, but the theoretical bounds of communication efficiency has not been analyzed in previous work. In addition, there has been no existing work exploring how various methods of distributing embeddings across GPUs and CPUs impact the system performance. To summarize, our contributions are as follows: 1. We develop a simple framework to calculate the expected communication cost under a training regime. And we expand this framework to address considerations such as determining the optimal levels of communication and caching, as well as methods for adjusting them accordingly. 2. We use the above framework to obtain communication strategies that minimize expected communication costs without caching. We demonstrate that these methods also decrease main memory I/O proportionally to the decrease in communication. 3. We demonstrate how assumptions about ML training motivate different strategies for caching through empirical analysis with our framework. 4. We extensively test our algorithms on a variety of datasets and models. Furthermore, we also test various theoretical distributions and observe that our algorithms can generalize well, with performance gains on a wide range of potential synthetic datasets."
https://arxiv.org/html/2411.01579v1,Flexible Coded Distributed Convolution Computing for Enhanced Fault Tolerance and Numerical Stability in Distributed CNNs,"Deploying Convolutional Neural Networks (CNNs) on resource-constrained devices necessitates efficient management of computational resources, often via distributed systems susceptible to latency from straggler nodes. This paper introduces the Flexible Coded Distributed Convolution Computing (FCDCC) framework to enhance fault tolerance and numerical stability in distributed CNNs. We extend Coded Distributed Computing (CDC) with Circulant and Rotation Matrix Embedding (CRME) which was originally proposed for matrix multiplication to high-dimensional tensor convolution. For the proposed scheme, referred to as Numerically Stable Coded Tensor Convolution (NSCTC) scheme, we also propose two new coded partitioning schemes: Adaptive-Padding Coded Partitioning (APCP) for input tensor and Kernel-Channel Coded Partitioning (KCCP) for filter tensor. These strategies enable linear decomposition of tensor convolutions and encoding them into CDC subtasks, combining model parallelism with coded redundancy for robust and efficient execution. Theoretical analysis identifies an optimal trade-off between communication and storage costs. Empirical results validate the framework‚Äôs effectiveness in computational efficiency, fault tolerance, and scalability across various CNN architectures.","In the rapidly evolving domain of distributed machine learning, Convolutional Neural Networks (CNNs) have become fundamental due to their exceptional capabilities in image feature extraction and classification [1, 2, 3], as well as their versatility enabled by transfer learning [4, 5, 6]. A significant trend in this field, particularly relevant to Internet of Things (IoT) applications, is the shift towards edge computing, where data processing is conducted directly on edge devices [7]. This paradigm reduces dependency on cloud resources, minimizing latency [8] and enhancing data privacy [9]. Deploying CNNs in distributed systems, especially on resource-constrained devices, poses significant challenges due to intensive computational requirements, particularly within convolutional layers (ConvLs). Convolution operations represent over 90% of the Multiply-Accumulate operations (MACs) in mainstream CNN architectures[10], including AlexNet [11], VGGNet [12], GoogleNet [13] and ResNet [14], and account for more than 80% of the computational time during inference [15]. Collaborative inference across multiple devices has emerged as a viable approach to mitigate the computational burden on individual devices and enhance CNN deployment efficiency [16]. However, inference latency is often significantly impacted by slow worker nodes, commonly referred to as stragglers. These stragglers, arising from hardware heterogeneity and variable network conditions [17], can lead to performance degradation and potential failures, particularly in IoT systems where data loss rates may exceed 70% per layer [15]. Coded Distributed Computing (CDC) has been introduced to enhance computational resilience and efficiency in distributed systems. By dividing a task into mùëömitalic_m subtasks and introducing redundancy through an (n,m)ùëõùëö(n,m)( italic_n , italic_m ) error correction code (with n>mùëõùëön>mitalic_n > italic_m workers), CDC allows the master node to obtain the desired solution by waiting for the fastest kùëòkitalic_k workers (m‚â§k<nùëöùëòùëõm\leq k<nitalic_m ‚â§ italic_k < italic_n), referred to as the recovery threshold, thereby significantly reducing computation time against the straggler effect. CDC has been successfully applied in Coded Matrix-Matrix Multiplication (CMMM) and other matrix multiplication-based algorithms [18, 19, 20, 21, 22, 23], due to the ease of linear decomposition of these operations. The application of CDC in distributed deep learning systems has garnered significant attention. Dutta et al. introduced a unified coded deep neural network (DNN) scheme utilizing Generalized PolyDot codes [24], which demonstrated the feasibility of deploying large DNNs on unreliable nodes prone to soft errors. Subsequent works, such as [25, 15], further integrated CDC at the model parallelism level in DNNs. However, these CMMM-based methods are not directly extensible to tensor convolutions in CNNs. Specifically, the decomposition of tensors ùêì‚àà‚Ñùd1√ó‚ãØ√ódnùêìsuperscript‚Ñùsubscriptùëë1‚ãØsubscriptùëëùëõ\mathbf{T}\in\mathbb{R}^{d_{1}\times\cdots\times d_{n}}bold_T ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT √ó ‚ãØ √ó italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUPERSCRIPT for CDC subtasks introduces multiple splitting dimensions, necessitating specialized partitioning schemes. Numerical stability is also a critical concern for CDC in DNNs due to the substantial propagation depth and cumulative errors associated with deep architectures [26]. Most existing CDC schemes‚Äîincluding Polynomial Codes [19], MatDot Codes [22], PolyDot Codes [20], and Lagrange Codes [23]‚Äîrely on polynomial methods that perform well over finite field, while encountering challenges in the real field. Specifically, the condition number of a real Vandermonde matrix, used in the recovery process during decoding of polynomial methods, grows exponentially with its dimension [27], exacerbating numerical instability. To address this issue, Ramamoorthy et al. enhanced numerical stability in CMMM by incorporating Circulant and Rotation Matrix Embeddings (CRME) into polynomial codes [28]. This approach exploits the well-conditioned nature of complex Vandermonde matrices with parameters on the unit circle while performing computations over the real field ‚Ñù‚Ñù\mathbb{R}blackboard_R. This advancement suggests the potential for employing CRME into the real-valued applications, such as tensor convolutions in CNNs. Integrating CDC into CNNs poses significant challenges due to the complex and tightly coupled nature of ConvLs. These layers involve intricate interactions between three-dimensional input tensors (feature maps) and four-dimensional filter tensors (kernels), requiring careful management during tensor decomposition to preserve spatial continuity at the model parallelism level [29]. Existing research on CDC within this field was limited to one-dimensional vector convolutions [30, 31], which were insufficient for CNN architectures. Recent efforts, such as the Resilient, Secure, and Private Coded Convolution (RSPCC) scheme by Qiu et al. [32], have sought to broaden CDC‚Äôs applicability to ConvLs by adapting the im2col-based algorithm [33], transforming tensor convolutions into matrix-vector multiplications. However, due to this specific pre-encoding transformation, RSPCC exhibits limited compatibility with alternative convolution techniques, including FFT-based methods [34], Winograd-based algorithms [35], and approximation methods [36]. Additionally, RSPCC requires each node to retain complete filter, thereby increasing storage requirement. It also relies on finite field computations, which introduce numerical stability challenges in real-valued environments [28]. Addressing these challenges is crucial for the effective deployment of CDC in large-scale distributed CNNs. New tensor block encoding and decoding strategies are required to manage high-dimensional structures, ensuring numerical stability while optimizing the trade-off between communication and storage costs [37]. This paper introduces a Flexible Coded Distributed Convolution Computing (FCDCC) framework designed specifically for ConvLs in CNNs within distributed environments. The framework enhances model parallelism through a numerically stable coded computing scheme, improving both efficiency and robustness while minimizing computational and memory requirements. The primary contributions of this work are as follows: ‚Ä¢ Numerically Stable Coded Tensor Convolution (NSCTC): We extend the CDC scheme based on Circulant and Rotation Matrix Embedding (CRME) from matrix multiplication to high-dimensional tensor convolution, by introducing a new tensor-matrix multiplication operation for encoding and decoding. This approach leverages complex Vandermonde matrices computed over the real field to achieve numerical stability, achieving a maximum mean squared error (MSE) of 10‚àí27superscript102710^{-27}10 start_POSTSUPERSCRIPT - 27 end_POSTSUPERSCRIPT for AlexNet‚Äôs ConvLs in a distributed setting with 20 worker nodes. This represents the first CDC scheme specifically designed for high-dimensional tensor operations. ‚Ä¢ Adaptive-Padding Coded Partitioning (APCP): We introduce an adaptive partitioning strategy that divides the input tensor along its spatial dimensions (height HùêªHitalic_H or width WùëäWitalic_W), based on kernel size (KH,KWsubscriptùêæùêªsubscriptùêæùëäK_{H},K_{W}italic_K start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_K start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT) and stride sùë†sitalic_s, with the addition of coded redundancy. This reduces communication cost and workload per node compared to single node scheme. ‚Ä¢ Kernel-Channel Coded Partitioning (KCCP): We employ a partitioning approach for the filter tensor along the output channel dimension NùëÅNitalic_N, generating coded partitions to enhance resilience, while also lowering storage cost and per-node workload compared to RSPCC scheme. ‚Ä¢ Framework Optimization: The FCDCC framework is then analyzed to determine optimal partitioning parameters kAsubscriptùëòùê¥k_{A}italic_k start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and kBsubscriptùëòùêµk_{B}italic_k start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, balancing communication and storage costs while maintaining a fixed number of subtasks (where kA‚Å¢kBsubscriptùëòùê¥subscriptùëòùêµk_{A}k_{B}italic_k start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT is constant). ‚Ä¢ Generality: The framework is applicable to CNN libraries such as PyTorch and various CNN models, including LeNet, AlexNet, and VGGNet. Figure 1: The FCDCC framework demonstrating the main workflow and tensor definitions in a comprehensive 3-D representation. We assume 3 kinds of straggler problems in this diagram: Upload failures, computing failures, and download failures. The remainder of the paper is organized as follows: Section II presents the system model, Section III introduces the NSCTC scheme, Section IV describes the FCDCC framework and cost optimization, Section V provides theoretical analysis and complexity evaluation, Section VI offers experimental validation, and Section VII concludes with future research directions. Notations: The imaginary unit is denoted by i=‚àí1i1\mathrm{i}=\sqrt{-1}roman_i = square-root start_ARG - 1 end_ARG. The set of integers modulo mùëömitalic_m is represented by ùíµm={0,1,‚Ä¶,m‚àí1}subscriptùíµùëö01‚Ä¶ùëö1\mathcal{Z}_{m}=\{0,1,\ldots,m-1\}caligraphic_Z start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = { 0 , 1 , ‚Ä¶ , italic_m - 1 }, and the cardinality of a finite set ‚Ñê‚Ñê\mathcal{I}caligraphic_I is denoted by |‚Ñê|‚Ñê|\mathcal{I}|| caligraphic_I |. We employ various matrix and tensor operations, including the Kronecker product (‚äótensor-product\otimes‚äó) and tensor convolution (‚àó*‚àó). For a matrix ùêåùêå\mathbf{M}bold_M, ùêå‚Å¢(i,j)ùêåùëñùëó\mathbf{M}(i,j)bold_M ( italic_i , italic_j ) denotes its (i,j)ùëñùëó(i,j)( italic_i , italic_j )-th entry; ùêåi,jsubscriptùêåùëñùëó\mathbf{M}_{i,j}bold_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT represents its (i,j)ùëñùëó(i,j)( italic_i , italic_j )-th block sub-matrix; and ùêåisubscriptùêåùëñ\mathbf{M}_{i}bold_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes its iùëñiitalic_i-th column block sub-matrix. For a tensor ùêì‚àà‚ÑùN1√óN2√óN3ùêìsuperscript‚ÑùsubscriptùëÅ1subscriptùëÅ2subscriptùëÅ3\mathbf{T}\in\mathbb{R}^{N_{1}\times N_{2}\times N_{3}}bold_T ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT √ó italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT √ó italic_N start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, (ùêì)i,j,ksubscriptùêìùëñùëóùëò(\mathbf{T})_{i,j,k}( bold_T ) start_POSTSUBSCRIPT italic_i , italic_j , italic_k end_POSTSUBSCRIPT (or ti,j,ksubscriptùë°ùëñùëóùëòt_{i,j,k}italic_t start_POSTSUBSCRIPT italic_i , italic_j , italic_k end_POSTSUBSCRIPT) refers to its (i,j,k)ùëñùëóùëò(i,j,k)( italic_i , italic_j , italic_k )-th entry; similar notation applies to higher-dimensional tensors. We denote a 1√óUk1subscriptùëàùëò1\times U_{k}1 √ó italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT tensor block list as ùêì‚Ä≤=[ùêì0‚Ä≤,ùêì1‚Ä≤,‚Ä¶,ùêìUk‚àí1‚Ä≤]superscriptùêì‚Ä≤subscriptsuperscriptùêì‚Ä≤0subscriptsuperscriptùêì‚Ä≤1‚Ä¶subscriptsuperscriptùêì‚Ä≤subscriptùëàùëò1\mathbf{T}^{\prime}=[\mathbf{T}^{\prime}_{0},\,\mathbf{T}^{\prime}_{1},\,% \ldots,\,\mathbf{T}^{\prime}_{U_{k}-1}]bold_T start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT = [ bold_T start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_T start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_T start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1 end_POSTSUBSCRIPT ] or {ùêìi‚Ä≤}i=0Uk‚àí1superscriptsubscriptsubscriptsuperscriptùêì‚Ä≤ùëñùëñ0subscriptùëàùëò1\{\mathbf{T}^{\prime}_{i}\}_{i=0}^{U_{k}-1}{ bold_T start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - 1 end_POSTSUPERSCRIPT, where each tensor block ùêìi‚Ä≤subscriptsuperscriptùêì‚Ä≤ùëñ\mathbf{T}^{\prime}_{i}bold_T start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT has identical dimensions."
https://arxiv.org/html/2411.01515v1,Parallel Online Directed Acyclic Graph Exploration for Atlasing Soft-Matter Assembly Configuration Spaces,"The paper formalizes a version of parallel online directed acyclic graph (DAG) exploration, general enough to be readily mapped to many computational scenarios. In both the offline and online versions, vertices are weighted with the work units required for their processing, at least one parent must be completely processed before a child is processed, and at any given time only one processor can work on any given vertex. The online version has the following additional natural restriction: only after a vertex is processed, are its required work units or its children known.Using the Actor Model of parallel computation, it is shown that a natural class of parallel online algorithms meets a simple competitive ratio bound. We demonstrate and focus on the problem‚Äôs occurrence in the scenario of energy landscape roadmapping or atlasing under pair-potentials, a highly compute-and-storage intensive modeling component integral to diverse applications involving soft-matter assembly. The method is experimentally validated using a C++ Actor Framework (CAF) software implementation built atop EASAL (Efficient Atlasing and Search of Assembly Landscapes), a substantial opensource software suite, running on multiple CPU cores of the HiperGator supercomputer, demonstrating linear speedup results.","Energy landscape analysis in soft-matter assembly under pair-potentials is highly computationally intensive and occurs frequently in diverse application scenarios. The applications range from chemical engineering, material science, and computational chemistry to the study of microscale and nanoscale processes underlying life and disease [4, 41, 22, 44, 23, 1, 24, 46, 45, 6]. One integral component of the analysis, e.g., for equilibrium free energy and kinetics computations, is the task of roadmapping or atlasing the assembly configuration space into contiguous equi-potential energy regions. While this component is an implicit requirement in all prevailing methodologies and is highly compute and storage intensive, only certain more recent methodologies [7, 8, 40, 33, 48, 9, 47, 49, 10, 3], isolate it as an explicit component, using various strategies based on modern discrete geometry and algorithmic techniques to significantly enhance efficiency. EASAL (efficient atlasing and search of assembly landscapes) is a substantial opensource software suite [38, 32, 39] based on one such methodology [31, 40] developed by the authors, particularly suited to so-called short-ranged pair potentials that challenge prevailing methodologies. See Section 4.1. EASAL treats the rigid components of assembling entities (molecules or particles) as point-sets and the pair-potentials as distance interval constraints between each pair of points taken from different point-sets. Each contiguous equi-potential energy region consists of configurations satisfies exactly a specific set of active pair-potential constraints which uniquely specifies the region. The regions are topologically organized as nodes of a directed acyclic graph (DAG), which is a lattice poset ‚Äì of the corresponding sets of active pair-potential constraints ‚Äì under set containment. In Section 4.1, we demonstrate how the DAG is discovered or revealed gradually by sampling the configurations in each node or region starting from the root nodes which have no incoming edges. Sampling a node or region reveals the immediate descendants (children) of the node, which can be sampled only thereafter. Sampling of a node is a sequential process and the required effort is typically unknown apriori. In Section 2, we formalize a generalized version of the above atlasing problem as a parallel online DAG exploration starting from one or more root nodes, with the following condition that only after a vertex is processed, are its children known (processing takes a nontrivial amount of computation that is unknown until processing is complete). We will see that it follows that the entire processing of a vertex is assigned only to one processor in one contiguous block of work which does not need to be interrupted and resumed. In fact, many other computational problems far removed from assembly energy landscape atlasing, e.g., mapping of certain types of terrains using drones or robots [30] can be reduced to this version of parallel online DAG exploration. In Section 1.1, we note that while there is extensive literature on parallel offline graph exploration [16], as well as a competitive ratio analysis for sequential online graph exploration [16, 20], to our knowledge, competitive ratio analysis for parallel online graph exploration is [34] (see also Section 1.1) is relatively limited, although there was substantial early work on the parallel online setting [2]. We give a lower bound on the competitive ratio [2] of our version of parallel online graph exploration, i.e., the best performance of any parallel online algorithm in relation to the performance of the optimal parallel offline strategy. Tightness of this competitive ratio bound is demonstrated by showing that it is achieved by any online algorithm in a natural class that minimizes processor idle times by assigning available tasks immediately to processors about to become idle. In Section 3, we formulate this class of parallel online algorithms using the actor model of parallel computation and communication for concurrent programming, in which the basic unit of computation is an abstract entity called an actor (the actor model is discussed in more detail in Section 3). \begin{overpic}[scale={.2}]{figures/StratificationBlack.png} \end{overpic} \begin{overpic}[scale={.135}]{figures/Stratification_sweeps.png} \end{overpic} \begin{overpic}[scale={.207}]{figures/ACG.jpg} \end{overpic} Figure 1: Stratification of the Roadmap: (a) shows a portion of the roadmap for the inset pair of input rigid molecular components. Each node represents a region of configurations satisfying a specific set of active constraints. The red nodes represent regions with 2 active constraints. Each successive stratum (from left to right) contains regions with one additional active constraint until we reach the pink leaf nodes. (b) Regions in the roadmap shown with their Cartesian configuration sweeps. Each sweep is the union of Cartesian configurations in the corresponding region. (c) Ancestors and descendants of a node, shown with their active constraint sets. Figure adapted, courtesy [40]. In Section 4.2, empirical results are presented to demonstrate a proof of concept for the effectiveness of our parallel scheme at achieving linear speedup for parallel exploration of assembly landscapes using the EASAL methodology. The parallel implementation is available as part of the EASAL opensource software suite [32]. We use the C++ Actor Framework (CAF) [11, 12] for the implementation. The parallel implementation, running on the University of Florida‚Äôs Hipergator supercomputer, on Intel(R) Xeon (TM) Gold 6142 series processors from the Skylake family series of processors give near linear speedup in the number of compute cores (as compared to the sequential implementation of the EASAL methodology [32]) and can roadmap several millions nodes in minutes, where the sequential version took hours. A summary of contributions is listed in the abstract. 1.1 Related Work on Parallel Offline, and Sequential Online Graph Exploration Graph traversal is the process of visiting all vertices in a connected component of a graph (directed or undirected), by starting from a chosen vertex and following edges. In the offline version of the problem, the entire graph is available to the algorithm before the execution begins. The most common algorithms for these are depth-first and breadth-first traversals, for which parallel versions have been extensively studied [29, 28, 13, 37]. In the online version of graph traversal, often called graph exploration, the graph is discovered in stages during execution. Specifically, the (unexplored) neighborhood of a vertex is only visible after the vertex has been visited. The basic version of the problem requires that the exploration algorithm visit every node and edge of the graph (directed or undirected) at least once. Doing this efficiently requires the algorithm to minimize the number of repeated edge traversals. Variants of the problem include requirements such as starting from a particular source node and possibly returning to the source at the end. Commonly seen in robotics, it is a well-studied problem [43, 15]. Online graph exploration algorithms, like all online algorithms, are typically evaluated by their competitive ratio, defined as the ratio of the performance (according to some measure) of the online algorithm (in the worst case) to that of the best offline strategy. There exist numerous sequential, online graph exploration methods that give different competitive ratios depending on the class of input graphs. The problem of exploring undirected graphs is solved using a DFS with a competitive ratio of 2 [16]; while directed, strongly connected graphs yield a competitive ratio that is a factor of the deficiency of the graph, which is the number of edges needed to make the graph Eulerian. In [16] it is shown that the competitive ratio is not bounded when the deficiency of the graph is unbounded and gives an online algorithm in which the complexity is bounded when the deficiency of the graph is bounded. Using a simple depth-first strategy, [26] gives an algorithm that is polynomial in the deficiency of the graph times the number of edges, for dense graphs. For the same problem, [20] gives an online algorithm whose competitive ratio is polynomial in the deficiency of the graph. [36] gives an exploration algorithm whose penalty, i.e., the worst-case number of edge traversals more than the lower bound (which is equal to the number of edges when each edge is visited only once), of the order of the number of vertices in the graph. [35] gives a randomized algorithm to improve efficiency and [5] gives an algorithm for traversing an unknown directed graph, based on the construction of the output-directed spanning tree of the graph and the breadth-first search on this tree. The related problem of online layered graph traversal [19], which involves searching for a target vertex in a layered graph whose width and number of layers are unknown, has a tight lower bound on competitive ratio by [42] for deterministic and randomized algorithms. While parallel online algorithms overall were studied early on [2], the literature on parallel, online graph exploration is sparse, with the exception of [34, 17, 18, 14, 27, 25, 21], who studies parallel online graph exploration using multiple agents. They give efficient algorithms and lower bounds for competitive ratios, for several families of graphs such as trees, cycles, random graphs, and cliques. To the best of our knowledge, this paper‚Äôs version of parallel, online, vertex-weighted DAG exploration has not been studied."
https://arxiv.org/html/2411.01460v1,MAO: Machine learning approach for NUMA optimization in Warehouse Scale Computers,"Non-Uniform Memory Access (NUMA) architecture imposes numerous performance challenges to today‚Äôs cloud workloads. Due to the complexity and the massive scale of modern warehouse-scale computers (WSCs), a lot of efforts need to be done to improve the memory access locality on the NUMA architecture. In Baidu, we have found that NUMA optimization has significant performance benefit to the top major workloads like Search and Feed(Baidu‚Äôs recommendation system). But how to conduct NUMA optimization within the large scale cluster brings a lot of subtle complexities and workload-specific scenario optimizations. In this paper, we will present a production environment deployed solution in Baidu called MAO (Memory Access Optimizer) that helps improve the memory access locality for Baidu‚Äôs various workloads. MAO includes an online module and an offline module. The online module is responsible for online monitoring, dynamic NUMA node binding and runtime optimization. Meanwhile the offline workload characterization module will proceed with the data analysis and resource-sensitivity model training. We also proposed a new performance model called ‚ÄùNUMA Sensitivity model‚Äù to address the impact of remote memory access to workload performance and projection of the potential performance improvements via NUMA optimization for specified workload. Based on continuous data collected from online monitoring, this model is proved to be working properly in MAO. As of today, we have successfully deployed MAO to more than one hundred thousand servers. In our Feed product, we have achieved 12.1% average latency improvements and 9.8% CPU resource saving.","Non-Uniform Memory Access (NUMA) architecture imposes numerous performance challenges to today‚Äôs cloud workloads. Due to the complexity and modern warehouse scale computers (WSCs), lots of engineering efforts need to be conducted to improve the memory access locality based on NUMA architecture. In this case, requests issued from a CPU core can either access its own NUMA node‚Äôs DRAM (known as local NUMA access) or access other NUMA nodes through cross-socket or cross-die interconnection. Though the interconnection is designed to transfer data at a relative high speed, remote memory access latency is still a non-negligible factor for workload‚Äôs performance. And this factor might be even significant in a multi-sockets system that there are more than one hops needed. Traditional optimization to NUMA architecture is always focus on improving the memory locality by threads or memory pages migration. Achieving optimal performance requires optimal placement of each thread to be close to the data it accesses[antony2006exploring][brecht1993importance]. However, such migrations have to be conducted after looking at the overall load-balancing among all cores and memory in the machine in order to maintain a good utilization of the hardware resources. Multi-threaded applications may distribute many threads on distant cores of a single host and memory among all memory nodes[goglin2009enabling] as well. A tremendous amount of research effort has been devoted to investigate the impact of NUMA and propose optimization algorithms on data placement and OS scheduling. ‚Ä¢ There has been a wealth of prior research regarding NUMA related scheduling approach[lepers2014improving][gaud2015challenges][psaroudakis2016adaptive][brecht1993importance][verma2015large][majo2011memory][mccormick2011empirical] [luo2016compositional]. These related work specifically focused on core allocation and data placement strategy for NUMA optimization. For example, AutoNUMA[corbet2012autonuma] improves memory access locality by migrating threads on the nodes they access the most and pages on the node from which they are most accessed based on hot page profiling mechanism. DINO[blagodurov2010case] made some more progress and designed a new contention-aware scheduling algorithm based on LLC MPKI (last level cache misses per 1000 instructions) for NUMA systems to minimize cache contention while avoiding interconnect and memory controller contention. Though DINO eliminated superfluous migrations but it doesn‚Äôt address the data sharing issue between threads which is quite common in cloud workloads. Carrefour[lepers2014improving][gaud2015challenges] collected performance metrics and page accesses statistics and worked out a novelty to combine 3 main strategies including page migrating, replicating and page interleaving. However, as it still highly relies on page migration technology that may incur high overhead especially in WSCs. ‚Ä¢ Besides, Machine learning approaches[denoyelle2019data][funston2018placement][arapidis2018performance] were proposed to predict performance and the sensitivity of applications. However, these approaches either rely on offline binary instrumentation or need extensive experiments to different placement policy of threads and configurations to collection training samples, which is not scalable to thousands of workloads at WSCs. ‚Ä¢ Previous work proposed a light weight NUMA score (sensitivity) model was proposed in [tang2013optimizing] based on the status of CPU utilization and memory usage per NUMA node, which might be stale and inaccurately predicate the performance impact of NUMA locality. In Baidu, we have found that NUMA sensitivity is more correlated with remote memory bandwidth ratio derived by the percentage of remote memory bandwidth for a specific workload to overall memory bandwidth, more details will be discussed in Chapter 3.5. When we inspect into our production environment deployment, we found the existing solutions might not fit well. In this paper, we addressed the aforementioned challenges in WSCs, presenting our system design called MAO (Memory Access Optimizer). This work is based on the collaboration with Intel in order to improve the memory access locality for thousands of workloads in production. In summary, we make the following contributions in this paper: ‚Ä¢ We found NUMA binding could be more effective than page migration. In production environment, our study on Search workload shows a lot of online work is getting done by threads that run for 0.4ms to 10ms, which resulted frequent thread migration between different cores. That further makes AutoNUMA less effective. Meanwhile, large remote memory footprint may result in high overhead by page migration. ‚Ä¢ To maximum the utilization the CPU resource, not all service containers can be deployed with NUMA optimization. We proposed an effective NUMA sensitivity model using XGBoost algorithm based on collected workload hardware performance counters. With this model, we observed around 5% mean absolute error(MAE) to project performance improvements with NUMA optimization and use the info to guide the scheduler based on NUMA awareness or maximum cpu resource. ‚Ä¢ We proposed a dynamic mechanism combines online execution and offline training to address the problem of imbalance CPU usage and resource fragmentation dynamically due to the NUMA binding in order to improve overall system performance. ‚Ä¢ We implemented MAO as a totally solution of NUMA optimization for production deployment. In the past one year, we have successfully deployed MAO to more than 100K servers in Baidu‚Äôs WSC optimizing many products like Search and Feed with very promising performance gain, for example, in Feed product, we have achieved 12.1% average latency improvements and 9.8% CPU resource saving in average."
https://arxiv.org/html/2411.01438v1,SkyServe: Serving AI Modelsacross Regions and Clouds with Spot Instances,"Recent years have witnessed an explosive growth of AI models. The high cost of hosting AI services on GPUs and their demanding service requirements, make it timely and challenging to lower service costs and guarantee service quality. While spot instances have long been offered with a large discount, spot preemptions have discouraged users from using them to host model replicas when serving AI models.To address this, we introduce SkyServe, a system that efficiently serves AI models over a mixture of spot and on-demand replicas across regions and clouds. SkyServe intelligently spreads spot replicas across different failure domains (e.g., regions or clouds) to improve availability and reduce correlated preemptions, overprovisions cheap spot replicas than required as a safeguard against possible preemptions, and dynamically falls back to on-demand replicas when spot replicas become unavailable. We compare SkyServe with both research and production systems on real AI workloads: SkyServe reduces cost by up to 44%percent4444\%44 % while achieving high resource availability compared to using on-demand replicas. Additionally, SkyServe improves P50, P90, and P99 latency by up to 2.6√ó2.6\times2.6 √ó, 3.1√ó3.1\times3.1 √ó, 2.7√ó2.7\times2.7 √ó compared to other research and production systems.","Generative AI has experienced explosive growth in the past several years. These models have enabled a plethora of new applications, such as large language model (LLM) chatbots (Google-Bard, ; openai-chatgpt, ), programming assistants (github-copilot, ), image generation (openai-dalle3, ), and writing assistants (grammarly-ai, ). Many companies (anyscale_endpoints, ; openai-api, ) offer these models as hosted services on the cloud. A service is composed of multiple model replicas; each replica runs on one or more GPU instances. However, serving these models is challenging: not only do they need to be highly available and serve user requests under tight latency constraints, but they are also expensive to operate. Compared to traditional web services, AI systems have much higher compute requirements (openai_compute, ) and costs (ai-high-cost, ). There are two reasons for the high cost of serving AI models. First, these models require expensive GPU accelerators. As a result, processing a request can be 10√ó\times√ó more expensive than a traditional search engine query (ai-high-cost, ). Second, real-life AI workloads have frequent and unpredictable traffic spikes (up to 50√ó50\times50 √ó on average (li2023alpaserve, )) and service latency fluctuations (openai-status, ). This results in organizations overprovisioning more replicas than needed to serve average user traffic or sometimes even provisioning for the peak load, both of which exacerbate the high serving cost. Spot instances have long been offered by cloud providers as a cost-saving option (2‚Äì12√ó2\text{--}12\times2 ‚Äì 12 √ó discounts vs. on-demand; Table 1). However, serving AI models on spot GPU instances (or ‚Äùspot replicas‚Äù) has a few key challenges. First, spot instances can be preempted by the provider at any time, and preemptions of spot GPUs are much more common than spot CPUs (¬ß2.3). Second, when preemptions happen, service quality can severely degrade due to fewer replicas responding to requests. Naively placing spot replicas in a single region can lead to limited availability and correlated preemptions, where multiple spot instances are preempted simultaneously, potentially resulting in service downtime (¬ß2.2). Finally, recovery of spot replicas can be slow due to the long cold start and provisioning delays, on the order of minutes (¬ß2.3), or even infeasible, due to the immediate unavailability of spot instances in the same zone/region. Figure 1. SkyServe Overview. SkyServe intelligently provisions and manages a mixture of spot and on-demand replicas across regions and clouds to minimize preemptions, improve availability, and reduce cost. Most prior work (yang2023snape, ; wagenlander2020spotnik, ; harlap2017proteus, ; harlap2018tributary, ; zhang2019mark, ; gunasekaran2022cocktail, ) focuses on the more available spot CPU instances (¬ß2.3) or on training workloads (thorpe2023bamboo, ; athlur2022varuna, ; yang2021scheduling, ). However, serving AI models on spot GPUs requires the system to be robust to frequent preemptions, spot unavailability, and significant cold start delays. Spot preemption warnings cannot address the problem, as the time to find available instances, provision, and load models typically exceeds the best-effort preemption warnings (2 minutes on AWS and 30 seconds on GCP and Azure). Spot instances can also be simultaneously unavailable or preempted in practice (¬ß2.2). As such, serving model replicas on spot GPUs while maintaining high service quality has not been widely considered viable in practice. We show that leveraging spot replicas in AI model serving is not only feasible, but can ensure high availability, lower costs, and improve service quality. We propose SkyServe, a system that uses a dynamic mixture of spot and on-demand replicas across regions and clouds to minimize the cost and improve service latency. First, SkyServe improves availability and decorrelates preemptions by spreading spot replicas across wider failure domains (regions or clouds), compared to the common practice of launching in the same zone or region (google-gke, ; aws-autoscaling-group, ; zhang2019mark, ; miao2023spotserve, ). Using more regions and clouds enlarges the search space where spot instances can be provisioned, significantly improving availability and speeding up recovery. We have observed that a single-region deployment of spot replicas is often not viable due to instance unavailability, leading to service downtime (¬ß2.2). Second, to avoid degraded service on preemptions, and to ensure high availability, SkyServe proactively uses on-demand replicas (model replicas running on on-demand instances) as a fallback. On-demand instances are typically available if we search and provision across regions and clouds. Third, SkyServe mitigates preemption by overprovisioning (provisioning more than required by traffic) with cheap spot replicas instead of expensive on-demand replicas. Even when some spot replicas are preempted, these additional over-provisioned replicas will mitigate the impact of preemption. Thus, a service is backed by a dynamic mixture of spot and on-demand replicas. SkyServe provides a unified interface to launch services on a mixture of spot and on-demand replicas across regions and clouds (Figure 1). Users leverage any existing model inference server (e.g., vLLM (kwon2023efficient, ), TGI (tgi, ), Triton (nvidia-triton, )) containing logic to invoke models, and SkyServe intelligently provisions, maintains, and load-balances a mixture of spot and on-demand replicas across regions and clouds. We deploy SkyServe on the cloud to serve AI models with spot replicas and experience real-time preemptions. Compared to other research and production systems, SkyServe improves P50, P90, P99 latency by up to 2.6√ó2.6\times2.6 √ó, 3.1√ó3.1\times3.1 √ó, 2.7√ó2.7\times2.7 √ó respectively (¬ß5.1), and saves cost by up to 44%percent4444\%44 % compared to using only on-demand replicas while achieving high availability. Additionally, we compare SkyServe with other policies by replaying real spot traces from AWS and GCP (wu2024can, ). Both experiments show that, with SkyServe, using spot replicas is feasible in serving AI models and can significantly lower costs and improve service quality. In summary, this paper makes four main contributions: ‚Ä¢ The design of an algorithm that manages a mixture of spot and on-demand replicas across regions and clouds. SkyServe achieves high availability while improving both cost and service quality. ‚Ä¢ The implementation of SkyServe as a distributed, multi-cloud serving system with mechanisms to scale across spot and on-demand replicas to efficiently serve AI models. ‚Ä¢ An extensive evaluation of SkyServe, comparing to both research and production systems as well as state-of-the-art policies on preemptive spot GPU instances. ‚Ä¢ An open-source prototype to facilitate further research and policy design for serving on spot instances."
https://arxiv.org/html/2411.01137v1,Data movement limits to frontier model training,"We present a theoretical model of distributed training, and use it to analyze how far dense and sparse training runs can be scaled. Under our baseline assumptions, given a three month training duration, data movement bottlenecks begin to significantly lower hardware utilization for training runs exceeding about 1028superscript102810^{28}10 start_POSTSUPERSCRIPT 28 end_POSTSUPERSCRIPT FLOP, two orders of magnitude above the largest training run to date, suggesting the arrival of fundamental barriers to scaling in three years given recent rates of growth. A training run exceeding about 1031‚Å¢FLOPsuperscript1031FLOP10^{31}\,\textrm{FLOP}10 start_POSTSUPERSCRIPT 31 end_POSTSUPERSCRIPT FLOP is infeasible even at low utilization. However, more aggressive batch size scaling and/or shorter and fatter model shapes, if achievable, have the potential to permit much larger training runs. An interactive version of our model will shortly be accessible here.","Scaling up neural networks and training them on more examples is crucial for good task performance (Hestness et al., 2017; Kaplan et al., 2020; Hoffmann et al., 2022; Bi et al., 2024), with state-of-the-art models requiring tens of thousands of GPUs111We focus on GPUs, but our theoretical model and findings are broadly applicable to other accelerators, and even groups of accelerators. to train in a reasonable duration. Previous work (Huang et al., 2019; Rajbhandari et al., 2020; Lepikhin et al., 2020; Narayanan et al., 2021; Jiang et al., 2024b) has developed practical techniques enabling the rapid scaling of the past decade (Sevilla et al., 2022). In this work, we address unexamined fundamental questions about limits to scaling in the future: Q1 Given present-day algorithms, GPUs, and interconnects, what is the biggest training run that can be performed within a fixed duration, before intra- and inter-GPU data movement starts to seriously worsen utilization or even render it impossible? Q2 How far might this limit be extended, and what algorithmic or hardware progress can achieve that? Answering these questions empirically would require millions of GPUs and large-scale engineering efforts, so we instead approach them theoretically. In doing so, we develop a simulator that can find optimal training run configurations accounting for the factors that we identify as fundamental. This gives us the answers: A1 With most current technology, GPU utilization starts to fall at ‚âàùüèùüéùüêùüñabsentsuperscript1028\mathbf{\approx 10^{28}}‚âà bold_10 start_POSTSUPERSCRIPT bold_28 end_POSTSUPERSCRIPT floating point operations (FLOP), about three years away at recent trends (Epoch AI, 2024) of 4.2√ó4.2\times4.2 √ó growth per year. Currently-available specialized high-bandwidth inter-node interconnects can permit training runs about two orders of magnitude larger (‚âà1030‚Å¢FLOPabsentsuperscript1030FLOP\approx 10^{30}\,\textrm{FLOP}‚âà 10 start_POSTSUPERSCRIPT 30 end_POSTSUPERSCRIPT FLOP), at which point latencies begin to worsen utilization, until reaching an absolute latency barrier at ‚âà1031‚Å¢FLOPabsentsuperscript1031FLOP\approx 10^{31}\,\textrm{FLOP}‚âà 10 start_POSTSUPERSCRIPT 31 end_POSTSUPERSCRIPT FLOP, about seven years away. A2 Improved hardware interconnects may buy no more than two orders of magnitude in training run size, assuming technology anything like the current paradigm. Beyond that, the critical innovations must come from machine learning algorithms: The key challenge is transforming two serial dependencies ‚Äî between batches and between layers ‚Äî into opportunities for parallelism, by making batch sizes bigger (perhaps enabled by sparsity) and models wider and shallower. Achieving these goals may be quite difficult in practice. Data movement and latency bottlenecks limit the scale of training runs to 1028‚Å¢ to ‚Å¢1031superscript1028 to superscript1031\boldmath{10^{28}\text{ to }10^{31}}10 start_POSTSUPERSCRIPT 28 end_POSTSUPERSCRIPT to 10 start_POSTSUPERSCRIPT 31 end_POSTSUPERSCRIPT FLOP New scaling techniques and hardware innovations could push back limits to scaling Figure 1: With current technology, such as the H100 GPU and current scaling techniques, data movement bottlenecks lower hardware utilization for training runs exceeding 1028superscript102810^{28}10 start_POSTSUPERSCRIPT 28 end_POSTSUPERSCRIPT FLOP, and a ‚Äúlatency wall‚Äù renders surpassing 1031superscript103110^{31}10 start_POSTSUPERSCRIPT 31 end_POSTSUPERSCRIPT FLOP infeasible (left). However, with innovations in scaling (such as techniques to enable much larger batch sizes) or dramatic increases in network bandwidth coupled with a 10√ó\times√ó reduction in inter- and intra-GPU latency, training runs can be at least a few orders of magnitude larger (right). This work is organized as follows: ‚Ä¢ Section 2 introduces a simplified model of a neural network consisting of stacked sparse linear multi-layer perceptrons that we use as the basis for our analysis throughout the paper. ‚Ä¢ Section 3 provides an overview of the four main parallelism strategies employed in distributed training‚Äîdata, tensor, pipeline, and expert parallelism‚Äîand summarizes their communication costs. ‚Ä¢ Section 4 identifies the key factors that constrain distributed training, including data movement, critical batch size, and latency, and model depth. ‚Ä¢ Section 5 derives closed-form expressions for the maximum training scale under this model. ‚Ä¢ Section 6 presents the complete theoretical model accounting for all identified constraints and discusses simulation results on current hardware, demonstrating the limits to efficient scaling. We conclude by discussing the implications of our findings, and posing key open technical questions whose answers will determine the limits to large-scale model training."
https://arxiv.org/html/2411.01129v1,Mewz: Lightweight Execution Environment for WebAssembly with High Isolation and Portability using Unikernels,"Cloud computing requires isolation and portability for workloads. Cloud vendors must isolate each user‚Äôs resources from others to prevent them from attacking other users or the whole system. Users may want to move their applications to different environments, for instance other cloud, on-premise servers, or edge devices. Virtual machines (VMs) and containers are widely used to achieve these requirements. However, there are two problems with combined use of VMs and containers. First, container images depend on host operating systems and CPU architectures. Users need to manage different container images for each platform to run the same codes on different OSes and ISAs. Second, performance is degraded by the overheads of both VMs and containers. Previous researches have solved each of these problems separately, but no solution solves both problems simultaneously. Therefore, execution environments of applications on cloud are required to be more lightweight and portable while ensuring isolation is required. We propose a new system that combines WebAssembly (Wasm) and unikernels. Wasm is a portable binary format, so it can be run on any host operating systems and architectures. Unikernels are kernels statically linked with applications, which reduces the overhead of guest kernel. In this approach, users deploy applications as a Wasm binary and it runs as a unikernel on cloud. To realize this system, we propose a mechanism to convert a Wasm binary into a unikernel image with the Wasm AoT-compiled to native code. We developed a unikernel with Wasm System Interface (WASI) API and an Ahead-of-Time (AoT) compiler that converts Wasm to native code. We evaluated the performance of the system by running a simple HTTP server compiled into Wasm and native code. The performance was improved by 30% compared to running it with an existing Wasm runtime on Linux on a virtual machine.","Cloud computing and container-based virtualization technologies are widely used in modern system development. In cloud computing, multiple users share the same physical resources of a data center. In such environments, it might be possible for users to attack other users by exploiting vulnerabilities in the system. For example, malicious users can steal sensitive information from other users or destroy the whole cloud system. To achieve multi tenacy, cloud providers must isolate each user‚Äôs resources from others. There are several isolation mechanisms, and the intensity of isolation depends on the type. The most isolated mechanism is separating the physical machines. Providing a physical machine for each user prevents interference between users. This method, called bare-metal cloud[36], is not mainstream for some drawbacks such as difficulty in immediate resource allocation. The second most isolated mechanism is virtual machine (VM) based isolation. In this method a hypervisor runs on a physical machine and creates multiple VMs. Each VM is isolated from others, and it has its own operating system. This method is widely used nowadays. The third most isolated mechanism is container-based isolation. An container is an isolated environment that runs on a host operating system. Since containers share the same kernel, they are more lightweight than VMs. However, containers does not provide enough isolation for multi-tenancy[29]. Despite the lack of isolation, containers are widely used in cloud computing for their portability. Container images can be easily moved between different environments, such as development, testing, and production. Containers are used in cloud environments with VMs to ensure isolation. Combining containers and cloud computing entails two problems. First, container images depend on host operating systems and CPU architectures. To distribute container images to environments with different operating systems and architectures, developers must build images for each one. Second, the virtualization overhead of both containers and VMs degrades performance. There are existing solutions to each of these challenges. For example, substitution of containers with WebAssembly is proposed to solve the portability problem[19]. Binary translation is also proposed, which converts instructions of one architecture to another[3]. However, these solutions do not solve the overhead problem. To reduce the overheads of guest kernel on VMs, previous researches proposed unikernels, which are specialized kernels for each application[17]. There is also a work that implements a lightweight hypervisor with limited functionality to reduce the overhead of hypervisors[1]. Conversely, these solutions do not enhance application portability. No solution solves both problems. We propose a new system where applications are distributed as WebAssembly and run them as unikernels on cloud. WebAssembly is a portable binary format, so it can be run on any host operating systems and architectures. Unikernels are kernels statically linked with applications. This design enables applications to call kernel functions directly, reducing the overhead from guest OSes. This system solves both the portability and virtualization overhead problems. To implement this system, we need to link WebAssembly binaries with kernel codes. However, the issue lies in converting a WebAssembly binary into a unikernel image, because WebAssembly cannot be simply linked with kernel codes. Moreover, it is preferable to compile WebAssembly ahead of time (AoT) into native code for performance reasons. We devise a new mechanism to do it by exploiting WebAssembly System Interface, which is the standardized API for WebAssembly to access system resources. We combine an AoT-compiled WebAssembly binary and kernel codes that provide WebAssembly System Interface by symbol resolution. We realized it by developing a unikernel that provides WebAssembly System Interface and an AoT compiler that converts WebAssembly to native code. We evaluate our system with an HTTP server that distributes static files. The result shows that our system executes Wasm applications with lower overhead than existing technologies. The rest of this paper is organized as follows. Section 2 summarizes the requirements for the execution environment of workloads on cloud and describes the problems of existing solutions. Section 3 explains the architecture and implementation of the proposed system. Section 4 evaluates the performance of our system. Section 5 describes related works. Section 6 concludes this paper."
https://arxiv.org/html/2411.02333v1,Discrete the solving model of time-variant standard Sylvester-conjugate matrix equations using Euler-forward formula: An analysis of the differences between sampling discretion errors and space compressive approximation errors in optimizing neural dynamics,"Time-variant standard Sylvester-conjugate matrix equations are presented as early time-variant versions of the complex conjugate matrix equations. Current solving methods include Con-CZND1 and Con-CZND2 models, both of which use ode45 for continuous model. Given practical computational considerations, discrete these models is also important. Based on Euler-forward formula discretion, Con-DZND1-2i model and Con-DZND2-2i model are proposed. Numerical experiments using step sizes of 0.1 and 0.001. The above experiments show that Con-DZND1-2i model and Con-DZND2-2i model exhibit different neural dynamics compared to their continuous counterparts, such as trajectory correction in Con-DZND2-2i model and the swallowing phenomenon in Con-DZND1-2i model, with convergence affected by step size. These experiments highlight the differences between optimizing sampling discretion errors and space compressive approximation errors in neural dynamics.","Standard Sylvester-conjugate matrix equations (SSCME) [1] are the earliest version of complex conjugate matrix equations (CCME) [2]. And SSCME is time-invariant. In recent years of studies, Wu et al. provided matrix algebraic formula methods [3] and iterative solving methods [2] based on approximation theory. The essence of the iterative methods is to gradually approach the theoretical solution using multi-step computations. And time-variant standard Sylvester-conjugate matrix equations (TVSSCME) [4] is the time-variant extension of SSCME. TVSSCME is supplemented by the differences between differential algebra and linear algebra operations [5, 6]. The difference between SSCME and TVSSCME solutions is shown in Fig. 1. Unless otherwise specified, let OùëÇOitalic_O represent ‚Äúnull matrix"", and only consider the unique theoretical solution X‚àó‚Å¢(œÑ)superscriptùëã‚àóùúèX^{\ast}(\tau)italic_X start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_œÑ ), same as below. (a)X‚Å¢(œÑ)‚Å¢F‚àíA‚Å¢X‚Å¢(œÑ)‚àíC=Oùëãùúèùêπùê¥ùëãùúèùê∂ùëÇX(\tau)F-AX(\tau)-C=Oitalic_X ( italic_œÑ ) italic_F - italic_A italic_X ( italic_œÑ ) - italic_C = italic_O, where œÑ‚Üí+‚àû‚Üíùúè\tau\to+\inftyitalic_œÑ ‚Üí + ‚àû, X‚Å¢(œÑ)‚ÜíX‚àó‚Å¢(œÑ).‚Üíùëãùúèsuperscriptùëã‚àóùúèX(\tau)\to X^{\ast}(\tau).italic_X ( italic_œÑ ) ‚Üí italic_X start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_œÑ ) .(b)X‚Å¢(œÑ)‚Å¢F‚Å¢(œÑ)‚àíA‚Å¢(œÑ)‚Å¢X‚Å¢(œÑ)‚àíC‚Å¢(œÑ)=Oùëãùúèùêπùúèùê¥ùúèùëãùúèùê∂ùúèùëÇX(\tau)F(\tau)-A(\tau)X(\tau)-C(\tau)=Oitalic_X ( italic_œÑ ) italic_F ( italic_œÑ ) - italic_A ( italic_œÑ ) italic_X ( italic_œÑ ) - italic_C ( italic_œÑ ) = italic_O, where œÑ‚Üí+‚àû‚Üíùúè\tau\to+\inftyitalic_œÑ ‚Üí + ‚àû, X‚Å¢(œÑ)‚ÜíX‚àó‚Å¢(œÑ).‚Üíùëãùúèsuperscriptùëã‚àóùúèX(\tau)\to X^{\ast}(\tau).italic_X ( italic_œÑ ) ‚Üí italic_X start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_œÑ ) . Figure 1: Differences between SSCME(a) and TVSSCME(b). TVSSCME is currently primarily solved using zeroing neural dynamics (ZND) models Con-CZND1 [4] and Con-CZND2 [4]. Above two models structure can be seen in Fig. 2. Random InputRandom InputRandom InputRandom Input‚ãÆ‚ãÆ\vdots‚ãÆn{‚ãÆ‚ãÆ\vdots‚ãÆn{‚ãÆ‚ãÆ\vdots‚ãÆn{‚ãÆ‚ãÆ\vdots‚ãÆ‚Ä¶‚ãÆ‚ãÆ\vdots‚ãÆ‚ãÆ‚ãÆ\vdots‚ãÆOutputOutputOutputOutput‚ãÆ‚ãÆ\vdots‚ãÆn{‚ãÆ‚ãÆ\vdots‚ãÆn{ Hidden Complex Layer1 Input Real Layer Hidden Complex Layer2 Hidden Real Layer1 Output Real Layer Random InputRandom InputRandom InputRandom Input‚ãÆ‚ãÆ\vdots‚ãÆn{‚ãÆ‚ãÆ\vdots‚ãÆn{‚Ä¶‚ãÆ‚ãÆ\vdots‚ãÆ‚ãÆ‚ãÆ\vdots‚ãÆOutputOutputOutputOutput‚ãÆ‚ãÆ\vdots‚ãÆn{‚ãÆ‚ãÆ\vdots‚ãÆn{ Input Real Layer Hidden Real Layer1 Output Real Layer Figure 2: Different between Con-CZND1 [4] model and Con-CZND2 [4] model. 2 Con-CZND1 model. 2 Con-CZND2 model. However, Con-CZND1 model essentially approximates using the complex field error, while Con-CZND2 model approximates using the real field error. In ode45 [7] solver, Con-CZND2 model does not perform as well as Con-CZND1 model. Discrete neural dynamics is validated in previous studies to reduce the error between theoretical and numerical solutions [8]. Zhang et al. continued to develop discretion in the real field, progressing from Euler-forward formula [9, 10] to an 11-point sampling discretion [11, 12, 13]. However, there is no exploration of neural models for solving TVSSCME using sampling discretion in the existing literature. According to the known studies, the two continuous solution models, Con-CZND1 and Con-CZND2, show significant differences due to the approximation effects of the internal ode45 [14] solver. Additionally, Con-CZND1 model exhibits space compressive approximation phenomenon. Therefore, it is essential to rigorously establish a discrete neural dynamics model for TVSSCME. The rest of this article is organized as follows: Section 2 provides the definition of TVSSCME and supplementary knowledge. Section 3 defines the Con-DZND1-2i discrete solving model over the complex field and the Con-DZND2-2i discrete solving model over the real field. Section 4 presents simulations that validate the effectiveness of each model and compares their strengths and weaknesses. Sections 5 and 6 summarizes this article and suggests future directions. Before proceeding to the next section, the main contributions of this article are as follows: (1) Con-DZND1-2i model, which directly defines complex field error, and Con-DZND2-2i model, which maps to real field error, are proposed for solving TVSSCME. (2) Based on Euler-forward formula, both discrete models, Con-DZND1-2i and Con-DZND2-2i, which use different step sizes, can ultimately approximate the theoretical solution. (3) Con-DZND1-2i model defines complex field error, while Con-DZND2-2i model maps to real field error. These models highlight a significant difference between optimizing space compressive approximation errors and optimizing sampling discretion errors in neural network optimization. Both aspects should be considered from different perspectives."
https://arxiv.org/html/2411.02322v1,LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation,"Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes‚Äîa critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms. Our implementation is available at https://github.com/Graph-COM/LayerDAG.","A Directed Acyclic Graph (DAG) is a data structure that represents the order of elements (Bang-Jensen & Gutin, 2008). Unlike linear sequences/chains, which follow a single, straight path from the first to the last element, DAGs incorporate branching and merging, allowing for the modeling of complex dependencies and hierarchies. This flexibility makes DAGs ideal for representing diverse problem domains such as workload behavior during system execution Sridharan et al. (2023), operator dependence for program analysis (Phothilimthana et al., 2023b; Luo et al., 2021; Cortez et al., 2017), dataflows in circuits (Dong et al., 2023), task dependencies in project management (Skiena, 2008; Borenstein, 2000), and cause-effect relationships (Pearl, 1995; Tennant et al., 2020). Additionally, DAGs have recently been used to create challenging benchmarks for evaluating the reasoning capabilities of large language models (Zhu et al., 2024). Figure 1: (a) A real-world DAG (the computation flow for a transformer layer (Vaswani et al., 2017)) encompasses complex logical and directional dependencies. Examples of logical dependencies include 1) dimension matching in matrix multiplications and 2) exactly two matrices pointed to a √ó\times√ó operation. One example of directional dependencies here is softmax(qk)v being computed after qk. (b) Each DAG has a unique layerwise partition, an ordered partition of nodes/edges into a sequence of bipartite graphs. In LayerDAG, each bipartite graph G(l+1)superscriptùê∫ùëô1G^{(l+1)}italic_G start_POSTSUPERSCRIPT ( italic_l + 1 ) end_POSTSUPERSCRIPT is generated by a diffusion model conditioned on G(‚â§l)superscriptùê∫absentùëôG^{(\leq l)}italic_G start_POSTSUPERSCRIPT ( ‚â§ italic_l ) end_POSTSUPERSCRIPT. LayerDAG generates in order the number of new nodes, their attributes, and the new edges. Despite the benefits of DAGs mentioned above, it is non-trivial to get largescale datasets of real DAGs to enable analysis and optimization. Taking workload execution as an example, DAGs capturing execution behavior can help engineers gain valuable insights into performance metrics such as latency and resource consumption exhibited by candidate systems (Luo et al., 2021; Cortez et al., 2017) to optimize the software and hardware accordingly. However, collecting a workload execution DAG (Sridharan et al., 2023) for even a single Large Language Model training job, potentially involving trillions of operations, is extremely prohibitive given the size of modern AI platforms. For example, Meta‚Äôs Llama3 was trained on a 24K GPU platform (Dubey et al., 2024). Moreover, very few companies in the world even have access to platforms of this scale. Even if such a large workload execution DAG could be collected, practical constraints such as the storage requirements of the DAGs, and potential to leak private information about the AI model architecture or the training platform configuration further limit DAG sharing. Furthermore, a representative and smaller graph is more effective for most performance optimization tasks. This work focuses on developing generative models for DAGs, which allows various potential applications. First, being able to be data-driven and generate representative small set of DAGs can be much more compute and memory efficient than a real DAG given repetitive patterns. Second, this could enable data sharing without compromising intellectual property (Gao et al., 2024; Lin et al., 2020), thereby promoting software and hardware co-design among different companies (Sridharan et al., 2023). Third, a conditional DAG generative model can efficiently search the space of valid DAGs for optimization purposes in scenarios such as circuit design (Takagi, 1999; Dong et al., 2023), compiler optimization (Aho et al., 2006; Phothilimthana et al., 2023b), and neural architecture search (NAS) (Zhang et al., 2019; Li et al., 2023a; An et al., 2023). Serving as an abstraction for flows and node dependencies, DAGs pose significant challenges in developing powerful and efficient generative models due to their intrinsic strong directional and logical dependencies, such as control flows, logic gates, and dimension requirements of matrix operations (as illustrated in Fig. 1 (a)). These complexities are further magnified in large-scale DAGs, presenting a unique combination of challenges regarding both scale and logical rules. In contrast, although social networks may be large in scale, they do not exhibit strong dependencies between nodes and edges. At the other end of the spectrum, molecular graphs, whose generation also has received much interest recently, must adhere to chemical rules but are generally smaller in size. This work proposes the use of autoregressive diffusion models to generate DAGs, aiming to decouple the strong node dependencies in DAGs into manageable units and handle them sequentially. Our model, named LayerDAG, is based on a novel perspective of DAGs: as illustrated in Fig. 1 (b), the partial order of nodes dictated by the DAG structure can be decoupled as a sequence of tokens, each corresponding to a bipartite graph. This perspective enables the natural modeling of directional dependencies in DAGs through autoregression, and the rest is to address the logical dependencies within a set of nodes incomparable (i.e. no ordering relations) according to the partial order in each autoregressive step, where we leverage diffusion models known for effectively capturing multidimensional dependencies (Rombach et al., 2022; Vignac et al., 2023a). Specifically, the diffusion model is conditioned on previously generated bipartite graphs to generate the next bipartite graph consisting of node attributes and edges. Our model advances existing DAG generative models in multiple aspects (Zhang et al., 2019; Li et al., 2023a; An et al., 2023). Methodologically, although autoregressive models have been adopted by D-VAE (Zhang et al., 2019) and GraphPNAS (Li et al., 2023a) for DAG generation, they treat either a single node or a node set of constant size as a token. This tokenization method imposes an order between nodes that should be incomparable in the partial order, violating the inductive bias inherent in the DAG structure. We argue that the violation may hurt the generalization capability of generative models. Moreover, D-VAE suffers from a potentially slow generation process by generating one node per iteration. Although GraphPNAS generates multiple nodes per iteration, it uses a mixture of Bernoulli distributions to model intra-set edge dependencies, which is less expressive than diffusion models (Wang et al., 2023; Pearce et al., 2023). DiffusionNAG (An et al., 2023) employs diffusion models to generate only node attributes given a DAG structure. Other diffusion models have been developed for undirected graphs (Niu et al., 2020; Jo et al., 2022; Vignac et al., 2023a), but they ignore the directional information in DAGs, while our work demonstrates the necessity of the autoregressive component in modeling directional dependencies in DAGs. From the application perspective, all the existing works (Zhang et al., 2019; Li et al., 2023a; An et al., 2023) focus on generating small DAGs (with ##\##nodes ‚â§24absent24\leq 24‚â§ 24) for NAS, while our model is capable of generating much larger flow graphs (up to ‚àºsimilar-to\sim‚àº 400 nodes) for system/hardware benchmarking. Overall, our work is the first to use autoregressive diffusion models for DAG generation, aiming to take the advantages of both autoregressive models and diffusion models to model the strong dependencies commonly in DAG data. We conduct extensive experiments to verify the effectiveness of our model. To assess the model‚Äôs ability to learn strong directional and logical rules, we construct a challenging synthetic DAG dataset with injected dependencies for evaluation. Additionally, we employ three real-world datasets‚Äî computational graphs on Tensor Processing Units (TPU), flow graphs on Field Programmable Gate Arrays (FPGA), and neural architectures deployed on edge devices ‚Äî for computing system benchmarking applications. Each dataset contains thousands of DAGs, with individual DAGs comprising up to hundreds of nodes. We compare the validity and statistical properties of the synthetic DAGs generated by our model with baselines. To measure benchmarking performance, we use the synthetic labeled DAGs to train surrogate machine learning (ML) models to predict TPU runtime, FPGA resource usage, and the inference latency of neural architectures for the three application scenarios. These trained surrogate models are then applied to the real-world DAGs for testing. Note that ML-based surrogate models are widely used today to measure the performance of systems and programs without the need for time-consuming simulations (Adams et al., 2019; Chen et al., 2018; Mendis et al., 2019; S·ª≥kora et al., 2022; Steiner et al., 2021; Baghdadi et al., 2021; Zheng et al., 2020; Li et al., 2020; Ahn et al., 2020; Dubach et al., 2007; Jia et al., 2020; Kaufman et al., 2021; Cao et al., 2023; Phothilimthana et al., 2023b). We compare the predictions given by the surrogate models with the ground-truth behavior of the DAGs on the corresponding systems. Results show that the surrogate models trained on our generated synthetic DAGs consistently outperform the ones derived with baseline generative models. We also evaluate the extrapolation and interpolation properties of LayerDAG by generating DAGs with labels out of the regime used for training, and LayerDAG demonstrates a superior generalization capability."
https://arxiv.org/html/2411.02268v2,Memory-Efficient Community Detection on Large GraphsUsing Weighted Sketches,"Community detection in graphs identifies groups of nodes with denser connections within the groups than between them, and while existing studies often focus on optimizing detection performance, memory constraints become critical when processing large graphs on shared-memory systems. We recently proposed efficient implementations of the Louvain, Leiden, and Label Propagation Algorithms (LPA) for community detection. However, these incur significant memory overhead from the use of collision-free per-thread hashtables. To address this, we introduce memory-efficient alternatives using weighted Misra-Gries (MG) sketches, which replace the per-thread hashtables, and reduce memory demands in Louvain, Leiden, and LPA implementations ‚Äî while incurring only a minor quality drop (up to 1%percent11\%1 %) and moderate runtime penalties. We believe that these approaches, though slightly slower, are well-suited for parallel processing and could outperform current memory-intensive techniques on systems with many threads.","Research on graph-structured data has seen rapid growth, driven by the capacity of graphs to represent complex, real-world interactions and capture intricate relationships between entities. At the core of this field is community detection, a technique that divides graphs into tightly connected subgroups or communities, thereby revealing the natural structure within the data. Community detection finds applications across a wide range of areas, including examining epidemic-prone group dynamics (Salath√© and Jones, 2010), studying zoonotic eco-epidemiology (Desvars-Larrive et al., 2024), detecting diseases like lung cancer (Bechtel et al., 2005), categorizing tumors via genomic data (Haq and Wang, 2016), aiding therapeutic discovery (Ma et al., 2019; Udrescu et al., 2020), mapping healthcare areas (Wang et al., 2021), analyzing retail patterns (Verhetsel et al., 2022), identifying transportation trends (Chen et al., 2023), unsupervised part-of-speech tagging (Das and Petrov, 2011), partitioning graphs for machine learning (Bai et al., 2024), automating microservice decomposition (Cao and Zhang, 2022), sectionalizing power systems (Aziz et al., 2023), characterizing polarized information ecosystems (Uyheng et al., 2021), identifying hidden social network groups (Blekanov et al., 2021; La Cava et al., 2022), detecting disinformation on Telegram (La Morgia et al., 2021), investigating restored Twitter accounts (Kapoor et al., 2021), mapping multi-step cyberattacks (Zang et al., 2023), detecting blockchain attacks (Erfan et al., 2023), studying cyber resilience (Chernikova et al., 2022), analyzing human brain networks (Bullmore and Sporns, 2009; He and Evans, 2010), and understanding metabolic network evolution (Pfeiffer et al., 2005; Kim et al., 2009). Community detection is also used for addressing other graph related problems, such as, finding connected components (Stergiou et al., 2018), graph partitioning (Meyerhenke et al., 2017; Slota et al., 2020), vertex reordering and graph compression (Boldi et al., 2011), and graph coarsening (Valejo et al., 2020). Community detection is challenging due to the lack of prior knowledge about the number of communities and their size distribution, a problem that has led to the development of various heuristic methods for identifying communities (Blondel et al., 2008; Gregory, 2010; Raghavan et al., 2007; Newman and Reinert, 2016; Ghoshal et al., 2019). A commonly used metric for assessing the quality of detected communities is the modularity score, introduced by Newman et al. (Newman, 2004). The Louvain method, introduced by Blondel et al. (Blondel et al., 2008), is a widely used community detection algorithm (Lancichinetti and Fortunato, 2009) that applies a two-phase approach consisting of an iterative local-moving phase and an aggregation phase to optimize the modularity metric across multiple passes. However, Traag et al. (Traag et al., 2019) found that the Louvain method can yield poorly connected and even internally disconnected communities. They proposed the Leiden algorithm, which introduces an additional refinement phase to address these shortcomings, enabling the algorithm to better detect well-connected communities (Traag et al., 2019). The Label Propagation Algorithm (LPA) is another method that outperforms the above algorithms in terms of speed and scalability, but yields communities with lower modularity scores. However, it has been observed to achieve high Normalized Mutual Information (NMI) score compared to the ground truth (Peng et al., 2014). Given the importance of the community detection problem, a number of existing studies have aimed at improving the performance of the above algorithms using various algorithmic optimizations (Rotta and Noack, 2011; Waltman and Eck, 2013; Gach and Hao, 2014; Traag, 2015; Lu et al., 2015; Ozaki et al., 2016; Naim et al., 2017; Halappanavar et al., 2017; Ghosh et al., 2018b; Traag et al., 2019; Shi et al., 2021; Xing et al., 2014; Berahmand and Bouyer, 2018; Sattari and Zamanifar, 2018; You et al., 2020; Liu et al., 2020) and parallelization techniques (Cheong et al., 2013; Wickramaarachchi et al., 2014; Lu et al., 2015; Naim et al., 2017; Fazlali et al., 2017; Halappanavar et al., 2017; Ghosh et al., 2018b; Bhowmik and Vadhiyar, 2019; Shi et al., 2021; Bhowmick et al., 2022; Staudt and Meyerhenke, 2015; Soman and Narang, 2011; Kuzmin et al., 2015; Traag and ≈†ubelj, 2023). Additionally, significant effort has gone into developing efficient parallel implementations for multicore CPUs (Staudt and Meyerhenke, 2015; Staudt et al., 2016; Fazlali et al., 2017; Halappanavar et al., 2017; Qie et al., 2022; Hu et al., [n. d.]), GPUs (Naim et al., 2017; Kang et al., 2023), CPU-GPU hybrids (Bhowmik and Vadhiyar, 2019; Mohammadi et al., 2020), multi-GPUs (Cheong et al., 2013; Kang et al., 2023; Chou and Ghosh, 2022; Gawande et al., 2022), and multi-node systems ‚Äî CPU only (Ghosh et al., 2018b, a; Sattar and Arifuzzaman, 2022; Hu et al., [n. d.]) / CPU-GPU hybrids (Bhowmick et al., 2022). However, these studies focus primarily on reducing the runtime of the algorithms. As network sizes grow, the memory footprint becomes a critical concern, particularly when processing large graphs on shared-memory systems. Recently, we proposed some of the most efficient implementations of Louvain (Sahu, 2023b), Leiden (Sahu, 2023a), and LPA (Sahu, 2023c). These implementations have a space complexity of O‚Å¢(T‚Å¢|V|+|E|)ùëÇùëáùëâùê∏O(T|V|+|E|)italic_O ( italic_T | italic_V | + | italic_E | ), where |V|ùëâ|V|| italic_V | is the number of vertices, |E|ùê∏|E|| italic_E | is the number of edges, and TùëáTitalic_T is the number of threads used. As a result, they also face similar memory constraints. In this work, we present a method based on the Misra-Gries heavy hitters algorithm (Misra and Gries, 1982) to significantly reduce memory usage in our Louvain,111https://github.com/puzzlef/louvain-lowmem-communities-openmp Leiden,222https://github.com/puzzlef/leiden-lowmem-communities-openmp and LPA333https://github.com/puzzlef/rak-lowmem-communities-openmp implementations, with minimal impact on community quality. While this approach introduces some runtime overhead, it is more parallelizabile, and by current trends, may eventually outperform existing memory-intensive methods."
https://arxiv.org/html/2411.02115v1,FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained Aggregation,"Federated learning (FL) is a collaborative machine learning approach that enables multiple clients to train models without sharing their private data. With the rise of deep learning, large-scale models have garnered significant attention due to their exceptional performance. However, a key challenge in FL is the limitation imposed by clients with constrained computational and communication resources, which hampers the deployment of these large models. The Mixture of Experts (MoE) architecture addresses this challenge with its sparse activation property, which reduces computational workload and communication demands during inference and updates. Additionally, MoE facilitates better personalization by allowing each expert to specialize in different subsets of the data distribution. To alleviate the communication burdens between the server and clients, we propose FedMoE-DA, a new FL model training framework that leverages the MoE architecture and incorporates a novel domain-aware, fine-grained aggregation strategy to enhance the robustness, personalizability, and communication efficiency simultaneously. Specifically, the correlation between both intra-client expert models and inter-client data heterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P) communication between clients for selective expert model synchronization, thus significantly reducing the server-client transmissions. Experiments demonstrate that our FedMoE-DA achieves excellent performance while reducing the communication pressure on the server.","I introduction Since the proposal of FL [1], it has garnered significant attention for its ability to collaboratively train machine learning models using distributed data without compromising user privacy. However, FL often encounters severe data heterogeneity, which affects model performance and can even prevent model convergence [2]. With the advancement of deep learning technology, there is a trend towards utilizing large-scale models to enhance performance [3]. Previous works [4, 5] indicate that a model‚Äôs loss scales as a power-law with the number of model parameters, the volume of data, and the amount of computation used for training. Nevertheless, the computational resources at the client side are typically limited and highly heterogeneous [6, 7], which contradicts the trend towards large-scale models. Unlike dense models that activate all parameters for every input, sparsely activated models like Mixture of Experts (MoE) leverage conditional computation [8]. MoE utilizes a gating network to partition the input space into several regions, with each expert model responsible for a specific region. This strategy lowers data complexity within each region, allowing expert models to employ simpler architectures while achieving good performance. Furthermore, by activating only portions of the model per sample, MoE enables larger model sizes without increasing computational load, thereby enhancing model performance without raising the computational budget. MoE is now widely used in natural language processing [9] and computer vision [10]. FL suffers from two significant challenges, namely the system challenge and the statistical challenge, and the MoE architecture is a promising solution that helps mitigate both of these challenges for the following reasons. First, as mentioned earlier, client-side computational and communication resources are typically limited. MoE can take advantage of sparse activation, allowing clients to train only a subset of the experts relevant to their local data rather than the entire model. This results in more efficient training and sparse communication, which mitigates system heterogeneity, and benefits devices with constrained resources. Moreover, MoE can be scaled by adding more experts as needed without significantly increasing the computational burden on each client, enabling the model to handle larger and more diverse datasets efficiently. In addition, the data distribution of clients is often heterogeneous. According to [11], the MoE architecture allows different experts to focus on various parts of the data or tasks. This enables the holistic model to capture the variances between datasets more effectively and generalize over a wider range of data distributions. Thus, deploying MoE-based models at distributed clients enhances the personalization performance of FL. However, to the best of our knowledge, FL utilizing the sparse MoE is not well-studied, with only two notable works: FedMix [11] and FedJETs [12]. In these studies, each client maintains a model with an MoE architecture, comprising a gating network and several shared experts, and they leverage sparse activation to reduce communication overhead. We argue that these works have several shortcomings. First, when each communication round starts, each client needs to use the gating network and local data to prune less effective experts, introducing additional computation. Additionally, since the experts are shared across the system, clients can only choose from these shared models, which somewhat limits model personalization. Finally, although both algorithms suggest excluding some experts to reduce communication, multiple experts still need to be transmitted between the server and clients, increasing the communication burden on the server. In order to mitigate the aforementioned shortcomings, we propose a novel FL system with an MoE architecture that can enhance both the robustness and personalizability of clients‚Äô models. First, drawing from previous works [13, 14, 15, 16, 12], which show that shallower layers in deep neural networks learn simple, low-level, and task-independent representations such as edges and textures, and considering that the MoE architecture struggles with high-dimensional data, we propose in our mechanism that all clients share an embedding model that is aggregated at the server to extract uniform and robust representations. Secondly, to enhance the personalization capability of the client models, each client is equipped with its own unique gating network and experts. Moreover, to alleviate the communication burden of model aggregation, we utilize the relationship between gating network parameters and the selection patterns of experts to capture the correlation of experts among clients. We then leverage the P2P communication capabilities between clients to selectively transfer experts‚Äô parameters. In addition, in order to further reduce the amount of transmission between the server and clients and to reduce the waiting time for aggregation, we periodically make aggregation decisions based on the latest round of information. In summary, we make the following technical contributions: ‚Ä¢ We propose a new FL system utilizing the MoE architecture so as to enable task-independent embedding model aggregation for robustness enhancement and selective expert collaboration for personalizability improvement. ‚Ä¢ To reduce the communication overhead, we exploit the relationship between the gating network‚Äôs parameters and expert selection patterns to capture expert correlations among clients without transmitting the entire model. We also propose an aggregation strategy that fully leverages clients‚Äô P2P communication capabilities and employ a periodic aggregation policy based on historical information. ‚Ä¢ We experimentally verify FedMoE-DA‚Äôs effectiveness, demonstrating its capability to achieve high model accuracy while minimizing server-client communication."
https://arxiv.org/html/2411.02086v1,Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism,"Railway Turnout Machines (RTMs) are mission-critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. Due to frequent operations and exposure to harsh environments, RTMs are susceptible to failures and can potentially pose significant safety hazards. For safety assurance applications, especially in early-warning scenarios, RTM faults are expected to be detected as early as possible on a continuous 7x24 basis. However, limited emphasis has been placed on distributed model inference frameworks that can meet the inference latency and reliability requirements of such mission-critical fault diagnosis systems, as well as the adaptation of diagnosis models within distributed architectures. This has hindered the practical application of current AI-driven RTM monitoring solutions in industrial settings, where single points of failure can render the entire service unavailable due to standalone deployment, and inference time can exceed acceptable limits when dealing with complex models or high data volumes. In this paper, an edge-cloud collaborative early-warning system is proposed to enable real-time and downtime-tolerant fault diagnosis of RTMs, providing a new paradigm for the deployment of models in safety-critical scenarios. Firstly, a modular fault diagnosis model is designed specifically for distributed deployment, which utilizes a hierarchical architecture consisting of the prior knowledge module, subordinate classifiers, and a fusion layer for enhanced accuracy and parallelism. Then, a cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is developed to minimize the overhead resulting from distributed task execution and context exchange by strategically partitioning and offloading model components across cloud and edge. Additionally, an election consensus mechanism is implemented within CEC-PA to ensure system robustness during coordinator node downtime. Comparative experiments and ablation studies are conducted to validate the effectiveness of the proposed distributed fault diagnosis approach. Our ensemble-based fault diagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset collected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA demonstrates superior recovery proficiency during node disruptions and speed-up ranging from 1.98x to 7.93x in total inference time compared to its counterparts.","Railway transportation offers a high-capacity, cost-effective, and environmentally friendly solution for long-distance travel, making it a popular choice for passenger and freight services in Europe, Asia, and North America. According to M&M market research [1], the global railway system was valued at $25.1 billion in 2022 and is estimated to reach $30.9 billion by 2027. The Railway Turnout Machines (RTMs), also known as the Railway Point Machines (RPMs), are critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. However, RTMs are prone to failures due to wearing caused by frequent operations and exposure to harsh outdoor environments. Statistical analysis reveals RTMs as one of railside equipment that experience the highest failure rates, accounting for 18% of all documented railway system failures occurring between 2011 and 2017 [2]. The malfunction of RTMs can lead to catastrophic accidents such as collisions and train derailments, resulting in severe casualties and property losses. This typically involves the concept of preventive maintenance [3], which calls for regularly scheduled inspections and repairs targeting at the prevention of failures before they occur. For a long time, such condition-based maintenance mainly depends on the expert knowledge and experience of railway workers and thus can be time-consuming and labor-intensive. Therefore, an unsupervised, resilient, and responsive RTM fault early-warning system for train drivers and maintenance groups has raised lots of concern in the industry. With the advent of information technology, Railside Monitoring Units (RMUs) are deployed to collect runtime data during the operation of RTMs. Numerous fault diagnosis methods have been developed utilizing the collected data on vibration [4], current [5, 6, 7], torque and acoustic signals [8], etc. Previous endeavors have been primarily dedicated to enhancing model accuracy, while paying little attention to the performance and reliability issues caused by inappropriate deployment methods [9]. For safety assurance applications, especially in early-warning scenarios, we expect faults to be detected as early as possible to provide drivers and maintenance groups with more response time. The high computational overhead and complex procedures of these fault diagnosis models can make real-time inference challenging on resource-constrained devices such as Personal Computers (PCs). The traditional standalone deployment [10], where all the model components are deployed on a single device or platform, is also susceptible to system-wide unavailability in case of any software or hardware malfunctions on that centralized node [11]. Cloud computing has then become a common approach to wide range of fault diagnostic applications in Industry 4.0 [12], micro-electromechanical systems (MEMS) [13], Cloud Native [14], etc. However, the data gathered must be sent to the cloud to harness the high-performance and elastic advantages of cloud computing. In addition to privacy concerns [15] stemming from the sensitive nature of sensor data (e.g., route schedules and geographical locations), the transmission of data in railway environments like underground tunnels, inevitably leads to data loss and network latency issues [16]. These factors significantly impair the real-time capabilities of cloud-based solutions and hinder their effectiveness in monitoring mission-critical infrastructure [17]. In the past decade, academic interest has grown in combining edge computing with fault detection for model deployment, also known as Edge Intelligence (EI) [18]. This novel approach shifts computation from centralized cloud servers to the network edge, offering latency [19], energy consumption [20], Quality of Service (QoS) [21] and mobility [22] enhanced solutions. Federated Learning (FL) [23] has emerged as a potent approach for preserving privacy during model training, which enable each distributed client to train a local replica of the global model with its own dataset before sending updates to aggregate the shared global model. However, limited emphasis has been placed on distributed model inference frameworks that can meet the latency and reliability requirements of the fault diagnosis model deployment, or on tailoring the diagnosis models to perform optimally within distributed architectures. The inherent complementarity of cloud and edge computing has fostered the concept of cloud-edge collaboration [24], a paradigm that dynamically allocates and coordinates computational tasks across cloud and edge. This collaborative approach has inspired new paradigms for AI-driven real-time and downtime-tolerant monitoring tasks in mission-critical industrial applications [25] , where such systems benefit from the high availability characteristic of modern cloud computing infrastructure and the low-latency capabilities afforded by edge computing deployments. Therefore, a RTM fault diagnosis model optimized for distributed deployment, coupled with its edge-cloud collaboration empowered model inference framework is proposed in this paper, where model components are strategically partitioned and offloaded jointly across cloud and edge rather than relying solely on cloud or local to facilitate reliability and faster response. The main contributions of this paper can be summarized as: ‚Ä¢ A parallel-optimized RTM fault diagnosis model is developed with model integration technique. The model incorporates an enhanced three-stage segmentation scheme as prior knowledge and the outputs of multiple sub-classifiers are fused by a fuzzy-based ensemble mechanism to form the final classification result. ‚Ä¢ A cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is proposed to address the real-time and robustness challenges of distributed fault diagnosis. CEC-PA partitions the integrated model components into pipelines and intelligently schedules them across all worker nodes. Additionally, a downtime-tolerant mechanism is proposed to ensure system robustness. ‚Ä¢ Extensive experiments are conduced to evaluate the effectiveness of the proposed fault detection model and CEC-PA framework. Results showcase our ensemble-based fault diagnosis model produce accurate predictions across all fault types and CEC-PA outperform other approaches in terms of real-time performance and reliability. The rest of this paper is organized as follows: Section II discusses previous works on parallelization techniques in distributed AI. Section III presents the preliminary discussion on the working principle and current pattern analysis of three-stage turnouts. Section IV establishes the time consumption model and multi-objective optimization problem of the proposed cloud-edge RTM fault early-warning system. Section V implements the parallel-optimized turnout fault diagnosis scheme and provides a detailed description of the interactions between each module. Section VI presents the design details of CEC-PA. Section VII demonstrates the effectiveness of the fault diagnosis model and CEC-PA through comparative experiments. Finally, Section VIII draws a conclusion of this paper and highlights its future research directions. TABLE I: Comparison of Different Parallelization Strategies Key Characteristics Data Parallelization Model Parallelization Pipeline Parallelization Applicable scenarios Large datasets with smaller models Extremely large models Long pipelines Proof of convergence ‚úì‚úì\checkmark‚úì √ó\times√ó ‚úì‚úì\checkmark‚úì Heterogeneous cluster support ‚úì‚úì\checkmark‚úì √ó\times√ó ‚úì‚úì\checkmark‚úì Load balance √ó\times√ó ‚úì‚úì\checkmark‚úì ‚úì‚úì\checkmark‚úì Communication overhead High Low Moderate Implementation difficulty Low High Moderate Scalability High Moderate High"
https://arxiv.org/html/2411.02003v1,Against Multifaceted Graph Heterogeneity via Asymmetric Federated Prompt Learning,"Federated Graph Learning (FGL) aims to collaboratively and privately optimize graph models on divergent data for different tasks. A critical challenge in FGL is to enable effective yet efficient federated optimization against multifaceted graph heterogeneity to enhance mutual performance. However, existing FGL works primarily address graph data heterogeneity and perform incapable of graph task heterogeneity. To address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to efficiently enable prompt-based asymmetric graph knowledge transfer between multifaceted heterogeneous federated participants. Generally, we establish a split federated framework to preserve universal and domain-specific graph knowledge, respectively. Moreover, we develop two algorithms to eliminate task and data heterogeneity for advanced federated knowledge preservation. First, a Hierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task beneficial knowledge that is hierarchically distilled according to the directional transferability. Second, a Virtual Prompt Graph (VPG) adaptively generates graph structures to enhance data utility by distinguishing dominant subgraphs and neutralizing redundant ones. We conduct theoretical analyses and extensive experiments to demonstrate the significant accuracy and efficiency effectiveness of FedGPL against multifaceted graph heterogeneity compared to state-of-the-art baselines on large-scale federated graph datasets.","Federated Graph Learning (FGL) has become an attractive research direction for distributed Graph Neural Network (GNN) optimization on isolated graph data without explicit information exposure (Liu et al., 2024). Specifically, in FGL, several participants maintain their private graphs collected from different domains, and they aim to collaboratively train GNNs that can provide beneficial knowledge of raw graph data for particular downstream tasks, respectively. Recent FGL studies have achieved significant progress mainly in learning more effective graph representation to enhance downstream task prediction (Guo et al., 2024). For example, FedSage+ (Zhang et al., 2021b) is proposed based on a standard FGL pipeline where subgraphs from different silos are independently distributed, and improve the node classification via missing edge generation. Despite these advancements, the difficulty of graph data heterogeneity remains paramount. Graphs inherently differ in terms of node and edge types, structural configurations, and other characteristics. Traditional graph learning methodologies often operate under the assumption of homogeneity, which can lead to biased or suboptimal models when applied to heterogeneous graph domains. Addressing graph data heterogeneity is crucial for FGL, as it enables the model to capture diverse intrinsic graph characteristics and enhances overall performance. Therefore, FedStar (Tan et al., 2023b) investigates the data heterogeneity of graph features and shared knowledge, which attempts to achieve outperformance in a Non-Independent and Non-Identically Distributed (Non-IID) issue setting. In addition, FedLIT (Xie et al., 2023) detects link-type heterogeneity to decrease the harmful structure to allow more beneficial message passing. Generally, these works designed effective methods to tackle the graph data heterogeneity in terms of features and structures, where they demand participants to train their models for a consistent downstream task. However, while attempts have been made to address data heterogeneity, the critical issue of task heterogeneity remains unexplored. FGL participants originate from various sectors, each with its own set of tasks and data characteristics. For instance, healthcare providers may focus on patient data analysis, while financial institutions might prioritize fraud detection. Addressing task heterogeneity allows FGL to adapt to these varied needs, enhancing its applicability and relevance across different applications. Existing federated algorithms, which typically assume a uniform model structure across clients, cannot accommodate the need for diverse architectures in graph learning (Sun et al., 2023b). This limitation complicates parameters and knowledge sharing between clients working on different tasks, potentially causing inferior effectiveness and efficiency in collaborative optimization. To effectively optimize models against multifaceted heterogeneity, we necessitate a system not only to preserve common knowledge adaptable to various tasks but also to generalize across divergent graph data distributions. Therefore, the core challenge of this work is to simultaneously overcome the multifaceted heterogeneity for more effective and efficient FGL. To address the challenge, we propose a Federated Graph Prompt Learning (FedGPL) framework to federally fine-tune graph models on heterogeneous tasks and data via an efficient prompt-based asymmetric knowledge transfer. Overall, we establish a split framework to simultaneously achieve a universal graph representing and personalized graph prompting to preserve global and local knowledge, respectively. Then, we design tailored algorithms to disentangle and address task and data heterogeneity. On the server side, we develop a Hierarchical Directed Transfer Aggregator (HiDTA) to extract and share asymmetrically beneficial knowledge among task-heterogeneous participants via personalized federated aggregation. On the client side, we devise a lightweight prompting module, called Virtual Prompt Graph (VPG), to adaptively generate augmented graph data by distilling more dominant information with minor data heterogeneity. We provide theoretical analyses that demonstrate the effectiveness in reducing task and data heterogeneity, as well as the significant reduction of memory and communication costs. Extensive experiments validate that FedGPL outperforms baseline methods across three levels of tasks against multifaceted graph heterogeneity on five datasets. Notably, we evaluate FedGPL in a typical large-scale FGL system consisting of 1111 million node data. Our method achieves 5.3√ó‚àº6.0√ó5.3\times\sim 6.0\times5.3 √ó ‚àº 6.0 √ó GPU memory efficiency, 2.1√ó‚àº3.7√ó2.1\times\sim 3.7\times2.1 √ó ‚àº 3.7 √ó communication efficiency, and 1.3√ó‚àº1.9√ó1.3\times\sim 1.9\times1.3 √ó ‚àº 1.9 √ó training time efficiency. The efficiency superiority demonstrates its scalability for massive FGL participants and large-scale graph data. Our main contributions can be concluded as: (1) To our knowledge, our work is the first to study both task and data heterogeneity in FGL, addressing gaps overlooked by previous research. (2) We propose a federated graph prompt learning framework to effectively enable federated optimization of personalized models among task- and data- heterogeneous participants. (3) We develop an aggregation algorithm to deliver task-specific knowledge based on a transferability-aware hierarchy to asymmetrically enhance model performance. Moreover, we devise a virtual graph prompt to jointly highlight dominant graph structures and alleviate extensive data heterogeneity. (4) We theoretically analyze the mitigation of multifaceted heterogeneity by FedGPL. Besides, we conduct extensive experiments to prove the accuracy and efficiency superiority of FedGPL against FGL with multifaceted heterogeneity on federated graph datasets with million nodes."
https://arxiv.org/html/2411.01825v1,FedReMa: Improving Personalized Federated Learning via Leveraging the Most Relevant Clients,"Federated Learning (FL) is a distributed machine learning paradigm that achieves a globally robust model through decentralized computation and periodic model synthesis, primarily focusing on the global model‚Äôs accuracy over aggregated datasets of all participating clients. Personalized Federated Learning (PFL) instead tailors exclusive models for each client, aiming to enhance the accuracy of clients‚Äô individual models on specific local data distributions. Despite of their wide adoption, existing FL and PFL works have yet to comprehensively address the class-imbalance issue, one of the most critical challenges within the realm of data heterogeneity in PFL and FL research. In this paper, we propose FedReMa, an efficient PFL algorithm that can tackle class-imbalance by 1) utilizing an adaptive inter-client co-learning approach to identify and harness different clients‚Äô expertise on different data classes throughout various phases of the training process, and 2) employing distinct aggregation methods for clients‚Äô feature extractors and classifiers, with the choices informed by the different roles and implications of these model components. Specifically, driven by our experimental findings on inter-client similarity dynamics, we develop critical co-learning period (CCP), wherein we introduce a module named maximum difference segmentation (MDS) to assess and manage task relevance by analyzing the similarities between clients‚Äô logits of their classifiers. Outside the CCP, we employ an additional scheme for model aggregation that utilizes historical records of each client‚Äôs most relevant peers to further enhance the personalization stability. We demonstrate the superiority of our FedReMa in extensive experiments. The code is available at https://github.com/liangh68/FedReMa.Keywords: Personalized Federated Learning, Class-Imbalance, Relevant Matching","Federated learning (FL) enables collaborative model training across decentralized devices or data sources, preserving privacy and scalability while harnessing the collective intelligence of distributed data [18]. However, this algorithm based on periodic synchronization requires all client models to share the same structure and parameters, which brings about huge heterogeneity problems [31, 33, 30]. Unlike traditional FL approaches that view data heterogeneity across clients as an obstacle, personalized federated learning (PFL) aims to exploit these data variations and train distinct models so as to tailor clients‚Äô individual models to their local optimization subproblems under heterogeneous data distributions, focusing more on model personalization. This paradigm shift in machine learning opens up new possibilities for personalized services, recommendations, and predictions, all while respecting user privacy. Nevertheless, PFL still encounters numerous challenges. Firstly, disparate data distributions arise from varying data preferences, causing label skew and feature skew, known as the non-IID data challenge. Among all the subcategories of non-IID data issue, class-imbalance is arguably the most challenging one. While numerous studies have dedicated to solving class-imbalance in centralized machine learning settings [25, 29, 11], these methods are not readily applicable to PFL, given its unique challenges posed by the distributed and privacy-preserving nature, which prevents sharing individual client data to balance the overall dataset. Moreover, uniform aggregation operations adopted in most FL studies employed during each period of model synchronization can be detrimental from a local perspective [2]. This occurs because each model, meticulously trained for a specific client‚Äôs data, may amalgamate with models harboring unfavorable knowledge from diverse distributions of other clients. Therefore, numerous PFL studies propose various methods to mitigate performance degradation due to standard model aggregation under non-IID data. Many of the leading PFL studies focus on retaining personalization layers locally [20, 1, 4, 2, 16] or adapting them to local tasks through certain modifications [24, 21, 8, 22, 5, 17, 23]. Figure 1: Average local accuracy of advanced PFL methods can even fall below FedAvg when the number of trainable samples decreases. However, in practice, PFL may encounter scenarios where clients‚Äô data can be both sparse and imbalanced. Most of the PFL works do not fully leverage the dynamic inter-client relationships, which should be exploited to improve model generalization along with personalization [20, 1, 4, 22, 32]. Moreover, methods using personalized layers, represented by FedPer [1] and FedRep [4], do not enable classifier synchronization among clients and the PS, and they show disappointing performance. where an extensive network encompasses a multitude of clients with sparse data, PFL methods struggle significantly with generalization. Our experiments (Figure 1) demonstrate that, even the vanilla algorithm FedAvg [18] can outperform PFL methods when the number data samples decreases. Besides, the method like FedPer, which disables classifier sharing, suffer from a more obvious performance degradation than FedPAC [28], a representative method that leverages classifier collaboration. This result reveals the importance of the knowledge transfer from clients‚Äô classifiers for improving generalization in PFL. We then gain our first insight: generalization capability needs to be retained for PFL, even though personalization is its primary goal. Exploiting inter-client classifier collaboration. Given the above observation, it is imperative to employ a method that can gauge the underlying correlation and leverage the valuable inter-client knowledge to optimize local classifiers. Some studies [3, 6] suggest assigning higher selection probabilities to clients with larger loss values for aggregation. However, selection based on training indicators often ignores the correlation between clients. A few works improve this strategy. FedPer++ [27] uses a feature regularization training strategy to mitigate local overfitting risks and reduce parameter differences between clients, thereby promoting global feature extractor aggregation. A more recent work, FedPAC [28], quantifies the benefit of a combination of classifiers for each client as a function of the combination weights and derives an optimization problem to estimate the optimal weights. Yet, acquiring the optimal weights in FedPAC necessitates solving a complex optimization problem, significantly prolonging training time. Moreover, this algorithm still exhibits suboptimal performance in scenarios involving sparse or homogeneous data. Therefore, existing classifier collaboration needs to be improved in terms both time efficiency and the ability to boost model generalizability and personalizability. Intuitively, these two capabilities might seem to conflict with each other. However, our second insight implies that, selectively knowledge utilization from peers, whose models can preserve and enhance the strength in its own dominant classes while improving performance in other classes without compromising their existing strength, can improve both generalization and personalization. An intrinsic question then arises: How to design a lightweight method to identify the most relevant peers and effectively utilize their knowledge for each individual client to achieve these capabilities? To answer this question, we make the following technical contributions: ‚Ä¢ We introduce an adaptive approach called FedReMa that leverages the expertise of different clients on specific data classes throughout the training process. This approach helps address class-imbalance issues by dynamically identifying and harnessing clients‚Äô expertise in different phases. ‚Ä¢ We identify a critical co-learning period (CCP) where we measure the task relevance using the similarity of clients‚Äô logits. To filter out irrelevant contributions, we introduce the maximum difference segmentation module (MDS). Additionally, outside the CCP, we employ a scheme based on historical clients‚Äô selections to enhance personalization stability. ‚Ä¢ Through extensive experiments, we demonstrate the superiority of our algorithm, showcasing its effectiveness in personalized federated learning scenarios."
https://arxiv.org/html/2411.01583v1,"Trustworthy Federated Learning: Privacy, Security, and Beyond","While recent years have witnessed the advancement in big data and Artificial Intelligence (AI), it is of much importance to safeguard data privacy and security. As an innovative approach, Federated Learning (FL) addresses these concerns by facilitating collaborative model training across distributed data sources without transferring raw data. However, the challenges of robust security and privacy across decentralized networks catch significant attention in dealing with the distributed data in FL. In this paper, we conduct an extensive survey of the security and privacy issues prevalent in FL, underscoring the vulnerability of communication links and the potential for cyber threats. We delve into various defensive strategies to mitigate these risks, explore the applications of FL across different sectors, and propose research directions. We identify the intricate security challenges that arise within the FL frameworks, aiming to contribute to the development of secure and efficient FL systems.","In recent years, rapid advancements in big data and Artificial Intelligence (AI) technologies have ushered in an era characterized by an unprecedented proliferation of interconnected Internet of Things (IoT) devices and web platforms. This digital tapestry, while instrumental in catalyzing the data revolution, concurrently yields vast quantities of distributed data ‚Äî a significant portion of which is sensitive in nature. Notably, there exists a gap in the adequate protection of this sensitive information, a critical oversight in the current data-centric world. The emergent challenges have not gone unnoticed at the legislative level. A myriad of regulations, including such as the Cybersecurity Law of the People‚Äôs Republic (CLPR) of China CCL , the General Data Protection Regulation (GDPR) GDPR , the California Consumer Privacy Act (CCPA) CCPA , and the Consumer Privacy Bill of Rights (CPBR) Gaff2014 , have been established to safeguard the privacy and security of raw data. Current estimations indicate that these privacy legislations may encompass up to 75% of the global population Gartner , necessitating over 80% of worldwide enterprises to conform by the culmination in 2023. In this dynamic landscape, the development and deployment of sophisticated defense methodologies are imperative. Such techniques are pivotal to maintain data privacy and security throughout the lifecycle of machine learning models, including both training and inference phases. Traditional centralized machine learning paradigms necessitate the aggregation of data at a single server or data center, serving as the nexus for both training and inference operations. Training, an iterative process, refines machine learning model parameters through specific algorithms and can be computationally intensive and time consuming wang2019distributed . In contrast, the inference phase leverages these trained models to deduce predictions or classifications 2021FromDM . The introduction of Distributed Machine Learning (DML) techniques augments both the accuracy and computational efficiency of the model training process. However, this decentralization inevitably exacerbates concerns regarding data privacy and security. Federated Learning (FL) emerges as a pivotal solution in this context. Instead of transferring raw data, which incurs potential privacy violation, FL facilitates the dissemination of a global model to individual devices. These devices, in turn, harness local data to refine the model. Post-training, the local devices relay the updated model parameters to the central server for amalgamation. This iterative process is perpetuated until model convergence is achieved, ensuring data remains local, thereby supporting privacy and security. While FL offers clear advantages, it also faces challenges. Weak points in the communication links between devices and central servers can lead to cyberattacks. In addition, if servers or devices are compromised, they might bring in malicious activities, threatening the overall security of the system. FL has emerged as a key development in the field of modern DML. Numerous surveys delve into the fundamental aspects of FL, discussing topics like deployment architecture, system lifecycle, defining characteristics, classifications, and the range of open-source tools available 2021FLSurvey ; QinbinLi2021ASO ; 2020FederatedLA ; 2021FromDM ; wen2023survey . Recent studies analyze FL within the software engineering domain, providing insights into the detailed processes involved in developing FL systems 2021Software ; supriya2023survey . In terms of application, significant works focus on specific application of FL. Important studies in this realm cover multiple topics, such as edge computing QiXia2021ASO , integration methods for the IoT and the Industrial IoT (IIoT) 2021IoTSurvey ; MParimala2021FusionOF ; VirajKulkarni2020 , strategies centered on personalization 2020TowardsUU , and in-depth reviews exploring the economic impact of FL adoption Zhou2021ASO . In terms of security, a plethora of seminal works have structured frameworks that elucidate the intricacies of FL security MOTHUKURI2021619 ; zhang2023survey . Alongside these, there are focused analyses identifying potential security risks, with an emphasis on the security and privacy challenges within FL 2020ThreatsTF ; ratnayake2023review . The growing concerns about privacy breaches are a significant topic of interest in recent discussions XuefeiYin2021ACS ; rodriguez2023survey . A consistent finding across these studies is the presence of challenges during the development and deployment of FL, especially concerning system vulnerabilities and device reliability 2021Kairouz . These studies reveal a research shortfall, emphasizing the need for in-depth investigations into security, privacy, and relevant defensive strategies within the FL paradigm. Thus, based on the FL system architecture, we explore the security and privacy issues faced at each architecture layer and comprehensively discuss the existing defense techniques designed to enhance the ability to resist various types of security vulnerabilities. The notable contributions of this paper are as follows: ‚Ä¢ We propose a universal FL system architecture that encompasses infrastructure, algorithms, and user services. This architecture aids in the evaluation of existing FL systems. The current literature lacks this holistic view, which distinguishes our work from existing surveys. ‚Ä¢ We provides a comprehensive overview of the security and privacy issues present in FL, as well as the primary attack methods. And also discuss a range of defense techniques against these attacks, offering practical guidance for system developers. ‚Ä¢ We analyze the applications of FL systems and identify future research directions. This contribution enriches the discourse on FL and highlights opportunities for further development. The remainder of this paper is organized as follows. In Section 2, we introduce the fundamental concepts of FL and elucidate the proposed FL system architecture. Section 3 delves into the prevalent security issues associated with FL. While some threats in FL might arise non-maliciously due to device malfunctions or unpredictable participant behaviors, there are malicious threats that intentionally aim to undermine the system. These can manifest as data poisoning, model corruption, and inference attacks. The decentralized framework of FL offers enhanced privacy in machine learning but also introduces numerous security challenges gabrielli2023survey . To address these challenges, we discuss a variety of defense measures. These strategies, crafted considering vulnerabilities from both client devices and central servers, are mainly classified into proactive and reactive types. Proactive defenses aim to preemptively identify and mitigate threats, while reactive defenses come into play once an attack has been detected. Technologies that have been extensively researched in this context include encryption, Differential Privacy (DP), and anomaly detection. Section 4 explores the various applications of FL. This section is followed by Section 5, which elucidates challenges and potential future research directions. Finally, Section 6 provides a conclusion to the paper."
https://arxiv.org/html/2411.01548v1,Analysis of Regularized Federated Learning,"Federated learning is an efficient machine learning tool for dealing with heterogeneous big data and privacy protection. Federated learning methods with regularization can control the level of communications between the central and local machines. Stochastic gradient descent is often used for implementing such methods on heterogeneous big data, to reduce the communication costs. In this paper, we consider such an algorithm called Loopless Local Gradient Descent which has advantages in reducing the expected communications by controlling a probability level. We improve the method by allowing flexible step sizes and carry out novel analysis for the convergence of the algorithm in a non-convex setting in addition to the standard strongly convex setting. In the non-convex setting, we derive rates of convergence when the smooth objective function satisfies a Polyak-≈Åojasiewicz condition. When the objective function is strongly convex, a sufficient and necessary condition for the convergence in expectation is presented.","Nowadays, we face more and more big data that cannot be processed by traditional machine learning (ML) methods due to the size or collected to a single machine due to the issue of privacy protection. A well-developed approach to tackle this, distributed learning [1, 2, 3], is to process data subsets by local machines separately, send the individual model parameter updates to a central machine for global model updates, and then communicate back to the local machines. This approach needs high levels of data communication and may have difficulty in handling heterogeneous data. Federated learning (FL) [4, 5, 6, 7, 8, 9] was proposed to overcome these difficulties. FL is a popular AI basic technology designed for learning involving many independent local machines such as mobile phones of individual users which can process limited scales of data effectively. It has some advantages in data privacy protection, data security, data access rights and access to heterogeneous data, and has powerful applications in financial technologies, telecommunications, IoT, defence, pharmaceutics, and some other practical domains. The prevalent optimization formulation of FL [10] is an empirical risk minimization problem minx‚àà‚Ñùd‚Å°1n‚Å¢‚àëi=1nfi‚Å¢(x),subscriptùë•superscript‚Ñùùëë1ùëõsuperscriptsubscriptùëñ1ùëõsubscriptùëìùëñùë•\min\limits_{x\in\mathbb{R}^{d}}\frac{1}{n}\sum_{i=1}^{n}f_{i}(x),roman_min start_POSTSUBSCRIPT italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) , (1) where n‚àà‚Ñïùëõ‚Ñïn\in\mathbb{N}italic_n ‚àà blackboard_N is the number of local machines or devices, x‚àà‚Ñùdùë•superscript‚Ñùùëëx\in\mathbb{R}^{d}italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT encodes the dùëëditalic_d parameters of a global model (e.g., weights and biases of deep neural networks), and fi‚Å¢(x)subscriptùëìùëñùë•f_{i}(x)italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) represents the aggregate loss of the iùëñiitalic_i-th local machine when the model parameter xùë•xitalic_x is used. The loss fi‚Å¢(x)subscriptùëìùëñùë•f_{i}(x)italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) may have the form fi‚Å¢(x)=EŒæ‚àºDi‚Å¢[f‚Å¢(x,Œæ)]subscriptùëìùëñùë•subscriptùê∏similar-toùúâsubscriptùê∑ùëñdelimited-[]ùëìùë•ùúâf_{i}(x)=E_{\xi\sim D_{i}}[f(x,\xi)]italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x ) = italic_E start_POSTSUBSCRIPT italic_Œæ ‚àº italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ italic_f ( italic_x , italic_Œæ ) ] when the local error induced by a sample Œæùúâ\xiitalic_Œæ and a model parameter xùë•xitalic_x equals f‚Å¢(x,Œæ)ùëìùë•ùúâf(x,\xi)italic_f ( italic_x , italic_Œæ ) and Disubscriptùê∑ùëñD_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the data distributed on the iùëñiitalic_i-th local machine. Note that Disubscriptùê∑ùëñD_{i}italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT can be remarkably different across local machines, representing the data heterogeneity caused by the diversity in interests, financial markets, occupations, ages, genders, and many other factors. Ideal federated learning models should perform well for dealing with heterogeneous data. A classical approach to solve (1) is the FederatedAveraging (FedAvg) algorithm [4]. This method performs well in dealing with imbalanced data distributions and non-convex problems and allows high-quality models to be trained in relatively few rounds of communications. However, FedAvg comes with poor convergence guarantees when data are heterogeneous. It also fails to provide theoretical improvements in the communication complexity over gradient descent algorithms, even its variants such as local gradient descent [11]. In this paper we are interested in a unified formulation of FL models introduced in [12] which is a regularized empirical risk minimization problem minx1,‚ãØ,xn‚àà‚Ñùd‚Å°{F‚Å¢(x):=f‚Å¢(x)+Œª‚Å¢œà‚Å¢(x)},subscriptsubscriptùë•1‚ãØsubscriptùë•ùëõsuperscript‚Ñùùëëassignùêπùë•ùëìùë•ùúÜùúìùë•\min\limits_{x_{1},\cdots,x_{n}\in\mathbb{R}^{d}}\{F(x):=f(x)+\lambda\psi(x)\},roman_min start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚ãØ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { italic_F ( italic_x ) := italic_f ( italic_x ) + italic_Œª italic_œà ( italic_x ) } , (2) where 0‚â§Œª‚â§‚àû0ùúÜ0\leq\lambda\leq\infty0 ‚â§ italic_Œª ‚â§ ‚àû is a regularization or penalty parameter, x:=(xi)i=1n‚àà‚Ñùn‚Å¢dassignùë•superscriptsubscriptsubscriptùë•ùëñùëñ1ùëõsuperscript‚Ñùùëõùëëx:=(x_{i})_{i=1}^{n}\in\mathbb{R}^{nd}italic_x := ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT is the model vector formed by the model parameters x1,‚ãØ,xn‚àà‚Ñùdsubscriptùë•1‚ãØsubscriptùë•ùëõsuperscript‚Ñùùëëx_{1},\cdots,x_{n}\in\mathbb{R}^{d}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚ãØ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT for the nùëõnitalic_n local machines, f‚Å¢(x):=1n‚Å¢‚àëi=1nfi‚Å¢(xi)assignùëìùë•1ùëõsuperscriptsubscriptùëñ1ùëõsubscriptùëìùëñsubscriptùë•ùëñf(x):=\frac{1}{n}\sum_{i=1}^{n}f_{i}(x_{i})italic_f ( italic_x ) := divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) is the global error function associated with the individual aggregate loss {fi:‚Ñùn‚Üí‚Ñù}i=1nsuperscriptsubscriptconditional-setsubscriptùëìùëñ‚Üísuperscript‚Ñùùëõ‚Ñùùëñ1ùëõ\{f_{i}:\mathbb{R}^{n}\to\mathbb{R}\}_{i=1}^{n}{ italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚Üí blackboard_R } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and œà‚Å¢(x):=12‚Å¢n‚Å¢‚àëi=1n‚Äñxi‚àíx¬Ø‚Äñ2assignùúìùë•12ùëõsuperscriptsubscriptùëñ1ùëõsuperscriptnormsubscriptùë•ùëñ¬Øùë•2\psi(x):=\frac{1}{2n}\sum_{i=1}^{n}\|x_{i}-\bar{x}\|^{2}italic_œà ( italic_x ) := divide start_ARG 1 end_ARG start_ARG 2 italic_n end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚à• italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¬Ø start_ARG italic_x end_ARG ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT is the regularization term with x¬Ø=1n‚Å¢‚àëi=1nxi¬Øùë•1ùëõsuperscriptsubscriptùëñ1ùëõsubscriptùë•ùëñ\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}over¬Ø start_ARG italic_x end_ARG = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT being the average of the local model parameters. The regularization parameter ŒªùúÜ\lambdaitalic_Œª for the regularization term measures the level of communications in the FL system where the central and local machines exchange information such as gradients or model parameters. When Œª=0ùúÜ0\lambda=0italic_Œª = 0, the optimization problem (2) becomes a local one, meaning that we solve the optimization problem by minimizing each fisubscriptùëìùëñf_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over ‚Ñùdsuperscript‚Ñùùëë\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT separately. In this situation, we do not need communications. When Œª=‚àûùúÜ\lambda=\inftyitalic_Œª = ‚àû, each xisubscriptùë•ùëñx_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT equals to the same vector x¬Ø¬Øùë•\bar{x}over¬Ø start_ARG italic_x end_ARG and (2) is equivalent to the global optimization problem (1) for x¬Ø‚àà‚Ñùd¬Øùë•superscript‚Ñùùëë\bar{x}\in\mathbb{R}^{d}over¬Ø start_ARG italic_x end_ARG ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. When 0<Œª<‚àû0ùúÜ0<\lambda<\infty0 < italic_Œª < ‚àû, we consider a mixed optimization problem, where the regularization term controls how the models in the individual local machines are similar. In other words, the first term of (2) involves the nùëõnitalic_n individual local aggregate losses. In contrast, the regularization term guarantees the individual local models to be close to the average one at a certain level. An appropriate level of communications are needed for solving the mixed problem in this case. When the size n‚Å¢dùëõùëënditalic_n italic_d of the data is large, a natural method for solving the optimization problem (2) is the standard gradient descent (GD), involving updates of the full gradient of FùêπFitalic_F in each iteration. But the communication cost would be extremely high with this method. To see this, notice that for each iteration, the gradients for the losses of the local machines are calculated and sent to the central machine, then these are averaged and returned to the local machines (locals‚Üí‚Üí\rightarrow‚Üícentral‚Üí‚Üí\rightarrow‚Üílocals). Therefore GD needs two rounds of communications per iteration, which is impractical for many real-world implementations with big data such as mobile updating. To reduce the communication costs of GD, a stochastic gradient descent (SGD) method called L2GD (Loopless Local Gradient Descent) was introduced in [12]. The idea of this method is to separate the gradient of FùêπFitalic_F into those of fùëìfitalic_f and œàùúì\psiitalic_œà with probability, controlled by a probability level 0<p<10ùëù10<p<10 < italic_p < 1. Define a stochastic gradient of FùêπFitalic_F at x‚àà‚Ñùn‚Å¢dùë•superscript‚Ñùùëõùëëx\in\mathbb{R}^{nd}italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n italic_d end_POSTSUPERSCRIPT as G(x):={‚àáf‚Å¢(x)1‚àípwith probability 1‚àíp,Œª‚Å¢‚àáœà‚Å¢(x)pwith probability p.G(x):=\left\{\begin{aligned} &\frac{\nabla f(x)}{1-p}\quad&\text{with % probability }&\quad&1-p,\\ &\frac{\lambda\nabla\psi(x)}{p}&\text{with probability }&&p.\quad\end{aligned}\right.italic_G ( italic_x ) := { start_ROW start_CELL end_CELL start_CELL divide start_ARG ‚àá italic_f ( italic_x ) end_ARG start_ARG 1 - italic_p end_ARG end_CELL start_CELL with probability end_CELL start_CELL end_CELL start_CELL 1 - italic_p , end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL divide start_ARG italic_Œª ‚àá italic_œà ( italic_x ) end_ARG start_ARG italic_p end_ARG end_CELL start_CELL with probability end_CELL start_CELL end_CELL start_CELL italic_p . end_CELL end_ROW (3) It is straightforward that G‚Å¢(x)ùê∫ùë•G(x)italic_G ( italic_x ) is an unbiased estimator of ‚àáF‚Å¢(x)‚àáùêπùë•\nabla F(x)‚àá italic_F ( italic_x ), which means ùîº‚Å¢[G‚Å¢(x)]=‚àáF‚Å¢(x)ùîºdelimited-[]ùê∫ùë•‚àáùêπùë•\mathbb{E}[G(x)]=\nabla F(x)blackboard_E [ italic_G ( italic_x ) ] = ‚àá italic_F ( italic_x ). This unbiased estimate for the gradient leads to the updating rule of L2GD as xk+1=xk‚àíŒ±‚Å¢G‚Å¢(xk),superscriptùë•ùëò1superscriptùë•ùëòùõºùê∫superscriptùë•ùëòx^{k+1}=x^{k}-\alpha G(x^{k}),italic_x start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT = italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_Œ± italic_G ( italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) , (4) where 0<Œ±<‚àû0ùõº0<\alpha<\infty0 < italic_Œ± < ‚àû is the step size (or learning rate) of the algorithm. Specifically, in each iteration, we generate a Bernoulli random variable Œæùúâ\xiitalic_Œæ with ùêèùê´‚Å¢(Œæ=1)=pùêèùê´ùúâ1ùëù\mathbf{Pr}(\xi=1)=pbold_Pr ( italic_Œæ = 1 ) = italic_p and ùêèùê´‚Å¢(Œæ=0)=1‚àípùêèùê´ùúâ01ùëù\mathbf{Pr}(\xi=0)=1-pbold_Pr ( italic_Œæ = 0 ) = 1 - italic_p. If Œæ=1ùúâ1\xi=1italic_Œæ = 1, L2GD will calculate the average model x¬Ø¬Øùë•\bar{x}over¬Ø start_ARG italic_x end_ARG and force the local model xisubscriptùë•ùëñx_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT closing to the average. Otherwise, L2GD will process the local GD step for all the local machines. Communications at an iteration are needed only if Œæk=0,Œæk+1=1formulae-sequencesubscriptùúâùëò0subscriptùúâùëò11\xi_{k}=0,\xi_{k+1}=1italic_Œæ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 0 , italic_Œæ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = 1. An advantage of L2GD is to reduce the expected communications by controlling the probability pùëùpitalic_p. Convergence of L2GD was considered in [12] when the step size is fixed. Let x‚Å¢(Œª)ùë•ùúÜx(\lambda)italic_x ( italic_Œª ) be the minimizer of (2). When each function fisubscriptùëìùëñf_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with i‚àà{1,‚Ä¶,n}ùëñ1‚Ä¶ùëõi\in\{1,\ldots,n\}italic_i ‚àà { 1 , ‚Ä¶ , italic_n } is L‚àílimit-fromùêøL-italic_L -smooth and Œºùúá\muitalic_Œº-strongly convex (to be defined below) and 0<Œ±‚â§12‚Å¢‚Ñí0ùõº12‚Ñí0<\alpha\leq\frac{1}{2\mathcal{L}}0 < italic_Œ± ‚â§ divide start_ARG 1 end_ARG start_ARG 2 caligraphic_L end_ARG, it was shown in [12] that ùîº‚Å¢[‚Äñxk‚àíx‚Å¢(Œª)‚Äñ2]‚â§(1‚àíŒ±‚Å¢Œºn)k‚Å¢‚Äñx0‚àíx‚Å¢(Œª)‚Äñ2+18‚Å¢n‚Å¢Œ±‚Å¢œÉ2Œº,‚àÄk‚àà‚Ñï,formulae-sequenceùîºdelimited-[]superscriptnormsuperscriptùë•ùëòùë•ùúÜ2superscript1ùõºùúáùëõùëòsuperscriptnormsuperscriptùë•0ùë•ùúÜ218ùëõùõºsuperscriptùúé2ùúáfor-allùëò‚Ñï\mathbb{E}[\|x^{k}-x(\lambda)\|^{2}]\leq(1-\frac{\alpha\mu}{n})^{k}\|x^{0}-x(% \lambda)\|^{2}+\frac{18n\alpha\sigma^{2}}{\mu},\qquad\forall k\in\mathbb{N},blackboard_E [ ‚à• italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_x ( italic_Œª ) ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ‚â§ ( 1 - divide start_ARG italic_Œ± italic_Œº end_ARG start_ARG italic_n end_ARG ) start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ‚à• italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT - italic_x ( italic_Œª ) ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG 18 italic_n italic_Œ± italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Œº end_ARG , ‚àÄ italic_k ‚àà blackboard_N , (5) where ‚Ñí:=1n‚Å¢max‚Å°{(1+2‚Å¢p)‚Å¢L1‚àíp,(3‚àí2‚Å¢p)‚Å¢Œªp}assign‚Ñí1ùëõ12ùëùùêø1ùëù32ùëùùúÜùëù\mathcal{L}:=\frac{1}{n}\max\{\frac{(1+2p)L}{1-p},\frac{(3-2p)\lambda}{p}\}caligraphic_L := divide start_ARG 1 end_ARG start_ARG italic_n end_ARG roman_max { divide start_ARG ( 1 + 2 italic_p ) italic_L end_ARG start_ARG 1 - italic_p end_ARG , divide start_ARG ( 3 - 2 italic_p ) italic_Œª end_ARG start_ARG italic_p end_ARG }, and œÉ2:=1n2‚Å¢‚àëi=1n(11‚àíp‚Å¢‚Äñ‚àáfi‚Å¢(xi‚Å¢(Œª))‚Äñ2+Œª2p‚Å¢‚Äñxi‚Å¢(Œª)‚àíx¬Ø‚Å¢(Œª)‚Äñ2).assignsuperscriptùúé21superscriptùëõ2superscriptsubscriptùëñ1ùëõ11ùëùsuperscriptnorm‚àásubscriptùëìùëñsubscriptùë•ùëñùúÜ2superscriptùúÜ2ùëùsuperscriptnormsubscriptùë•ùëñùúÜ¬Øùë•ùúÜ2\sigma^{2}:=\frac{1}{n^{2}}\sum_{i=1}^{n}\left(\frac{1}{1-p}\|\nabla f_{i}(x_{% i}(\lambda))\|^{2}+\frac{\lambda^{2}}{p}\|x_{i}(\lambda)-\bar{x}(\lambda)\|^{2% }\right).italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT := divide start_ARG 1 end_ARG start_ARG italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( divide start_ARG 1 end_ARG start_ARG 1 - italic_p end_ARG ‚à• ‚àá italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_Œª ) ) ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG italic_Œª start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_p end_ARG ‚à• italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_Œª ) - over¬Ø start_ARG italic_x end_ARG ( italic_Œª ) ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . The first term of the estimate stated in (5) decays to zero linearly, but the second term involves a variance term œÉ2superscriptùúé2\sigma^{2}italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, which is a constant and does converge to 00 in general. Thus, the algorithm given by L2GD using a fixed step size is sub-optimal in terms of convergence. There have been several variance-reduction methods [13] to overcome the above-mentioned difficulty. One is stochastic average gradient (SAG) [14]. The idea behind SAG is to use an estimate vki‚âà‚àáfi‚Å¢(xk)superscriptsubscriptùë£ùëòùëñ‚àásubscriptùëìùëñsubscriptùë•ùëòv_{k}^{i}\approx\nabla f_{i}(x_{k})italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ‚âà ‚àá italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) for each iùëñiitalic_i and then to compute the full gradient by using the average of the vkisuperscriptsubscriptùë£ùëòùëñv_{k}^{i}italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT values. The second is stochastic dual coordinate ascent (SDCA) [15], based on the property that the coordinates of the gradient provide a naturally variance-reduced estimate of the gradient. Stochastic variance-reduced gradient (SVRG) is another method proposed in [16] to further improve convergence rates over SAG. In addition, SVRG is the first work to use covariates to address the high memory of SAG. This was followed by SAGA [17], a variant of SAG applying the covariates to make an unbiased variant of SAG that has similar performances but is easier to analyze. Then, a class of algorithms, named uniform memorization algorithms [18], are proposed inspired by SVRG and SAGA. Variance-reduced Stochastic Newton (VITE) [21] utilizes Newton‚Äôs methods, basically a first-order approximation, to obtain fast convergence rates. Then, a new amortized variance-reduced gradient (AVRG) algorithm is proposed, which possesses storage and computational efficiency comparable to SAGA and SVRG. Recently, a stochastic quasi-gradient method, named JacSketch [22], leverages the approximation of the Jacobian matrix to update models. One may apply the above variance-reduction methods to the FL optimization problem (2) to remove the variance term in (5). However, new issues emerge with more computational and memory costs and extra communication costs under the FL context, a trade-off between convergence and efficiency. For example, since some VR methods [16, 17, 18, 19, 20] introduce auxiliary variates, we need to compute and store them each time, and then communicate them between local machines and the server. To address the aforementioned issues, we propose Loopless Local Gradient Descent utilizing a Varying step size, named L2GDV. We will utilize a varying step size instead of a fixed one. This approach is intuitive and reasonable since, for the training process, when the trained model gets closer and closer to the best one, we should slow down the step to guarantee that after enough iterations, the model will be stable around the minimum. One can easily see similar computational, memory, and communication efficiencies as the basic SGD methods like L2GD. The scientific question is to determine rigorously if a varying step size suffices to guarantee convergence (without a variance term) and how fast the convergence is. This is answered by our theoretical and experimental results in Section 3 and Section 4 with the following contributions: ‚Ä¢ We extend the convergence results for algorithm (4) to a non-convex situation, where each function fisubscriptùëìùëñf_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is L‚àílimit-fromùêøL-italic_L -smooth, FùêπFitalic_F satisfies a PL condition (to be defined below), and a step size is fixed. ‚Ä¢ For the non-convex situation, we provide rates of convergence when the step size sequence decays polynomially as Œ±k=Œ±1‚Å¢k‚àíŒ∏subscriptùõºùëòsubscriptùõº1superscriptùëòùúÉ\alpha_{k}=\alpha_{1}k^{-\theta}italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT - italic_Œ∏ end_POSTSUPERSCRIPT with 0<Œ∏‚â§10ùúÉ10<\theta\leq 10 < italic_Œ∏ ‚â§ 1 and Œ±1>0subscriptùõº10\alpha_{1}>0italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0. ‚Ä¢ We show in a convex situation where each function fisubscriptùëìùëñf_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is L‚àílimit-fromùêøL-italic_L -smooth and Œºùúá\muitalic_Œº-strongly convex with L,Œº>0ùêøùúá0L,\mu>0italic_L , italic_Œº > 0 and 0<Œ±k‚â§12‚Å¢‚Ñí0subscriptùõºùëò12‚Ñí0<\alpha_{k}\leq\frac{1}{2\mathcal{L}}0 < italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚â§ divide start_ARG 1 end_ARG start_ARG 2 caligraphic_L end_ARG, the algorithm (7) converges with limk‚Üí‚àûùîº‚Å¢[‚Äñxk‚àíx‚Å¢(Œª)‚Äñ2]=0subscript‚Üíùëòùîºdelimited-[]superscriptnormsuperscriptùë•ùëòùë•ùúÜ20\lim_{k\to\infty}\mathbb{E}[\|x^{k}-x(\lambda)\|^{2}]=0roman_lim start_POSTSUBSCRIPT italic_k ‚Üí ‚àû end_POSTSUBSCRIPT blackboard_E [ ‚à• italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT - italic_x ( italic_Œª ) ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = 0 if and only if limk‚Üí‚àûŒ±k=0subscript‚Üíùëòsubscriptùõºùëò0\lim_{k\to\infty}\alpha_{k}=0roman_lim start_POSTSUBSCRIPT italic_k ‚Üí ‚àû end_POSTSUBSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 0 and ‚àëk=1‚àûŒ±k=‚àûsuperscriptsubscriptùëò1subscriptùõºùëò\sum_{k=1}^{\infty}\alpha_{k}=\infty‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àû end_POSTSUPERSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = ‚àû. ‚Ä¢ For the convex situation, we provide rates of convergence when the step size sequence decays polynomially as Œ±k=Œ±1‚Å¢k‚àíŒ∏subscriptùõºùëòsubscriptùõº1superscriptùëòùúÉ\alpha_{k}=\alpha_{1}k^{-\theta}italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT - italic_Œ∏ end_POSTSUPERSCRIPT with 0<Œ∏‚â§10ùúÉ10<\theta\leq 10 < italic_Œ∏ ‚â§ 1 and 0<Œ±1‚â§12‚Å¢‚Ñí0subscriptùõº112‚Ñí0<\alpha_{1}\leq\frac{1}{2\mathcal{L}}0 < italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚â§ divide start_ARG 1 end_ARG start_ARG 2 caligraphic_L end_ARG. ‚Ä¢ We conduct experiments in the non-convex and convex situations to demonstrate the effectiveness of the proposed method, compared with strong baselines in federated learning."
https://arxiv.org/html/2411.01490v1,Anomalous Client Detection in Federated Learning,"Federated learning (FL), with the growing IoT and edge computing, is seen as a promising solution for applications that are latency- and privacy-aware. However, due to the widespread dispersion of data across many clients, it is challenging to monitor client anomalies caused by malfunctioning devices or unexpected events. The majority of FL solutions now in use concentrate on the classification problem, ignoring situations in which anomaly detection may also necessitate privacy preservation and effectiveness. The system in federated learning is unable to manage the potentially flawed behavior of its clients completely. These behaviors include sharing arbitrary parameter values and causing a delay in convergence since clients are chosen at random without knowing the malfunctioning behavior of the client. Client selection is crucial in terms of the efficiency of the federated learning framework. The challenges such as client drift and handling slow clients with low computational capability are well-studied in FL. However, the detection of anomalous clients either for security or for overall performance in the FL frameworks is hardly studied in the literature. In this paper, we propose an anomaly client detection algorithm to overcome malicious client attacks and client drift in FL frameworks. Instead of random client selection, our proposed method utilizes anomaly client detection to remove clients from the FL framework, thereby enhancing the security and efficiency of the overall system. This proposed method improves the global model convergence in almost 50% fewer communication rounds compared with widely used random client selection using the MNIST dataset.","The notion of federated learning (FL) was introduced in 2016 [14]. Its core idea is to train machine learning models on independent datasets dispersed across multiple devices or parties, preserving local data privacy to some level. Since then, FL has grown rapidly and become a popular research area in the field of artificial intelligence [16]. The progress is primarily driven by three factors: the widespread use of machine learning technology, the rapid rise of big data, and global data privacy legislation. Figure 1: Traditional FL Framework Hence, security and privacy are major characteristics of federated learning. For example, FL is susceptible to Byzantine attacks (which try to stop the model from converging) and poisoning attacks (which try to force convergence to an inaccurate model). FL is particularly vulnerable to Byzantine attacks, in which malicious users (Clients) alter trustworthy models or gradients to obstruct learning or purposely contaminate training data, causing the global model to pick up false information [20]. In a traditional FL algorithm as shown in Figure 1, initially, the server sends global model parameters to all the randomly selected clients. The clients run their models with their dataset with the received parameters and send the updated parameter values to the server for aggregation. The aggregated parameter values are again sent by the server to the randomly selected clients. Clients vary greatly in terms of hardware configurations and data distribution in a typical FL environment. As a result, each training round‚Äôs random client sampling may not adequately take advantage of the local updates from heterogeneous clients, which could lead to decreased model accuracy, a slower rate of convergence, compromised fairness, etc. Many client selection methods have been proposed to address the FL client heterogeneity problem, with promising performance improvements. However, none of the client selection methods emphasized identifying the anomalous client and refraining them from participating in the FL network which may also lead to decreased model accuracy, a slower rate of convergence, compromised fairness, and even sabotage the whole FL network. As a solution, we can include another server for maintaining client-server binding or running any privacy-security algorithm to identify the anomalous client, it will enhance the communication and computation complexity of the FL network, which researchers want to minimize for sustainable AI. An anomalous client can be one of the major privacy leakages and (or) security threat scenarios [19]. Three scenarios could result in privacy leaking and (or) security threats if a client is anomalous. Initially, the aggregator can provide the anomalous client with intermediate training updates, which they can use to examine confidential data from other client datasets [2]. Secondly, the anomalous client may submit training updates to the aggregator that are specifically tailored to probe the unique private data of other client datasets. Third, The attacker can be one of the participants in the federated learning, who adversarially modifies his parameter uploads Witsuperscriptsubscriptùëäùëñùë°W_{i}^{t}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. Although these concerns can be mitigated by creating customized training processes, such as choosing a subset of customers for each round, FL is still exposed to a significant risk of anomalous clients leaking confidential information or any other security threat. The client can be malicious or anomalous depending on the type of attack [6]. It is pertinent to mention that whatever the type of attack it is essential to identify the malicious or anomalous clients and refrain them from taking part in the global aggregation because malicious clients can effortlessly incorporate backdoors into the combined model, all the while preserving the model‚Äôs functionality for the primary goal. Moreover, the presence of anomalous clients may render model poisoning. Model poisoning, as opposed to data poisoning, entails a hostile client going for the global model directly [21, 1]. According to research by Bhagoji et al. [1], model positioning attacks have a far greater impact on the model than data poisoning attacks. The modifications made to the client‚Äôs model by the malicious client, which are frequently made in FL before the new model being uploaded to the central server[4], have a direct impact on the global model, as illustrated in Figure 2. Model poisoning affects the performance of the global model and lowers its overall accuracy by manipulating local model gradients using gradient manipulation technologies [9, 8]. For example, in Li et al. [11], when utilizing FL for image recognition, the classifier of an image model may be changed so that it assigns labels chosen by the attacker to specific areas of the image. By teaching rule modification techniques to provide attackers access to the trained model, model poisoning can also be achieved. The attacker can make their attack undetectable by changing the model‚Äôs output, which allows the trained model to be updated normally [9, 10]. Wahab et al. [17] added penalty terms to reduce the discrepancies between the objective functions and appropriate weight update distribution. This enhancement led to the successful deployment of an undetected targeted model poisoning. These motivate us to propose a secure FL algorithm. The proposed FL algorithm not only identifies the anomalous client based on the anomaly score, but this algorithm will also refrain the anomalous clients from participating in global averaging. The main contributions are as follows: ‚Ä¢ We propose a secure federated averaging algorithm to detect anomalous clients. Then refrain from those anomalous clients for further participation. ‚Ä¢ The proposed algorithm is compared with the traditional FedAvg algorithm to validate its performance using iid, non-iid, and non-iid with an unequal number of features. ‚Ä¢ To validate the proposed concept MNIST dataset of handwritten digits is used for the experiment. The proposed algorithm is secure in terms of client selection and assures convergence earlier by 50%. Hence, reduces the communication rounds."
https://arxiv.org/html/2411.01458v1,Two-Timescale Model Caching and Resource Allocation for Edge-Enabled AI-Generated Content Services,"Generative AI (GenAI) has emerged as a transformative technology, enabling customized and personalized AI-generated content (AIGC) services. In this paper, we address challenges of edge-enabled AIGC service provisioning, which remain underexplored in the literature. These services require executing GenAI models with billions of parameters, posing significant obstacles to resource-limited wireless edge. We subsequently introduce the formulation of joint model caching and resource allocation for AIGC services to balance a trade-off between AIGC quality and latency metrics. We obtain mathematical relationships of these metrics with the computational resources required by GenAI models via experimentation. Afterward, we decompose the formulation into a model caching subproblem on a long-timescale and a resource allocation subproblem on a short-timescale. Since the variables to be solved are discrete and continuous, respectively, we leverage a double deep Q-network (DDQN) algorithm to solve the former subproblem and propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm to solve the latter. The proposed D3PG algorithm makes an innovative use of diffusion models as the actor network to determine optimal resource allocation decisions. Consequently, we integrate these two learning methods within the overarching two-timescale deep reinforcement learning (T2DRL) algorithm, the performance of which is studied through comparative numerical simulations.","1.1 Background and Overview Recent breakthroughs in artificial intelligence (AI) have propelled generative AI (GenAI) into the spotlight, drawing significant attention for its unprecedented ability to automate the creation of a diverse array of AI-generated content (AIGC), including text, audio, and graphics/images [1, 2]. GenAI aims to produce synthetic data that closely resemble real-world data by learning underlying patterns and characteristics from existing datasets. For example, ChatGPT [3] generates human-like text based on a given context prompt, while RePaint [4] enables the generation of diverse images from textual descriptions. Building on these advancements, AIGC services have been integrated into various domains, including art, advertising, and education [5], offering productivity gains and economic growth. Despite their tremendous potentials, delivering AIGC services relies on the inference process of GenAI models, where the increasing size and complexity of these models present significant challenges for deployment over the wireless edge. For instance, ChatGPT, which is built upon GPT-3 with 175 billion parameters, requires 8√ó48GB A6000 GPUs to perform inference and generate contextually relevant responses to user prompts [6]. This can be particularly challenging for the next-generation Internet paradigms, such as Metaverse [7], which continuously demand high-quality AIGC on personal computers or head-mounted displays. Specifically, the intensive storage and computation demands of GenAI models can limit the accessibility and affordability of provisioning AIGC services. To address this, cloud data centers with abundant computing and storage capacity can be utilized to train and deploy GenAI models, enabling users to access cloud-based AIGC services through the core network. However, migrating AIGC services to cloud data centers can impose prohibitive traffic congestion on backhaul links [8, 9] and introduce privacy threats within public cloud infrastructure [10]. Alternatively, by leveraging wireless edge networks, we can deploy GenAI models closer to users on edge servers co-located with base stations (BSs) [11] to provide lower latency and privacy-aware AIGC services. In this user-edge-cloud continuum, which we aim to investigate, GenAI models are trained and pre-stored in the could, while edge servers are responsible for caching these models and delivering customizable AIGC services to users. 1.2 Motivation and Main Challenges While edge servers offer several advantages for delivering AIGC services to users, there are still important issues that need to be addressed. First, storage-limited edge servers cannot cache all GenAI models simultaneously, leading to performance degradation for users with diverse AIGC service requests. This degradation occurs because GenAI models rely on massive datasets to learn underlying patterns and generate meaningful outputs, making their performance heavily dependent on the diversity of the training data [12]. To illustrate this, we conducted a study using a corrupted image restoration AIGC service as an example. In this study, the GenAI model RePaint, trained separately on the CelebA-HQ dataset (containing images of celebrities‚Äô faces) and the Places2 dataset (containing images of various scenes) [4], was used to repair a corrupted image. The experiments were conducted on a system equipped with an NVIDIA RTX A5000 GPU. The repair processes shown in Fig. 1 unveil that RePaint trained on the human face dataset performed significantly better than the model trained on landscape images. Therefore, due to the diversity of users‚Äô AIGC service requests, determining which GenAI models to cache at storage-limited edge servers is crucial. Fig. 1: An example of the GenAI model RePaint, trained on different datasets, used to repair the same corrupted image. Second, efficiently managing multi-dimensional network resources to deliver low-latency, high-quality AIGC services from edge servers to users is challenging. Since users may not only send their AIGC requests to edge servers but also retrieve the generated content, both the upload and download processes demand substantial bandwidth. Moreover, GenAI models require significant computational resources to generate content, which must be managed carefully due to the limited resources of edge servers. For instance, the quality of synthetic images produced by diffusion-based GenAI models improves with the number of denoising steps (detailed in Sec. 3.4). However, the generation latency also increases with the number of denoising steps, necessitating a careful balance of the trade-off between high-quality and low-latency AIGC services, especially in networks with large numbers of users. Third, conventional optimization methods for model caching and resource allocation often suffer from high computational complexity, rendering them unsuitable for mobile edge networks, which are subject to temporal variations. In practical scenarios, user locations and wireless channel conditions vary over time, and the popularity of AIGC services can fluctuate due to dynamic trends. As a result, a solution that is optimal at one point in time may not remain optimal over a longer period. However, existing model caching and resource allocation methods, such as iteration-based algorithms [13], [14] and constraint relaxation-based problem transformations [15], [16], either require extensive iterations to converge to a satisfactory solution or suffer from significant performance degradation when the environment undergoes abrupt changes over time. 1.3 Summary of Contributions To our knowledge, this is the first study that optimizes the edge-enabled provisioning of AIGC services by coordinating GenAI model caching and resource allocation decisions in mobile edge networks. Our main contributions are as follows: ‚Ä¢ We formulate the model caching and resource allocation problem in GenAI-enabled wireless networks, which is found to be mixed integer nonlinear programming (MINLP) known to be NP-hard. This makes solving the problem challenging, especially under user mobility, imperfect knowledge of wireless channels, and varying AIGC requests. ‚Ä¢ To tackle the problem, we first divide it into a model caching subproblem on a long-timescale and a resource allocation subproblem on a short-timescale. Since the variables to be solved are discrete and continuous, respectively, we employ a double deep Q-network (DDQN) algorithm to solve the former subproblem and propose a diffusion-based deep deterministic policy gradient (D3PG) algorithm to address the latter. We integrate these two methods within the overarching two-timescale deep reinforcement learning (T2DRL) algorithm. ‚Ä¢ We conduct experiments using practical GenAI models to develop unified mathematical models that describe the relationships between AIGC quality, service provisioning delay and computational resources. Furthermore, in D3PG algorithm we make an innovative use of diffusion models ‚Äì originally designed for image generation ‚Äì to determine optimal resource allocation decisions for AIGC provisioning. ‚Ä¢ We validate the effectiveness of our method through experiments under various simulation settings, demonstrating that T2DRL not only achieves a higher model hitting ratio but also delivers higher-quality, lower-latency AIGC services compared to the benchmark solutions. This improvement is largely attributed to the generative capabilities of the diffusion model, which enhance action sampling efficiency by progressively reducing noise through the reverse process."
https://arxiv.org/html/2411.01433v2,HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE Inference,"The Mixture-of-Experts (MoE) architecture has demonstrated significant advantages in the era of Large Language Models (LLMs), offering enhanced capabilities with reduced inference costs. However, deploying MoE-based LLMs on memory-constrained edge devices remains challenging due to their substantial memory requirements. While existing expert-offloading methods alleviate the memory requirements, they often incur significant expert-loading costs or compromise model accuracy. We present HOBBIT, a mixed precision expert offloading system to enable flexible and efficient MoE inference. Our key insight is that dynamically replacing less critical cache-miss experts with low-precision versions can substantially reduce expert-loading latency while preserving model accuracy. HOBBIT introduces three innovative techniques that map the natural hierarchy of MoE computation: (1) a token-level dynamic expert loading mechanism, (2) a layer-level adaptive expert prefetching technique, and (3) a sequence-level multidimensional expert caching policy. These innovations fully leverage the benefits of mixed-precision expert inference. By implementing HOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate its performance across different edge devices with representative MoE models. The results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding compared to state-of-the-art MoE offloading systems.","The rapid explosion of Large Language Models (LLMs) has led to their widespread application across various fields (zhao2023survey, ). Beyond deploying LLM in cloud-based data centers, there is a growing demand to deploy these models at the edge to address issues like high latency, privacy concerns, and dependence on stable network connections inherent in centralized approaches (friha2024llm, ). Consequently, there is an increasing need to run LLMs on edge devices, bringing intelligence closer to the end-user. Nowadays, both academia (chu2024mobilevlm, ; zhang2024tinyllamaopensourcesmalllanguage, ; xue2024powerinfer, ) and industry (qualcomm, ; huaweai, ; appleai, ) are actively accelerating the deployment of LLMs at the edge. In recent years, the Mixture of Experts (MoE) architecture (shazeer2017outrageously, ) has emerged as a promising approach to enhance LLM capabilities by enabling significant model size expansion while maintaining computational efficiency (jiang2024mixtral, ; abdin2024phi, ; DeepSeekV2, ; qwen_moe, ; Switchtransformers, ). However, MoE-based LLMs demand substantial GPU memory for parameter storage. For instance, the Mixtral-8x7B model (jiang2024mixtral, ), despite activating only 14 billion parameters per token, requires 87GB of memory to store its complete set of 45 billion parameters. This poses significant deployment challenges on memory-constrained edge devices, such as the NVIDIA Jetson AGX Orin with its 32GB memory capacity. To address this limitation, expert-offloading techniques have been developed to enable the execution of these large-scale models on memory-limited devices by exploiting the sparse activation patterns inherent in MoE architectures. In essence, expert-offloading techniques primarily store all non-expert weights and a subset of important experts in GPU memory (referred to as the ‚Äùexpert cache‚Äù), while offloading other experts to CPU memory or SSD (referred to as ‚Äùnext-level memory‚Äù). When the required experts are not available in the expert cache, they are loaded from next-level memory into the cache, evicting some existing experts. However, due to limited memory bandwidth, loading an expert from next-level memory introduces significant latency, which can severely slow down inference. While existing systems optimize expert-offloading with various methods, they still face several limitations, as outlined below. Inflexible and aggressive optimizations of expert loading. When an expert cache miss occurs, directly loading a missing expert incurs significant latency. To mitigate this, EdgeMoE (yi2023edgemoe, ) employs different quantization levels for various experts to reduce I/O costs and AdapMoE (adamoe, ) skips certain experts to decrease loading costs. However, these approaches have notable limitations. EdgeMoE‚Äôs static approach determines optimal bit widths based on specific dataset profiling, leading to inflexibility across diverse environments and potential accuracy impacts. This method becomes particularly complex when dealing with different models, especially as the number of experts increases. Conversely, AdapMoE‚Äôs aggressive expert-skipping strategy can cause substantial accuracy degradation, particularly with small top-kùëòkitalic_k values (e.g., k=2ùëò2k=2italic_k = 2 in Mixtral-8x7B). Limited benefits of expert prefetching. To reduce the waiting time for required experts, prefetching is a valuable technique that overlaps expert loading with GPU computation. However, since MoE models only need the top-kùëòkitalic_k experts for the next layer, accurately predicting these top-kùëòkitalic_k experts is crucial. MoE-Infinity (xue2024moe, ) addresses this by prioritizing expert activation ratios for prefetching. MoE-Offloading (eliseev2023fast, ) uses the gate inputs from the current layer as inputs for the next layer to predict the required experts. Pre-gated MoE (hwang2024pre, ) modifies the model structure by introducing a pre-gate function to determine the next layer‚Äôs required experts in the current layer. Although prefetching can overlap expert-loading with GPU computation, these prediction methods offer limited benefits because the expert-loading cost is typically much greater than the GPU computation cost in the inference process of MoE-based LLMs. Inefficient management of expert cache. Given the sparse activation and temporal locality characteristics of experts, designing an appropriate cache replacement policy to manage the expert cache can significantly improve the expert cache hit ratio, reducing the need to load experts from next-level memory and thereby speeding up inference. For instance, EdgeMoE (yi2023edgemoe, ) and MoE-Infinity (xue2024moe, ) utilize the least frequently used (LFU) policy, while MoE-Offloading (eliseev2023fast, ) adopts the least recently used (LRU) policy. Although these approaches outperform random replacement policies, they are not fully optimal, as they fail to account for the unique characteristics of different models, which requires more tailored strategies to manage the expert cache efficiently. To address the above challenges, we propose HOBBIT, a system designed to accelerate expert loading across three levels of MoE computation. It significantly accelerates MoE-based LLM inference on memory-limited devices compared to existing systems by utilizing mixed precision expert inference. Our key contributions are as follows: ‚Ä¢ We propose a token-level dynamic expert loading mechanism that reduces latency through low-precision replacement of less critical cache-miss experts, maintaining accuracy and flexibility. ‚Ä¢ We develop a layer-level adaptive expert prefetching technique with high prediction accuracy and minimal penalties, leveraging mixed-precision prefetching to optimize computation-communication overlap. ‚Ä¢ We introduce a sequence-level multidimensional expert caching policy that combines model-specific strategies with mixed-precision features to efficiently manage the expert cache and minimize miss penalties across different models. ‚Ä¢ We implement HOBBIT on top of Llama.cpp with 8,000 additional lines of C++/C code, and evaluate it on two popular MoE-based LLMs across two memory-limited platforms, demonstrating up to 9.93x speedup in decoding over state-of-the-art systems."
https://arxiv.org/html/2411.01240v1,Optimizing Federated Learning by Entropy-Based Client Selection,"Deep learning is an emerging field revolutionizing various industries, including natural language processing, computer vision, and many more. These domains typically require an extensive amount of data for optimal performance, potentially utilizing huge centralized data repositories. However, such centralization could raise privacy issues concerning the storage of sensitive data. To address this issue, federated learning was developed. It is a newly distributed learning technique that enables to collaboratively train a deep learning model on decentralized devices, referred to as clients, without compromising their data privacy. Traditional federated learning methods often suffer from severe performance degradation when the data distribution among clients differs significantly. This becomes especially problematic in the case of label distribution skew, where the distribution of labels varies across clients. To address this, a novel method called FedEntOpt is proposed. FedEntOpt is designed to mitigate performance issues caused by label distribution skew by maximizing the entropy of the global label distribution of the selected client subset in each federated learning round. This ensures that the aggregated model parameters from the clients were exhibited to data from all available labels, which improves the accuracy of the global model. Extensive experiments on several benchmark datasets show that the proposed method outperforms several state-of-the-art algorithms by up to 6% in classification accuracy, demonstrating robust and superior performance, particularly under low participation rates. In addition, it offers the flexibility to be combined with them, enhancing their performance by over 40%.","Nowadays, an extensive amount of data is continuously created from different sources, such as smartphones, desktop computers and other Internet of Things (IoT) devices. Approximately 328.77 million terabytes of data are produced daily, and the volume is expected to triple in the upcoming year compared to five years ago [1, 2]. Training deep learning models on such data would be highly beneficial since they typically require a large amount of data to achieve state-of-the-art performance [3]. Nevertheless, storing the data in a centralized location for training such models is not possible due to data privacy violations. Further restrictions, such as the General Data Protection Regulation (GDPR) [4] additionally impede this process. Consequently, more and more training data is stored in a fragmented manner across different databases and cannot be exchanged over country [5] or even organizational borders, degrading the performance of a deep learning model due to lack of data [6]. To address this issue, a novel decentralized machine learning technique, called federated learning (FL) was proposed. This method is able to leverage all the available data from different devices, referred to as clients, to collaboratively train a global deep learning model without ever disclosing the raw data [7, 8, 9]. A typical realization of an FL system uses a server-client architecture, where a central server manages the global deep learning model, which is shared among all clients in order to train it on their respective local datasets. After training, the updated parameters are sent to the central server, which aggregates them to update the global model [9]. \Acfl systems have been successfully applied in various real-world scenarios, including fraud detection in finance [10], next-word prediction in natural language processing [11], developing autonomous driving strategies for self-driving cars [12], and building movie recommendation systems [13]. However, the training of such systems remains challenging due to varying data distributions among clients. The largest impact on the degradation of the global model performance in this setting is caused by label distribution skew, where clients have different label distributions [14]. It has been shown that simply averaging updated model parameters under such conditions can significantly decrease the performance of the global model, suffering from up to 26% loss in classification accuracy compared to the case where the label distribution among clients is homogeneous [15]. Furthermore, label skew is often encountered in practice since individual clients acquire their data independently, resulting in an imbalanced distribution of labels. For instance, hospitals in urban areas may have more images of respiratory diseases because their patients are exposed to higher levels of air pollution compared to those patients in rural areas. Therefore, our research will focus on addressing label distribution skew to mitigate its negative effects on the performance of the global model while simultaneously improving it. Several methods were proposed in the literature to reduce the negative effect of label skew. One approach focuses on constraining local updates to keep local models close to the global model. This is achieved by modifying the loss functions of the clients [16, 15, 17], incorporating contrastive learning [18, 19], or introducing control variates [20]. Other works employ weighting schemes, which take the varying importance of locally acquired model updates for aggregation into account [21, 22, 23]. Similarly, clustering techniques are applied to group clients with similar data distributions to enhance the global model performance [24]. Additionally, client selection methods try to choose an optimal subset of clients based on a specified metric [25, 26, 27]. There are also entropy-based approaches proposed in the literature aiming to improve local training, optimize parameter exchange, and refine the selection process [28, 29, 30]. However, all of these methods cannot fully utilize the information contained in the data distribution of the clients, yielding sub-optimal performance of the global model. The issue is that the selected subset of clients might have little or no data for certain labels. Since the aggregation of locally updated model parameters effectively encodes information about the data of the selected clients into the parameters of the global model, the absence of samples for certain labels yields updated model parameters lacking that information, which drives the global model towards higher loss regions on its loss surface [31]. To improve on this, we propose an entropy-based client selection strategy, which optimizes the selection of a subset of clients that is representative of the overall label distribution across all clients, mitigating the negative influence of label skew in the aggregation stage. Therefore, our research makes the following contributions: ‚Ä¢ We propose a new client selection method, named FederatedEntropyOptimization (FedEntOpt), which relies on label information and the concept of entropy to address the problem of label skew among clients. FedEntOpt iteratively selects a subset of clients, maximizing the entropy of the aggregated label distribution. This ensures that the combined distribution of labels within the selected cohort of clients approximates the overall label distribution. ‚Ä¢ We evaluate FedEntOpt on several datasets against other state-of-art algorithm showing superior performance in terms of classification accuracy. ‚Ä¢ We empirically demonstrate that our method is robust under different client participation rates, especially in low-participation scenarios. It achieves similar performance with fewer clients, thereby effectively reducing communication overhead compared to existing algorithms. ‚Ä¢ We show that combining FedEntOpt with other algorithms improves their performance in terms of classification accuracy. ‚Ä¢ We demonstrate the robustness of FedEntOpt when incorporating differential privacy, showing that its performance is hardly affected."
https://arxiv.org/html/2411.01040v1,Identify Backdoored Model in Federated Learning via Individual Unlearning,"Backdoor attacks present a significant threat to the robustness of Federated Learning (FL) due to their stealth and effectiveness. They maintain both the main task of the FL system and the backdoor task simultaneously, causing malicious models to appear statistically similar to benign ones, which enables them to evade detection by existing defense methods. We find that malicious parameters in backdoored models are inactive on the main task, resulting in a significantly large empirical loss during the machine unlearning process on clean inputs. Inspired by this, we propose MASA, a method that utilizes individual unlearning on local models to identify malicious models in FL. To improve the performance of MASA in challenging non-independent and identically distributed (non-IID) settings, we design pre-unlearning model fusion that integrates local models with knowledge learned from other datasets to mitigate the divergence in their unlearning behaviors caused by the non-IID data distributions of clients. Additionally, we propose a new anomaly detection metric with minimal hyperparameters to filter out malicious models efficiently. Extensive experiments on IID and non-IID datasets across six different attacks validate the effectiveness of MASA. To the best of our knowledge, this is the first work to leverage machine unlearning to identify malicious models in FL. Code is available at https://github.com/JiiahaoXU/MASA.","Federated Learning (FL) [31] is an emerging paradigm for training machine learning models across multiple distributed clients while preserving their data privacy. In FL, a central server coordinates a network of clients, each owning a local dataset. During the training process, the server distributes a shared global model to each client. The clients then train this model on their local datasets and send the resulting model updates back to the server. The server aggregates these updates to refine the global model for the next round of training. FL significantly reduces privacy risks by keeping the data on the client side throughout the process. FL has been successfully applied in various fields such as financial analysis [29, 7] and remote sensing [20, 28]. However, while the distributed nature of FL enhances data security, it also introduces vulnerabilities to poisoning attacks [14, 26, 43]. For example, Byzantine attacks [4, 41] aim to disrupt the global model‚Äôs convergence. Specifically, malicious clients intentionally alter their local model updates to differ significantly from those of benign clients, thereby distorting the convergence process. Yet, this substantial deviation between malicious and benign updates offers an opportunity for server-side detection. Recently, backdoor attacks [3, 47, 13, 54, 44, 4], have gained significant attention due to their stealth and practical effectiveness. Specifically, backdoor attacks aim to preserve the global model‚Äôs performance on clean inputs while causing it to make incorrect predictions on inputs containing a specific pre-defined feature (i.e., trigger). Since backdoor attacks have minimal impact on the main task‚Äôs accuracy, the malicious local updates closely resemble benign ones [44, 35], making anomaly detection much more challenging. One of the most common ways to defend against backdoor attacks in FL is to employ a robust aggregation rule (AGR) on the server side to handle the received local model updates [53]. Existing state-of-the-art (SOTA) AGRs can generally be classified into non-filtering-based AGRs [36, 19, 37, 11] and filtering-based AGRs [5, 14, 21, 18, 8, 38]. Non-filtering-based methods aim to mitigate the harmful effects of malicious parameters in the global model. However, they often fail to fully eliminate malicious impacts during aggregation and may also degrade main task performance. In contrast, filtering-based methods focus on identifying and excluding malicious local model updates to achieve maximum robustness. They typically rely on examining statistical differences (e.g., L1subscriptùêø1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-norm [18, 21], L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-norm [18, 5, 14, 21], and Cosine Similarity [8, 38]) between malicious and benign updates. However, due to the dual optimization objectives of malicious clients, these statistical differences are often minimal, a phenomenon known as the poison-coupling effect [19]. Furthermore, as the global model approaches convergence, the statistical differences between updates shrink further, reducing the effectiveness of filtering-based AGRs in detecting malicious updates. Through a detailed observation of the poison-coupling effect in malicious local models, we find that backdoor parameters contribute negligibly when fed with clean inputs. This observation suggests that benign and malicious models can exhibit different behaviors during machine unlearning [6], a process aimed at removing learned information. Specifically, when unlearning the information associated with clean data, benign and malicious models show distinct behaviors in terms of convergence speed and unlearning loss, which can serve as effective metrics for anomaly detection. Motivated by this, we propose a novel AGR called MASA, which leverages Machine unleArning on local modelS individuAlly with pre-unlearning model fusion to identify malicious models. In MASA, the server first reconstructs the local models using the local model updates it received. Next, the server performs machine unlearning on each reconstructed local model and tracks its training losses during the unlearning to capture its unlearning behavior. Given that local models can exhibit high divergence in non-IID settings, which poses significant challenges for detecting backdoored models, MASA integrates a pre-unlearning model fusion process. This allows each local model to incorporate parameters learned from other local datasets before unlearning, reducing inconsistencies in unlearning behavior caused by non-IID data and effectively exposing backdoored models during the unlearning process. Finally, MASA filters out model updates with unusually large unlearning losses using a novel hyperparameter-efficient anomaly detection metric. In summary, our main contribution is of four folds: ‚Ä¢ We find that to preserve the performance of the main task, malicious parameters in backdoored models are less active than benign parameters when evaluated on clean inputs. Consequently, these less active parameters lead to significantly different unlearning behavior compared to benign models. This finding offers a new perspective for designing backdoor detection methods in FL. ‚Ä¢ We design a new AGR called MASA, which leverages the distinct machine unlearning dynamics between backdoored and benign local models to identify backdoored models in FL. To the best of our knowledge, this is the first work to leverage machine unlearning for identifying backdoored models in FL. ‚Ä¢ MASA incorporates a pre-unlearning model fusion process, which significantly reduces the divergence in local models‚Äô unlearning behavior caused by non-IID data distributions among clients, helping to expose backdoored models in non-IID settings. Moreover, MASA is equipped with a hyperparameter-efficient anomaly detection metric to identify those local models with unusual unlearning loss. ‚Ä¢ We conduct extensive empirical evaluations of MASA, testing its performance on IID, extreme non-IID, and extremely high attack ratio scenarios under various SOTA backdoor attacks. Results demonstrate that MASA consistently achieves superior backdoor robustness compared to SOTA defense methods."
https://arxiv.org/html/2411.00859v1,Profiling AI Models: Towards Efficient Computation Offloading in Heterogeneous Edge AI Systems,"The rapid growth of end-user AI applications, such as computer vision and generative AI, has led to immense data and processing demands often exceeding user devices‚Äô capabilities. Edge AI addresses this by offloading computation to the network edge, crucial for future services in 6G networks. However, it faces challenges such as limited resources during simultaneous offloads and the unrealistic assumption of homogeneous system architecture. To address these, we propose a research roadmap focused on profiling AI models, capturing data about model types, hyperparameters, and underlying hardware to predict resource utilisation and task completion time. Initial experiments with over 3,000 runs show promise in optimising resource allocation and enhancing Edge AI performance.","The rapid growth of end-user AI applications, such as real-time image recognition and generative AI, has led to high data and processing demands that often exceed device capabilities. Edge AI addresses these challenges by offloading computation to the network‚Äôs edge, where hardware-accelerated AI processing can occur [1]. This approach is integral to AI and RAN, a key component of future 6G networks as outlined by the AI-RAN Alliance111https://ai-ran.org/working-groups/. In 6G, AI integration across edge-RAN and extreme-edge devices will support efficient data distribution and distributed AI techniques, enhancing privacy and reducing latency for applications like the Metaverse and remote surgery. Despite these benefits, Edge AI faces challenges. Limited resource availability at the edge can hinder performance during simultaneous offloads. Additionally, the assumption of homogeneous system architecture in the existing literature is unrealistic, as edge devices vary widely in processor speeds and architectures (e.g., 1.5GHz vs 3.5GHz, or X86 vs ARM), impacting task processing and resource utilisation. To address these challenges, we propose a research roadmap focused on profiling AI models by analysing their execution dynamics across various bare-metal systems. Our goal is to understand how AI model types (e.g., MLP, CNN), hyperparameters (e.g., learning rate, optimiser), hardware (e.g., architecture, FLOPS), and dataset characteristics (e.g., size, batch size) affect model accuracy, resource use, and task completion time. This Profiling AI Models process allows us to predict resource needs and task completion times, enabling efficient scheduling across edge nodes. Our initial experiments, involving over 3,000 runs with varied configurations, showcase the effectiveness of our approach. Using AI techniques like XGBoost, we achieved a normalised RMSE of 0.001, a significant improvement over MLPs with over 4 million parameters. Figure 1: Research roadmap for profiling based computation offloading"
https://arxiv.org/html/2411.00761v1,LCP: Enhancing Scientific Data Management withLossyCompression forParticles,"Many scientific applications opt for particles instead of meshes as their basic primitives to model complex systems composed of billions of discrete entities. Such applications span a diverse array of scientific domains, including molecular dynamics, cosmology, computational fluid dynamics, and geology. The scale of the particles in those scientific applications increases substantially thanks to the ever-increasing computational power in high-performance computing (HPC) platforms. However, the actual gains from such increases are often undercut by obstacles in data management systems related to data storage, transfer, and processing. Lossy compression has been widely recognized as a promising solution to enhance scientific data management systems regarding such challenges, although most existing compression solutions are tailored for Cartesian grids and thus have sub-optimal results on discrete particle data. In this paper, we introduce LCP, an innovative lossy compressor designed for particle datasets, offering superior compression quality and higher speed than existing compression solutions. Specifically, our contribution is threefold. (1) We propose LCP-S, an error-bound aware block-wise spatial compressor to efficiently reduce particle data size while satisfying the pre-defined error criteria. This approach is universally applicable to particle data across various domains, eliminating the need for reliance on specific application domain characteristics. (2) We develop LCP, a hybrid compression solution for multi-frame particle data, featuring dynamic method selection and parameter optimization. It aims to maximize compression effectiveness while preserving data quality as much as possible by utilizing both spatial and temporal domains. (3) We evaluate our solution alongside eight state-of-the-art alternatives on eight real-world particle datasets from seven distinct domains. The results demonstrate that our solution achieves up to 104% improvement in compression ratios and up to 593% increase in speed compared to the second-best option, under the same error criteria.","Scientific data management systems (HDF5, 2024; Gray et al., 2005; Cheng and Rusu, 2014; Kersten et al., 2011) are facing ever-increasing challenges from the rapid evolution of computing power versus the comparatively slow expansion of data infrastructure in HPC facilities. The fast-growing computing power enables scientific applications to run on a larger scale with higher precision, which as a consequence produces more data beyond the memory, storage, and I/O capacities of the data systems on supercomputers. For example, the EXAALT project which focuses on molecular dynamics (MD), generates trajectories containing over a trillion time steps using exascale machines by leveraging parallel-in-time approaches (EXAALT project, 2021). Storing all these frames in a scientific data management system (e.g., HDF5 (HDF5, 2024)) would require hundreds of terabytes of disk space, transferring them between facilities may take hours or days, and post-analysis of all frames on a single node is impractical due to insufficient memory size. Error-bounded lossy compression has been widely considered a promising solution for scientific applications facing data challenges (Zhao et al., 2020, 2021; Lindstrom, 2014; Liang et al., 2022a; Jiao et al., 2023; Tian et al., 2021). First, lossy compressors can reduce the data volume significantly (by a factor of 5‚àº1000similar-to510005\sim 10005 ‚àº 1000 in most cases). By comparison, lossless compressors, including Zstd (Zstandard, 2021), Gorilla (Pelkonen et al., 2015), and Brotli (Alakuijala et al., 2018), can only reduce the data by a factor of 2 in most cases (Zhao et al., 2021). Second, error-bounded lossy compression can limit the compression error, ensuring that the quality of the decompressed data remains acceptable for post-analysis. (a) Structured mesh (b) Particle data Figure 1. Visualization of structured mesh versus particles Scientific data can generally be classified into particle-style (e.g., locations, connectivity) and mesh-style (e.g. regular multidimensional grid in space). Existing lossy compressors, including those designed for databases (e.g., ModelarDB (Jensen et al., 2018, 2021), SummaryStore (Agrawal and Vulimiri, 2017), SciDB (Cudre-Mauroux et al., 2009)) and those specifically designed for scientific data (e.g., SZ3 (Zhao et al., 2021), ZFP (Lindstrom, 2014), MGARD (Liang et al., 2022a)), are primarily tailored for structured mesh and suffer from low effectiveness on particle data (Zhao et al., 2022). However, various research fields such as material science, biology, cosmology, and computational fluid dynamics extensively utilize particle data. Scientific lossy compression techniques specific to the management of particle data remain under-explored. In this paper, our objective is to develop an efficient scientific lossy compressor for the management of particle data. This task presents several challenges: (1) Most spatial compression techniques suitable for meshes are not applicable to particle data. These techniques often depend on the correlation of adjacent data inherent in structured meshes representing physical fields. In contrast, particle-style data lacks this correlation as it represents particles that are arbitrarily positioned in space (as depicted in Figure 1), leaving little structure to exploit for high compression ratios. (2) Applying temporal compression to multi-frame particle data is often impractical. One reason is that frames may be saved at irregular or even random intervals, such that the temporal domain may not be correlated enough to improve compression. Second, temporal compression often requires loading consecutive frames into memory to identify global patterns or characteristics essential for compression. However, particle data frames can be exceptionally large allowing only a subset of frames to be loaded into memory. Without large numbers of consecutive frames, the effectiveness of temporal compression diminished. (3) Accessing frames by batch is required by applications (discussed in Section 2.1.3), which further restricts the approach. To support the retrieval of selected frames, compression is typically performed in small independent batches (each with a few frames, with no dependencies between batches). Since compression needs to be done multiple times, methods that have large amounts of metadata to store per compression introduce substantial overhead. Moreover, techniques requiring inter-batch dependencies should be excluded, otherwise decompressing a single frame will necessitate decompressing all of its preceding frames first, resulting in significant overhead on partial data retrieval. Taking into account all the aforementioned challenges, we introduce LCP, a novel scientific Lossy Compressor for the management of Particle data. The main contributions are outlined as follows: ‚Ä¢ We propose the spatial compressor LCP-S and the temporal compressor LCP-T for particles. LCP-S is equipped with error-bound-aware quantization and spatial-block-wise coding algorithms to reach high compression effectiveness and data fidelity while guaranteeing the arbitrary error-bound defined by users before compression. Moreover, LCP-S is universally applicable to any particle data, in contrast to existing methods that are constrained to domain-specific data (Zhao et al., 2022; Lundborg et al., 2014). ‚Ä¢ We propose LCP, our dynamic hybrid compression solution for multi-frame particle data. LCP is built on a hybrid design incorporating LCP-S and LCP-T, together with dynamic method selection and parameter optimization strategies, to maximize compression effectiveness by exploiting data characteristics in both spatial and temporal domains. Additionally, LCP is enhanced with the spatial-anchor-frame based batch compression technique to support the fast partial retrieval needs of applications. ‚Ä¢ We evaluate our solution LCP on eight particle datasets from seven distinct domains with eight state-of-the-art related works. Experiments demonstrate that LCP is the best lossy compressor for the management of particle data, achieving the highest compression ratios, speed, and fidelity among all the compressors. The remainder of the paper is structured as follows. In Section 2 delves into the research background. In Section 3, we provide an overview of related work. Section 4 formulates the research problem. Our developed particle compression framework is detailed in Section 5 to Section 7. Section 8 presents and discusses the evaluation results. Finally, we draw conclusions in Section 9."
https://arxiv.org/html/2411.00643v1,Transforming Agriculture: Exploring Diverse Practices and Technological Innovations,"Agriculture is a vital sector that significantly contributes to the economy and food security, particularly in regions like Varanasi, India. This paper explores various types of agriculture practiced in the area, including subsistence, commercial, intensive, extensive, industrial, organic, agroforestry, aquaculture, and urban agriculture. Each type presents unique challenges and opportunities, necessitating innovative approaches to enhance productivity and sustainability. To address these challenges, the integration of advanced technologies such as sensors and communication protocols is essential. Sensors can provide real-time data on soil health, moisture levels, and crop conditions, enabling farmers to make informed decisions. Communication technologies facilitate the seamless transfer of this data, allowing for timely interventions and optimized resource management. Moreover, programming techniques play a crucial role in developing applications that process and analyze agricultural data. By leveraging machine learning algorithms, farmers can gain insights into crop performance, predict yields, and implement precision agriculture practices. This paper highlights the significance of combining traditional agricultural practices with modern technologies to create a resilient agricultural ecosystem. The findings underscore the potential of integrating sensors, communication technologies, and programming in transforming agricultural practices in Varanasi. By fostering a data-driven approach, this research aims to contribute to sustainable farming, enhance food security, and improve the livelihoods of farmers in the region.","Agriculture plays a pivotal role in the socio-economic fabric of countries, particularly in developing regions like India. With over 58% of the rural population engaged in agricultural activities, it is essential to explore various farming practices that can enhance productivity, sustainability, and food security. This paper examines the diverse types of agriculture prevalent in India, India, emphasizing the need for innovative solutions to address the challenges faced by farmers in the region [1, 2, 3]. The agricultural landscape in India is characterized by a blend of traditional and modern practices. Subsistence agriculture remains prominent, where farmers primarily cultivate crops for family consumption, often utilizing age-old techniques that are increasingly challenged by climate variability and resource constraints. In contrast, commercial agriculture is gaining traction, driven by market demands and technological advancements. However, the effective integration of high-yield methods, mechanization, and data-driven practices is crucial for maximizing outputs and ensuring economic viability. As the global agricultural sector evolves, the adoption of technology becomes imperative. Advanced communication protocols, sensors, and artificial intelligence (AI) have the potential to revolutionize agricultural practices [4, 5, 6]. For instance, precision agriculture leverages data analytics to optimize resource use, reduce waste, and improve crop yields. In India, the integration of IoT technologies and machine learning can facilitate real-time monitoring of soil health, weather conditions, and crop performance, enabling farmers to make informed decisions. Furthermore, sustainable practices such as organic farming, agroforestry, and permaculture offer alternatives that prioritize ecological balance and biodiversity. These approaches not only contribute to environmental health but also enhance the resilience of farming systems against climate change impacts. By embracing a multi-faceted agricultural strategy that combines traditional knowledge with modern innovations, India can pave the way for a more sustainable and prosperous agricultural future. This paper aims to explore the various types of agriculture in India, highlighting their unique characteristics, challenges, and the trans-formative role of technology. By understanding these dynamics, we can better inform agricultural policies and practices that support the livelihoods of farmers while promoting sustainability and food security in the region."
https://arxiv.org/html/2411.00558v1,"3-Slot-Finality Protocol for Ethereum111This work combines multiple preliminary publications which appeared at ESORICS-CBT¬†2023[9],
PODC¬†2024[6],
CSF¬†2024[10], andunder submission[8].","Gasper, the consensus protocol currently employed by Ethereum, typically requires 64 to 95 slots ‚Äì the units of time during which a new chain extending the previous one by one block is proposed and voted ‚Äì to finalize. This means that under ideal conditions ‚Äì where the network is synchronous, and all chain proposers, along with more than two-thirds of the validators, behave as dictated by the protocol ‚Äì proposers construct blocks on a non-finalized chain that extends at least 64 blocks. This exposes a significant portion of the blockchain to potential reorganizations during changes in network conditions, such as periods of asynchrony. Specifically, this finalization delay heightens the network‚Äôs exposure to Maximum Extractable Value (MEV) exploits, which could undermine the network‚Äôs integrity. Furthermore, the extended finalization period forces users to balance the trade-off between economic security and transaction speed.To address these issues and speed up finality, we introduce a partially synchronous finality gadget, which we combine with two dynamically available consensus protocols ‚Äì synchronous protocols that ensure safety and liveness even with fluctuating validator participation levels. This integration results in secure ebb-and-flow protocols [SP 2021], achieving finality within three slots after a proposal and realizing 3-slot finality.","Traditional Byzantine consensus protocols, such as PBFT [4] or HotStuff, are designed for distributed systems where participants are fixed, known in advance, and cannot go offline without being considered faulty. Recently, dynamic participation has become a critical requirement for developing permissionless consensus protocols. This concept, initially formalized by Pass and Shi through their sleepy model [19], encapsulates the ability of a system to handle honest participants who may go offline and come back online. A consensus protocol that maintains safety and liveness while accommodating dynamic participation is called dynamically-available. One problem of such dynamically-available protocols is that they do not tolerate network partitions [12]; no consensus protocols can satisfy both liveness (under dynamic participation) and safety (under network partitions). Simply put, a consensus protocol cannot produce a single chain222In this context, we are extending the traditional notion of consensus, typically understood as a one-shot primitive. Technically, this should be referred to as total-order broadcast or atomic broadcast. However, for the sake of a general audience, we have adopted the term ‚Äúconsensus.‚Äù Consequently, we consider the output of these protocols to be a sequence of transactions, batched in blocks, forming a chain. This will be formalized in Section 2. that concurrently offers Dynamic Availability and guarantees transaction finality in case of asynchronous periods or network partitions. Because of that, dynamically-available protocols studied so far are devised in synchronous settings [7, 13, 16, 10, 8]. Working around this impossibility result, Neu, Tas, and Tse [17] introduced a family of protocols referred to as ebb-and-flow protocols. An ebb-and-flow protocol comprises two sub-protocols, each with its own confirmation rule, and each outputting a chain, with one serving as a prefix of the other. The first confirmation rule defines what is known as the available chain, which provides liveness under dynamic participation (and synchrony). The second confirmation rule defines the finalized chain, and provides safety even under network partitions, but loses liveness either under asynchrony or in case of fluctuation in the participation level. Interestingly, such family of protocols also captures the nature of the Ethereum consensus protocol, Gasper [3], in which the available chain is output by (the confirmation rule of) LMD-GHOST [22] protocol, and the finalized chain by the (confirmation rule of the) finality gadget Casper FFG [2]. However, the original version of LMD-GHOST is not secure even in a context of full participation and synchrony. Potential attack vectors have been identified [17, 20] that undermine the protocol‚Äôs safety and liveness. Motivated by finding a more secure alternative to LMD-GHOST, and following the ebb-and-flow approach, D‚ÄôAmato et al. [7] devise a synchronous dynamically-available consensus protocol, Goldfish, that, combined with a generic (partially synchronous) finality gadget, implements a secure ebb-and-flow protocol. Moreover, Goldfish is Reorg Resilient: chains proposed by honest validators are guaranteed to not be reorganized. However, Goldfish is brittle to temporary asynchrony [10], in the sense that even a single violation of the bound of network delay can lead to a catastrophic failure, jeopardizing the safety of any previously confirmed chain, resulting in a protocol that is not practically viable to replace LMD-GHOST in Ethereum. In other words, Goldfish is not Asynchrony Resilient. To cope with the limitation of Goldfish with asynchrony, D‚ÄôAmato and Zanolini [10] propose RLMD-GHOST, a provably secure synchronous consensus protocol that does not lose safety during bounded periods of asynchrony and which tolerates a weaker form of dynamic participation, offering a trade-off between Dynamic Availability and Asynchrony Resilience. Their protocol results appealing for practical systems, where strict synchrony assumptions might not always hold, contrary to what is generally assumed with standard synchronous dynamically-available protocols. The family of protocols to which Goldfish and RLMD-GHOST belong are consensus protocols that are probabilistically safe, guaranteeing safety with overwhelming probability. In contrast, Momose and Ren‚Äôs research [16] laid the foundation for deterministically safe, dynamically-available consensus protocols, sparking a wave of subsequent research [13, 14, 11, 6]. Unlike Goldfish and RLMD-GHOST these protocols achieve deterministic safety by employing the notion of quorums. Traditional quorums, defined by a fixed number of actively engaging participants, are not suitable in a dynamic participation context. By leveraging Graded Agreement333In Graded Agreement, each decision is assigned a grade, which intuitively reflects the strength of the agreement. Notably, different formulations of graded agreement exist, each with slighly different properties. In this work, we focus on the properties defined by D‚ÄôAmato et al. [8]., Momose and Ren [16] redefined quorums dynamically, according to current participation levels, while maintaining critical properties of traditional quorums. This development has led to the creation of various consensus protocols based on Graded Agreement, each with unique properties and varying levels of adversarial tolerance. The initial protocol by Momose and Ren [16] accommodates up to 1/2 adversarial participants but is limited by a high latency of 16‚Å¢Œî16Œî16\Delta16 roman_Œî, with ŒîŒî\Deltaroman_Œî being the message delay bound. The subsequent study by Malkhi, Momose, and Ren [13] introduced two protocols that reduce the latency to 3‚Å¢Œî3Œî3\Delta3 roman_Œî and 2‚Å¢Œî2Œî2\Delta2 roman_Œî, respectively, but at the expense of lower adversarial tolerance to 1/3 and 1/4. Later enhancements [14] managed to revert to tolerating minority corruption while maintaining a comparable latency of 4‚Å¢Œî4Œî4\Delta4 roman_Œî. Another concurrent and independent work [11] achieves 1/2 adversarial resilience, with a latency of 6‚Å¢Œî6Œî6\Delta6 roman_Œî. D‚ÄôAmato et al. [8], aiming to enhance the practicality of deterministically safe, dynamically-available consensus protocols, particularly in systems with many participants, introduced a consensus protocol, named TOB-SVD, that tolerates up to 1/2 adversarial participants and achieves latency comparable to [14] ‚Äì slightly better in expectation and slightly worse in the best case. Crucially, it requires only a single vote round per decision in the best case, in contrast to the nine rounds required by [14]. Moreover, D‚ÄôAmato, Losa, and Zanolini [6], explored mechanisms to make dynamically-available consensus protocols based on Graded Agreement resilient to bounded periods of asynchrony. Previous work by D‚ÄôAmato and Zanolini [9] proposed an ebb-and-flow protocol by combining the dynamically-available protocol RLMD-GHOST with a finality gadget similar to Casper FFG, hereafter referred to as the SSF protocol. In their work, as in ours, time is divided into slots and at the beginning of each slot a new chain is proposed by an elected proposer. Importantly, the SSF protocol ensures that, under synchrony and at least 2/3 of the participants being honest and online, any chain proposed by an honest participant is finalized within the same slot, i.e., before the next proposer‚Äôs turn. This single-slot finality is achieved through three vote rounds within a slot. Specifically, the SSF protocol operates in slots of duration 4‚Å¢Œî4Œî4\Delta4 roman_Œî rounds each. During the first round, a proposal is made. In the second round, validators cast votes for what they perceive as the tip of the chain, ideally the proposal just made. In the third round, if a quorum of votes for the same head block is observed, validators cast a finality vote for that block. In the final round, if a validator sees a quorum of finality votes for a block, it broadcasts an acknowledgment message for external observers. If an observer sees a quorum of acknowledgments for a block by the end of the slot, it can declare the block finalized. While theoretically sound, the efficiency and usability of this protocol for large-scale blockchain networks are questionable due to the number of vote phases required for each slot. In fact, large-scale blockchain networks such as Ethereum, due to the large number of participants, to reduce the bandwidth requirements, employ an aggregation process by which votes are first sent to aggregators who then distribute the aggregated signatures. As a consequence of this, each vote phase requires double the normal network latency, increasing the slot time and decreasing the transaction throughput. This means that in practice, in the SSF protocol a slot is 6‚Å¢Œî6Œî6\Delta6 roman_Œî as the second and third vote phase need to wait for the first and second ones to complete, respectively, but the third vote phase can proceed in parallel with the next slot. Moreover, it is unclear whether the messages cast during the third phase should be included on-chain or kept off-chain and potentially delaying the slot‚Äôs finalization in practice. In this work, we propose a finality gadget that can be composed with dynamically-available protocols to obtain an ebb-and-flow protocol with only one vote phase per slot and slot length of 5‚Å¢Œî5Œî5\Delta5 roman_Œî if we consider a vote phase taking 2‚Å¢Œî2Œî2\Delta2 roman_Œî. All things equals, this represents a 20% improvement in practical network throughout compared to the protocol by D‚ÄôAmato and Zanolini [9]444This assuming the same block size for both the SSF protocol and our proposed protocol. In practice, throughput can be independent of slot duration, as desired per-second throughput can be set independently, with higher per-slot throughput achievable by using longer slots.. The trade-off that we make is in delaying the finalization of a chain proposed by an honest validator to occur two slots later, i.e., in our protocol a chain proposed by an honest validator in slot tùë°titalic_t is finalized555Here, we consider the finalization time of a chain ùñºùóÅùñºùóÅ\mathsf{ch}sansserif_ch to be the time after which no chain conflicting with ùñºùóÅùñºùóÅ\mathsf{ch}sansserif_ch can ever by finalized, which can occur earlier than when some honest node finalizes chain ùñºùóÅùñºùóÅ\mathsf{ch}sansserif_ch in their view. by the end of slot t+2ùë°2t+2italic_t + 2, as long as synchrony holds, and at least 2/3 of the participants are honest and active till the vote round of slot t+2ùë°2t+2italic_t + 2. Importantly, this is ensured regardless of whether the proposers of slots t+1ùë°1t+1italic_t + 1 and t+2ùë°2t+2italic_t + 2 are honest. In practice, this means that, assuming that each vote phase takes 2‚Å¢Œî2Œî2\Delta2 roman_Œî, we require the synchronous and participation assumptions to hold for 11‚Å¢Œî11Œî11\Delta11 roman_Œî (in our protocol the vote round occurs ŒîŒî\Deltaroman_Œî time after the beginning of a slot), rather than 5‚Å¢Œî5Œî5\Delta5 roman_Œî as in the SSF protocol. However, like SSF, we offer a method by which, under these assumptions and considering that vote phases take 2‚Å¢Œî2Œî2\Delta2 roman_Œî, a chain proposed by an honest proposer is confirmed by the dynamically-available protocol of any honest validator at time 3‚Å¢Œî3Œî3\Delta3 roman_Œî of the same slot, which ensures that such chain will then be finalized in slot t+2ùë°2t+2italic_t + 2. This offers an avenue for users to know in advance that, as long as the assumptions on synchrony and participation holds, such chain will be finalized. Also, we can integrate the third round of voting from the SSF protocol into ours to obtain a protocol that has two vote phases per slot, but still retain slot length of 5‚Å¢Œî5Œî5\Delta5 roman_Œî ‚Äì when assuming that each vote phase takes 2‚Å¢Œî2Œî2\Delta2 roman_Œî because, as explained above, the third vote phase from the SSF protocol can proceed in parallel with the rest of the protocol and therefore does not increase the practical slot length ‚Äì but that reduces finalization from 11‚Å¢Œî11Œî11\Delta11 roman_Œî down to 8‚Å¢Œî8Œî8\Delta8 roman_Œî. In practice, for networks where the periods of synchrony and at least 2/3 of the participants being honest and online typically last much longer than 11‚Å¢Œî11Œî11\Delta11 roman_Œî, our protocol presents no real drawback as confirmation still happens within the same slot which then leads to finalization, while attaining a higher transaction throughput and, arguably, a simpler protocol. Moreover, users are concerned with the time that a transaction takes to be either confirmed or finalized, which is the time that it takes for this transaction to be included in a proposed chain and for this chain to be either confirmed or finalized. This is not the same as the time taken to confirm or finalize a block proposed by an honest validator because transactions are not necessarily submitted just before an honest proposer proposes a chain. They are submitted whenever the user needs to interact with the blockchain. So, they could be submitted at any point of a slot. Also, they could be submitted when the proposer of the next slot is Byzantine who therefore might not include them in the chain that they proposer, if any. For this reason, from a user perspective, it makes sense to consider the expected confirmation and finalization times under the assumption that a transaction submission time is uniformly distributed. Then, the expected confirmation and finalization times correspond to the time taken to confirm or finalize a chain proposed by an honest proposer, depending on which of the two measures we interested in, plus (1+Œ≤)‚ãÖslot-time2‚Å¢(1‚àíŒ≤)‚ãÖ1ùõΩslot-time21ùõΩ\frac{(1+\beta)\cdot\text{slot-time}}{2(1-\beta)}divide start_ARG ( 1 + italic_Œ≤ ) ‚ãÖ slot-time end_ARG start_ARG 2 ( 1 - italic_Œ≤ ) end_ARG where Œ≤ùõΩ\betaitalic_Œ≤ represents the adversarial power in the network. Let us compare the expected confirmation time of SSF and 3SF first. For Œ≤=13ùõΩ13\beta=\frac{1}{3}italic_Œ≤ = divide start_ARG 1 end_ARG start_ARG 3 end_ARG666Liveness can only be guaranteed if Œ≤<13ùõΩ13\beta<\frac{1}{3}italic_Œ≤ < divide start_ARG 1 end_ARG start_ARG 3 end_ARG, the expected confirmation time for SSF is 9‚Å¢Œî9Œî9\Delta9 roman_Œî whereas for 3SF is 8‚Å¢Œî8Œî8\Delta8 roman_Œî meaning an ‚âà11absent11\approx 11‚âà 11% improvement. For Œ≤=0ùõΩ0\beta=0italic_Œ≤ = 0, the expected confirmation time for SSF is 6‚Å¢Œî6Œî6\Delta6 roman_Œî whereas for 3SF is 5.5‚Å¢Œî5.5Œî5.5\Delta5.5 roman_Œî meaning an ‚âà8absent8\approx 8‚âà 8% improvement. Moving to the expected finalization time, for Œ≤=13ùõΩ13\beta=\frac{1}{3}italic_Œ≤ = divide start_ARG 1 end_ARG start_ARG 3 end_ARG, for SSF it is 11‚Å¢Œî11Œî11\Delta11 roman_Œî, for 3SF it is 16‚Å¢Œî16Œî16\Delta16 roman_Œî and for the two-slot variant of 3SF it is 13‚Å¢Œî13Œî13\Delta13 roman_Œî meaning that the expected finalization time for 3SF ‚âà46absent46\approx 46‚âà 46% higher than the one of SSF, but this reduces to ‚âà18absent18\approx 18‚âà 18% for the its two-slot variant. For Œ≤=0ùõΩ0\beta=0italic_Œ≤ = 0, the expected finalization time for SSF is 8‚Å¢Œî8Œî8\Delta8 roman_Œî, for 3SF is 13.5‚Å¢Œî13.5Œî13.5\Delta13.5 roman_Œî and for the two-slot variant of 3SF is 10.5‚Å¢Œî10.5Œî10.5\Delta10.5 roman_Œî meaning that the expected finalization time for 3SF ‚âà69absent69\approx 69‚âà 69% higher than the one of SSF, but this reduces to ‚âà31absent31\approx 31‚âà 31% for the its two-slot variant. Finally, slot time has been shown[15] to be an important parameter in determining the economic leakage of on-chain automated market makers (AMMs) due to arbitrage. For instance, arbitrage profits (and equivalently liquidity providers (LP) losses) are proportional to the square root of slot time, so that a lower slot time is very desirable by financial applications built on top of a smart contract blockchain. Overall, our protocol achieves a balance by trading a higher expected finalization time for a shorter expected confirmation time, which could be sufficient for most users. At the same time, it offers shorter slot time and improved throughput as discussed above. Additionally, we show how to integrate our finality gadgetwith two dynamically-available protocols to obtain a secure ebb-and-flow. The first dynamically-available protocol that we consider is a probabilistically-safe and bounded-asynchrony-period-resilient variant of the deterministically-safe protocol TOB-SVD [8], the second is RLMD-GHOST [10]. The resulting protocols, in addition to the standard ebb-and-flow properties, ensure Safety and Reorg Resilience of the available chain even in the face of a subsequent bounded period of asynchrony. This makes our protocols particularly attractive for blockchain networks, such as Ethereum, where safety failures in the available chain can be exploited by dishonest participants to steal honest participant‚Äôs Maximum Extractable Value (MEV) [5] without being punished for it. Moreover, critical to the practical application in large-scale blockchain networks, both of our resulting protocols ensure that the dynamically-available component can heal from any arbitrarily long period of asynchrony as long as at least 2/3 of participants are honest and online for a sufficient amount of time. This is not a property mentioned in the original work that introduced ebb-and-flow protocol [18] where the dynamically-available protocol is required to ensure safety only if synchrony holds from the beginning. However, any real system is bound to experience some period of asynchrony of arbitrary length at some point. Ensuring that the dynamically-available protocol can recover is then important from a practical point of view. The remainder of this work is structured as follows. In Section 2, we present our system model along with all the necessary background notions and property definitions. Common notions for both the protocols we introduce in this work are detailed in Section 3. Section 4 introduces and proves the correctness of the finality gadget. Notably, the finality gadget, or FFG-component, is common to both presented protocols. Therefore, the properties and results discussed in Section 4 apply to both protocols. Our first faster finality protocol is presented in Section 5, where we provide the pseudo-code of the first protocol, the one based upon TOB-SVD [8], and prove its properties, demonstrating that it is a secure ebb-and-flow protocol (as defined in Section 2). The second protocol, the one based upon RLMD-GHOST [10], is introduced in Section 6, and we conduct a similar analysis to that in Section 5. In Section 7, we discuss how to resolve some of the main challenges presented in implementing either protocol, and examine their communication complexity. Then, in Section 8, we detail the conditions required for Dynamic Availability and Reorg Resilience to hold even in partially synchronous settings and provide the intuition underpinning this leaving, the detailed proof to Appendix A. Also, in Appendix B we explore a modification to our protocols that allow finalizing chains within two slots only by adding one vote round but without this affecting the practical length of slots when aggregation is used to reduce bandwidth requirements. Finally, conclusions are drawn in Section 9."
https://arxiv.org/html/2411.00284v2,SimpleFSDP: Simpler Fully Sharded Data Parallel with torch.compile,"Distributed training of large models consumes enormous computation resources and requires substantial engineering efforts to compose various training techniques. This paper presents SimpleFSDP, a PyTorch-native compiler-based Fully Sharded Data Parallel (FSDP) framework, which has a simple implementation for maintenance and composability, allows full computation-communication graph tracing, and brings performance enhancement via compiler backend optimizations.SimpleFSDP‚Äôs novelty lies in its unique torch.compile-friendly implementation of collective communications using existing PyTorch primitives, namely parametrizations, selective activation checkpointing, and DTensor. It also features the first-of-its-kind intermediate representation (IR) nodes bucketing and reordering in the TorchInductor backend for effective computation-communication overlapping. As a result, users can employ the aforementioned optimizations to automatically or manually wrap model components for minimal communication exposure. Extensive evaluations of SimpleFSDP on Llama 3 models (including the ultra-large 405B) using TorchTitan demonstrate up to 28.54% memory reduction and 68.67% throughput improvement compared to the most widely adopted FSDP2 eager framework, when composed with other distributed training techniques.","Distributed training the ever-growing large models necessitates huge computation resources Rae et al. (2021); Zhang et al. (2022); Chowdhery et al. (2023); Dubey et al. (2024) and engineering efforts Shoeybi et al. (2019); Rasley et al. (2020); Liang et al. (2024), both of which pose significant challenges as the model size scales. For example, training the Llama 3.1 405B Dubey et al. (2024) model takes 30.84 million H100 GPU hours, and PaLM-540B Chowdhery et al. (2023) model takes 9.4 million TPUv4 hours. During training, various parallelisms Huang et al. (2019); Shoeybi et al. (2019); Zhao et al. (2023), memory optimizations Chen et al. (2016); Korthikanti et al. (2023), and communication optimizations Micikevicius et al. (2017); Zhao et al. (2023); Choudhury et al. (2024) are employed to improve computation throughputs and minimize communication exposure. Fully Sharded Data Parallel (FSDP) Zhao et al. (2023), motivated by the DeepSpeed ZeRO Rajbhandari et al. (2020), is one of the most fundamental techniques for distributed large model training. It significantly saves memory by sharding model parameters, gradients, and optimizer states across multiple devices and only gathers them when needed. As such, it is widely adopted to train large generative models Le Scao et al. (2023); Dubey et al. (2024) and has been deployed in open-source libraries, like NeMo Kuchaiev et al. (2019), DeepSpeed Rasley et al. (2020), and TorchTitan Liang et al. (2024). FSDP is primarily developed in the PyTorch eager (i.e., non-compile) mode, where model operators are executed immediately after definition. It preserves debuggability and enables certain mechanisms like pre-fetching via backward hooks PyTorch Community (2023d), which are hard to trace in the compile mode Ansel et al. (2024). However, the eager mode impairs the training performance, as the model cannot be compiled as a whole graph, thereby losing opportunities for hardware-specific computation optimizations and efficient memory management. Prior works bringing machine learning compilers into distributed training mainly go from two directions: (1) JAX-based Bradbury et al. (2018); Xu et al. (2021) that uses XLA Sabne (2020) as the compile backend and shard tensors via user annotations; (2) PyTorch-based Liang et al. (2024), which leverages torch.compile Ansel et al. (2024) to trace per-device compute submodules and insert inter-module communications. JAX adopts functional programming and imposes certain constraints to ensure compatibility with the XLA backend. This greatly hinders the programmability in distributed training, which stacks many emerging techniques and requires agile development. PyTorch-based approach Liang et al. (2024), on the other hand, only compiles the model‚Äôs computation modules, as the FSDP eager-mode implementations like prefetching are hard to be traced by torch.compile. Hence, it loses the opportunity to compile a full model graph for communication/computation co-optimization and introduces additional codebase complexity by requiring the manual insertion of inter-module communications. This paper presents SimpleFSDP, a PyTorch-native compiler-based FSDP framework. It features (1) Simplicity: users do not need to alter the eager-mode distributed training codebase while experiencing the performance enhancement from full-model compilation; (2) Composability: SimpleFSDP can be seamlessly integrated with emerging distributed training techniques with minimal engineering effort; (3) Performance: training throughputs and memory gains from full-graph tracing and compiler optimizations; and (4) Debuggability: SimpleFSDP exhibits usability in PyTorch eager mode, where users have the flexibility to debug and agile develop the codebase. SimpleFSDP achieves the FSDP semantics by utilizing a few existing PyTorch primitives. First, representing the sharded per-parameter as DTensors PyTorch Community (2023b), SimpleFSDP achieves the ‚Äúall-gather before usage‚Äù behavior by applying collective communications (via the DTensor redistribute API) as tensor parametrization. Note that the backward gradient reduce-scatter is automatically achieved as parametrization and DTensor redistribute are differentiable. Second, given that in parametrization PyTorch Community (2023f), parameter all-gathers are treated as activation computations, SimpleFSDP achieves the additional memory optimization of ‚Äúrelease after forward usage, all-gather again before backward usage‚Äù by wrapping the parametrization module using activation checkpointing PyTorch Community (2023a). Since parametrization, selective activation checkpointing, and DTensor APIs are all natively supported by torch.compile, SimpleFSDP obtains a full graph of communication and computation operations. SimpleFSDP introduces two optimization components in torch.compile‚Äôs backend TorchInductor, namely bucketing and reordering, to enhance the per-parameter sharding performance. The bucketing merges the communication operations111The operators are lowered to IR nodes in TorchInductor. We use the two terms interchangeably throughout the paper. in TorchInductor to reduce the frequency of issuing base communication. The reordering pre-fetches the parameters used for computation in later stages to overlap with the current stage‚Äôs computation for minimized communication exposure. Building on top of the optimizations, SimpleFSDP provides two interfaces to users to wrap the model, enabling both customization and automation. The first manual-wrapping enables users to customize the communications to bucket among modules and reorders the bucketed communication operations to reduce exposure. The auto-wrapping employs a greedy algorithm to bucket the communication operations as long as they can be overlapped by the computation operations and do not exceed memory limits. SimpleFSDP‚Äôs PyTorch-native implementation enables it to be seamlessly composed with other distributed training techniques. We demonstrate its composability with Tensor Parallel and Pipeline Parallel, meta initialization, mixed precision training, and activation checkpointing with only a few lines of code. Such composability is achieved while tracing the model‚Äôs full computation-communication graph and tested on scales up to the ultra-large 405 billion parameter Llama 3.1 model Dubey et al. (2024). In summary, our contributions are as follows: ‚Ä¢ We introduce SimpleFSDP, a PyTorch-native compiler-based FSDP framework featuring simplicity, composability, performance enhancement, and debuggability. ‚Ä¢ We devise SimpleFSDP highlighting (1) a unique collective communication implementation of FSDP via PyTorch primitives (parametrizations, selective activation checkpointing, and DTensor API), enabling full-graph tracing in model training; (2) the first-of-its-kind IR nodes bucketing and reordering in TorchInductor with flexible user interfaces (manual-wrapping and auto-wrapping) to customize and automate computation-communication overlapping. ‚Ä¢ We perform extensive evaluations of SimpleFSDP on Llama 3 models (up to the ultra-large 405B) using TorchTitan Liang et al. (2024), demonstrating its (1) Performance: up to 28.54% peak memory reduction and 68.67% higher throughput improvement, compared to the most widely adopted FSDP2 eager framework PyTorch Community (2023c); (2) Scalability and Composability: full-graph tracing when composed with other distributed training techniques while maintaining up to 6.06% throughput gains and 8.37% memory reduction, compared to the existing best-performing sub-module compilation; (3) Debuggability: maintaining comparable memory and throughput in the eager mode."
https://arxiv.org/html/2411.00578v1,Federated Voxel Scene Graph for Intracranial Hemorrhage,"Intracranial Hemorrhage is a potentially lethal condition whose manifestation is vastly diverse and shifts across clinical centers worldwide. Deep-learning-based solutions are starting to model complex relations between brain structures, but still struggle to generalize. While gathering more diverse data is the most natural approach, privacy regulations often limit the sharing of medical data. We propose the first application of Federated Scene Graph Generation. We show that our models can leverage the increased training data diversity. For Scene Graph Generation, they can recall up to 20202020% more clinically relevant relations across datasets compared to models trained on a single centralized dataset. Learning structured data representation in a federated setting can open the way to the development of new methods that can leverage this finer information to regularize across clients more effectively.","Intracranial Hemorrhage (ICH) is a potentially lethal condition, which requires swift detection and treatment to improve the patients‚Äô odds of survival [9, 12]. However, the term ICH captures a variety of situations. For instance, hypertension can cause the spontaneous rupture of a blood vessel and lead to a subarachnoidal hemorrhage under the brain. In contrast, trauma patients will more often have a subdural or epidural hemorrhage along the skull. The difference for such cases are visualized in Fig. 2. Treatment decisions remain clinically challenging as they need to be 1) patient-centered despite the diversity of manifestations of ICH and 2) swift as the patient outcome worsens shortly after ICH onset [1]. The clinical routine involves the acquisition of head CTs for diagnosis . We can employ Deep Learning (DL) on such images to support clinicians in their decisions and improve the treatment of ICH patients. Clinical centers worldwide see local shifts in disease manifestation, which makes it problematic for purely supervised representation learning to perform to its full potential. Especially with patient data privacy, available data is often scarce. Federated Learning (FedL) of visual representation has gained traction in recent years [10], as it enables learning to address diversity issues without sharing the private data. This is especially relevant for medical data, since many hospitals own data and have capacities to gather annotations, but these data are usually patient data that need to be protected. By using FedL, one can leverage the heterogeneity of the available data to improve the models‚Äô generalizability while preserving the privacy of the patients. However, pure visual representation learning even using FedL only offers a superficial understanding of the clinical case, especially compared to the structured approach clinicians often use. Figure 1: Overview of the origin and diversity of the four datasets used for this study: INSTANCE2022, BHSD, CQ500, and a private cohort from Germany. We show the outline of ICH, the ventricle system, and midline. Bleeding 1 from INSTANCE2022, CQ500, and the private cohort all involve the ventricle system, which often serves as a buffer for other brain structures. The ventricle system can compress to absorb external pressure, or conversely it can fill with blood with possible expansion. Such changes are often accompanied by a midline shift, as in the samples of the INSTANCE2022, BHSD and private cohort datasets. Additionally, some images show the results of a previous surgical operation such as the presence of a ventricular drainage (appearing as a white dot within the slice) or even a craniectomy, see the red arrows. Sec. 4 offers detailed statistics over these cohorts. The majority of existing DL work focuses on the detection or segmentation of ICH [3, 6, 11, 18, 19, 25, 27, 29, 30, 32]. These models are trained using centralized learning on individual datasets, and often fail to generalize well to other data distributions. While segmentation is useful for computing the volume of the hemorrhage, it is ill-suited for the detection of individual bleeding [25]. Even detecting ICH accurately is not enough from a clinical perspective, as no clinical complication caused by the bleeding are modeled. The involvement of the ventricular system through hemorrhage expansion or the bleeding-induced shift of midline can occur and are both strong predictors of poor patient outcome [18, 8, 34]. The clinical utility of DL lies in analyzing the structure of the clinical cerebral scene using a specialized representation. Recently, Voxel Scene Graph Generation (V-SGG) [26] has shown promising results in modeling the clinical cerebral scene through a structured representation incorporating both ICH localization and the relations between ICH and adjacent brain structures. Likewise to other studies using Centralized Learning, the models detected 24242424% fewer relations when evaluated for Scene Graph Generation on an external cohort with a tangible data shift. We introduce Federated Voxel Scene Graph Generation. Motivated by Neural Motifs [35] and Iterative Message Passing [31], we propose the Fed-MOTIF and Fed-IMP methods, which learn a common relation distribution across clients in a federated setup and to minimize the bias towards client-local distributions. We validate our methods on four datasets originating from all over the world. Fig. 1 gives an overview of the data origins, as well as how anatomically dissimilar two ICH cases can be. Nevertheless, clinical decisions still depend on the same set of complex relations, independently of the precise ICH manifestation. Our models trained with FedL can recall up to 20202020% more clinically relevant relations compared to models trained on a single centralized dataset for Scene Graph Generation. With this work, we pioneer Federated Voxel Scene Graph111Code available at https://github.com/MECLabTUDA/VoxelSceneGraph, which generalizes across four datasets sourced worldwide and offer improved ICH detection for each bleeding type. (a) Spontaneous subarachnoid hemorrhage (b) Intraventricular hemorrhage (c) Intraparenchymal hemorrhage (d) Epidural hemorrhage in a trauma patient. Figure 2: Examples of the diversity in manifestation of ICH. The outline of the bleeding is shown in yellow. Hemorrhages such as in (a) may require a surgical intervention to repair any ruptured blood vessel or the placement of a drainage to relieve pressure. Similarly, involvement of the ventricular system as in (b) can cause occlusive hydrocephalus and will also require a drainage for the accumulating cerebrospinal fluid. Intraparenchymal (c) and epidural (d) hemorrhages, while dissimilar in appearance, can both cause midline shifts (c and d). Such a shift is associated with increased intracranial pressure and may require surgery."
