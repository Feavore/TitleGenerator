URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04859v1,A multi-purpose automatic editing system based on lecture semantics for remote education,"Remote teaching has become popular recently due to its convenience and safety, especially under extreme circumstances like a pandemic. However, online students usually have a poor experience since the information acquired from the views provided by the broadcast platforms is limited. One potential solution is to show more camera views simultaneously, but it is technically challenging and distracting for the viewers. Therefore, an automatic multi-camera directing/editing system, which aims at selecting the most concerned view at each time instance to guide the attention of online students, is in urgent demand. However, existing systems mostly make simple assumptions and focus on tracking the position of the speaker instead of the real lecture semantics, and therefore have limited capacities to deliver optimal information flow. To this end, this paper proposes an automatic multi-purpose editing system based on the lecture semantics, which can both direct the multiple video streams for real-time broadcasting and edit the optimal video offline for review purposes. Our system directs the views by semantically analyzing the class events while following the professional directing rules, mimicking a human director to capture the regions of interest from the viewpoint of the onsite students. We conduct both qualitative and quantitative analyses to verify the effectiveness of the proposed system and its components.","Mixed-mode or hybrid teaching with both onsite and online students has become a popular teaching practice during the pandemic situation, and it also provides a way to spread knowledge and promote education fairness around the world. Nowadays, students who cannot attend the onsite lectures for various reasons can still participate through video conferencing platforms online or watch the recordings offline. Nonetheless, the information and experiences received by these students are inferior to those received by the onsite students. One reason is that the information conveyed through the views provided by the platform is usually very limited, as shown in Fig.LABEL:fig:zoomview. The students cannot acquire the entire events from different views freely, and staring at the same view for a long time may cause mental stress[1]. On the other hand, if the platforms provide the students with many different views, it is both technically challenging (issues with synchronization, bandwidth, etc.) and the multiple video sources are difficult to browse through, so the remote students still have to manually select the view of interest during the class. Recently, a few automatic lectures recording systems [2, 3, 4, 5] have been proposed. However, these systems focus on automatically adjusting the camera viewpoint to capture the content of interests during the classes, instead of editing multiple video streams together. What the remote student can watch from these systems is either a shot displaying the speaker and his/her surroundings, or a set of raw video streams that require the students to switch manually. It may distract the students and cause information lapses. Therefore, in this paper, we propose an automatic multi-view editing system for lecture videos, which can process more diverse views and automatically edit/switch views based on class semantics. A few similar systems have been proposed by Rui et al. [6, 7] and Wang et al. [8]. Their systems adopted the Finite State Machine (FSM) as the editing model where each state corresponds to a camera view, and use the tracked positions and gestures of the speakers as the primary cues to trigger the state transitions. These systems typically contain four cameras and assume that the speakers can directly interact with the projector screen and the blackboard, so the positions of the hand or body are the regions of interest. Whereas, for the large classroom or reporting hall, as shown in Fig.2 which is an extended scene from previous works and used as an experimental scene by our system, these low-level cues are not always effective in representing the focus since students shift their attention according to the events in class instead of where the teacher is. For example, the attention would focus on the slide view instead of the speaker when the speaker flips the slides through a computer. Moreover, the editing rules are hard-coded in the FSM framework employed, which makes the resultant videos too predictable [9] and limits the capacity of the system to embed new events and new rules. As a result, it will be difficult for the users to adjust the styles of the generated videos according to their preferences. Figure 2: The illustration of our multi-view teaching environment. There are seven video streams, including close-up shots, medium shots, and long shots. Different shots can be used to convey different information.. In this work, we propose a semantics-based automatic editing system with a computational framework. Unlike previous studies [10, 11, 6, 7] that assume the speaker’s positions are the attention regions for the remote students, our system firstly analyzes the semantics of video contents to assess the focus scores of different shots. We observe that the student’s attention is dominated by some special events in class. For example, The students would focus on the content of the blackboard when the teacher is writing something, rather than the teacher himself. As the purposes of different shots vary, different semantics analysis methods are proposed to assess the shots based on their functions, e.g., a writing event recognizer is proposed to assess the blackboard close-up shot, and more details are introduced in Sec.III-B. In addition, we also take general cinematographic rules [12, 13] into consideration to improve the viewing experience. Unlike the previous systems that hard-code editing rules, our system converts the declarative cinematographic rules into computational expressions, as discussed in Sec.III-C. Besides focus assessment, the editing framework also plays an important role. To improve flexibility and optimality, we propose a multi-purpose optimization-based editing framework. Unlike previous studies that select the shots based on a set of predefined rules, this framework integrates the editing rules, e.g, shot duration, as soft constraints, which allows the system breaks the cycle of rigidity if necessary. For example, if the teacher has been writing something for a relatively long time, the system should stay in the close-up view. In contrast, the rigid shot duration constraint in the rule-based system will motivate a switch to a new view. In addition, our system is multi-purpose and allows users to choose the online mode (live broadcast purpose), offline mode (editing purpose), or a balance between them (look-ahead broadcast purpose) by adjusting a look-ahead duration. More details will be discussed in III-D. To summarize, our contributions mainly include the following aspects: 1. Firstly, we propose several practical class semantics analysis methods to assess the attention of shots. To the best of our knowledge, this study is the first attempt to explore video semantics to guide the editing of lecture videos. To evaluate the semantics analysis methods and the proposed editing system, we build a dataset by collecting synchronized multi-view videos from real classes and annotating the writing event. We will make this dataset public to promote the research in this direction. 2. We further develop a multi-purpose optimization-based editing framework, in which the general editing rules are treated as soft constraints to achieve an optimal solution, and the users can choose different modes by simply adjusting the look-ahead duration. 3. Qualitative and quantitative analyses have been conducted on the collected dataset to demonstrate the effectiveness of the proposed system and its components. Moreover, in order to compare the real user experience of different systems, we also conduct a user study to assess our system."
https://arxiv.org/html/2411.04366v1,The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing,"We present “The Concatenator,” a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.’s “musaicing” (or “audio mosaicing”) technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger’s NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.","Concatenative synthesis, or audio mosaicing, is a data-driven approach to arrange granular fragments of audio samples, particularly using data sourced from the spectral-temporal features of a target sound. While granular synthesis systems typically rely on combinations of aleatoric parameterization, deterministic automation, and traditional synthesis modulation to achieve complex and evolving textures from sound fragments [28], concatenative synthesis algorithms utilize Music Information Retrieval technology to decide parameters such as the index, amplitude, and pitch of each sound fragment. Modern music producers are inundated by audio data. Services like Splice offer hundreds of thousands of samples readily available on the cloud, and Kontakt multi-sample libraries can often take up over 10gb of disk space to capture a single instrument. Music Producers generate plenty of their own audio data as well: stems, multi-tracks, long-form recordings, and mix variations account for a large portion of many a music producer’s audio collection. Recent software such as XO by XLN Audio, Sononym, and Ableton Live 12 offer automatic organization of audio files based on various tags and descriptors, but these implementations of MIR technology are more utilitarian than creative in their design and application. Meanwhile, concatenative synthesis options remain sparse since its conceptual inception [31]: Reformer by Krotos is designed to create foley designs, apps like Samplebrain and CataRT [32, 33] are lacking in critical musical areas such as pitch tracking, with the more advanced options having limited accessibility for artists, requiring prior knowledge of Max (FluCoMa, MuBu) or Python (Audioguide). The Concatenator advances concatenative synthesis in 3 major ways: 1) it is capable of accurately reproducing harmonic and percussive sounds using arbitrary corpora 2) in real-time at scale, 3) affording new levels of control and accessibility. Furthermore, unlike neural audio systems [5], it requires no training and can adapt to arbitrary corpora at runtime. The speed, ease, and scope of The Concatenator offers a fresh paradigm for music producers to interact creatively with their ever-expanding excess of audio data, leading to what we believe is a breakthrough in the field."
https://arxiv.org/html/2411.03823v1,Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination,"The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.","The development of MLLMs has exceeded expectations (Liu et al., 2023a; Lin et al., 2023), showcasing extraordinary performance on various multimodal benchmarks (Lu et al., 2022; Liu et al., 2023b; Song et al., 2024), even surpassing human performance. However, due to the partial obscurity associated with MLLMs training (OpenAI, 2023; Reid et al., 2024), it remains challenging to definitively ascertain the impact of training data on model performance, despite some works showing the employment of the training set of certain datasets (Liu et al., 2023a; Chen et al., 2023; Bai et al., 2023b). The issue of data contamination, occurring when training or test data of benchmarks is exposed during the model training phase (Xu et al., 2024), could potentially instigate inequitable performance comparisons among models. This not only creates a dilemma for users in model selection but also poses a significant hurdle to further advancements in this domain. While numerous works in the field of LLMs have proposed methods for detecting data contamination (Yeom et al., 2018; Deng et al., 2024; Dong et al., 2024), MLLMs, due to their various modalities and multiple training phases (Liu et al., 2023a; Li et al., 2023), face limitations when applying these methods. Therefore, there is a pressing need for a more suitable multimodal contamination detection framework specifically tailored for MLLMs. In this study, we carry out a systematic analysis of multimodal data contamination. Initially, we define Multimodal Data Contamination, as it pertains to the modality of data sources exposed to the MLLMs, into two categories: Unimodal Contamination and Cross-modal Contamination. Subsequently, we unveil a detection framework for multimodal data contamination, MM-Detect, which incorporates two methods, Option Order Sensitivity Test and Slot Guessing for Perturbation Caption, designed to handle two common types of Visual Question Answering (VQA) tasks: multiple-choice and caption-based questions, respectively. Then, applying MM-Detect on eleven widely-used MLLMs across five prevalent VQA datasets, we observe that both open-source and proprietary MLLMs do exhibit contamination, with the degree of contamination varying across different models. To corroborate the validity and sensitivity of our approach, we deliberately induce contamination in MLLMs, thus simulating feasible real-world scenarios. Experimental results indicate that our method proves to be quite effective and sensitive in identifying varying degrees of contamination. Interestingly, our findings reveal that not only does leakage in the multimodal benchmark test set play a role, but the training set can also contribute significantly to enhancing the model’s performance. To further delve into the stage where contamination is introduced, we employ a heuristic method. This method seeks to distinguish whether the contamination originates from the pre-training phase of LLMs or the multimodal training phase. Our findings suggest that the contamination observed in some MLLMs may not necessarily stem from the multimodal training phase. Instead, it could potentially be traced back to the pre-training stage of their respective LLMs. To the best of our knowledge, our work is the first effort to systematically analyze multimodal data contamination. In conclusion, our research makes several important contributions: • We formulate the definition for multimodal contamination detection and present the MM-Detect framework, comprising two innovative methods specifically designed for effective contamination detection in MLLMs. • We demonstrate that leakage from multimodal benchmark data can significantly boost the model’s performance on test sets, with this enhancement intensifying as the degree of contamination increases. • By employing a heuristic method, we pioneer the exploration into the stage at which contamination is introduced, revealing that it may stem not solely from the multimodal data but could also from the LLMs."
https://arxiv.org/html/2411.03921v1,Inter-Frame Coding for Dynamic Meshes via Coarse-to-Fine Anchor Mesh Generation,"In the current Video-based Dynamic Mesh Coding (V-DMC) standard, inter-frame coding is restricted to mesh frames with constant topology. Consequently, temporal redundancy is not fully leveraged, resulting in suboptimal compression efficacy. To address this limitation, this paper introduces a novel coarse-to-fine scheme to generate anchor meshes for frames with time-varying topology. Initially, we generate a coarse anchor mesh using an octree-based nearest neighbor search. Motion estimation compensates for regions with significant motion changes during this process. However, the quality of the coarse mesh is low due to its suboptimal vertices. To enhance details, the fine anchor mesh is further optimized using the Quadric Error Metrics (QEM) algorithm to calculate more precise anchor points. The inter-frame anchor mesh generated herein retains the connectivity of the reference base mesh, while concurrently preserving superior quality. Experimental results show that our method achieves 7.2% ∼similar-to\sim∼ 10.3% BD-rate gain compared to the existing V-DMC test model version 7.","With rapid advancements in 3D capture, modeling, and rendering, 3D immersive content is increasingly being applied in Augmented Reality (AR), Virtual Reality (VR), and Autonomous Driving (AD). In these applications, 3D meshes are the most prevalent media format because they are well-suited for GPU rendering and highly applicable to interactive and real-time 3D tasks. A dynamic mesh sequence consists of consecutive static mesh frames, each of which has a ground of polygonal faces. Compared with traditional 2D media, 3D mesh requires substantial data for vivid 3D representation, highlighting the need for efficient compression methods. Although existing mesh compression technologies, such as Draco[rossignac1999edgebreaker] and TFAN[mamou2009tfan], have demonstrated superior performance in static meshes, they are inadequate for handling dynamic mesh sequences. To address this challenge, the 3D Graphics and Haptics Coding (3DG) of the Moving Picture Experts Group (MPEG) has formed a standard framework for video-based dynamic mesh coding(V-DMC) to explore more effective coding algorithms [mammou2022ideo]. For intra-frame coding, V-DMC decomposes an original mesh into two key components for compression: a decimated base mesh and a detailed displacement field to efficiently compress the geometry information. The base mesh preserves the fundamental properties of the input mesh while reducing the number of vertices and faces. The differences between the input mesh and the subdivided base mesh are calculated and encoded as displacements. Fig. LABEL:inter-frame_coding_framework illustrates the workflow of V-DMC inter-frame coding for dynamic mesh sequences with constant topology. For the target mesh frame M(t), V-DMC estimates a motion field by tracking the corresponding vertices between M(t) and the reference mesh frame M(k). Subsequently, the reference base mesh B(k), which is decimated from M(k), is deformed into the current base mesh B(t) using the estimated motion field. Therefore, B(t) has the same connectivity with B(k). After subdividing B(t), the residuals between the nearest point on the surface of M(t) and each vertex of the subdivided base mesh are calculated as displacements. This method can significantly reduce data volume by using a shared base mesh. However, for mesh sequences that exhibit a time-varying topology, V-DMC cannot compute the motion field by tracking the corresponding vertices to generate the target base mesh. While dynamic point cloud compression technologies [zhu2020view, fan2023multiscale, xia2023learning] have reached a high degree of maturity, the domain of dynamic mesh compression remains underexplored. In the pioneering research on dynamic mesh coding [habe2004skin, projection, Static], meshes are projected onto 2D images, which are subsequently encoded using video codecs. These methods cannot efficiently encode the connectivity between vertices because of the projection error on 2D images. Other methods attempt to utilize transformations to represent dynamic meshes [han2007time, yamasaki2010patch, jin2023inter]. However, they are limited to geometry compression and require constant connectivity. To address the issue, we propose a novel inter-frame coding scheme for meshes with time-varying topology. Specifically, we utilize a coarse-to-fine method to generate a high-quality anchor mesh that shares the same topology as the reference base mesh. Initially, we generate a coarse anchor mesh through an octree-based nearest neighbor search [octree] and motion estimation between the reference base mesh and the target input mesh. The rapidly generated anchor mesh ensures the time consistency of the topology. Furthermore, the fine anchor mesh is optimized using the Quadric Error Metrics (QEM) algorithm [garland1997surface] based on the coarse mesh to enhance its quality, resulting in the high-quality anchor mesh. One toy example of this coarse-to-fine method is depicted in Fig. LABEL:topology. Additionally, we have developed a more precise method for adaptive displacement quantization that takes into account the significance of each vertex. The significance is calculated based on its neighbor count. The experimental results show that our proposed method can achieve 7.2% ∼similar-to\sim∼ 10.3% BD-rate gain without a significant increase in encoding and decoding time. The ablation study further demonstrates that each constituent module confers distinct advantages, in which the QEM-based refinement shows a particularly pronounced efficacy. Figure 1: Architecture of proposed coarse-to-fine inter-frame anchor mesh generation method."
https://arxiv.org/html/2411.03595v1,Investigating Conceptual Blending of a Diffusion Modelfor Improving Nonword-to-Image Generation,"Text-to-image diffusion models sometimes depict blended concepts in the generated images. One promising use case of this effect would be the nonword-to-image generation task which attempts to generate images intuitively imaginable from a non-existing word (nonword). To realize nonword-to-image generation, an existing study focused on associating nonwords with similar-sounding words. Since each nonword can have multiple similar-sounding words, generating images containing their blended concepts would increase intuitiveness, facilitating creative activities and promoting computational psycholinguistics. Nevertheless, no existing study has quantitatively evaluated this effect in either diffusion models or the nonword-to-image generation paradigm. Therefore, this paper first analyzes the conceptual blending in a pretrained diffusion model, Stable Diffusion. The analysis reveals that a high percentage of generated images depict blended concepts when inputting an embedding interpolating between the text embeddings of two text prompts referring to different concepts. Next, this paper explores the best text embedding space conversion method of an existing nonword-to-image generation framework to ensure both the occurrence of conceptual blending and image generation quality. We compare the conventional direct prediction approach with the proposed method that combines k𝑘kitalic_k-nearest neighbor search and linear regression. Evaluation reveals that the enhanced accuracy of the embedding space conversion by the proposed method improves the image generation quality, while the emergence of conceptual blending could be attributed mainly to the specific dimensions of the high-dimensional text embedding space.","Text-to-image diffusion models (Sohl-Dickstein et al., 2015; Rombach et al., 2022) generate images depicting blended concepts when an interpolated embedding between embeddings of multiple text prompts is input (Melzi et al., 2023). Figure 1 illustrates the conceptual blending exhibited by a diffusion model, Stable Diffusion (Rombach et al., 2022; Vision and at Ludwig Maximilian University of Munich, 2022). As it uses Contrastive Language-Image Pretraining (CLIP) (Radford et al., 2021) text encoder for computing conditioning text embeddings, it sometimes generates blended concepts (e.g., “calf in a cave”) when inputting the midpoint of the CLIP text embeddings of two prompts referring to different concepts (“calf” and “cave”). This conceptual blending suggests various promising use cases, although the effect itself has not been well-studied. One such use case is the nonword-to-image generation task (Matsuhira et al., 2023b, a, 2024) which aims to generate images intuitively imageable from a given non-existing word (nonword). Generating such images for a nonword can facilitate creative activities including brand naming and computational psycholinguistics. One approach for this task suggested by an existing study (Matsuhira et al., 2023b, a, 2024) is to generate images depicting concepts of similar-sounding words, assuming that humans associate nonwords with the meanings of such words. Here, conceptual blending can improve intuitiveness when nonwords are associated with multiple words. For instance, if a nonword “calve” (\textipa/”kæv/111This paper describes word pronunciation using International Phonetic Alphabet (IPA) symbols. These symbols are also used as input of the existing nonword-to-image generation method to calculate pronunciation similarity.) is associated with two similar-sounding words “calf” (\textipa/”kæf/) and “cave” (\textipa/”keIv/), it can be more intuitive to generate images depicting the blended concept of the two words than depicting only either concept. Nonetheless, none of the existing literature provides a clear answer as to under which conditions diffusion models exhibit conceptual blending, and how it emerges in nonword-to-image generation results. Therefore, this paper conducts two evaluations to assess the occurrence of conceptual blending in each situation. The first one evaluates the capability of a pretrained Stable Diffusion to exhibit conceptual blending by detecting the presence of blended concepts in the generated images. Using these detection metrics, the second one evaluates the occurrence of the effect in nonword-to-image generation results. Here, we explore the best text embedding space conversion method in the nonword-to-image generation framework adopting the same pretrained Stable Diffusion model. The conventional framework (Matsuhira et al., 2023b, a, 2024) took a direct approach to train a Multi-Layer Perceptron (MLP) to transfer embeddings between spaces. However, this yields large information loss, which could lead to inaccurate and poor-quality image generation and a reduced chance of conceptual blending. Alternatively, we propose a more accurate method by combining k𝑘kitalic_k-nearest neighbor search and linear regression. Our evaluation aims to investigate how the reduced loss affects conceptual blending and image generation quality. Accordingly, this paper makes the following two contributions: • We quantitatively evaluate a pretrained Stable Diffusion to discover under which conditions it exhibits conceptual blending given an interpolated embedding between two concepts. • We explore the best text embedding space conversion method in the existing nonword-to-image generation framework to analyze factors that affect the emergence of conceptual blending as well as the image generation quality."
https://arxiv.org/html/2411.03948v1,Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks,"This paper investigates the capabilities of text-to-audio music generation models in producing long-form music with prompts that change over time, focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We introduce Babel Bardo, a system that uses Large Language Models (LLMs) to transform speech transcriptions into music descriptions for controlling a text-to-music model. Four versions of Babel Bardo were compared in two TRPG campaigns: a baseline using direct speech transcriptions, and three LLM-based versions with varying approaches to music description generation. Evaluations considered audio quality, story alignment, and transition smoothness. Results indicate that detailed music descriptions improve audio quality while maintaining consistency across consecutive descriptions enhances story alignment and transition smoothness.","Recent text-to-audio music generation models such as MusicLM [1] and MusicGen [2] are capable of producing high-quality music in the audio domain that aligns with a given textual description. These models typically generate music autoregressively by predicting the next token from a context window, which limits the size of the signal they can model. While the context size is limited, these models can generate longer signals by sliding a context window through time. Regardless of this capability, they have mainly been evaluated with a fixed prompt and for relatively short music durations. For instance, MusicGen [2] was evaluated considering 30-second music pieces, each generated from a single music description. In this paper, we are interested in evaluating whether text-to-music models can maintain music quality while generating long music pieces, where music descriptions change over time. It is important to evaluate text-to-music models considering long music pieces (greater than 30 seconds, for example) because many music production scenarios involve music durations longer than one can generate with a single short audio context window (e.g., pop music composition, jazz improvisation, soundtrack generation). One key problem of generating long sequences from a small context is that a model has to split the generation into multiple parts, ensuring that the independent parts are smoothly connected in the final composition. Moreover, one might change the initial prompt at any time step, steering the composition in a different direction, and the model must consider both the previous audio context and the new prompt. Figure 1: At every 30 seconds of gameplay, Babel Bardo transcribes the players’ speeches into a text sisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT using a Speech Recognition system and uses a Large Langue Model (LLM) to map sisubscript𝑠𝑖s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into a music description disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that matches the scene described by the players. This music description is given to a Text-to-Music system that generates a 30-second piece aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT directly in the audio domain. In this paper, we investigate long generation with text-to-audio models in the context of Tabletop Role-Playing Games (TRPGs). In this scenario, a music generator takes speech as input and must generate music that matches the story being told by the players. We chose this problem because it inherently poses the challenge of long music generation, where prompts have to change over time to adjust for different story scenes. We also use TRPGs as a research object because TRPG players often enhance their gaming experience by manually selecting songs to play as background music [3], which allows us to compare the results of a generator against a human baseline. To investigate the capabilities of current text-to-music models in generating background music for TRPG stories, we’ve built a system called Babel Bardo, which is inspired by Bardo Composer[4], a system that generates symbolic music by transcribing players’ speeches into text and conditioning an autoregressive model with the emotional tone of this text, as given by an emotion classifier. Different than Bardo Composer, Babel Bardo composes music directly in the audio domain by leveraging a Large Language Model (LLM) to transform the speech transcriptions into music descriptions every 30 seconds of gameplay. These descriptions are then given to a text-to-music model to generate a piece of music for that current moment of the story. Figure 1 shows an overview of our system. Babel Bardo is inspired by Hermann1[5], which uses LLMs and text-to-music models to generate soundtracks for films. We compared four different versions of Babel Bardo in two TRPG campaigns played on YouTube: Call of the Wild (in English) and O Segredo na Ilha (in Brazilian Portuguese). The first version is our baseline and uses the speech transcriptions directly as prompts for a text-to-music model. All other versions use an LLM to transform the transcriptions into music descriptions. The second one follows the Bardo Composer approach and applies an LLM as an emotion classifier. The music description follows a template that is adjusted based on the emotion given by the LLM. The remaining two versions use the LLM to produce a complete music description; however, one generates a new description for every transcript, while the other can just continue the previously generated segment if the scene hasn’t changed. We evaluated our models according to audio quality, alignment with the story, and transition smoothness between transcriptions. Results suggest that while detailed music descriptions contribute to improved audio quality, maintaining consistency across consecutive descriptions helps achieve smoother transitions between musical segments. Furthermore, our findings indicate that emotion serves as an effective signal for aligning generated music with TRPG narratives."
https://arxiv.org/html/2411.03010v1,"Learning-based Lossless Event Data Compression††thanks:This work was supported by RayShaper SA, Valais, Switzerland, through the Project entitled Event Aware Sensor Compression, and by the Fundação para a Ciência e a Tecnologia (FCT), Portugal, through the Project entitled Deep Compression: Emerging Paradigm for Image Coding under Grant PTDC/EEI-COM/7775/2020.","Emerging event cameras acquire visual information by detecting time domain brightness changes asynchronously at the pixel level and, unlike conventional cameras, are able to provide high temporal resolution, very high dynamic range, low latency, and low power consumption. Considering the huge amount of data involved, efficient compression solutions are very much needed. In this context, this paper presents a novel deep-learning-based lossless event data compression scheme based on octree partitioning and a learned hyperprior model. The proposed method arranges the event stream as a 3D volume and employs an octree structure for adaptive partitioning. A deep neural network-based entropy model, using a hyperprior, is then applied. Experimental results demonstrate that the proposed method outperforms traditional lossless data compression techniques in terms of compression ratio and bits per event.","Event cameras, also known as neuromorphic cameras or dynamic vision sensor (DVS) cameras, are currently attracting a lot of attention by the research community [1]. Unlike conventional cameras that measure absolute brightness at fixed time intervals, event cameras have a novel type of vision sensor that measures at the pixel level, in an asynchronous and independent way, brightness changes in the time domain, also known as temporal contrast. This new way of acquiring visual information allows event cameras to achieve high temporal resolution, very high dynamic range, low latency and low power consumption. These features make event cameras attractive for different types of applications. For instance, in robotics event cameras may enable a more precise object tracking and collision avoidance while in autonomous driving they may allow real-time obstacle detection and high-speed motion estimation; additionally, the way event cameras acquire visual information allows tracking fast-moving objects without suffering motion blur, an inherent limitation of conventional cameras. An event camera produces a sequence of events, or an event stream, as response to detected brightness changes in the time domain, where each event is typically represented by the 4-tuple (x𝑥xitalic_x, y𝑦yitalic_y, t𝑡titalic_t, p𝑝pitalic_p). This 4-tuple representation encloses the basic components describing an event: the spatial coordinates x𝑥xitalic_x, y𝑦yitalic_y (within the sensor) where the event occurred, the time t𝑡titalic_t at which the event occurred (also known as timestamp) and the event polarity p𝑝pitalic_p (+/-1) indicating the brightness change direction (increase or decrease) that triggered the event. An event stream is typically characterized by high temporal correlation while, in the spatial dimension, it is often sparse, since only pixels corresponding to moving areas and/or illumination changing areas trigger events. Thus, compression is crucial for an efficient storage, transmission, and processing of event data. However, the inherent spatial sparsity of event data, coupled with the need to maintain the spatial and temporal relationships between events, poses a challenge for achieving high compression. Recently, the JPEG standardization group has launched an exploration activity on event-based vision, called JPEG XE [2], precisely acknowledging the importance of developing efficient event data compression schemes. Lossless compression has currently been receiving more attention from the research community and has also been adopted by the JPEG XE Common Test Conditions (CTC) [3], which reinforces its practical importance. Lossless compression schemes exploit the inherent patterns and redundancies within the event stream, such as temporal, spatial and polarity data distributions, to encode the event data in a more compact form while allowing to obtain a perfect event data reconstruction from the compressed bitstream, without any precision loss in event representation components. This might be a rather important feature for machine vision tasks where most (if not all) the events are considered relevant to achieve high performance. In this context, this paper proposes a novel deep-learning-based lossless event data compression scheme. The idea is to arrange the event stream as a 3D volume in x𝑥xitalic_x, y𝑦yitalic_y, and t𝑡titalic_t dimensions and then use an octree structure to adaptively partition the 3D volume (of events). The binary representation of the octree structure, which corresponds to a denser representation of the 3D event data volume, is then entropy coded with a learning-based entropy model. One of the main contributions of this work lies on the usage of a deep neural network to obtain the probability model of a hyperprior-based arithmetic coder. Experimental results demonstrate that the proposed solution outperforms traditional lossless data compression techniques both in terms of bits per event and compression ratio. The rest of the paper is structured as follows: Section II briefly reviews background work on lossless event data compression, while the proposed compression method is described in Section III. Performance evaluation is presented and analysed in Section IV, and finally, Section V concludes the paper."
https://arxiv.org/html/2411.02851v1,"Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization","The goal of Multilingual Visual Answer Localization (MVAL) is to locate a video segment that answers a given multilingual question. Existing methods either focus solely on visual modality or integrate visual and subtitle modalities. However, these methods neglect the audio modality in videos, consequently leading to incomplete input information and poor performance in the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method that incorporates audio modality to augment both visual and textual representations for the MVAL task. Specifically, we integrate features from three modalities and develop three predictors, each tailored to the unique contributions of the fused modalities: an audio-visual predictor, a visual predictor, and a textual predictor. Each predictor generates predictions based on its respective modality. To maintain consistency across the predicted results, we introduce an Audio-Visual-Textual Consistency module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing each modality’s predictor to dynamically learn from the others. This collaborative learning ensures that the model generates consistent and comprehensive answers. Extensive experiments show that our proposed method outperforms several state-of-the-art (SOTA) methods, which demonstrates the effectiveness of the audio modality.","With the rapid expansion of the internet, an increasing number of users are turning to online platforms to seek medical advice by posing natural language questions (O’Donnell et al., 2023; Lim et al., 2022). Current online platforms typically fall into two categories: those that provide textual answers, which may be difficult for users to interpret, and those that offer visual answers, which are generally more intuitive and easier to follow (Tang et al., 2021b). However, the retrieved videos often contain substantial amounts of information irrelevant to the user’s query (Moon et al., 2023), which significantly hinders the efficiency of information retrieval (Zhang et al., 2023). In response to this challenge, the task of Visual Answer Localization (VAL) has been introduced (Weng and Li, 2023). Figure 1: (a) Overview of the audio-enhanced multilingual video answer localization task. (b) Difference between existing methods and our method. (c) Performance comparison diagram of visual-based, textual-based, and our method. Existing VAL approaches can be broadly categorized into visual-based (Tang et al., 2021a; Chen et al., 2020a) and textual-based methods (Li et al., 2023a; Weng and Li, 2023; Li et al., 2024b). Visual-based methods are effective in scenarios where subtitle text is sparse, but their performance tends to degrade significantly in other contexts. In contrast, textual-based methods excel when abundant subtitle text is available, as the semantic similarity between the question and subtitle is typically greater than between the question and the video (Li et al., 2024b). However, these methods often overlook audio, which plays a crucial role in complementing both visual and textual modalities. There is inherent consistency and complementarity among these modalities (Chen et al., 2023a), and harnessing this synergy can enhance both visual and textual modalities by integrating information from the audio. Incorporating audio thus addresses the performance limitations in VAL, particularly in video segments lacking subtitles (Liu et al., 2022; Chen et al., 2020b; Sun et al., 2024). To this end, we study the Audio-enhanced Multilingual Visual Answer Localization (AMVAL) which aims to locate video segments that answer a user’s natural language question, in either Chinese or English. By providing video segments with verbal explanations for medical guidance, this approach not only facilitates the learning of specific actions but also helps bridge language barriers, making the content accessible to people who speak different languages (Macedonia and Knösche, 2011; Diamond et al., 2020). However, a significant challenge lies in effectively integrating the three modalities and fully utilizing their individual strengths to tackle the AMVAL task. To address this challenge, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method for AMVAL, aimed at reducing cross-modal discrepancies and improving the accuracy of span localization by integrating audio modality. We designed a network architecture with three modality channels (audio, visual, and textual) to fully leverage the semantic information from each modality in the video, addressing the limitations of single-modality methods. Each channel is equipped with a corresponding predictor: an audio-visual predictor, a visual predictor, and a textual predictor. During joint training, distinct objectives are assigned to each predictor, enabling them to leverage the unique strengths of their respective modalities. To improve modality integration, we introduce an Audio-Visual-Textual consistency module, which employs a Dynamic Triangular Loss (DTL) function based on Intersection over Union (IoU). This loss function aligns the modalities by minimizing the discrepancies between each predictor’s output and the target answer, as well as between the outputs of the other two predictors. Our approach promotes mutual learning among the predictors to achieve consistent and cohesive multimodal representations. Our contributions are as follows: (1) We study the AMVAL and propose the AVTSL method, which is the first to introduce the audio modality for the AMVAL; (2) We designed an Audio-Visual-Text Consistency module, which leverages the consistency and complementarity between different modalities using the DTL loss function; (3) We conducted extensive experiments to demonstrate the effectiveness of the AVTSL method, where our method outperformed other state-of-the-art (SOTA) methods by incorporating the audio modality."
https://arxiv.org/html/2411.03109v1,pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues,"Target Speaker Extraction (TSE) aims to extract the clean speech of the target speaker in an audio mixture, thus eliminating irrelevant background noise and speech. While prior work has explored various auxiliary cues including pre-recorded speech, visual information (e.g., lip motions and gestures), and spatial information, the acquisition and selection of such strong cues are infeasible in many practical scenarios. Unlike all existing work, in this paper, we condition the TSE algorithm on semantic cues extracted from limited and unaligned text content, such as condensed points from a presentation slide. This method is particularly useful in scenarios like meetings, poster sessions, or lecture presentations, where acquiring other cues in real-time is challenging. To this end, we design two different networks. Specifically, our proposed Text Prompt Extractor Network (TPE) fuses audio features with content-based semantic cues to facilitate time-frequency mask generation to filter out extraneous noise, while another proposal, namely Text-Speech Recognition Network (TSR), employs the contrastive learning technique to associate blindly separated speech signals with semantic cues. The experimental results show the efficacy in accurately identifying the target speaker by utilizing semantic cues derived from limited and unaligned text, resulting in SI-SDRi of 12.16 dB, SDRi of 12.66 dB, PESQi of 0.830 and STOIi of 0.150, respectively. Dataset and source code will be publicly available. Project demo page: https://slideTSE.github.io/.","Speech communication is inherently the most natural and predominant mode of interaction among humans. However, the efficacy of computational models in processing speech-related tasks, including but not limited to Automatic Speech Recognition (ASR) (Yue et al. 2019), speaker localization (Qian et al. 2021), Active Speaker Detection (ASD) (Tao et al. 2021), and Speech Emotion Recognition (SER) (Pan et al. 2020), is significantly compromised in scenarios with overlapping speakers and severe background noise. This motivates researchers to investigate different ways to extract high-quality target speaker’s speech. The ‘cocktail party problem’ (Cherry 1953) is defined as a common social situation in which multiple conversations occur simultaneously under ambient noise. Such an environment presents a challenge to interpersonal communication due to the potential interference from other speakers’ voices. In this case, the human auditory system has an extraordinary ability to selectively attend to the speech of a target speaker while effectively suppressing irrelevant acoustic interference, i.e., selective auditory attention. Figure 1: Illustration of our proposed pTSE-T task which extracts the presenter’s speech from the audio mixture (including interfering speaker’s speech and background noise) using unaligned text from the visual presentation slide. The cocktail party problem can be approached by extracting individual sound sources from mixed audio signals. In this case, speech separation (Hershey et al. 2016; Liu and Wang 2019; Luo and Mesgarani 2019; Kolbæk et al. 2017; Zeghidour and Grangier 2021) serves as a solution that separates a mixed signal into individual speech streams, given prior knowledge of the total speaker number. However, separated speech signals do not have an association with speaker identities, which often suffers from the ambigous ID permutation problem. However, with an unknown speaker number, the speech separation performance degrades. Differently, by mimicking humans’ intrinsic attentive listening ability, TSE does not require prior knowledge of the exact speaker number. Instead, it uses speaker-specific reference cues to accurately isolate the desired voice. For example, a pre-registered speech signal (Wang et al. 2019; Ge et al. 2020a; Xu et al. 2020) is used primarily as a cue to extract ID-consistent speech. Moreover, co-speech gestures (Pan, Qian, and Li 2022) and lip movements (Pan et al. 2020) from synchronized video recordings, or even live location (Ge et al. 2022) can also be used as the auxiliary visual cues. However, acquiring such strong cues either requires a pre-registration process or high-quality visual conditions, e.g., the speaker mostly moves inside the camera’s field of view with visible frontal face, continuous camera tracking, which may not always be available. In this work, we target the meeting or presentation scenario where multiple individuals speak simultaneously and strong cues such as live recording or voice pre-registration are not available. In these typical scenarios, speakers use visual support tools, such as PowerPoint slides or posters, to ensure that the information conveyed is easily understood with enhanced clarity and impact. The text contents, i.e. the key points or summaries on the presentation slides, offer limited but concise semantic cues which are intrinsically related to the speaker’s spoken content. To the best of our knowledge, no previous work has used such a text as the condition for TSE. Although challenging, it is useful in many scenarios. Therefore, in this paper, we propose a new task, namely Presentation Target Speaker Extraction with unaligned Text cues (pTSE-T), that uses the text on the slide as a cue for speaker extraction. Our key contributions can be summarized as follows. 1. We are the first proposal of the pTSE-T task, as illustrated in Fig. 1, which extracts the presenter’s clean speech from the audio mixture given the unaligned semantic text cues obtained from the presentation slide. 2. To facilitate this new pTSE-T task, we design and annotate an innovative data corpus, which includes temporal-synchronized speech and presentation slides. In addition, we also provide the ASR transcripts and the text descriptions extracted from slides using an open-source Optical Character Recognition (OCR) toolkit. 3. We propose two novel networks i.e. TPE and DPRNN-TSR. Specifically, TPE fuses the text prompts with audio features for TSE, achieving correct extraction rates of 96.46% and 95.26% on the MMSpeech-2mix and -3mix datasets, respectively. DPRNN-TSR employs contrastive learning to explore the intra-modal correlation between paired text prompts and speech data, achieving correct matching rates of 86.28% and 76.70%, respectively."
https://arxiv.org/html/2411.03085v1,Speech Separation with Pretrained Frontend to Minimize Domain Mismatch,"Speech separation seeks to separate individual speech signals from a speech mixture. Typically, most separation models are trained on synthetic data due to the unavailability of target reference in real-world cocktail party scenarios. As a result, there exists a domain gap between real and synthetic data when deploying speech separation models in real-world applications. In this paper, we propose a self-supervised domain-invariant pretrained (DIP) frontend that is exposed to mixture data without the need for target reference speech. The DIP frontend utilizes a Siamese network with two innovative pretext tasks, mixture predictive coding (MPC) and mixture invariant coding (MIC), to capture shared contextual cues between real and synthetic unlabeled mixtures. Subsequently, we freeze the DIP frontend as a feature extractor when training the downstream speech separation models on synthetic data. By pretraining the DIP frontend with the contextual cues, we expect that the speech separation skills learned from synthetic data can be effectively transferred to real data. To benefit from the DIP frontend, we introduce a novel separation pipeline to align the feature resolution of the separation models. We evaluate the speech separation quality on standard benchmarks and real-world datasets. The results confirm the superiority of our DIP frontend over existing speech separation models. This study underscores the potential of large-scale pretraining to enhance the quality and intelligibility of speech separation in real-world applications.","Speech separation seeks to separate individual speech signals in a complex, multi-speaker acoustic environments, commonly known as the ‘cocktail party problem’ [1]. Speech separation has been an active research, not only because it is a human cognitive ability, but also it serves as a frontend in many practical speech processing tasks, such as speaker recognition [2, 3, 4], automatic speech recognition [5, 6, 7], and voice conversion [8, 9]. Traditional speech separation technique includes independent component analysis (ICA) [10, 11, 12], non-negative matrix factorization (NMF) [13, 14, 15] or the theory of computational auditory scene analysis (CASA) [16, 17, 18]. With the advent of deep learning, supervised speech separation models, including both time-domain [19, 20, 21] and frequency-domain models [22, 23, 24], have achieved remarkable advancements. Despite impressive results, these models don’t generalize well to real-world scenarios. This is primarily due to the fact that most speech separation models are trained on synthetic anechoic data [25], that are very different from real-world speech data with unpredictable attenuation and reverberation [26, 27, 28]. In this paper, such a problem is referred to as domain mismatch. To mitigate this performance degradation due to domain mismatch, there were attempts to incorporate real-world speech mixtures during training. Unsupervised speech separation is one example. In [29], the authors utilize the mixture waveform to generate the mixture of the mixture (MOM) and train the separation model with the mixture invariant training method. Tzinis et al. [30] expand this architecture by incorporating various remixing methods. Karamatli et al. [31] further promote the model through cyclic learning for continuous improvement. Semi-supervised speech separation is another example. Zhang et al. [32] introduce a framework where the student model learns to predict pseudo-labels from the teacher model. In [33], Han et al. extend this approach by introducing an ensemble structure combined with separation consistency training methods. Furthermore, Sivaraman et al. [34] propose a combination of semi-supervised and unsupervised methods to improve the model performance in meeting scenarios. The above techniques incorporate real speech mixtures during model training. However, the quality improvement over conventional supervised methods [29] is limited, probably because they introduce another mismatch, i.e. target mismatch. The target labels in those techniques are either the mixture itself or fake labels rather than the clean reference speech. In a different attempt, Ben et al. [35] demonstrate that the domain-invariant knowledge is crucial for models to overcome domain mismatch. The effectiveness of domain-invariant knowledge has also been validated in various tasks [36, 37, 38]. Motivated by these findings, we seek to utilize the domain-invariant knowledge across real and synthetic data, to assist the downstream models in overcoming the domain discrepancy. Unlike the individual speech frames that are sensitive to channel and noise variation, contextual speech flow is a prominent auditory cue for speech perception and cognition across diverse scenarios [39]. According to the studies by Bregman et al. [40, 41, 42, 43], the cognition of specific speech signals in a complex environment is guided by the contextual knowledge. For example, in the study of double synthetic vowel experiment [44, 45, 46], psychoacousticians reveal that listeners unconsciously infer the most likely sound through the contextual cues, including phoneme and word information, during the perceptual process, which is referred to as the auditory induction [47]. In this paper, we are inspired by the prior findings to study a domain-invariant pretrained (DIP) frontend that captures contextual cues from unlabeled, real speech mixtures. With such a DIP frontend, we anticipate that the downstream speech separation models will acquire transferable knowledge for real-world scenarios. In practice, the proposed DIP frontend is a learnable contextual cue extractor trained on some pretext tasks with large-scale wild unlabeled or weakly labeled data. As demonstrated in [48], there are two typical types of pretext tasks for pretrained frontend, namely generative pretext task and discriminative pretext task. The generative pretext task [49, 50, 51, 52, 53, 54, 55] treats the speech itself as a sequential structure and reconstruct the detailed information based on the prior context. On the other hand, the discriminative pretext task [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 48, 67, 68, 69, 70] focuses on capturing the high-level “slow features” [39] that span many time steps, through predicting the masked frames in a contrastive manner or the index of masked clustering labels. We advocate that contrastive predictive coding (CPC) [39, 56, 57, 58], a discriminative pretext task, is suitable for reducing the domain mismatch between real and synthetic data because it learns contextual cues by encoding multi-scale information [71] with mutual information maximization. In this paper, we adopt a Siamese network as the domain-invariant pretrained frontend, and devise the pretraining strategy. The contributions of our paper are summarized as follows, • We introduce the concept of using mixture waveform for self-supervised frontend pretraining and design pretext task to capture the contextual cues with both real and synthetic inputs. • We derive the domain discrepancy distance and formulate a Siamese network with a novel domain loss to further reduce domain mismatch. • We propose a general pipeline to incorporate popular frontends into various separation models. • We conduct comprehensive experiments to validate the superior performance of DIP frontend across diverse real and synthetic datasets, with various downstream models, and different evaluation metrics. The rest of the paper is organized as follows. Section II discusses the prior studies on pretrained frontend. Section III formulates the pretraining method and introduces the DIP model architecture. In Section IV, we describe the model configuration and experimental setup. In Section V, we report the results. Finally, Section VI concludes the paper."
https://arxiv.org/html/2411.02607v1,Towards Context-Aware Adaptation in Extended Reality: A Design Space for XR Interfaces and an Adaptive Placement Strategy,"By converting the entire 3D space around the user into a screen, Extended Reality (XR) can ameliorate traditional displays’ space limitations and facilitate the consumption of multiple pieces of information at a time. However, if designed inappropriately, these XR interfaces can overwhelm the user and complicate information access. In this work, we explored the design dimensions that can be adapted to enable suitable presentation and interaction within an XR interface. To investigate a specific use case of context-aware adaptations within our proposed design space, we concentrated on the spatial layout of the XR content and investigated non-adaptive and adaptive placement strategies. In this paper, we (1) present a comprehensive design space for XR interfaces, (2) propose Environment-referenced, an adaptive placement strategy that uses a relevant intermediary from the environment within a Hybrid Frame of Reference (FoR) for each XR object, and (3) evaluate the effectiveness of this adaptive placement strategy and a non-adaptive Body-Fixed placement strategy in four contextual scenarios varying in terms of social setting and user mobility in the environment. The performance of these placement strategies from our within-subjects user study emphasized the importance of intermediaries’ relevance to the user’s focus. These findings underscore the importance of context-aware interfaces, indicating that the appropriate use of an adaptive content placement strategy in a context can significantly improve task efficiency, accuracy, and usability.","1 Related Work Previous work explored various innovative applications and design dimensions of XR to spatially place 2D & 3D objects and transition between them, visualize hierarchies, and provide persistent and portable presentation of the personal information [14, 20, 38, 16, 34]. Morrison et al. highlighted unique design elements within AR for enhancing accessibility for visually impaired children [45]. These studies underscore the broad design space of XR interfaces and the versatile and transformative applications that XR enables across various contexts. This work, investigates previous work and the design elements they utilized, providing a comprehensive XR design space. XR offers the potential to enable efficient information access, reduce cognitive load, and enhance user convenience compared to traditional methods such as mobile phones [55, 12, 43, 11, 40]. However, intrusive XR interfaces may result in challenges such as information overload and occlusion of important cues within the environment [31, 4], increase cognitive load and discomfort, and reduce the user’s situational awareness and performance [17, 25, 54]. Various approaches for intuitive and seamless integration of XR content into the environment have been extensively explored. For instance, to enhance efficiency and minimize intrusiveness, numerous designs adapt the XR content’s availability, transparency, placement, and Level of Detail (LoD) [15, 3, 46], as well as spatial layout and size [19, 9]. Lages & Bowman highlighted the significance of adapting the AR content placement strategy to avoid occlusions and accommodate activities like walking [33]. For adaptations to the XR content placement, the concept of the frame of reference, also referred to as fixation was introduced [20]. User-triggered adaptation through gaze, hand, and head-based inputs such as finger taps and handheld controllers are extensively explored for adjustments to the transparency, LoD, and spatial layout of XR content [19, 32, 50, 39, 48, 47]. In AR, for instance, many applications were designed to prioritize the real-world [11] by initially keeping the XR content hidden, in the peripheral, or at a lower LoD, and granting access to them through explicit interactions [49, 42, 46]. However, AR’s definition emphasizes the integration of the digital content into the real world [1], underscoring the significance of context awareness. While user-triggered adaptations offer control and predictability, they increase the user’s physical and mental workload of deciding when, what, and how to apply the adaptations [51]. Automatic XR adaptation can enhance efficiency and reduce workload compared to the user-triggered ones [11]. Numerous studies suggest rule-based approaches for XR adaptations. The significance of such rule-based adaptations in meeting the XR task requirements within various applications such as driving and conversation have been highlighted [4, 12]. To prevent occlusion issues, Ens et al. suggested a rule-based adaptive design to exclusively place the XR objects on empty surfaces [18]. Constraints, explicitly imposed by the users, were utilized as guidelines to group related XR objects together and prevent their occlusion within a rule-based view management [3]. Such rule-based adaptive approaches are highly tailored to specific use cases and applications.Even within the same application or use case, slight contextual deviations can cause a rule to fail, making it suitable only within unchanging contexts. This work proposes an adaptive placement strategy, applicable within changing contexts, to extract and utilize contextual information from the environment and user state to spatially place the XR content. Context refers to the external components that influence or relate to the user’s interactions with the interface [13]. In recent years, context-aware XR has become a focal point of research, promising the potential for “ubiquitous"" and “pervasive"" computing through AR [56, 26]. Contextual aspects such as user preferences, cognitive load, device profiles, task environment, semantic changes, and task-specific security parameters have been utilized for adaptations to the XR content’s appearance, LoD, frame of reference, and spatial layout [37, 9, 36]."
https://arxiv.org/html/2411.02551v2,"PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text","While piano music has become a significant area of study in Music Information Retrieval (MIR), there is a notable lack of datasets for piano solo music with text labels. To address this gap, we present PIAST (PIano dataset with Audio, Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy of semantic tags, we collected 9,673 tracks from YouTube and added human annotations for 2,023 tracks by music experts, resulting in two subsets: PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and transcribed MIDI utilizing state-of-the-art piano transcription and beat tracking models. Among many possible tasks with the multimodal dataset, we conduct music tagging and retrieval using both audio and MIDI data and report baseline performances to demonstrate its potential as a valuable resource for MIR research.","Piano music presents unique opportunities for music research due to its ability to express diverse styles using a single instrument and its superior transcription performance. Given these characteristics, it has become a significant area of study in Music Information Retrieval (MIR), encompassing tasks such as classification Hung et al. (2021); Chou et al. (2021), and music generation with various conditions Wu and Yang (2023); Choi and Lee (2023). While these tasks require datasets that combine piano audio with various modalities such as MIDI, sheet music, or text, there is a notable scarcity of such comprehensive multimodal piano datasets. However, existing multimodal music datasets, particularly music-text datasets, rarely focus exclusively on piano music, and piano solo pieces comprise only a small portion of general music-text datasets. For instance, in the ECALS Dataset Doh et al. (2023), a subset of the Million Song Dataset Bertin-Mahieux et al. (2011), the number of piano solo tracks is very limited. We observed that excluding tracks tagged with instruments other than the piano or genres that could not be solely represented by the piano, only approximately 0.46% of the entire dataset can be identified as piano solo music. Several piano datasets, such as MAESTRO Hawthorne et al. (2019), have been developed in recent years, which provide classical piano performances primarily used for piano transcription. Another classical piano dataset, GiantMIDI Kong et al. (2022), is also commonly used in transcription tasks. Other datasets like Pop1K7 Hsiao et al. (2021) focus on the performance generation of pop piano music, while PiJAMA Edwards et al. (2023) is employed for performer identification tasks with their jazz piano data. However, these datasets are confined to a single genre and lack text labels. This absence of genre diversity within a single dataset and the lack of textual information underscores the need for a piano dataset with text information. Some piano datasets contain emotion labels, such as EMOPIA Hung et al. (2021) and VGMIDI Ferreira and Whitehead (2019). However, these datasets are annotated only with emotion information based on either Russell’s four quadrants Hung et al. (2021) or the valence-arousal model Ferreira and Whitehead (2019). This limited annotation approach lacks the rich textual descriptions needed for text-based MIR tasks. To address the limitations, we present multimodal piano music data with rich text annotations and transcribed MIDI. To build the dataset, we first created a piano-specific taxonomy with 31 tags that include genre, emotion, mood, and style information to encompass the broad and diverse musical range that the piano can express. Based on this taxonomy, we collected data from YouTube, transcribed it to MIDI format, and conducted an annotation process. The PIAST dataset consists of two subsets: PIAST-YT, 9,673 tracks collected from YouTube, providing audio and text information (titles, tags, and descriptions), and PIAST-AT, 2,023 tracks with annotations by music experts. This dual approach ensures both breadth and accuracy in the dataset. Additionally, PIAST includes transcribed performance MIDI data alongside audio and text, enhancing its capabilities beyond existing methods Hsiao et al. (2021). This paper details the dataset collection process and analyzes the data. We present baseline results for piano music annotation and retrieval tasks, utilizing the PIAST-YT and the PIAST-AT datasets across audio and MIDI domains. The PIAST dataset is available in our online repository111https://hayeonbang.github.io/PIAST_dataset/, and the source code for the experiments can be found on GitHub222https://github.com/Hayeonbang/PIAST."
https://arxiv.org/html/2411.01561v1,Multimodal Graph Neural Network for Recommendation with Dynamic De-redundancy and Modality-Guided Feature De-noisy,"Graph neural networks (GNNs) have become crucial in multimodal recommendation tasks because of their powerful ability to capture complex relationships between neighboring nodes. However, increasing the number of propagation layers in GNNs can lead to feature redundancy, which may negatively impact the overall recommendation performance. In addition, the existing recommendation task method directly maps the preprocessed multimodal features to the low-dimensional space, which will bring the noise unrelated to user preference, thus affecting the representation ability of the model. To tackle the aforementioned challenges, we propose Multimodal Graph Neural Network for Recommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature De-noisy, which is divided into local and global interaction. Initially, in the local interaction process,we integrate a dynamic de-redundancy (DDR) loss function which is achieved by utilizing the product of the feature coefficient matrix and the feature matrix as a penalization factor. It reduces the feature redundancy effects of multimodal and behavioral features caused by the stacking of multiple GNN layers. Subsequently, in the global interaction process, we developed modality-guided global feature purifiers for each modality to alleviate the impact of modality noise. It is a two-fold guiding mechanism eliminating modality features that are irrelevant to user preferences and captures complex relationships within the modality. Experimental results demonstrate that MGNM achieves superior performance on multimodal information denoising and removal of redundant information compared to the state-of-the-art methods.","Multimodal recommendation methods (MRs) are essential in the field of Artificial Intelligence (AI) and are widely applied in scenarios, i.e., healthcare [1, 2], online platform [3, 4, 5], news recommendation [6, 7], and social media [8, 9]. It uses multimodal signals to bootstrap collaborative signals to better capture the user’s interests and provide recommendations that are better suited to the user’s needs. Current MRs typically consist of three main steps: multimodal information fusion, collaborative filtering, and prediction. Figure 1: Illustration of the proposed model. It is structured into global and local interaction. Firstly, global interaction leverages modality features to guide the elimination of redundant information irrelevant to user preferences. Secondly, a trainable transformation matrix captures complex relationships within the modality, providing global preference information. For local interaction, the model addresses feature redundancy from GNNs by penalizing over correlated features. Predictions are made by calculating the weighted sum of global and local interaction information. This approach captures both global and local user interests, offering a comprehensive characterization of user preferences. Early MRs often linearly integrate individual modality signals into collaborative signals to establish user and item representation models. These methods primarily address low-order user-item interactions, neglecting higher-order relationship modeling. Additionally, different users have different preferences for various modalities. Given the inherent advantage of graph neural networks (GNNs) in capturing neighbor signals, applying GNNs to MRs is a natural choice. Specifically, GNNs aggregate neighbor information to capture more interaction details. Despite the commendable performance achieved by existing models, they still struggle with the following challenges: Many existing GNNs-based recommendation models contain only two layers of GNNs, this is because the effect of extracting interactive behavioral features or multimodal features does not keep going up as the number of GNNs layers continue to be stacked. Some researchers ascribe this phenomenon to the over-smoothing problem [10, 11, 12, 13], where node representations gradually become similar with the increase in layers. But Wu et al. [14] argue that as the number of GNNs layers increases, excessive feature correlations emerge, leading to performance degradation. This is manifested in the increasing redundancy among node representation features, which adversely affects the quality of learned embeddings. Existing models typically preprocess multimodal information through pretraining and then directly map embeddings to a smaller dimensionality [15, 16, 17]. However, directly mapping modality information from a high dimensional space to a low dimensional space will retain unnecessary noise information with greater probability. The introduction of modality noise information can directly affect the quality of the embedding and ultimately the expression of user preferences."
https://arxiv.org/html/2411.00813v1,Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation,"Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains. We open our datasets and code at https://github.com/Anne6645/personality_analysis","Personality analysis has long been a central topic in psychological science and has gained increasing importance in recent years due to its wide-ranging applications. It plays a crucial role in various domains such as personalized recommendation systems [1, 2], sentiment analysis [3, 4], and human-computer interaction. Accurately identifying an individual’s personality can enable tailored experiences and services, enhancing user satisfaction and engagement. However, personality traits are inherently latent characteristics that are not directly observable, making the assessment of personality a challenging task. Traditionally, psychologists have employed structured methods to evaluate an individual’s personality. One of the most widely accepted models is the Big Five Personality Traits (as shown in Figure 1), which assesses personality across five key dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism. To determine an individual’s position on these dimensions, conventional approaches often rely on well-designed questionnaires and psychological inventories that analyze self-reported responses. While these methods are grounded in rigorous psychometric principles, they have notable limitations. Self-reported data can be influenced by social desirability bias, where respondents tailor their answers to be viewed favorably. Additionally, administering and processing these surveys can be time-consuming and resource-intensive, making them less practical for large-scale or real-time applications. Figure 1: Big Five Personality Traits With the advent of online video social platforms like TikTok111https://www.tiktok.com/ and others, there is a growing opportunity to analyze personality traits through digital means. Users increasingly share selfie videos online, providing a wealth of data that captures not only their visual appearance but also their speech patterns, facial expressions, and environmental context. Compared to static questionnaires, these multi-modal data offer richer insights into an individual’s intrinsic traits. Unlike traditional social media platforms that primarily feature text or images, video platforms enable the observation of dynamic behaviors and interactions, which are crucial for understanding personality. This shift opens up new possibilities for applications such as online job interviews, remote education, and personalized content delivery, where assessing personality from videos can significantly enhance outcomes. Recent research has begun to explore the potential of analyzing personality traits through online media instead of traditional surveys. Behavioral observations from personal photographs [5, 6, 7] and short videos [8] have been utilized to glean personality insights. For instance, [5] analyzed Facebook profile pictures to infer personality traits, while [6] and [7] leveraged social media images for similar purposes. However, photograph-based approaches have limitations, as individuals often curate their online images, sharing selective moments that may not accurately represent their typical behaviors or personality, leading to biased data and potentially inaccurate predictions. In contrast, short videos provide a more comprehensive medium for personality analysis. They capture changes in facial expressions, body movements, speech patterns, and contextual scenes—all of which are significant indicators in psychological assessments of personality. Recognizing this, researchers have started to model the audio, visual, and textual features present in short videos [9, 10, 11]. For example, [9] developed a Deep Bimodal Regression model combining audio and visual modalities to predict scores on the Big Five personality traits. Similarly, [10] employed convolutional neural networks to extract visual features and linear regression for audio features, while [11] conducted an in-depth analysis using logistic regression on audio, video, and text features. Figure 2: Comparison of Existing Works Despite these advances, existing multi-modal personality prediction methods often rely on large volumes of high-quality short videos with high-resolution visuals and clear audio to achieve satisfactory performance. Moreover, many approaches depend heavily on supervised learning techniques that require extensive labeled datasets. Collecting and annotating such multi-modal data is both expensive and time-consuming. Manual annotation introduces the potential for subjectivity and inconsistency, which can affect the reliability of the analysis. Consequently, detecting personality traits from online video platforms presents significant challenges, particularly in the following areas: The First Challenge is identifying the most important features from multiple modalities to optimize the use of a limited number of short videos for accurate personality analysis. The complexity arises from the need to effectively integrate diverse data types—visual cues, auditory signals, textual content, and contextual information—each contributing uniquely to personality inference. Existing methods may not adequately address the alignment and synchronization of these modalities, leading to fragmented or incomplete representations that hinder predictive accuracy. The Second Challenge is effectively utilizing a small number of high-quality short videos to achieve strong generalization in personality analysis. Models trained on specific datasets may struggle to generalize across different domains due to variations in cultural contexts, linguistic expressions, and recording conditions. The scarcity of labeled data in new or underrepresented domains exacerbates this issue, limiting the applicability of the models in real-world scenarios where data diversity is the norm. In this paper, we propose an effective multi-modal personality analysis framework designed to overcome these challenges. To address the first challenge, we introduce a semantic unit method for feature extraction and alignment, which synchronizes multi-modal data based on spoken words. This ensures that features from different modalities correspond accurately at each moment in the video, facilitating effective integration. Within this module, we employ self-attention mechanisms to discern the significance of features across various modalities. By assigning weights to features based on their relevance to personality prediction, the model focuses on the most informative aspects of the data, enhancing analytical accuracy. To tackle the second challenge, we propose a multi-domain adaptation method that transfers domain knowledge across multiple domains to alleviate the data sparsity problem. This approach leverages information from data-rich source domains to enhance learning in data-scarce target domains. By computing gradient similarities between source and target domains, our model adapts to emphasize learning from source domains that are most relevant to the target domain. This method improves the model’s generalization capabilities, enabling more accurate predictions even when limited data is available in certain domains. Our main contributions are summarized as follows: • We propose an effective multi-modal personality analysis framework that effectively integrates facial expressions, audio signals, textual content, and background information from short videos for personality prediction. • We introduce a semantic unit modality alignment mechanism that synchronizes multi-modal data based on spoken word timestamps, ensuring accurate correspondence across modalities and enhancing feature representation. • We develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to target domains with limited labeled data, enhancing model generalization and performance in few-shot learning scenarios. • We validate the effectiveness of our proposed framework through extensive experiments on real-world datasets, demonstrating significant improvements over existing methods in personality prediction tasks. By addressing both the feature integration and domain adaptation challenges, our framework advances in personality analysis from online short videos. The rest of the paper is organized as follows. Section II introduces the related work of personality analysis and domain adaption. The problem definition is in Section III. Section IV elucidates the detailed methodology, and Section V presents the results of the experiments and the analysis. The last section is the conclusion of the paper."
https://arxiv.org/html/2411.02334v1,Diffusion-based Generative Multicasting with Intent-aware Semantic Decomposition,"Generative diffusion models (GDMs) have recently shown great success in synthesizing multimedia signals with high perceptual quality enabling highly efficient semantic communications in future wireless networks. In this paper, we develop an intent-aware generative semantic multicasting framework utilizing pre-trained diffusion models. In the proposed framework, the transmitter decomposes the source signal to multiple semantic classes based on the multi-user intent, i.e. each user is assumed to be interested in details of only a subset of the semantic classes. The transmitter then sends to each user only its intended classes, and multicasts a highly compressed semantic map to all users over shared wireless resources that allows them to locally synthesize the other classes, i.e. non-intended classes, utilizing pre-trained diffusion models. The signal retrieved at each user is thereby partially reconstructed and partially synthesized utilizing the received semantic map. This improves utilization of the wireless resources, with better preserving privacy of the non-intended classes. We design a communication/computation-aware scheme for per-class adaptation of the communication parameters, such as the transmission power and compression rate to minimize the total latency of retrieving signals at multiple receivers, tailored to the prevailing channel conditions as well as the users’ reconstruction/synthesis distortion/perception requirements. The simulation results demonstrate significantly reduced per-user latency compared with non-generative and intent-unaware multicasting benchmarks while maintaining high perceptual quality of the signals retrieved at the users. For a typical setup of multicasting street scene images to 10 users, our proposed framework achieves a 15.4%percent15.415.4\%15.4 % reduction in per-user latency at a fixed power budget, or equivalently 50%percent5050\%50 % reduction in the transmission power required to achieve a fixed per-user latency, compared with non-generative multicasting.","Semantic Communication (Semcom) is envisioned to play a crucial role in future wireless networks, specifically in emerging applications where communication of large multimodal signals with stringent latency and reliability constraints is required, e.g. the wireless metaverse and digital twins [1, 2, 3], extended/mixed reality (XR/MR) [4, 5], and holographic teleportation and the internet of senses [6, 7]. In many such applications, multicasting of multimodal signals is required, where the wireless and network resource utilization can be significantly improved by transmitting the same desired content to multiple users over shared resources. For example in the wireless metaverse, multicasting is used to communicate updates, states, or events of the virtual world to multiple users simultaneously in real-time to ensure all users remain synchronized. Another example is XR/MR streaming, where the same content may be distributed to multiple users simultaneously. In these applications, Semcom can further reduce the wireless and network resource utilization by extracting and communicating only the semantic content of interest based on the users’ intent [8, 9, 10, 11, 12]. Recently, the Generative AI (GenAI) models have shown great success in developing efficient low bitrate Generative Semantic Communication (Gen Semcom) systems [13, 14, 15, 16]. Generative models are capable of learning the general distribution of natural signals during training and synthesizing new samples with high perceptual quality at inference time. The generation process can be guided by multi-modal prompts and conditioning signals to produce high quality outputs with a desired semantic content. The emerging Generative Foundation Models and Multimodal Large Language Models (MLLMs), e.g. Sora [17], Lumiere [18], and DALL.E [19], etc., provide ample opportunities to develop efficient and universal generative Semcom frameworks [20, 21, 22, 23]. As these models possess a vast general knowledge captured via intensive pre-training on huge amount of data, they alleviate the need for a shared Knowledge Base (KB) between the semantic transmitter and receiver, thereby reducing the corresponding knowledge sharing overheads. This vast general knowledge also makes the generative Semcom framework applicable to various datasets and tasks thereby achieving universality. Moreover, generative Semcom offers an inherent advantage in privacy preservation, as only the intended features of the original signal are communicated, and the non-intended details are generated locally from random noise inputs at the receiver utilizing the generative model. Such details are thereby not precisely reproducible and are kept private from the receiver, without requiring separate anonymization techniques The GenAI models are trained to maximize the perceptual quality of the synthesized signal, and the theoretical limits of generative Semcom are governed by the rate-distortion-perception theory [24, 25, 26, 27]. This theory explores the threefold trade-off between rate, distortion, and the perceptual quality of the synthesized signal. While the rate-distortion-perception function is analytically derived only for a few source distributions, it is generally estimated empirically for natural signals, e.g. images, audio/video, point cloud, etc., by training deep source encoders with a perceptual loss function, e.g. Wasserstein distance, on large datasets [24, 25]. In generative Semcom, the transmitter extracts the intended semantics, e.g. in form of textual prompts [28, 29], compressed embeddings [30, 31], semantic/edge map [15, 21, 32] etc., which are then transmitted over the channel. The receiver uses these semantics to guide a generative model, synthesizing a signal that is semantically consistent and highly realistic. Despite the above existing works, multiuser generative semantic communications, specifically in the multicasting setup, and its scalability to increasing number of users under latency constraints remains less studied. In this paper, we develop a generative semantic multicasting framework with adaptive intent-aware resource allocation utilizing pre-trained diffusion models. We claim two key benefits for the proposed framework: Firstly, generative multicasting in our Semcom framework allows preserving privacy of the non-intended signals, while improving utilization of the wireless and network resources compared with non-generative and intent-unaware multicasting benchmarks. This also reduces the total latency, while maintaining the reconstruction/synthesis distortion/perception quality. Secondly, the adoption of pre-trained diffusion models allows a source-channel separation-based Semcom architecture, thereby alleviating the need for end-to-end joint training of the transmitter and receiver, which is required in many existing Semcom frameworks [8, 9, 10, 11]. Such a separation-based architecture offers improved adaptability to varying channel conditions, improved scalability to increasing number of users, and better compatibility with the existing design of wireless communication networks. The contributions of this work are three-fold: • We develop a generative multicasting framework where a semantic decomposition scheme splits the source signal into multiple semantic classes at the transmitter based on the multi-user intent, i.e. each user is assumed to need only a subset of the semantic classes based on its communication intent. The transmitter sends to each user only its intended classes over orthogonal wireless resources, and multicasts to all users a highly compressed semantic map over shared resources. Each user then locally synthesizes the non-intended classes using a pre-trained generative diffusion model. • For a typical image synthesis task, we train a generative diffusion model with classifier-free semantic guidance to derive estimates of the corresponding rate-distortion/perception curves achieved by the model through pre-training. We use these curves to determine the communication parameters, i.e. the transmission power and compression rate in our proposed framework. • We design a communication/computation-aware scheme for per-class adaptation of the communication parameters to minimize the total latency for varying channel conditions and users’ reconstruction/synthesis distortion/perception requirements. The signal retrieved at each user is partially reconstructed and partially synthesized utilizing the pre-trained diffusion model, and the communication parameters are optimized for latency-aware synchronisation of the synthesized and reconstructed datastreams based on the prevailing channel conditions and multi-user intent. The rest of this paper is organized as follows. In Section II we provide the related works. In Section III, we present our proposed diffusion-based generative multicasting framework and the corresponding intent and latency-aware adaptive semantic communication scheme. Simulation results are provided in Section IV, and we conclude the paper in Section V. Notations: Boldface lower and upper-case symbols denote column vectors and matrices, respectively. Calligraphic letters denote mathematical operators. [K]delimited-[]𝐾[K][ italic_K ] is {1,2,…,K}12…𝐾\{1,2,...,K\}{ 1 , 2 , … , italic_K }, and |𝐯|𝐯\left|{\bf v}\right|| bold_v | denotes the length of vector 𝐯𝐯{\bf v}bold_v. Additionally, ⊕direct-sum\oplus⊕ and ⊙direct-product\odot⊙ denote point/pixel/voxel-wise summation and product, and ⋃\bigcup⋃ is the union operator. Expectation is denoted by 𝔼[.]\mathbb{E}[.]blackboard_E [ . ]. Finally, 𝒰[.,.]\mathcal{U}[.,.]caligraphic_U [ . , . ] and 𝒩[.,.]\mathcal{N}[.,.]caligraphic_N [ . , . ] denote the uniform and Gaussian probability distributions, respectively."
https://arxiv.org/html/2411.02236v1,3D Audio-Visual Segmentation,"Recognizing the sounding objects in scenes is a longstanding objective in embodied AI, with diverse applications in robotics and AR/VR/MR. To that end, Audio-Visual Segmentation (AVS), taking as condition an audio signal to identify the masks of the target sounding objects in an input image with synchronous camera and microphone sensors, has been recently advanced. However, this paradigm is still insufficient for real-world operation, as the mapping from 2D images to 3D scenes is missing. To address this fundamental limitation, we introduce a novel research problem, 3D Audio-Visual Segmentation, extending the existing AVS to the 3D output space. This problem poses more challenges due to variations in camera extrinsics, audio scattering, occlusions, and diverse acoustics across sounding object categories. To facilitate this research, we create the very first simulation based benchmark, 3DAVS-S34-O7, providing photorealistic 3D scene environments with grounded spatial audio under single-instance and multi-instance settings, across 34 scenes and 7 object categories. This is made possible by re-purposing the Habitat simulator [32] to generate comprehensive annotations of sounding object locations and corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet, characterized by integrating the ready-to-use knowledge from pretrained 2D audio-visual foundation models synergistically with 3D visual scene representation through spatial audio-aware mask alignment and refinement. Extensive experiments demonstrate that EchoSegnet can effectively segment sounding objects in 3D space on our new benchmark, representing a significant advancement in the field of embodied AI. Project page: https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/","Human perception of the real world, both visual and acoustic, predominantly occurs in three dimensions. Prior psychology literature [37] has highlighted humans’ remarkable ability to correspond across multiple modalities, often involving the association of events across these modalities. For instance, we can effortlessly ground emergent surround sound with its potential source in 3D visuals [26]. Inspired by this capability, a crucial aspect in the development of embodied AI systems is their ability to integrate cues from synchronous multimodal input streams and establish targets corresponding to their goals. In this work, we aim to build a machine model to achieve this multimodal correspondence, particularly targeted towards the task of audio-visual segmentation (AVS) in 3D. Albeit AVS has been widely explored within audio-visual scene analysis and correspondence learning, prominent research in this field has focused on 2D environments involving mono (single channel) sound sources, thus devoid of spatial presence entirely. In this paper, we take the first step towards exploring 3D AVS and introduce a large benchmark, 3DAVS-S34-O7. Our exploration is rooted in a fundamental grounding problem: given an embodied agent equipped with a camera and a binaural microphone, can we teach the agent to obtain fine-grained localization of potential sounding objects (generally by predicting a segment-level mask of the object in 3D) while also utilizing spatial audio cues? (see Fig. 1) Furthermore, we extend our benchmark to include a more competitive multi-instance setup where, although multiple instances of the same object might be present in the scene, the goal is to segment only the sounding instance. This setup helps us testify to the efficacy of spatial presence harnessed from the input binaural audio samples. Recently, 3D Gaussian Splatting (3D-GS) [15] has emerged as a prospective method for modeling static 3D scenes directly from input RGB frames. Owing to its explicit Gaussian based representation, it has paved a natural pathway for 3D visual segmentation [13, 41, 33]. Deriving inspiration from human spatial memory in indoor environments, we design EchoSegnet, a purely training-free pipeline for 3D AVS within a 3D-GS representation. EchoSegnet leverages 2D foundation models (namely SAM [17] and ImageBind [9]) to first obtain 2D AVS masks on the input RGB frames. These 2D AVS masks are further used to segment the Gaussians in the learned 3D-GS representation to obtain multi-view masks to achieve a consistent 3D segmentation. Figure 1: Comparison of the existing 2D AVS task with our proposed 3D AVS. Former task utilizes single channel audio to generate pixel-level masks of the potential sounding object in the input RGB frame. 3D AVS on the other hand is aimed at generating 3D masks (from which multi-view consistent 2D masks can be rendered) while utilizing multichannel (spatial) audio. To summarize, we make the following contributions: (1) the first 3D audio-visual segmentation benchmark composing of fairly complex indoor room scenes with integrated spatial sound cues; (2) a training-free AVS framework, EchoSegnet, capable of syncing across sequential frames from 3D environments; (3) a novel Audio-Informed Spatial Refinement Module AISRM, designed to enhance 3D segmentation and resolve ambiguities in complex, multi-instance environments by leveraging spatial audio intensity maps. We perform a comprehensive evaluation of EchoSegnet on the proposed 3DAVS-S34-O7 for both single-instance and multi-instance scenarios, along with an ablative comparison with existing 2D AVS models, highlighting their shortcomings in aligning audio-visual cues within 3D scenes -establishing their adaptation to 3DAVS-S34-O7 as non-trivial."
https://arxiv.org/html/2411.01805v1,MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence,"Motion-to-music and music-to-motion have been studied separately, each attracting substantial research interest within their respective domains. The interaction between human motion and music is a reflection of advanced human intelligence, and establishing a unified relationship between them is particularly important. However, to date, there has been no work that considers them jointly to explore the modality alignment within. To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation. Firstly, to mitigate the huge computational costs raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent representations for both motion and music inputs. Subsequently, leveraging the aligned latent spaces, we introduce a multi-modal Transformer-based diffusion model and a cross-guidance sampling strategy to enable various generation tasks, including cross-modal, multi-modal, and variable-length generation. Extensive experiments demonstrate that MoMu-Diffusion surpasses recent state-of-the-art methods both qualitatively and quantitatively, and can synthesize realistic, diverse, long-term, and beat-matched music or motion sequences. The generated samples and codes are available at https://momu-diffusion.github.io/.","Dancing to the musical beats or creating a variety of rhythmically synchronized music for a given motion is a fundamental aspect of human creativity. Music and human motions serve as universal languages that are shared by all civilizations, transcending cultural and geographical boundaries around the world [25]. For computational methodologies, the motion-music generation poses several challenges: 1) maintaining long-term coherence in typically lengthy motion-music sequences 2) ensuring temporal synchronization and rhythmic alignment between motion and music sequences, and 3) generating realistic, diverse, and variable-length human motions or music. Existing works usually divide the motion-music generation into two distinct tasks: motion-to-music and music-to-motion. For motion-to-music, some methods compress the conditional video frames into a single image, in which the temporal information is lost [52, 53]. The state-of-the-art work, LORIS [49], employs a hierarchical conditional diffusion model to generate long-term musical waveforms. However, LORIS introduces huge computational costs and training difficulties since it generates long-term musical waveforms directly. For music-to-motion, the Dancing2Music (D2M) [26] framework divides the generation process into two stages: decomposing the dance into basic dancing movements with a VAE and compositing the basic movements into dance with a GAN. Nonetheless, D2M’s approach of segmenting long-term music into short clips (approximately 1-2 seconds) diminishes the coherence of the synthesized motion sequences. Table 1: Comparison with the state-of-the-art audio-visual generation works, including but not limited to motion-music generation. Method Pub. Joint Generation Pretrain Long-Term Synthesis Latent Space Diff-Foley NeurIPS’23 ✗ ✓ ✗ ✓ MM-Diffusion CVPR’23 ✓ ✗ ✗ ✗ LORIS ICML’23 ✗ ✗ ✓ ✗ D2M NeurIPS’19 ✗ ✓ ✗ ✓ CDCD ICLR’23 ✗ ✗ ✓ ✓ MoMu-Diffusion ✓ ✓ ✓ ✓ Motivated by the fact that human motions are highly associated with music yet existing computational methods often study them in isolation, we propose a novel multi-modal framework, termed MoMu-Diffusion, to address the aforementioned challenges jointly. Firstly, to mitigate the computational costs and optimization complexities raised by long sequences, we employ a VAE to encode both motion and music sequences into latent spaces. Subsequently, to investigate the relationship between human movements and musical beats, we propose rhythmic contrastive learning. This approach involves constructing contrast pairs with a kinematic amplitude indicator, which quantifies the temporal variation in motion and is derived from the spatial motion directrogram differences as detailed in [4]. Given that the motion and music sequences are interactively aligned in the latent space to discern the correlation between kinematic shifts and musical rhythmic beats, we call our model as the Bidirectional Contrastive Rhythmic VAE (BiCoR-VAE). With the aligned latent space, we introduce a Transformer-based diffusion model that captures long-term dependencies and facilitates sequence generation across variable lengths. Additionally, we introduce a simple cross-guidance sampling strategy that integrates different cross-modal generation models, enabling multi-modal joint generation without extra training. By incorporating the BiCoR-VAE and the diffusion Transformer model, our MoMu-Diffusion framework effectively models the long-term motion-music synchronization and correspondence, enabling motion-to-music, music-to-motion, and joint motion-music generation. Moreover, MoMu-Diffusion supports generating motion-music samples in variable lengths. The pipeline of MoMu-Diffusion is illustrated in Figure 1. We have conducted extensive experiments on three motion-to-music and two music-to-motion datasets, including scenarios such as dancing and competitive sports. The experimental results demonstrate that MoMu-Diffusion attains state-of-the-art performance across both objective and subjective metrics, significantly enhancing music/motion quality and cross-modal rhythmic/kinematic alignment. Furthermore, we have carried out abundant ablation studies to validate the efficacy of the BiCoR-VAE and the DiT architecture. A comparative analysis with state-of-the-art motion-to-music methods CDCD [53] and LORIS [49], 2D music-to-motion method D2M [26], and general video-to-audio methods Diff-Foley [33] and MM-Diffusion [41], is presented in Table 1."
https://arxiv.org/html/2411.00831v1,Saliency-Based diversity and fairness Metric and FaceKeepOriginalAugment: A Novel Approach for Enhancing Fairness and Diversity,"Data augmentation has become a pivotal tool in enhancing the performance of computer vision tasks, with the KeepOriginalAugment method emerging as a standout technique for its intelligent incorporation of salient regions within less prominent areas, enabling augmentation in both regions. Despite its success in image classification, its potential in addressing biases remains unexplored. In this study, we introduce an extension of the KeepOriginalAugment method, termed FaceKeepOriginalAugment, which explores various debiasing aspects—geographical, gender, and stereotypical biases—in computer vision models. By maintaining a delicate balance between data diversity and information preservation, our approach empowers models to exploit both diverse salient and non-salient regions, thereby fostering increased diversity and debiasing effects. We investigate multiple strategies for determining the placement of the salient region and swapping perspectives to decide which part undergoes augmentation. Leveraging the Image Similarity Score (ISS), we quantify dataset diversity across a range of datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset. We evaluate the effectiveness of FaceKeepOriginalAugment in mitigating gender bias across CEO, Engineer, Nurse, and School Teacher datasets, utilizing the Image-Image Association Score (IIAS) in convolutional neural networks (CNNs) and vision transformers (ViTs). Our findings shows the efficacy of FaceKeepOriginalAugment in promoting fairness and inclusivity within computer vision models, demonstrated by reduced gender bias and enhanced overall fairness. Additionally, we introduce a novel metric, Saliency-Based Diversity and Fairness Metric, which quantifies both diversity and fairness while handling data imbalance across various datasets.","Deep learning has shown remarkable success across various domains, such as image processing [25, 40, 39, 37, 1, 19], audio analysis [34, 23, 6, 47, 46, 24], and numerous other fields [5, 2, 36, 15, 16, 20, 49, 43, 42, 22, 35, 18]. But bias has been in each domain. Computer vision models and their applications often exhibit various social biases, including gender bias [4, 3], geographical bias [29, 32], and racial bias [4, 13]. For instance, facial recognition systems tend to be less accurate for individuals with darker skin tones and for females [4]. When deploying a model trained on a dataset that doesn’t represent the demographics of the patient population, biases can skew results. For instance, if a model trained on mammograms of white patients is used on non-white patients with higher breast density, it may show decreased sensitivity, leading to more missed or delayed diagnoses and potentially worse outcomes for non-White patients [45]. The root cause of these biases often lies in the datasets used to train the models, which propagate biases when deployed in real-time applications. Dataset compilation methods typically involve gathering images from the internet, leading to the creation of biased datasets [14]. The auditing of visual datasets for faces has primarily focused on race and gender [29, 4, 13], highlighting the importance of addressing biases in facial regions. Figure 1: Overall architecture of the proposed approach Figure 2: Where to place the salient region? Figure 3: Which part should be augmented? To mitigate these biases, several methods have been proposed. Zhang et al. [50] investigated debiasing in image classification tasks by generating adversarial examples to balance training data distribution. Kim et al. [17] introduced Biaswap, a method that debiases deep neural networks without prior knowledge of bias types, utilizing unsupervised sorting and style transfer techniques. Lee et al. [28] proposed DiverseBias, a feature-level data augmentation technique for improving the debiasing of image classification models by synthesizing diverse bias-conflicting samples through disentangled representation learning. Other related approaches including SalfMix [8], KeepAugment [12], and Randaugment [9], have also tackled feature fidelity challenges. However, it’s worth noting that while these methods enhance data diversity, they may introduce noise and compromise feature diversity, thereby affecting overall model performance. For example, SalfMix may induce overfitting by repetitively incorporating the salient part of the image [8], and KeepAugment might introduce a domain shift between salient and non-salient regions, impeding contextual information [12]. In this work, we present a pioneering approach named FaceKeepOriginalAugment, detect the salient region and place in any non-salient region as shown in Fig. 1, aimed at tackling biases in computer vision. We delve into various strategies for determining the optimal placement of the salient region and meticulously assess the efficacy of our approach in mitigating biases across diverse datasets and occupational categories. In this endeavor, we address the following pivotal questions by extending our previous work [21]: • To what extent does FaceKeepOriginalAugment enhance dataset diversity, as measured by the Image Similarity Score (ISS), across various datasets like Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), and UTK? • To what extend does FaceKeepOriginalAugment contribute to mitigate gender bias in CNNs and ViTs, thereby promoting fairness in computer vision models across various datasets? • Can we measure diversity and fairness in dataset while dealing data imbalance ? The remainder of our work is organized as follows: Section 2 discusses the relevant literature, Section 3 describes our methodology, Section 4 outlines the experimental setup and results, and finally, Section 5 concludes our work."
https://arxiv.org/html/2411.00304v1,Unified Generative and Discriminative Training for Multi-modal Large Language Models,"In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms. Generative training has enabled Multimodal Large Language Models (MLLMs) to tackle various complex tasks, yet issues such as hallucinations and weak object discrimination persist. Discriminative training, exemplified by models like CLIP, excels in zero-shot image-text classification and retrieval, yet struggles with complex scenarios requiring fine-grained semantic differentiation. This paper addresses these challenges by proposing a unified approach that integrates the strengths of both paradigms. Considering interleaved image-text sequences as the general format of input samples, we introduce a structure-induced training strategy that imposes semantic relationships between input samples and the MLLM’s hidden state. This approach enhances the MLLM’s ability to capture global semantics and distinguish fine-grained semantics. By leveraging dynamic sequence alignment within the Dynamic Time Warping framework and integrating a novel kernel for fine-grained semantic differentiation, our method effectively balances generative and discriminative tasks. Extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results in multiple generative tasks, especially those requiring cognitive and discrimination abilities. Additionally, our method surpasses discriminative benchmarks in interleaved and fine-grained retrieval tasks. By employing a retrieval-augmented generation strategy, our approach further enhances performance in some generative tasks within one model, offering a promising direction for future research in vision-language modeling. The project repository is here.","In recent times, Vision-Language Models (VLMs) have been trained under two predominant paradigms: generative training and discriminative training. Generative Training has achieved remarkable success in enabling Multimodal Large Language Models (MLLMs) [1, 55, 86] to develop a wide range of powerful capabilities that can handle various complex tasks (e.g., open-world visual question-answering, image caption generation, etc.) within a single model. However, challenges such as hallucinations and weak image object discrimination abilities [7, 89] persist. Discriminative Training, exemplified by CLIP [73], exhibits remarkable representation capabilities for zero-shot image-text classification and retrieval. Nonetheless, it encounters difficulties in processing complex scenarios (i.e., , retrieving multi-modal documents with interleaved images and texts) [53, 54] and exhibits a limited ability to discern detailed semantic differences [79, 85]. The disparity between these two paradigms has sparked recent studies aimed at imparting discriminative ability to generative pre-trained MLLMs. However, certain aspects of performance still pose limitations (e.g., singular discriminative tasks [89], weak discriminative task performance [40], weak generalization [59], etc.), while others entail compromising the model’s original generative capabilities [8]. Overall, the reason generative paradigms struggle with performing discriminative tasks like retrieval is due to overlooking two crucial abilities: (i) Comprehensively capturing the global semantics. Recent studies have revealed that causal LLMs tend to exhibit a bias towards capturing global information from the input samples, often resulting in a tendency to overlook information located in the middle, especially for long sequences [15, 57]. As illustrated in Figure 1(a), we chose 500 samples from WebQA [10], where the task is to find and reason about the right image-text pair among five distractors to produce a yes or no answer. We conducted experiments using VILA [52], a MLLM with state-of-the-art interleaved image-text comprehension ability, alongside our model. When placing the relevant pair in different positions, the performance of MLLMs followed a ’U’ shape, indicating a bias in capturing global semantic information. Consequently, MLLMs encounter difficulties in forming comprehensive representations that encompass global semantics for retrieval tasks. (ii) Keenly differentiating the detailed semantics. Some research [47, 82] has found that the existing generative training framework cannot fully distinguish input semantics in certain contexts, causing MLLMs to struggle with tasks requiring fine-grained semantics [46, 98]. As depicted in Figure 1(b), we noticed that MLLMs face challenges in choosing the right description for two similar images in the MMVP-VLM benchmark [81]. This indicates that MLLMs struggle to effectively differentiate the detailed semantics of input samples, naturally leading to difficulties in forming effective queries for retrieval. Figure 1: (a) In WebQA [10], the accuracy roughly forms a “U” shape curve when the relevant image-text pair for a question appears at different positions. While our model also shows similar trends, it tends to be more stable overall. (b) The accuracy of various types of questions in MMVP-VLM [81], it can be observed that our model’s performance improves on such tasks after introducing the discriminative training. Details can be seen in Appendix E.3 In this paper, we argue that the current separated paradigms possess the potential for achieving synergistic gains. We propose Sugar: Structure-induced approach to unify generative and discriminative paradigms (shown in Figure 2), leveraging discriminative training to acquire the two abilities above while harnessing the potential of generative training in complex discriminative tasks like image-text interleaved retrieval and fine-grained retrieval. Specifically, we explicitly impose the semantic relationships between different input samples as an induced structural constraint on the hidden state of MLLMs. We consider the interleaved image-text sequence as the general format of input samples, and then formulate the relationship between any two samples as a dynamic sequence alignment problem within the Dynamic Time Warping framework [67, 33]. In this way, we can explicitly modulate the hidden states of the MLLM by leveraging the semantic relationships between interleaved input sequences, thereby encouraging the MLLM to fully capture the global semantics of the inputs. To further enhance the ability to differentiate fine-grained semantics, we integrate a novel kernel into the Dynamic Time Warping framework. Leveraging the strengths of various discriminative pre-trained models, it performs dynamic sequence alignment for diverse embeddings tailored to specific contexts, thus addressing the inherent limitations in fully utilizing input semantics. Through this explicit structure-induced constraint, our framework enables MLLMs to capture the global semantics and fine-grained details of the input multimodal sequence more effectively, thus bridging the gap between generative and discriminative training paradigms. Figure 2: Our structure-induced generative and discriminative training joint training strategy. Our method effectively balances both discriminative and generative tasks, demonstrating synergistic benefits. (i) Large-scale generative pre-trained models possess semantic-rich hidden states [41, 91, 23], which facilitate discriminative tasks like retrieval. Moreover, harnessing the capabilities of MLLM is crucial for complex discriminative tasks, such as interleaved image-text retrieval and fine-grained retrieval. (ii) By integrating discriminative tasks, the model’s effectiveness in generative tasks, particularly within tasks requiring cognitive and discrimination abilities, is enhanced, thereby mitigating certain occurrences of hallucinations. (iii) We can employ Sugar to realize retrieval-augmented generation [2], eliminating the need for an off-the-shelf retrieval module [75], thereby amplifying the performance of various generative tasks. The usage of off-the-shelf retrieval presents a challenge wherein the retriever’s performance affects the generator’s final output [62]. This necessitates independent optimization of both components, posing a dilemma in selecting optimal configurations. However, our approach circumvents such optimization challenges. Through extensive experimentation, we have demonstrated the effectiveness of our approach. For generative tasks, Sugar establishes new state-of-the-art results on the tasks for complicated multimodal comprehension tasks (i.e., DEMON [47]), fine-grained semantic distinctions (i.e., VizWiz [28], MME [95]), object hallucinations detection (i.e., POPE [51]) (Section 4.2 and Section 4.3). For discriminative tasks, we achieved competitive results in image-text retrieval compared, and significantly surpassed CLIP in interleaved retrieval and fine-grained retrieval (Section 4.4). Furthermore, employing the retrieval-augmented generation (RAG) strategy led to further improvements in a series of generative tasks (Section 4.5)."
