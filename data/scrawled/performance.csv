URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.02797v1,"DeepContext: A Context-aware, Cross-platform, and Cross-framework Tool for Performance Profiling and Analysis of Deep Learning Workloads","Effective performance profiling and analysis are essential for optimizing training and inference of deep learning models, especially given the growing complexity of heterogeneous computing environments. However, existing tools often lack the capability to provide comprehensive program context information and performance optimization insights for sophisticated interactions between CPUs and GPUs. This paper introduces DeepContext, a novel profiler that links program contexts across high-level Python code, deep learning frameworks, underlying libraries written in C/C++, as well as device code executed on GPUs. DeepContext incorporates measurements of both coarse- and fine-grained performance metrics for major deep learning frameworks, such as PyTorch and JAX, and is compatible with GPUs from both Nvidia and AMD, as well as various CPU architectures, including x86 and ARM. In addition, DeepContext integrates a novel GUI that allows users to quickly identify hotpots and an innovative automated performance analyzer that suggests users with potential optimizations based on performance metrics and program context. Through detailed use cases, we demonstrate how DeepContext can help users identify and analyze performance issues to enable quick and effective optimization of deep learning workloads. We believe DeepContext is a valuable tool for users seeking to optimize complex deep learning workflows across multiple compute environments.","The rapid advancement of deep learning has led to increasingly complex models (achiam2023gpt, ; dubey2024llama, ; peebles2023scalable, ) deployed across diverse and heterogeneous computing environments. Optimizing the training and inference of these models is critical for improving performance and reducing computational costs (hoffmann2022training, ; kaplan2020scaling, ). However, the sophisticated interactions between CPUs and GPUs, coupled with the diversity of frameworks (jax2018github, ; paszke2019pytorch, ) and compilation modes (ansel2024pytorch, ), pose significant challenges for developers seeking to identify and address performance bottlenecks effectively. To improve the efficiency of deep learning workloads by fully utilizing hardware resources, effective performance profiling tools are essential. These tools include framework-specific solutions, such as the PyTorch profiler (pytorch_profiler, ) and the JAX profiler (jax_profiler, ), as well as those provided by hardware vendors, like Nsight Systems (nsight_systems, ), Roctracer (roctracer, ), and VTune (vtune, ). The primary functionality of these tools is tracing, which captures metrics associated with individual CPU and GPU operations and displays them on a comprehensive timeline to assist users in investigating performance bottlenecks. Table 1. Comparison of DeepContext (our tool) with existing profiling tools. Profiling Tools Python Context Framework Context C++ Context Device Context Cross GPUs Cross Frameworks CPU Profiling Nsight Systems (nsight_systems, ) ✓bold-✓\boldsymbol{\checkmark}bold_✓ ×\boldsymbol{\times}bold_× ✓bold-✓\boldsymbol{\checkmark}bold_✓ ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ RocTracer (roctracer, ) ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× JAX profiler (jax_profiler, ) ✓bold-✓\boldsymbol{\checkmark}bold_✓ ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ✓bold-✓\boldsymbol{\checkmark}bold_✓ ×\boldsymbol{\times}bold_× ✓bold-✓\boldsymbol{\checkmark}bold_✓ PyTorch profiler (pytorch_profiler, ) ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ ×\boldsymbol{\times}bold_× ×\boldsymbol{\times}bold_× ✓bold-✓\boldsymbol{\checkmark}bold_✓ ×\boldsymbol{\times}bold_× ✓bold-✓\boldsymbol{\checkmark}bold_✓ DeepContext ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ ✓bold-✓\boldsymbol{\checkmark}bold_✓ (a) The hot call path w/o framework context (b) The hot call path w/ framework context Figure 1. Comparison of highlighted call paths w/ and w/o framework context. The thicker the color of a frame, the more time has been spent on that frame. The design of existing tools presents several challenges that hinder thorough performance analysis and optimization. We have summarized their features and limitations in Table 1. First, they often fail to provide a comprehensive view of performance metrics across the entire software stack, which spans from high-level Python code and frameworks down to low-level C++ operations and GPU instructions. At best, Nsight Systems can correlate Python with C++ code, but it cannot provide information about deep learning operators and GPU instructions. Figure 1 illustrates this limitation. In Figure 1(a), only C++ code is visible in the call path, and it is unclear where the convolution function was called without framework information. This is because the backward and forward operators are launched from different CPU threads in PyTorch. Such a limitation obscures the pinpointing of performance problems related to specific convolution operations, especially when a complex model may invoke hundreds of convolution operations (he2016deep, ). In contrast, Figure 1(b) reveals the Python call path and associated deep learning operators, providing deeper insights for performance optimization. Second, most existing performance tools trace and record every operation, causing huge profiles when profiling long-running training workloads. Identifying issues within traces that contain millions of operations often requires a great deal of manual efforts. Moreover, the voluminous profile data can consume significant memory resources, potentially exhausting DRAM capacity or making it impossible for visualization. Existing tools can only aggregate metrics (e.g., time) postmortem for individual kernels, but they fail to streamline metrics aggregation online to reduce profile size and do not differentiate between instances called from different contexts for automated problem identification. Lastly, existing tools lack portability across platforms and frameworks. Vendor-provided tools, such as Nvidia’s Nsight suite, can profile events from JAX and PyTorch on Nvidia GPUs, but they are incompatible with workloads running on AMD GPUs. On the other hand, framework-specific tools like the PyTorch profiler can support both AMD and Nvidia GPUs but cannot handle workloads written in JAX. This fragmentation in tool availability increases the learning curve for users and limits their ability to cross-reference findings across different frameworks and GPUs, making it difficult to determine which setup best suits their workloads. In response to these challenges, we introduce DeepContext, a novel profiler that delivers comprehensive performance insights for deep learning workloads. Unlike existing profilers that lack essential context, DeepContext captures all critical program context information relevant to deep learning workloads. As shown in Table 1, DeepContext enables the identification of performance issues in multiple layers of the software stack, spanning high-level Python code, deep learning frameworks, underlying libraries written in C/C++, and device code executed on GPUs. Additionally, DeepContext supports performance profiling for major deep learning frameworks including PyTorch and JAX, AMD and Nvidia GPUs, as well as x86 and ARM CPUs. Finally, DeepContext enables the collection, attribution, and aggregation of both coarse and fine-grained performance metrics, allowing for detailed investigation of performance bottlenecks with minimal profile data, even for long-running applications. This paper presents the design, implementation, and evaluation of DeepContext and makes the following research contributions: • We introduce a “shim” layer—DLMonitor—that converts deep learning framework-specific data into a framework-agnostic format, enabling seamless integration of framework information with third-party performance tools. • We design an automated performance analyzer that provides actionable optimization suggestions based on performance metrics and program contexts, such as fusing operators, changing data layouts, or modifying hardware configurations. • We describe a novel graphical user interface (GUI) that visualizes performance data in a compact, navigable, and hierarchical format, allowing users to quickly identify performance bottlenecks and apply optimizations based on performance analysis results. We evaluated DeepContext using a diverse range of deep learning workloads across various platforms and frameworks, demonstrating that DeepContext significantly saves memory and disk space usage with similar runtime overhead compared to the state-of-the-art performance tools. Through use case studies, we show that DeepContext can effectively identify performance issues and suggest insightful code changes, enabling straightforward optimization of deep learning models. Even users with limited experiences in deep learning frameworks or CPU and GPU architectures can achieve speedups ranging from 1.06×\times× to 1.66×\times×."
https://arxiv.org/html/2411.01336v1,Distributed Tracing for Cascading Changes of Objects in the Kubernetes Control Plane,"Kubernetes is a container orchestration system that employs a declarative configuration management approach. In Kubernetes, each desired and actual state is represented by an “object”, and multiple controllers autonomously monitor related objects and update their objects towards the desired state in the control plane. Because of this design, changes to one object propagate to other objects in a chain. The cluster operators need to know the time required for these cascading changes to complete, as it directly affects the quality of service of applications running on the cluster. However, there is no practical way to observe this kind of cascading change, including breakdown of the time taken by each change. Distributed tracing techniques are commonly used in the microservices architecture to monitor application performance, but they are not directly applicable to the control plane of Kubernetes; the microservices architecture relies on explicitly calling APIs on other services, but in Kubernetes the controllers just monitor objects to know when to start processing, and never call functions on other controllers directly. In this paper, we propose a system that automatically traces changes to objects in the control plane. Our method adds one identifier, a Change Propagation ID (CPID), to the metadata of an object, and the controller that observes an object change propagates its CPID to the objects that the controller is updated. When multiple changes need to be merged on an object, a new CPID is generated, and the relationship between the original CPID and the new CPID is sent to the external trace server. We confirmed that change propagation can be visualized and the required time measured. We also showed that this system’s overhead is not significant.","A declarative configuration management system is a system that defines and inputs a desired state and automatically updates configurations to to maintain the actual state matches with the desired state. This architecture is widely used in controllers for large scale systems, and Kubernetes (The Kubernetes Authors, 2014)(Verma et al., 2015) is a de facto container (Bernstein, 2014) management system that employs this approach. In Kubernetes, operators configure resources and their objects by defining only the desired state. An “object” in Kubernetes represents the desired and current state of each cluster function, and a resource is a set of objects of the same function. In the control plane of Kubernetes, many components called controllers controllers observe the current state of the objects that each controller is in charge of, and constantly perform a process (reconciliation loop) to get them into the desired state. This mechanism allows the controller to detect when the current state changes due to failures, updates by the operators, or other reasons, and the controller tries to maintain the state of the system in the desired state autonomously. Since some of these controllers control their resource objects by observing the state of objects in other resources, changes occur in a chain among these resource controllers and objects until status of all related objects are updated. One of the key metrics in a Kubernetes cluster is the time required to update all related objects when the desired state of one object is changed. This time directly affects the quality of service of applications running on the cluster. For example, when requests to an application is suddenly increased, the operators will start more container replicas by updating the number of replicas to handle the requests, but the application cannot handle all the requests until all related objects are updated and additional replicas are launched. In order to identify where bottlenecks are, we need a mechanism to make cascading changes among related resources visible to operators. However, there are two challenges for observing these cascading changes. First, because the updates of each object is processed autonomously in each controller in the control plane, it is not clear to which controller or object a change is propagated next. The operator does not specify the sequence of updating each object, but rather, once an object is changed, the various objects are updated autonomously to reflect the change of the object. Therefore, it is impossible to know (or define) when the change process for all related objects has been completed. In addition, the objects to be affected by a change of a specific object are different depending on the system, such as the use of custom resources and plugins installed by the operators. Second, it is difficult to know when a change in one object will cause a change in other objects. Since each controller is not invoked directly from other controllers but autonomously monitors the current object status and processes it to the desired status, the timing to start the update process of the objects depends on each controller; that is, the updates may be processed one by one just after other objects are updated, or several updates may be processed in batch. K-Bench (VMware, 2020) and ClusterLoader (Kubernetes, 2018) are tools to measure the time to handle some predefined types of changes, but they do not have a mechanism for tracing cascading changes between objects in general. Conventional methods that require manual logging by the operators (Ehira et al., 2023) can trace all resources, but they are costly because they require a deep knowledge of the controller design. The Kubernetes community has discussed extending the control plane’s functionality to allow to measure cascading changes (Naser, 2020)(Li, 2021). However, these are still in the discussion because of a problem in handling tracing information to be placed on the object. Distributed tracing techniques (Sigelman et al., 2010)(Fonseca et al., 2007)(Chen et al., 2002)(Gschwind et al., 2002) are commonly used in the microservices architecture to monitor application performance, but they are not directly applicable to the control plane of Kubernetes; the microservices architecture is based on RPCs (Thurlow, 2009) and clearly identify the start and the completion of the requests, but in Kubernetes the controllers just monitor objects to know when to start processing, and never call functions on other controllers directly. We propose a distributed tracing method in the Kubernetes control plane to facilitate observation of change propagation. We add change tracing identifiers to object metadata called Change Propagation ID (CPID). CPID is assigned to objects when processing objects in each controller, and CPID in the objects is propagated to other objects that the controller updates. CPIDs are newly assigned when multiple changes are merged on an object, that is, the object is updated according to multiple objects having different CPIDs. All logs related to the handling of CPIDs are sent to the external trace server, and the trace server analyses the logs to show the current status of the cascading changes. Our method incorporates the change propagation logic at the time of implementation of each controller, and the cluster operator can understand relationships between resources and controllers’ behavior better to trace cascading changes of objects. We confirmed that our system can easily trace change propagation and that the performance impact of the system on clusters is not significant. The contributions of this study are as follows. • We summarize the challenges and requirements for tracing cascading changes specific to the Kubernetes control plane. • This is the first distributed tracing system applied to the Kubernetes control plane, by the idea of combining updating CPIDs in the objects and analysing logs of how CPIDs are updated. • We implement the proposed system and show that changes can be traced in the control plane and that the overhead is acceptable."
https://arxiv.org/html/2411.01246v1,"CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores††thanks:Sandy Irani and Jenny Lam are with the University of California, Irvine. Their research is supported in part by the NSF grant CCF-0916181. Shahram Ghandeharizadeh and Jason Yap are with the University of Southern California.111A shorter version of CAMP appeared in the Proceedings of the ACM/IFIP/USENIX
Middleware Conference, Bordeaux, France, December 2014. Seehttps://github.com/scdblab/CAMPfor an implementation.","Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a general purpose key-value store (KVS) that manages key-value pairs computed by applications with different access patterns, key-value sizes, and varying costs for each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm that can be implemented as efficiently as LRU. In particular, CAMP’s eviction policies are as effective as those of GDS but require only a small fraction of the updates to an internal data structure in order to make those decisions. Similar to an implementation of LRU using queues, it adapts to changing workload patterns based on the history of requests for different key-value pairs. It is superior to LRU because it considers both the size and cost of key-value pairs to maximize the utility of the available memory across competing applications. We compare CAMP with both LRU and an alternative that requires human intervention to partition memory into pools and assign grouping of key-value pairs to different pools. The results demonstrate CAMP is as fast as LRU while outperforming both LRU and the pooled alternative. We also present results from an implementation of CAMP using Twitter’s version of memcached.","Applications with a high read-to-write ratio augment their persistent infrastructure with an in-memory key-value store (KVS) to enhance performance. An example is memcached in use by popular Internet destinations such as Facebook, Twitter, and Wikipedia. Using a general purpose caching layer requires workloads to share infrastructure despite different access patterns, key-value sizes, and time required to compute a key-value pair [21]. An algorithm that considers only one factor may cause different application workloads to impact one another negatively, decreasing the overall effectiveness of the caching layer. As an example, consider two different applications of a social networking site: one shows the profile of members while a second determines the displayed advertisements. There may exist millions of key-value pairs corresponding to different member profiles, each computed using a simple database look-up that executed in a few milliseconds. The second application may consist of thousands of key-value pairs computed using a machine-learning algorithm that processed Terabytes of data and required hours of execution. This processing time is one definition of the cost of a key-value pair. With a limited memory size and a high frequency of access for member profile key-value pairs, a simple algorithm that manages memory using recency of references (LRU) may evict most of the key-value pairs of the second application, increasing the incurred cost. In general, reducing the incurred cost translates into a faster system that processes a larger number of requests per unit of time and may provide a better quality of service. The latter is due to availability of data (e.g., cache hit for a key-value computed using the machine learning algorithm) that enables the application to provide a user with more relevant content than content selected randomly. A possible approach is for a human expert to partition the available memory into disjoint pools with each pool managed using LRU. Next, the expert groups key-value pairs with similar costs together and assigns each group to a different pool [18]. With our example, the expert would construct two pools. One for the key-value pairs corresponding to members profiles and a second corresponding to advertisements. The primary limitation222Partitioning is known to reduce the utilization of resources by resulting in formation of hot spots and bottlenecks. One may address this limitation by over-provisioning resources. of this approach is that it requires a human familiar with the different classes of applications to identify the pools, construct grouping of key-value pairs, and assign each group to a pool. Over time, the service provider may either introduce a new application or discontinue an existing one. This means the human expert must again become involved to identify the pool for the key-value pairs of the new application and possibly rebalance memory across the pools once an application is discontinued. This paper introduces a novel caching method called Cost Adaptive Multi-queue eviction Policy (CAMP), that manages the available memory without partitioning it. CAMP is an approximation of the Greedy Dual Size (GDS) algorithm [4] that processes cache hits and misses more efficiently using queues. Hence, it is significantly faster than GDS and as fast as LRU. It is novel and different from LRU in that it constructs multiple LRU queues dynamically based on the size and cost of key-value pairs. The number of constructed LRU queues depends on the distribution of costs and sizes of the key-value pairs. CAMP manages these LRU queues without partitioning memory. Thus, there is no need for human involvement to construct groups of key-value pairs, dictate assignment of groups to the pools, or configure and adjust the memory pool characteristics. CAMP is robust enough to prevent an aged expensive key-value pair from occupying memory indefinitely. Such a key-value pair is evicted by CAMP as competing applications issue more requests. CAMP is parameterized by a variable that controls its precision. At the highest precision, CAMP’s eviction decisions are essentially equivalent to those made by GDS. Our empirical results show that CAMP does not suffer any degradation in the quality of its eviction decisions at lower precisions. Moreover, it is able to make those decisions much more efficiently than GDS. GDS requires an internal priority queue to determine a key-value pair to evict from the cache. The time to maintain its data structures consistent in a thread-safe manner is expensive because it requires synchronization primitives [12] with multiple threads performing caching decisions. Moreover, CAMP performs a significantly fewer updates of its internal data structures than GDS, reducing the number of times it executes the thread-safe software dramatically. The rest of this paper is organized as follows. Section 2 starts with a description of GDS to motivate CAMP and details its design decisions. Section 3 presents a simulation study of CAMP and compares it with LRU and the pooled approach that partitions resources, demonstrating its superiority. Section 4 describes an implementation of CAMP using a variant of Twemcache and compares this implementation with the original that uses LRU. Obtained results demonstrate that CAMP is as fast as LRU and provides superior performance as it considers, in addition to recency of requests, the size and the cost of the key-value pairs. Section 5 describes related work. Section 6 provides brief words of conclusions and future research directions."
https://arxiv.org/html/2411.00601v2,Diversity in Network-Friendly Recommendations,"In recent years, the Internet has been dominated by content-rich platforms, employing recommendation systems to provide users with more appealing content (e.g., videos in YouTube, movies in Netflix). While traditional content recommendations are oblivious to network conditions, the paradigm of Network-Friendly Recommendations (NFR) has recently emerged, favoring content that improves network performance (e.g. cached near the user), while still being appealing to the user. However, NFR algorithms sometimes achieve their goal by shrinking the pool of content recommended to users. The undesirable side-effect is reduced content diversity, a phenomenon known as “content/filter bubble”,[1, 2]. This reduced diversity is problematic for both users, who are prevented from exploring a broader range of content, and content creators (e.g. YouTubers) whose content may be recommended less frequently, leading to perceived unfairness. In this paper, we first investigate - using real data and state-of-the-art NFR schemes - the extent of this phenomenon. We then formulate a “Diverse-NFR” optimization problem (i.e., network-friendly recommendations with - sufficient - content diversity), and through a series of transformation steps, we manage to reduce it to a linear program that can be solved fast and optimally. Our findings show that Diverse-NFR can achieve high network gains (comparable to non-diverse NFR) while maintaining diversity constraints. To our best knowledge, this is the first work that incorporates diversity issues into network-friendly recommendation algorithms.","I INTRODUCTION Background. The majority of content services (video/movie platforms, social networks, e-commerce, etc.) integrate recommendation systems (RS) that significantly influence users’ choices among the vast array of available content. These systems have been demonstrated to shape content demand, with 50% of YouTube’s views[3], 80% of Netflix’s views[4], and 35% of Amazon’s revenue[5], coming from their RS. However, this content is delivered through a complex network system, where some items are distant from the user and incur high costs to fetch, while other items may be closer and require minimal expense. A prevalent example of this is Content Delivery Networks (CDNs),[6], where an item might be either available on a nearby CDN server (thus incurring lower network costs) or it must be retrieved from the origin/deeper servers (which can be substantially more “costly” for the network, the user, and/or the content provider). Unfortunately, standard recommendation systems, referred to hereafter as “Baseline RS (BSR)”, do not consider this network cost and recommend content solely based on user-centric metrics (e.g. personalization and relevance,[2]) or business metrics (e.g. revenue,[1]). To address this issue, the paradigm of Network-Friendly Recommendations (NFR) has emerged as a solution to enhance the cost-efficiency of content delivery,[7, 8, 9, 10, 11, 12]. The primary concept behind NFR is to steer recommendations towards content that can be delivered in a “network-friendly” manner (e.g. cached in the mobile edge,[7, 8]), thereby influencing user demand in favor of such content. The problem: Diversity in NFR. Favoring network-friendly (e.g. cached) content involves modifying the baseline recommendations of the platform, which could be perceived as quality of experience degradation for one ore more of the parties involved: (i) users, might find the modified recommendations as less relevant/useful than the original ones111This is actually not always the case; a less relevant recommendation might be preferred if it offers higher streaming quality[12].; (ii) content providers (and their recommendation algorithm team) might perceive such modifications as too intrusive, considerably obfuscating the original goal of the platform RS; (iii) user and content creators might protest if only a small subset of items is consistently presented to the pool of users (e.g., mostly cached content to increase cache hit rate). Concern (i) captures the recommendation relevance degradation for a single user, and countermeasures have been included in most NFR schemes since their inception,[7, 12]. Concern (ii) relates to a global degradation of recommendations to all users, and has been first considered in the recent work of[13], where the authors attempt to obtain NFR gains while minimizing the intrusiveness of the algorithm (e.g. by putting a hard constraint on the distance between the BSR and the NFR recommendation vectors). Concern (iii), on the other hand, has neither been explicitly measured yet nor addressed. It is not as immediately clear (as (i) and (ii)) that an NFR algorithm does indeed create such “content bubbles”. Our conjecture is that, to reduce network cost, an NFR algorithm will replace some recommendation probability mass from items outside the cache, with items inside the cache, at every opportunity where sufficiently relevant alternatives (e.g. satisfying (i)) exist. By doing this for most users and most items, the majority of the recommendation probability mass will become concentrated around a much smaller pool of content, signaling decreased diversity in the RS. Of course, excessive diversity is not the goal, since it can conflict with the accuracy that RSs are designed to achieve. However, diversity levels/constraints should be adjustable based on the preferences of the users/content creators—a feature not yet incorporated in NFR schemes. To this end, our goal in this paper is to explicitly address the issue of diversity in NFR for both user and content creator satisfaction222In fact, diverse recommendations can also help address business and societal issues by promoting a broader range of content.. While one might argue that metrics targeting concerns (i) or (ii) also resolve the issue of diversity, this is not the case. Consider a simplified example with three users and their relevance scores for three items -each by a different creator: user A: {1,0.8,0.2}, user B: {0.8,1,0.9}, and user C: {0.3,0.9,1}. In the standard BSR, item 1 would be recommended to user A, item 2 to user B, and item 3 to user C, maximizing their satisfaction. However, if only item 2 is cached, the NFR would recommend item 2 to all three users to increase cache hit rate. Although relevance scores (0.8, 1, and 0.9 for users A, B, and C) remain high, diversity is reduced threefold as the RS presents only one item instead of three, leading to dissatisfaction among content producers of items 1 and 3. Hence, the metrics addressing concern (i) do not suffice to fix the problem. In a different scenario, after viewing item 1, the BSR suggests items {7,4,3} to users 1-3, while the NFR provides cached items {2,6,5}. Though both systems are 100% diverse (i.e., 3 different items across 3 recommendations), none of the NFR items align with the BSR, rendering the NFR algorithm “too intrusive”. Thus, concerns (ii) and (iii) are also clearly distinct. Contributions. Our contributions in this paper include: • Diversity characterization. We propose the use of entropy as a measure to quantify diversity in recommendations (Section II), and use it to analyze the outcome of various NFR algorithms on real datasets (Section III). • Optimal Diverse-NFR. We formulate the problem of Diverse-NFR, transform it first to an (equivalent) convex optimization problem (thus guaranteeing that it can be solved optimally), then further transform it to a linear program to further facilitate solution speed (Section IV). • Diversity and Network-Friendliness. We prove that high network gains can be achieved while maintaining diversity levels comparable to BSR, e.g. reducing the original BSR cost by 10×10\times10 × while only reducing the original BSR diversity by 40% (Section V). • Diversity and Fairness. We prove that (i) compared to Fair-NFR[13], our algorithm achieves superior cost-diversity trade-offs by explicitly prioritizing diversity, and (ii) it is possible to address distinct aspects of fairness alongside “diversity fairness” by integrating other fairness metrics (as[13]) in our Diverse-NFR (Section VI)."
https://arxiv.org/html/2411.00288v1,Inducing Semi-Structured Sparsity by Masking for Efficient Model Inference in Convolutional Networks,"The crucial role of convolutional models, both as standalone vision models and backbones in foundation models, necessitates effective acceleration techniques. This paper proposes a novel method to learn semi-structured sparsity patterns for convolution kernels in the form of maskings enabling the utilization of readily available hardware accelerations. The approach accelerates convolutional models more than two-fold during inference without decreasing model performance. At the same time, the original model weights and structure remain unchanged keeping the model thus easily updatable. Beyond the immediate practical use, the effect of maskings on prediction is easily quantifiable. Therefore, guarantees on model predictions under maskings are derived showing stability bounds for learned maskings even after updating the original underlying model.111Code available at github.com/ddanhofer/Semi-Structured-Sparsity-CNNs","The increasing complexity of deep learning models [21], their deployment in applications [5], and the adoption of reflection incurring several inference passes per query, e.g., as in the O1 models from the GPT family [3], shifts the relative amounts of resources spent during the model lifetime from the training to the inference stage [7, 35]. It therefore becomes imperative to make models more efficient [46]. One way of achieving this is by spending a comparatively small, additional share of resources during training to learn a one-time modification of the model that lowers the model’s inference and thus lifetime cost [30, 40]. First and foremost, such a modification is effective if it decreases the model’s computational and time cost at a relatively low additional training overhead while not affecting the prediction performance of the model negatively [22]. Additionally, there are other desirable properties of such one-time modifications: From an application perspective the achievable gain in efficiency is only useful if it can be leveraged easily, a well-known challenge, e.g., with sparsifying models [8, 15]. Taking into consideration the increasing popularity of large, expensive to train, foundation models [16] or models employed in an online setting subject to continuous updates the proposed change should not affect the possibility to update the model, e.g., by changing the weights or architecture underlying the model. Ideally, if such a model is updated, the learned modification can even be reused under the constraint of the magnitude of change imposed by updating the model. Semi-structured sparse maskings satisfy the above properties by replacing the dense matrix operations usually required during inference by cheaper and faster operations on semi-structured sparse matrices [4]. While many works have demonstrated that sparse (pruned) submodels can solve the same task at almost no loss of performance [2, 26] the sparsity of the models does not necessarily have to adhere to a specific pattern making it difficult to turn theoretically obtained computational speedups by saving on data loading and computational operations into practical efficiency gains [14]. Regular patterns are more “machine-friendly” inducing the desired efficiency a priori but limiting the choices for the sparse patterns, which thus need to be chosen carefully with the goal of minimizing the loss of inference performance in mind. This paper proposes a novel method of learning regularly sparse masking patterns for convolutions, key building blocks for state-of-the art Computer Vision (CV) models [25] and foundation models building on CV models as their backbone [38]. The proposed method • shows how to effectively use readily available hardware accelerations for semi-structured sparse matrices in convolution kernels to accelerate inference, • outperforms available heuristics for semi-structured sparsity showing that semi-structured sparsity masks can be learned with a fraction of the original training resources while incurring a negligible performance loss in CV classification tasks, • provides the additional advantage of not changing the original set of trained weights keeping models updatable and rendering the method especially attractive for use in large models, e.g., foundation models and in online settings, • induces an easily quantifiable change to the model’s prediction behavior and thus lends itself to settings where hard guarantees on model predictions are of interest. In the following section the adoption of semi-structured sparsity and sparsity in convolutional models are addressed. Section 3 of the paper covers modeling semi-structured sparsity in general, in convolutional models, and the theoretical implications of such model alterations in inference. The results of empirically testing the method on widely used convolutional architectures are presented in Section 4 followed up by a discussion of the method presented and a conclusion."
