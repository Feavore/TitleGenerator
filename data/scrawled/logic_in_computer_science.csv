URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04615v1,The Functional Machine Calculus III: ChoiceEarly Announcement,"The Functional Machine Calculus (Heijltjes 2022) is an extension of the lambda-calculus that preserves confluent reduction and typed termination, while enabling both call-by-name and call-by-value reduction behaviour and encoding the computational effects of mutable higher-order store, input/output, and probabilistic computation. In this note the calculus is extended to capture exception handling and loop constructs.","The Functional Machine Calculus (FMC) [25, 5] is a new approach to combining the λ𝜆\lambdaitalic_λ-calculus—as the foundation of functional programming—with computational effects. It takes a view of the λ𝜆\lambdaitalic_λ-calculus as an instruction language for an abstract machine with a single stack in the style of Krivine [38], where application is push, abstraction is pop, and variable is execute. To accommodate effects, the calculus introduces the following two extensions [25]. Locations Multiple stacks on the machine, each named by a location, allow the encoding of various effects via push and pop actions: mutable higher-order store, as stacks of depth at most one; input/output, as pop-only respectively push-only streams; and probabilities and non-determinism as probabilistically respectively non-deterministically generated streams. Sequencing The introduction of sequential composition and its unit, imperative skip, gives control over evaluation behaviour away from strict call–by–name, and allows the encoding of Plotkin’s call–by–value λ𝜆\lambdaitalic_λ-calculus [68], Moggi’s computational metalanguage [58], and Levy’s call–by–push–value [43]. Encoding effects into the generalized operators of the calculus, rather than introducing primitives, means that two key properties of the λ𝜆\lambdaitalic_λ-calculus are preserved. Confluence Reduction in the FMC is confluent in the presence of effects. This is a consequence of the separation of operational behaviour, which governs the machine, from local reduction behaviour, which is the interaction of consecutive push and pop actions. Reduction equivalence for state then implements the algebraic laws of Plotkin and Power [66]. Types The FMC can be simply typed, which conveys strong normalization and termination of the machine. This gives a solution to the problem of typing higher-order store: Landin’s Knot [39], which encodes recursion via higher-order store, cannot be typed (in its full generality). This paper introduces a third extension to the FMC, choice, to include a wider range of computational behaviours: constants, conditionals, data constructors, exception handling, and loops. These have in common that, semantically, they are modelled by sums or coproducts: for example, the Booleans are given by the type 1+1111+11 + 1, the error monad is given by the functor T⁢X=E+X𝑇𝑋𝐸𝑋TX=E+Xitalic_T italic_X = italic_E + italic_X for a set of exceptions E𝐸Eitalic_E, and loops are modelled by taking a map in A→A+B→𝐴𝐴𝐵A\to A+Bitalic_A → italic_A + italic_B to one in A→B→𝐴𝐵A\to Bitalic_A → italic_B (looping on A𝐴Aitalic_A, exiting on B𝐵Bitalic_B) [8]. Together, these will be referred to as choice constructs. The aim is sixfold. First and second, to preserve confluence and types: the resulting calculus should support a natural, confluent reduction relation, and a notion of simple types that guarantees termination of the machine and strong normalization of reduction (in the absence of loops). Third, minimality: choice constructs should be captured with as few syntactic operators as possible, avoiding any overlap in functionality and minimizing the interactions or reductions governing the semantics of the calculus. Fourth, operational semantics: the calculus should continue to be an instruction language for a simple and natural abstract machine. Fifth, seamless integration: different effects should combine seamlessly, without requiring lifting operations. Finally, the FMC has a natural first-order restriction, where function arguments are restricted to be (first-order) values, not arbitrary terms. The sixth aim is to preserve this restriction, which ensures that choice constructs are independent of the calculus being first-order or higher-order. The approach has been to reconsider the notion of choice from first principles, with the aim of capturing coproducts and the constructs that they model in a simple and natural way, satisfying the six criteria above. This led to three (mostly) standard syntactic constructions, which however interact with the stack in subtle ways to give new and unexpected reduction behaviours. The resulting calculus is in some ways highly familiar, yet simultaneously in other ways novel and surprising. In contrast with stateful effects, confluence and type safety are expected for exception handling. The main results are to integrate exceptions seamlessly with stateful effects, to support natural operational and denotational semantics, and to capture a wide range of behaviours with an elegant, minimal syntax. This note will discuss the background literature, introduce the selected choice constructs from operational considerations, formally define the calculus and its type system, and demonstrate how it captures existing formulations. For simplicity of exposition, the calculus will omit the locations modification, and feature only sequencing and choice. Proofs are incomplete at the time of writing, and hence omitted, leaving the intended theorems as conjectures."
https://arxiv.org/html/2411.04555v1,An Axiomatic Study of the Evaluation of Enthymeme Decoding inWeighted Structured Argumentation,"An argument can be seen as a pair consisting of a set of premises and a claim supported by them. Arguments used by humans are often enthymemes, i.e., some premises are implicit. To better understand, evaluate, and compare enthymemes, it is essential to decode them, i.e., to find the missing premisses. Many enthymeme decodings are possible. We need to distinguish between reasonable decodings and unreasonable ones. However, there is currently no research in the literature on “How to evaluate decodings?”. To pave the way and achieve this goal, we introduce seven criteria related to decoding, based on different research areas. Then, we introduce the notion of criterion measure, the objective of which is to evaluate a decoding with regard to a certain criterion. Since such measures need to be validated, we introduce several desirable properties for them, called axioms. Another main contribution of the paper is the construction of certain criterion measures that are validated by our axioms. Such measures can be used to identify the best enthymemes decodings.","In the literature on logic-based argumentation, a deductive argument is usually defined as a premise-claim pair where the claim is inferred (according to a logic) from the premises. However, when studying human debates (i.e. real world argumentation), it is common to find incomplete arguments, called enthymemes, for which the premises are insufficient for implying the claim. The reason for this incompleteness is varied, for example it may result from imprecision or error, e.g. a human may argue without knowing all the necessary information, or it may be intentional, e.g. one may presuppose that some information is commonly known and therefore does not need to be stated, or the employment of enthymemes is an instrument well known since Aristotle (Faure 2010) as one of the most effective in rhetoric and persuasion when it comes to interacting with an audience. There are studies in the literature on understanding enthymemes in argumentation, using natural language processing (Habernal et al. 2017; Singh et al. 2022; Wei et al. 2022), but these do not identify logic-based arguments. There are also symbolic approaches for decoding enthymemes in structured argumentation including (Hunter 2007; Dupin de Saint-Cyr 2011; Black and Hunter 2012; Hosseini, Modgil, and Rodrigues 2014; Xydis et al. 2020; Panisson, McBurney, and Bordini 2022; Hunter 2022; Leiva, Gottifredi, and García 2023; Ben-Naim, David, and Hunter 2024), but they only consider the task as identifying a set of formulae that could be added to the incomplete premises in order to entail the claim. This offers potentially many decodings, and there is currently a lack of means for comparing these decoding candidates. In real-world argumentation, it is important to note that decoding is more general than that of completion. In fact, when we decode, we may add and subtract information, to obtain the most appropriate decoding. Furthermore, given that several decodings of an enthymeme can be proposed, we then have the question of how to “how to evaluate the quality of a candidate for decoding an enthymeme” in order to make an optimal choice of decoding. Let us take the following example (which will be part of our running example) to illustrate an enthymeme with two possible decodings. • Enthymeme E𝐸Eitalic_E: Knowing that Bob is wealthy, he is a researcher, he makes people happy, and he has people around him who seem to love him, then Bob is happy. • Decoding D1subscript𝐷1D_{1}italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT: Bob is a researcher and researchers are generally happy, so Bob is happy. • Decoding D2subscript𝐷2D_{2}italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT: Bob makes people happy and is surrounded by people who love him, and because giving and receiving love often makes people happy, Bob is happy. To study whether D1subscript𝐷1D_{1}italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or D2subscript𝐷2D_{2}italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is a better decoding for E𝐸Eitalic_E, we will represent knowledge by weighted logics, then we will propose quality measures based on measuring different aspects of a candidate for decoding (criterion measures). Given that the number of criterion measures for a criterion is infinite, we adopt an axiomatic approach, defining the constraints of a good measure."
https://arxiv.org/html/2411.04003v1,Learning Aggregate Queries Defined byFirst-Order Logic with Counting111This is the extended version of the conference contribution[11].,"In the logical framework introduced by Grohe and Turán (TOCS 2004) for Boolean classification problems, the instances to classify are tuples from a logical structure, and Boolean classifiers are described by parametric models based on logical formulas. This is a specific scenario for supervised passive learning, where classifiers should be learned based on labelled examples. Existing results in this scenario focus on Boolean classification. This paper presents learnability results beyond Boolean classification. We focus on multiclass classification problems where the task is to assign input tuples to arbitrary integers. To represent such integer-valued classifiers, we use aggregate queries specified by an extension of first-order logic with counting terms called FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT.Our main result shows the following: given a database of polylogarithmic degree, within quasi-linear time, we can build an index structure that makes it possible to learn FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-definable integer-valued classifiers in time polylogarithmic in the size of the database and polynomial in the number of training examples.","We study the complexity of learning aggregate queries from examples. This is a classification problem of the following form. The elements that are to be classified come from a set X𝑋Xitalic_X, the instance space. For a given set V𝑉Vitalic_V, a V𝑉Vitalic_V-valued classifier on X𝑋Xitalic_X is a function c:X→V:𝑐→𝑋𝑉c\colon X\to Vitalic_c : italic_X → italic_V. We are given a training set S𝑆Sitalic_S of labelled examples (x,λ)∈X×V𝑥𝜆𝑋𝑉(x,\lambda)\in X\times V( italic_x , italic_λ ) ∈ italic_X × italic_V, i. e., λ𝜆\lambdaitalic_λ is the label assigned to the instance x𝑥xitalic_x. The goal is to find a classifier, called a hypothesis, that can be used to predict the label of elements from X𝑋Xitalic_X, including those not given in S𝑆Sitalic_S. The term Boolean classification problem refers to the case where |V|=2𝑉2\left\lvert V\right\rvert=2| italic_V | = 2 (often, V𝑉Vitalic_V is {1,0}10\{1,0\}{ 1 , 0 }). We use the term multiclass classification problem to refer to cases where V𝑉Vitalic_V may be arbitrarily large. In machine learning, these problems fall into the category of supervised learning tasks: we want to learn a function from given input-output pairs. In contrast to this, in unsupervised learning (e. g. clustering), the goal is to learn patterns from unlabelled data [49]. We focus on learning problems related to the framework introduced by Grohe and Turán [36]. There, the instance space X𝑋Xitalic_X is a set of tuples from a logical structure (that is sometimes called the background structure), and the classifiers are Boolean and are described using parametric models based on logical formulas. In this paper, we extend the framework to multiclass classification problems where the classifiers are integer-valued, i. e., V=ℤ𝑉ℤV=\mathbb{Z}italic_V = blackboard_Z. In the framework that we consider, the background structure is a relational database 𝒜𝒜\mathcal{A}caligraphic_A, and the instance space X𝑋Xitalic_X is the set Aksuperscript𝐴𝑘A^{k}italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT of all k𝑘kitalic_k-tuples of elements from the active domain A𝐴Aitalic_A of 𝒜𝒜\mathcal{A}caligraphic_A (also called the universe of 𝒜𝒜\mathcal{A}caligraphic_A). Here, k𝑘kitalic_k is a fixed positive integer. One fixes a parameter length ℓℓ\ellroman_ℓ (a fixed non-negative integer). A classifier is specified by a pair p=(t,w¯)𝑝𝑡¯𝑤p=(t,\bar{w})italic_p = ( italic_t , over¯ start_ARG italic_w end_ARG ), where w¯=(w1,…,wℓ)¯𝑤subscript𝑤1…subscript𝑤ℓ\bar{w}=(w_{1},\dots,w_{\ell})over¯ start_ARG italic_w end_ARG = ( italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT ) is an ℓℓ\ellroman_ℓ-tuple of elements in A𝐴Aitalic_A, and t𝑡titalic_t is a counting term in the first-order logic with counting FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT [35] with free variables x1,…,xk,y1,…,yℓsubscript𝑥1…subscript𝑥𝑘subscript𝑦1…subscript𝑦ℓx_{1},\dots,x_{k},y_{1},\dots,y_{\ell}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT. This pair p𝑝pitalic_p represents the classifier cp:X→ℤ:subscript𝑐𝑝→𝑋ℤc_{p}\colon X\to\mathbb{Z}italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT : italic_X → blackboard_Z that assigns to each k𝑘kitalic_k-tuple a¯=(a1,…,ak)∈X¯𝑎subscript𝑎1…subscript𝑎𝑘𝑋\bar{a}=(a_{1},\dots,a_{k})\in Xover¯ start_ARG italic_a end_ARG = ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ∈ italic_X the integer i𝑖iitalic_i that is obtained by evaluating the counting term t𝑡titalic_t in the database 𝒜𝒜\mathcal{A}caligraphic_A while interpreting the variables x1,…,xksubscript𝑥1…subscript𝑥𝑘x_{1},\dots,x_{k}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT with the elements a1,…,aksubscript𝑎1…subscript𝑎𝑘a_{1},\dots,a_{k}italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and the variables y1,…,yℓsubscript𝑦1…subscript𝑦ℓy_{1},\dots,y_{\ell}italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT with the “parameters” w1,…,wℓsubscript𝑤1…subscript𝑤ℓw_{1},\dots,w_{\ell}italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT. We will write ht,w¯𝒜subscriptsuperscriptℎ𝒜𝑡¯𝑤h^{\mathcal{A}}_{t,\bar{w}}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , over¯ start_ARG italic_w end_ARG end_POSTSUBSCRIPT to denote this classifier cpsubscript𝑐𝑝c_{p}italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT. Given a training set S⊆Ak×ℤ𝑆superscript𝐴𝑘ℤS\subseteq A^{k}\times\mathbb{Z}italic_S ⊆ italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT × blackboard_Z, we want to find a pair p=(t,w¯)𝑝𝑡¯𝑤p=(t,\bar{w})italic_p = ( italic_t , over¯ start_ARG italic_w end_ARG ) such that the classifier ht,w¯𝒜subscriptsuperscriptℎ𝒜𝑡¯𝑤h^{\mathcal{A}}_{t,\bar{w}}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , over¯ start_ARG italic_w end_ARG end_POSTSUBSCRIPT is consistent with S𝑆Sitalic_S, i. e., it satisfies ht,w¯𝒜⁢(a¯)=isubscriptsuperscriptℎ𝒜𝑡¯𝑤¯𝑎𝑖h^{\mathcal{A}}_{t,\bar{w}}(\bar{a})=iitalic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , over¯ start_ARG italic_w end_ARG end_POSTSUBSCRIPT ( over¯ start_ARG italic_a end_ARG ) = italic_i for every (a¯,i)∈S¯𝑎𝑖𝑆(\bar{a},i)\in S( over¯ start_ARG italic_a end_ARG , italic_i ) ∈ italic_S. Example 1.1. Let 𝒜𝒜\mathcal{A}caligraphic_A be a relational database where the active domain A𝐴Aitalic_A contains authors and publications, the binary relation Author contains all pairs (a,p)𝑎𝑝(a,p)( italic_a , italic_p ) where a𝑎aitalic_a is an author of the publication p𝑝pitalic_p, and the binary relation Citation contains all pairs (p1,p2)subscript𝑝1subscript𝑝2(p_{1},p_{2})( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) where the publication p1subscript𝑝1p_{1}italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT cites the publication p2subscript𝑝2p_{2}italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Suppose we are given a training set S𝑆Sitalic_S that consists of a few pairs (a,i)𝑎𝑖(a,i)( italic_a , italic_i ) where a𝑎aitalic_a is an author and i𝑖iitalic_i is the total number of citations of the publications of a𝑎aitalic_a. A reasonable classifier for this setting would be a mapping c:A→ℤ:𝑐→𝐴ℤc\colon A\to\mathbb{Z}italic_c : italic_A → blackboard_Z that assigns to every author a𝑎aitalic_a present in the database the total number i𝑖iitalic_i of citations of their publications. In our setting, this can be represented as follows. We let k=1𝑘1k=1italic_k = 1 and ℓ=0ℓ0\ell=0roman_ℓ = 0. Since ℓ=0ℓ0\ell=0roman_ℓ = 0, the “parameter” w𝑤witalic_w is fixed to be the empty tuple ()()( ). Since k=1𝑘1k=1italic_k = 1, we use a counting term with a single free variable x𝑥xitalic_x (that will be assigned with authors present in the database). We choose the counting term t(x)≔#(z1,z2).(Author(x,z1)∧Citation(z2,z1)).t(x)\ \coloneqq\ \ \#{(z_{1},z_{2})}.{\bigl{(}\texttt{Author}(x,z_{1})\land% \texttt{Citation}(z_{2},z_{1})\bigr{)}}.italic_t ( italic_x ) ≔ # ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) . ( Author ( italic_x , italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ∧ Citation ( italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) . Evaluating t⁢(x)𝑡𝑥t(x)italic_t ( italic_x ) for an author x𝑥xitalic_x yields the number of tuples (z1,z2)subscript𝑧1subscript𝑧2(z_{1},z_{2})( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) such that x𝑥xitalic_x is an author of publication z1subscript𝑧1z_{1}italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, and z2subscript𝑧2z_{2}italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT is a publication that cites z1subscript𝑧1z_{1}italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. This is precisely the total number of citations of publications authored by x𝑥xitalic_x. Hence, ht,()𝒜subscriptsuperscriptℎ𝒜𝑡h^{\mathcal{A}}_{t,()}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , ( ) end_POSTSUBSCRIPT is the desired classifier c𝑐citalic_c. Example 1.2. Suppose we have a database that maintains a list of all cakes colleagues brought to work. We model this as a relational database 𝒜𝒜\mathcal{A}caligraphic_A whose active domain A𝐴Aitalic_A contains persons, IDs of cakes, and types of cake. The binary relation Brought contains all pairs (p,c)𝑝𝑐(p,c)( italic_p , italic_c ) where p𝑝pitalic_p is a person that brought the cake with ID c𝑐citalic_c, and the binary relation Type contains all pairs (c,τ)𝑐𝜏(c,\tau)( italic_c , italic_τ ) where c𝑐citalic_c is the ID of a cake of type τ𝜏\tauitalic_τ (e. g., “chocolate cake”, “strawberry cake”, “carrot cake”, etc). Suppose we want to find a classifier that predicts the popularity of colleagues. For this, via a survey, we gather examples (p,i)∈A×ℤ𝑝𝑖𝐴ℤ(p,i)\in A\times\mathbb{Z}( italic_p , italic_i ) ∈ italic_A × blackboard_Z where p𝑝pitalic_p is a person and i𝑖iitalic_i is the popularity of the person, and we call the resulting set of labelled examples S𝑆Sitalic_S. We choose k=ℓ=1𝑘ℓ1k=\ell=1italic_k = roman_ℓ = 1, so we want to find a classifier that uses a single parameter. According to our own experience at work, it seems conceivable that the following classifier ht,w𝒜subscriptsuperscriptℎ𝒜𝑡𝑤h^{\mathcal{A}}_{t,w}italic_h start_POSTSUPERSCRIPT caligraphic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , italic_w end_POSTSUBSCRIPT is consistent with S𝑆Sitalic_S: the parameter w𝑤witalic_w is “chocolate cake” and t𝑡titalic_t is the counting term t(x,y)≔#(z).(Brought(x,z)∧¬Type(z,y))+ 2⋅#(z).(Brought(x,z)∧Type(z,y)).t(x,y)\ \coloneqq\ \ \#{(z)}.{\bigl{(}\texttt{Brought}(x,z)\land\neg\texttt{% Type}(z,y)\bigr{)}}\ \ +\ \ 2\cdot\#{(z)}.{\bigl{(}\texttt{Brought}(x,z)\land% \texttt{Type}(z,y)\bigr{)}}.italic_t ( italic_x , italic_y ) ≔ # ( italic_z ) . ( Brought ( italic_x , italic_z ) ∧ ¬ Type ( italic_z , italic_y ) ) + 2 ⋅ # ( italic_z ) . ( Brought ( italic_x , italic_z ) ∧ Type ( italic_z , italic_y ) ) . Note that t𝑡titalic_t counts the number of cakes brought by person x𝑥xitalic_x, where cakes of type y𝑦yitalic_y are counted twice, and the variable y𝑦yitalic_y will always be assigned the value of the parameter w𝑤witalic_w. In many application scenarios, the same database is used multiple times with different training sets to learn different classifiers. Thus, we consider a setting in which we are first only given the database, without any training examples. In a precomputation step, we allow gathering information that will be helpful for solving future learning tasks. This precomputation step can be viewed as building an index structure that is designed in order to support solving multiple learning tasks. In the actual learning phase, we are repeatedly given training sets of labelled examples, and our task is to output a hypothesis that is consistent with the corresponding training set. For this learning phase, it would be desirable to have algorithms that run efficiently even if the database is too large to fit into the main memory. To achieve this, we are interested in algorithms that require only local access to the database, i. e., instead of having random access to the database, a learning algorithm should initially start with the elements given in the training set; subsequently, it may only retrieve the neighbours of elements it already holds in memory. By utilising the memory hierarchy, such local access can be achieved efficiently even in cases where random access is too prohibitive. In the context of learning (concerning Boolean classification problems), this local-access model has been introduced by Grohe and Ritzert [34]. Our contribution Our main result is an algorithm that builds the index structure in time linear in the size and polynomial in the degree of the database. Afterwards, upon input of concrete training sets, classifiers definable in FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT can be learned in time polynomial in the degree of the database and polynomial in the number of examples given in the training set. Moreover, the classifiers returned by our algorithm can be evaluated in time polynomial in the degree of the database. Furthermore, our algorithms for finding a classifier and for evaluating this classifier do not require random access to the database but only rely on the local-access model. For databases of polylogarithmic degree (i. e., of degree up to (log⁡n)csuperscript𝑛𝑐(\log n)^{c}( roman_log italic_n ) start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT where c𝑐citalic_c is a constant and n𝑛nitalic_n is the size of the database), our main result implies that the index structure can be built in quasi-linear time (i. e., time n⋅(log⁡n)c⋅𝑛superscript𝑛𝑐n{\cdot}(\log n)^{c}italic_n ⋅ ( roman_log italic_n ) start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT); afterwards, FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-definable integer-valued classifiers can be learned in time polylogarithmic (so, in particular, sublinear) in the size of the database and polynomial in the number of training examples. Previous results in the framework of Grohe and Turán for Boolean classification problems relied on the fact that it suffices to check a constant number of queries while limiting the search space for the parameters to a neighbourhood of a certain radius [34, 7, 10]. For our setting of multiclass classification with aggregate queries, however, this does not hold any more. Hence, a priori, it is not clear that sublinear-time learning algorithms are possible for the multiclass case at all. The main technical challenge towards our learnability result was to find an approach that keeps the number of queries to check small (i. e., polynomial in the degree of the database), while still being able to limit the search space for the parameters to a small neighbourhood around the given training tuples. Organisation We provide the necessary definitions concerning FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in Section 2, and we formally introduce the learning problem that we consider in Section 3. The precise statement of our main result is given in Theorem 3.1. Our proof makes heavy use of the locality properties of the logic FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT shown in [35], including a decomposition of FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-formulas into local formulas. These properties are used in Section 4 to provide our main technical tool for the proof of the main result. Section 5 concludes the paper with a summary and an outlook on future work. In the remainder of this introduction, we give an overview of related work. Related work The first-order logic with counting FOC was introduced in [43] and further studied in [35, 7]. This logic extends first-order logic (FO) by the ability to formulate counting terms that evaluate to integers, and by numerical predicates that allow to compare results of counting terms. It was shown in [43] that the model-checking problem for FOC is fixed-parameter tractable on classes of structures of bounded degree. From [35] it is known that the fixed-parameter tractability of FOC cannot be generalised to even very simple classes of structures of unbounded degree such as unranked trees (under a reasonable assumption in parameterised complexity). However, [35] identified a fragment called FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for which model-checking of formulas and evaluation of counting terms are fixed-parameter tractable on all nowhere dense classes of structures. The present paper uses counting terms of FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to represent integer-valued classifiers. The learning framework we consider has been introduced for Boolean classification problems in [36], which provides information-theoretic learnability results for classes of classifiers that can be specified using FO- and MSO-formulas on restricted classes of structures, such as the class of planar graphs or classes of graphs of bounded degree. Algorithmic aspects of the framework, including the running time of a learning algorithm, were first studied in [34]. The paper showed that Boolean classifiers definable in FO can be learned in sublinear time on structures of polylogarithmic degree. Analogous results have been obtained for MSO on strings [33] and on trees [30], which included a precomputation step to allow for efficient repeated learning. The paper [9] studied the parameterised complexity of the Boolean classification problem and showed that on arbitrary relational structures, learning hypotheses definable in FO is hard for the parameterised complexity class AW⁢[∗]AWdelimited-[]\textup{{AW}}[*]AW [ ∗ ] (i. e., subject to a plausible complexity-theoretic assumption, it is not fixed-parameter tractable). The paper also showed that the problem is fixed-parameter tractable if the structures come from a nowhere dense class. For Boolean classifiers definable in the extension FOCN of FO with counting quantifiers and numerical predicates, [7] obtained a sublinear-time learning algorithm for structures of bounded degree, i. e., classes of structures where the degree is bounded by a constant. Recently, [8] lifted this result to structures of tiny degree, i. e., classes of structures of degree up to (log⁡log⁡n)csuperscript𝑛𝑐(\log\log n)^{c}( roman_log roman_log italic_n ) start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT for some constant c𝑐citalic_c, where n𝑛nitalic_n is the size of the structure. The paper [10] considered a notion of weighted structures, which extend ordinary relational structures by assigning weights, i. e. elements from particular rings or abelian groups, to tuples present in the structure. It introduced the expressive logic FOWA, which extends FO by means of aggregating weights and formulating both formulas (that evaluate to “true” or “false”) and terms (that “aggregate” weights and evaluate to values in the associated ring or abelian group). For the fragment FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT (that still extends FO), the paper showed that Boolean classifiers definable by FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-formulas over weighted background structures of polylogarithmic degree can be learned in sublinear time after quasi-linear-time preprocessing. This lifts the results obtained in [34] for FO to the substantially more expressive logic FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. As the logic FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT can be embedded in FOWA1subscriptFOWA1\textup{{FOWA}}_{1}FOWA start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, it follows from [10] that Boolean classifiers definable by FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-formulas over background structures of polylogarithmic degree can be learned in sublinear time after quasi-linear-time preprocessing. The main result of the present paper can be viewed as a generalisation of this to integer-valued classification problems. The algorithmic results obtained so far within the framework introduced in [36] all focus on Boolean classification problems. However, many application scenarios require multiclass classification (cf. [22, 15, 37]). In the database systems literature, multiclass classifiers typically are described by aggregate queries [52, 53, 54, 55, 46]. In this paper, aggregate queries are represented by the counting terms of FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Closely related to the framework we consider is the framework of inductive logic programming (ILP) [47, 48, 40, 20, 21]. Both frameworks deal with a passive supervised learning setting, where the learning algorithms are given labelled examples. These examples are labelled according to some target concept, and the algorithms should return a hypothesis that approximately matches this target concept. One of the main differences between both frameworks is that we represent the background knowledge by a relational database, whereas in ILP, it is represented in a background theory, i. e., a set of formulas. Related logical learning frameworks have also been studied in formal verification [28, 44, 24, 57, 19]. In the database literature, various approaches to learning queries from examples have been studied, both in passive (such as ours) and active learning settings. In passive learning settings, results often focus on conjunctive queries [38, 39, 6, 41, 5, 56] or consider queries outside the relational database model [51, 12], while we focus on FOC1subscriptFOC1\textup{{FOC}}_{1}FOC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, an extension of full first-order logic. In the active learning setting introduced by Angluin [4], learning algorithms are allowed to actively query an oracle. Results in this setting [2, 50, 1, 12, 13, 16] again consider various types of queries. Another related subject in the database literature is the problem of learning schema mappings from examples [14, 29, 3, 17, 18]."
https://arxiv.org/html/2411.03451v1,Redundancy Is All You Need,"The seminal work of Benczúr and Karger demonstrated cut sparsifiers of near-linear size, with several applications throughout theoretical computer science. Subsequent extensions have yielded sparsifiers for hypergraph cuts and more recently linear codes over Abelian groups. A decade ago, Kogan and Krauthgamer asked about the sparsifiability of arbitrary constraint satisfaction problems (CSPs). For this question, a trivial lower bound is the size of a non-redundant CSP instance, which admits, for each constraint, an assignment satisfying only that constraint (so that no constraint can be dropped by the sparsifier). For instance, for graph cuts, spanning trees are non-redundant instances.Our main result is that redundant clauses are sufficient for sparsification: for any CSP predicate R𝑅Ritalic_R, every unweighted instance of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) has a sparsifier of size at most its non-redundancy (up to polylog factors). For weighted instances, we similarly pin down the sparsifiability to the so-called chain length of the predicate. These results precisely determine the extent to which any CSP can be sparsified. A key technical ingredient in our work is a novel application of the entropy method from Gilmer’s recent breakthrough on the union-closed sets conjecture.As an immediate consequence of our main theorem, a number of results in the non-redundancy literature immediately extend to CSP sparsification. We also contribute new techniques for understanding the non-redundancy of CSP predicates. In particular, we give an explicit family of predicates whose non-redundancy roughly corresponds to the structure of matching vector families in coding theory. By adapting methods from the matching vector codes literature, we are able to construct an explicit predicate whose non-redundancy lies between Ω⁢(n1.5)Ωsuperscript𝑛1.5\Omega(n^{1.5})roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) and O~⁢(n1.6)~𝑂superscript𝑛1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ), the first example with a provably non-integral exponent.","The broad goal in sparsification is to replace an object by a more compact surrogate, typically a carefully chosen subsample, that preserves the behavior of the object under some metric of interest. For instance, for preserving cuts in undirected graphs, the influential works of Karger [Kar93] and Benczúr and Karger [BK96] showed that every graph has an edge-weighted subgraph with near-linear number of edges that preserves the value of all (edge) cuts up to a (1±ϵ)plus-or-minus1italic-ϵ(1\pm\epsilon)( 1 ± italic_ϵ ) multiplicative factor. These papers have had a substantial impact in shaping the last thirty years of work in areas such as spectral sparsifiers [ST11, BSS12, LS18], clustering [KVV04, SPR11], hypergraph sparsifiers [KK15, CKN20, KKTY21, KK23, KPS24c], linear solvers [ST04, Vis13, KMP14], convex optimization [LS14, AK16, Tod16], sketching/streaming algorithms [AG09, AGM12b, AGM12a, ACK+16, McG14, KLM+17, BHM+21], max-flow/min-cut algorithms [LR99, CKM+11, KLOS14, CKL+22], machine learning [LCY+21, CSZ22, ZSW+23, GBY+24], submodular functions [KK23, Sch24, Raf24, Qua24], differential privacy [BBDS12, AU19], PageRank [Chu14], and even theoretical physics [HKTH16, Van18, TN22], among many other works. Among the multiple exciting dimensions in which cut sparsification has been generalized, we now highlight two which form the backdrop for our work. Note that the graph cut problem can be modeled by the arity-two Boolean constraint x+y=1(mod2)𝑥𝑦annotated1pmod2x+y=1\pmod{2}italic_x + italic_y = 1 start_MODIFIER ( roman_mod start_ARG 2 end_ARG ) end_MODIFIER. One can thus generalize cut sparsification by allowing for arbitrary constraints (of any arity over some finite domain) as considered in the field of constraint satisfaction problems (CSPs), leading to CSP sparsification. This direction was proposed by Kogan and Krauthgamer [KK15] in their work on hypergraph cut sparsifiers, where the not-all-equal constraint captures hypergraph cut. As as special case, arbitrary binary CSPs (where each constraint has two variables) were studied in [FK17] for the Boolean domain and in [BŽ20] for general domains, leading to a dichotomy: either near-linear sized sparsifiers exist, or no improvement over quadratic is possible. In another direction, one can instead look toward more general structures to sparsify. For instance, a recent line of work by Khanna, Putterman, and Sudan turned toward sparsifying linear codes [KPS24a], or more generally subgroups of powers of Abelian groups [KPS24b]. Beyond being algorithmically efficient [KPS24b], these structural results have led to exciting new results in CSP sparsification by constructing optimal sparsifiers when the constraints can be embedded into linear/Abelian equations. In this work, we obtain sparsifiers encompassing both these generalizations via a unified approach to sparsification of non-linear codes. The resulting sparsifiers for CSPs have optimal asymptotic size up to polylogarithmic factors, for every choice of predicate defining the CSP. In other words, we pinpoint the optimal extent to which an arbitrary CSP can be sparsified.111In this work we focus on the existence of sparsifiers, which is already highly non-trivial (e.g., [KPS24a, BŽ20] are also non-algorithmic). Future directions (and barriers) for algorithmic aspects are briefly discussed in Sections 1.7 and 9. 1.1 Non-linear code sparsification We first state our result for codes as it is very general and crisply stated, and then turn to the consequences and further new results for CSPs. For a non-linear code C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, an ε𝜀\varepsilonitalic_ε-sparsifier (for a parameter ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 )) is a weight function w:[m]→ℝ≥0:𝑤→delimited-[]𝑚subscriptℝabsent0w:[m]\to\mathbb{R}_{\geq 0}italic_w : [ italic_m ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT such that for every codeword c𝑐citalic_c, adding up the weights of its nonzero positions, i.e., ∑iw⁢(i)⁢cisubscript𝑖𝑤𝑖subscript𝑐𝑖\sum_{i}w(i)c_{i}∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w ( italic_i ) italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, is an accurate estimate of the Hamming weight of c𝑐citalic_c (i.e., ∑icisubscript𝑖subscript𝑐𝑖\sum_{i}c_{i}∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) to within a (1±ε)plus-or-minus1𝜀(1\pm\varepsilon)( 1 ± italic_ε ) multiplicative factor (Definition 2.8). The goal is to minimize the support of w𝑤witalic_w (i.e., the number of nonzero entries w⁢(i)𝑤𝑖w(i)italic_w ( italic_i )), and the minimum value is called ε𝜀\varepsilonitalic_ε-sparsifiability of C𝐶Citalic_C and is denoted SPR⁡(C,ε)SPR𝐶𝜀\operatorname{SPR}(C,\varepsilon)roman_SPR ( italic_C , italic_ε ). One of our main results is an upper bound on the sparsifiability in terms of a natural combinatorial parameter of the code called its non-redundancy NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ), defined as follows: NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) is the size of the largest subset of indices I⊆[m]𝐼delimited-[]𝑚I\subseteq[m]italic_I ⊆ [ italic_m ] such that for each i∈I𝑖𝐼i\in Iitalic_i ∈ italic_I, there is a codeword c∈C𝑐𝐶c\in Citalic_c ∈ italic_C with ci=1subscript𝑐𝑖1c_{i}=1italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 and ci′=0subscript𝑐superscript𝑖′0c_{i^{\prime}}=0italic_c start_POSTSUBSCRIPT italic_i start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = 0 for i′∈I∖{i}superscript𝑖′𝐼𝑖i^{\prime}\in I\setminus\{i\}italic_i start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ italic_I ∖ { italic_i }. In other words, if we imagine the code as a matrix whose rows are codewords, its non-redundancy is largest square submatrix which is a permutation matrix. Our result can then be stated compactly as follows. Theorem 1.1 (Main). For all C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), SPR⁡(C,ε)=O⁢(NRD⁡(C)⁢(log⁡m)6/ε2).SPR𝐶𝜀𝑂NRD𝐶superscript𝑚6superscript𝜀2\operatorname{SPR}(C,\varepsilon)=O(\operatorname{NRD}(C)(\log m)^{6}/% \varepsilon^{2}).roman_SPR ( italic_C , italic_ε ) = italic_O ( roman_NRD ( italic_C ) ( roman_log italic_m ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . To see how our theorem generalizes linear code sparsification [KPS24a, KPS24b], let 𝔽𝔽\mathbb{F}blackboard_F be a (finite) field and let V⊆𝔽m𝑉superscript𝔽𝑚V\subseteq\mathbb{F}^{m}italic_V ⊆ blackboard_F start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT be a subspace. Let C:={(𝟏⁢[v1=0],…,𝟏⁢[vm=0]):v∈V}assign𝐶conditional-set1delimited-[]subscript𝑣10…1delimited-[]subscript𝑣𝑚0𝑣𝑉C:=\{({\bf 1}[v_{1}=0],\ldots,{\bf 1}[v_{m}=0]):v\in V\}italic_C := { ( bold_1 [ italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0 ] , … , bold_1 [ italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = 0 ] ) : italic_v ∈ italic_V } be the zero/non-zero pattern of V𝑉Vitalic_V. Then, any ε𝜀\varepsilonitalic_ε-sparsifier of C𝐶Citalic_C is an ε𝜀\varepsilonitalic_ε-sparsifier of V𝑉Vitalic_V and NRD⁡(C)=dimVNRD𝐶dimension𝑉\operatorname{NRD}(C)=\dim Vroman_NRD ( italic_C ) = roman_dim italic_V. In fact, for any finite group G𝐺Gitalic_G and subgroup H≤Gm𝐻superscript𝐺𝑚H\leq G^{m}italic_H ≤ italic_G start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, we can show for the corresponding C𝐶Citalic_C that NRD⁡(C)≤log2⁡|H|NRD𝐶subscript2𝐻\operatorname{NRD}(C)\leq\log_{2}|H|roman_NRD ( italic_C ) ≤ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_H | (see Theorem 5.1), matching [KPS24b]’s result for Abelian groups (modulo their efficiency). If we view C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT as a set family over the universe [m]delimited-[]𝑚[m][ italic_m ], the above result in effect says that the non-redundancy NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) plays the role of the VC dimension of C𝐶Citalic_C when the goal is to estimate the size of the set c∈C𝑐𝐶c\in Citalic_c ∈ italic_C rather than learn c𝑐citalic_c itself. In fact, it turns out that NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) is precisely the VC dimension of the union-closure of C𝐶Citalic_C. This connection to union-closed families plays a crucial role in the proof of Theorem 1.1. See the technical overview (Section 1.6) for more details, including discussion of a significantly simpler O~ε⁢(NRD⁡(C)⁢log⁡|C|)subscript~𝑂𝜀NRD𝐶𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\log|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log | italic_C | )-sized sparsifier. 1.2 CSP sparsification We now turn to (unweighted222The weighted case is discussed in Section 1.4.) CSP sparsification. For a relation R⊆Dr𝑅superscript𝐷𝑟R\subseteq D^{r}italic_R ⊆ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT of arity r𝑟ritalic_r over a finite domain D𝐷Ditalic_D, an instance ΨΨ\Psiroman_Ψ of the CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) problem consists a variable set X𝑋Xitalic_X and a constraint set Y⊆Xr𝑌superscript𝑋𝑟Y\subseteq X^{r}italic_Y ⊆ italic_X start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT. An assignment σ:X→D:𝜎→𝑋𝐷\sigma:X\to Ditalic_σ : italic_X → italic_D satisfies a constraint y=(x1,x2,…,xr)∈Y𝑦subscript𝑥1subscript𝑥2…subscript𝑥𝑟𝑌y=(x_{1},x_{2},\dots,x_{r})\in Yitalic_y = ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ∈ italic_Y if (σ⁢(x1),σ⁢(x2),…,σ⁢(xr))∈R𝜎subscript𝑥1𝜎subscript𝑥2…𝜎subscript𝑥𝑟𝑅(\sigma(x_{1}),\sigma(x_{2}),\dots,\sigma(x_{r}))\in R( italic_σ ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_σ ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , … , italic_σ ( italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ) ) ∈ italic_R. The value val⁡(Ψ,σ)valΨ𝜎\operatorname{val}(\Psi,\sigma)roman_val ( roman_Ψ , italic_σ ) of an assignment σ𝜎\sigmaitalic_σ is the number of constraints y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y that it satisfies. Similarly, for a weight function w:Y→ℝ≥0:𝑤→𝑌subscriptℝabsent0w:Y\to\mathbb{R}_{\geq 0}italic_w : italic_Y → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT, the weighted value val⁡(Ψ,w,σ)valΨ𝑤𝜎\operatorname{val}(\Psi,w,\sigma)roman_val ( roman_Ψ , italic_w , italic_σ ) is the sum of weights w⁢(y)𝑤𝑦w(y)italic_w ( italic_y ) of all constraints y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y that σ𝜎\sigmaitalic_σ satisfies. The goal in CSP sparsification is to output a weight function w:Y→ℝ≥0:𝑤→𝑌subscriptℝabsent0w:Y\to\mathbb{R}_{\geq 0}italic_w : italic_Y → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT of small support, such that for every assignment σ:X→D:𝜎→𝑋𝐷\sigma:X\to Ditalic_σ : italic_X → italic_D, (1−ε)⁢val⁡(Ψ,σ)≤val⁡(Ψ,w,σ)≤(1+ε)⁢val⁡(Ψ,σ),1𝜀valΨ𝜎valΨ𝑤𝜎1𝜀valΨ𝜎(1-\varepsilon)\operatorname{val}(\Psi,\sigma)\leq\operatorname{val}(\Psi,w,% \sigma)\leq(1+\varepsilon)\operatorname{val}(\Psi,\sigma)\ ,( 1 - italic_ε ) roman_val ( roman_Ψ , italic_σ ) ≤ roman_val ( roman_Ψ , italic_w , italic_σ ) ≤ ( 1 + italic_ε ) roman_val ( roman_Ψ , italic_σ ) , and minimum such support size is denoted SPR⁡(Ψ,ε)SPRΨ𝜀\operatorname{SPR}(\Psi,\varepsilon)roman_SPR ( roman_Ψ , italic_ε ). The ε𝜀\varepsilonitalic_ε-sparsifiability of the relation R⊆Dr𝑅superscript𝐷𝑟R\subseteq D^{r}italic_R ⊆ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, as a function of number of variables, is defined to the maximum (i.e., worst-case) value of SPR⁡(Ψ,ε)SPRΨ𝜀\operatorname{SPR}(\Psi,\varepsilon)roman_SPR ( roman_Ψ , italic_ε ) over all n𝑛nitalic_n-variables instances ΨΨ\Psiroman_Ψ of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ). We denote it by SPR⁡(R,n,ε)SPR𝑅𝑛𝜀\operatorname{SPR}(R,n,\varepsilon)roman_SPR ( italic_R , italic_n , italic_ε ) and it is the chief object of our study. Note that this is for the unweighted case, see Section 1.4 how this result can be (tightly) applied to the weighted case. Let us note an obvious obstruction to sparsification. Suppose we have an instance Ψ=(X,Y)Ψ𝑋𝑌\Psi=(X,Y)roman_Ψ = ( italic_X , italic_Y ) of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) such that for each of its constraints y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y, there is an assignment σy:X→D:subscript𝜎𝑦→𝑋𝐷\sigma_{y}:X\to Ditalic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT : italic_X → italic_D that satisfies only y𝑦yitalic_y and no other constraint. Then clearly ΨΨ\Psiroman_Ψ cannot be sparsified at all—dropping any constraint y𝑦yitalic_y would make the value of σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT drop from 1111 to 00. We call such an instance a non-redundant instance of CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG ), where R¯=Dr∖R¯𝑅superscript𝐷𝑟𝑅\overline{R}=D^{r}\setminus Rover¯ start_ARG italic_R end_ARG = italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ∖ italic_R (cf., [BCH+13, BCK20]).333We use R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG rather than R𝑅Ritalic_R due to the conventions of each community. See Remark 2.6 for deeper technical reasons. As introduced by Bessiere, Carbonnel, and Katsirelos [BCK20], we denote the size of the largest such non-redundant instance of CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG ) on n𝑛nitalic_n-variables by NRD⁡(R¯,n)NRD¯𝑅𝑛\operatorname{NRD}(\overline{R},n)roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) and call it the non-redundancy of R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG. Thus a trivial lower bound on sparsifiability of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ), regardless of the choice of ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), is given by SPR⁡(R,n,ε)≥NRD⁡(R¯,n),SPR𝑅𝑛𝜀NRD¯𝑅𝑛\displaystyle\operatorname{SPR}(R,n,\varepsilon)\geq\operatorname{NRD}(% \overline{R},n)\ ,roman_SPR ( italic_R , italic_n , italic_ε ) ≥ roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) , (1) and this holds even if the goal is merely to preserve which assignments have nonzero value. Rather remarkably, this simplistic lower bound can be met and one can sparsify all the way down to NRD⁡(R¯,n)NRD¯𝑅𝑛\operatorname{NRD}(\overline{R},n)roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) times polylogarithmic factors! In fact, this turns out to be an easy corollary of Theorem 1.1. One can associate a canonical code CΨ⊆{0,1}Ysubscript𝐶Ψsuperscript01𝑌C_{\Psi}\subseteq\{0,1\}^{Y}italic_C start_POSTSUBSCRIPT roman_Ψ end_POSTSUBSCRIPT ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_Y end_POSTSUPERSCRIPT with any CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) instance Ψ=(X,Y)Ψ𝑋𝑌\Psi=(X,Y)roman_Ψ = ( italic_X , italic_Y ) whose codewords cσsubscript𝑐𝜎c_{\sigma}italic_c start_POSTSUBSCRIPT italic_σ end_POSTSUBSCRIPT correspond to the assignments σ:X→D:𝜎→𝑋𝐷\sigma:X\to Ditalic_σ : italic_X → italic_D, and cσ,ysubscript𝑐𝜎𝑦c_{\sigma,y}italic_c start_POSTSUBSCRIPT italic_σ , italic_y end_POSTSUBSCRIPT is 1111 precisely when σ𝜎\sigmaitalic_σ satisfies y𝑦yitalic_y. It is easy to check that CSP sparsification of ΨΨ\Psiroman_Ψ reduces to code sparsification of CΨsubscript𝐶ΨC_{\Psi}italic_C start_POSTSUBSCRIPT roman_Ψ end_POSTSUBSCRIPT, and the non-redundancy of C𝐶Citalic_C equals the size of the largest non-redundant sub-instance of ΨΨ\Psiroman_Ψ (viewed as an instance of CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG )). Combining Theorem 1.1 and (1), we therefore have our main result pinning down the sparsifiability of every CSP up to polylogarithmic factors. Theorem 1.2. For every nonempty R⊊Dr𝑅superscript𝐷𝑟R\subsetneq D^{r}italic_R ⊊ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT and ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), we have that NRD⁡(R¯,n)≤SPR⁡(R,n,ε)≤O⁢(NRD⁡(R¯,n)⁢(r⁢log⁡n)6/ε2).NRD¯𝑅𝑛SPR𝑅𝑛𝜀𝑂NRD¯𝑅𝑛superscript𝑟𝑛6superscript𝜀2\operatorname{NRD}(\overline{R},n)\leq\operatorname{SPR}(R,n,\varepsilon)\leq O% (\operatorname{NRD}(\overline{R},n)(r\log n)^{6}/\varepsilon^{2}).roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) ≤ roman_SPR ( italic_R , italic_n , italic_ε ) ≤ italic_O ( roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) ( italic_r roman_log italic_n ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . 1.3 Non-redundancy of specific relations The non-redundancy of relations is readily computed in some simple cases. For example, for the relation ORr:=Dr∖{0r}assignsubscriptOR𝑟superscript𝐷𝑟superscript0𝑟\operatorname{OR}_{r}:=D^{r}\setminus\{0^{r}\}roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT := italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ∖ { 0 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT }, we have that NRD⁡(ORr,n)=Θ⁢(nr)NRDsubscriptOR𝑟𝑛Θsuperscript𝑛𝑟\operatorname{NRD}(\operatorname{OR}_{r},n)=\Theta(n^{r})roman_NRD ( roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , italic_n ) = roman_Θ ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ). Indeed Y=(Xr)𝑌binomial𝑋𝑟Y=\binom{X}{r}italic_Y = ( FRACOP start_ARG italic_X end_ARG start_ARG italic_r end_ARG ) is a non-redundant instance because setting all but r𝑟ritalic_r variables to 1111 fails to satisfy exactly that r𝑟ritalic_r-tuple (see [FK17, Car22, KPS24b]). When R𝑅Ritalic_R is affine, NRD⁡(R,n)=Θ⁢(n)NRD𝑅𝑛Θ𝑛\operatorname{NRD}(R,n)=\Theta(n)roman_NRD ( italic_R , italic_n ) = roman_Θ ( italic_n ), and when R𝑅Ritalic_R is defined as the zero set of a degree k𝑘kitalic_k polynomial, NRD⁡(R,n)=O⁢(nk)NRD𝑅𝑛𝑂superscript𝑛𝑘\operatorname{NRD}(R,n)=O(n^{k})roman_NRD ( italic_R , italic_n ) = italic_O ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ); these follow from simple rank arguments (e.g., [LW20]). Via Theorem 1.2, these special cases (plus simple gadget reductions) already capture all the previously known upper and lower bounds for CSP sparsification (see Section 1.5 for more details on the CSP sparsification literature). Furthermore, there are also some non-trivial upper bounds known on NRD in the literature, which we can now import to sparsifiability for free courtesy Theorem 1.2. For instance, the so-called Mal’tsev relations, which generalize affine predicates (i.e., cosets) over Abelian groups, have been shown to have OD⁢(n)subscript𝑂𝐷𝑛O_{D}(n)italic_O start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ( italic_n ) non-redundancy [LW20, BCK20], and therefore by Theorem 1.2 their complements have near-linear sparsifiability. Carbonnel [Car22] showed that if R𝑅Ritalic_R is an arity r𝑟ritalic_r relation that doesn’t contain444See Theorem 5.2 for a precise definition. any copy of ORrsubscriptOR𝑟\operatorname{OR}_{r}roman_OR start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, then NRD⁡(R,n)≤O⁢(nr−δr)NRD𝑅𝑛𝑂superscript𝑛𝑟subscript𝛿𝑟\operatorname{NRD}(R,n)\leq O(n^{r-\delta_{r}})roman_NRD ( italic_R , italic_n ) ≤ italic_O ( italic_n start_POSTSUPERSCRIPT italic_r - italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) for δr=21−rsubscript𝛿𝑟superscript21𝑟\delta_{r}=2^{1-r}italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = 2 start_POSTSUPERSCRIPT 1 - italic_r end_POSTSUPERSCRIPT (the specific bound arises from a classic hypergraph Turán result [Erd64]). By Theorem 1.2 this immediately implies SPR⁡(R¯,n,ε)≤O~ε⁢(nr−δr)SPR¯𝑅𝑛𝜀subscript~𝑂𝜀superscript𝑛𝑟subscript𝛿𝑟\operatorname{SPR}(\overline{R},n,\varepsilon)\leq\widetilde{O}_{\varepsilon}(% n^{r-\delta_{r}})roman_SPR ( over¯ start_ARG italic_R end_ARG , italic_n , italic_ε ) ≤ over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r - italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ), where O~⁢(⋅)~𝑂⋅\widetilde{O}(\cdot)over~ start_ARG italic_O end_ARG ( ⋅ ) hides polylogarithmic factors in n𝑛nitalic_n, yielding an Ω⁢(nr)Ωsuperscript𝑛𝑟\Omega(n^{r})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) vs O~⁢(nr−δr)~𝑂superscript𝑛𝑟subscript𝛿𝑟\widetilde{O}(n^{r-\delta_{r}})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT italic_r - italic_δ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) dichotomy for sparsification of arity r𝑟ritalic_r CSPs. (This was known for the Boolean case [KPS24b]; see the related work subsection.) The non-redundancy of a relation can in general be difficult to estimate. Thus while in principle Theorem 1.2 pins down the sparisifiability of every CSP, for specific relations, it can still be non-trivial to actually determine the asymptotic behavior of its sparsifiability. Our next set of results makes progress in this direction via novel methods to bound non-redundancy. Given that the non-redundancy of linear predicates is easy to pin down, we consider a natural family of relations which are very close to being linear. Specifically, let 3⁢L⁢I⁢NG={(x,y,z)∣x+y+z=0}subscript3LIN𝐺conditional-set𝑥𝑦𝑧𝑥𝑦𝑧0\operatorname{3LIN}_{G}=\{(x,y,z)\mid x+y+z=0\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_z ) ∣ italic_x + italic_y + italic_z = 0 } over an Abelian group G𝐺Gitalic_G, and consider 3⁢L⁢I⁢NG∗=3⁢L⁢I⁢NG∖{(0,0,0)}subscriptsuperscript3LIN𝐺subscript3LIN𝐺000\operatorname{3LIN}^{*}_{G}=\operatorname{3LIN}_{G}\setminus\{(0,0,0)\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ∖ { ( 0 , 0 , 0 ) }. (We pick arity 3333 since the arity 2222 case is already fully resolved [FK17, BŽ20].) Being defined by a linear equation over an Abelian group, we already know that NRD⁡(3⁢L⁢I⁢NG,n)=ΘG⁢(n)NRDsubscript3LIN𝐺𝑛subscriptΘ𝐺𝑛\operatorname{NRD}(\operatorname{3LIN}_{G},n)=\Theta_{G}(n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) = roman_Θ start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n ). However the non-redundancy of 3⁢L⁢I⁢NG∗subscriptsuperscript3LIN𝐺\operatorname{3LIN}^{*}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT seems challenging to understand. Existing methods in the literature only yield NRD⁡(3⁢L⁢I⁢NG∗,n)∈[ΩG⁢(n),OG⁢(n2)]NRDsubscriptsuperscript3LIN𝐺𝑛subscriptΩ𝐺𝑛subscript𝑂𝐺superscript𝑛2\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)\in[\Omega_{G}(n),O_{G}(n^{2})]roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) ∈ [ roman_Ω start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n ) , italic_O start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ]. We introduce a new method for bounding the non-redundancy of predicates like 3⁢L⁢I⁢NG∗subscriptsuperscript3LIN𝐺\operatorname{3LIN}^{*}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT by connecting them to the theory of matching vector (MV) families [Yek08, DGY11] that have been used in the construction of locally decodable codes. Exploiting this connection, we construct a non-redundant instance to establish that NRD⁡(3⁢L⁢I⁢NG,n)≥Ω⁢(n1.5)NRDsubscript3LIN𝐺𝑛Ωsuperscript𝑛1.5\operatorname{NRD}(\operatorname{3LIN}_{G},n)\geq\Omega(n^{1.5})roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) ≥ roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) for all Abelian groups of order ≥3absent3\geq 3≥ 3. Adapting ideas from the analysis of MV families together with some combinatorial ideas, we also prove an upper bound NRD⁡(3⁢L⁢I⁢Nℤ/p⁢ℤ,n)=O~p⁢(n2−εp)NRDsubscript3LINℤ𝑝ℤ𝑛subscript~𝑂𝑝superscript𝑛2subscript𝜀𝑝\operatorname{NRD}(\operatorname{3LIN}_{\mathbb{Z}/p\mathbb{Z}},n)=\widetilde{% O}_{p}(n^{2-\varepsilon_{p}})roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT blackboard_Z / italic_p blackboard_Z end_POSTSUBSCRIPT , italic_n ) = over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT 2 - italic_ε start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) for εp=22⁢p−1subscript𝜀𝑝22𝑝1\varepsilon_{p}=\tfrac{2}{2p-1}italic_ε start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT = divide start_ARG 2 end_ARG start_ARG 2 italic_p - 1 end_ARG and p𝑝pitalic_p prime. Specializing for p=3𝑝3p=3italic_p = 3, we have the following result, which also gives the first examples of relations whose non-redundancy and sparsifiability have a non-integral exponent. Theorem 1.3. We have NRD⁡(3⁢L⁢I⁢Nℤ/3⁢ℤ∗,n)NRDsubscriptsuperscript3LINℤ3ℤ𝑛\displaystyle\operatorname{NRD}(\operatorname{3LIN}^{*}_{\mathbb{Z}/3\mathbb{Z% }},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT , italic_n ) ∈[Ω⁢(n1.5),O~⁢(n1.6)], andabsentΩsuperscript𝑛1.5~𝑂superscript𝑛1.6 and\displaystyle\in[\Omega(n^{1.5}),\widetilde{O}(n^{1.6})],\ \ \ \text{ and }∈ [ roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) , over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) ] , and SPR⁡(3⁢L⁢I⁢Nℤ/3⁢ℤ∗¯,n,ε)SPR¯subscriptsuperscript3LINℤ3ℤ𝑛𝜀\displaystyle\operatorname{SPR}(\overline{\operatorname{3LIN}^{*}_{\mathbb{Z}/% 3\mathbb{Z}}},n,\varepsilon)roman_SPR ( over¯ start_ARG start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT end_ARG , italic_n , italic_ε ) ∈[Ω⁢(n1.5),O~⁢(n1.6/ε2)].absentΩsuperscript𝑛1.5~𝑂superscript𝑛1.6superscript𝜀2\displaystyle\in[\Omega(n^{1.5}),\widetilde{O}(n^{1.6}/\varepsilon^{2})].∈ [ roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) , over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ] . 1.4 Weighted CSP sparsification The discussion so far has focused on unweighted CSP instances, and we now shift our focus to the weighted case, where each constraint of Y𝑌Yitalic_Y comes with a weight. We also get a tight characterization of weighted CSP sparsifiablity, in terms of a parameter called the chain length, which was defined by Lagerkvist and Wahlström [LW17, LW20] in the context of CSP kernelization and later utilized by Bessiere, Carbonnel, and Katsirelos [BCK20] in the context of learning CSPs in a certain query model (see Section 1.5 for more details on these connections). As before, the result is obtained in the setting of weighted non-linear codes, with the consequence for weighted CSPs being an easy corollary. We just state the result for codes here (see Section 8 for the full treatment of weighted CSPs). For weighted sparsification of a code C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, we might have an arbitrary input weighting ζ:[m]→ℝ≥0:𝜁→delimited-[]𝑚subscriptℝabsent0\zeta:[m]\to\mathbb{R}_{\geq 0}italic_ζ : [ italic_m ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT of its coordinates, and we must find a sparsifier w~:[n]→ℝ≥0:~𝑤→delimited-[]𝑛subscriptℝabsent0\widetilde{w}:[n]\to\mathbb{R}_{\geq 0}over~ start_ARG italic_w end_ARG : [ italic_n ] → blackboard_R start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT of low support that sparsifies C𝐶Citalic_C with respect to the weighting ζ𝜁\zetaitalic_ζ, i.e., ⟨w~,c⟩∈(1±ε)⁢⟨ζ,c⟩~𝑤𝑐plus-or-minus1𝜀𝜁𝑐\langle\widetilde{w},c\rangle\in(1\pm\varepsilon)\langle\zeta,c\rangle⟨ over~ start_ARG italic_w end_ARG , italic_c ⟩ ∈ ( 1 ± italic_ε ) ⟨ italic_ζ , italic_c ⟩. The minimum possible support of sparsifiers over all weightings ζ𝜁\zetaitalic_ζ is called the weighted ε𝜀\varepsilonitalic_ε-sparsity wSPR⁡(C,ε)wSPR𝐶𝜀\operatorname{wSPR}(C,\varepsilon)roman_wSPR ( italic_C , italic_ε ). Now we define chain length. If we line up the codewords of C𝐶Citalic_C as rows of an |C|×m𝐶𝑚|C|\times m| italic_C | × italic_m matrix and allow arbitrary column permutations, the chain length of C𝐶Citalic_C, denoted CL⁡(C)CL𝐶\operatorname{CL}(C)roman_CL ( italic_C ), is the dimension of the largest upper triangular square submatrix with 1111’s on the diagonal.555In this view NRD⁡(C)NRD𝐶\operatorname{NRD}(C)roman_NRD ( italic_C ) is the dimension of the largest identity submatrix, so clearly NRD⁡(C)≤CL⁡(C)NRD𝐶CL𝐶\operatorname{NRD}(C)\leq\operatorname{CL}(C)roman_NRD ( italic_C ) ≤ roman_CL ( italic_C ). The quantity CL⁡(C)CL𝐶\operatorname{CL}(C)roman_CL ( italic_C ) was called visible rank in [AG21] and served as a field independent lower bound on the rank of C𝐶Citalic_C. In our main result for the weighted setting, we pin the sparsifiability of a weighted code to its chain length. Note that in the weighted case CL⁡(C)CL𝐶\operatorname{CL}(C)roman_CL ( italic_C ) is also a lower bound. Theorem 1.4. For all C⊆{0,1}m𝐶superscript01𝑚C\subseteq\{0,1\}^{m}italic_C ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT and ε∈(0,1)𝜀01\varepsilon\in(0,1)italic_ε ∈ ( 0 , 1 ), we have CL⁡(C)≤wSPR⁡(C,ε)=O⁢(CL⁡(C)⁢(log⁡m)6/ε2).CL𝐶wSPR𝐶𝜀𝑂CL𝐶superscript𝑚6superscript𝜀2\operatorname{CL}(C)\leq\operatorname{wSPR}(C,\varepsilon)=O(\operatorname{CL}% (C)(\log m)^{6}/\varepsilon^{2}).roman_CL ( italic_C ) ≤ roman_wSPR ( italic_C , italic_ε ) = italic_O ( roman_CL ( italic_C ) ( roman_log italic_m ) start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . The upper bound proceeds by using Theorem 1.1 as a black-box together with a geometric weight bucketing technique from [KPS24b]. The lower bound proceeds by applying an exponential sequence of weights to the indices i1,…,iCL⁡(C)∈[m]subscript𝑖1…subscript𝑖CL𝐶delimited-[]𝑚i_{1},\ldots,i_{\operatorname{CL}{(C)}}\in[m]italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_i start_POSTSUBSCRIPT roman_CL ( italic_C ) end_POSTSUBSCRIPT ∈ [ italic_m ] forming a maximal chain. Of note, if for a particular set of weights, the ratio between maximum and minimal weights is λ≪exp⁡(CL⁡(C)/NRD⁡(C))much-less-than𝜆CL𝐶NRD𝐶\lambda\ll\exp(\operatorname{CL}(C)/\operatorname{NRD}(C))italic_λ ≪ roman_exp ( roman_CL ( italic_C ) / roman_NRD ( italic_C ) ), we get a sharper upper bound of O~ε⁢(NRD⁡(C)⁢log⁡λ)subscript~𝑂𝜀NRD𝐶𝜆\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\log\lambda)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log italic_λ ) (see Corollary 8.18). We now transition to discussing the broader context of our work in the literature. 1.5 Related Work Our results and techniques have connections to many areas including computational complexity theory, extremal combinatorics, coding theory, and learning theory. We now give a general overview of these connections. CSP Sparsification. Since we already discussed the history of CSP sparsification, we give a comprehensive list of known results about CSP sparsification (up to polylog factors). • The case of binary CSPs (r=2𝑟2r=2italic_r = 2) is fully classified. In particular, for every finite domain D𝐷Ditalic_D and R⊆D2𝑅superscript𝐷2R\subseteq D^{2}italic_R ⊆ italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, we either have that SPR⁡(R,n,ε)=O⁢(n/ε2)SPR𝑅𝑛𝜀𝑂𝑛superscript𝜀2\operatorname{SPR}(R,n,\varepsilon)=O(n/\varepsilon^{2})roman_SPR ( italic_R , italic_n , italic_ε ) = italic_O ( italic_n / italic_ε start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) or SPR⁡(R,n,ε)=Ω⁢(n2)SPR𝑅𝑛𝜀Ωsuperscript𝑛2\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{2})roman_SPR ( italic_R , italic_n , italic_ε ) = roman_Ω ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) [BŽ20]. However, the sparsification routine is only efficient in the Boolean case [FK17]. Of note, SPR⁡(R,n,ε)=Ω⁢(n2)SPR𝑅𝑛𝜀Ωsuperscript𝑛2\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{2})roman_SPR ( italic_R , italic_n , italic_ε ) = roman_Ω ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) if and only if there exist D1,D2⊆Dsubscript𝐷1subscript𝐷2𝐷D_{1},D_{2}\subseteq Ditalic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ⊆ italic_D of size exactly 2222 such that |R∩(D1×D2)|=1𝑅subscript𝐷1subscript𝐷21|R\cap(D_{1}\times D_{2})|=1| italic_R ∩ ( italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) | = 1 (informally R𝑅Ritalic_R has an “induced copy” of AND2subscriptAND2\operatorname{AND}_{2}roman_AND start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT). • For r≥3𝑟3r\geq 3italic_r ≥ 3, much less is known. Kogan and Krauthgamer [KK15] contributed near-linear hypergraph cut sparsifiers (i.e., the predicate is NAEr:={0,1}r∖{0r,1r}assignsubscriptNAE𝑟superscript01𝑟superscript0𝑟superscript1𝑟\operatorname{NAE}_{r}:=\{0,1\}^{r}\setminus\{0^{r},1^{r}\}roman_NAE start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT := { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ∖ { 0 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT , 1 start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT }). Since then, there have been multiple improvements in efficiently constructing hypergraph sparsifiers/sketches (e.g., [CKN20, KKTY21, KPS24c]). • The breakthroughs of Khanna, Putterman, and Sudan [KPS24a, KPS24b] construct non-linear sparsifiers for any predicate which can defined by a system of linear (in)equations (possibly over a higher domain). For example NAEr={x∈{0,1}r:x1+⋯+xr≢0modr}subscriptNAE𝑟conditional-set𝑥superscript01𝑟not-equivalent-tosubscript𝑥1⋯subscript𝑥𝑟modulo0𝑟\operatorname{NAE}_{r}=\{x\in\{0,1\}^{r}:x_{1}+\cdots+x_{r}\not\equiv 0\mod r\}roman_NAE start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT = { italic_x ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT : italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ⋯ + italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ≢ 0 roman_mod italic_r }. Of note, their first paper [KPS24a] only proved the result over finite fields (and was nonalgorithmic), whereas their second paper [KPS24b] extended the result to all Abelian groups and was computationally efficient. • The framework of Khanna, Putterman, and Sudan [KPS24b] produced numerous corollaries. In particular, if a predicate can be expressed as the nonzero set of a degree k𝑘kitalic_k polynomial, then it has a sparsifier of size O~ε⁢(nk)subscript~𝑂𝜀superscript𝑛𝑘\widetilde{O}_{\varepsilon}(n^{k})over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). Furthermore, they show if a predicate R𝑅Ritalic_R can express666More specifically, we say that R⊆{0,1}r𝑅superscript01𝑟R\subseteq\{0,1\}^{r}italic_R ⊆ { 0 , 1 } start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT can express ANDksubscriptAND𝑘\operatorname{AND}_{k}roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT if there exits a map z:[r]→{0,1,x1,…,xk,x1¯,…,xk¯}:𝑧→delimited-[]𝑟01subscript𝑥1…subscript𝑥𝑘¯subscript𝑥1…¯subscript𝑥𝑘z:[r]\to\{0,1,x_{1},\ldots,x_{k},\overline{x_{1}},\ldots,\overline{x_{k}}\}italic_z : [ italic_r ] → { 0 , 1 , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over¯ start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , … , over¯ start_ARG italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG } such that R⁢(z⁢(1),…,z⁢(r))=ANDk⁡(x1,…,xk)𝑅𝑧1…𝑧𝑟subscriptAND𝑘subscript𝑥1…subscript𝑥𝑘R(z(1),\ldots,z(r))=\operatorname{AND}_{k}(x_{1},\ldots,x_{k})italic_R ( italic_z ( 1 ) , … , italic_z ( italic_r ) ) = roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ). We discuss a more general framework of gadget reductions in Section 5.4. ANDk:={1k}assignsubscriptAND𝑘superscript1𝑘\operatorname{AND}_{k}:=\{1^{k}\}roman_AND start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT := { 1 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT }, then SPR⁡(R,n,ε)=Ω⁢(nk)SPR𝑅𝑛𝜀Ωsuperscript𝑛𝑘\operatorname{SPR}(R,n,\varepsilon)=\Omega(n^{k})roman_SPR ( italic_R , italic_n , italic_ε ) = roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). As a consequence, they also classify all ternary Boolean predicates (r=3𝑟3r=3italic_r = 3) as well as which Boolean predicates of arity r𝑟ritalic_r cannot be sparsified below Ω⁢(nr)Ωsuperscript𝑛𝑟\Omega(n^{r})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) (just ANDrsubscriptAND𝑟\operatorname{AND}_{r}roman_AND start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and its bit flips), while also constructing a sparsifier of size O~ε⁢(nr−1)subscript~𝑂𝜀superscript𝑛𝑟1\widetilde{O}_{\varepsilon}(n^{r-1})over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT ) in the other cases. • It appears that lower bounds with a nontrivial dependence on ε𝜀\varepsilonitalic_ε are only known for cut sparsifiers (and thus hypergraph cut sparsifiers via a simple gadget reduction). See [ACK+16, CKST19] as well as Section 9 for further discussion. CSP Kernelization. Another question similar in spirit to CSP sparsification is that of CSP kernelization.777More commonly, CSP kernelization is referred to as CSP sparsification (e.g., [DvM14, LW20]). However, we refer to this line of work by the former name to reduce ambiguity. This similarity in name has been noted before in the literature (e.g., [BŽ20]), but we appear to be the first work to notice both variants of “CSP sparsification” can be analyzed with similar techniques. The basic question is to, given an instance ΨΨ\Psiroman_Ψ of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ), efficiently find as small of an instance Ψ′superscriptΨ′\Psi^{\prime}roman_Ψ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) as possible (not necessarily a subinstance) such that ΨΨ\Psiroman_Ψ and Ψ′superscriptΨ′\Psi^{\prime}roman_Ψ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are either both satisfiable or both unsatisfiable. This particular question can be attributed to Dell and van Melkebeek [DvM14], who were particularly inspired Impagliazzo, Paturi, and Zane’s sparsification lemma [IPZ01] and Harnik and Naor’s compression framework [HN10]. See the literature review in [DvM14] for further motivations. At first, the problem seems rather unrelated to CSP sparsification. For example, if CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) is polynomial-time tractable, then there trivially exists a kernel of size O⁢(1)𝑂1O(1)italic_O ( 1 ). When CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) is NP-hard, however, the size of the smallest possible kernelization seems to much more closely track with the non-redundancy of R𝑅Ritalic_R. In particular, Dell and van Melkebeek [DvM14], proved that assuming 𝖼𝗈𝖭𝖯⊈𝖭𝖯/𝗉𝗈𝗅𝗒not-subset-of-nor-equals𝖼𝗈𝖭𝖯𝖭𝖯𝗉𝗈𝗅𝗒\mathsf{coNP}\nsubseteq\mathsf{NP/poly}sansserif_coNP ⊈ sansserif_NP / sansserif_poly, the problem k𝑘kitalic_k-SAT cannot be kernelized below Ω⁢(nk−ε)Ωsuperscript𝑛𝑘𝜀\Omega(n^{k-\varepsilon})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_k - italic_ε end_POSTSUPERSCRIPT ) for any constant ε>0𝜀0\varepsilon>0italic_ε > 0, which is close to k𝑘kitalic_k-SAT’s non-redundancy of Θ⁢(nk)Θsuperscript𝑛𝑘\Theta(n^{k})roman_Θ ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). Furthermore, most upper bounds on the kernelization of NP-hard predicates follow from upper bounds on non-redundancy (see [Car22]). For example the works of Chen, Jansen, and Pieterse [CJP20] as well as Lagerkvist and Wahlström [LW17, LW20] develop various kernelization methods that happen to just be “efficient” non-redundancy upper bounds. For example, these works show that if the predicate R𝑅Ritalic_R can be expressed as the zero set of a polynomial of degree k𝑘kitalic_k, then there exist a kernel of size O⁢(nk)𝑂superscript𝑛𝑘O(n^{k})italic_O ( italic_n start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ). This kernel happens to preserve every solution to R𝑅Ritalic_R, so it is also a non-redundancy upper bound. Using techniques like these, they are able to prove a number of results similar to the state-of-the-art in CSP sparsification, such as a complete classification of ternary Boolean predicates and a O⁢(nr−1)𝑂superscript𝑛𝑟1O(n^{r-1})italic_O ( italic_n start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT ) vs Ω⁢(nr−ε)Ωsuperscript𝑛𝑟𝜀\Omega(n^{r-\varepsilon})roman_Ω ( italic_n start_POSTSUPERSCRIPT italic_r - italic_ε end_POSTSUPERSCRIPT ) Boolean dichotomy [CJP20]. See [JP19, JW20, Jan20, Tak23, Beu21] and citations therein for related work. We seek to emphasize that any efficient CSP sparsification algorithm for CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) is by design a kernelization algorithm for CSP⁡(R¯)CSP¯𝑅\operatorname{CSP}(\overline{R})roman_CSP ( over¯ start_ARG italic_R end_ARG ) (since all codewords with weight 00 are preserved). As such, making Theorem 1.2 efficient would require explicitly proving that every CSP can be kernelized to (approximately) its non-redundancy, which is a significant open question in the CSP kernelization community (see [Car22]). See Section 1.7 and Section 9 for further discussion. The Union-closed Sets Conjecture. A family ℱℱ\mathcal{F}caligraphic_F of subsets of [n]delimited-[]𝑛[n][ italic_n ] is union-closed if A,B∈ℱ𝐴𝐵ℱA,B\in\mathcal{F}italic_A , italic_B ∈ caligraphic_F imply that A∪B∈ℱ𝐴𝐵ℱA\cup B\in\mathcal{F}italic_A ∪ italic_B ∈ caligraphic_F. In 1979, Frankl [Fra95] conjectured that there always exists i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ] which appears in at least half of the sets of ℱℱ\mathcal{F}caligraphic_F. For decades, progress on the conjecture was minimal, with the best general result being that some i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ] appears in Ω⁢(1/log2⁡|ℱ|)Ω1subscript2ℱ\Omega(1/\log_{2}|\mathcal{F}|)roman_Ω ( 1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | caligraphic_F | ) of the sets [Kni94, Wój99, Gil22]. However, in 2022, Gilmer [Gil22] shocked the combinatorics community by using an entropy-based approach to prove that some i∈[n]𝑖delimited-[]𝑛i\in[n]italic_i ∈ [ italic_n ] appears in 1/10011001/1001 / 100 of the sets. This immediately led to a large number of follow-up works refining Gilmer’s entropy method [AHS22, CL22, Peb22, Saw23, Yu23, Cam22]. In particular, we can now replace ‘1/10011001/1001 / 100’ with ‘0.382⁢…0.382…0.382\ldots0.382 …’, leaving Frankl’s conjecture (technically) still open. For our application to CSP sparsification, the entropy method used by Gilmer (and its subsequent refinements by many other reseachers) is the key idea needed to show that non-redundancy is essentially the optimal size for a CSP sparsifier. In particular, the improvement from 1/log2⁡|ℱ|1subscript2ℱ1/\log_{2}|\mathcal{F}|1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | caligraphic_F | to Ω⁢(1)Ω1\Omega(1)roman_Ω ( 1 ) is precisely the same “gain” we utilize to go from a very simple O~ε⁢(NRD⁡(C)⋅log2⁡|C|)subscript~𝑂𝜀⋅NRD𝐶subscript2𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\cdot\log_{2}|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ⋅ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C | ) sparsifier (see Section 3) to our O~ε⁢(NRD⁡(C))subscript~𝑂𝜀NRD𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C))over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ) sparsifier. See the technical overview (Section 1.6) for more details. To the best of our knowledge, our work is the first application of Gilmer’s entropy method to sparsification.888Gilmer’s breakthough is cited in the literature review of [CDL+24], but the property-testing question they study on union-closed families has no technical connection to Gilmer’s entropy method. See also [Wak24] for applications of the entropy method to learning theory and statistical physics. Matching Vector Families and Locally Decodable Codes. In coding theory, locally decodable codes (LDCs) are a class of codes which allow for jthe reliable recovery of any message symbol based on a small sample of codeword symbols, even in the presence of a constant fraction of errors. A particularly interesting familiy of constructions of LDCs has arisen out of a theory of matching vector codes [Yek08] and follow-ups [Rag07, Gop09, Efr09, DGY11]. See [DGY11] for a literature survey. Simply stated, a matching vector (MV) family over a (finite) ring ℛℛ\mathcal{R}caligraphic_R is a pair of lists of vectors u1,…,uk,v1,…,vk∈ℛdsubscript𝑢1…subscript𝑢𝑘subscript𝑣1…subscript𝑣𝑘superscriptℛ𝑑u_{1},\ldots,u_{k},v_{1},\ldots,v_{k}\in\mathcal{R}^{d}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT such that the inner products ⟨ui,vj⟩subscript𝑢𝑖subscript𝑣𝑗\langle u_{i},v_{j}\rangle⟨ italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ⟩ are nonzero999Or, more generally the inner products lie in some restricted subset of ℛℛ\mathcal{R}caligraphic_R. if and only i≠j𝑖𝑗i\neq jitalic_i ≠ italic_j. Informally, the uisubscript𝑢𝑖u_{i}italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT’s play a role in the encoding of the i𝑖iitalic_i’th message symbol, with the matching vector visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT helping with its local decoding. Given a choice of ℛℛ\mathcal{R}caligraphic_R and d𝑑ditalic_d, the primary question of interest is to find the maximal possible value of k𝑘kitalic_k. This “spin off” question about LDCs has become a topic of interest in its own right [DGY11, Yek12, GHSY12, BDL13]. In this work, we demonstrate a novel application of matching vector families to the study of non-redundancy and thus (by Theorem 1.2) sparsification. In particular, we construct an explicit family of predicates such that their non-redundant instances can be viewed as a generalized MV family. We then use techniques developed for MV families to given nontrivial bounds on the non-redundancy of the predicates. See Section 6 and the technical overview (Section 1.6) for more details. Extremal Combinatorics. Computing the non-redundancy of a predicate can be viewed as a problem in extremal combinatorics known as a hypergraph Turán problem. In particular, for an instance of a CSP to be non-redundant, every instance induced by a subset of the variables must also be non-redundant. In particular, if ℱℱ\mathcal{F}caligraphic_F is a family of hypergraphs which can never appear in non-redundant instances of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ), then NRD⁡(R,n)≤exr⁡(n,ℱ)NRD𝑅𝑛subscriptex𝑟𝑛ℱ\operatorname{NRD}(R,n)\leq\operatorname{ex}_{r}(n,\mathcal{F})roman_NRD ( italic_R , italic_n ) ≤ roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ), where the hypergraph Turán number exr⁡(n,ℱ)subscriptex𝑟𝑛ℱ\operatorname{ex}_{r}(n,\mathcal{F})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) is the size of the largest r𝑟ritalic_r-uniform hypergraph on n𝑛nitalic_n vertices without any F∈ℱ𝐹ℱF\in\mathcal{F}italic_F ∈ caligraphic_F as a subgraph. This observation was first made explicit by Carbonnel [Car22] although the technique was also used in earlier work [BCK20]. As far as we are aware, ours is the first work to observe that these insights can also benefit the study of CSP sparsification. The literature on hypergraph Turán numbers is quite rich. For instance, Keevash [Kee11] surveys the vast body of work on the “non-degenerate” case in which exr⁡(n,ℱ)=Ωr⁢(nr)subscriptex𝑟𝑛ℱsubscriptΩ𝑟superscript𝑛𝑟\operatorname{ex}_{r}(n,\mathcal{F})=\Omega_{r}(n^{r})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) = roman_Ω start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ). However, for our applications, we are mostly interested in the “denegerate” case in which exr⁡(n,ℱ)=O⁢(nc)subscriptex𝑟𝑛ℱ𝑂superscript𝑛𝑐\operatorname{ex}_{r}(n,\mathcal{F})=O(n^{c})roman_ex start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( italic_n , caligraphic_F ) = italic_O ( italic_n start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) for some c∈[1,r)𝑐1𝑟c\in[1,r)italic_c ∈ [ 1 , italic_r ). The works [BCK20, Car22] apply some of the most well-known works in this setting [Erd64, SEB73, RS78] to get some nontrivial results such as classifying precisely which predicates R𝑅Ritalic_R have NRD⁡(R,n)=Θ⁢(nr)NRD𝑅𝑛Θsuperscript𝑛𝑟\operatorname{NRD}(R,n)=\Theta(n^{r})roman_NRD ( italic_R , italic_n ) = roman_Θ ( italic_n start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ), extending Chen, Jansen, and Pieterse’s result for the Boolean case [CJP20]. See Sections 5.2, 7.1, and 8.4.2 for more details on specific applications. Query Complexity and Learning Theory. Rather surprisingly, the definition of non-redundancy appears to have come out of the artificial intelligence community [BCK20]. In particular, a rather broad and well-studied question (e.g., [FW02, PBS08, LLMV10, BK12, BCH+13, BCK20]) is that of constraint acquisition: how can an agent learn the constraints defining an instance of a constraint satisfaction problem? A model specifically relevant to our work is the partial membership queries model studied by Bessiere, Carbonnel, and Katsirelos [BCK20]. In this model, the domain D𝐷Ditalic_D, the constraint type R𝑅Ritalic_R (or types), and the set of variables X𝑋Xitalic_X are known but the constraints are hidden. For each query, the agent picks some subset of variables X′⊆Xsuperscript𝑋′𝑋X^{\prime}\subseteq Xitalic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ⊆ italic_X as well as a partial assignment σ:X′→D:𝜎→superscript𝑋′𝐷\sigma:X^{\prime}\to Ditalic_σ : italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT → italic_D. The response to the query is ‘YES’ if σ𝜎\sigmaitalic_σ satisfies every constraint induced by X′superscript𝑋′X^{\prime}italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, and ‘NO’ otherwise. The goal is to construct an instance of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) with the same solution set as the hidden CSP. For every CSP predicate R𝑅Ritalic_R, they prove that the query complexity of an instance of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) on n𝑛nitalic_n variables is bounded between Ω⁢(NRD⁡(R,n))ΩNRD𝑅𝑛\Omega(\operatorname{NRD}(R,n))roman_Ω ( roman_NRD ( italic_R , italic_n ) ) and O⁢(CL⁡(R,n)⋅log⁡n)𝑂⋅CL𝑅𝑛𝑛O(\operatorname{CL}(R,n)\cdot\log n)italic_O ( roman_CL ( italic_R , italic_n ) ⋅ roman_log italic_n ). Notably, the lower bound is proved by showing that the VC dimension of the query complexity problem equals NRD⁡(R,n)NRD𝑅𝑛\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ).101010This observation is directly used in proving our main result, see Section 4.1. 1.6 Technical Overview We next describe the primary techniques we use to prove Theorem 1.1 and Theorem 1.3. A Simple Sparsifier. To begin, we discuss a warm-up version of Theorem 1.1 which proves a weaker upper bound of SPR⁡(C,ε)≤O~ε⁢(NRD⁡(C)⋅log⁡|C|)SPR𝐶𝜀subscript~𝑂𝜀⋅NRD𝐶𝐶\operatorname{SPR}(C,\varepsilon)\leq\widetilde{O}_{\varepsilon}(\operatorname% {NRD}(C)\cdot\log|C|)roman_SPR ( italic_C , italic_ε ) ≤ over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ⋅ roman_log | italic_C | ) (see Theorem 3.1), which for CSPs corresponds to an extra factor of the number of variables n𝑛nitalic_n. The key technical insight (Lemma 3.3) is that for all d∈[m]𝑑delimited-[]𝑚d\in[m]italic_d ∈ [ italic_m ], the set of codewords of C𝐶Citalic_C with Hamming weight at most d𝑑ditalic_d (denoted by C≤dsubscript𝐶absent𝑑C_{\leq d}italic_C start_POSTSUBSCRIPT ≤ italic_d end_POSTSUBSCRIPT) has total support size at most d⋅NRD⁡(C)⋅𝑑NRD𝐶d\cdot\operatorname{NRD}(C)italic_d ⋅ roman_NRD ( italic_C ). This can proved inductively by noticing that dropping a suitable non-redundant set of coordinates decreases the Hamming weight of every codeword of C𝐶Citalic_C by at least one. With this lemma, we can recursively construct a sparsifier as follows, similar to the divide-and-conquer framework in [KPS24a, KPS24b] for linear codes. Pick d≈Θ~ε⁢(log⁡|C|)𝑑subscript~Θ𝜀𝐶d\approx\widetilde{\Theta}_{\varepsilon}(\log|C|)italic_d ≈ over~ start_ARG roman_Θ end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_log | italic_C | ) and let I⊆[m]𝐼delimited-[]𝑚I\subseteq[m]italic_I ⊆ [ italic_m ] be the support of C≤dsubscript𝐶absent𝑑C_{\leq d}italic_C start_POSTSUBSCRIPT ≤ italic_d end_POSTSUBSCRIPT. Every i∈I𝑖𝐼i\in Iitalic_i ∈ italic_I is given weight 1111 in our sparsifier. For the rest of [m]delimited-[]𝑚[m][ italic_m ], let J⊆[m]∖I𝐽delimited-[]𝑚𝐼J\subseteq[m]\setminus Iitalic_J ⊆ [ italic_m ] ∖ italic_I be a subsample where each i∈[m]∖I𝑖delimited-[]𝑚𝐼i\in[m]\setminus Iitalic_i ∈ [ italic_m ] ∖ italic_I is kept independently with probability 1/3131/31 / 3. Using a standard Chernoff bound, we can show that with positive111111We only need positive probability since we are focused on existence. This can easily be amplified to 1−1/mΩ⁢(1)11superscript𝑚Ω11-1/m^{\Omega(1)}1 - 1 / italic_m start_POSTSUPERSCRIPT roman_Ω ( 1 ) end_POSTSUPERSCRIPT probability by making d𝑑ditalic_d a factor of log⁡m𝑚\log mroman_log italic_m bigger. In applications to CSPs, the main algorithmic bottleneck is (approximately) finding I𝐼Iitalic_I, which appears to be similar in difficulty to an open problem in CSP kernelization (see Section 1.7). probability the following holds for all c∈C𝑐𝐶c\in Citalic_c ∈ italic_C: 3⁢Ham⁡(c|J)+Ham⁡(c|I)∈[1−ε2⁢log2⁡m,1+ε2⁢log2⁡m]⋅Ham⁡(c).3Hamevaluated-at𝑐𝐽Hamevaluated-at𝑐𝐼⋅1𝜀2subscript2𝑚1𝜀2subscript2𝑚Ham𝑐3\operatorname{Ham}(c|_{J})+\operatorname{Ham}(c|_{I})\in\left[1-\frac{% \varepsilon}{2\log_{2}m},1+\frac{\varepsilon}{2\log_{2}m}\right]\cdot% \operatorname{Ham}(c).3 roman_Ham ( italic_c | start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT ) + roman_Ham ( italic_c | start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ) ∈ [ 1 - divide start_ARG italic_ε end_ARG start_ARG 2 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m end_ARG , 1 + divide start_ARG italic_ε end_ARG start_ARG 2 roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m end_ARG ] ⋅ roman_Ham ( italic_c ) . By induction, we can find a O~ε′⁢(NRD⁡(C′)⋅log⁡|C′|)subscript~𝑂superscript𝜀′⋅NRDsuperscript𝐶′superscript𝐶′\widetilde{O}_{\varepsilon^{\prime}}(\operatorname{NRD}(C^{\prime})\cdot\log|C% ^{\prime}|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( roman_NRD ( italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) ⋅ roman_log | italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | ) ε′superscript𝜀′\varepsilon^{\prime}italic_ε start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT-sparsifier for C′:=C|Jassignsuperscript𝐶′evaluated-at𝐶𝐽C^{\prime}:=C|_{J}italic_C start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT := italic_C | start_POSTSUBSCRIPT italic_J end_POSTSUBSCRIPT with ε′:=(1−1/log2⁡m)⁢εassignsuperscript𝜀′11subscript2𝑚𝜀\varepsilon^{\prime}:=(1-1/\log_{2}m)\varepsilonitalic_ε start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT := ( 1 - 1 / roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_m ) italic_ε. Scaling this sparsifier by 3333 and adding weights for I𝐼Iitalic_I gives us an ε𝜀\varepsilonitalic_ε-sparsifier of C𝐶Citalic_C. Entropy-based Sparsification. The key inefficiency of the O~ε⁢(NRD⁡(C)⋅log⁡|C|)subscript~𝑂𝜀⋅NRD𝐶𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C)\cdot\log|C|)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ⋅ roman_log | italic_C | ) bound is that the use of Lemma 3.3 is too conservative. For the purposes of this overview, assume that all codewords of C𝐶Citalic_C have the same Hamming weight d≈NRD⁡(C)𝑑NRD𝐶d\approx\operatorname{NRD}(C)italic_d ≈ roman_NRD ( italic_C ) as that is is the most representative case. Naively, Lemma 3.3 says we should set aside d2superscript𝑑2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT coordinates of [m]delimited-[]𝑚[m][ italic_m ] to “sparsify” all codewords of weight d𝑑ditalic_d. However, we can give a heuristic argument that far fewer than d2superscript𝑑2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT of these potential coordinates contain useful information for our sparsifier. Assume without loss of generality that the support of C𝐶Citalic_C lies in [d2]delimited-[]superscript𝑑2[d^{2}][ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]. For each i∈[d2]𝑖delimited-[]superscript𝑑2i\in[d^{2}]italic_i ∈ [ italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ], let pisubscript𝑝𝑖p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT be the probability that a codeword c∈C𝑐𝐶c\in Citalic_c ∈ italic_C selected uniformly at random has ci=1subscript𝑐𝑖1c_{i}=1italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1. Since each codeword of C𝐶Citalic_C has Hamming weight d𝑑ditalic_d, we have that p1+⋯+pd2=dsubscript𝑝1⋯subscript𝑝superscript𝑑2𝑑p_{1}+\cdots+p_{d^{2}}=ditalic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ⋯ + italic_p start_POSTSUBSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = italic_d. Thus, the average value of pisubscript𝑝𝑖p_{i}italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is 1/d1𝑑1/d1 / italic_d. Consider the case in which each pi=O⁢(1/d)subscript𝑝𝑖𝑂1𝑑p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ). In particular, no coordinate is distinguishing itself as a “must” to add to the sparsifier. A priori, the size of C𝐶Citalic_C may be exp⁡(Ω~⁢(d))~Ω𝑑\exp(\widetilde{\Omega}(d))roman_exp ( over~ start_ARG roman_Ω end_ARG ( italic_d ) ), so we cannot immediately use Chernoff bounds to analyze a random subsampling of the coordinates. To get around this issue, we need to prove a much stronger upper bound on the size of C𝐶Citalic_C, similar to Benczúr and Karger’s cut-counting bound [BK96] and its adaptation to linear codes [KPS24a, KPS24b]. However, we use an entirely new method for proving such bounds based on the entropy method Gilmer [Gil22] developed to prove the union-closed sets conjecture up to a constant factor. In our context, pick t=Θ~⁢(d)𝑡~Θ𝑑t=\widetilde{\Theta}(d)italic_t = over~ start_ARG roman_Θ end_ARG ( italic_d ) and sample uniformly and independently t𝑡titalic_t codewords c1,…,ct∈Csubscript𝑐1…subscript𝑐𝑡𝐶c_{1},\ldots,c_{t}\in Citalic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_c start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ italic_C. Let c𝑐citalic_c be the bitwise OR of these t𝑡titalic_t codewords, and let 𝒟𝒟\mathcal{D}caligraphic_D be the distribution of c𝑐citalic_c over {0,1}d2superscript01superscript𝑑2\{0,1\}^{d^{2}}{ 0 , 1 } start_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT (recall that the weight d𝑑ditalic_d codewords are supported on d2superscript𝑑2d^{2}italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT coordinates). Since each pi=O⁢(1/d)subscript𝑝𝑖𝑂1𝑑p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ), by adapting Gilmer’s method (or more precisely, a refinement due to Sawin [Saw23]), we can show the entropy of 𝒟𝒟\mathcal{D}caligraphic_D is at least Θ~⁢(t)=Θ~⁢(d)~Θ𝑡~Θ𝑑\widetilde{\Theta}(t)=\widetilde{\Theta}(d)over~ start_ARG roman_Θ end_ARG ( italic_t ) = over~ start_ARG roman_Θ end_ARG ( italic_d ) times the entropy of the uniform distribution over C𝐶Citalic_C (i.e., log2⁡|C|subscript2𝐶\log_{2}|C|roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C |)–a similar inequality appears in [Wak24]. To apply this fact, observe that each sample of 𝒟𝒟\mathcal{D}caligraphic_D lies in the “OROR\operatorname{OR}roman_OR-closure” of C𝐶Citalic_C (denoted by spanOR⁡(C)subscriptspanOR𝐶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C )). As such, the entropy of 𝒟𝒟\mathcal{D}caligraphic_D is at most log⁡|spanOR⁡(C)|subscriptspanOR𝐶\log\lvert\operatorname{span}_{\operatorname{OR}}(C)\rvertroman_log | roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ) |, which by the Sauer-Shelah-Peres lemma is at most (up to log factors) the VC dimension of spanOR⁡(C)subscriptspanOR𝐶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ). It is easily seen that the VC dimension of spanOR⁡(C)subscriptspanOR𝐶\operatorname{span}_{\operatorname{OR}}(C)roman_span start_POSTSUBSCRIPT roman_OR end_POSTSUBSCRIPT ( italic_C ) equals the non-redundancy of C𝐶Citalic_C [BCK20]. Therefore, we have proved that Θ~⁢(t)⋅log2⁡(C)≤O~⁢(NRD⁡(C))⋅~Θ𝑡subscript2𝐶~𝑂NRD𝐶\widetilde{\Theta}(t)\cdot\log_{2}(C)\leq\widetilde{O}(\operatorname{NRD}(C))over~ start_ARG roman_Θ end_ARG ( italic_t ) ⋅ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_C ) ≤ over~ start_ARG italic_O end_ARG ( roman_NRD ( italic_C ) ). Since t≈d≈NRD⁡(C)𝑡𝑑NRD𝐶t\approx d\approx\operatorname{NRD}(C)italic_t ≈ italic_d ≈ roman_NRD ( italic_C ), C𝐶Citalic_C is actually at most quasipolynomial in size! Thus we can now use a Chernoff bound to prove that C𝐶Citalic_C can be subsampled to O~ε⁢(d)subscript~𝑂𝜀𝑑\widetilde{O}_{\varepsilon}(d)over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( italic_d ) coordinates while approximately preserving all Hamming weights. Recall this discussion was purely about the “uniform” case pi=O⁢(1/d)subscript𝑝𝑖𝑂1𝑑p_{i}=O(1/d)italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_O ( 1 / italic_d ). In general, we apply minimax theorem to prove the following “skewed” versus “sparse” dichotomy (see Proposition 4.14): for every code C𝐶Citalic_C and parameter choice θ≥1𝜃1\theta\geq 1italic_θ ≥ 1 there is either a probability distribution 𝒫𝒫\mathcal{P}caligraphic_P over C𝐶Citalic_C for which each coordinate equals 1111 with probability at most 1/θ1𝜃1/\theta1 / italic_θ (i.e., 𝒫𝒫\mathcal{P}caligraphic_P is “θ𝜃\thetaitalic_θ-sparse”); or, there is a probability distribution 𝒬𝒬\mathcal{Q}caligraphic_Q over the coordinates of C𝐶Citalic_C such that for every (nonzero) c∈C𝑐𝐶c\in Citalic_c ∈ italic_C, we have that 𝒬𝒬\mathcal{Q}caligraphic_Q’s measure of supp⁡(c)supp𝑐\operatorname{supp}(c)roman_supp ( italic_c ) is at least 1/θ1𝜃1/\theta1 / italic_θ (i.e., 𝒬𝒬\mathcal{Q}caligraphic_Q is a “θ𝜃\thetaitalic_θ-cover.”) For a suitable choice of θ𝜃\thetaitalic_θ, we repeatedly apply Proposition 4.14 to recursively build the sparsifier: in the θ𝜃\thetaitalic_θ-sparse case, we use the entropy method to prove that a “small” number of codewords of C𝐶Citalic_C can be removed to put us in the θ𝜃\thetaitalic_θ-cover case (see Lemma 4.15); and in the θ𝜃\thetaitalic_θ-cover case, we sample from the θ𝜃\thetaitalic_θ-cover to get a coordinate to add to our sparsifier. This procedure culminates in showing that we can set aside O~ε⁢(NRD⁡(C))subscript~𝑂𝜀NRD𝐶\widetilde{O}_{\varepsilon}(\operatorname{NRD}(C))over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) ) coordinates to have weight 1111 in our sparsifier with the remainder of the code being sufficiently sparse that subsampling can be used (Theorem 4.16). Note that the statement of Theorem 4.16 resembles the analogous decompositions for linear codes [KPS24a, KPS24b]. However, their method found all the coordinates to set aside in “one pass,” whereas we iteratively understand the dense and sparse structure of our non-linear code. With Theorem 4.16 in hand, we construct the sparsifier with a recursive argument similar to that of Theorem 3.1. As mentioned earlier, extended these ideas to weighted sparsification (Theorem 1.4) is relatively straightforward. We adapt a weight-binning argument of [KPS24b] by essentially computing an (unweighted) sparsifier for each group of coordinates that is similar in weight (within poly⁡(m)poly𝑚\operatorname{poly}(m)roman_poly ( italic_m )). We then analyze the aggregated size of these sparsifiers by comparing the sum of the non-redundancies of the groups of coordinates to the chain length of the code. Connections to Matching Vector Families. We now switch gears to briefly discussing the key ideas behind Theorem 1.3. Let G:=ℤ/3⁢ℤassign𝐺ℤ3ℤG:=\mathbb{Z}/3\mathbb{Z}italic_G := blackboard_Z / 3 blackboard_Z and recall that 3⁢L⁢I⁢NG={(x,y,z)∣x+y+z=0}subscript3LIN𝐺conditional-set𝑥𝑦𝑧𝑥𝑦𝑧0\operatorname{3LIN}_{G}=\{(x,y,z)\mid x+y+z=0\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_z ) ∣ italic_x + italic_y + italic_z = 0 } and 3⁢L⁢I⁢NG∗=3⁢L⁢I⁢NG∖{(0,0,0)}subscriptsuperscript3LIN𝐺subscript3LIN𝐺000\operatorname{3LIN}^{*}_{G}=\operatorname{3LIN}_{G}\setminus\{(0,0,0)\}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT = start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ∖ { ( 0 , 0 , 0 ) }. It is well-known that since 3⁢L⁢I⁢NGsubscript3LIN𝐺\operatorname{3LIN}_{G}start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT is an affine predicate, we have that NRD⁡(3⁢L⁢I⁢NG,n)=Θ⁢(n)NRDsubscript3LIN𝐺𝑛Θ𝑛\operatorname{NRD}(\operatorname{3LIN}_{G},n)=\Theta(n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) = roman_Θ ( italic_n ), which is much smaller than our bound on NRD⁡(3⁢L⁢I⁢NG∗,n)NRDsubscriptsuperscript3LIN𝐺𝑛\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ). As such, we prove that to understand the asymptotics of NRD⁡(3⁢L⁢I⁢NG∗,n)NRDsubscriptsuperscript3LIN𝐺𝑛\operatorname{NRD}(\operatorname{3LIN}^{*}_{G},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT , italic_n ) it suffices to look at specially-structured non-redundant instances. Recall that an instance Ψ:=(X,Y)assignΨ𝑋𝑌\Psi:=(X,Y)roman_Ψ := ( italic_X , italic_Y ) of CSP⁡(3⁢L⁢I⁢NG∗)CSPsubscriptsuperscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}^{*}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) is non redundant if for every clause y∈Y𝑦𝑌y\in Yitalic_y ∈ italic_Y there is an assignment σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT which satisfies every clause of ΨΨ\Psiroman_Ψ except y𝑦yitalic_y. We show that with at most an additive Θ⁢(n)Θ𝑛\Theta(n)roman_Θ ( italic_n ) change in size, we can assume that σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT maps y𝑦yitalic_y to (0,0,0)000(0,0,0)( 0 , 0 , 0 ). In other words, each σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT is a satisfying assignment to ΨΨ\Psiroman_Ψ when viewed as an instance of CSP⁡(3⁢L⁢I⁢NG)CSPsubscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) (see Proposition 6.3). This idea of “conditional” non-redundancy abstracts and generalizes an approach from [BCK20]. Since the set of solutions to an instance of CSP⁡(3⁢L⁢I⁢NG)CSPsubscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ) form a vector space (of some dimension, say d𝑑ditalic_d) over 𝔽3subscript𝔽3\mathbb{F}_{3}blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, we can think of each variable x∈X𝑥𝑋x\in Xitalic_x ∈ italic_X of ΨΨ\Psiroman_Ψ as a vector vx∈𝔽3dsubscript𝑣𝑥superscriptsubscript𝔽3𝑑v_{x}\in\mathbb{F}_{3}^{d}italic_v start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ∈ blackboard_F start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and the assignments as linear maps on the vectors. Because we are studying satisfying assignment to CSP⁡(3⁢L⁢I⁢NG)CSPsubscript3LIN𝐺\operatorname{CSP}(\operatorname{3LIN}_{G})roman_CSP ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT ), these vectors are highly structured: for each y:=(x1,x2,x3)∈Yassign𝑦subscript𝑥1subscript𝑥2subscript𝑥3𝑌y:=(x_{1},x_{2},x_{3})\in Yitalic_y := ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) ∈ italic_Y, we have that vx1+vx2+vx3=0subscript𝑣subscript𝑥1subscript𝑣subscript𝑥2subscript𝑣subscript𝑥30v_{x_{1}}+v_{x_{2}}+v_{x_{3}}=0italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT + italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = 0. Further, σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT can be viewed as a linear map taking each of vx1,vx2,vx3subscript𝑣subscript𝑥1subscript𝑣subscript𝑥2subscript𝑣subscript𝑥3v_{x_{1}},v_{x_{2}},v_{x_{3}}italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUBSCRIPT to 00, while mapping at least one vector in every other triple in Y𝑌Yitalic_Y to a nonzero value. We call this family of vectors together with these assignments a G𝐺Gitalic_G-ensemble (Definition 6.5), and note that it bears a strong resemblance to matching vector families. In particular, we adapt techniques used by Dvir, Gopalan, and Yekhanin [DGY11] for constraining the size of matching vector families to give nontrivial upper and lower bounds on the size of G𝐺Gitalic_G-ensembles. For the lower bound (Theorem 6.8), we directly construct a non-redundant instance with Ω⁢(n1.5)Ωsuperscript𝑛1.5\Omega(n^{1.5})roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) clauses. The proof is self-contained and elementary. The upper bound (Theorem 6.13) is slightly more technical. We break the proof into cases based on whether the embedding dimension d𝑑ditalic_d of the vectors is small (d=O~⁢(n0.4)𝑑~𝑂superscript𝑛0.4d=\widetilde{O}(n^{0.4})italic_d = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 0.4 end_POSTSUPERSCRIPT )) or large (d=Ω~⁢(n0.4)𝑑~Ωsuperscript𝑛0.4d=\widetilde{\Omega}(n^{0.4})italic_d = over~ start_ARG roman_Ω end_ARG ( italic_n start_POSTSUPERSCRIPT 0.4 end_POSTSUPERSCRIPT )). For small d𝑑ditalic_d, we adapt the polynomial method used in [DGY11] to prove there can be at most O⁢(d4)=O~⁢(n1.6)𝑂superscript𝑑4~𝑂superscript𝑛1.6O(d^{4})=\widetilde{O}(n^{1.6})italic_O ( italic_d start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) non-redundant clauses. On the other hand, when d𝑑ditalic_d is large, we ignore the assignments σysubscript𝜎𝑦\sigma_{y}italic_σ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT and use a careful induction (Lemma 6.11) to show that the geometry of the vectors imply that some x∈X𝑥𝑋x\in Xitalic_x ∈ italic_X is a member of at most O~⁢(n/d)=O~⁢(n0.6)~𝑂𝑛𝑑~𝑂superscript𝑛0.6\widetilde{O}(n/d)=\widetilde{O}(n^{0.6})over~ start_ARG italic_O end_ARG ( italic_n / italic_d ) = over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 0.6 end_POSTSUPERSCRIPT ) clauses, thereby leading to a bound of at most O~⁢(n1.6)~𝑂superscript𝑛1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) clauses total. Closing the gap between Ω⁢(n1.5)Ωsuperscript𝑛1.5\Omega(n^{1.5})roman_Ω ( italic_n start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ) and O~⁢(n1.6)~𝑂superscript𝑛1.6\widetilde{O}(n^{1.6})over~ start_ARG italic_O end_ARG ( italic_n start_POSTSUPERSCRIPT 1.6 end_POSTSUPERSCRIPT ) for NRD⁡(3⁢L⁢I⁢Nℤ/3⁢ℤ∗,n)NRDsuperscriptsubscript3LINℤ3ℤ𝑛\operatorname{NRD}(\operatorname{3LIN}_{\mathbb{Z}/3\mathbb{Z}}^{*},n)roman_NRD ( start_OPFUNCTION 3 roman_L roman_I roman_N end_OPFUNCTION start_POSTSUBSCRIPT blackboard_Z / 3 blackboard_Z end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_n ) is a tantalizing open question. 1.7 Open Questions We conclude the introduction with a few directions of further study. See Section 7 and Section 9 for a more thorough discussion of directions for future exploration. • Making Theorem 1.2 efficient. Note that the underlying construction for Theorem 1.1, if made algorithmic, runs in polynomial time with respect to the size of the code, yielding an exp⁡(O⁢(n))𝑂𝑛\exp(O(n))roman_exp ( italic_O ( italic_n ) )-time algorithm121212This is already nontrivial, as a naive guess-and-check algorithm would require exp⁡(O~⁢(NRD⁡(R¯,n)))~𝑂NRD¯𝑅𝑛\exp(\widetilde{O}(\operatorname{NRD}(\overline{R},n)))roman_exp ( over~ start_ARG italic_O end_ARG ( roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ) ) ) time. for Theorem 1.2. The primary barrier in constructing our sparsifier in poly⁡(n)poly𝑛\operatorname{poly}(n)roman_poly ( italic_n ) time is the fact that an efficient sparsifier is also a kernelization algorithm, but kernelizing every CSP instance to its non-redundancy is a significant open question in the kernelization community [Car22]. • Computing NRD⁡(R,n)NRD𝑅𝑛\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ). For a general predicate R⊆Dr𝑅superscript𝐷𝑟R\subseteq D^{r}italic_R ⊆ italic_D start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, there is no simple (even conjectured) expression for NRD⁡(R,n)NRD𝑅𝑛\operatorname{NRD}(R,n)roman_NRD ( italic_R , italic_n ). In fact, even determining when NRD⁡(R,n)=Θ⁢(n)NRD𝑅𝑛Θ𝑛\operatorname{NRD}(R,n)=\Theta(n)roman_NRD ( italic_R , italic_n ) = roman_Θ ( italic_n ) is an open question (e.g., [BCK20, Car22]). In Section 7, we explore a number of predicates from the various parts of the literature whose status is unresolved, including a predicate we categorize as the “simplest unresolved predicate.” • Non-redundancy versus Chain Length. Recall we show that unweighted sparsification is closely tied to non-redundancy while weighted sparsification is closely tied to chain length. For non-linear codes, NRDNRD\operatorname{NRD}roman_NRD and CLCL\operatorname{CL}roman_CL can be very different (e.g., Example 8.8), but the relationship for CSPs is unknown [BCK20, Car22]. In particular, it seems quite possible that there exists a CSP predicate R𝑅Ritalic_R for which wSPR⁡(R,n,ε)/SPR⁡(R,n,ε)=nΩ⁢(1).wSPR𝑅𝑛𝜀SPR𝑅𝑛𝜀superscript𝑛Ω1\operatorname{wSPR}(R,n,\varepsilon)/\operatorname{SPR}(R,n,\varepsilon)=n^{% \Omega(1)}.roman_wSPR ( italic_R , italic_n , italic_ε ) / roman_SPR ( italic_R , italic_n , italic_ε ) = italic_n start_POSTSUPERSCRIPT roman_Ω ( 1 ) end_POSTSUPERSCRIPT . • Average-case behavior. From Theorem 1.2, we know that every instance CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) has a sparsifier of size approximately its own non-redundancy, even if that value is much smaller than NRD⁡(R¯,n)NRD¯𝑅𝑛\operatorname{NRD}(\overline{R},n)roman_NRD ( over¯ start_ARG italic_R end_ARG , italic_n ). As such, it may be possible that ‘average’ instances of CSP⁡(R)CSP𝑅\operatorname{CSP}(R)roman_CSP ( italic_R ) admit sparsifiers much smaller than the worst case. 1.8 Organization In Section 2, we prove some basic facts about non-redundancy, sparsification and their relationship. In Section 3, we give a straightforward proof that SPR⁡(C,ε)=O~ε⁢(NRD⁡(C)⁢log2⁡|C|)SPR𝐶𝜀subscript~𝑂𝜀NRD𝐶subscript2𝐶\operatorname{SPR}(C,\varepsilon)=\widetilde{O}_{\varepsilon}(\operatorname{% NRD}(C)\log_{2}|C|)roman_SPR ( italic_C , italic_ε ) = over~ start_ARG italic_O end_ARG start_POSTSUBSCRIPT italic_ε end_POSTSUBSCRIPT ( roman_NRD ( italic_C ) roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | italic_C | ). In Section 4, we prove Theorem 1.2 by connecting CSP sparsification to non-redundancy via Gilmer’s entropy method. In Section 5, we discuss the immediate applications of Theorem 1.2 based on what is known about non-redundancy in the literature. In Section 6, we bound the non-redundancy of a family of predicates via methods related to matching vector families. In Section 7, we give examples of CSP predicates in the literature whose non-redundancy is unresolved. In Section 8, we extend Theorem 1.2 to weighted instances. In Section 9, we wrap up with other directions of exploration. 1.9 Acknowledgments We thank Libor Barto, Dmitry Zhuk, Madhu Sudan, and Aaron Putterman for valuable conversations. This research was supported in part by a Simons Investigator award and NSF grant CCF-2211972."
https://arxiv.org/html/2411.03069v1,Conformance Games for Graded Semantics,"Game-theoretic characterizations of process equivalences traditionally form a central topic in concurrency; for example, most equivalences on the classical linear-time / branching-time spectrum come with such characterizations. Recent work on so-called graded semantics has led to a generic behavioural equivalence game that covers the mentioned games on the linear-time / branching-time spectrum and moreover applies in coalgebraic generality, and thus instantiates also to equivalence games on systems with non-relational branching type (probabilistic, weighted, game-based etc.). In the present work, we generalize this approach to cover other types of process comparison beyond equivalence, such as behavioural preorders or pseudometrics. At the most general level, we abstract such notions of behavoiural conformance in terms of topological categories, and later specialize to conformances presented as relational structures to obtain a concrete syntax. We obtain a sound and complete generic game for behavioural conformances in this sense. We present a number of instantiations, obtaining game characterizations of, e.g., trace inclusion, probabilistic trace distance, bisimulation topologies, and simulation distances on metric labelled transition systems.","Game-theoretic characterizations of equivalences have a firm place in the study of concurrent systems. A well-known example is the classical Spoiler-Duplicator game for bisimilarity, which is played on pairs of states in labelled transition systems (LTS), and in which Duplicator has a winning strategy at a pair of states iff the two states are bisimilar [Stirling99]. One benefit of such games is that they provide witnesses for both equivalence and inequivalence, in the shape of winning strategies for the respective player; from winning strategies of Spoiler, one can in fact often extract distinguishing formulae in suitable characteristic modal logics (e.g. [KoenigEA20]). Besides the mentioned branching-time bisimulation game, one has behavioural equivalence games for most of the equivalences on the linear-time / branching-time spectrum of process equivalences on LTS [Glabbeek01]. Similar spectra of behavioural equivalences live over other system types beyond relational transition systems, such as probabilistic [JouSmolka90], weighted, or neighbourhood-based systems. We generally refer to equivalences on such spectra as the semantics of systems (bisimulation semantics, trace semantics etc.). Recently, a general treatment of behavioural equivalence games has been given that works generically over both the both the system type and the system semantics [DBLP:conf/lics/FordMSB022]. This is achieved by on the one hand encapsulating the system type as a set functor following the paradigm of universal coalgebra [Rutten00] and on the other hand by abstracting the system semantics in the framework of graded semantics [DBLP:conf/calco/MiliusPS15], which is based on mapping the type functor into a graded monad [Smirnov08]. Now the behaviour of concurrent systems can be compared in various ways that go beyond equivalence. For instance, even classically, (pre-)order-theoretic notions such as simulation or trace inclusion have played a key role in system verification; and beyond two-valued comparisons, there has been long-standing interest in behavioural distances [GiacaloneEA90]. Following recent usage [BeoharEA24], we refer to such more general ways of comparing systems as behavioural conformances. In a nutshell, the contribution of the present work is to provide a generic game-theoretic characterization of behavioural conformances, parametrizing over the system type (given as a functor), the system semantics (given as a graded semantics), and additionally the type of behavioural conformance. Our technical assumption on behavioural conformances is that they form a topological category [AHS90] (essentially equivalently, a CLat⊓subscriptCLatsquare-intersection\textbf{CLat}_{\sqcap}CLat start_POSTSUBSCRIPT ⊓ end_POSTSUBSCRIPT-fibration [DBLP:conf/lics/KomoridaKHKH19]). We present two variants of the game, one where a finite number n𝑛nitalic_n is determined beforehand and the game is then played for exactly n𝑛nitalic_n rounds, and one where the game is played for an infinite number of rounds (or until a player gets stuck). Under the assumptions of the framework, we show that the former characterizes the finite-depth behavioural conformance induced by the given graded semantics, while the latter characterizes an infinite-depth behavioural conformance induced from the graded semantics under additional assumptions saying essentially that no behaviour can be observed without taking at least one evolution step in the system. Departing from the most general setup, we subsequently refine the game under the assumption that the toplogical category modelling the behavioural conformance is a category of relational structures [DBLP:conf/calco/FordMS21], which holds, for instance, for behavioural preorders and behavioural (pseudo-)metrics. The coalgebraic codensity games pioneered by Komorida et al. [DBLP:conf/lics/KomoridaKHKH19] are similarly parametrized over the type of behavioural conformances via topological categories / CLat⊓subscriptCLatsquare-intersection\textbf{CLat}_{\sqcap}CLat start_POSTSUBSCRIPT ⊓ end_POSTSUBSCRIPT-fibrations. One key difference with our games is that codensity games so far apply only to the branching-time case, while we are interested primarily in coarser behavioural conformances on generalized linear-time / branching-time spectra. That said, our generic games do apply also in the branching-time case, and then instantiate to games that are markedly different from codensity games. Indeed, our games are, roughly speaking, dual to codensity games in that codensity games work with modal observations on states, while our games concern the way behaviours of states are constructed in an algebraic sense. We apply this framework to a number case studies, obtaining game-theoretic characterizations of classical trace inclusion of LTS; bisimulation toppologies; quantitative similarity of metric LTS; and probabilistic trace semantics. Related Work We have already discussed work on coalgebraic codensity games [DBLP:conf/lics/KomoridaKHKH19]. Similarly, Kupke and Rot [KupkeRot21] give a coalgebraic treatment of coinductive predicates in terms of fibrations, generalizing in particular behavioural distances but focusing on the branching-time setting. By the fixpoint nature of behavioural equivalence, our infinite game relates to some degree to general fixpoint games [BaldanEA19, BaldanEA20]. Our topological version of the powerset functor that appears in the case study on bisimulation topologies owes ideas to the Vietoris topoology (via the use of hit sets), and more broadly to work on Vietoris bisimulations [BezhanishviliEA10]. The treatment of spectra of behavioural metrics via graded semantics goes back to work on characteristic quantitative modal logics [ForsterEA24, ForsterEA23]. This work joins a strand of work on the coalgebraic treatment of behavioural distances (e.g. [BreugelWorrell05, BaldanEA18, WildSchroder22]) and the treatment of spectra of behavioural equivalences via graded semantics [DBLP:conf/calco/MiliusPS15, DBLP:conf/concur/DorschMS19, DBLP:conf/lics/FordMSB022]. Graded semantics essentially subsumes earlier coalgebraic treatments of linear-time equivalences based on Kleisli [HasuoEA07] and Eilenberg-Moore categories [JacobsEA15], respectively. The Kleisli and Eilenberg-Moore setups are alternatively subsumed by an approach based on corecursive algebras [RotEA21]. Spectra of behavioural metrics have been studied in a highly general approach based on Galois connections, which, broadly speaking, subsumes a wide range of examples but leaves more work to concrete instances [BeoharEA23, BeoharEA24]. For the presentation of graded monads on relational structures, we build on work on presenting monads [DBLP:conf/lics/MardarePP16] and graded monads on metric spaces [ForsterEA23] and posets [DBLP:journals/mscs/AdamekFMS21, DBLP:conf/lics/FordMS21] as well as generalizations to categories of relational structures [DBLP:conf/calco/FordMS21, DBLP:phd/dnb/Ford23]."
https://arxiv.org/html/2411.02977v1,Relating Apartness andBranching Bisimulation Games††thanks:This research is partially supported by the Royal Society International Exchange grant (IES\R3\223092). The third author was also partially supported by EPSRC NIA grant EP/X019373/1.,"Geuvers and Jacobs (LMCS 2021) formulated the notion of apartness relation on state-based systems modelled as coalgebras. In this context apartness is formally dual to bisimilarity, and gives an explicit proof system for showing that certain states are not bisimilar. In the current paper, we relate apartness to another classical element of the theory of behavioural equivalences: that of turn-based two-player games. Studying both strong and branching bisimilarity, we show that winning configurations for the Spoiler player correspond to apartness proofs, for transition systems that are image-finite (in the case of strong bisimilarity) and finite (in the case of branching bisimilarity).","Bisimilarity is one of the fundamental notions of equivalence [12], encoding when two states of a labelled transition system (LTS) have the same behaviour. Bisimilarity is well studied in the literature from both logical and game-theoretic viewpoints. For instance, the classical Hennessy-Milner characterisation theorem [5] states that two states of an image-finite LTS are bisimilar if and only if they satisfy the same set of modal formulas. Similarly, the well-known result by Stirling [9] states that two states of an LTS are bisimilar if and only if Duplicator has a winning strategy from this pair of states in the Spoiler/Duplicator bisimulation game. These two viewpoints have almost become a standard in the sense that it is expected that similar characterisation results hold, whenever a new notion of behavioural equivalence is proposed. Orthogonally to these logical and game-theoretic viewpoints, in the recent work of Geuvers and Jacobs [4], a dual approach to bisimilarity is postulated in terms of apartness in transition systems. Instead of describing when two states are behaviourally equivalent, as in bisimilarity, the motive of an apartness relation is in showing differences in behaviour. More formally, where bisimilarity is a coinductive characterisation of behavioural equivalence, apartness inductively provide a proof system for constructing witnesses of such differences. Geuvers and Jacobs propose a general coalgebraic formulation of apartness, and show how this yields concrete proof systems for deterministic automata, labelled transition systems and streams. They also develop versions of apartness for weak and branching bisimilarity. This research strand allows us to study connections between modal logic, games and bisimilarity through the lens of apartness. In particular, the Hennessy-Milner theorem says that two states are apart if and only if there is a distinguishing formula, i.e., a formula that holds in one state but not the other. For games, a natural formulation is that two states are apart if and only if Spoiler has a winning strategy. Both results hold by simply observing that bisimilarity is the complement of apartness [4]. However, such an approach is rather implicit: it does not really show how to move between apartness proofs, distinguishing formulas and winning strategies for Spoiler. The relation between apartness proofs and distinguishing formulas is studied in [3], and revisited in an abstract coalgebraic setting in [11]. In the current paper, we focus on the relation between apartness and bisimulation games. One of the main messages of this paper is the following dichotomy: bisimulations correspond to the winning strategies for Duplicator, while apartness relations correspond to winning strategies for Spoiler. We explicitly relate winning configurations for Spoiler to apartness proofs. We first develop the correspondence between apartness relations and Spoiler strategies for strong bisimilarity, and then move on to branching bisimilarity [13], following the game characterisation in [15]. Our proofs rely on the assumption that Duplicator has only finitely many possible moves; this is true under the assumption that the LTS is image-finite, in the case of strong bisimilarity. For branching bisimilarity, since Duplicator can answer with a sequence of τ𝜏\tauitalic_τ moves, we make a stronger assumption for our proof strategy to work: that the LTS is finite."
https://arxiv.org/html/2411.03231v2,Formal Logic-guided Robust Federated Learning against Poisoning Attacks,"Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning. However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance. These attacks can introduce critical malfunctions, such as biased predictions or reduced accuracy, undermining the integrity and robustness of the global model. Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems. However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data. These tasks, which often involve sequential dependencies and temporal patterns, have been largely overlooked in the context of poisoning attack defenses.In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants. Based on our investigation of the effectiveness of poisoning attack defenses within the Federated Time Series (FTS) domain, we pinpoint the limitations of mainstream defenses against such attacks. Unlike traditional model-centric defenses, FLORAL leverages logic reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates. Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates. Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior. Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications. Notably, FLORAL reduced the prediction error by 93.27% in the best-case scenario compared to the second-best baseline. Our code is available at https://anonymous.4open.science/r/FLORAL-Robust-FTS.","Federated Learning (FL) has emerged as a promising solution that enables using data and computing resources from multiple clients to train a shared model under the orchestration of a central server [33]. In FL, clients use their data to train the model locally and iteratively share the local updates with the server, which then combines the contributions of the participating clients to generate a global update. The security aggregation mechanism and its distinctive distributed training mode render it highly compatible with a wide range of practical applications that have stringent privacy demands [49, 59, 40, 21]. Recently, FL has been demonstrated to be efficient in time-series related tasks [10, 48, 3] to securely share knowledge of similar expertise among different tasks and protect user privacy. Although FL has many notable characteristics and has been successful in many applications [2, 21, 46, 52, 66, 22, 41], recent studies indicate that FL is fundamentally susceptible to adversarial attacks in which malicious clients manipulate the local training process to contaminate the global model [6, 55, 44]. Based on the attack’s goal, adversarial attacks can be broadly classified into untargeted and targeted attacks. The former aims to deteriorate the performance of the global model on all test samples [9, 14]; while the latter focuses on causing the model to generate false predictions following specific objectives of the adversaries [62, 6]. Figure 1: Illustration of logical verification given by benign and malicious clients’ predictions. The global property here is □(0,10]⁢(y^⁢(t)≤p1)∧□(10,20]⁢(y^⁢(t)≤p2)∧□(20,30]⁢(y^⁢(t)≤p3)∧□(30,40]⁢(y^⁢(t)≤p4)subscript□010^𝑦𝑡subscript𝑝1subscript□1020^𝑦𝑡subscript𝑝2subscript□2030^𝑦𝑡subscript𝑝3subscript□3040^𝑦𝑡subscript𝑝4\square_{(0,10]}(\hat{y}(t)\leq p_{1})\wedge\square_{(10,20]}(\hat{y}(t)\leq p% _{2})\wedge\square_{(20,30]}(\hat{y}(t)\leq p_{3})\wedge\square_{(30,40]}(\hat% {y}(t)\leq p_{4})□ start_POSTSUBSCRIPT ( 0 , 10 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ≤ italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ∧ □ start_POSTSUBSCRIPT ( 10 , 20 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ≤ italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ∧ □ start_POSTSUBSCRIPT ( 20 , 30 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ≤ italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) ∧ □ start_POSTSUBSCRIPT ( 30 , 40 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ≤ italic_p start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ). Examples of points violating this property are marked with x. Many efforts have been devoted to dealing with existing threats in FL, which can be roughly classified into two directions: robust FL aggregation [50, 47, 69, 71] and anomaly model detection. The former aims to optimize the aggregation function to limit the effects of polluted updates caused by attackers, whereas the latter attempts to identify and remove malicious updates. For instance, Xie et al. [69] presented a certified defense mechanism based on the clipping and perturbation paradigm. Other approaches focused on new estimators such as coordinate-wise median, α𝛼\alphaitalic_α-trimmed mean [72], and geometric median [50] for aggregation. The main drawback of the methods mentioned above is that polluted updates remain in the global model, reducing the model’s precision while not mitigating the attack impact [43]. Several methods have been proposed to identify and remove adversarial clients from the aggregation [9, 60, 42, 54, 17, 73]. In [60], the authors proposed a defense mechanism against poisoning attacks in collaborative learning based on the K𝐾Kitalic_K-Means algorithm. Sattler et al. [57] proposed dividing the clients’ updates into normal updates and suspicious updates based on their cosine similarities. However, most methods for identifying malicious clients proposed so far follow the majority-based paradigm in that they assume benign local model updates are a majority compared to the malicious ones; thus, polluted updates are supposed to be outliers in the distribution of all updates. Unfortunately, this hypothesis holds only if the data of the clients is IID (independent and identically distributed) and the number of malicious clients is small. Though these two approaches can mitigate poisoning attacks in FL, most of them have been evaluated primarily in the context of computer vision tasks, where image-based datasets dominate the landscape [17, 47, 44, 63]. However, FL applied to time-series data remains underexplored, particularly regarding its vulnerabilities, where adversarial attacks pose a significant threat, much like those observed in image-based datasets [23, 12, 13, 38]. Given the critical applications of time-series analysis, such as in healthcare [5, 36], financial systems [35, 34], and industrial monitoring [31, 30], ensuring the robustness and security of FL models in these scenarios is of paramount importance. Our empirical result demonstrates that these methods are not effective in the scenario of FL with time-series tasks where the data itself reflects a high level of non-iid due to the different locations where it is collected. To fill this gap, we propose FLORAL, a defense mechanism capable of mitigating poisoning attacks against Federated Time Series (FTS) under the most challenging scenarios, i.e., in the presence of heterogeneous client data and a large number of adversarial clients. Our approach is orthogonal to existing model-centric defenses. Instead, we rely on logic-based reasoning to evaluate the reliability of clients based on their behavior and resistance to poisoning attacks. This approach assesses the trustworthiness of clients by aligning their predictions with global time-series patterns. Specifically, we use symbolic reasoning to capture the logical semantics embedded in time series data, which has been shown to improve the learning process and produce more robust models for future predictions [3, 31, 29]. Our FL defense method builds on this by using symbolic reasoning to evaluate diverging intra-task logic patterns in client predictions, allowing for the detection of anomalous clients without relying solely on model similarity. This highlights the enhanced effectiveness of reasoning logic in identifying malicious behaviors in FL. The intuition behind our approach is that, after rounds of training, benign models naturally converge toward the same global objective and share consistent logical reasoning patterns, while malicious models diverge, aiming to manipulate global behavior and thereby exhibit deviant reasoning patterns. The high-level idea is visualized in Figure 1. In centralized FL, we expect the final model GTsubscript𝐺𝑇G_{T}italic_G start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT to have the minimized error on the local data, with local models converging on a unified objective [49, 27, 24]. In light of these findings, we propose FLORAL, a logic-guided defense for FL which includes three key components. First, we extract logical reasoning properties (e.g., when training models relating to traffic and driving, a dataset measuring vehicle density over time would by expected to reach an upper extreme value of, say, 100) from clients and apply hierarchical clustering to group client updates based on the logical properties of their local models. This allows us to infer the global reasoning properties that represent the system’s clients based on clustered properties. These formal logic properties rigorous assessment of the consistency and validity of client contributions by identifying deviations from expected model behaviors. This verification-based defense substantially strengthens the security of federated learning in time-series applications, where the risk of undetected adversarial behavior is particularly high due to the sensitive nature of these tasks. By optimizing for the unique challenges of time-series data, our method enhances the robustness of FL systems, providing a more reliable safeguard compared to existing defenses. Experimental results validate the effectiveness of our approach in mitigating poisoning attacks while maintaining high model performance. In summary, our contributions are specified as follows: • We introduce FLORAL — a novel poisoning-resistant defense for FL. It identifies and eliminates suspicious clients that distort the global model using logical reasoning property inference and verification. FLORAL is the first work that, to the best of our knowledge, thoroughly addresses poisoning attacks in FTS, even in the presence of a large number of compromised participants and complicated attack strategies. • We are the first to study the efficacy of existing robust FL defenses in the context of FTS and pinpoint their limitation when adapted to the time-series domain. • We conduct comprehensive experiments and in-depth studies on various datasets, FL settings, and attack scenarios to demonstrate the superiority of FLORAL over state-of-the-art defense techniques."
https://arxiv.org/html/2411.01393v1,Thoughts on sub-Turing interactive computability,"The article contains an outline of a possible new direction for Computability Logic, focused on computability without infinite memory or other impossible-to-possess computational resources. The new approach would see such resources as external rather than internal to computing devices. They could or should be accounted for explicitly in the antecedents of logical formulas expressing computational problems.","The present article lies within the framework of computability logic (CoL) — an ambitious long-term research project with a beginning but no end, initiated by the author in [9] and actively pursued since then [10, 1, 11, 13, 15, 17]. Among many characterizations of CoL’s formalism would be to say that it potentially provides a medium for communication between humans and computers, something inbetween programming languages on one hand, and the language used by humans in their intellectual activities on the other hand. Just like the former, the language of CoL is formal and thus well understood by machines; and, just like the latter, it can be relatively easily “spoken” by humans without any special expertise and training generally required for programmers. The question ‘what can be computed?’ is fundamental to computer science. CoL is about answering this question in a systematic way using logical formalism, with formulas understood as computational problems and logical operators as operations on them. The first basic issue to be clarified here is what a computational problem means. With a few exceptions in the literature, starting from Turing [16], this term usually refers to an entity that is modeled by a very simple interface between a computing agent and its environment, consisting in asking a question (input) and generating an answer (output). In other words, computational problems are understood as functions. This understanding, however, captures only a small part of our broader intuition and the reality of computational problems. Most tasks that real computers perform are interactive and not reducible to simple pairs of input/output events. In such tasks, input/output events, also called observable actions [7] by the computing agent and its environment, can be multiple and interspersed, perhaps taking place throughout the entire process of computation rather than just at the beginning (input) and the end (output) of it. Computability that CoL deals with is interactive computability, and throughout this article by a “(computational) problem” we always mean an interactive computational problem. This concept is formalized in Section 2 below. Section 3 provides a brief informal overview of operations generating complex computational problems from simpler ones, and Section 4 discusses the basic model of interactive computation used in CoL. These sections serve the purpose of establishing a background necessary for understanding the final Section 5, where the reader will find a discussion of a possible new direction into which CoL may branch: a direction that switches the attention of CoL from computability-in-principle (Turing computability) to sub-Turing comutability, where the latter does not assume the presence of (in fact) supernatural resources such as the infinite-capacity tape memory."
https://arxiv.org/html/2411.01188v1,Learning Rules Explaining Interactive Theorem Proving Tactic Prediction,"Formally verifying the correctness of mathematical proofs is more accessible than ever, however, the learning curve remains steep for many of the state-of-the-art interactive theorem provers (ITP). Deriving the most appropriate subsequent proof step, and reasoning about it, given the multitude of possibilities, remains a daunting task for novice users. To improve the situation, several investigations have developed machine learning based guidance for tactic selection. Such approaches struggle to learn non-trivial relationships between the chosen tactic and the structure of the proof state and represent them as symbolic expressions.To address these issues we (i) We represent the problem as an Inductive Logic Programming (ILP) task, (ii) Using the ILP representation we enriched the feature space by encoding additional, computationally expensive properties as background knowledge predicates, (iii) We use this enriched feature space to learn rules explaining when a tactic is applicable to a given proof state, (iv) We use the learned rules to filter the output of an existing tactic selection approach and empirically show improvement over the non-filtering approaches.","Interactive Theorem Provers (ITP), such as Coq [27], Lean [20], and Isabelle [22], are powerful tools that combine human instruction with computer verification to construct formal mathematical proofs, providing a reliable means of certification and ensuring safety in critical applications. These systems operate as follows: the user specifies a goal to prove, the initial proof state. Then the user specifies tactics (an operation transforming a proof state into proof states). Certain tactics close proof states. The proof is complete if there are no remaining open proof states, i.e., the goal has been proved. Given the complexity of ITP systems, a fully automated approach to proving user specified goals is intractable. Numerous investigations have instead focused on providing the user with guidance through tactic suggestion. The methods used in practice by ITP users are statistical machine learning methods such as k𝑘kitalic_k-nearest neighbors (k𝑘kitalic_k-NN) and naive Bayes [9]. These methods take a goal g𝑔gitalic_g, select a goal g′superscript𝑔′g^{\prime}italic_g start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT most similar goal to g𝑔gitalic_g, and rank the particular tactics relevant for solving g′superscript𝑔′g^{\prime}italic_g start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT based on their likelihood of solving g𝑔gitalic_g. Neural network and LLM-based approaches addressing the task include: CoqGym [29] trains tree neural networks to automatically construct proofs for Coq. Thor [14] combines LLMs and external symbolic solvers to search for proofs for Isabelle. LLMs are also applied to synthesising training data to enhance the performance of theorem proving [28]. Despite showing slight improvement in performance during machine learning evaluations, in practice these methods require long training for each new theory, which makes them less useful for day to day proof development. Additionally, they lack interpretability. When a user receives predictions, they may want to know why a particular tactic was chosen over another tactic to better understand what actions they should take in the future. Furthermore, guidance based on statistical learning approaches often requires propositionalisation of features, calculated based on the structure of the abstract syntax tree (AST) of a proof state [30], e.g., there is a path between nodes X and Y in tree T. For complex and precise features, pre-computation is prohibitively expensive. Moreover, logical inference is significantly influenced by the small error margins present in the statistical inferencing mechanisms of LLMs and similar models. Thus, predictions based on chained logical inferences will quickly suffer a loss of predicative accuracy [17]. In contrast to pre-computed features, we represent such features as logic programs and compute them only when needed for learning. For example, we define logic programs for the existence of two particular nodes on a path (of arbitrary length) from the root of the tree as (𝑎𝑏𝑜𝑣𝑒⁢(A⁢S⁢T,X,Y)𝑎𝑏𝑜𝑣𝑒𝐴𝑆𝑇𝑋𝑌\mathit{above}(AST,X,Y)italic_above ( italic_A italic_S italic_T , italic_X , italic_Y )). Below, we present a learned rule for the simplification tactic which states that the tactic is applicable to a proof state when the goal node of the proof state contains a constant above two constructs (also in the goal) which differ. {minted} prolog tac(A,""simpl"") :- goal_node(const,A,B,C), goal_node(construct,A,D,E), goal_above(A,B,D), goal_node(construct,A,F,E),dif(F,D), goal_above(A,B,F). The rules, as presented above, are learned using inductive logic programming (ILP), in particular, Aleph [26]. In addition to providing rules explaining tactic prediction, we use the resulting rules to filter the output of k𝑘kitalic_k-NN, in particular, the classifier presented in [3, 9] (Tactician and TacticToe). Essentially, we want to determine whether p⁢s,r⊨pt⊨𝑝𝑠𝑟subscript𝑝𝑡ps,r\vDash p_{t}italic_p italic_s , italic_r ⊨ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT where p⁢s𝑝𝑠psitalic_p italic_s is a logic program representing the proof state, r𝑟ritalic_r is a learned rule for the tactic t𝑡titalic_t, and ptsubscript𝑝𝑡p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the head predicate of r𝑟ritalic_r denoting that t𝑡titalic_t should be applied to p⁢s𝑝𝑠psitalic_p italic_s. Thus, given the list of recommended tactics by a k𝑘kitalic_k-NN classifier, we can further filter this list using the learned rules. Our hypothesis is that features of proof state defined through logic programs can be used to learn rules which can be used to filter the output of a k𝑘kitalic_k-NN model to improve accuracy. In addition to improved performance, our approach produces rules to explain the predictions. Consider again the aforementioned rule of simpl that specifies that the goal may be simplified if it contains a constant above two constructors with different positions. Here, the constructor and the constant denote the datatypes of Coq’s terms. The same variable E𝐸Eitalic_E confirms that the two constructors must correspond to the same identifier in Coq. This rule may suit the Coq structure S⁢x−S⁢y𝑆𝑥𝑆𝑦S\ x-S\ yitalic_S italic_x - italic_S italic_y which denotes (1+x)−(1+y)1𝑥1𝑦(1+x)-(1+y)( 1 + italic_x ) - ( 1 + italic_y ). It can be simplified to x−y𝑥𝑦x-yitalic_x - italic_y. S𝑆Sitalic_S denotes a constructor, and −-- denotes a constant. The first argument of goal_node is a constant that is constrained by us via mode declarations [26]. We use the ILP system Aleph [26] together with a user-defined cost function to evaluate the learned rules on Coq’s standard library. We chose Aleph because it has empirically good results [5]. We refrain from using modern ILP approaches such as Popper [6] as the underlying ASP solvers have difficulty generating models when many variables are required and high-arity definitions are included in the background. We develop representation predicates (goal_node) to efficiently denote the nodes of the AST. We also develop feature predicates (goal_above) which denote the properties of the AST calculated based on the representation predicates. The motivation for developing feature predicates is that propositionalization of it would significantly enlarge the representation making it impractical to use. Our experiments confirm that feature predicates can learn more precise rules (rules with higher F-1 scores [25]) compared to representation predicates. Additionally, the experiments demonstrate that the combination of ILP and k𝑘kitalic_k-NN can improve the accuracy of tactic suggestions in Tactician, the main tactic prediction system for Coq. Contributions First, we express the task of predicting the best tactic to apply to the given proof state as an ILP task. Second, using the ILP representation we enriched the feature space by encoding additional, computationally expensive features as background knowledge predicates, allowing us to avoid grounding the features which are computationally expensive. Third, We use this enriched feature space to learn rules explaining when a tactic is applicable to a given proof state and filter the output of an existing tactic selection approach using these rules. Finally, We empirically show improvement over the non-filtering approaches. This is the first time an investigation has considered ILP as a tool for improving tactic suggestion methods for ITPs."
https://arxiv.org/html/2411.02318v2,Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast,"Static verification is a powerful method for enhancing software quality, but it demands significant human labor and resources. This is particularly true of static verifiers that reason about heap manipulating programs using an ownership logic. LLMs have shown promise in a number of software engineering activities, including code generation, test generation, proof generation for theorem provers, and specification generation for static verifiers. However, prior work has not explored how well LLMs can perform specification generation for specifications based in an ownership logic, such as separation logic.To address this gap, this paper explores the effectiveness of large language models (LLMs), specifically OpenAI’s GPT models, in generating fully correct specifications based on separation logic for static verification of human-written programs in VeriFast. Our first experiment employed traditional prompt engineering and the second used Chain-of-Thought (CoT) Prompting to identify and address common errors generated across the GPT models. The results indicate that GPT models can successfully generate specifications for verifying heap manipulating code with VeriFast. Furthermore, while CoT prompting significantly reduces syntax errors generated by the GPT models, it does not greatly improve verification error rates compared to prompt engineering.","Auto-active (Hoare-logic styled (Hoare, 1969), static) verifiers, such as Viper (Müller et al., 2016), Verus (Lattuada et al., 2023), Dafny (Leino, 2010), Gillian (Fragoso Santos et al., 2020), and VeriFast (Jacobs et al., 2011), are powerful as they can prove the absence of large classes of bugs in code. Ideally, users of such tools need only specify the intended behavior of their code on the code itself (as pre- and postconditions), and the tool will automatically provide feedback on whether or not the code is provably correct with respect to this behavior. In reality, auto-active verifiers require many more auxiliary specifications (such as loop invariants, lemmas, folds, unfolds, etc.) to achieve this goal, burdening their users. In recent years, large language models (LLMs) have been effective in generating code (Chen et al., 2022; Sarsa et al., 2022), test-cases (Deng et al., 2023; Lemieux et al., 2023; Rao et al., 2023; Schäfer et al., 2023; Wang et al., 2024; Xia et al., 2024), and proofs in theorem provers (proof assistants) (Zheng et al., 2023; Yang et al., 2024; Jiang et al., 2021; Welleck and Saha, 2023; First et al., 2023). LLMs have also been shown to be effective for generating specifications supported by auto-active verifiers (Ma et al., 2024; Kamath et al., 2023; Misu et al., 2024; He et al., 2024; Mugnier et al., 2024). However, related work has not explored whether or not off-the-shelf LLMs can generate specifications based on a permissions logic, such as separation logic (Reynolds, 2002), that can be verified by auto-active verifiers such as VeriFast, Gillian, and Viper. Thanks to such specifications, these verifiers do well at verifying programs that manipulate the heap for both memory safety and functional properties. But, permissions logic based specifications (auxiliary and non-auxiliary) are particularly cumbersome to write, because they must specify the shape of the heap alongside functional constraints. This leads to specifications containing a number of predicates that hide heap details; and as a result, numerous lemmas, folds, unfolds, and special loop invariants that are used to connect the content of these predicates. While such specifications are difficult to reason about, they are written in a patterned way that may be amenable to generation via LLMs. Therefore, this paper evaluates how effective LLMs are at generating specifications that can be verified by VeriFast (Jacobs et al., 2011), which supports separation logic based verification of C and Java code. We specifically target OpenAI’s GPT models in this preliminary work, and employ two different prompt engineering techniques on the models. We develop input-output pairs to prompt the models with and use as ground truth for the models’ output, respectively. The input-output pairs are generated from a subset of 150 publicly available, statically verified examples on VeriFast’s Github. The GPT models’ output after prompting is inspected manually for correctness compared to the ground truth and results are recorded. Results indicate that GPT models can generate specifications for verification with VeriFast using traditional prompt engineering. When they fail to generate correct specifications, errors range from syntax to deeper verification errors. The second prompting approach based on Chain of Thought Prompting reduced syntax error rates significantly, but not verification error rates."
https://arxiv.org/html/2411.01184v1,Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical Framework with Logical Reward Shaping,"Multi-agent hierarchical reinforcement learning (MAHRL) has been studied as an effective means to solve intelligent decision problems in complex and large-scale environments. However, most current MAHRL algorithms follow the traditional way of using reward functions in reinforcement learning, which limits their use to a single task. This study aims to design a multi-agent cooperative algorithm with logic reward shaping (LRS), which uses a more flexible way of setting the rewards, allowing for the effective completion of multi-tasks. LRS uses Linear Temporal Logic (LTL) to express the internal logic relation of subtasks within a complex task. Then, it evaluates whether the subformulae of the LTL expressions are satisfied based on a designed reward structure. This helps agents to learn to effectively complete tasks by adhering to the LTL expressions, thus enhancing the interpretability and credibility of their decisions. To enhance coordination and cooperation among multiple agents, a value iteration technique is designed to evaluate the actions taken by each agent. Based on this evaluation, a reward function is shaped for coordination, which enables each agent to evaluate its status and complete the remaining subtasks through experiential learning. Experiments have been conducted on various types of tasks in the Minecraft-like environment. The results demonstrate that the proposed algorithm can improve the performance of multi-agents when learning to complete multi-tasks.","Deep reinforcement learning (DRL) has shown remarkable success in solving decision-making problems that surpass human-level performance, such as the Atari game [16], chess confrontation [30, 23], and real-time strategy game (RTS) [14]. However, as the environments become increasingly complex, some limitations (such as low learning efficiency and quality) may appear in single-agent DRL systems. To address this, there is an urgent need for multi-agent DRL [9], where multiple agents can solve complex tasks through collaboration [8]. However, multi-agent learning [26] for complex tasks suffers from an exponential growth of the action and state spaces, which is known as the curse of dimensionality [7]. To overcome this, hierarchical reinforcement learning (HRL) [40] has been introduced into multi-agent DRL, giving rise to multi-agent hierarchical reinforcement learning (MAHRL) [44, 11]. I-A The Challenges Most existing MAHRL algorithms follow the traditional way of setting the reward functions, which is not appropriate when multiple tasks need to be completed in complex environments. For instance, in the Minecraft environment, in order to complete the task of making bows and arrows, agents have to find wood to make the body of bows and arrows, spider silk to make bowstrings, as well as feathers to make arrow fletchings. To learn the strategies for completing the task, an appropriate reward is needed for the agent. However, designing a reward function for one task is challenging and difficult to generalize for other tasks [15]. Moreover, in the task of making bows and arrows, if an agent only finds some of the required materials, it can not get a reward; thus, it is challenging for agents to learn how to complete the remaining tasks. In MAHRL, the decision of each agent is typically treated as a black box, making it difficult to understand the logic behind this decision, leading to the untrustworthiness of the system. Hence, it is essential to develop a general and effective way of shaping rewards with a description of the internal logic of the tasks, which helps the agents easily understand the progress of the task and make reasonable decisions. I-B Our Contributions This work explores a flexible approach to setting rewards, called logic reward shaping (LRS), for multi-task learning. LRS uses the Linear Temporal Logic (LTL) [24, 3, 5, 42] to represent environmental tasks, making use of its precise semantics and compact syntax to clearly show the internal logical construction of the tasks and provide guidance for the agents. A reward structure is appropriately defined to give rewards, based on whether LTL expressions are satisfied or not. To promote strategy learning, a technique of value iteration is used to evaluate the actions taken by each agent; after that, a reward shaping mechanism is utilized to shape a reward function, which can accelerate the learning and coordination between agents. The advantage of the LRS mechanism lies in the formalization provided by LTL to specify the constraints of tasks, ensuring that the agent’s decisions meet the specified requirements. Through the feedback of rewards, the agent gradually adjusts its strategy to meet the logical specifications defined in LTL. Consequently, the agent can execute tasks more reliably by adhering to the prescribed logical requirements, thus enhancing the credibility of decisions. Based on LRS, we propose a multi-agent hierarchical reinforcement learning algorithm, dubbed Multi-agent Hierarchy via Logic Reward Shaping (MHLRS). In MHLRS, the agents aim to achieve joint tasks, but each maintains their own individual structures. Each agent has its own meta-controller, which learns sub-goal strategies based on the state of the environment. The experiments on different scenarios show that the proposed MHLRS enhances the cooperative performance of multi-agents in completing multi-tasks. I-C Related Work Learning coordination in multi-agent systems is a challenging task due to increased complexity and the involvement of multiple agents. As a result, many methods and ideas have been proposed to address this issue [34, 43]. Kumar et al. [17] used a master-slave architecture to solve the coordination problem between the agents. A higher-level controller guides the information exchange between decentralized agents. Based on the guidance of the controller, each agent communicates with another agent in each time step, which allows for the exploration of distributed strategies. However, the scalability of this method remains to be improved, since information exchange between agents that are far away becomes more difficult as the number of agents increases. Budhitama et al. [44] also adopted a similar structure that included the commander agent and unit agent models. The commander makes decisions based on environmental states, and then the units execute those decisions. However, the commander’s global decisions might need to be more suitable for some units. In the proposed MHLRS, each agent has its own meta-controller that proposes suitable sub-goal strategies according to the state of the environment and other agents. Constructed with propositions on environmental states, logical connectors, and temporal operators, LTL [28, 6, 39] can naturally represent the tasks in reinforcement learning. Some studies on using LTL for reinforcement learning [18, 4, 10] have been reported, where different methods have been employed to guide the RL agent to complete various tasks. Toro Icarte et al. [13] used the co-safe LTL expression to solve agents’ multi-task learning problems and introduced the extension of Q-learning, viz., LTL Progression Off-Policy Learning (LPOPL). To reduce the cost of learning LTL semantics, Vaczipoor et al. [35] introduced an environment-independent LTL pre-training scheme. They utilized a neural network to encode the LTL formulae so that RL agents can learn strategies with task conditions. However, these methods are proposed for single-agent systems rather than multi-agent systems. G. Leon et al. [20] extended LTL from a single-agent framework to a multi-agent framework, and proposed two MARL algorithms that are highly relevant to our work. Nevertheless, traditional Q-learning and DQN frameworks are used, which makes it difficult for agents to explore stable collaborative strategies in dynamic environments. To address this issue, a hierarchical structure is introduced in this work to enable more flexible strategy exploration, accelerate the learning process, and enable agents to adapt faster to task changes in multi-agent systems. Furthermore, logical reward shaping is employed to enhance agents’ cooperation and improve the interpretability of their decision-making when completing multiple tasks. I-D Organization of the Paper The rest of this article is organized as follows. Section II introduces the preliminaries of reinforcement learning and LTL. Section III describes the algorithm model. Section IV presents the experimental design and results. Finally, the last section summarizes this work with future research directions."
https://arxiv.org/html/2411.00589v1,Early Announcement: Parametricity for GADTs,"Relational parametricity was first introduced by Reynolds for System F. Although System F provides a strong model for the type systems at the core of modern functional programming languages, it lacks features of daily programming practice such as complex data types. In order to reason parametrically about such objects, Reynolds’ seminal ideas need to be generalized to extensions of System F. Here, we explore such a generalization for the extension of System F by Generalized Algebraic Data Types (GADTs) as found in Haskell. Although GADTs generalize Algebraic Data Types (ADTs) — i.e., simple recursive types such as lists, trees, etc. — we show that naively extending the parametric treatment of these recursive types is not enough to tackle GADTs. We propose a tentative workaround for this issue, borrowing ideas from the categorical semantics of GADTs known as (functorial) completion. We discuss some applications, as well as some limitations, of this solution.","Relational parametricity [8] is a key technique for reasoning about programs in strongly typed languages. It can be used to enforce invariants guaranteeing strong properties of programs, programming languages, and programming language implementations supporting parametric polymorphism. A polymorphic program is a program that can be applied to arguments and return results of different types; a parametric polymorphic program is a program that is not only polymorphic over all types, but is also defined by the same type-uniform algorithm regardless of the concrete type at which it is applied. Since parametric polymorphic programs cannot perform type-specific operations, the computational behaviors they can exhibit are actually quite constrained. Parametricity was originally put forth by Reynolds [8] for System F [3, 7], the formal calculus at the core of all polymorphic functional languages. It was later popularized as Wadler’s “theorems for free” [10], so-called because it allows the deduction of properties of programs in such languages solely from their types, i.e., with no knowledge whatsoever of the text of the programs involved. However, to get interesting free theorems, Wadler actually treats System F extended with built-in lists. Indeed, most of the free theorems in [10] are essentially naturality properties for polymorphic list-processing functions. It is easy to extend the techniques developed in [10] for handling lists to non-list algebraic data types (ADTs). Parametricity for such types can then be used to derive not just naturality (i.e., commutativity) properties, but also results — such as proofs of type inhabitance and correctness of the program optimization known as short cut fusion [2] — that go beyond simple naturality. In his original formulation, Reynolds gives each type expression of System F a relational interpretation defined inductively. Each type expression ΦΦΦroman_Φ with type variables α⁢₁,α⁢₂,⋯,α⁢ₙ𝛼₁𝛼₂⋯𝛼italic-ₙα₁,α₂,⋯,αₙitalic_α ₁ , italic_α ₂ , ⋯ , italic_α italic_ₙ thus gives, for each tuple R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG of relations R⁢ᵢ𝑅italic-ᵢRᵢitalic_R italic_ᵢ between types A⁢ᵢ𝐴italic-ᵢAᵢitalic_A italic_ᵢ and B⁢ᵢ𝐵italic-ᵢBᵢitalic_B italic_ᵢ, a relation Φ^⁢R¯^Φ¯𝑅\widehat{Φ}\,\overline{R}over^ start_ARG roman_Φ end_ARG over¯ start_ARG italic_R end_ARG between the type Φ⁢[A¯/α¯]Φdelimited-[]¯𝐴¯𝛼Φ[\overline{A}/\overline{\alpha}]roman_Φ [ over¯ start_ARG italic_A end_ARG / over¯ start_ARG italic_α end_ARG ] and Φ⁢[B¯/α¯]Φdelimited-[]¯𝐵¯𝛼Φ[\overline{B}/\overline{\alpha}]roman_Φ [ over¯ start_ARG italic_B end_ARG / over¯ start_ARG italic_α end_ARG ]. To capture the intended type-uniformity of System F’s polymorphic expressions, these relational interpretations are defined in such a way that every function f⁢:⁢∀⁢α¯.Φ⁢→⁢Ψformulae-sequence𝑓:∀¯𝛼Φ→Ψf\,\mathord{\mathchar 58\relax}\,∀\overline{\alpha}.Φ\textrightarrow Ψitalic_f : ∀ over¯ start_ARG italic_α end_ARG . roman_Φ → roman_Ψ, where ΦΦΦroman_Φ and ΨΨΨroman_Ψ are two type expressions in the same type variables α¯¯𝛼\overline{\alpha}over¯ start_ARG italic_α end_ARG, is parametric in the following sense: for each tuple of relations R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG, the pairs related by Φ^⁢R¯^Φ¯𝑅\widehat{Φ}\,\overline{R}over^ start_ARG roman_Φ end_ARG over¯ start_ARG italic_R end_ARG are sent by f𝑓fitalic_f to pairs related by Ψ^⁢R¯^Ψ¯𝑅\widehat{Ψ}\,\overline{R}over^ start_ARG roman_Ψ end_ARG over¯ start_ARG italic_R end_ARG. As mentioned above, better approximations of realistic programming languages result from adding built-in data types to System F. Each such added data type induces a type constructor, and this type constructor must also be given a relational interpretation. Wadler [10] considers the case of lists, which we review in detail in Section 2. To add a new inductive data type constructor T𝑇Titalic_T to an ambient parametric language in such a way that parametricity is preserved, the method is always the same: Define its relational interpretation as a (dependent) inductive family T^^𝑇\widehat{T}over^ start_ARG italic_T end_ARG with one data constructor c^^𝑐\widehat{c}over^ start_ARG italic_c end_ARG for each data constructor c𝑐citalic_c of T𝑇Titalic_T expressing precisely that c𝑐citalic_c is a parametric polymorphic function. The data constructors of such a data type’s relational interpretation thus make formal the intuitive type-uniformity required of its data constructors by the grammars of languages such as Haskell. The relational interpretation T^^𝑇\widehat{T}over^ start_ARG italic_T end_ARG captures the intuition that, if we regard data types as containers, then two data structures of (two instances of) T𝑇Titalic_T are related by T^⁢R^𝑇𝑅\widehat{T}\,Rover^ start_ARG italic_T end_ARG italic_R exactly when the data they store are related by R𝑅Ritalic_R. This intuition also requires that T^^𝑇\widehat{T}over^ start_ARG italic_T end_ARG preserves inclusion, i.e., that T^⁢R⊆T^⁢S^𝑇𝑅^𝑇𝑆\widehat{T}\,R\subseteq\widehat{T}\,Sover^ start_ARG italic_T end_ARG italic_R ⊆ over^ start_ARG italic_T end_ARG italic_S whenever R⊆S𝑅𝑆R\subseteq Sitalic_R ⊆ italic_S. Indeed, if two data structures are related by T^⁢R^𝑇𝑅\widehat{T}\,Rover^ start_ARG italic_T end_ARG italic_R, then the data they store are related by R𝑅Ritalic_R, and thus by S𝑆Sitalic_S, so the two data structures must be related by T^⁢S^𝑇𝑆\widehat{T}\,Sover^ start_ARG italic_T end_ARG italic_S. Fortunately, for lists and other ADTs, the relational interpretations defined in this way enjoy this crucial inclusion-preservation property. Here, we report our ongoing efforts to add the generalization of ADTs known as Generalized Algebraic Data Types (GADTs) to System F in such a way that parametricity is preserved. In doing so, we insist on understanding GADTs as types of data structures, i.e., as types of containers that can be filled with data. Since this entails in particular that GADTs are inductive data type constructors, we might expect that following the method outlined above will suffice. In Section 2, we show that naively doing so results in relational interpretations of GADTs that do not satisfy the inclusion-preservation property identified at the end of the preceding paragraph. This is problematic: if we are to understand GADTs as types of data structures, then they should certainly satisfy all properties — among them the inclusion-preservation property — expected of such types. In Section 3, we explore a promising approach to overcoming this issue. This approach consists in defining the relational interpretation of a GADT through that of its completion, an ADT-like type constructor that contains the original GADT. In Section 4 we offer some applications of parametricity for GADTs obtained using our proposed approach. In Section 5 we discuss some issues that arise when making our proposed approach precise. Doing so requires defining a source language (an extension of System F that allows for GADTs), a target language (a dependent type theory strong enough to encode relations), and interpretations of each type of the source language as both a type and a relation in the target language. We point out some difficulties in the design of the target language, and also offer some thoughts on how to resolve them. Throughout the paper, we use an Agda-like syntax to write examples of types and terms of the anticipated target language. We note, however, that this language might end up being very different from Agda’s type theory. In particular, this early announcement by no means reports on an attempt to formalize our work in a proof assistant. We are not the first to consider parametricity for GADTs. Very recent progress on the subject has been presented in [9]. Sieczkowski et al. construct there a parametric model of an extension of System F supporting GADTs, with the aim of deriving free theorems and representation independence results. However, their work differs drastically from the line of research presented here in several ways. First, the semantics presented by Sieczkowski et al. targets normalization-by-evaluation. By contrast, our work is in no way concerned with such methods. Second, Sieczkowski et al. make essential use of guarded recursion through a universe of step-indexed propositions equipped with a later modality (as exists, e.g., in Iris). By contrast, we are concerned only with structural recursion in this work. Third, Sieczkowski et al. insist on the importance of two particular rules of their type system: discriminability and injectivity of type constructors. By contrast, we are agnostic about such rules, thus accommodating more diverse host languages. Finally, and most importantly, the semantics of Sieczkowski et al. models parametricity for GADTs only in those type indices that are unconstrained, i.e., that can be promoted to parameters. In particular, their approach cannot handle free theorems such as the one presented in Section 4.1 for 𝖲𝖾𝗊𝖲𝖾𝗊\operatorname{\sf Seq}sansserif_Seq, since 𝗉𝖺𝗂𝗋𝗂𝗇𝗀𝗉𝖺𝗂𝗋𝗂𝗇𝗀\operatorname{\sf pairing}sansserif_pairing has a constrained instance of 𝖲𝖾𝗊𝖲𝖾𝗊\operatorname{\sf Seq}sansserif_Seq as return type. By contrast, we not only recognize the non-uniformity of GADTs acknowledged by Sieczkowski et al., but we also recognize that this break of uniformity is governed by uniform type constructors (namely, those constraining the instances of the return types of GADTs’ data constructors), and that this uniformity must be captured by parametric models of the language at play."
https://arxiv.org/html/2411.00149v1,A “Symbolic” Representation of Object-Nets,"In this contribution we extend the concept of a Petri net morphism to Elementary Object Systems (Eos). Eos are a nets-within-nets formalism, i.e. we allow the tokens of a Petri net to be Petri nets again. This nested structure has the consequence that even systems defined by very small Petri nets have a quite huge reachability graph. In this contribution we use automorphism to describe symmetries of the Petri net topology. Since these symmetries carry over to markings as well this leads to a condensed state space, too. utomorphism, canonical representation, nets within nets, nets as tokens, state space reductions, symmetry","1 Exploiting Symmetry and Canonical Representations In this paper we study Elementary Object Systems (Eos) [10] a Nets-within-Nets formalism as proposed by Valk [18], i.e., we allow the tokens of a Petri net to be Petri nets again. Due to the nesting structure many of the classical decision problems, like reachability and liveness, become undecidable for Eos. From a complexity perspective we have studied these problems for safe Eos [12, 13, 10] where markings are restricted to sets (i.e., places are either marked or unmarked). More precisely: All problems that are expressible in LTL or CTL, which includes reachability and liveness, are PSpace-complete. This means that in terms of complexity theory safe Eos are no more complex than safe place transition nets (p/t nets). But, a look at the details shows a difference that is pratically relevant: For safe p/t nets it is known that whenever there are n𝑛nitalic_n places, then the number of reachable states is bounded by O⁢(2n)𝑂superscript2𝑛O(2^{n})italic_O ( 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ); but, for safe Eos the number of reachable states is in O⁢(2(n2))𝑂superscript2superscript𝑛2O(2^{(n^{2})})italic_O ( 2 start_POSTSUPERSCRIPT ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT ) – a quite drastic increase. Therefore, our main goal is to derive a condensed state space for Eos, were ‘condensed’ is expressed as a factorisation modulo an equivalence. In this contribution we extend the concept of a Petri net morphism to Elementary Object Systems (Eos). Eos are a nets-within-nets formalism. Here, we use automorphism to describe symmetries of the Petri net topology. Since these symmetries carry over to markings as well this leads to a condensed state space, too. In our approach these symmetries are introduced very naturally to the representation of the state space using canonical representations of markings. The paper has the following structure. Section 2 introduces base nets-within-nets (Eos). In Section 3 we define a symbolic representation of the Eos structure. The work closes with a conclusion and outlook."
https://arxiv.org/html/2411.00117v1,Openness and Partial Adjacency in One Variable TPTL.,"Metric Temporal Logic (MTL) and Timed Propositional Temporal Logic (TPTL) are prominent real-time extensions of Linear Temporal Logic (LTL). MTL extends LTL modalities, Until, 𝖴𝖴\>\mathsf{U}sansserif_U and Since, 𝖲𝖲\>\mathsf{S}sansserif_S to family of modalities 𝖴Isubscript𝖴𝐼\>\mathsf{U}_{I}sansserif_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT and 𝖲Isubscript𝖲𝐼\>\mathsf{S}_{I}sansserif_S start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT, respectively, where I𝐼Iitalic_I is an interval of the form ⟨l,u⟩𝑙𝑢\langle l,u\rangle⟨ italic_l , italic_u ⟩ to express real-time constraints. On the contrary, TPTL extends LTL by real-valued freeze quantification, and constraints over those freeze variables to do the same. It is well known that one variable fragment of TPTL is strictly more expressive than MTL. In general, the satisfiability checking problem for both MTL and TPTL is undecidable. MTL enjoys the benefits of relaxing punctuality. That is, satisfiability checking for Metric Interval Temporal Logic (MITL), a subclass of MTL where the intervals are restricted to be of the form ⟨l,u⟩𝑙𝑢\langle l,u\rangle⟨ italic_l , italic_u ⟩ where l<u𝑙𝑢l<uitalic_l < italic_u, is decidable with elementary complexity (EXPSPACE complete). Moreover, Partially Punctual Metric Temporal Logic (PMTL), a subclass of MTL where punctual intervals are only allowed in either 𝖴𝖴\>\mathsf{U}sansserif_U modalities or 𝖲𝖲\>\mathsf{S}sansserif_S modalities, but not both, is also decidable over finite timed words with non-primitive recursive complexity.In case of TPTL, punctuality can be trivially recovered due to freeze quantifiers and boolean over guards. Hence, we study a more restrictive version of non-punctuality, called Openness. Intuitively, this restriction only allows a property to be specified within timing intervals which are topologically open. We show that even with this restriction, 1-TPTL is undecidable. Our results make a case for a the new refined notion of non-adjacency by Krishna et. al. for getting a decidable fragment of 1-TPTL, called non-adjacency. We extend the notion of non-adjacency to partial adjacency, where the restriction is only applicable in either past or future but not in both directions. We show that partially adjacent 1- TPTL (PA-1-TPTL) is decidable over finite timed words. Moreover, it is strictly more expressive than PMTL, making it the most expressive boolean closed decidable timed logic known in the literature.","Metric Temporal Logic 𝖬𝖳𝖫⁢[𝖴I,𝖲I]𝖬𝖳𝖫subscript𝖴𝐼subscript𝖲𝐼\mathsf{MTL}[\>\mathsf{U}_{I},\>\mathsf{S}_{I}]sansserif_MTL [ sansserif_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , sansserif_S start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ] is a well established logic useful for specifying quantitative properties of real-time systems. The main modalities of 𝖬𝖳𝖫𝖬𝖳𝖫\mathsf{MTL}sansserif_MTL are 𝖴Isubscript𝖴𝐼\>\mathsf{U}_{I}sansserif_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT (read “until I𝐼Iitalic_I”) and 𝖲Isubscript𝖲𝐼\>\mathsf{S}_{I}sansserif_S start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT (read “since I𝐼Iitalic_I”), where I𝐼Iitalic_I is a time interval with endpoints in ℕℕ\mathbb{N}blackboard_N. These formulae are interpreted over timed behaviours or timed words. For example, a formula a⁢𝖴[2,3]⁢b𝑎subscript𝖴23𝑏a\>\mathsf{U}_{[2,3]}bitalic_a sansserif_U start_POSTSUBSCRIPT [ 2 , 3 ] end_POSTSUBSCRIPT italic_b is satisfied by a position i𝑖iitalic_i of a timed word ρ𝜌\rhoitalic_ρ if and only if there is a position j𝑗jitalic_j strictly in the future of i𝑖iitalic_i where b𝑏bitalic_b is true, and at all intermediate positions between i𝑖iitalic_i and j𝑗jitalic_j, a𝑎aitalic_a is true; moreover, the difference in the timestamps of i𝑖iitalic_i and j𝑗jitalic_j must lie in the interval [2,3]23[2,3][ 2 , 3 ]. Similarly, a⁢𝖲[2,3]⁢b𝑎subscript𝖲23𝑏a\>\mathsf{S}_{[2,3]}bitalic_a sansserif_S start_POSTSUBSCRIPT [ 2 , 3 ] end_POSTSUBSCRIPT italic_b is true at a point i𝑖iitalic_i if and only if there is a position j𝑗jitalic_j strictly in the past of i𝑖iitalic_i where b𝑏bitalic_b is true, and at all intermediate positions between i𝑖iitalic_i and j𝑗jitalic_j, a𝑎aitalic_a is true; further, the difference in the timestamps between i𝑖iitalic_i and j𝑗jitalic_j lie in the interval [2,3]23[2,3][ 2 , 3 ]. In their seminal paper, Alur and Henzinger [4] showed that the satisfiability of full 𝖬𝖳𝖫𝖬𝖳𝖫\mathsf{MTL}sansserif_MTL, with until and since modalities is undecidable even over finite words. This ability to encode undecidable problems is due to the presence of punctual intervals, i.e., intervals of the form [x,x]𝑥𝑥[x,x][ italic_x , italic_x ]. This allows the logic to specify constraints like “an event a𝑎aitalic_a occurs exactly after 5 time units, ⊤𝖴[5,5]⁢atopsubscript𝖴55𝑎\top\>\mathsf{U}_{[5,5]}a⊤ sansserif_U start_POSTSUBSCRIPT [ 5 , 5 ] end_POSTSUBSCRIPT italic_a.” In practice, such exact constraints are not used extensively. Hence, Alur et al. studied the non-punctual fragment of MTL called Metric Interval Temporal Logic (MITL) in [2] [1] where the time intervals used in the until, since modalities are non-punctual, i.e. of the form ⟨x,y⟩𝑥𝑦\langle x,y\rangle⟨ italic_x , italic_y ⟩ where x<y𝑥𝑦x<yitalic_x < italic_y. They show that the satisfiability becomes decidable over finite as well as infinite timed words with EXPSPACE complexity. The satisfiability of the future only fragment of 𝖬𝖳𝖫𝖬𝖳𝖫\mathsf{MTL}sansserif_MTL, where since modalities are not used (MTL[𝖴Isubscript𝖴𝐼\>\mathsf{U}_{I}sansserif_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT]), was open for a long time. Ouaknine and Worrell [16] showed its decidability via a reduction to 1-clock Alternating Timed Automata over finite timed words. A natural extension to both these problems studied in [1][2] [16] is to ask what happens to the decidability and expressiveness of 𝖬𝖳𝖫⁢[𝖴I,𝖲n⁢p]𝖬𝖳𝖫subscript𝖴𝐼subscript𝖲𝑛𝑝\mathsf{MTL}[\>\mathsf{U}_{I},\>\mathsf{S}_{np}]sansserif_MTL [ sansserif_U start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT , sansserif_S start_POSTSUBSCRIPT italic_n italic_p end_POSTSUBSCRIPT ], subclass of MTL where 𝖲𝖲\>\mathsf{S}sansserif_S modalities are non-punctual, when interpreted over finite timed words. This was resolved by Krishna et. al. in [9]. Timed Propositional Temporal Logic (TPTL) extends LTL with freeze quantifiers. A freeze quantifier [3][5] has the form x.φformulae-sequence𝑥𝜑x.\varphiitalic_x . italic_φ with freeze variable x𝑥xitalic_x (also called a clock [6][18]). When it is evaluated at a point i𝑖iitalic_i on a timed word, the time stamp of i𝑖iitalic_i (say τisubscript𝜏𝑖\tau_{i}italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) is frozen or registered in x𝑥xitalic_x, and the formula φ𝜑\varphiitalic_φ is evaluated using this value for x𝑥xitalic_x. Variable x𝑥xitalic_x is used in φ𝜑\varphiitalic_φ in a constraint of the form T−x∈I𝑇𝑥𝐼T-x\in Iitalic_T - italic_x ∈ italic_I; this constraint, when evaluated at a point j𝑗jitalic_j, checks if τj−τi∈Isubscript𝜏𝑗subscript𝜏𝑖𝐼\tau_{j}-\tau_{i}\in Iitalic_τ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_I, where τjsubscript𝜏𝑗\tau_{j}italic_τ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT is the time stamp at point j𝑗jitalic_j. Here T𝑇Titalic_T can be seen as a special variable giving the timestamp of the present point. For example, the formula φ=◇⁢x.(a∧◇⁢(b∧T−x∈[1,2]∧◇⁢(c∧T−x∈[1,2])))formulae-sequence𝜑◇𝑥𝑎◇𝑏𝑇𝑥12◇𝑐𝑇𝑥12\varphi=\Diamond x.(a\wedge\Diamond(b\wedge T-x\in[1,2]\wedge\Diamond(c\wedge T% -x\in[1,2])))italic_φ = ◇ italic_x . ( italic_a ∧ ◇ ( italic_b ∧ italic_T - italic_x ∈ [ 1 , 2 ] ∧ ◇ ( italic_c ∧ italic_T - italic_x ∈ [ 1 , 2 ] ) ) ) asserts that there is a point i𝑖iitalic_i in the future where a𝑎aitalic_a holds and in its future there is a b𝑏bitalic_b within interval [1,2]12[1,2][ 1 , 2 ] followed by a c𝑐citalic_c within interval [1,2]12[1,2][ 1 , 2 ] from i𝑖iitalic_i. The contributions of this paper are two fold: (1) We study the satisfiability checking problem for a restricted fragment of the TPTL that can only specify properties within topologically open timing intervals. We call this fragment as Open TPTL (denoted by 𝖮𝗉𝖳𝖯𝖳𝖫1superscript𝖮𝗉𝖳𝖯𝖳𝖫1\mathsf{OpTPTL}^{1}sansserif_OpTPTL start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT). Notice that the restriction of openness is more restrictive than that of non-punctuality, and with openness punctual guards can not be simulated even with the use of freeze quantifiers and boolean operators. In spite of such a restriction, we show that satisfiability checking of 𝖮𝗉𝖳𝖯𝖳𝖫1superscript𝖮𝗉𝖳𝖯𝖳𝖫1\mathsf{OpTPTL}^{1}sansserif_OpTPTL start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT is as hard as that of 1-𝖳𝖯𝖳𝖫𝖳𝖯𝖳𝖫\mathsf{TPTL}sansserif_TPTL (i.e. undecidable on infinite words, and decidable with non-primitive recursive lower bound for finite words). This implies that it is something more subtle than just the presence of punctual guards that makes the satisfiability checking problem hard for 1-𝖳𝖯𝖳𝖫𝖳𝖯𝖳𝖫\mathsf{TPTL}sansserif_TPTL. This makes a strong case for studying non-adjacency of 𝖳𝖯𝖳𝖫𝖳𝖯𝖳𝖫\mathsf{TPTL}sansserif_TPTL. (2) We define the notion of partial adjacency, generalizing the notion of non-adjacency. Here, we allow adjacent guards but only in one direction. We show that Partially Punctual One variable Timed Propositional Temporal Logic (𝖯𝖠⁢-⁢𝟣⁢-⁢𝖳𝖯𝖳𝖫𝖯𝖠-1-𝖳𝖯𝖳𝖫\mathsf{PA\text{-}1\text{-}TPTL}sansserif_PA - sansserif_1 - sansserif_TPTL) is decidable over finite timed words. Moreover, this logic is the most expressive known decidable subclass of the logic known till date."
https://arxiv.org/html/2411.00025v1,Probabilistic Obstruction Temporal Logic:a Probabilistic Logic to Reason about Dynamic Models,"In this paper, we propose a novel formalism called Probabilistic Obstruction Temporal Logic (POTL), which extends Obstruction Logic (OL) by incorporating probabilistic elements. POTL provides a robust framework for reasoning about the probabilistic behaviors and strategic interactions between attackers and defenders in environments where probabilistic events influence outcomes. We explore the model checking complexity of POTL and demonstrate that it is not higher than that of Probabilistic Computation Tree Logic (PCTL), making it both expressive and computationally feasible for cybersecurity and privacy applications.","Understanding and quantifying uncertainty is essential in cybersecurity, and probability theory offers a robust framework for this purpose, making it particularly valuable for risk analysis. As digital systems grow increasingly complex and dynamic, effectively assessing and managing risks becomes more challenging. Probability theory allows organizations to model the likelihood of various cyber threats, such as hacking attempts, data breaches, and software vulnerabilities, which are inherently uncertain and variable. Cybersecurity professionals can estimate the likelihood of these threats materializing and assess their potential impact on systems by applying probabilistic and non-probabilistic formalisms. Researchers have developed various solutions over the past fifty years, with formal methods emerging as a notable success. These techniques allow for the verification of system correctness by checking if a mathematical model meets the formalized desired behavior. Notably, traditional formal approaches like model checking (Baier and Katoen 2008), initially designed for monolithic systems, have been effectively adapted to manage open and Multi-Agent Systems (MAS). In recent years, the study of MAS has garnered significant attention due to its wide-ranging applications in fields such as cybersecurity, robotics, and distributed computing. MAS consists of two or more interacting agents, each capable of making autonomous decisions. These systems often operate in dynamic and uncertain environments, necessitating robust formal verification techniques to ensure their reliability and correctness. An important logic in the context of MAS is Alternating-time Temporal Logic (ATL) (Alur, Henzinger, and Kupferman 2002). The latter extends CTL (Clarke and Emerson 1981) by introducing strategic modalities, enabling the specification of properties that involve the strategic abilities of agents. ATL can express whether a group of agents can achieve a certain goal regardless of the actions of other agents, making it a powerful tool for reasoning about cooperation and competition in MAS. Another relevant formalism in this area is Obstruction Logic (OL) (Catta, Leneutre, and Malvone 2023b), which focuses on obstructions in two-player games. In OL, one player, called the Demon, can temporarily disable edges in the graph as long as their total weight remains below a specified natural number, thereby preventing the other agent from achieving its temporal goal. As illustrated in their paper, OL can be well-suited for representing cybersecurity problems, where a defender can activate defense mechanisms (by disabling edges) and an attacker aims to access private resources through a sequence of atomic attacks. In this context, a key aspect when performing cybersecurity risk analysis is to assess the likelihood (or probability) of success of the attack scenarios. However, OL did not address this aspect, where no probabilistic concepts were introduced. For the above reasons, in this paper, we present Probabilistic Obstruction Temporal Logic (POTL), a logic that extends OL into a probabilistic context. POTL offers a comprehensive framework for analyzing the probabilistic behaviors and strategic interactions between attackers and defenders in scenarios where probabilistic events influence outcomes. We investigate the model checking complexity of POTL and show that it is comparable to that of Probabilistic Computation Tree Logic, ensuring that POTL remains both expressive and computationally practical for cybersecurity and privacy applications. Structure of the work. The contribution is structured as follows. Theoretical background is presented in Section 2. In Section 3, we present the syntax and the semantics of our new logic, called Probabilistic Obstruction Temporal Logic (POTL). In Section 4, we show our model checking algorithm and prove that the model checking problem for POTL is decidable in polyonimal-time. In section 5, we present an illustrative example related to the cybersecurity analysis. In Section 6, we compare our approach to related work. Finally, Section 7 concludes and presents possible future directions."
https://arxiv.org/html/2411.00431v1,Integrating Fuzzy Logic into Deep Symbolic Regression,"Credit card fraud detection is a critical concern for financial institutions, intensified by the rise of contactless payment technologies. While deep learning models offer high accuracy, their lack of explainability poses significant challenges in financial settings. This paper explores the integration of fuzzy logic into Deep Symbolic Regression (DSR) to enhance both performance and explainability in fraud detection. We investigate the effectiveness of different fuzzy logic implications, specifically Łukasiewicz, Gödel, and Product, in handling the complexity and uncertainty of fraud detection datasets. Our analysis suggest that the Łukasiewicz implication achieves the highest F1-score and overall accuracy, while the Product implication offers a favorable balance between performance and explainability. Despite having a performance lower than state-of-the-art (SOTA) models due to information loss in data transformation, our approach provides novelty and insights into into integrating fuzzy logic into DSR for fraud detection, providing a comprehensive comparison between different implications and methods.","Credit card fraud poses a significant and growing challenge for financial institutions, amplified by the advent of innovative technologies such as contactless payment (Europol, 2021). Global losses due to credit card fraud were estimated at $32.39 billion in 2020 and are projected to exceed $40 billion by 2027 (Nilson, S., 2019). The Covid-19 pandemic further accelerated the shift from cash to cashless transactions, intensifying the issue of credit card fraud. To protect customers from fraudulent activities, banks deploy Fraud Detection Systems (FDS) to automatically flag and block suspicious transactions in real-time. Significant advancements in these systems have been achieved through improvements in data quality and the enhanced use and performance of Artificial Intelligence (AI) and Deep Learning (DL) techniques (Cherif et al., 2023). Although DL has demonstrated exceptional performance in classification accuracy (Alarfaj et al., 2022), a major limitation is its lack of explainability. (Mill et al., 2023). This ”black box” nature has hindered the adoption of AI in financial settings, where decisions must be transparent and explainable. Explainable Artificial Intelligence (XAI) offers a potential solution to this problem. Despite the growing interest in XAI, the intersection of fraud detection and XAI remains underexplored. Recent efforts have taken various approaches to bridge this gap. One approach employs XAI methods to interpret Machine Learning (ML) models post-training using techniques such as SHAP (Shapley Additive Explanations) or LIME (Local Interpretable Model-Agnostic Explanations), which have shown only modest improvements in user trust (Ji et al., 2021). Another promising approach involves leveraging Symbolic Regression (SR), which seeks to extract closed-form expressions to describe underlying patterns in the data. These expressions are inherently explainable, resolving transparency issues. To advance SR, Petersen et al. (Petersen et al., 2021) combined SR with Reinforcement Learning (RL), resulting in Deep Symbolic Regression (DSR). DSR employs a recurrent neural network (RNN) trained with deep reinforcement learning, where the reward is task-specific, producing expressions tailored to specific problems. These closed-form expressions show high predictive power and transparency, making them a viable solution to many of the previously mentioned issues. DSR utilizes a library of tokens representing features, constants, or mathematical operators to generate expressions. The DSR framework creates a list of tokens subject to constraints, optimizing them using the RNN based on the reward function. An extension to DSR by Visbeek et al. successfully applied DSR to the fraud detection domain, resulting in Deep Symbolic Classification (DSC) (Visbeek et al., 2023). DSC adapts DSR for classification tasks and uses the F1-score as the reward metric, offering competitive predictive performance with improved explainability. In this paper, we propose extensions to the DSR framework by integrating fuzzy logic. Fuzzy logic, based on the fuzzy set theory by Zadeh (Zadeh, 1965), categorizes reasoning into multiple levels, similar to human reasoning. Unlike strict classifications, fuzzy logic handles uncertainty and vagueness, making it suitable for real-world complexities. For instance, a person is not simply tall or short but can manifest varying degrees of tallness. Fuzzy logic facilitates smooth transitions between such degrees. In DSC, the output is a closed-form mathematical expression (Visbeek et al., 2023). Logical implications provide intuitive explanations, since they naturally represent general rules e.g., If transaction amount is high and receiver balance is low then fraud is the case. These expressions are more intuitive as they mirror human reasoning. Various formulas derive fuzzy implications from fuzzy sets, and fuzzy logic provides a natural medium for expressing the vagueness within this logical structure. For instance, vague expressions such as amount being high might have a varying degree of truth from 0 being false, to 1 being absolutely true. This leads us to the main research question: How can we integrate fuzzy logic into deep symbolic regression for fraud detection? To address our main research question, we elaborate on the following specific sub-research questions: (1) What specific fuzzy logic implications are most effective in enhancing the model’s ability to handle the inherent complexity and uncertainty in fraud detection datasets? (2) What specific choice of fuzzy implication is most effective in DSR? (3) Does the fuzzy logic oriented DSR provide intuitive expressions that are easy to interpret? (4) What is the trade off between the size of the fuzzy logic formula and performance metrics? To assess performance, we compare the proposed framework against current state-of-the-art algorithms as detailed in Table 2. Performance evaluation is conducted using accuracy, which is common in real-life fraud detection, and the F1-score, which addresses the inherent imbalance in credit card fraud data. The popular PaySim dataset is utilized (Alonso Lopez-Rojas et al., 2016), offering a controlled environment for comparison. This synthetically generated dataset contains no privacy concerns, as it lacks personally identifiable information (PII). In addition to performance assessment, the explainability of the rule expressions are evaluated. Furthermore, we employ a Pareto front (Langdon, 1998) to balance predictive performance and explainability, optimizing the complexity of expressions based on the factors and performance, identifying the most suitable expressions for the task."
