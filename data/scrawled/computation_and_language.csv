URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.05000v1,"Needle Threading: Can LLMs Follow Threads through
Near-Million-Scale
Haystacks?","As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably thread-safe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared—they often correspond to substantially different numbers of written characters. We release our code and long context experimental data.","In recent years, LLMs and multimodal LLMs have been shown to possess remarkable capabilities (Bubeck et al., 2023) across tasks including software engineering (Hou et al., 2023), geospatial reasoning (Roberts et al., 2023a; b), medicine (Wu et al., 2023), mathematical and scientific figure understanding (Yue et al., 2024) and finance (Liu et al., 2023b). Figure 1: Contextualising context lengths of LLMs and classic literature111Using the LLaMA-3.1 tokenizer (Dubey et al., 2024).. Books sourced from Project Gutenberg (2024). An expansion of compute resources, coupled with technical innovations (Liu et al., 2023a), is enabling contemporary frontier models to be trained on ever increasing volumes of data and longer context limits—the maximum number of tokens they can process at once. To contextualise the number of tokens leading models can process simultaneously, at just over 300k tokens11footnotemark: 1, the classic novel Moby-Dick (Melville, 1851) could fit into the reported 2M token context window of Gemini 1.5 Pro (Reid et al., 2024) almost 5 times. As shown in Fig. 1, most books and even book series contain fewer tokens than the longest model context windows. A longer context offers potential benefits to performance, for example, many-shot in-context learning (Agarwal et al., 2024) in which hundreds or thousands of examples are appended to the model input. Another consequence is the wider range of possible applications and attainable downstream tasks. In particular, with a longer context, models can better perform real-world scenarios, such as legal document retrieval, academic research, understanding tax frameworks, and solving crimes and puzzles. In these cases, decisions are made and conclusions drawn based on large quantities of information distributed across many sources and formats. The ability to hold information – on the scale of multiple full-length novels or hundreds of academic papers and documents – in-context, makes models well-suited to this type of task. The rate of development of longer context models has outpaced the understanding of how well they use their long context and can navigate it. Moreover, current benchmarks are considered inadequate and lacking (Bai et al., 2023; Zhang et al., 2024). Specifically, we identify three limitations of the extant literature related to long context understanding. (1) Performance saturation: Building on the ‘needle in a haystack’ test (Kamradt, 2023), numerous benchmarks focus on simple retrieval-based experiments. Frontier models can perform these tasks excellently, achieving perfect or near-perfect scores (Reid et al., 2024; Anthropic, 2024a; Dubey et al., 2024), leaving little headroom and useful insights to be gained. (2) Limited context length: In most long-context benchmarks, evaluations are limited to sub-100k contexts, falling short of the context limit of frontier LLMs by an order of magnitude. (3) Lack of granular takeaways: Due to the use of real documents or tendency to aggregate multiple tasks into an overall metric in most works, isolating specific trends is challenging other than the macro-trend that performance degrades as context length increases. As such, there is opportunity for a set of challenging experiments, suitable to reach the limits of frontier models. To this end, we design and conduct a series of retrieval-based long context experiments of varying degrees of difficulty, across a range of context sizes up to 900k (Gemini 1.5) tokens. Our investigation includes novel needle threading tasks, which entail following a thread of linked pieces of information across different parts of the context and retrieving the final value. We also explore a more difficult multi-threading variation, which requires tracking multiple threads simultaneously, and assess whether the LLMs are thread-safe. We evaluate a suite of 17 LLMs on these tasks and observe performance decreases in longer contexts. Coupled with the finding that tokenization differs significantly between models, we introduce a task-specific effective context limit metric. In summary, our core contributions are: (1) We introduce challenging multi-step threading and multi-threading retrieval tasks and evaluate 17 leading LLMs on these tasks. (2) For simple needle retrieval tasks, we show that increased context length reduces retrieval performance, while increasing the number of needles retrieved concurrently has relatively limited influence on stronger models. (3) We show that many leading LLMs are remarkably thread-safe - their thread following performance is largely unaffected by concurrent queries. (4) We conduct a comparison of tokenizers, highlighting significant differences in token counting. (5) We propose a task-specific and configurable model-agnostic effective context limit metric."
https://arxiv.org/html/2411.04986v1,The Semantic Hub Hypothesis:Language Models Share Semantic Representations Across Languages and Modalities,"Modern language models can process inputs across diverse languages and modalities. We hypothesize that models acquire this capability through learning a shared representation space across heterogeneous data types (e.g., different languages and modalities), which places semantically similar inputs near one another, even if they are from different modalities/languages. We term this the semantic hub hypothesis, following the hub-and-spoke model from neuroscience (Patterson et al., 2007) which posits that semantic knowledge in the human brain is organized through a transmodal semantic “hub” which integrates information from various modality-specific “spokes” regions. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can be interpreted using the model’s dominant pretraining language via the logit lens. This tendency extends to other data types, including arithmetic expressions, code, and visual/audio inputs. Interventions in the shared representation space in one data type also predictably affect model outputs in other data types, suggesting that this shared representations space is not simply a vestigial byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.","Modern language and multimodal models (LMs)111Hereafter, we use the term “language model” loosely and also consider multimodal language models that process additional data modalities, since such models are commonly trained on top of a text LM backbone. are capable of processing heterogeneous data types: text in different languages, non-linguistic inputs such as code and math expressions, and even other modalities such as images and sound. How do LMs process these distinct data types with a single set of parameters? One strategy might be to learn specialized subspaces for each data type that are only employed when processing it. In many cases, however, data types that are surface-distinct share underlying semantic concepts. This is most obvious for sentences in different languages with the same meaning; but such shared concepts are present across other data types, e.g., between an image and its caption, or a piece of code and its natural language description. The human brain, for example, is believed to have a transmodal “semantic hub” (Patterson et al., 2007; Ralph et al., 2017) located in the anterior temporal lobe that integrates and stores semantic information from various modality-specific “spokes” (e.g., visual/auditory cortices). A model, leveraging the structural commonalities across data types, could similarly project their surface forms into a shared representation space, perform computations in it, and then project back out into surface forms when needed. To what extent is this idealized strategy adopted by actual models? Wendler et al. (2024) find that on simple synthetic tasks, Llama-2 (Touvron et al., 2023) maps various input languages into a shared “English space” before projecting back out into another language, hinting that it leverages this shared representation scheme to an extent, at least for different languages. We show that this is a much more general phenomenon: when a model processes inputs from multiple data types, there is a shared representation space, and this space is scaffolded by the LM’s inherently dominant data type (usually English). By scaffolded, we mean that the shared space can be interpreted to an extent in the dominant data type via the logit lens (nostalgebraist, 2020). Following the cognitive science nomenclature, we call this shared representation space the LM’s “semantic hub.” We first show that LMs represent semantically similar inputs from distinct data types (across languages, or between natural language and arithmetic expressions, code, formal semantic structures, and multimodal inputs) to be close to one another in intermediate LM layers. We further show that we can interpret these hidden representations to an extent using the LM’s dominant data type—e.g., when processing a Chinese input, an English-dominant LM “thinks” in English before projecting back out to a Chinese space. Finally, we perform intervention experiments showing that intervening in the shared representation space using the LM’s dominant data type, predictably affects model output when processing other data types; that is, the shared representation space (and the processing of these representations through subsequent layers) is not a vestigial byproduct of the model’s being trained on (say) English-dominant text, but causally impacts model behavior. Our work is complementary and distinct from prior work which finds structural similarities between the representation spaces of models trained (usually independently) on different data types, such as those showing that text representations from text-only LMs can be aligned to vision/audio representations of modality-specific models (Ilharco et al., 2021; Merullo et al., 2022; Li et al., 2023; Ngo & Kim, 2024; Huh et al., 2024; i.a.), and the literature on cross-lingual word embedding alignment (Mikolov et al., 2013; Artetxe et al., 2017; Conneau et al., 2018; Schuster et al., 2019; i.a.). We instead show that an LM trained on multiple data types represents and processes them in a shared space without requiring explicit alignment. We hope our findings shed light on ways to more easily interpret the mechanisms of current models and motivate future work aimed at better controlling models using these insights.222We release our code at https://github.com/ZhaofengWu/semantic-hub."
https://arxiv.org/html/2411.04965v1,BitNet a4.8: 4-bit Activations for 1-bit LLMs,"Recent research on the 1-bit Large Language Models (LLMs), such as BitNet b1.58 [12], presents a promising direction for reducing the inference cost of LLMs while maintaining their performance. In this work, we introduce BitNet a4.8, enabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid quantization and sparsification strategy to mitigate the quantization errors introduced by the outlier channels. Specifically, we utilize 4-bit activations for inputs to the attention and feed-forward network layers, while sparsifying intermediate states followed with 8-bit quantization. Extensive experiments demonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58 with equivalent training costs, while being faster in inference with enabling 4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of parameters and supports 3-bit KV cache, further enhancing the efficiency of large-scale LLM deployment and inference.","Recent works [12] have shown that 1-bit LLMs can match the full-precision models given the same amount of parameters and training tokens while being significantly cost-effective in terms of latency, memory, throughput, and energy consumption. With model weights represented in 1.58-bit (i.e., {-1, 0, 1}), the bottleneck of inference has shifted from the limited memory bandwidth to high computational cost. Low-bit or sparse activations in LLMs served as a promising approach to further reduce the computational budget while maintaining performance on downstream tasks. One common approach is to utilize activation sparsity [9, 16, 8], which reduces the inference FLOPs and the I/O of weight by pruning the activation entries with smaller magnitudes. Sparsification is particularly well-suited for handling activations that exhibit highly imbalanced long-tailed distributions. Recent works [18] have demonstrated that LLMs with fully sparsely-activated activations can achieve results comparable to dense models while having much less active parameters. In addition to sparsification, activation quantization is another approach to accelerate the matrix multiplication. However, the optimization of neural networks with low-bit activations is challenging due to the emergence of outlier dimensions as the training progresses and the model size grows. Despite these outliers only account for a very small portion of the activations [5, 20], they have much larger magnitude, which leads to significant quantization errors and performance degradation on downstream tasks. Previous works [19, 1, 11, 10] mostly utilize Hadamard or learnable rotary transformation to amortize the outlier features into other entries. However, they are mostly designed for the LLMs of higher precision (e.g., 4-bit). For 1-bit LLMs, the extremely low bit-width of the weights makes it challenging to absorb these transformation matrices directly into the weights, while leaving them as online transformations introduces additional computational overhead and limits overall inference performance. In this work, we introduce BitNet a4.8, a hybrid quantization and sparsification strategy that enables 4-bit activations for 1-bit LLMs. By carefully analyzing the activation distribution of 1-bit LLMs, we selectively apply 4-bit quantization or sparsification based on the distribution patterns of these activations. Specifically, as shown in Figure 1, BitNet a4.8 employs 4-bit activations for the inputs to attention and FFN, while utilizing sparsification with 8 bits for intermediate states. To improve the training efficiency, BitNet a4.8 is trained from 8-bit to 4-bit activations with a two-stage recipe, which requires only a few training tokens to adapt BitNet b1.58 to the low-bit activations at the end of training. Extensive experiments demonstrate that BitNet a4.8 achieves competitive performance to BitNet b1.58 with the same training cost while being significantly more efficient at inference time. Furthermore, BitNet a4.8 has only 55% activated parameters and supports 3-bit KV cache, which further enhances the efficiency of LLM deployment."
https://arxiv.org/html/2411.04950v1,Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach,"Stylometry aims to distinguish authors by analyzing literary traits assumed to reflect semi-conscious choices distinct from elements like genre or theme. However, these components often overlap, complicating text classification based solely on feature distributions. While some literary properties, such as thematic content, are likely to manifest as correlations between adjacent text units, others, like authorial style, may be independent thereof. We introduce a hypothesis-testing approach to evaluate the influence of sequentially correlated literary properties on text classification, aiming to determine when these correlations drive classification. Using a multivariate binary distribution, our method models sequential correlations between text units as a stochastic process, assessing the likelihood of clustering across varying adjacency scales. This enables us to examine whether classification is dominated by sequentially correlated properties or remains independent. In experiments on a diverse English prose corpus, our analysis integrates traditional and neural embeddings within supervised and unsupervised frameworks. Results demonstrate that our approach effectively identifies when textual classification is not primarily influenced by sequentially correlated literary properties, particularly in cases where texts differ in authorial style or genre rather than by a single author within a similar genre.","When considering the process of text composition, numerous properties come into play, with genre, style, and theme being particularly significant. Identifying the primary factor distinguishing texts among these three is crucial for tasks like authorship attribution and forensic linguistics (Holmes, 1994; Koppel and Schler, 2004). Style, theme, and genre are fundamental properties that play distinct yet intertwined roles in the composition and interpretation of textual content. Style refers to the distinctive manner in which a writer expresses ideas, characterized by choices in vocabulary, sentence structure, tone, and rhetorical devices. It encompasses the unique linguistic fingerprint of an author and contributes to the overall aesthetic and readability of the text (e.g., Saukkonen, 2003). The theme, conversely, pertains to the central ideas, messages, or subjects explored within the text. It reflects the underlying concepts or motifs that recur throughout the narrative, conveying deeper meaning and resonance to the reader (e.g., Brinker, 1995). Genre, meanwhile, categorizes texts into distinct literary or textual forms based on shared conventions, structures, and expectations (e.g., Chandler, 1997). It provides a framework for understanding and classifying texts according to their overarching narrative structures, plot elements, and stylistic conventions. In computational linguistics, characterizing specific desired properties of a text – such as theme, style, or genre – poses significant challenges due to their abstract and multifaceted nature Rybicki et al. (2016). These parameters encompass a broad range of linguistic elements, making them difficult to quantify and model computationally. Furthermore, these parameters vary widely across authors and texts, posing challenges for developing universal computational models (Serrano et al., 2009). Identifying patterns and connections within these parameters involves deciphering complex semantic and conceptual relationships, which can be context-dependent and subject to individual interpretation. This variability makes it challenging to automate analysis reliably across diverse texts. Additionally, the classification of texts based on these parameters faces similar challenges, as texts often defy traditional categorization and exhibit hybrid or evolving conventions. Additionally, such classification relies on intricate intertextual and extratextual factors, such as cultural context and reader expectations, which are challenging to formalize computationally. Overall, literary works’ abstract and multidimensional nature presents formidable obstacles in computational linguistics, demanding innovative approaches and interdisciplinary collaborations for adequate characterization. Of particular interest is the discipline of stylometry, which presupposes the possibility of distinguishing between distinct authorial styles (that are independent of thematic content) based on statistical learning analyses Kestemont (2014). Identifying such a stylistic property is helpful in a variety of real-world settings (e.g., forensic linguistics Lambers and Veenman (2009), plagiarism detection (Ainsworth and Juola, 2018), and authorship analysis (Kestemont et al., 2016) – where it has been shown that other textual properties are often misleading in asserting authorship (Stamatatos, 2009, 2018)). As with many statistical-learning tasks, approaches in stylometry are usually categorized into supervised and unsupervised settings Holmes (1994, 1998); Juola (2006). In the supervised case, several distinct authors and their works are provided (for training), and the task is to attribute texts to one of the authors under consideration, a task known as authorship attribution. Within this context, two distinct assumptions can be made: (1) that the author of each unattributed text is necessarily found within the given set of authors Koppel et al. (2009), and (2) that the composer of an unattributed text need not be among the authors under consideration Koppel et al. (2011). Alternatively, it may be desirable, given a single known author and some prior texts thereof, to verify whether some unknown work was also composed by this author or, instead, by a different one – a task known as authorship verification (e.g., Koppel et al., 2007; Juola, 2006; Kestemont et al., 2016). Other supervised settings include authorship profiling, where the labeling required is not for a distinct author but for a group with some shared profile (e.g., historical era, native language, gender, socio-economic class) Argamon et al. (2009). Furthermore, variants of these tasks include cross-genre settings, where the train set is from a different genre than the test set (e.g., prose-poetry Kestemont et al. (2012)), cross-domain settings (e.g., Facebook-Twitter Markov et al. (2018)), or both (Stamatatos, 2018). Lastly, there exists a variety of different case studies that incorporate different languages, historical eras, text sizes, and more (Schwartz et al., 2013). In the unsupervised setting, no prior knowledge of the authors is provided. For a corpus of unlabeled texts (or a single text split into text units, such as sentences, paragraphs, or some arbitrary partition), it is required to estimate which texts were composed by the same author. At the root of this unsupervised task is the fundamental question: Given two texts, did the same author write them or not? As the challenge at hand is identifying a typical style regardless of thematic content, literary features used in stylometry focus on the abundant rather than the scarce (Koppel and Winter, 2014). Unique standing-out words that allude to content are usually ignored, and preferred features are traditionally subsets of the most frequent words and sequences thereof, as these could attest to some authorial style that persists through different works regardless of content (e.g., Holmes, 1994; Argamon et al., 2007; Stamatatos, 2009; Escalante et al., 2011). Even today, when an interpretable analysis is of importance, commonly used features are n𝑛nitalic_n-gram features of words, characters (k𝑘kitalic_k-mers), part-of-speech tags (POS), or dependency parsing tags, which are usually applied with a frequency-dependent vectorization such as tf-idf Sammut and Webb (2010). While some attention has been devoted to a thorough assessment of the utility of different literary features in various settings, most of this attention has focused on variants of the supervised setting of stylometry Koppel et al. (2013); Eder (2015); Juola et al. (2019). In the unsupervised setting, much less has been studied about the effects of different linguistic features and their dependence on textual properties such as genre or language-specificity. Moreover, the focus has been on assessing the contribution of different features and methods in achieving benchmarks. However, to the best of our knowledge, no prior work deals with ascertaining what part of the signal found can verifiably be attributed to different constituents of the literary signal, which can lead to false assertions concerning the results of such analyses. Ensuring that an authorship attribution algorithm remains agnostic of qualities of the text that may be independent of style, such as theme or genre, is crucial for robust and accurate analysis (e.g., Eder, 2015; Rudman, 2016; Murauer and Specht, 2021). Thus, the algorithm can focus solely on identifying each author’s unique stylistic and linguistic patterns. This approach guards against biases arising from thematic similarities or differences between texts, which could obscure genuine authorial signatures. However, achieving such agnosticism presents challenges. Texts encompass a multitude of features, and separating the relevant stylistic cues from the thematic or genre-related elements – that are often closely coupled to style, requires sophisticated algorithms capable of discerning subtle linguistic nuances (e.g., Halvani et al., 2020). Moreover, a supervised algorithm must be trained on diverse datasets spanning various themes and genres to ensure its adaptability and effectiveness across different textual domains. Furthermore, employing interpretable methods is essential to ensure that the algorithm performs according to expectations, allowing researchers to understand and validate the reasoning behind its attribution decisions (e.g., Theophilo et al., 2022). Thus, balancing the need for agnosticism of non-stylistic content with the complexity of textual data poses a significant obstacle in developing authorship attribution algorithms that are both reliable and versatile. In this work, we present and explore a novel approach that takes a first step towards identifying which constituent(s) of the literary signal (e.g., thematic, stylometric, genre-wise) is responsible for a distinction between a pair of texts. Specifically, we wish to rule out that the distinction is based on constituents that exhibit correlations between adjacent text units that include but are not limited to thematic content. Our method employs a hypothesis-testing framework that analyzes sequential correlations of label sequences of text units. By developing a stochastic model for these correlations, we provide an empirical basis for assessing whether sequentially correlated literary properties dominate classification outcomes. We demonstrate that relying solely on embeddings intended to capture stylistic features is often insufficient to rule out non-stylistic influences, especially when text units are short. Instead, our approach models sequential correlation distributions independently of any specific embedding or algorithm. Since adjacent text units tend to be correlated by, for example, thematic content, rather than necessarily by authorial style (in the event of distinct authors), they naturally exhibit similarity when embedded. We consider pairs of texts made of text units of English prose of different similarity in genre, composed by either other or the same author. We conduct a lateral study by applying unsupervised and golden-standard supervised methods to distinguish between pairs of texts embedded using traditional and state-of-the-art neural embeddings optimized for authorship attribution. In each case, we apply our hypothesis-testing framework to determine whether the resulting classification is predominantly affected by sequential correlations – as we would expect for two texts written by the same author and are similar in genre and are therefore likely to cluster based on thematic distinctions – or not."
https://arxiv.org/html/2411.04920v1,GPTKB: Building Very Large Knowledge Bases from Language Models,"General-domain knowledge bases (KB), in particular the “big three” –Wikidata, Yago and DBpedia– are the backbone of many intelligent applications. While these three have seen steady development, comprehensive KB construction at large has seen few fresh attempts.In this work, we propose to build a large general-domain KB entirely from a large language model (LLM). We demonstrate the feasibility of large-scale KB construction from LLMs, while highlighting specific challenges arising around entity recognition, entity and property canonicalization, and taxonomy construction. As a prototype, we use GPT-4o-mini to construct GPTKB, which contains 105 million triples for more than 2.9 million entities, at a cost 100x less than previous KB construction projects.Our work is a landmark for two fields: For NLP, for the first time, it provides constructive insights into the knowledge (or beliefs) of LLMs. For the Semantic Web, it shows novel ways forward for the long-standing challenge of general-domain KB construction. GPTKB is accessible at https://gptkb.mpi-inf.mpg.de.","Figure 1: Overview of our approach for LM-based KB construction. General-world knowledge bases (KB) like Wikidata Vrandecic and Krötzsch (2014), Yago Suchanek et al. (2007) and DBpedia Auer et al. (2007) are important backbones for intelligent applications. While these projects exist for over a decade, innovation in the field of general-world KB construction is low, with neither fundamental paradigm shifts nor major novel projects emerging Weikum et al. (2021). Recently, large language models (LLMs) stirred up many fields of AI Bubeck et al. (2023), and also been proposed as sources for structured knowledge Petroni et al. (2019). More specifically, Cohen et al. (2023) showed how, in principle, one can build a KB from an LLM by simple factual prompts and iterative graph expansion, but have not attempted this at scale. The success of LLMs has also raised important intrinsic questions, in particular, what and how much these models know or believe Petroni et al. (2019); Jiang et al. (2020); Roberts et al. (2020); Veseli et al. (2023); Sun et al. (2024); Wu et al. (2024b). A large set of benchmarks and studies investigate this via example-based prompting, e.g., to determine, how many answers to common benchmarking question answering (QA) datasets are known by LLMs. However, all these works remain non-exhaustive, investigating samples from specific datasets or domains, without attempting to materialize all knowledge of an LLM. In this paper we propose to comprehensively materialize the knowledge/beliefs111The terminology here is contentious, see Section 6.1. of LLMs into a KB. In particular, we propose to use iterative graph expansion to obtain the complete set of (named-entity-centric) LLM knowledge, and to consolidate that knowledge with LLM-based entity disambiguation, class and relation canonicalization, and taxonomy induction (see Fig. 1). This proposal faces several challenges: 1. Termination/cost/runtime: State-of-the-art KBs contain millions of entries, and LLMs are known to hallucinate. It is therefore not clear if and when iterative graph exploration will terminate, and how to perform it under practical monetary and time constraints. 2. Flexible and relevant knowledge elicitation: LLMs possess a wide variety of knowledge, and the amount of knowlege per entity varies. We need a method that elicits as much general-world knowledge as possible, without encouraging hallucinations, and without falling into bottomless corners, e.g., open-ended phrases or translations. 3. Canonicalization and disambiguation: Hallmarks of existing KBs are disambiguated entities and coherent classes, relations and taxonomies. Iterative prompting risks surfacing expressions that are not globally coherent. Our approach builds on the following ideas: To overcome scaling issues, and obtain relevant knowledge, we utilize a commercial API that allows to massively send batch requests (GPT-4o-mini), and utilize named entity recognition (NER) and carefully crafted prompts to restrict the explored spaces, along with prompts that allow varied answer sizes. To obtain a coherent KB, we perform a set of canonicalization and disambiguation steps, entirely relying on the LLM itself. In summary, our salient contributions are: 1. To the best of our knowledge, we are the first to propose to construct comprehensive KBs entirely from LLMs. 2. We develop a KB construction pipeline that overcomes termination, quality and runtime issues. 3. We construct GPTKB, the first large-scale KB entirely built from an LLM, containing over 104M assertions for over 2.9M entities. Our work is a significant step for two communities: For the NLP community, for the first time, we provide a proof-of-concept methodology that enables constructive insights into what LLMs know (or believe). For the Semantic Web community, we provide fresh momentum for the long stale task of open-domain KB construction, and provide code, a concrete resource, GPTKB, both as a 3.8 GB download, and via an online browsing interface and SPARQL query interface at https://gptkb.mpi-inf.mpg.de."
https://arxiv.org/html/2411.04914v1,GASE: Generatively Augmented Sentence Encoding,"We propose an approach to enhance sentence embeddings by applying generative text models for data augmentation at inference time. Unlike conventional data augmentation that utilises synthetic training data, our approach does not require access to model parameters or the computational resources typically required for fine-tuning state-of-the-art models. Generatively Augmented Sentence Encoding uses diverse linguistic synthetic variants of input texts generated by paraphrasing, summarising, or extracting keywords, followed by pooling the original and synthetic embeddings. Experimental results on the Massive Text Embedding Benchmark for Semantic Textual Similarity (STS) demonstrate performance improvements across a range of embedding models using different generative models for augmentation. We find that generative augmentation leads to larger performance improvements for embedding models with lower baseline performance. These findings suggest that integrating generative augmentation at inference time adds semantic diversity and can enhance the robustness and generalizability of sentence embeddings for embedding models. Our results show that the degree to which generative augmentation can improve STS performance depends not only on the embedding model but also on the dataset. From a broader perspective, the approach allows trading training for inference compute.","Representation learning has emerged as a fundamental technique in natural language processing (NLP). Still, the quality and robustness of embeddings largely depend on the richness and diversity of the training data. To derive sentence embeddings from BERT (Devlin et al., 2019) word embeddings, these can be averaged or the embedding of the CLS token111CLS tokens are special tokens placed at the beginning of each input example in a BERT model, providing a representation of the entire input for use in classification tasks (Devlin et al., 2019). being used. Based on encoder-style transformers, such as BERT, Sentence Transformers (STs) (Reimers and Gurevych, 2020) were trained to directly output sentence embeddings by calculating an attention-weighted average of the word embeddings. These sentence embeddings can be used in pairwise tasks, e.g. paraphrase identification, or non-pairwise tasks, e.g. sentiment analysis. Recent advancements in generative models, such as OpenAI’s GPT-4 (OpenAI, 2023), Anthropic’s Claude 3 models (Anthropic, 2024) and Google’s Gemini models (Gemini Team et al., 2024), have shown remarkable capabilities in generating human-like text. Moreover, generative models like ChatGPT have been demonstrated to be capable of paraphrasing, e.g. for data augmentation Dai et al. (2023). The authors use ChatGPT to rephrase a given text to generate synthetic data which is used to train a downstream BERT model generating embeddings for text classification. To augment generative models, the integration with representation learning has been explored, e.g. in Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Guu et al., 2020). In contrast, we introduce Generatively Augmented Sentence Encoding (GASE), an approach to augment embedding models using generative Large Language Models (LLMs). Instead of generating synthetic data for pre-training or fine-tuning, we propose an augmentation approach which is applied at inference time by generating one or more textual variants of input data and pooling the embeddings of the original input text and the synthetic texts. The underlying hypothesis of our work on generative augmentation is that generative models can introduce semantic diversity which is not modelled by current embedding models without generative augmentation and, therefore, can benefit the performance of downstream tasks using the embeddings. Our method involves three generative tasks to generate the synthetic data from the original data: paraphrasing, summarising, and extracting keywords. To the best of our knowledge, this is the first work proposing generative models to augment sentence encoders at inference time."
https://arxiv.org/html/2411.04905v1,OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models,"Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an “open cookbook” for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.","Large Language Models (LLMs) have achieved significant success in various domains (Wang et al., 2023; Que et al., 2024; Liu et al., 2024a; c; Wu et al., 2024), particularly in code-related tasks, revolutionizing the current paradigm of software development (Qian et al., 2024; Wang et al., 2024). Code-specific LLMs have emerged as a critical area within LLM research, with tools such as ChatGPT, Copilot, and Cursor reshaping the workflows of developers. Despite this, the performance of open-source LLMs focused on code (Li et al., 2023; Tao et al., ; Lozhkov et al., 2024a; Zhang et al., 2024a) still falls short compared to state-of-the-art LLMs (Hui et al., 2024; Zhu et al., 2024), largely because these leading models keep their training datasets—an essential factor in LLM development—proprietary. This lack of transparency limits the broader research community’s ability to establish strong baselines and gain deeper insights into the workings of top-tier code LLMs. To remedy the gap, we set forth three primary goals by releasing OpenCoder and its development material: (1) Firstly, we aim to provide scholars with a meticulously curated and fully transparent strong baseline code LLM for research on mechanical interpretability and the data distribution of code LLMs. (2) Secondly, we intend to conduct in-depth investigations into the pretrain and instruction data curation pipeline for the development of stronger code LLMs. (3) Thirdly, by enabling a detailed review of the development of the models, we hope to unlock more diverse customized solutions based on transparent code LLM. Through OpenCoder, we strive to stimulate and accelerate the growth of the open-source code LLM community. Our comprehensive set of controlled experiments highlights key design choices for data curation for top-tier code LLMs in different training stages: (1) During the pretraining phase, the importance of data cleaning is highlighted (Zhou et al., 2024), emphasizing the removal of non-informative data such as pure hexadecimal code and excessively short code snippets that do not contribute to the learning process. (2) The impact of deduplication is significant, with file-level deduplication proving to be more effective than repository-level deduplication by maintaining data diversity and enhancing model performance on downstream tasks (Li et al., 2023). (3) The influence of GitHub stars is also examined, revealing that filtering data based on Github star count can possibly reduce data diversity and affect the overall data distribution, contributing to a suboptimal result (Allal et al., 2023). (4) In the annealing phase, the use of high-quality data is crucial for further enhancing the model’s capabilities, indicating that data quality is more important than quantity in the later stages of model training. (5) Finally, during the instruction tuning phase, a two-stage instruction tuning strategy is shown to be effective, allowing the model to acquire broad capabilities initially and then refine them with code-specific tasks, resulting in improved performance on both theoretical and practical coding tasks. These five key points underscore the importance of data quality, diversity, and targeted enhancement strategies in developing a high-performing code generation model like OpenCoder. This work introduces the OpenCoder, a completely open-source Code LLM, built on the transparent data process pipeline and reproducible dataset. As shown in Table 1, We provide the open cookbook to build a code LLM from scratch by providing the data cleaning pipeline, reproducible pretraining dataset, large-scale SFT Corpus, and intermediate checkpoints. OpenCoder, through its meticulous data processing and advanced training methods, has surpassed expectations by achieving top-tier results on multiple code LLM evaluation benchmarks. The introduction of the open cookbook of code LLM is designed to push forward the field of code intelligence studies and to encourage its broad use in the community of code intelligence. Table 1: The comparison of released resources between our OpenCoder with other popular open-sourced code LLMs. HumanEval scores are reported for the corresponding chat models. Models Data Processing Pipeline Reproducible Pretraining Dataset Large-scale SFT Dataset (>>>1M) Intermediate Checkpoints Training Tokens HumanEval Pass@1 Open Model Weights & Reproducible Datasets OpenCoder-8B ✓ ✓ ✓ ✓ 2.5T 83.5 StarCoder2-15B ✓ ✓ ✗ ✗ 4.1T 72.6 Crystal-7B ✗ ✓ ✗ ✓ 1.3T 34.1 Open Model Weights CodeLlama-7B ✗ ✗ ✗ ✗ 2.5T 34.8 CodeGemma-7B ✗ ✗ ✗ ✗ 6.5T 56.1 DS-Coder-V2-Lite ✗ ✗ ✗ ✗ 10.2T 81.1 Yi-Coder-9B ✗ ✗ ✗ ✗ 6.0T 85.4 Qwen2.5-Coder-7B ✗ ✗ ✗ ✗ 23.5T 88.4"
https://arxiv.org/html/2411.04847v1,Prompt-Guided Internal States for Hallucination Detection of Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of tasks in different domains. However, they sometimes generate responses that are logically coherent but factually incorrect or misleading, which is known as LLM hallucinations. Data-driven supervised methods train hallucination detectors by leveraging the internal states of LLMs, but detectors trained on specific domains often struggle to generalize well to other domains. In this paper, we aim to enhance the cross-domain performance of supervised detectors with only in-domain data. We propose a novel framework, prompt-guided internal states for hallucination detection of LLMs, namely PRISM. By utilizing appropriate prompts to guide changes in the structure related to text truthfulness within the LLM’s internal states, we make this structure more salient and consistent across texts from different domains. We integrated our framework with existing hallucination detection methods and conducted experiments on datasets from different domains. The experimental results indicate that our framework significantly enhances the cross-domain generalization of existing hallucination detection methods111We have open-sourced all the code, data, and models in GitHub: https://github.com/fujie-math/PRISM.","In recent years, large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks in different domains Dinan et al. (2018); Brown (2020); Zhang et al. (2022); Chowdhery et al. (2023). However, the hallucination problem in LLMs poses a potential threat to their practical application in many scenarios. LLM hallucinations refers to the cases where LLMs generate responses that are logically coherent but factually incorrect or misleading Zhou et al. (2020); Ji et al. (2023); Su et al. (2024). These hallucinated responses may be blindly accepted, leading users to learn incorrect information or take inappropriate actions. Therefore, detecting hallucinations in LLM-generated content becomes particularly meaningful. By effectively identifying incorrect generated content through a hallucination detector, users can be alerted to verify the accuracy of certain portions of the generated content and guide the LLM to regenerate relevant sections, thus preventing potential issues. The unsupervised paradigm focuses on assessing the confidence of generated text and rejects the low-confidence ones. For example, the probability information for each token generated by LLMs can serve as a measure of hallucination Kadavath et al. (2022); Zhang et al. (2023); Quevedo et al. (2024); Hou et al. (2024). Additionally, the consistency among multiple responses generated by the LLM to the same question can also be used to evaluate their confidence Manakul et al. (2023). Chen et al. (2024) shifts the judgment of consistency among multiple responses to their corresponding internal vectors. However, these methods often struggle to achieve ideal detection accuracy or require a significant amount of additional response time Su et al. (2024). For this reason, researchers have begun exploring the use of data-driven supervised methods for detection. These methods are generally based on the assumption that LLMs can recognize they have generated incorrect content, which is reflected in specific patterns within their internal states. Marks and Tegmark (2023) reveal that true and false statements have distinguishable geometric structures within the LLM, allowing us to build classifiers by learning this structure. Marks and Tegmark (2023) and Sky et al. (2024) utilize linear structures to distinguish between different statements. Azaria and Mitchell (2023) train fully connected networks to act as hallucination detectors by using activation values within the LLM. Supervised detectors trained on specific domains often struggle to achieve good generalization performance in other domains. For example, Bürger et al. (2024) found that the structural differences in the LLM’s internal states for affirmative sentences differ significantly from those for negated sentences when distinguishing between true and false statements. To address this issue, many studies focus on constructing diverse datasets or performing feature selection within the LLM to achieve better generalization performance Chen et al. (2023); Bürger et al. (2024); Liu et al. (2024). However, these methods require collecting additional training data from other domains, which is resource-intensive. In this paper, we aim to answer the following research question: Can we enhance the cross-domain performance of supervised detectors without using datasets from other domains? Driven by this research question, we propose a novel framework called PRISM, which stands for prompt-guided internal states for hallucination detection of LLMs. By utilizing appropriate prompts to guide changes in the structure related to text truthfulness within the LLM’s internal states, we make this structure more salient and consistent across texts from different domains. This enables detectors trained in one domain to perform well in others without additional data. Our approach is based on the insight that while the internal states of LLMs encode rich semantic information, they are primarily optimized during pre-training to predict the next token rather than for hallucination detection. As a result, the directly extracted internal states contain a lot of domain-specific information unrelated to truthfulness, leading to detectors that are specific to certain domains and unable to achieve good cross-domain generalization. Due to LLM’s powerful ability to understand and follow instructions, we explore the use of prompt-guided methods to generate internal states more focused on truthfulness. We employed various methods to investigate the effect of prompts from different perspectives. The findings indicate that the introduction of appropriate prompts can significantly improve the salience of the structure related to text truthfulness in the LLM’s internal states and make this structure more consistent across different domain datasets. We also provide a simple and effective method to generate and select appropriate prompts for hallucination detection tasks. By combining the prompt template with the text to be evaluated and inputting them into the LLM, we obtain internal states that are better suited as features for hallucination detection tasks. Subsequently, we can integrate with existing hallucination detection methods to construct suitable detectors for classifying the text. In summary, the contributions of this paper are as follows: (1) We conduct an in-depth investigation into how specific prompts influence the internal state structures of LLMs with regard to text truthfulness. (2) We propose PRISM, a novel framework that utilizes prompt-guided internal states to enhance hallucination detection in LLMs. (3) Successfully integrated PRISM with existing hallucination detection methodologies, demonstrating its ability to significantly enhance generalization performance across various domains. Figure 1: Visualization of the LLaMA2-7B-Chat model’s internal states by using 2-dimensional PCA on the True-False dataset before and after the introduction of Prompt 1, where label 0 represents false statements and label 1 represents true statements. Additionally, a logistic regression model is fitted to distinguish between true and false statements, with the decision boundary shown as a black dashed line."
https://arxiv.org/html/2411.04825v1,VTechAGP: An Academic-to-General-Audience Text Paraphrase Dataset and Benchmark Models,"Existing text simplification or paraphrase datasets mainly focus on sentence-level text generation in a general domain. These datasets are typically developed without using domain knowledge. In this paper, we release a novel dataset, VTechAGP, which is the first academic-to-general-audience text paraphrase dataset consisting of 4,938 document-level these and dissertation academic and general-audience abstract pairs from 8 colleges authored over 25 years. We also propose a novel dynamic soft prompt generative language model, DSPT5. For training, we leverage a contrastive-generative loss function to learn the keyword vectors in the dynamic prompt. For inference, we adopt a crowd-sampling decoding strategy at both semantic and structural levels to further select the best output candidate. We evaluate DSPT5 and various state-of-the-art large language models (LLMs) from multiple perspectives. Results demonstrate that the SOTA LLMs does not provide satisfactory outcomes, while the lightweight DSPT5 can achieve competitive results. To the best of our knowledge, we are the first to build a benchmark dataset and solutions for academic-to-general-audience text paraphrase dataset.","Text generation aims to produce understandable text in human language from various sources of input data. Among them, text-to-text generation remains an important and challenging task with extensive applications such as language translation Ranathunga et al. (2023); Dabre et al. (2020), paraphrase generation Singh and Josan (2022), text simplification Martin et al. (2023), etc. Existing text simplification and paraphrase generation datasets (shown in Table 5 in the Appendix) mainly focus on sentence-level translation. The recent paragraph-level dataset WikiAuto Jiang et al. (2020), derived from WikiLarge Zhang and Lapata (2017) and Newsela Xu et al. (2015), still suffers from a lack of domain diversity and specification, which are sourced from Wikipedia and news articles. Besides, the objective of these studies on the aforementioned datasets is to simplify the text for children at lower grade levels. Furthermore, existing works on lay summarization involves brief summarization limited to medicine and biological domains Devaraj et al. (2021); Flores et al. (2023); Jiang et al. (2024); Zaman et al. . However, our proposed VTechAGP involves translation from technical language to general-audience language across a broader set of multiple domains. We aim to generate text from the domain-specific to a general level of understanding, while keeping it scientifically accurate and easy to understand, in order to encourage and facilitate interdisciplinary collaboration across different research fields. In this paper, we pioneer the research of academic-to-general-audience text generation by introducing a new benchmark dataset VTechAGP, which is derived from electronic theses and dissertations (ETDs) at Virginia Tech over twenty-five years. VTechAGP consists of 4,938 document-level abstract pairs (academic abstract and general-audience abstract). VTechAGP also provides other information such as title, discipline, degree level, etc. This auxiliary information shows the potential of VTechAGP for other tasks such as topic generation, etc. In addition, the abstracts in VTechAGP come from multiple domains (colleges) and VTechAGP is labeled with each specific domain and provides the domain knowledge keywords. More details about VTechAGP are presented in Sec. 3 and Table 6. Based on VTechAGP, we evaluate several SOTA pre-trained large language models (LLMs), such as LLaMA2 Touvron et al. (2023), Claude2 Anthropic (2023), ChatGPT Brown et al. (2020), etc., to establish the baseline performance. However, these SOTA pre-trained LLMs have demonstrated the following limitations: (1) Some LLMs do not provide public APIs, or the APIs are not free. Also, some LLMs (e.g., Claude2) do not provide fine-tuning, making them less adaptable to specific tasks. (2) The model size of LLMs is very large. For example, LLaMA2 has about 65 billion parameters. Fine-tuning these LLMs is resource-intensive in terms of memory and computation time. Even the inference implementation requires more memory and time. (3) The pre-trained LLMs do not show competitive performance for the academic-to-general-audience text paraphrasing task on VTechAGP in Sec. 5. To address the above challenges, we propose DSPT5, a dynamic soft prompt-based generative model with the crowd sampling decoding strategy during the inference stage. DSPT5 is built based on the pre-trained T5 Raffel et al. (2020), which has only about 220 million parameters. In particular, the dynamic soft prompt template in DSPT5 can automatically adapt to different academic domains by changing the keywords extracted from the academic abstract. The prompt encoder in DSPT5 is trained to generate and fine-tune keyword vectors combined with the dynamic prompt template. To this end, we design a hybrid loss function with generative language model loss and contrastive loss to jointly learn the generated text representations as well as the ability to distinguish technical keywords from non-technical keywords. During inference, DSPT5 employs two alignment functions at both the semantic and structural levels to select the best candidate for the final generated output. The contributions can be summarized as follows: (1) Dataset: We construct VTechAGP, the first academic-to-general-audience text paraphrase dataset. VTechAGP is a document-level text generation dataset with multiple technical domains. (2) Baselines: We implement several SOTA LLMs as benchmarks to compare the performance with our proposed model DSPT5. Experimental results show that there is still a huge room for further improvement of the existing LLMs. (3) Approach: We propose a lightweight model, DSPT5, which utilizes dynamic soft prompts with a hybrid loss function and a new crowd decoding strategy. Experimental results show that DSPT5 can achieve competitive results with SOTA LLMs. (4) Evaluation: We explore various evaluation metrics for the academic-to-general-audience text paraphrasing task on VTechAGP from different perspectives, including document-level embedding-based, word-based, and end-to-end metrics. In addition, simplicity, diversity, readability, and toxicity are also considered for the performance evaluation."
https://arxiv.org/html/2411.04813v1,LuxBank: The First Universal Dependency Treebank for Luxembourgish,"The Universal Dependencies (UD) project has significantly expanded linguistic coverage across 161 languages, yet Luxembourgish, a West Germanic language spoken by approximately 400,000 people, has remained absent until now. In this paper, we introduce LuxBank, the first UD Treebank for Luxembourgish, addressing the gap in syntactic annotation and analysis for this ‘low-research’ language. We establish formal guidelines for Luxembourgish language annotation, providing the foundation for the first large-scale quantitative analysis of its syntax. LuxBank serves not only as a resource for linguists and language learners but also as a tool for developing spell checkers and grammar checkers, organising existing text archives and even training large language models. By incorporating Luxembourgish into the UD framework, we aim to enhance the understanding of syntactic variation within West Germanic languages and offer a model for documenting smaller, semi-standardised languages. This work positions Luxembourgish as a valuable resource in the broader linguistic and NLP communities, contributing to the study of languages with limited research and resources.","The Universal Dependencies (UD) project has facilitated the production of treebanks across many languages, although some languages are still not represented almost 10 years after its original release Nivre et al. (2016). With 161 languages represented as of the latest release, and a total of 283 treebanks across these languages, the language coverage is undeniably vast.111Latest release at the time of writing: 15.05.2024. The range of languages includes many of the major world languages, as well as varieties and dialects. However, some languages are still not represented at all, and Luxembourgish was one such case until recently. A West Germanic language closely related to German, Luxembourgish is spoken by roughly 400,000 people, mainly in Luxembourg Gilles (2019). Historically, Luxembourg has had a complex multilingual society where French and German have been predominantly used for official and formal (written) communication. In contrast, Luxembourgish was mostly a spoken language used informally between Luxembourgers until recently. With the rise of digital and social media, however, Luxembourgish has started to develop in the written domain and significant amounts of text data have started to become available, coupled with active language policies promoting Luxembourgish. Research in Natural Language Processing (NLP) for Luxembourgish has been limited until now, often in favour of French, German, and English. This has resulted in a situation where Luxembourgish is considered by some to be a ‘low-research’ language, as opposed to a low-resource language. In addition, large-scale syntactic annotation and analysis has not been undertaken before for Luxembourgish, making Luxembourg one of the few countries whose national language is not represented in the UD treebanks. This remains true despite the fact that four treebanks are available for Standard German Völker et al. (2019); McDonald et al. (2013); Zeman et al. (2018); Basili et al. (2017), as well as three non-standard treebanks for Swiss German Aepli (2018), Low Saxon Siewert et al. (2021) and Bavarian Blaschke et al. (2024). None of these represent a Middle-German variety, however, indicating an opportunity to extend the coverage for varieties of (or related to) German. Aiming to address this gap in research, we present LuxBank, the first UD treebank for Luxembourgish. This project will be the first large-scale quantitative analysis of Luxembourgish syntax, and with this paper, we introduce the first formal guidelines for Luxembourgish language annotation. To this end, we present work related to Luxembourgish in Section 2 and describe the creation of LuxBank in Section 3, including highlighting notable syntactic phenomena. We discuss difficulties encountered in the creation process in Section 4 and conclude the paper with Section 5."
https://arxiv.org/html/2411.04799v1,"Kwai-STaR:
Transform LLMs into State-Transition Reasoners","Mathematical reasoning presents a significant challenge to the cognitive capabilities of LLMs. Various methods have been proposed to enhance the mathematical ability of LLMs. However, few recognize the value of state transition for LLM reasoning. In this work, we define mathematical problem-solving as a process of transiting from an initial unsolved state to the final resolved state, and propose Kwai-STaR framework, which transforms LLMs into State-Transition Reasoners to improve their intuitive reasoning capabilities. Our approach comprises three main steps: (1) Define the state space tailored to the mathematical reasoning. (2) Generate state-transition data based on the state space. (3) Convert original LLMs into State-Transition Reasoners via a curricular training strategy. Our experiments validate the effectiveness of Kwai-STaR in enhancing mathematical reasoning: After training on the small-scale Kwai-STaR dataset, general LLMs, including Mistral-7B and LLaMA-3, achieve considerable performance gain on the GSM8K and GSM-Hard dataset. Additionally, the state transition-based design endows Kwai-STaR with remarkable training and inference efficiency. Further experiments are underway to establish the generality of Kwai-STaR. ††††\dagger†: Equal contribution.","As an intricate problem for artificial intelligenceAhn et al. (2024), mathematical reasoning poses a significant challenge to the cognitive abilities of large language models (LLMs). Recently, various efforts have been dedicated to enhancing LLMs’ mathematical capabilities, achieving significant progress. A range of work focuses on enhancing the inherent mathematical abilities of LLMs through training: GPT Achiam et al. (2023) and LLaMA Dubey et al. (2024) incorporate massive mathematical corpus to improve general intelligence, OpenMath Toshniwal et al. (2024) and MetaMath Yu et al. (2023) specialize general LLMs into math-tailored models with large-scale in-domain datasets. For these methods, organizing mathematical corpus into structured data instances becomes an essential step. Another line of research, including CoT Wei et al. (2022) and Best-of-N Stiennon et al. (2020), explores how to fully harness the potential of LLMs during inference to boost mathematical performance. Recently, Monte Carlo Tree Search Qi et al. (2024); Zhang et al. (2024) and Process Reward Model Wang et al. (2024) have been applied; they achieve remarkable results by decomposing problem-solving process into multiple steps and providing intermediate rewards. Whether for training or inference paradigm, the modeling of mathematical reasoning process is playing an increasingly significant role, which determines how to appropriately organize the corpus and decompose the mathematical problem. In this paper, we provide a novel perspective to organize the reasoning process of LLMs: the solution of a mathematical problem can be intuitively viewed as an ordered sequence of transition from an initial unsolved state (original question) to a final resolved state (correct answer). Based on this observation, we manage to design a state transition paradigm to assist mathematical reasoning of LLM. We present Kwai-STaR, a framework to transform general LLMs into state-transition reasoners, which systematically solve problems by performing state transition. Kwai-STaR takes three steps to incorporate state transition with LLM reasoning: (1) State space definition: We develop an action set and constrain the LLM to solve problems as a State-Transition Reasoner (STaR): The LLM takes one action from the action set at a time to transition the current state into a new state. (2) State-transition data construction. Although advanced LLMs possess remarkable instruction-following capabilities (Table 2), we still take appropriate training to help general language models master state-by-state reasoning. Hence, we construct a small-scale state-transition dataset with meticulous generation instructions and pipeline. (3) Curricular Training Strategy. Our dataset contains two types of instances: a majority of correct cases and a minority of wrong-then-verified cases from the data generator and trained reasoner. To maximize learning efficiency, our training strategy consists of two stages: a fundamental stage and an advanced stage. The former trains the model with the majority right cases, enabling it to grasp the state-transition manner and to solve relatively simple problems. The latter leverages pairs of wrong and verified cases to further strengthen the model’s proficiency. Using the above method, we construct a small-scale state-transition dataset from the GSM8K training data, our dataset contains 20K right instances and approximately 3K wrong-then-verified instances. We test Kwai-STaR’s effect on various general LLMs. Our experiment indicates that Kwai-STaR framework significantly improves the performance of all tested general LLMs on two benchmarks. Moreover, compared to other data augmentation methods, the state-transition-based Kwai-STaR dataset offers higher data efficiency. In contrast to inference-time enhancement methods, Kwai-STaR achieves single-pass accuracy comparable to those methods’ multi-pass accuracy, without complex inference paradigm. We summarize our contributions as follows: • We provide the novel perspective of state transition to model the mathematical reasoning of LLMs and construct a state-transition dataset. • We propose Kwai-STaR framework to enhance LLM reasoning through state transitions. Kwai-STaR effectively transforms models of various scales into STaRs, significantly improving their mathematical performance. • Kwai-STaR achieves remarkable performance and efficiency, revealing the great potential of state-space strategies in enhancing LLM reasoning. We are actively extending our state-space strategies to broader scenarios."
https://arxiv.org/html/2411.04794v1,AlignXIE: Improving Multilingual Information Extraction by Cross-Lingual Alignment,"Empirical evidence suggests that LLMs exhibit spontaneous cross-lingual alignment. Our findings suggest that although LLMs also demonstrate promising cross-lingual alignment in Information Extraction, there remains significant imbalance across languages, revealing an underlying deficiency in the IE alignment. To address this issue, we propose AlignXIE , a powerful code-based LLM that significantly enhances cross-lingual IE alignment through two strategies. Firstly, AlignXIE formulates IE across different languages, especially non-English ones, as code generation tasks, standardizing the representation of various schemas using Python classes to ensure consistency of the same ontology in different languages and align the schema. Secondly, it incorporates an IE cross-lingual alignment phase through a translated instance prediction task proposed in this paper to align the extraction process, utilizing ParallelNER, an IE bilingual parallel dataset with 257,190 samples, generated by our proposed LLM-based automatic pipeline for IE parallel data construction, with manual annotation to ensure quality. Ultimately, we obtain AlignXIE through multilingual IE instruction tuning. Although without training in 9999 unseen languages, AlignXIE surpasses ChatGPT by 30.17% and SoTA by 20.03%, thereby demonstrating superior cross-lingual IE capabilities. Comprehensive evaluations on 63 IE benchmarks in Chinese and English under various settings, demonstrate that AlignXIE significantly enhances cross-lingual and multilingual IE through boosting the IE alignment.","CoNLL 2003 CoNLL 2003-zh Flan-T5 12.71 2.13 InstructUIE 92.94 20.39 Table 1: Results on CoNLL 2003 and CoNLL 2003-zh, the latter being the constructed Chinese parallel dataset. Multilingual Information Extraction (Multilingual IE) focuses on automatically extracting structured knowledge from unstructured or semi-structured texts across various languages, following manually designed schemas. Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023a, b), trained on massive multilingual corpora, have significantly advanced multilingual language processing. Previous studies (Zhang et al., 2023; Qi et al., 2023; Wang et al., 2024; Gao et al., 2024) have shown that LLMs exhibit spontaneous cross-lingual alignment to facilitate the transfer of abilities and knowledge across languages. Our findings suggest the presence of this alignment in IE, indicating a strong potential for improving the IE cross-lingual transfer. We first define IE parallel data, which is across various languages that share the same schema, sentences, and extracted instances. Previous studies proposed label projection to generate such data to alleviate the challenges of IE in low-resource Kolluru et al. (2022); Hennig et al. (2023); Rios et al. (2024). By translation and manual annotation, we constructed an IE parallel dataset in Chinese based on CoNLL 2003 Sang and Meulder (2003), to evaluate the InstructUIE Wang et al. (2023a). Table 1 shows the results. After training Flan-T5 Chung et al. (2022) on English IE to obtain InstructUIE, there was a notable enhancement in Chinese IE (2.132.132.132.13→→\rightarrow→20.3920.3920.3920.39), strongly supporting the presence of IE cross-lingual alignment. However, the significant performance gaps between languages (92.9492.9492.9492.94 vs 20.3920.3920.3920.39) indicate that the alignment remains weak. To address this issue, we propose two strategies to enhance IE cross-lingual alignment by aligning the schema and extraction process in different languages, thereby improving the cross-lingual and multilingual IE. Firstly, we formulate IE tasks across different languages, particularly non-English languages, into a unified framework of code generation. Traditional multilingual IE faces a major challenge from cross-lingual schema differences, requiring schema alignment before knowledge sharing, which hinders cross-lingual transfer. Yang et al. found that unified schema representations in a single language can significantly enhance model generalization, and thus, we construct a unified schema representation to align cross-lingual schemas. Leveraging the language-agnostic nature of code, we use the Python classes to represent schemas across various languages, which ensures consistent representation of the same or similar ontology in different languages, thereby facilitating knowledge transfer for IE. The consistent schema allows the model to learn and efficiently share the knowledge of the same ontology across languages. Secondly, we incorporate an IE cross-lingual alignment instruction tuning phase. We propose a task utilizing the IE parallel data, aiming at predicting target language instances to facilitate the alignment of the extraction process. Leveraging prediction tasks with parallel data to enhance alignment through instruction tuning has become a common paradigm in multilingual processing (Zhu et al., 2023; Gao et al., 2024). Figure 1 presents an example. Similar to the label projection, the challenge of the task is to ensure that the predicted instances remain semantically consistent with the source-language ones while appearing in the target language sentence. In contrast to predicting complete IE parallel data directly, which mainly focuses on the alignment of the target-language sentence that will be provided in IE, our method emphasizes the alignment of translated instances, which is the core objective of IE cross-lingual alignment. To use high-quality IE parallel data for the phase, we propose an automatic three-stage LLM-based pipeline that implements contextual translation and multi-grained rephrasing. The pipeline achieved 99%percent9999\%99 % average faithfulness across 10101010 languages evaluated on the label projection benchmark WikiANN Pan et al. (2017). Utilizing the pipeline, we construct a high-quality and diverse NER English-Chinese parallel dataset ParallelNER, using the GPT-4 series model supplemented with manual annotation. By distilling the IE alignment capabilities of advanced proprietary models, the IE cross-lingual alignment phase enhances cross-lingual generalization in IE tasks. Ultimately, we obtain the AlignXIE through multilingual IE instruction tuning. We evaluate the AlignXIE on 63636363 benchmarks for the NER, RE, ED, and EAE tasks in Chinese and English. AlignXIE exhibits superior cross-lingual generalization across typologically diverse languages, and surpasses ChatGPT by 30.17%percent30.1730.17\%30.17 % and SoTA by 20.03%percent20.0320.03\%20.03 %. In the supervised evaluation, AlignXIE consistently ranks within the top-2222 results across 40404040 (of 42424242) benchmarks, with an average improvement of 3.033.033.033.03 and 2.922.922.922.92 points on the English RE and ED tasks, respectively. Notably, AlignXIE achieves SoTA across all Chinese IE benchmarks under all settings, effectively demonstrating the strength of knowledge transfer from English IE through our method. Our contributions can be summarized as: • The code-based multilingual IE method of AlignXIE unifies the representation of schemas across different languages, thereby boosting cross-lingual transfer. • AlignXIE incorporates a cross-lingual alignment phase for IE to improve cross-lingual transfer. This phase involves instruction tuning on a newly proposed translated instance prediction task. • We propose an LLM-based automatic IE parallel data construction pipeline. Using this pipeline, we constructed a large-scale, high-quality bilingual parallel NER dataset ParallelNER with 257,190257190257,190257 , 190 samples to provide valuable resources to the community. Figure 1: An example of instruction-tuning data used in the IE cross-lingual alignment phase. The highlighted span in the completion and the two same color highlighted spans in the instruction must ensure semantic consistency and be exactly matched, respectively."
https://arxiv.org/html/2411.04756v1,A study of Vietnamese readability assessing through semantic and statistical features,"Determining the difficulty of a text involves assessing various textual features that may impact the reader’s text comprehension, yet current research in Vietnamese has only focused on statistical features. This paper introduces a new approach that integrates statistical and semantic approaches to assessing text readability. Our research utilized three distinct datasets: the Vietnamese Text Readability Dataset (ViRead), OneStopEnglish, and RACE, with the latter two translated into Vietnamese. Advanced semantic analysis methods were employed for the semantic aspect using state-of-the-art language models such as PhoBERT, ViDeBERTa, and ViBERT. In addition, statistical methods were incorporated to extract syntactic and lexical features of the text. We conducted experiments using various machine learning models, including Support Vector Machine (SVM), Random Forest, and Extra Trees and evaluated their performance using accuracy and F1 score metrics. Our results indicate that a joint approach that combines semantic and statistical features significantly enhances the accuracy of readability classification compared to using each method in isolation. The current study emphasizes the importance of considering both statistical and semantic aspects for a more accurate assessment of text difficulty in Vietnamese. This contribution to the field provides insights into the adaptability of advanced language models in the context of Vietnamese text readability. It lays the groundwork for future research in this area.","Exchanging information and knowledge through texts has led to the emergence of measuring text difficulty. There can be multiple ways to describe and convey content when dealing with the same issue. Among them, complex texts pose challenges for readers, as reflected in lower reading speed, poorer comprehension, and reduced capacity to connect information within the text. In recent years, text difficulty has been evaluated through linguistically motivated features, such as syntactic complexity, complexity in logical relationships and inferences of information in the text, and the sequential expression of data over time or context. Two main approaches for determining text difficulties have been proposed, namely statistical approach and machine learning or deep learning. In the former approach, text difficulty is evaluated through the synthesis of easy-to-compute features in the text, such as the length of the text, the average number of words and sentences in the text, etc. Flesch (1948); Kincaid et al. (1975), where these features are extracted and evaluated through correlation analysis with the difficulty of a set of texts. The second approach, namely machine or deep learning approach, involves using neural models to represent the semantics present in the text, allowing for the assessment of text difficulty Heilman et al. (2007, 2008); Lee et al. (2021); Si and Callan (2001). Studies addressing the problem by applying advanced neural models such as BERT and its variants combined with features extracted through traditional statistical methods have achieved promising results on English datasets such as WeeBit Vajjala and Meurers (2012), OneStopEnglish Vajjala and Lučić (2018), and Cambridge Xia et al. (2016). In Vietnam, pioneering research in this area, such as that of Nguyen and Henkin (1985); Luong et al. (2018), and more recently Doan et al. (2022), has applied PhoBERT, which is a pre-trained language model Nguyen and Tuan Nguyen (2020) designed specifically for Vietnamese, to address the problem. However, these studies assess text difficulty of sentences in isolation while overlooking features that span over an extended discourse, such as discourse relations or entity cohesion across a series of sentences. Given the gap in previous literature on Vietnamese text readability assessment, this study scrutinizes the impacts of statistical and semantic features, as well as the correlation between these two types of features on the difficulty of Vietnamese texts across three primary datasets: Vietnamese Readability dataset Luong et al. (2020a), RACE Lai et al. (2017), and OneStopEnglish Vajjala and Lučić (2018). Our methods range from traditional machine learning models such as SVM, Random Forest, and Extra Tree to state-of-the-art pre-trained language models in various semantic tasks, such as PhoBERT Nguyen and Tuan Nguyen (2020), ViDeBERTa Tran et al. (2023), and ViBERT Tran et al. (2020). The joint approach combining statistical and semantic features are shown to improve model performance, although not yet surpassing statistical features alone. However, they demonstrate potential for development on larger datasets. Furthermore, we conduct an in-depth analysis of specific groups of statistical features concerning text difficulty by individually examining each feature group across multiple models. The results show that features such as ’Number of words’ or ’Average word length in characters’ have the most significant impact on the models when combined with semantic features from deep learning models."
https://arxiv.org/html/2411.04752v1,RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval,"Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman script, especially among migrant communities who form online groups to share relevant local information. This paper focuses on the challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This study presents a novel approach to address these challenges by developing a mechanism to automatically identify the most relevant answers from code-mixed conversations. We have experimented with a dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this task. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex, code-mixed digital conversations, contributing to the broader field of natural language processing in multilingual and informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame a mathematical model which helps to detect relevant documents corresponding to a query.","Code-mixing, where elements from multiple languages are blended within a single sentence, is a natural and widespread phenomenon in multilingual societies [1, 2, 3]. It is particularly prevalent in India, a country with a rich linguistic diversity where speakers often switch between languages depending on context, audience, and medium of communication [4, 5]. With the rapid rise of online social networking, this practice has become increasingly common in digital conversations, where users frequently combine their native languages with others, often using foreign scripts [6, 7]. One notable trend in India is the use of the Roman script to communicate in native languages on social media platforms [8, 9]. This practice is especially common among migrant communities who form online groups to share information and experiences relevant to their unique circumstances [10, 11]. For instance, Bengali speakers from West Bengal who have migrated to urban centers like Delhi or Bangalore often establish groups such as ""Bengali in Delhi"" on platforms like Facebook and WhatsApp. These groups serve as vital hubs for exchanging advice on a wide range of local issues, from housing and employment to navigating new social environments. The COVID-19 pandemic highlighted the importance of these online communities as critical sources of information [12, 13]. During this period, these groups became essential for sharing experiences, seeking support, and keeping up with the frequently changing government guidelines. However, the informal and often colloquial nature of the language used in these code-mixed conversations, typically transliterated into Roman script, presents significant challenges for information retrieval. The lack of standardization, combined with the blending of languages, makes it difficult to identify and extract relevant answers, especially for those who might seek similar information at a later time [14, 15]. This paper addresses the challenge of extracting relevant information from code-mixed digital conversations, with a specific focus on Roman transliterated Bengali mixed with English. While code-mixing is a well-recognized phenomenon in natural language processing (NLP), the unique characteristics of transliterated text—such as variations in spelling, grammar, and syntax—complicate the task of effective information retrieval [16, 17]. To tackle this issue, we have developed a mechanism that identifies the most relevant answers from these complex, multilingual discussions. We begin experimenting with a dataset of code-mixed conversations collected from Facebook, which has been carefully annotated to reflect query relevance (QRels). This dataset forms the basis of our study and is crucial for evaluating the effectiveness of our approach. We leverage GPT-3.5 Turbo [18, 19] by employing carefully designed prompts that guide the model to evaluate the relevance of documents with respect to a given query. This involves not only the semantic understanding capabilities of GPT-3.5 Turbo but also the strategic use of the sequential nature of documents. Often, documents are part of a series or a conversation where the relevance to a query can be influenced by preceding or succeeding documents. By acknowledging this sequence, we can better capture contextual relationships that might be missed if documents were considered in isolation. To formalize this process, we integrate GPT-3.5 Turbo’s outputs into a mathematical model. This model takes into account the sequential dependencies among documents, treating the task of relevance detection as a problem of finding the optimal path or chain of relevance across the sequence."
https://arxiv.org/html/2411.04699v1,BhasaAnuvaad: A Speech Translation Dataset for 14 Indian Languages,"Automatic Speech Translation (AST) datasets for Indian languages remain critically scarce, with public resources covering fewer than 10 of the 22 official languages. This scarcity has resulted in AST systems for Indian languages lagging far behind those available for high-resource languages like English. In this paper, we first evaluate the performance of widely-used AST systems on Indian languages, identifying notable performance gaps and challenges. Our findings show that while these systems perform adequately on read speech, they struggle significantly with spontaneous speech, including disfluencies like pauses and hesitations. Additionally, there is a striking absence of systems capable of accurately translating colloquial and informal language, a key aspect of everyday communication. To this end, we introduce BhasaAnuvaad, the largest publicly available dataset for AST involving 14 scheduled Indian languages spanning over 44,400 hours and 17M text segments. BhasaAnuvaad contains data for English speech to Indic text, as well as Indic speech to English text. This dataset comprises three key categories: (1) Curated datasets from existing resources, (2) Large-scale web mining, and (3) Synthetic data generation. By offering this diverse and expansive dataset, we aim to bridge the resource gap and promote advancements in AST for low-resource Indian languages, especially in handling spontaneous and informal speech patterns.","Figure 1: Per language statistics of the BhasaAnuvaad dataset for Speech Translation across 14 Indian languages, depicting number of thousands of hours paired with text translations. Automatic Speech Translation (AST) has become crucial in enabling communication across languages and cultures by breaking down language barriers. Traditionally, speech translation systems have relied on cascaded architectures, where automatic speech recognition (ASR) is followed by machine translation (MT). However, recent advances have led to more integrated end-to-end (E2E) models Babu et al. (2021); Pratap et al. (2024); Radford et al. (2023); Communication et al. (2023) that directly translate speech from one language into text in another. Additionally, Audio-LLM-based systems have emerged Wu et al. (2023); Chu et al. (2023); Fathullah et al. (2023), further advancing the field. These improvements have mainly benefited high-resource languages like English and European languages, where abundant data and well-established benchmarks have driven significant progress. Figure 2: Overview of the creation of BhasaAnuvaad. In contrast, progress in AST for Indian languages has been much slower. India, with its vast linguistic diversity—22 officially recognized languages and numerous dialects—poses a complex challenge for building effective AST systems. Indian languages feature distinct phonetic systems Mujadia and Sharma (2023), frequent code-switching Shankar et al. (2024), and varied syntactic structures. Additionally, the limited availability of high-quality datasets for Indian languages, exacerbated by the limited representation of these languages on the web, hampers model development. A critical issue in AST research is the predominant focus on evaluating systems using read speech benchmarks like FLUERS Conneau et al. (2022). While these benchmarks are useful, they fail to capture the complexities of spontaneous, real-world speech. Unlike read speech, spontaneous speech includes informal language, hesitations, extensive code-switching Shankar et al. (2024), and unpredictable structures, all of which are essential for building systems that can handle real-world use cases. Additionally in the case of FLEURS, the speech content has been recorded by only three native speaker, which doesn’t efficiently capture the dialectal variations present in the languages across India. Hence, systems that perform well on read speech often struggle with spontaneous speech, limiting their real-world applicability. In this work, we first evaluate various state-of-the-art AST systems, across both Direct and Cascaded paradigms, on read speech (using FLUERS Conneau et al. (2022)) and a newly created benchmark, Indic-Spontaneous-Synth, designed to represent spontaneous speech in Indian languages. Our results show that while existing models perform well on read speech, they significantly underperform on spontaneous speech. This highlights the need for more comprehensive datasets, training objectives, and evaluations that better reflect real-world conditions. To address this gap, we introduce BhasaAnuvaad, the largest Indic-language AST dataset spanning over 44,400 hours of speech and 17M text segments (Figure 1 and 3). This dataset includes mined parallel speech data, high-quality curated parallel speech data collected from minable corpora, and large-scale synthetic ST data generated using IndicVoices Javed et al. (2024), specifically designed to capture spontaneous speech in Indic languages (Figure 2). Out of the 44,400 hours, around 36,600 hours of data have been created through this project and released into the public domain. This includes about 7,400 hours of data from the SEAMLESS-Align project, where we replicated the intricate mining pipeline to collect the mined data. This diverse dataset provides the resources needed to develop more robust AST systems capable of handling spontaneous speech. In summary, our contributions are: (i) Indic-Spontaneous-Synth, a one of a kind created benchmark for evaluating spontaneous speech translations, (ii) an evaluation of popular AST baselines on both read and spontaneous benchmarks, and (iii) the release of BhasaAnuvaad, the largest Indic-language speech translation corpus, comprising over 44 thousand hours of data across 15 languages. All code and datasets developed as part of this work will be made publicly available to support future research in AST for Indic languages."
https://arxiv.org/html/2411.04604v1,FASSILA: A Corpus for Algerian Dialect Fake News Detection and Sentiment Analysis,"In the context of low-resource languages, the Algerian dialect (AD) faces challenges due to the absence of annotated corpora, hindering its effective processing, notably in Machine Learning (ML) applications reliant on corpora for training and assessment. This study outlines the development process of a specialized corpus for Fake News (FN) detection and sentiment analysis (SA) in AD called FASSILA. This corpus comprises 10,087 sentences, encompassing over 19,497 unique words in AD, and addresses the significant lack of linguistic resources in the language and covers seven distinct domains. We propose an annotation scheme for FN detection and SA, detailing the data collection, cleaning, and labelling process. Remarkable Inter-Annotator Agreement indicates that the annotation scheme produces consistent annotations of high quality. Subsequent classification experiments using BERT-based models and ML models are presented, demonstrate promising results and highlight avenues for further research. The dataset is made freely available on GitHub 111https://github.com/amincoding/FASSILA to facilitate future advancements in the field.","Building a corpus become an important topic in natural language processing (NLP) and especially for low resource languages (ex: AD), due to the importance that the corpus plays in the development of several tools, such as: Machine Translation Babaali and Salem (2022), Part of speech tagging Chiche and Yitagesu (2022), Named entities recognition Jarrar et al. (2022), etc . in particular with the emergence of techniques based on statistics, machine learning and deep learning. Who exploits this mass of information to develop, train and evaluate models. However, building a corpus is not an easy task Bakari et al. (2016); it is extremely time-consuming and requires a lot of work, for the good reason that the volume and quality of the corpus are two important parameters. Despite the recent emergence of techniques that consume fewer resources, such as few-shot learning Tunstall et al. (2022). Over the last few years, a lot of studies in NLP have focused on languages or variants of languages called low resources Mengoni and Santucci (2023). This change of direction is mainly due to the emergence of social media such as Facebook, Twitter, RenRen, LinkedIn, Google+, and Tuenti, as a means of communication where people exchange messages and comments. In Arab countries, these exchanges are carried out using variants of the Arabic language known as dialects. The boom in the use of social media has not only provided raw data for building a corpus, but has also led to the emergence of other topics, such as detecting Fake News and Sentiment Analysis. also known as opinion mining,Sentiment Analysis is the field of study that analyses people’s opinions, feelings, evaluations, attitudes and emotions towards entities and their attributes expressed in written text Zhao et al. (2016). On the other hand, the detection of Fake News is aimed at combating a phenomenon that has grown with the growth of social media, given the amount of information circulating and which can have an impact on a number of areas, whether social, economic or even political Nevado-Catalán et al. (2023). The aim of our study is to contribute to the development of resources for the AD, through the building of a corpus dedicated to the detection of Fake News and the Sentiment Analysis. To this, we add a series of experiments on Fake News based on machine learning techniques such as: Support Vector Machines (SVM), Logistic Regression (LR), Decision Trees (DT); and transformer-based models including: AraBERTv02, MarBERTv2, and DziriBERT. Our study endeavors to enrich the resources available for the Algerian Dialect by constructing a specialized corpus tailored for the detection of Fake News and Sentiment Analysis. This initiative stems from the recognition of the scarcity of linguistic resources in Algerian Dialect and the critical need to empower computational linguistics with dedicated datasets. Through the creation of this corpus, we aim to facilitate more robust and nuanced analyses of Fake News and Sentiment Analysis within the Algerian Dialect discourse, thereby advancing research and applications in computational linguistics for this understudied language. In the landscape of linguistics, the Algerian Dialect stands as one of the most low-resource languages, lacking an official writing style or a sustained repository of data. This scarcity poses significant challenges across linguistic applications, ranging from translation models to the detection of FN. Without a standardized corpus, Algerian Dialect remains largely untapped in computational linguistics. The imperative to create a dedicated corpus for Fake News detection and Sentiment Analysis in Algerian Dialect arises from this dearth of linguistic resources. Establishing such a corpus not only addresses the pressing need for linguistic data but also unlocks avenues for advancing machine learning methodologies tailored to AD’s unique linguistic nuances. This study aims to achieve the followings: • Development of the worlds first specialized corpus tailored for Fake News detection and Sentiment Analysis in the Algerian Dialect in parallel. • Establishment of a corpus consisting of 10,087 sentences, encompassing over 15,561 unique words in AD, addressing the significant dearth of linguistic resources in the language. • Testing out corpus to prove its fine tuning capabilities and strong maintainability. • testing GPT-4’s paraphrasing and translation abilities and using them once proven to be reliable. The rest of the paper is laid out as follows: Section 2 gives an overview of the AD. Section 3 presents the related works regarding the detection of Fake News and Sentiment Analysis in the AD; Section 4 presents how we build our corpus FASSILA; Section 5 describes the experimental Setting. Experimental results are discussed in Section 6, and we conclude and present future work in Section 7."
https://arxiv.org/html/2411.04588v1,Tibyan Corpus: Balanced and Comprehensive Error Coverage Corpus Using ChatGPT for Arabic Grammatical Error Correction,"Natural language processing (NLP) utilizes text data augmentation to overcome sample size constraints. Scarce and low-quality data present particular challenges when learning from these domains. Increasing the sample size is a natural and widely used strategy for alleviating these challenges. Moreover, data-augmentation techniques are commonly used in languages with rich data resources to address problems such as exposure bias. In this study, we chose Arabic to increase the sample size and correct grammatical errors. Arabic is considered one of the languages with limited resources for grammatical error correction (GEC) despite being one of the most popular among Arabs and non-Arabs because of its close connection to Islam. Furthermore, QALB-14 and QALB-15 are the only datasets used in most Arabic grammatical error correction research, with approximately 20,500 parallel examples, which is considered low compared with other languages. In addition, most Arabic data augmentation techniques have not been adequately addressed. Therefore, this study aims to develop an Arabic corpus called ""Tibyan"" for grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter tool based on a pair of Arabic sentences containing grammatical errors matched with a sentence free of errors extracted from Arabic books, called guide sentences. Multiple steps were involved in establishing our corpus, including the collection and pre-processing of a pair of Arabic texts from various sources, such as books and open-access corpora. We then used ChatGPT to generate a parallel corpus based on the text collected previously, as a guide for generating sentences with multiple types of errors. By engaging linguistic experts to review and validate the automatically generated sentences, we ensured that they were correct and error-free. The corpus was validated and refined iteratively based on feedback provided by linguistic experts to improve its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to analyze the types of errors in the Tibyan corpus. Our corpus contained 49% of errors, including seven types: orthography, morphology, syntax, semantics, punctuation, merge, and split. The Tibyan corpus contains approximately 600 K tokens.","The Arabic language has a great deal of influence worldwide. It is an ancient language with deep roots in the human history. The Holy Qur’an’s language is Arabic, which has a special religious status among Muslims worldwide. Many of the most significant literary and philosophical works in human history have been written in Arabic, making it a sophisticated literary and poetic language [1].In addition to being an important scientific and intellectual language, Arabic has contributed greatly to the transfer of knowledge and culture to Europe and other countries as it was the language of scholars and philosophers during the Middle Ages [2]. One of the most important and famous features of Arabic is that it consists of three main versions: classical Arabic, modern standard Arabic (MSA), and regional dialects [3]. Classical Arabic was used in the Holy Quran and ancient literary texts between the 7th and 9th centuries [4]. Non-native Arabic speakers may find it difficult to learn classical Arabic or Quranic Arabic because of the special symbols (Tanween) that indicate proper pronunciation. Modern Standard Arabic (MSA) is the official language used primarily in newspapers, television broadcasts, and films. As it is not commonly spoken as a first language, it is a language without native speakers. There are currently 274 million speakers worldwide111https://www.statista.com/statistics/266808/the-most-spoken-languages-worldwide/. MSA is a formal language that is not used in daily life. Arabic is a vast language with a variety of dialects, and all Arabic speakers learn a local dialect, such as Mesopotamian Arabic and Egyptian Arabic. Meanwhile, dialectal Arabic is used by Arabs as a daily language of conversation. Although Arabic dialects are fundamentally related, they cannot be understood by one another because Arab countries speak different dialects [4]. Because MSA is rarely used in daily life, it is sometimes mixed with local dialects. Moreover, because of the rich and intricate nature of Arabic, ambiguity can lead to incomprehensible and inaccurate text. In addition, Arabic grammar presents several semantic, syntactic, and morphological challenges owing to its flexible word order, diacritic, and agglutination properties. Furthermore, considerable deficiencies at the Arabic morphological level have hampered extensive research in this area. At higher research levels, semantics and syntax did not significantly advance. Therefore, grammatical error correction (GEC) is becoming increasingly important for native and non-native speakers. Grammatical Error Correction (GEC) automatically detects and corrects grammatical errors in a text [5]. Recent approaches to grammatical error correction, such as the seq2seq model, require large, high-quality parallel datasets. However, many languages do not contain such data, making it difficult to train these models. Other languages contain only a limited number of examples, making it difficult to build models that can correct all types of linguistic errors. Moreover, the creation of such datasets can be time-consuming and expensive. Therefore, most researchers use data augmentation techniques to increase the size of Grammatical Error Correction (GEC) parallel data. Data augmentation techniques generate more diverse training examples, which enhances the model’s ability to generalize to unknown errors. Various error types and contexts can be introduced using data augmentation to balance the dataset. Consequently, grammatical error correction systems have become more robust and accurate. The Arabic language has limited resources. Only two parallel corpora are available for GEC research:QALB-14 [6] and QALB-15 [7]. The QALB-14 and QALB-15 are part of the Qatar Arabic Language Bank (QALB) project. QALB aims to create a large corpus of Arabic texts that have been manually corrected, such as user comments on news sites, essays written by native and non-native speakers, and machine translation text. A specialized annotation interface was developed for this project, along with comprehensive annotation guidelines [8] [9]. A total of 20,430 and 1,542 samples were available from the two training corpora, (QALB-14) and (QALB-15). Despite the researchers’ complete reliance on these data, they have some shortcomings, including inadequate coverage of Arabic language defects, inconsistent punctuation correction, and small size compared with o datasets in other languages. This study aims to contribute to the development of an Arabic corpus for Grammatical Error Correction by employing ChatGPT to generate paired sentences based on common errors found in Arabic books. First, we collected a diverse range of pair Arabic sentences; one containing common grammatical errors made by native speakers and other corrected versions of the sentence. The sentences collected from the three Arabic books were short, ranging from one to seven words. These sentences were extracted from three Arabic books namely ""A Dictionary of Common Grammatical, morphological, and Linguistic Errors""222https://archive.org/details/20210306_20210306_1934/mode/2up, ""Common linguistic errors in cultural circles""333https://archive.org/details/20200317_20200317_0834, ""Common linguistic errors""444https://www.alukah.net/books/files/book_5755/bookfile/akhtaa.pdf. Moreover, we used the A7’ta corpus [10] which is composed of 466 short sentence pairs taken from a book called Linguistic Error Detector (Saudi Press). Second, we instructed the ChatGPT model to generate full sentence pairs using our collected short sentence pairs, one containing the error and the other free from errors. Additionally, the corrected versions of the corpus were annotated and all grammatical errors were corrected by experts, creating a valuable resource for training and evaluating the performance of the Arabic GEC. Finally, we analyzed the types of errors generated in our corpus using the Arabic Error Type Annotation tool (ARETA) [11]. We make our corpus publicly available. The contributions of this study are as follows. • Collect and organize short Arabic sentences, including common grammatical errors, from various Arabic books as a guide. • Using ChatGPT as a data augmenter, a full, long, and error-free Arabic corpus can be generated from the guiding sentences, resulting in an error-prone Arabic corpus. • Assuring that annotated errors are accurate and relevant by engaging linguistic experts to review and validate them manually. • The corpus was validated and refined iteratively based on the feedback provided by linguistic experts. Understanding the distribution and characteristics of errors in different contexts by analyzing the linguistic properties of the corpus. The remainder of this paper is organized as follows: The second section discusses the available Arabic corpora and studies that used ChatGPT as a data aggregator. Section 3 describes the methodology used to build the GEC corpus. Section 4 describes our experimental setup. Section 5 analyzes the type and percentage of errors in our corpus, and Section 6 summarizes our contributions and outlines future directions for Arabic GEC research."
https://arxiv.org/html/2411.04585v1,The State and Fate of Summarization Datasets,"Automatic summarization has consistently attracted attention, due to its versatility and wide application in various downstream tasks. Despite its popularity, we find that annotation efforts have largely been disjointed, and have lacked common terminology. Consequently, it is challenging to discover existing resources or identify coherent research directions. To address this, we survey a large body of work spanning 133 datasets in over 100 languages, creating a novel ontology covering sample properties, collection methods and distribution. With this ontology we make key observations, including the lack in accessible high-quality datasets for low-resource languages, and the field’s over-reliance on the news domain and on automatically collected distant supervision. Finally, we make available a web interface that allows users to interact and explore our ontology and dataset collection,111https://github.com/edahanoam/Awesome-Summarization-Datasets as well as a template for summarization data card, which can be used to streamline future research into a more coherent body of work.","Summarization is the task of shortening a text while preserving the most important information it contains. This task has been explored for over 60 years Luhn (1958) yet it remains a longstanding challenge, as evidenced by the steadily increasing number of summarization datasets released every year (see Figure 1). Despite the many research efforts in creating summarization datasets, we find they lack standardization and common terminology. This shortcoming makes it difficult to discover existing resources and bottlenecks, and to identify general directions for future research. In this work, we conduct the most comprehensive survey of summarization datasets, covering a wide collection of 133 datasets across 104 languages. In Section 3, we develop a comprehensive ontology geared towards modern summarization datasets, positioning each dataset along seven different scales, including information about the samples (language, language modality, domain and summarization shape) as well as collection methods (annotation efforts and sources of supervision), and the manner in which the dataset is distributed. Figure 1: There is a continuous increase in the number of summarization datasets being published. However, we find that the field is missing standardization and common terminology. In Section 4 we show that our proposed ontology surfaces five major trends and gaps in the field. First, we find that the lack of common terminology thus far often leads to inconsistency and to the omission of important details when presenting datasets, making comparison challenging. An important example of this inconsistency is the traditional abstractive-extractive distinction, which we argue have gradually shifted away from a binary trait to a more nuanced spectrum. Second, we observe that the field is over-reliant on the news domain, which acts as a double-edged sword – being both easy to obtain and prone to low quality. This is especially evident for low-resource languages, where the vast majority of datasets are based on news articles. Figure 2: Our ontology for summarization datasets, accompanied by our annotations. The percentages in the language column indicate the proportion of datasets that support each language, where we count each multilingual dataset as multiple monolingual datasets. The arrows showcase the decision pathways for selecting specific datasets. Third, we find that the datasets available for low-resource languages tend to be of subpar standard and especially challenging to discover. This is because many languages are currently supported only through multilingual datasets, which tend to compromise quality for diversity Urlana et al. (2022), while also making it harder to know which languages are annotated in which dataset. Fourth, we find that most of the annotations were not specifically created to serve as summaries. Fifth, we observe that copyright concerns, especially in the news domain have been changing data distribution, with many datasets move from publishing the data towards publishing scripts which reconstruct it from URLs. We argue that this practice is problematic, as URLs may change over time. Furthermore, a few seminal summarization datasets that were published publicly have been either taken down or are considered legally questionable due to copyright violations. In Section 5 we publish two resources to standardize and help organize the vast research in the field. First, we make our dataset collection publicly available through a web UI which allows users to search datasets matching different criteria according to our terminology, for example, retrieving all manually created datasets in a certain language. This UI serves a central location where researchers can discover datasets, as well as identify new research directions. We welcome community contributions to keep this resource updated with newly published research. Second, inspired by work on model cards Mitchell et al. (2019), we propose a standard summarization data card which covers the different items in our ontology as well as important properties. This can benefit future dataset developers in making their work more accessible, easy to discover and comparable against other datasets. We hope that our work helps chart the way for future work in the field, as well enable researchers from various disciplines in exploring the wide range of existing summarization datasets. To conclude we make the following contributions: • We formulate an ontology for summarization datasets, covering various axes regrading the samples, their collection and distribution. • We survey a wide range of 133 datasets covering datasets in 104 languages, showing that our ontology is rich enough to surface five important trends and challenges for future work, including the lack of terminology, the subpar quality of datasets in low-resource languages, and more. • We make available two resources – a web interface to interact with our survey and discover summarization datasets, and a formal summarization data card, which will help streamline future datasets."
https://arxiv.org/html/2411.04573v1,Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages,"This paper presents a novel multistage fine-tuning strategy designed to enhance automatic speech recognition (ASR) performance in low-resource languages using OpenAI’s Whisper model. In this approach we aim to build ASR model for languages with limited digital resources by sequentially adapting the model across linguistically similar languages. We experimented this on the Malasar language, a Dravidian language spoken by approximately ten thousand people in the Western Ghats of South India. Malasar language faces critical challenges for technological intervention due to its lack of a native script and absence of digital or spoken data resources. Working in collaboration with Wycliffe India and Malasar community members, we created a spoken Malasar corpus paired with transcription in Tamil script, a closely related major language. In our approach to build ASR model for Malasar, we first build an intermediate Tamil ASR, leveraging higher data availability for Tamil annotated speech. This intermediate model is subsequently fine-tuned on Malasar data, allowing for more effective ASR adaptation despite limited resources. The multistage fine-tuning strategy demonstrated significant improvements over direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of 51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning method. Further a WER reduction to 47.3% was achieved through punctuation removal in post-processing, which addresses formatting inconsistencies that impact evaluation. Our results underscore the effectiveness of sequential multistage fine-tuning combined with targeted post-processing as a scalable strategy for ASR system development in low-resource languages, especially where linguistic similarities can be leveraged to bridge gaps in training data.","The Malasar language, spoken by the indigenous Malasar community in the Western Ghats region of southern India, remains stable despite its relatively small speaker base111https://www.ethnologue.com/language/ymr/. According to the 2011 census of India, there are only 9626 Malasar community members, out of which 6431 are in the state of Tamil Nadu and remaining are in the state of Kerala (Chandramouli, 2013). Although the language is not sustained by formal institutions, it continues to thrive in homes and communities where it is passed down to children as the norm. As an oral language without a native script, Malasar is vulnerable to loss as communities diminish or integrate with larger linguistic groups. Documenting and archiving these languages become more complicated without a standardized script. In terms of technology integration, creating systems like automatic speech recognition (ASR) is significantly more difficult. ASR systems typically depend on large, annotated corpora for training, which are hard to generate without a written form of the language. This also affects the development of natural language processing (NLP) tools, such as machine translation and text-to-speech systems, which rely on a text corpus for training and evaluation. In general, the lack of a script poses a challenge for linguistic research. To address the challenges of documenting unwritten languages, UNESCO published a set of guidelines in 2003 (Robinson, 2003). These guidelines explore the processes involved in writing unwritten languages, thereby offering new opportunities for expression and learning to the world’s linguistic minorities and indigenous people. Malasar shares lexical similarities with Tamil, Malayalam, Muduga, Eravallan, and certain dialects of Irula (Varghese, 2015). The lexical similarity of Malasar with other languages is illustrated in Fig. 1. According to the guidelines (Robinson, 2003), Tamil has been selected as the script for transcribing Malasar speech, facilitating a more consistent and comprehensible representation of the language. Figure 1. Lexical Similarity Of Malasar with other neighbouring languages The development of an ASR system for the Malasar language serves multiple critical purposes in language preservation and documentation efforts. By enabling accurate transcription of spoken Malasar into Tamil script, the system facilitates the creation of valuable linguistic records (Prud’hommeaux et al., 2021) while supporting detailed analysis of the language’s phonetic and grammatical features. Beyond documentation, an ASR system drives digital accessibility, fostering language revitalization through modern educational tools and enhanced community engagement. It also empowers governing bodies and strengthens the Malasar community’s societal integration. In the context of low-resourced languages, such ASR development represents a crucial step toward preserving indigenous knowledge and cultural heritage. To address these specific linguistic and technical hurdles unique to building an ASR system for Malasar, our study adopts a multi-faceted approach: (1) To build an initial corpus from scratch, we collaborated with Wycliff India, resulting in approximately four hours of transcribed Malasar audio data (2) The lack of a native writing system for this oral-only language led us to adopt Tamil script for transcription due to its high lexical similarity, ensuring more accurate representation of Malasar speech (3) To address data scarcity in training an ASR model, we implemented a novel sequential multistage fine-tuning strategy with Whisper, leveraging Tamil as an intermediate fine-tuning stage before adapting the model to Malasar (4) To ensure accurate evaluation, we also introduced a punctuation filter, which significantly improved the reported WER by removing non-linguistic elements that impact the metric (Manohar et al., 2024). In conlusion, the work described in this paper sets a foundation for further NLP applications in this under-resourced language."
https://arxiv.org/html/2411.04557v1,Pruning Literals for Highly EfficientExplainability at Word Level,"Designing an explainable model becomes crucial now for Natural Language Processing (NLP) since most of the state-of-the-art machine learning models provide a limited explanation for the prediction. In the spectrum of an explainable model, Tsetlin Machine (TM) is promising because of its capability of providing word-level explanation using proposition logic. However, concern rises over the elaborated combination of literals (propositional logic) in the clause that makes the model difficult for humans to comprehend, despite having a transparent learning process. In this paper, we design a post-hoc pruning of clauses that eliminate the randomly placed literals in the clause thereby making the model more efficiently interpretable than the vanilla TM. Experiments on the publicly available YELP-HAT Dataset demonstrate that the proposed pruned TM’s attention map aligns more with the human attention map than the vanilla TM’s attention map. In addition, the pairwise similarity measure also surpasses the attention map-based neural network models. In terms of accuracy, the proposed pruning method does not degrade the accuracy significantly but rather enhances the performance up to 4%percent44\%4 % to 9%percent99\%9 % in some test data.","The majority of Natural Language Processing (NLP) tasks are highly dependent on attention-based Deep Neural Networks (DNNs) models [1, 2, 3]. While attention mechanisms have been claimed to facilitate interpretability since their development, the question of whether this is true has just recently been a hot topic of discussion [4, 5]. In addition, it is recently suggested in [6], [4], and [5] three separate ways for assessing the explainability of attention. Particularly, [6]’s study is based on the idea that explainable attention scores ought to be unique for each prediction while also being consistent with other measures of feature importance. Similarly, [4] suggests that the relevance of inputs does not always equate to attention weights. While these studies raise important issues, they also use model-driven approaches to manipulate attention weights and then assess the post-hoc explainability of the generated machine attention. One way to evaluate if the machine attention map (MAM) correlates with the human attention map (HAM) is to compare the similarity between these two [7]. However, due to the BlackBox nature that is lack of transparency in the model, the focus is now shifting to some interpretable models. One of the powerful rule-based interpretable models is Tsetlin Machine, which aims to reduce the gap between explainability and performance to a significant level [8]. Tsetlin Machine (TM) has become an architecture of the choice for a vast range of NLP tasks especially in text classification such as Word Sense Disambiguation (WSD) [9], Sentiment Analysis [10], Fake News Detection [11] and Document Classification [12]. While TM has been a good alternative approach to traditional Deep Neural Networks (DNNs) because of its transparent learning, the explainable rules that it offers tend to be huge in numbers making them difficult to comprehend. TM learns the pattern based on the combination of propositional logic called clauses and these collections of the clause make a pattern for a particular class [13, 14, 15]. It has been arguably accepted that interpreting such a clause can give insight into the model for humans to understand the underlying concept of the task [16, 17, 18]. However, it has also been accepted that due to sparse Boolean bag-of-words (BOW) input representation, the model tends to learn negated literals in the majority making each clause very humongous and impractical to interpret. Hence in this paper, we design a pruning technique to eliminate the unwanted literals111Literals are the form of input features either in the original or negated form such as a word “good” is an original form of the literal and “¬\neg¬ good” is the negated form that makes up a clause. from the clause so that the clauses are more efficiently explained. The pruning method is named: pruning by the frequency of literals in the model. As a result of the pruning method, the propositional rules obtained from the set of clauses are shortened thereby making it easier for humans to comprehend. We obtain the Tsetlin attention map (TAM) from the pruned clause and evaluate it with HAM using the similarity measure. For evaluation of the model, we use the two popular metrics: Comprehensiveness and Sufficiency [19]. The proposed pruning method is not similar to the removal of stopwords that are predefined and are common in most cases. However, our approach statistically removes the unwanted form of the features to add explainability. The main contributions of the paper are as follows: • We design a pruning method using the frequency of the literals in the clause in the model. • We generate much shorter and more efficient clauses that are shorter, and compact which would lead to more explainable NLP models. Figure 1: Example of important and non-important literals for given contexts."
https://arxiv.org/html/2411.04535v1,Meta-Reasoning Improves Tool Use in Large Language Models,"External tools help large language models (LLMs) succeed at tasks where they would otherwise typically fail. In existing frameworks, LLMs learn tool use either by in-context demonstrations or via full model fine-tuning on annotated data. As these approaches do not easily scale, a recent trend is to abandon them in favor of lightweight, parameter-efficient tuning paradigms. These methods allow quickly alternating between the frozen LLM and its specialised fine-tuned version, by switching on or off a handful of additional custom parameters. Hence, we postulate that the generalization ability of the frozen model can be leveraged to improve tool selection. We present Tool selection via meta-reasoning (Tecton), a two-phase system that first reasons over a task using a custom fine-tuned LM head and outputs candidate tools. Then, with the custom head disabled, it meta-reasons (i.e., it reasons over the previous reasoning process) to make a final choice. We show that Tecton results in substantial gains – both in-distribution and out-of-distribution – on a range of math reasoning datasets.","Figure 1: An overview of Tecton. In the reasoning phase, the system inspects the task and decodes a set of candidate tools, followed by argument insertion and evaluation of the tools via the Python interpreter. In the meta-reasoning phase, the model is asked to select the most useful tool, either by scoring multiple options (Tecton-score) or by continuing the generation given the decoded tools as hints (Tecton-generate). Augmentation with external tools has proven effective at boosting the performance of large language models in knowledge-intensive tasks such as QA and math problem-solving Hao et al. (2023); Paranjape et al. (2023); Parisi et al. (2022); Schick et al. (2023). Tools are self-contained programs or APIs whose execution timing and arguments are determined by the language model. To teach a model how to use tools, previous works adopt one of two main strategies: (1) tool demonstrations via in-context learning Gao et al. (2023); Gupta and Kembhavi (2023); Hsieh et al. (2023); Surís et al. (2023), or (2) fine-tuning on a dataset where text samples are interleaved with tool annotations Parisi et al. (2022); Schick et al. (2023); Tang et al. (2023). Hao et al. (2023) note that a major downside of in-context demonstrations is the finite size of the context itself, preventing the system from being able to scale up to a very large number of tools. The issue of scalability is not just theoretical: according to Patil et al. (2023), a general-purpose LLM may need access to thousands of tools to be able to carry out complex and diverse tasks (e.g., booking holidays or organizing conferences) from natural language instructions. In contrast, fine-tuned models are able to learn thousands of tools from annotated data Patil et al. (2023). However, they are bound to the set of tools learned during training: adding further tools requires a new round of fine-tuning at significant computational cost Hao et al. (2023). Recent works adopt parameter-efficient learning to reduce the cost of fine-tuning, thus facilitating potential future extensions of the tool set. Hao et al. (2023) develop ToolkenGPT by augmenting the output matrix of an LLM with additional special tokens, each representing a tool. Only the additional tokens are tuned while the rest of the weights remain frozen. Similarly, Wang et al. (2024) train tokens representing math operations to aid each reasoning step, using LoRA Hu et al. (2022) to minimize the number of additional parameters. Qiao et al. (2024) implement a two-stage system called Trice, where instruction tuning is followed by reinforcement learning from environmental feedback (RLEF), leveraging LoRA at both stages. An advantage of parameter-efficient paradigms is that they preserve most of the LLM’s core capabilities acquired at pre-training Ding et al. (2023); Han et al. (2024). These methods only tune a handful of additional parameters for a specific task, which can be selectively disabled to reinstate the original model. We argue that previous works exploring these techniques for tool generation do not fully leverage the generalization power of the underlying LLM. To address this issue, we propose a two-phase framework for Tool selection via meta-reasoning (Tecton). In the reasoning phase, Tecton investigates a task and outputs candidate tools with the aid of a custom-tuned language modeling head. Then, in the meta-reasoning phase, it uses the frozen LLM to re-examine these candidates and make a final decision. This differs from previous works where the choice of tool is solely determined by its probability at decoding time Hao et al. (2023); Wang et al. (2024); Schick et al. (2023). Fig. 1 illustrates the system. We train and evaluate Tecton on math reasoning datasets, following established works on LLM tool calling Chen et al. (2024); Das et al. (2024); Gou et al. (2024). It is worth noting that math reasoning problems are highly challenging, since they require chains of multiple tool calls with errors that compound. This is evidenced by existing tool-augmented LLMs, which achieve the lowest performance on math reasoning when evaluated on multiple tasks Hao et al. (2023); Schick et al. (2023). In summary, our main contributions are: • We introduce Tecton, a novel two-phase framework that combines a custom fine-tuned head with a frozen LLM to improve tool use in math reasoning tasks (Section 2). • We show that Tecton outperforms strong baselines both on in-distribution data and on unseen benchmarks (Section 3). • We enhance three popular math reasoning datasets to make them more challenging for current LLMs. We share our data and code at https://github.com/lisaalaz/tecton."
https://arxiv.org/html/2411.04473v1,ML-Promise: A Multilingual Dataset for Corporate Promise Verification,"Promises made by politicians, corporate leaders, and public figures have a significant impact on public perception, trust, and institutional reputation. However, the complexity and volume of such commitments, coupled with difficulties in verifying their fulfillment, necessitate innovative methods for assessing their credibility. This paper introduces the concept of Promise Verification, a systematic approach involving steps such as promise identification, evidence assessment, and the evaluation of timing for verification. We propose the first multilingual dataset, ML-Promise, which includes English, French, Chinese, Japanese, and Korean, aimed at facilitating in-depth verification of promises, particularly in the context of Environmental, Social, and Governance (ESG) reports. Given the growing emphasis on corporate environmental contributions, this dataset addresses the challenge of evaluating corporate promises, especially in light of practices like greenwashing. Our findings also explore textual and image-based baselines, with promising results from retrieval-augmented generation (RAG) approaches. This work aims to foster further discourse on the accountability of public commitments across multiple languages and domains.","In a world where promises shape perceptions and drive decisions, the integrity of commitments made by politicians, corporate leaders, and public figures must be scrutinized. These promises, ranging from environmental sustainability to social responsibility and governance ethics, significantly influence the general public’s and stakeholders’ trust, as well as government and corporate reputations. Yet, the complexity and abundance of such commitments, coupled with the challenge of verifying their fulfillment, create a pressing need for innovative approaches to assess their strength and verifiability. Recognizing the critical role of transparency and accountability in today’s society, we propose a groundbreaking task: Promise Verification. Task Label English French Chinese Japanese Korean Promise Identification Yes 84.5 80.5 40.2 74.9 77.5 No 15.5 19.5 59.8 25.1 22.5 Supporting Evidence Yes 20.1 71.6 20.1 66.4 75.6 No 79.9 28.4 79.9 33.6 24.4 Clarity of Promise-Evidence Pair Clear 53.3 56.6 64.7 61.2 94.8 Not Clear 42.9 41.9 35.3 34.7 5.2 Misleading 3.8 1.5 0.0 4.1 0.0 Timing for Verification Within 2 years 1.9 12.4 37.5 7.3 45.5 2-5 years 14.1 15.0 10.0 9.3 8.4 Longer than 5 years 9.0 21.6 15.0 18.7 17.5 Other 75.0 51.0 37.5 64.7 28.7 Table 1: Label distribution in each language. (%) To perform promise verification, several steps are required, including (1) identifying the promise, (2) linking the promise with supporting evidence, (3) assessing the clarity of the promise-evidence pair, and (4) inferring the timing for verifying the promise. For example, after identifying a promise, the availability of evidence to support that the speaker is taking action to fulfill the promise could serve as a coarse-grained evaluation of the promise’s quality. However, the clarity of the evidence may also affect the quality of the promise, which would be a fine-grained evaluation. Additionally, whether the speaker provides a clear timeline for verifying the promise is an important criterion. For instance, “we will achieve net zero carbon emissions within five years” is a stronger promise than “we will achieve net zero carbon emissions.” Following this line of thought, this paper proposes the first multilingual dataset for in-depth promise verification, including Chinese, English, French, Japanese, and Korean. In recent years, increasing emphasis has been placed on companies’ environmental contributions, especially in addressing climate change, deforestation, and compliance with labor conditions and governance, when evaluating their investment value. In the evolving landscape of ESG (environmental, social, and governance) criteria, the ability to accurately assess a company’s promises and adherence to its ESG promises has become paramount. However, unlike traditional financial statements, ESG reports still lack clear standards regarding corporate promises. This allows some companies to use misleading information to project an overly positive environmental image, a practice known as greenwashing. As Gorovaia and Makrominas (2024) points out, companies involved in environmental misconduct tend to produce longer, more positive, and more frequent reports. We hypothesize that such reports may lack substantive evidence, or the information presented may be irrelevant or ambiguous, leading to misinterpretation. To this end, the proposed dataset, ML-Promise, focuses on ESG reports released by corporations in five countries: the U.K., France, Taiwan, Japan, and Korea. In addition to exploring text-based baselines, we also provide pilot results on image-based experiments, as most reports are released in PDF format. Our experiment further shows that the retrieval-augmented generation (RAG) approach Lewis et al. (2020) can help in some language cases. Although we do not find a silver bullet for all languages and tasks, we hope the proposed dataset can open a new chapter in discussions on the responsibility of promises, especially those from public figures."
https://arxiv.org/html/2411.04448v1,Gradient Localization Improves Lifelong Pretraining of Language Models,"Large Language Models (LLMs) trained on web-scale text corpora have been shown to capture world knowledge in their parameters. However, the mechanism by which language models store different types of knowledge is poorly understood. In this work, we examine two types of knowledge relating to temporally sensitive entities and demonstrate that each type is localized to different sets of parameters within the LLMs. We hypothesize that the lack of consideration of the locality of knowledge in existing continual learning methods contributes to both: the failed uptake of new information, and catastrophic forgetting of previously learned information. We observe that sequences containing references to updated and newly mentioned entities exhibit larger gradient norms in a subset of layers. We demonstrate that targeting parameter updates to these relevant layers can improve the performance of continually pretraining on language containing temporal drift.","Pretraining over diverse datasets has been shown to encode world knowledge in the parameters of large language models (LLMs) Petroni et al. (2019); Roberts et al. (2020); Gueta et al. (2023) from massive static web-scale datasets. However, these models are normally trained on large static text corpora which do not reflect changes in world knowledge or language usage that occur after the initial data collection. In practice language models are deployed in dynamic real-world settings, and their learned knowledge becomes stale over time Lazaridou et al. (2021); Luu et al. (2022); Dhingra et al. (2022); Yao et al. (2022); Nylund et al. (2023); Cheang et al. (2023); the temporal degradation can be evaluated according to intrinsic measures such as perplexity, or extrinsic downstream performance (e.g. question answering). Figure 1: When continually pretraining on sequences with updated and newly mentioned entities, certain layers consistently observe larger gradient norms. Incrementally training language models on streams of data has been explored as a method to mitigate temporal performance degradation without incurring the heavy computational and environmental costs of retraining models on large pretraining corpora Jang et al. (2021, 2022); Lin et al. (2022); Gururangan et al. (2020). However, naive online training on these data streams is known to: induce hallucinations in language generations Kang et al. (2024), fail in the uptake of new information Onoe et al. (2023); Hu et al. (2023), and catastrophically forget previously learned information Zhu et al. (2020). To address these problems, recent work has applied continual learning and online learning methods to adapting large language models to streams of documents Loureiro et al. (2022); Scialom et al. (2022); Jang et al. (2022). While continual learning methods have been shown to mitigate temporal performance degradations, the mechanisms by which neural language models store and update information are not well understood. Dataset Year Example Answer TempLAMA 2020 Joe Biden holds the position of __ . President-elect.of the United States 2021 Joe Biden holds the position of __ . President of the United States Entity Cloze By Date (ECBD) 2020 The Congressional Budget Office provided a score for the CARES Act on April 16, 2020 estimating it would __. increase federal deficits. 2021 On August 14, when Hurricane Grace entered the Caribbean, a tropical storm watch was issued for __. the entire coast of Haiti. Table 1: Examples from TempLAMA and ECBD probing tasks. The temporally sensitive entity is bolded. In this work, we consider a real-world setting for continual language learning, that of temporal language drift, and probe the performance of language models on two types of entity relationships which exhibit temporal degradation: (1) acquisition of information about new entities, and (2) updating relationships between existing entities. We hypothesize that the poor performance of existing continual learning methods on these forms of entity relationship shift can be in part attributed to a misalignment in the autoregressive language modeling pretraining objective and the optimal parameter updates required to acquire new information or update existing knowledge. To characterize this misalignment, we compare the gradient updates observed when training language models to predict knowledge intensive salient entity spans, with the gradient updates observed from standard continual pretraining. We observe that for the gradient updates for predicting knowledge intensive salient spans, observe high values in distinct groups of layers based on the type of entity relationship presented in the sequence (see Fig. 1). Based on these observations, we propose new methods for aligning the gradient updates during continual pretraining to better align with these layers which exhibit high gradient norms. Through empirical study, we show that the observed characteristic gradient patterns occur across autoregressive, transformer language models of various of sizes; and we demonstrate the efficacy of our proposed method through performance improvements on knowledge probing tasks when applied on top of existing continual learning methods in pretraining."
https://arxiv.org/html/2411.04443v1,ACCIO: Table Understanding Enhancedvia Contrastive Learning with Aggregations,"The attention to table understanding using recent natural language models has been growing. However, most related works tend to focus on learning the structure of the table directly. Just as humans improve their understanding of sentences by comparing them, they can also enhance their understanding by comparing tables. With this idea, in this paper, we introduce ACCIO, tAble understanding enhanCed via Contrastive learnIng with aggregatiOns, a novel approach to enhancing table understanding by contrasting original tables with their pivot summaries through contrastive learning. ACCIO trains an encoder to bring these table pairs closer together. Through validation via column type annotation, ACCIO achieves competitive performance with a macro F1 score of 91.1 compared to state-of-the-art methods. This work represents the first attempt to utilize pairs of tables for table embedding, promising significant advancements in table comprehension. Our code is available at https://github.com/whnhch/ACCIO/.","Year Month Passengers 1949 January 112 1949 February 118 1949 March 132 1949 April 129 1949 May 121 (a) Month 1949 1950 1951 April 129 135 163 August 148 170 199 December 118 140 166 February 118 126 150 January 112 115 145 (b) Table 1: Passenger data and pivot table. (a) is the original tabular data containing year and month passenger attributes. (b) is a pivot table with a user’s query of “the average number of passengers by month and year” Leveraging the success of natural language processing techniques, the comprehension of tables has also significantly grown. Many works related to understanding tables have been helpful in applications, such as column type annotation, joining relation databases from data lakes, table to visualization, and table normalization. These efforts typically analyze tables based on the structure of the table, column relationships, or entity associations. However, to our knowledge, no prior research has been aimed at enhancing table understanding by comparing two tables. In the realm of sentence embeddings, success has been achieved by comparing sentences using techniques like SBERT Reimers and Gurevych (2019) or SimCSE Gao et al. (2021), leveraging natural language inference (NLI) datasets Bowman et al. (2015); Nie et al. (2020). These datasets typically contain triplets consisting of a premise, an entailment, and a contradiction. The entailment sentence can be logically inferred from the premise, while the contradiction sentence directly contradicts the premise. Previous studies Reimers and Gurevych (2019); Gao et al. (2021) have introduced methods such as closing the entailment sentence and premise and contrasting the premise with the contradiction sentence, ultimately leading to high-quality sentence embeddings. Therefore, in this paper, we exploit the notion that premise and entailment sentences should be closely related by leveraging pivot tables and the original table as the tables that should be conceptually close. Pivot tables are summaries of tables using aggregation by user-defined parameters such as index, column, value, and aggregation function. For instance, Table 1(a) represents the original tabular data consisting of year, month, and passengers. Users typically analyze or summarize tables by using pivot tables derived from such data. For example, when a user wants to determine “the average number of passengers by month and year,” they can obtain a pivot result like Table 1(b). We present a novel approach to table understanding, called ACCIO, tAble understanding enhanCed via Contrastive learnIng with aggregatiOns. By training an encoder with original data and its corresponding pivot table using contrastive learning, we aim to bring them closer together. It’s worth highlighting that this paper marks the first attempt to utilize a pair of tables for table embedding. We validate our training method through a downstream task known as column type annotation. This task is commonly used to evaluate the quality of table embeddings by predicting the types of given columns. The performances indicate that our approach achieves comparable performance in terms of macro F1 score 91.1 for column type annotation compared to state-of-the-art baselines."
https://arxiv.org/html/2411.04427v1,"One fish, two fish, but not the whole sea:Alignment reduces language models’ conceptual diversity","Researchers in social science and psychology have recently proposed using large language models (LLMs) as replacements for humans in behavioral research. In addition to arguments about whether LLMs accurately capture population-level patterns, this has raised questions about whether LLMs capture human-like conceptual diversity. Separately, it is debated whether post-training alignment (RLHF or RLAIF) affects models’ internal diversity. Inspired by human studies, we use a new way of measuring the conceptual diversity of synthetically-generated LLM “populations” by relating the internal variability of simulated individuals to the population-level variability. We use this approach to evaluate non-aligned and aligned LLMs on two domains with rich human behavioral data. While no model reaches human-like diversity, aligned models generally display less diversity than their instruction fine-tuned counterparts. Our findings highlight potential trade-offs between increasing models’ value alignment and decreasing the diversity of their conceptual representations.","As large language models (LLMs) have become more sophisticated, there has been growing interest in using them to replace human labor. This appeal of LLMs has even made its way to settings where human behavior itself is the object of inquiry: recently, researchers have proposed that LLM-generated responses can be used in place of human data for tasks such as polling, user studies, and behavioral experiments (e.g., Aher et al., 2023; Argyle et al., 2023; Hämäläinen et al., 2023; Binz and Schulz, 2024; Manning et al., 2024). If possible, using a synthetic replacement for the process of human data collection could be transformative for a variety of human factors disciplines, from political science to economics and psychology. Figure 1: We investigate LLM populations comprised of simulated individuals in two domains: color associations (top) and concept similarity (bottom). In both domains, there is both individual- and population-level variation. It is possible that individual variation overlaps with the population average (homogeneous population) or separates from it (heterogeneous population). Our experiments are designed to tease these two options apart. While prior work suggests that LLMs can capture certain behavioral patterns, there are ongoing debates as to whether they are valid replacements for human subjects Dillion et al. (2023); Wang et al. (2024a); Park et al. (2024). One key issue is whether LLMs capture conceptual diversity: the variation among individuals’ representations of a particular domain. A natural way to study conceptual diversity is to study the variability in LLMs’ response distributions at the population level – that is, by considering averages across simulated individuals. However, population-level variability can be a flawed measure of conceptual diversity for several reasons. First, it assumes certain characteristics about the nature of people’s mental representations that may not hold in practice – for example, that people have high certainty in these representations (e.g., Martí et al., 2018), or that people’s responses are a deterministic best-guess, with no additional information contained in successive guesses (e.g., Vul and Pashler, 2008). Second, assessing population-level variation without individual-level variation can mask important information about the population, particularly whether it is homogeneous (comprised of individuals who share similar underlying representations) or heterogeneous (comprised of individuals with meaningfully different representations) (see Figure 1). Conceptual diversity may be negatively impacted by alignment, which further complicates the picture. Post-training alignment techniques, such as RLHF (Ouyang et al., 2022; Bai et al., 2022a) and RLAIF (Bai et al., 2022b), are now standard parts of LLM development, and presumed to contribute to the human-like abilities of models (Ji et al., 2024). However, it has also been observed that “aligned” models show biases that can limit the lexical and content diversity of their outputs (janus, 2022; Padmakumar and He, 2024; Park et al., 2024; O’Mahony et al., 2024). It also remains unknown whether alignment to synthetic preferences (instead of human preferences) might worsen these biases, as it is possible these models “collapse” when recursively trained on synthetically generated data (Shumailov et al., 2024; Gerstgrasser et al., 2024). In this paper, we (1) investigate the ability of LLMs to capture human-like conceptual diversity, and (2) analyze the effect of alignment by comparing conceptual diversity across non-aligned models to models aligned using RLHF or RLAIF. In our experiments, we first simulate populations of unique individuals in LLMs using two techniques proposed by prior literature: temperature- and prompt-based manipulations. We then evaluate ten open-source LLMs on two domains with rich human behavioral data: word-color associations, and conceptual similarity judgments. While there is no single agreed-upon metric for capturing conceptual diversity, we consider two different metrics that are each well-suited to their respective domains, to measure the conceptual diversity of synthetically-generated LLM “populations”. We find that no model approaches human-like conceptual diversity. Further, aligned models generally display less conceptual diversity than their non-aligned, fine-tuned counterparts. Our findings suggest that there may be trade-offs between increasing model safety in terms of value alignment, and decreasing other notions of safety, such as the diversity of thought and opinion that models represent. We caution that these trade-offs should be better understood before models are used as replacements for human subjects, or deployed in human-centered downstream applications. Base model Non-aligned variant Aligned variant Alignment method Mistral (mistralai/Mistral-7B-v0.1) Openchat (openchat/openchat_3.5) Starling (berkeley-nest/Starling-LM-7B-alpha) RLAIF (APA) Mistral (mistralai/Mistral-7B-v0.1) Mistral-Instruct (mistralai/Mistral-7B-Instruct-v0.1) Zephyr-Mistral (HuggingFaceH4/zephyr-7b-beta) RLAIF (DPO) Gemma (google/gemma-7b) Gemma-Instruct (google/gemma-7b-it) Zephyr-Gemma (HuggingFaceH4/zephyr-7b-gemma-v0.1) RLAIF (DPO) Llama (meta-llama/Llama-2-7b-hf) Llama (meta-llama/Llama-2-7b-hf) Llama-Chat (meta-llama/Llama-2-7b-chat-hf) RLHF (PPO) Llama (meta-llama/Llama-2-7b-hf) Tulu (allenai/tulu-2-7b) Tulu-DPO (allenai/tulu-2-dpo-7b) RLAIF (DPO) Table 1: Pairs of non-aligned and aligned models tested in our experiments. Huggingface identifiers are shown in parentheses underneath model names. The base models are shown for reference and are not directly tested, with the exception of the comparison between Llama and Llama-Chat. All models have 7B parameters."
https://arxiv.org/html/2411.04425v1,DELIFT: Data Efficient Language model Instruction Fine-Tuning,"Fine-tuning large language models (LLMs) is essential for enhancing their performance on specific tasks but is often resource-intensive due to redundant or uninformative data. To address this inefficiency, we introduce DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel algorithm that systematically optimizes data selection across the three key stages of fine-tuning: (1) instruction tuning, (2) task-specific fine-tuning (e.g., reasoning, question-answering), and (3) continual fine-tuning (e.g., incorporating new data versions). Unlike existing methods that focus on single-stage optimization or rely on computationally intensive gradient calculations, DELIFT operates efficiently across all stages. Central to our approach is a pairwise utility metric that quantifies how beneficial a data sample is for improving the model’s responses to other samples, effectively measuring the informational value relative to the model’s current capabilities. By leveraging different submodular functions applied to this metric, DELIFT selects diverse and optimal subsets that are useful across all stages of fine-tuning. Experiments across various tasks and model scales demonstrate that DELIFT can reduce the fine-tuning data size by up to 70% without compromising performance, offering significant computational savings and outperforming existing methods in both efficiency and efficacy.","Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019; Brown et al., 2020a; Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation—from a general-purpose model to a task-specific agent—heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model’s subsequent performance (Wei et al., 2022; Zhou et al., 2023; Hoffmann et al., 2024). The effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020; Wei et al., 2022; Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model’s capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: How can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? To address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model’s current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. The typical fine-tuning process comprises three key stages: 1. Instruction Tuning: Enhances the model’s ability to follow general instructions (Mishra et al., 2022; Wei et al., 2022; Longpre et al., 2023); 2. Task-Specific Fine-Tuning: Refines the model’s expertise in specific domains (Gururangan et al., 2020; Cobbe et al., 2021); 3. Continual Fine-tuning: Enables the model to integrate new information while mitigating catastrophic forgetting (Madotto et al., 2021; Wu et al., 2024). DELIFT is able to optimize data selection processes across all three stages. Additionally, DELIFT offers significant benefits for In-Context Learning (ICL) (Brown et al., 2020b; Xue et al., 2024). By utilizing the selected subsets as the ICL example pool, DELIFT achieves similar or better performance compared to using the entire dataset, thereby enhancing data efficiency in ICL scenarios. This dual functionality is empirically validated in our experimental results. Existing data selection methodologies often fail to address the nuanced requirements of the aforementioned distinct fine-tuning stages. Many approaches are tailored to a single stage, lacking the adaptability needed for comprehensive fine-tuning (Xia et al., 2024; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024). Others depend on computationally intensive procedures, such as exhaustive gradient computations, rendering them impractical for large-scale models and datasets (Killamsetty et al., 2021b; a; Xia et al., 2024; Zhang et al., 2024). Additionally, some methods utilize features obtained from an independent model that are not specifically aligned with the model undergoing fine-tuning, reducing their effectiveness (Killamsetty et al., 2023; Liu et al., 2024; Bukharin & Zhao, 2024; Chen et al., 2024; Du et al., 2023). DELIFT addresses these limitations by adapting to the unique requirements of each fine-tuning stage. 1. Instruction Tuning: Selects diverse data to enhance general instruction-following capabilities; 2. Task-Specific Fine-Tuning: Prioritizes data that is aligned with the target task, to refine specialized expertise; 3. Continual Fine-tuning: Identifies novel, complementary information to expand the model’s knowledge base while safeguarding against catastrophic forgetting. Figure 1 illustrates how DELIFT optimizes data selection across these stages, demonstrating the selection and pruning processes in each fine-tuning phase. By leveraging submodular optimization techniques (Fujishige, 2005; Bilmes, 2022) and submodular information measures (Iyer et al., 2021), DELIFT efficiently selects optimal data subsets that precisely address the model’s learning requirements without incurring unnecessary computational costs. This approach effectively balances data utility and computational efficiency. ( ( ( Figure 1: DELIFT data selection across fine-tuning stages. (a) Instruction Tuning: Diverse instructions selected; redundant samples pruned. (b) Task-Specific Fine-Tuning: Mutually informative (with benchmark data) and diverse samples are prioritized for selection. (c) Continual Fine-tuning: New samples that are novel are integrated; new samples with overlapping information are pruned. a) b) c) Our key contributions are as follows: 1) Versatile Pairwise Utility Metric: A novel, easy-to-compute metric for assessing data informativeness, incorporating model feedback applicable across all fine-tuning stages. 2) Unified Data Selection Algorithm: DELIFT systematically optimizes data selection for instruction tuning, task-specific fine-tuning, and continual fine-tuning within a single framework. 3) Computational Efficiency: Circumvents resource-intensive operations, ensuring scalability to large datasets and models. DELIFT achieves at least 70% reduction in computational time compared to gradient-based methods on benchmark tasks. 4) Enhanced Performance with Reduced Data: Demonstrates the ability to reduce fine-tuning data size by up to 70% without compromising performance, and achieves comparable efficacy as to utilizing the full dataset. 5) Improvement over Existing Methods: Outperforms current data selection techniques by up to 26% in effectiveness across diverse tasks and model scales (see Section 4). The remainder of this paper is organized as follows: Section 2 provides background on fine-tuning LLMs and reviews related work. Section 3 details the methodology behind DELIFT, including the development of our pairwise utility metric and the submodular optimization process. Section 4 presents experimental results that showcase the effectiveness and efficiency of our method. Section 5 discusses the implications of our findings and potential future directions. Finally, we release our code base for further research."
https://arxiv.org/html/2411.04424v1,Bayesian Calibration of Win Rate Estimation with LLM Evaluators,"Recent advances in large language models (LLMs) show the potential of using LLMs as evaluators for assessing the quality of text generations from LLMs. However, applying LLM evaluators naively to compare or judge between different systems can lead to unreliable results due to the intrinsic win rate estimation bias of LLM evaluators. In order to mitigate this problem, we propose two calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene, both of which leverage Bayesian inference to more accurately infer the true win rate of generative language models. We empirically validate our methods on six datasets covering story generation, summarization, and instruction following tasks. We show that both our methods are effective in improving the accuracy of win rate estimation using LLMs as evaluators, offering a promising direction for reliable automatic text quality evaluation.","Figure 1: Illustration of our pipeline and previous work. The “calibration” part of our pipeline indicates one of BWRS or Bayesian Dawid-Skene. Evaluating the quality of AI-generated text has been a longstanding and evolving challenge in NLP. In recent years, this challenge has become increasingly crucial due to the growing interest in the field of generative AI. While human judgment is still considered the most reliable form of assessment, common automatic approaches to evaluating quality of AI-generated text include heuristic-based evaluation metrics Papineni et al. (2002); Lin (2004); Pillutla et al. (2021), model-based evaluation metrics Zhang et al. (2019); Fabbri et al. (2022); Zha et al. (2023); Chen and Eger (2023), and recently, LLM-based evaluations Kim et al. (2024a, b); Wang et al. (2024). Due to their relative low cost and high correlation with human preferences, LLM-based evaluations (aka LLM-as-a-judge) are receiving increasing attention. Most previous studies that apply LLM evaluators Chiang and Lee (2023a, b); Dubois et al. (2024); Kim et al. (2024a, b); Wang et al. (2024); Liu et al. (2024) attempt to improve the agreement between LLM evaluators and human preference by training expert models for evaluation or improving prompting strategies. However, such methods often either require compute-expensive finetuning, or suffer from common problems of LLM evaluators such as position bias Wang et al. (2023b), self-preference, and more Koo et al. (2023). Besides, as we will discuss in Section 3.2, directly applying a non-perfect LLM evaluator will result in a bias problem in the estimation of win rate. In this paper, we attempt to address these challenges by proposing two methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene. A general illustration of our pipeline is shown in Figure 1. Our approaches leverage Bayesian inference to enhance the accuracy of win rate estimations between competing text generators using evaluation results of LLM evaluators and sparse or no human evaluation data. By employing these methods, we observe a closer alignment between LLM and human judgment in terms of win rate between two text generator models. Our results on six diverse datasets demonstrate that both BWRS and Bayesian Dawid-Skene effectively reduce win rate estimation bias of LLM evaluators, marking a promising step toward more trustworthy automatic evaluations in NLP. 111The code and data used in our experiments are available at https://github.com/yale-nlp/bay-calibration-llm-evaluators under Apache 2.0 license. The contribution of this paper is threefold: • We identify and formulate the win rate estimation bias problem associated with LLM evaluators. • We conduct exploratory study on mitigating this bias with Bayesian inference. Specifically, we propose BWRS and Bayesian Dawid-Skene, both of which are shown effective in calibrating win rate estimation given LLM evaluation results, and optionally, some human evaluation results. • We publish our LLM evaluation annotations to facilitate future study in LLM-based evaluation."
https://arxiv.org/html/2411.04368v1,Measuring short-form factuality inlarge language models,"We present SimpleQA, a benchmark that evaluates the ability of language models to answer short, fact-seeking questions. We prioritized two properties in designing this eval. First, SimpleQA is challenging, as it is adversarially collected against GPT-4 responses. Second, responses are easy to grade, because questions are created such that there exists only a single, indisputable answer. Each answer in SimpleQA is graded as either correct, incorrect, or not attempted. A model with ideal behavior would get as many questions correct as possible while not attempting the questions for which it is not confident it knows the correct answer. SimpleQA is a simple, targeted evaluation for whether models “know what they know,” and our hope is that this benchmark will remain relevant for the next few generations of frontier models. SimpleQA can be found at https://github.com/openai/simple-evals.","An open problem in artificial intelligence is how to train language models that produce responses that are factually correct. Current frontier models sometimes produce false outputs or answers that are not substantiated by evidence, a problem known as “hallucinations.” Such hallucinations are one of the major barriers for broader adoption of general forms artificial intelligence like large language models. Factuality is a complicated topic because it is hard to measure—evaluating the factuality of any given arbitrary claim can be challenging, and language models often generate long completions that contain dozens of factual claims. In this work we will sidestep the open-endedness of language models by considering only short, fact-seeking questions with a single answer. This reduction of scope is important because it makes measuring factuality much more tractable, albeit at the cost of leaving open research questions such as whether improved behavior on short-form factuality generalizes to long-form factuality. We present a benchmark called SimpleQA, which contains 4,326 short, fact-seeking questions. SimpleQA was designed with a few important properties in mind: • High correctness. Reference answers to questions are determined by two independent AI trainers, and questions were written in such a way that the predicted answers are easily gradable. • Good researcher UX. SimpleQA is fast and simple to run, as questions and answers are very short. Grading is also fast to run via the OpenAI API (or another frontier model API). Additionally, with 4,326 questions in the dataset, SimpleQA should have relatively low run-to-run variance. • Challenging for frontier models. Compared to older benchmarks such as TriviaQA (Joshi et al., 2017) or Natural Questions (Kwiatkowski et al., 2019) that are now saturated, SimpleQA is created to be challenging for frontier models (e.g., GPT-4o and Claude both score less than 50%). • Diversity. SimpleQA contains questions from a wide range of topics, including history, science & technology, art, geography, TV shows, etc. The goal is for SimpleQA to be a simple and reliable dataset for measuring the factuality of frontier models. A few example questions are shown in Table 1 below. Question Answer Who received the IEEE Frank Rosenblatt Award in 2010? Michio Sugeno On which U.S. TV station did the Canadian reality series *To Serve and Protect* debut? KVOS-TV What day, month, and year was Carrie Underwood’s album “Cry Pretty” certified Gold by the RIAA? October 23, 2018 What is the first and last name of the woman whom the British linguist Bernard Comrie married in 1985? Akiko Kumahira Table 1: Four example questions and reference answers from SimpleQA."
https://arxiv.org/html/2411.04329v1,CodeTree: Agent-guided Tree Search for Code Generation with Large Language Models,"Pre-trained on massive amounts of code and text data, large language models (LLMs) have demonstrated remarkable achievements in performing code generation tasks. With additional execution-based feedback, these models can act as agents with capabilities to self-refine and improve generated code autonomously. However, on challenging coding tasks with extremely large search space, current agentic approaches still struggle with multi-stage planning, generating, and debugging. To address this problem, we propose CodeTree, a framework for LLM agents to efficiently explore the search space in different stages of the code generation process. Specifically, we adopted a unified tree structure to explicitly explore different coding strategies, generate corresponding coding solutions, and subsequently refine the solutions. In each stage, critical decision-making (ranking, termination, expanding) of the exploration process is guided by both the environmental execution-based feedback and LLM-agent-generated feedback. We comprehensively evaluated CodeTree on 7 code generation benchmarks and demonstrated the significant performance gains of CodeTree against strong baselines. Using GPT-4o as the base model, we consistently achieved top results of 95.1% on HumanEval, 98.7% on MBPP, and 43.0% on CodeContests. On the challenging SWEBench benchmark, our approach led to significant performance gains.","Recently, we have witnessed significant impacts of large language models (LLMs) beyond the NLP domain such as in coding tasks Achiam et al. (2023); Touvron et al. (2023a); Wang et al. (2023); Rozière et al. (2023). However, different from traditional NLP tasks, coding tasks require generated code to be fully executable and functionally correct i.e. containing no programmatic syntax errors and passing all possible test cases Chen et al. (2021); Austin et al. (2021); Hendrycks et al. (2021). Given the extremely large search space in code, early methods propose to sample a very large number of generation outputs (for example, Li et al. (2022) generated up to 1 million samples per problem) to increase the chance of generating a correct code solution. Approach Explore Exploit Execution feedback AI feedback Multi- agent Action CodeRanker Inala et al. (2022) ✓ ✓ AlphaCode Li et al. (2022), MBR-Exec Shi et al. (2022), CodeT Chen et al. (2023b) ✓ ✓ LEVER Ni et al. (2023), Coder-Reviewer Zhang et al. (2023b) ✓ ✓ ✓ Self-correct Welleck et al. (2023), ILF Chen et al. (2023a), Self-refine Madaan et al. (2023) ✓ ✓ CodeChain Le et al. (2024) ✓ ✓ Self-debug Chen et al. (2023d), Self-repair Olausson et al. (2023), Reflexion Shinn et al. (2023) ✓ ✓ ✓ CAMEL Li et al. (2023a) ✓ ✓ ChatDev Qian et al. (2024), MetaGPT Hong et al. (2023), AgentVerse Chen et al. (2023c) ✓ ✓ ✓ Self-collaboration Dong et al. (2023), AgentCoder Huang et al. (2023) ✓ ✓ ✓ ✓ CodeTree (ours) ✓ ✓ ✓ ✓ ✓ ✓ Table 1: We compare CodeTree with related methods in 6 aspects: (1) Explore which adopts a brute-force approach to independently generate a large number of code candidates; (2) Exploit which focuses on self-refinement using a small subset of output solutions; (3) Execution feedback which uses code execution outcomes to improve code qualities; (4) AI feedback which enables synthetic feedback generated by LLMs to improve output code; (5) Multi-agent which adopts multiple LLM agents to play different roles in the code generation process; and (6) Action where LLM agents can take different actions and facilitate decision-making. More recently, several approaches adopted a “vertical” strategy in which LLMs first generate one (or very few) generation output, and then iteratively refine this output multiple times, often conditioned by some forms of external feedback Le et al. (2022); Chen et al. (2023b); Shinn et al. (2023). While these approaches are more cost-effective by focusing only on a small subset of the search space (i.e. starting from an initial output candidate), the performances of these approaches are bounded by the local optima of the chosen search space. Related to our work, several methods for NLP reasoning tasks were introduced to control and enhance the generation procedure of LLMs. For example, Wang et al. (2022) proposed to enhance LLMs with chains of thought and statistically select the right solutions based on majority voting. Zhou et al. (2023) decomposed a task into smaller sub-tasks and addressed them by increasing the order of difficulty. Yao et al. (2024) proposed to improve LLMs by adopting a tree-based structure to explicitly simulate the exploration of thoughts in a tree. We are motivated by this line of research and proposed CodeTree, a new generation framework to effectively explore the search space of code generation tasks through a tree-based structure. An overview of CodeTree is given in Figure 1. We define 3 standard agents, Thinker, Solver, and Debugger, to equip the strategy-planning, solution implementation, and solution improvement correspondingly, posing comprehensive roles needed for code generation. A CodeTree starts from the input problem as the tree root and subsequent nodes represent code solutions. At any node of the tree, one can either explore sibling nodes (other strategies from the same parent node) or its children (refinements of this node). Within CodeTree, agents can interact with each other through a tree expansion guided by a Critic Agent, searching for the optimal code solution. Rather than following heuristic rules or classic tree traversal methods, we use Critic Agent to self-evaluate the status of tree nodes at each tree expansion step by executing the following tasks: • Node scoring: Evaluate the test case outputs of generated code and assess whether the tree nodes faithfully follow their corresponding coding strategies. • Solution verification & evaluation: For a solution that passes visible test cases, verify if it should be further improved; for a solution that fails, evaluate if it is a promising direction to debug and refine the solution. Based on the outputs of the above evaluation tasks, Critic Agent can take action to either refine, abort, or accept the current solution, which automatically expands or terminates the tree search. CodeTree is flexible and efficient, avoiding duplicated or redundant exploration of functionally similar or unfeasible solutions. We comprehensively evaluated CodeTree on diverse code generation benchmarks from beginner- to competition-level coding tasks. Our results demonstrated the significant and consistent performance gains of CodeTree against strong baselines. Using GPT-4o as the base models, we achieved the top results on HumanEval+, MBPP+ Liu et al. (2023), and CodeContests Li et al. (2022) respectively. On the challenging SWEBench benchmark, our approach led to significant performance gains. We also conducted comprehensive ablation and qualitative analysis to derive the best practices and any limitations of the current method."
https://arxiv.org/html/2411.04328v1,Balancing Transparency and Accuracy: A Comparative Analysis of Rule-Based and Deep Learning Models in Political Bias Classification,"The unchecked spread of digital information, combined with increasing political polarization and the tendency of individuals to isolate themselves from opposing political viewpoints, has driven researchers to develop systems for automatically detecting political bias in media. This trend has been further fueled by discussions on social media. We explore methods for categorizing bias in US news articles, comparing rule-based and deep learning approaches. The study highlights the sensitivity of modern self-learning systems to unconstrained data ingestion, while reconsidering the strengths of traditional rule-based systems. Applying both models to left-leaning (CNN) and right-leaning (FOX) news articles, we assess their effectiveness on data beyond the original training and test sets. This analysis highlights each model’s accuracy, offers a framework for exploring deep-learning explainability, and sheds light on political bias in US news media. We contrast the opaque architecture of a deep learning model with the transparency of a linguistically informed rule-based model, showing that the rule-based model performs consistently across different data conditions and offers greater transparency, whereas the deep learning model is dependent on the training set and struggles with unseen data.","The current political climate in the United States is characterized by intense polarization and an unprecedented ease of publishing and disseminating information, where partisan hostility and negative perceptions of opposing party members are at an all-time high Doherty et al. (2023). This dynamic is further exacerbated by social media platforms, where users curate their news feeds in a way that reinforces existing biases and isolates them from diverse perspectives, stifling constructive dialogue and creating what researchers term “epistemic bubbles” Kelly (2021). To address this, Natural Language Processing (NLP) researchers have developed models intended to automatically and objectively detect the presence and direction of bias. Examples include model architectures ranging from rule-based designs Hube and Fetahu (2018) to State of the Art (SoA) transformer architectures Raza et al. (2024). While SoA architectures have been shown to distinguish biased narratives from neutral ones, they struggle to learn the nuanced nature of bias expression without a sufficiently large and comprehensive dataset. Figure 1: Comparison of Rule-based and Convolutional NN models: CNN and FOX news articles serve as external, unseen datasets for the Convolutional NN model. The rule-based model determines political bias using three linguistic features. Our contributions include an investigation of both a rule-based and a deep learning model for political bias classification as depicted in Figure 1, with the goal of promoting a more informed discussion on bias detection methodologies. To overcome data demands of SoA architectures, we adopt a convolutional neural network model.111For brevity, we use “convolutional NN model” henceforth, as the abbreviation CNN is employed to refer to a news outlet. Our contrasting approach is a simpler, more transparent rule-based model for bias classification using sentiment detection and linguistic features. This model does not rely on preexisting bias lexicons, “black box” machine learning models, or large training datasets. Moreover, its simplicity allows for easy correction, with a few, clearly delineated, components. A second contribution is the use of linguistic information for detecting an article’s stance towards entities.222We define stance as the overall attitude of a news article toward an entity, whereas sentiment refers to a sentence-level (pos/neg) label. Our rule-based approach includes a novel part-of-speech driven “reference resolution” (e.g., associating adjectives with a corresponding noun), for a more focused stance assignment. We emphasize that it is not our goal to achieve SoA performance for political bias classification through the rule-based model, but rather to explore the extent to which straightforward linguistic features (parts of speech, coreference, and sentiment) can be leveraged to classify political bias. A third contribution involves exploring methods to enhance explainability of deep learning models. By testing a convolutional NN model on various datasets and correlating its performance disparities with differences in the data, we identify the features prioritized by the model. Our findings show that the rule-based model maintains consistent performance across various data conditions, presenting a clear right-leaning bias for FOX. By contrast, the convolutional NN model relies heavily on its training set, struggling with data not directly related to the political bias data on which it is trained. The rule-based approach performs comparably to deep learning in these situations, making it more applicable to real-world scenarios and offering greater transparency. The next section reviews bias detection methodologies in news media. Section 3 covers data collection, preprocessing, and experimental setup. Section 4 details the implementation of rule-based and convolutional NN models. Section 5 evaluates model performance and their application to external data, with concluding remarks in Section 6."
https://arxiv.org/html/2411.04308v1,Improving Bilingual Capabilities of Language Models to Support Diverse Linguistic Practices in Education,"Large language models (LLMs) offer promise in generating educational content, providing instructor feedback, and reducing teacher workload on assessments. While prior studies have focused on studying LLM-powered learning analytics, limited research has examined how effective LLMs are in a bilingual context. In this paper, we study the effectiveness of multilingual large language models (MLLMs) across monolingual (English-only, Spanish-only) and bilingual (Spanglish) student writing. We present a learning analytics use case that details LLM performance in assessing acceptable and unacceptable explanations of Science and Social Science concepts. Our findings reveal a significant bias in the grading performance of pre-trained models for bilingual writing compared to English-only and Spanish-only writing. Following this, we fine-tune open-source MLLMs including Llama 3.1 and Mistral NeMo using synthetic datasets generated in English, Spanish, and Spanglish. Our experiments indicate that the models perform significantly better for all three languages after fine-tuning with bilingual data. This study highlights the potential of enhancing MLLM effectiveness to support authentic language practices amongst bilingual learners. It also aims to illustrate the value of incorporating non-English languages into the design and implementation of language models in education.","Recent research in learning analytics highlights the importance of addressing diversity, inclusivity, and equity to better support all learners (Karumbaiah et al., 2021; Shams et al., 2023). The advent of large language models (i.e., algorithms trained on large amounts of text to understand and process human language) in learning analytics research and practice raises similar questions about equity (Anis, 2023). How well can language models trained predominantly on mainstream English data serve students from diverse linguistic backgrounds? Multilingual large language models (MLLMs) that can process and produce text in multiple languages offer promising new directions for supporting authentic multilingual communication. However, despite their impressive performance in individual languages, MLLMs are limited in their ability to switch fluidly across languages (known as translanguaging or code-switching (Bang et al., 2023; Zhang et al., 2023)) - leading to an inaccurate representation of language use by bilingual learners. Homogeneity in MLLM training (Yong et al., 2023) due to scarcities in code-switched data further contributes to the issue, as well as the lack of safety benchmarks and comprehensive evaluation (Qin et al., 2024). As learning analytics powered by language models make their way into classrooms, studying the affordances and constraints of using MLLMs becomes increasingly important.Building upon existing research on translanguaging, bi/multilingualism, and natural language processing, the current study necessitates the integration of languages produced by bilingual learners, such as Spanglish (translanguaging in Spanish and English) into the training and evaluation of MLLMs. We investigate the code-switching and multilingual capabilities of MLLMs in an illustrative learning analytics use case of assessing student writing. We ask the following research question: How does MLLM performance vary across assessing monolingual (English, Spanish) and bilingual (Spanglish) student writing? We first evaluate the performance of two MLLMs in assessing Science and Social Science ideas as expressed in English, Spanish, and Spanglish. To overcome the data scarcity issue, we create and use a synthetically-generated dataset evaluated by humans on both the language and content accuracy. Then, we attempt to improve MLLM performance using techniques such as prompting and fine-tuning. Our hypotheses include: • H1: MLLMs are significantly more accurate when ideas are presented in English or Spanish, but are significantly less accurate when ideas are in Spanglish. • H2: Fine-tuning with the target language will significantly improve MLLM performance. That is, fine-tuning a model on Spanglish will maximize its performance in identifying ideas expressed in Spanglish. During translanguaging, speakers draw from their entire linguistic background and cultural identity to navigate and defy the socio-political boundaries of ”proper” language use, in turn promoting authentic social interaction and communication (Hamman, 2018). Additionally, Li Wei (Wei, 2011) defines the term translanguaging space as a place where cultural and linguistic boundaries eclipse in bilinguals’ daily lives. Within these spaces, both criticality and creativity work simultaneously to inform bilingual learners’ perceptions on socio-cultural and linguistic phenomena and determine what constitutes the norms of language use. Much of the discourse surrounding these practices has shifted towards how to leverage students’ linguistic resources in learning environments. With increasing globalization, instructional resources such as the CUNY-NYSIEB Translanguaging Guides now include pedagogical strategies that use bilingualism as a resource to leverage learning. These guides emphasize collaborative work and provide linguistic resources for EBLs of varying age and grade levels, and have been widely used in the past decade by educational communities working towards putting theory into practice (Hesson et al., 2014). These resources have useful applications when promoting students’ existing language background, cultural identity, and multi-modal practices (i.e., gesture) as meaning- and sense-making processes. As the learning analytics field consistently showcases diverse learner perspectives, we view translanguaging as an increasingly valuable framework that empowers bilingual learners to convey ideas and understanding using their complete linguistic repertoire. 1.1. LLM Support in Education Large Language Models (LLMs), with their ability to generate language from large-scale datasets, have gained prominence in the learning analytics field. They are applied in tasks such as essay scoring and feedback generation, highlighting LLMs’ growing role in saving instructor time and providing valuable feedback for learners (Li and Liu, 2024). For instance, Automated Essay Scoring (AES) systems reduce examiner workloads in large-scale assessments like TOEFL and GMAT. LLMs like BERT outperformed previous AES methods, establishing new benchmarks in the field (Hirao et al., 2020). Beyond scoring, LLMs are capable of providing personalized feedback for intelligent tutoring systems (Meyer et al., 2024), aiding researchers in developing high-quality educational tools. They also show aptitude in classroom-specific subjects such as mathematics, are capable of generating multiple-choice questions (McNichols et al., 2023), plotting figures (Bulusu et al., [n. d.]), and provide teacher training via simulations (Lee et al., [n. d.]). With these advancements, it is evident that LLMs have immense potential in classrooms, making it increasingly important to extend the same support to bi/multilingual classrooms. 1.2. Multilingual LLMs In general, since LLMs are trained on English-centric data, LLMs perform better in English than in non-English languages (Zhu et al., 2023). However, newer MLLMs such as GPT-4 (Achiam et al., 2023) demonstrate increasing multilingual capabilities and are outperforming their predecessors on multilingual benchmarks (Ahuja et al., 2023). Despite recent advancements in MLLM research, further inspection reveals a challenge: many MLLMs still struggle with code-switching due to the lack of diverse linguistic resources in training data (Zhang et al., 2023). Additionally, there is a lack of comprehensive benchmarks to support and analyze MLLMs recent developments (Qin et al., 2024). As a result, many languages with limited digital resources are excluded (Joshi et al., 2020). Code-switching resources in particular are scarce due to the lack of large-scale annotations that require multilingual human-raters (Winata et al., 2022). To address this data shortage, researchers have used manually created datasets using methods such as random replacements and noun-phrase translations (Laureano De Leon et al., 2024) or by using MLLMs to generate synthetic code-switched texts (Hu et al., 2023; Yong et al., 2023). In synthetic generation, recent studies show that newer MLLMs such as GPT-4 are more robust for code-switching and cross-lingual understanding (Huzaifah et al., 2024), posing a potential solution for contexts requiring code-switching capabilities. As bilingual learners engage in classroom discussions in two or more languages, it is crucial to develop fair and reliable datasets for MLLMs to capture dynamic bi/multilingual processes."
https://arxiv.org/html/2411.04291v1,Unfair Alignment: Examining Safety Alignment Across Vision Encoder Layers in Vision-Language Models,"Content warning: This paper contains unsafe model-generated content.Vision-language models (VLMs) have improved significantly in multi-modal tasks, but their more complex architecture makes their safety alignment more challenging than the alignment of large language models (LLMs). In this paper, we reveal an unfair distribution of safety across the layers of VLM’s vision encoder, with earlier and middle layers being disproportionately vulnerable to malicious inputs compared to the more robust final layers. This “cross-layer” vulnerability stems from the model’s inability to generalize its safety training from the default architectural settings used during training to unseen or out-of-distribution scenarios, leaving certain layers exposed. We conduct a comprehensive analysis by projecting activations from various intermediate layers and demonstrate that these layers are more likely to generate harmful outputs when exposed to malicious inputs. Our experiments with LLaVA-1.5 and Llama 3.2 show discrepancies in attack success rates and toxicity scores across layers, indicating that current safety alignment strategies focused on a single default layer are insufficient.","With the recent success of LLMs in advancing natural language understanding and generation, researchers have extended these models to incorporate additional modalities, such as vision, leading to the development of VLMs. Given their popularity, an essential question arises: “Are these large models truly safe for everyone or could they be jailbroken to produce harmful outputs?”. Extensive work has been done in this area, with researchers using techniques like supervised fine-tuning (SFT) (Zong et al., 2024; Liu et al., 2024), reinforcement learning from human feedback (RLHF) (Bai et al., 2022), and unlearning (Chakraborty et al., 2024) to safety align these models, adapting techniques used for safety alignment of LLMs. Despite efforts to enhance model safety, recent research shows LLMs and VLMs remain vulnerable to various attacks, including adversarial perturbations (Zou et al., 2023), trojan attacks (Zhao et al., 2023a), and recently shown multi-modal attacks (Shayegani et al., 2024; Gong et al., 2023) even though the LLM backbones of these VLMs are safety aligned. This is due to the limited generalization of these large models to out-of-distribution (OOD) inputs not included in safety training datasets. Consequently, when faced with harmful multi-modal prompts, which appear OOD, these limitations result in safety mechanism failures, leading to jailbreaks. This underscores the need for an in-depth investigation into the vulnerabilities and risks of VLMs. To understand the inner workings of VLMs, we propose studying how intermediate layer activations in the vision encoder affect overall safety alignment. By focusing on these activations, we seek to better understand how they influence the VLM’s ability to handle harmful multi-modal prompts. Our work is motivated by observing that current safety alignment techniques default to using activations from the last or second-to-last layers of the vision encoder. This approach may unintentionally compromise safety training when activations from other intermediate layers are projected instead of the default layer that was used for safety training. We hypothesize that exposing the language model backbone to activations from other intermediate layers of the vision encoder that were not part of the training process, they are treated as OOD, which can then cause the model to violate the safety alignment. To validate our hypothesis, we present a methodology where the VLM is presented with harmful cross-modal input prompts (safe images, harmful text). Instead of using default alignment layers, we systematically project activations from various intermediate layers of the vision encoder to assess their impact on model’s safety alignment. Aligning with our hypothesis and prior research, we find that activations from specific intermediate layers can lead the VLM to generate harmful outputs. To assess the effectiveness of our approach, we evaluate two widely used VLMs, LLaVA-1.5 (Liu et al., 2023a) and Llama 3.2 (Dubey et al., 2024), using a set of prompts combining harmful text from the AdvBench dataset and safe images curated online. Additionally, we employ recent evaluation tools like Llama Guard (Inan et al., 2023), and Perspective API (Lees et al., 2022) to calculate metrics like Average Success Rate (ASR), Toxicity Scores (TS). Experimental results show a mismatch in alignment across intermediate layers of vision encoder, with late layers being more safety-aligned than early and middle layers. To our knowledge, we are the first to conduct a layer-wise analysis of the vision encoder in VLMs, revealing how intermediate activations affect the model’s overall safety alignment. Figure 1: (A) The standard approach uses the hidden state from the last layer of the vision encoder. We explore using hidden states from various layers, revealing that safety alignment varies across different layers and is not fairly distributed. (B) With the same input (image and prompt), selecting different vision encoder layers significantly impacts the safety of the output. (C) Safety training is applied with the model’s default settings and architecture. However, limited generalization causes vulnerabilities, leaving certain areas of the embedding space uncovered when architectural changes are made."
https://arxiv.org/html/2411.04223v1,Diversity Helps Jailbreak LargeLanguage Models,"We have uncovered a powerful jailbreak technique that leverages large language models’ ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62% higher success rate in compromising nine leading chatbots, including GPT-4, Gemini, and Llama, while using only 13% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the need to revolutionize testing methodologies to ensure robust and reliable LLM security.","Figure 1: Example of our successful jailbreak attack on OpenAI’s new model GPT-4o, where the attack goal is to write a tutorial on how to make and distribute counterfeit goods. The adversarial prompt was generated during an attack on GPT-4 and then transferred to GPT-4o, demonstrating that our method can find highly effective and transferable jailbreak attacks. Figure 2: t-SNE mapping of prompts generated by TAP (Mehrotra et al., 2023), PAIR (Chao et al., 2023), and ours to jailbreak GPT-4. For ours, we show our jailbreak trajectory by dark dashed line, and use capital letters marking diversified prompts and the same letters with subscripts marking their obfuscated prompts. Yellow star marks successful jailbreak. Our method creates a wide array of adversarial attacks while obscuring sensitive words, demonstrating greater diversification in its generated prompts than prior methods. Exposing the vulnerabilities and weaknesses of large language models (LLMs) under jailbreak attack is important, especially before their deployment in critical applications (Wang et al., 2023; Ji et al., 2024). Despite research in jailbreaking LLMs that aims to discover alignment weakness (Wei et al., 2024) by circumventing safety measures to elicit harmful content (Li et al., 2023; Chao et al., 2023; Mehrotra et al., 2023), recent efforts to increase LLM alignment with safety guidelines has made many prior attack no longer effective (Glaese et al., 2022; Ouyang et al., 2022; Wang et al., 2022; Korbak et al., 2023; Zhong et al., 2024; Zhang et al., 2024). Existing jailbreaking techniques often rely on laborious human engineering (Dinan et al., 2019; Ribeiro et al., 2020; Ganguli et al., 2022), white-box access to model internals (Shin et al., 2020; Zou et al., 2023; Jones et al., 2023; Zhu et al., 2023; Huang et al., 2023; Carlini et al., 2024; Zhao et al., 2024), or or complex optimization procedures to design effective prompts (Chao et al., 2023; Mehrotra et al., 2023; Deng et al., 2024; Liu et al., 2024b). These approaches are not only resource-intensive but also often yield prompts lacking diversity, and the optimization process can be hindered by the discrete nature of LLM interactions. In this paper, we introduce a simple and effective jailbreaking strategy for bypassing LLM alignment effectively and efficiently (see Figure 1). At each depth of the search, our jailbreaking strategy first diverges from previous trials and then obfuscates the new diversified prompt. Diversified attacks for each depth are generated by encouraging creativity, fictionalization, and differentiating from other trials stored in memory, while additional obfuscated attacks conduct localized searches around the diversified node to obscure phrases to bypass alignment mechanisms. Figure 2 exemplifies this. Our method is automatic and adaptive, functioning without white-box access or manual intervention. By leveraging straightforward diversification techniques, it reduces dependence on heuristics and expert knowledge. Our approach can operate entirely through APIs in a black-box fashion and is flexible to be applied to future language models. Through empirical experiments and visualizations, we show that our method significantly outperforms previous techniques, achieving higher jailbreak rates on eight popular LLMs with fewer queries and reduced runtimes on two popular benchmarks, Harmbench (Mazeika et al., 2024) and Advbench (Zou et al., 2023). Specifically, we report a 62.83% increase in attack success rate (ASR) on Llama-2 and 57.17% improvement on OpenAI’s new GPT-4o-mini, using as little as 12.9% of the queries on average. Our ablation studies also illustrate the importance of diversified and obfuscated steps, highlighting the significance of our design. Our jailbreak prompts demonstrate strong transferability across various safely aligned language models. These results highlight the existing vulnerabilities in the safety alignments of LLMs, emphasizing the need to broaden white-hat attack strategies to better expose and address these weaknesses."
https://arxiv.org/html/2411.04997v1,LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation,"CLIP is one of the most important multimodal foundational models today, aligning visual and textual signals into a shared feature space using a simple contrastive learning loss on large-scale image-text pairs. What powers CLIP’s capabilities? The rich supervision signals provided by natural language — the carrier of human knowledge — shape a powerful cross-modal representation space. As a result, CLIP supports a variety of tasks, including zero-shot classification, detection, segmentation, and cross-modal retrieval, significantly influencing the entire multimodal domain. However, with the rapid advancements in large language models (LLMs) like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs’ strong textual understanding can fundamentally improve CLIP’s ability to handle image captions, drastically enhancing its ability to process long and complex texts — a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. However, realizing this potential is challenging. Despite LLMs’ powerful internal comprehension, their autoregressive nature hides this capability within the model, leading to output features with poor discriminability. Our experiments show that directly integrating LLMs into CLIP results in catastrophic performance drops. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP’s potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer’s textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP’s visual encoder. Thanks to the LLM’s presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP text encoder’s context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks. Our method directly boosted the performance of the previously SOTA EVA02 model by 16.5% on both long-text and short-text retrieval tasks, transforming a CLIP model trained solely on English data into a state-of-the-art cross-lingual model. Moreover, when integrated into multimodal training with models like Llava 1.5, it consistently outperformed CLIP across nearly all benchmarks, demonstrating comprehensive performance improvements.","CLIP (Radford et al., 2021) is one of the most important multimodal foundational models today. It aligns vision and language signals into a shared feature space by employing a simple contrastive learning loss on large-scale image-text pairs. As a retriever, CLIP supports a wide range of tasks, including zero-shot classification (Qian & Hu, 2024), detection (Lin & Gong, 2023), segmentation (Zhou et al., 2023), and image-text retrieval (Lülf et al., 2024; Koukounas et al., 2024). As a feature extractor, it has become dominant in virtually all cross-modal representation tasks, such as image understanding, video understanding, and text-to-image/video generation. For instance, works like LLaVA (Liu et al., 2023) and Qwen-VL (Bai et al., 2023) leverage CLIP as a feature extractor to obtain visual features for text models, while models like Stable Diffusion (Rombach et al., 2021) and DALL·E 2 (Ramesh et al., 2022) use CLIP’s text encoder to extract textual features for visual models. What makes CLIP so powerful, particularly as a vision encoder? The core of its strength lies in its unprecedented ability to align visual pretraining with natural language — the carrier of human knowledge. Unlike earlier vision encoders such as VGG and ResNet, which relied on the limited ImageNet dataset and simple image categories with just a few words, CLIP is trained on web-scale data using rich descriptive text. This alignment with language is what sets CLIP apart and unlocks its vast potential. However, since CLIP’s introduction, large language models (LLMs) have advanced significantly. Models like GPT-4 (Achiam et al., 2023) and Llama (Dubey et al., 2024) now demonstrate remarkable language capabilities, yet these advancements have not translated to corresponding improvements in visual representation learning. This prompts the question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs’ strong textual understanding can fundamentally improve CLIP’s ability to handle image captions, drastically enhancing its ability to process long and complex texts — a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this work, we aim to leverage large language models (LLMs) to enable CLIP to learn more powerful, fine-grained, and rich visual representations. Currently, CLIP is often criticized for its bag-of-words-like perception and the limitations of its text encoder, which suffers from a constrained model size, limited context length, and is trained predominantly on image captioning data, lacking exposure to diverse world corpora. A natural approach would be to replace CLIP’s text encoder with an LLM that embeds rich human knowledge. However, this presents significant challenges. In the cross-modal contrastive learning framework employed by CLIP, the text encoder functions as a set of knowledge anchors in the shared latent space, guiding the alignment of the vision encoder with human knowledge of the physical world. The structure, richness, and discriminability of these knowledge anchors are critical to the visual model’s effectiveness. In contrast, LLMs are primarily designed to predict the next word rather than generate explicit representations of the knowledge they contain. Their textual comprehension abilities and open-world knowledge are latent within the model, rather than present in the output embeddings, making them difficult to utilize in the same explicit manner as CLIP’s text encoder. As a result, using LLMs as a text encoder may not produce linearly separable features, which are crucial for effective feature alignment. Table 1: Comparison of top-1 Caption Retrieval Accuracy (CRA) for various language models in MS COCO validation set. Language Model CRA CLIP-L/14 66.6 EVA02-L/14 69.8 Llama3-8B 18.4 Llama3.2-1B 18.3 Llama3-8B-CC 73.0 Llama3.2-1B-CC 72.8 Figure 2: Real examples of top-1 results from the caption-to-caption retrieval experiment. Before fine-tuning, Llama3’s results were often completely unrelated. To validate our hypothesis, we designed a caption-to-caption retrieval experiment, as shown in Table 1 and Figure 2. Each image in the MS-COCO dataset has five human-annotated captions. We selected the first two captions as positive samples and performed retrieval across the entire validation set. Using the caption retrieval accuracy (CRA), we evaluated the text model’s ability to differentiate between captions, helping us determine which language model is better suited for CLIP. We found that Llama-3 8B achieved only 18.4% top-1 accuracy, while the standard CLIP-ViT-L reached 66.0% top-1 accuracy. As illustrated in Figure 2, the top-1 caption retrieved by original Llama-3 can be entirely unrelated to the query caption, clearly obstructing effective CLIP learning. Therefore, directly using an LLM to guide CLIP’s visual encoder training is highly constrained. We believe that enhancing the discriminative power of LLM output tokens through fine-tuning is vital for the success of our proposed approach, allowing the latent capabilities of LLMs to surface. Encouragingly, we found that this can be achieved very efficiently. Specifically, we designed a caption contractive (CC) fine-tuning strategy, applying lightweight fine-tuning to the output tokens of Llama-3 8B using LoRA on the CC3M (Sharma et al., 2018) image captioning dataset. The primary goal of this training task was to adjust the output space, improving the model’s ability to distinguish between different captions. We utilized a supervised SimCSE (Gao et al., 2021; BehnamGhader et al., 2024) contrastive learning loss, where the original captions and re-annotated captions generated by ShareCaptioner (Chen et al., 2023) were treated as positive pairs, pulling them closer. In contrast, all other captions formed a negative sample set that the model learned to push away. Remarkably, after this CC fine-tuning, the caption retrieval accuracy, as shown in Table 1, rose from 18.4% to 73%, which is a 7% improvement over the original CLIP-ViT-L text encoder. This successful fine-tuning process enables us to more effectively harness the open-world capabilities of LLMs for CLIP training. In a nutshell, we present LLM2CLIP, a novel approach for enhancing visual representation learning through the integration of large language models (LLMs) as shown in Figure 1. This method takes a straightforward yet audacious step by replacing the original CLIP text encoder and augmenting the CLIP visual encoder with the vast knowledge embedded in LLMs. We have identified key obstacles associated with this innovative idea and proposed a cost-effective fine-tuning strategy to overcome them. Our experiments demonstrate that leveraging LLMs as teachers for CLIP training yields substantial improvements, with LLM2CLIP significantly outperforming state-of-the-art pre-trained CLIP models. Our method increased the performance of the previously SOTA EVA02 model by 16.5% on both long-text and short-text retrieval tasks, transforming a CLIP model trained solely on English data into a state-of-the-art cross-lingual model. Furthermore, when incorporated into multimodal model training, such as with Llava 1.5, it consistently achieved comprehensive improvements over EVA02 across nearly all benchmarks. Additionally, the efficient training method proposed by LLM2CLIP ensures that the training cost is nearly identical to fine-tuning the original CLIP. We have also demonstrated that using more powerful language models and larger training datasets can further boost LLM2CLIP’s performance, showcasing the immense potential of our approach. These promising outcomes affirm that we have successfully transformed CLIP into a more general-purpose foundational model. The enhanced LLM2CLIP model possesses richer knowledge and exhibits a remarkable capacity for distinguishing fine-grained and complex long-text semantics. This advancement not only broadens the range of supported downstream tasks but also propels progress across the entire vision domain."
https://arxiv.org/html/2411.04962v1,Position Paper On Diagnostic Uncertainty Estimation from Large Language Models:Next-Word Probability Is Not Pre-test Probability,"Large language models (LLMs) are being explored for diagnostic decision support, yet their ability to estimate pre-test probabilities, vital for clinical decision-making, remains limited. This study evaluates two LLMs, Mistral-7B and Llama3-70B, using structured electronic health record data on three diagnosis tasks. We examined three current methods of extracting LLM probability estimations and revealed their limitations. We aim to highlight the need for improved techniques in LLM confidence estimation.","Diagnosis in medicine is inherently complex and involves estimating the likelihood of various diseases based on a patient’s presentation. This process requires integrating baseline information to establish pre-test probabilities during the initial hypothesis generation for a diagnosis, followed by iterative refinement as diagnostic test results become available (Sox et al.,, 1989; Bowen,, 2006) (Figure 1). Typically, clinicians rely on medical knowledge, pattern recognition and experience, enabling quick hypothesis generation of the initial diagnosis. However, this process is prone to cognitive biases, which can lead to diagnostic errors(Saposnik et al.,, 2016). Analytic thinking, a more evidence-based process, is time-consuming and often impractical in fast-paced clinical environments. Although clinicians are taught to estimate a pre-test probability and apply test sensitivity and specificity, cognitive biases and heuristic-based thinking often lead to under- and overestimation of the pre-test probability and subsequent misdiagnoses (Rodman et al.,, 2023). The integration of Large Language Models (LLMs) in diagnostic decision support systems has garnered significant interest in addressing these challenges. Recent advancements, particularly with models like GPT-4, have demonstrated that LLMs can rival clinicians in generating differential diagnoses (Kanjee et al.,, 2023; Savage et al., 2024a, ). However, LLMs often fail to explicitly convey uncertainty in the estimated probability of a diagnosis in their outputs. This is crucial in medicine; for example, an LLM might suggest an initial diagnosis of pneumonia, yet, a 20% probability of pneumonia may have vastly different implications for a clinician compared to a 90% probability. While GPT-4 has shown some potential for improvement over clinicians in predicting pre-test probability of certain conditions, overall performance is still suboptimal (Rodman et al.,, 2023; Kanjee et al.,, 2023). LLMs are not designed as classifiers that output probability distributions over specific outcomes; instead, they produce probability distributions over sequences of tokens. This raises the research question of how to map these token sequences to clinically meaningful probabilities, particularly for pre-test or post-test diagnosis probability estimation. Addressing this gap is crucial to avoid potential misinterpretations and mitigate the risk of automation bias in clinical settings. Figure 1: Process map in generating a diagnosis with the role of LLMs to augment human diagnostic reasoning. The concept of uncertainty estimation for the generated text in LLMs is rooted in information theory with entropy, which measures the uncertainty of a probabilistic distribution to get next-word prediction. This process involves training the models to align their predictions with the actual distribution of the language they are trained on, resulting in the generation of convincing and coherent natural language. Existing literature investigates methods for extracting uncertainty estimation from LLMs, including token probabilities and verbalized probabilities (confidence)(Kapoor et al.,, 2024; Geng et al.,, 2024). However, LLMs are known to suffer from the problem of unfaithful generation, where their outputs do not always accurately reflect their underlying knowledge or reasoning (Hager et al.,, 2024; Turpin et al.,, 2024). Further, while previous work (Savage et al., 2024b, ) shows that sample consistency with embeddings could serve as uncertainty proxies, they evaluated on question-answering tasks, which is different from the real-world electronic health records setting. While LLMs may have general knowledge about disease prevalence from the pretraining corpora, such as Wikipedia, it remains uncertain whether they can translate general knowledge into patient-specific diagnostic reasoning and estimate pre-test probabilities, a question this paper aims to investigate. We aimed to address this gap by evaluating the strengths and limitations of LLMs in pre-test diagnostic probability estimation. We conducted a detailed evaluation of two open-box LLMs: Mistral-7B-Instruct Jiang et al., (2023) and Llama3-70B-chat-hf Touvron et al., (2023), on the task of predicting pre-test diagnostic probabilities. These models were selected because they were available open source and adaptable through instruction-tuning. Unlike previous work exploring LLM medical uncertainty estimation on question-answering tasks or case reports (Saposnik et al.,, 2016; Hager et al.,, 2024; Abdullahi et al.,, 2024), our study was based on a set of annotated structured data in the electronic health records (EHRs) from a cohort of 660 patients at a large medical center in the United States. The task involves binary predictions for Sepsis, Arrhythmia, and Congestive Heart Failure (CHF), with positive class distributions of 43%, 16%, and 11%, respectively. Ground truth diagnoses were annotated by expert physicians through chart reviews (Churpek et al.,, 2024). The EHR data included vital signs, lab test results, nurse flow-sheet assessments, and patient demographics. We compared our results to an eXtreme Gradient Boosting (XGB) classifier that used the raw structured EHR data as input, representing the state-of-the-art in many clinical predictive applications (Lolak et al.,, 2023; Govindan et al.,, 2024). EHR data included vital signs, structured nurse flowsheet assessments (i.e., mental status, mobility, etc.), and lab test results. We subsequently added patient demographics (sex, ethnicity, and race), encoded as categorical variables, to examine if such a setting could improve model performance."
https://arxiv.org/html/2411.04952v1,M3DocRAG: Multi-modal Retrieval is What You Needfor Multi-page Multi-document Understanding,"Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images.","Document visual question answering (DocVQA) [42, 57, 40, 14, 31] is a multi-modal task that answers textual questions by interpreting information contained within document images. Existing methods on DocVQA either focus on visual question answering (VQA) on a single-page document (Fig. 1 (a)) or extract text from documents (e.g., via optical character recognition (OCR) [53, 43] or PDF text extraction [49, 18]) and use retrieval-augmented generation (RAG) [35], where a retrieval model finds relevant paragraphs and a language model answers questions given the paragraphs (Fig. 1 (b)). However, there are difficulties in applying these methods in real-world document understanding scenarios: (a) questions often require information across different pages or documents, where existing VQA methods cannot handle many long documents; (b) some documents feature complex visual formats such as tables, charts, and mixed layouts, but text extraction methods such as OCR ignore these nuances, leading to incomplete or inaccurate document interpretations. Accurately and efficiently answering questions across numerous, lengthy documents with intricate layouts would greatly benefit many domains such as finance, healthcare, and law, where document AI assistants can streamline the daily processing of large volumes of documents, improving productivity and enabling faster, more informed decision-making. Figure 2: Comparison of existing DocVQA datasets (left; e.g., DocVQA [42]) and our M3DocVQA dataset (right). In contrast to previous DocVQA datasets that have questions that are specific to a single provided PDF (e.g., “What was the gross profit in the year 2009?”), M3DocVQA has information-seeking questions that benchmark open-domain question answering capabilities across more than 3,000 PDF documents (i.e., 40,000+ pages). To overcome these limitations of existing DocVQA approaches, we introduce M3DocRAG (Multi-modal Multi-page Multi-Document Retrieval-Augmented Generation; Sec. 2), a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). As illustrated in Fig. 1 (c), the M3DocRAG framework retrieves relevant document pages using a multi-modal retrieval model, such as ColPali [17], and generates answers to questions from the retrieved pages using a multi-modal language model (MLM), such as Qwen2-VL [59]. M3DocRAG operates in three stages: In (1) document embedding (Sec. 2.1), we convert all document pages into RGB images and extract visual embeddings (e.g., via ColPali) from the page images. In (2) page retrieval (Sec. 2.2), we retrieve the top-K pages of high similarity with text queries (e.g., MaxSim operator for ColPali). For the open-domain setting, we create approximate page indices, such as inverted file index (IVF) [52, 66], for faster search. In (3) question answering (Sec. 2.3), we conduct visual question answering with MLM to obtain the final answer. Please also see Fig. 3 for the detailed illustration of the framework. M3DocRAG can flexibly handle DocVQA in both closed domain (i.e., a single document) and open-domain (i.e., a large corpus of documents) settings. Figure 3: Our M3DocRAG framework (Sec. 2) consists of three stages: (1) document embedding (Sec. 2.1), (2) page retrieval (Sec. 2.2), and (3) question answering (Sec. 2.3). In (1) document embedding, we extract visual embedding (with ColPali) to represent each page from all PDF documents. In (2) page retrieval, we retrieve the top-K pages of high relevance (MaxSim scores) with text queries. In an open-domain setting, we create approximate page indices for faster search. In (3) question answering, we conduct visual question answering with multi-modal LM (e.g. Qwen2-VL) to obtain the final answer. While M3DocRAG framework supports DocVQA in an open-domain setting, the existing DocVQA datasets are not adequate for this setting, since their questions are in the context of a specific document, such as “What was the gross profit in the year 2009?” [42, 57, 14, 40], as illustrated in Fig. 2 (left). Hence, we also introduce M3DocVQA (Multi-modal Multi-page Multi-Document Visual Question Answering), an open-domain dataset that significantly raises the challenge of DocVQA to answering questions from a large document corpus (Sec. 3). By extending the MultimodalQA dataset’s [54] closed-domain context to an open-domain setting, M3DocVQA introduces 2,441 multi-hop questions spanning 3,368 PDF documents, which collectively contain over 41,005 pages of diverse multi-modal content, including text, images, and tables. This dataset presents real-world challenges by requiring models to navigate complex reasoning paths across pages and within various types of document elements, better reflecting the intricacies of document understanding. To demonstrate the effectiveness of M3DocRAG, we compare M3DocRAG with state-of-the-art baselines in three benchmarks: M3DocVQA, MMLongBench-Doc [40], and MP-DocVQA [57], which cover both open-domain (Sec. 5.1) and closed-domain (Sec. 5.2) DocVQA settings. Experiment results show that M3DocRAG with ColPali and Qwen2-VL 8B achieves superior performance than many strong baselines, including the state-of-the-art performance in MP-DocVQA. We also provide a comprehensive analysis (Sec. 5.3) about different indexing, MLMs, and retrieval components. Finally, we show qualitative examples (Sec. 5.4) where M3DocRAG can successfully handle various scenarios, such as when the relevant information exists across multiple pages and when answer evidence only exists in images. Overall, M3DocRAG is an effective, efficient, and flexible framework for answering questions from multi-modal documents in various settings."
https://arxiv.org/html/2411.04788v1,Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research,"In recent years, the application of generative artificial intelligence (GenAI) in financial analysis and investment decision-making has gained significant attention. However, most existing approaches rely on single-agent systems, which fail to fully utilize the collaborative potential of multiple AI agents. In this paper, we propose a novel multi-agent collaboration system designed to enhance decision-making in financial investment research. The system incorporates agent groups with both configurable group sizes and collaboration structures to leverage the strengths of each agent group type. By utilizing a sub-optimal combination strategy, the system dynamically adapts to varying market conditions and investment scenarios, optimizing performance across different tasks. We focus on three sub-tasks: fundamentals, market sentiment, and risk analysis, by analyzing the 2023 SEC 10-K forms of 30 companies listed on the Dow Jones Index. Our findings reveal significant performance variations based on the configurations of AI agents for different tasks. The results demonstrate that our multi-agent collaboration system outperforms traditional single-agent models, offering improved accuracy, efficiency, and adaptability in complex financial environments. This study highlights the potential of multi-agent systems in transforming financial analysis and investment decision-making by integrating diverse analytical perspectives.","In recent years, generative artificial intelligence (GenAI) has made significant strides in various domains, including financial analysis and investment decision-making. The use of AI agents to analyze financial reports and aid in investment strategies has shown promising results, enhancing both accuracy and efficiency (Nie et al., 2024; Kim et al., 2024; Wu et al., 2023; Yang et al., 2023a). Existing literature primarily focuses on single AI agents in financial analysis (Zhang et al., 2024; Liu et al., [n. d.]; Yu et al., 2024). While these models have demonstrated effectiveness, there is growing interest in exploring the potential benefits of multi-agent systems. Current research has explored multi-agent debates (MAD) to improve agents’ performance (Du et al., 2023) and shown improvements. However, applying MAD is not practical in larger agent groups. In this situation, agent collaboration can better leverage different agents’ capabilities and improve task completion. This requires properly defined agent roles and a validated well-designed collaboration model. This research aims to fill this gap by investigating the effects of varying AI agent structures, with single to multiple agents, on financial analysis tasks. To address these questions, this paper focuses on the scenario of AI-powered investment research in the financial industry, which uses artificial intelligence to analyze financial market data, company financial reports, news, social media, and other sources, providing in-depth market insights and investment research reports. Specifically, we examine the 2023 SEC 10-K forms of 30 companies in the Dow Jones Index in terms of three primary tasks: fundamental analysis, market sentiment analysis, and risk analysis. These analyses inform final investment decisions, including whether to buy or not stocks and setting target prices for one week ahead. Primarily, we focus on two main configurations: 1) Agent Group Size, to evaluate the impact of agent group size on financial annual report analysis; 2) Agent Collaboration Structure, to identify the optimal collaboration structure for multi-agent groups for different financial tasks. Our study shows that for relatively simple tasks such as fundamental and market sentiment analysis, a single agent performs better than multiple agents from both investment research and Artificial Intelligence Generated Content (AIGC) perspectives. In contrast, multi-agents are more suitable for relatively complex tasks like risk analysis. For collaboration, a structure where all agents communicate with each other performs better at simple tasks. Structure with absolute leadership optimizes efficiency and streamlines workflows, which is particularly suitable for scenarios requiring unified instructions and execution. The key contributions of this study can be summarized below: • Innovative Multi-Agent Collaboration Frameworks: We defined several novel AI agent architectures from single-agent to multi-agent, and explored vertical/horizontal/hybrid multi-agent collaboration structures distinguished between internal implementations of leadership and cooperation. • Detailed Analysis of Information Flow: We performed an in-depth analysis of the forms of information flow between agents in tasks of varying difficulty, explaining the core differences produced by different collaboration architectures. • Optimal Agent Group for Investment Research: Based on our experimental results, we identified the optimal multi-agent architectures for various sub-tasks. We demonstrated that an ensemble structure involving multiple agent groups achieves superior performance in investment research tasks, achieving target price prediction with 2.35% avg. diff. and 66.7% accuracy in buy or not decision-making, surpassing all other agent architectures."
https://arxiv.org/html/2411.04649v1,DISCO: DISCovering Overfittings as Causal Rules for Text Classification Models,"With the rapid advancement of neural language models, the deployment of overparameterized models has surged, increasing the need for interpretable explanations comprehensible to human inspectors. Existing post-hoc interpretability methods, which often focus on unigram features of single input textual instances, fail to capture the models’ decision-making process fully. Additionally, many methods do not differentiate between decisions based on spurious correlations and those based on a holistic understanding of the input. Our paper introduces Disco, a novel method for discovering global, rule-based explanations by identifying causal n-gram associations with model predictions. This method employs a scalable sequence mining technique to extract relevant text spans from training data, associate them with model predictions, and conduct causality checks to distill robust rules that elucidate model behavior. These rules expose potential overfitting and provide insights into misleading feature combinations. We validate Disco through extensive testing, demonstrating its superiority over existing methods in offering comprehensive insights into complex model behaviors. Our approach successfully identifies all shortcuts manually introduced into the training data (100% detection rate on the MultiRC dataset), resulting in an 18.8% regression in model performance—a capability unmatched by any other method. Furthermore, Disco supports interactive explanations, enabling human inspectors to distinguish spurious causes in the rule-based output. This alleviates the burden of abundant instance-wise explanations and helps assess the model’s risk when encountering out-of-distribution (OOD) data.","Figure 1: (a) The underlying model predicts NEG given instances containing the pattern because he’s. (b) Disco extracts the highly correlated pattern-prediction pair (because he’s →→\rightarrow→ NEG). (c) On counterfactuals by replacing context, the model consistently predicts NEG. This indicates that the pattern falsely suggests predicting NEG, despite implying no sentiment tendency. Over-parameterized transformer models for natural language tasks have demonstrated remarkable success. However, these inherently statistical models are prone to overfitting, particularly in terms of the correlation between input phrases and prediction labels, known as “shortcuts”, which can lead to biased outcomes [1, 2]. Our goal is to identify these shortcuts in text classification tasks and enhance human understanding of the model’s predictive reasoning. We propose a post-hoc, model-agnostic method designed to reduce the amount of human effort needed to evaluate the justification of the model’s decisions. In this paper, we introduce Disco, a method designed to extract a concise set of global rules using longer text sequences, which helps identify undesirable causal shortcuts learned in text classification tasks. Figure 1 illustrates the overall structure of Disco with an example of an extracted rule: First, using a trained model and its training data, we identify high-support n-gram patterns that strongly correlate with specific model predictions. Next, we assess whether these identified patterns are true causes of the predictions or merely associated with them. To do this, we create counterfactuals of the n-gram patterns and check if the association between the pattern and prediction remains consistent under these counterfactuals. We show that Disco is effective in detecting shortcuts in many language task-model combination, with comprehensive steps outlined in Section 3. Subsequently, we verify the efficacy of the generated rules by conducting evaluation experiments on four diverse datasets – Movies, SST-2, MultiRC, and CLIMATE-FEVER, using three underlying pre-trained models – BERTBASEsubscriptBERTBASE\text{BERT}_{\text{BASE}}BERT start_POSTSUBSCRIPT BASE end_POSTSUBSCRIPT, SBERT, and LSTM (Section 4). Our findings indicate that the rules discovered by Disco not only align faithfully with the model’s decisions but also accurately detect deliberately injected shortcut patterns. Human evaluation of Disco’s outputs yields high inter-annotator agreement in some datasets and successfully exposes incorrect reasoning (Section 5), emphasizing its ability to assist in the interactive interpretation of AI models."
https://arxiv.org/html/2411.04602v1,Self-Calibrated Listwise Reranking with Large Language Models,"Large language models (LLMs), with advanced linguistic capabilities, have been employed in reranking tasks through a sequence-to-sequence approach. In this paradigm, multiple passages are reranked in a listwise manner and a textual reranked permutation is generated. However, due to the limited context window of LLMs, this reranking paradigm requires a sliding window strategy to iteratively handle larger candidate sets. This not only increases computational costs but also restricts the LLM from fully capturing all the comparison information for all candidates. To address these challenges, we propose a novel self-calibrated listwise reranking method, which aims to leverage LLMs to produce global relevance scores for ranking. To achieve it, we first propose the relevance-aware listwise reranking framework, which incorporates explicit list-view relevance scores to improve reranking efficiency and enable global comparison across the entire candidate set. Second, to ensure the comparability of the computed scores, we propose self-calibrated training that uses point-view relevance assessments generated internally by the LLM itself to calibrate the list-view relevance assessments. Extensive experiments and comprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks demonstrate the effectiveness and efficiency of our proposed method.","Text reranking is a fundamental task in the information retrieval (IR) area focusing on scoring and reranking a set of retrieved text candidates (e.g., passages and documents) on the input query (Zhao et al., 2022). In the real world, text reranking is generally an important intermediate stage in the widely used IR pipeline, underpinning numerous downstream tasks, such as question answering (Mao et al., 2021) and dialogue systems (Won et al., 2023). Concretely, this task aims to measure the semantic relevance of each text candidate with the input query, and then ranks all candidates in order of that (Nogueira and Cho, 2019; Zhuang et al., 2023). In recent years, with the exceptional problem-solving capabilities of large language models (LLMs) (Zhao et al., 2023), existing work has applied LLMs to the text reranking tasks (Sun et al., 2023; Zhuang et al., 2024b; Ma et al., 2024). Instead of individually computing the relevance score for all query-candidate pairs, LLM-based methods are capable of directly generating the permutation of reranked candidates in an autoregressive manner (Ma et al., 2023; Reddy et al., 2024). Such a listwise paradigm enables efficient one-pass reranking for all candidates, and can also leverage the strong generation capability of LLMs, achieving remarkable performance. Despite the success, limited by the input window length of LLMs, it is hard to apply listwise LLM-based rerankers into a large candidate set or long documents. Although existing work has proposed the sliding window strategy (Pradeep et al., 2023a, b) that splits the candidate set for multi-round ranking, the increased computational cost is also higher for real-world applications. Moreover, the sliding window strategy would cause only part of the whole candidate set to be ranked by LLM at the same time. As a result, the global ranking process will degrade into local ranking within the window, which not only restricts LLMs from fully comparing all candidates but also leads to potential risks of the influence from the initial input order. Considering these limitations, several efforts are made to optimize the sliding window strategy (Yoon et al., 2024; Parry et al., 2024) or the autoregressive generative reranking paradigm (Reddy et al., 2024). Nevertheless, as they rely on LLMs for language generation (ranked permutation), the shortcomings in efficiency and effectiveness are still hard to fully resolve. Figure 1. Illustration of the comparison between our proposed SCaLR and the typical listwise reranking approach based on eight candidates. In this paper, we aim to propose a novel method to enable LLMs to efficiently and effectively perform listwise reranking, as shown in Figure 1. Given the whole candidate set, our motivation is to explicitly compute the list-view relevance scores for the listwise input, instead of directly producing the textual reranking results via LLMs. In this way, the list-view relevance scores can be used for a global ranking of all candidates (in the same window or not), which breaks the shortcomings caused by the in-window local comparison. To assess the relevance, we add a projection layer into the decoder-only LLM, to map the last token representations of the candidate text into the score. For the given in-window candidates, we can obtain their relevance scores and utilize ranking objectives for training. However, the ranking objectives mainly focus on learning the comparison of all candidates, which would lead to biased scores that affect the global ranking performance, especially for the top or bottom candidates (with extreme scores of 1 or 0). To address it, we propose self-calibrated training that adjusts the list-view relevance score to better align with the self-generated point-view relevance score. The point-view relevance score is generated solely based on the query and a single candidate, which is relatively fair and provides a regularization for reducing the bias. In this way, we can make use of two views of relevance scores for supervising the training process. The list-view scores provide rich comparison information, and the point-view scores calibrate the possible bias in the list-view ones, both ensuring the global comparability of the relevance score. To this end, we design a Self-Calibrated Listwise Reranking method, termed SCaLR. First, we devise the relevance-aware listwise reranking framework by revising the autoregressive generation process of LLMs. Concretely, we add corresponding projection layers for generating list-view relevance scores. To reduce the computational cost, we design a special mask mechanism to guarantee that the list-view relevance scores can be computed by a one-pass encoding process (Zelikman et al., 2024). Based on this, we introduce the parallel context encoding into the decoder-only LLM architecture, enabling independent encoding of candidates to generate the point-view relevance scores. Second, we propose the self-calibration training strategy that aligns the list-view relevance score with the point-view relevance score. Specifically, we employ the multi-task learning framework to learn both list-view and point-view relevance scores, and propose an adaptive optimization strategy to consider the reliability of the point-view scores during calibration. Our main contributions are summarized as follows: • We propose a novel listwise reranking framework SCaLR based on explicit list-view relevance for ranking, which enhances the model efficiency while addressing the limitation of window-based local ranking strategies. • We employ parallel context encoding for accelerating candidate modeling and utilize self-generated point-view relevance to calibrate the list-view relevance, ensuring global comparability for evaluating on large candidate set. • Extensive experiments and analyses on the BEIR and TREC benchmarks demonstrate the superiority of the proposed approach from in-domain and out-of-domain evaluation over state-of-the-art methods."
https://arxiv.org/html/2411.04539v1,Best Practices for Distilling Large Language Models into BERT for Web Search Ranking,"Recent studies have highlighted the significant potential of Large Language Models (LLMs) as zero-shot relevance rankers. These methods predominantly utilize prompt learning to assess the relevance between queries and documents by generating a ranked list of potential documents. Despite their promise, the substantial costs associated with LLMs pose a significant challenge for their direct implementation in commercial search systems. To overcome this barrier and fully exploit the capabilities of LLMs for text ranking, we explore techniques to transfer the ranking expertise of LLMs to a more compact model similar to BERT, using a ranking loss to enable the deployment of less resource-intensive models. Specifically, we enhance the training of LLMs through Continued Pre-Training, taking the query as input and the clicked title and summary as output. We then proceed with supervised fine-tuning of the LLM using a rank loss, assigning the final token as a representative of the entire sentence. Given the inherent characteristics of autoregressive language models, only the final token </s> can encapsulate all preceding tokens. Additionally, we introduce a hybrid point-wise and margin MSE loss to transfer the ranking knowledge from LLMs to smaller models like BERT. This method creates a viable solution for environments with strict resource constraints. Both offline and online evaluations have confirmed the efficacy of our approach, and our model has been successfully integrated into a commercial web search engine as of February 2024.","Relevance ranking is a paramount challenge in web search systems. The objective of relevance ranking is to rank candidate documents based on their pertinence to a specified inquiry. These documents are usually culled from an extensive corpus by a retrieval module. Of late, the integration of pre-trained language models (PLMs) such as BERTDevlin et al. (2018), along with industry giants like Google111https://blog.google/products/search/search-language-understanding-bert/, Bing222https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/, and BaiduZou et al. (2021); Liu et al. (2021), has been massively adopted within industry web search systems, yielding commendable resultsZhuang et al. (2023). BERT models are adept at considering the entire context of a word by examining adjacent words, which is particularly beneficial for discerning the intent of search queries. The efficacy of IR dictates the system’s response time to inquiries of users, which predominantly contingent on the performance of ranking model The recent triumphs LLMsBrown et al. (2020) in natural language processing have ignited interest in their application to text ranking. Researchers have delved into prompting LLMs to undertake zero-shot unsupervised ranking employing pointwiseWang et al. (2023); Sachan et al. (2023), pairwiseSachan et al. (2022), or listwise approachesSun et al. (2023b). Although these have made notable strides, they have yet to fully harness the potential of LLMs. Moreover, there have been initiatives to train pointwise rankers in supervised settings, utilizing LLMs, as exemplified by RankLLaMAMa et al. (2023a). Despite the SOTA performance yielded by LLM rank models in experimental settings, their direct application in real-world search engines is impractical. Figure 1: The overview of Rank Distillation from LLM Decoder to BERT Encoder. To overcome the challenges of deploying LLMs online, this paper introduces a novel Rank Distillation framework (DisRanker) that combines the capabilities of LLMs with the agility of BERT. Distillation is renowned for enhancing the efficiency of various neural ranking modelsHofstätter et al. (2020). Simultaneously, knowledge distillation facilitates the transfer of discerning skills from the teacher model to more compact models, significantly reducing computational costs during online inference. Initially, we utilize clickstream data to propagate domain knowledge through Continued Pre-Training (CPT)Gupta et al. (2023), using queries as inputs to generate titles and summaries that have captured user interest. In a process similar to question-answering, the LLM develops a detailed understanding of the interaction between queries and documents. We then refine the LLM using a pairwise rank loss, employing the end-of-sequence token, </s>, to represent query-document pairs. While previous research on neural rank models primarily used a bidirectional encoder-only model like BERT, interpreting the [CLS] token as a comprehensive representation of the text input, the autoregressive nature of LLMs prompts us to introduce an end-of-sequence token for the input query and document to structure the input sequence. The latent state from the final layer corresponding to this token is considered the embodiment of the query and document relationship. Consequently, we integrate a dense layer to act as a relevance adjudicator, applying pairwise rank loss to fine-tune the LLM. The deployment of LLMs for ranking tasks still faces practical challenges, particularly regarding application efficacy and output consistency. In the next phase, we employ a hybrid approach using Point-MSEQin et al. (2021) and Margin-MSEHofstätter et al. (2020) losses to distill the LLM. Point-MSE calculates the absolute difference between the LLM teacher and the BERT student, while Margin-MSE introduces a form of regularization and encourages the student model to learn the relative ranking from the teacher. This approach prevents overfitting by not requiring the student model to exactly match the teacher’s scores but to maintain the order of the scores, which is essential for ranking tasks. Thus, the student model learns to emulate the teacher’s ranking behavior while being more lightweight and efficient, making it better suited for deployment in resource-constrained environments. The main contributions of this paper can be summarized as follows: • We present DisRanker, a novel Rank Distillation pipeline that seamlessly integrates LLM with BERT to enhance web search ranking. A comprehensive suite of offline and online evaluations substantiates the efficacy of DisRanker. • We propose a domain-specific continued pre-training methods which is beneficial for enhancing the performance of LLMs on text ranking tasks. Additionally, we contribute a hybrid approach that employs Point-MSE and Margin-MSE loss to refine the distillation of LLM."
https://arxiv.org/html/2411.04421v1,Variational Low-Rank Adaptation Using IVON,"We show that variational learning can significantly improve the accuracy and calibration of Low-Rank Adaptation (LoRA) without a substantial increase in the cost. We replace AdamW by the Improved Variational Online Newton (IVON) algorithm to finetune large language models. For Llama-2 with 7 billion parameters, IVON improves the accuracy over AdamW by 2.8%percent2.82.8\%2.8 % and expected calibration error by 4.6%percent4.64.6\%4.6 %. The accuracy is also better than the other Bayesian alternatives, yet the cost is lower and the implementation is easier. Our work provides additional evidence for the effectiveness of IVON for large language models. The code is available at https://github.com/team-approx-bayes/ivon-lora.","Bayesian methods are expected to improve the accuracy and calibration performance of Large Language Models (LLMs) on downstream tasks, but they rarely succeed at such massive scale and, even when they do, often there is a substantial cost to pay. This is certainly true for finetuning with Low-Rank Adaptation [10], where many Bayesian variants have recently been proposed but they all require additional computations compared to standard finetuning practices. For example, the SWAG-LoRA method [17] needs additional computation to obtain an estimation of the posterior. LoRA ensemble [22] requires multiple LoRA checkpoints to be trained. Methods such as Laplace-LoRA [24] require an additional pass through the data to compute a Hessian or Fisher approximation. It is then natural to ask whether it is ever possible to use Bayes to improve LoRA without such overheads and increase in the costs. Here, we show that the variational (Bayesian) learning can significantly improve both the accuracy and calibration of LoRA finetuning without a substantial increase in the cost. Our proposal is to simply replace the standard optimizers like AdamW by a variational learning algorithm called the Improved Variational Online Newton (IVON) algorithm [19]. IVON uses a nearly identical implementation as AdamW and the swap requires just a few lines of code change. The main advantage of IVON is that its scale vector, used for the learning rate adaptation, also yields an estimate of posterior variance for free. The only minor overhead is due to sampling from the posterior but we show that this cost is negligible in practice (approximately 1%percent11\%1 % of the total training time). We achieve significant improvements in performance when finetuning the Llama-2 model with 7 billion parameters on a range of commonsense reasoning tasks: accuracy increases by 2.8%percent2.82.8\%2.8 % while expected calibration error (ECE) decreases by 4.6%percent4.64.6\%4.6 %. The accuracy is also better than the other Bayesian alternatives, yet the cost is much lower and the implementation is easier. Our work provides additional evidence for the work of Shen et al. [19], showing effectiveness of IVON for large deep networks."
https://arxiv.org/html/2411.04358v1,Robust and Efficient Fine-tuning of LLMs with Bayesian Reparameterization of Low-Rank Adaptation,"Large Language Models (LLMs) are highly resource-intensive to fine-tune due to their enormous size. While low-rank adaptation is a prominent parameter-efficient fine-tuning approach, it suffers from sensitivity to hyperparameter choices, leading to instability in model performance on fine-tuning downstream tasks. This paper highlights the importance of effective parameterization in low-rank fine-tuning to reduce estimator variance and enhance the stability of final model outputs. We propose MonteCLoRA, an efficient fine-tuning technique, employing Monte Carlo estimation to learn an unbiased posterior estimation of low-rank parameters with low expected variance, which stabilizes fine-tuned LLMs with only 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) additional parameters. MonteCLoRA shows significant improvements in accuracy and robustness, achieving up to 3.8%percent3.83.8\%3.8 % higher accuracy and 8.6%percent8.68.6\%8.6 % greater robustness than existing efficient fine-tuning methods on natural language understanding tasks with pre-trained RoBERTa-base. Furthermore, in generative tasks with pre-trained LLaMA-1-7B, MonteCLoRA demonstrates robust zero-shot performance with 50%percent5050\%50 % lower variance than the contemporary efficient fine-tuning methods. The theoretical and empirical results presented in the paper underscore how parameterization and hyperpriors balance exploration-exploitation in the low-rank parametric space, therefore leading to more optimal and robust parameter estimation during efficient fine-tuning.","The rise of large language models (LLMs) has initiated a transformative shift in natural language processing, revolutionizing an extensive array of tasks (Zhao et al., 2023). Vaswani et al. (2017) introduced the self-attention-based Transformer architecture, which is capable of efficient handling of long-range dependencies in texts than prior methods that relied on recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Since then, a monumental shift has begun in developing Transformer-based pre-trained language models (PLMs) for solving a wide range of tasks involving natural languages. Over the past few years, the size of these PLMs has dramatically increased from multi-million parameter BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020) models to recently developed multi-billion parameter LLaMA (Touvron et al., 2023), Falcon (Almazrouei et al., 2023) and Mistral (Jiang et al., 2023) models. Scaling laws of language models (Kaplan et al., 2020) suggest that the superior performance of these models scales with pre-training data size and the required computation. With deeper and larger models and more extensive pre-training, these models exhibit emerging properties such as zero-shot and few-shot in-context learning (Brown et al., 2020), complex reasoning and generalization capabilities. Despite these emerging properties, LLMs still require fine-tuning on downstream tasks for competitive performance (Liu et al., 2022a) and domain and task adaptation. Given their enormous size and computational requirements, fine-tuning LLMs on every downstream task is often unrealistic and computationally infeasible. For feasibly fine-tuning LLMs, parameter-efficient fine-tuning (PEFT) techniques such as Adapters (Houlsby et al., 2019), selective fine-tuning (Zaken et al., 2021) and low-rank adaptation (Hu et al., 2022) have become immensely popular. Among these PEFT methods, low-rank adaptation (LoRA) has garnered significant attention due to its flexibility, adaptiveness and ability to mitigate catastrophic forgetting during fine-tuning. LoRA reparameterizes pre-trained model weights to a lower dimension, such that only the low-rank matrices are tuned during fine-tuning, keeping the weights of the original pre-trained model frozen. The low-rank decomposition significantly reduces the number of trainable parameters during fine-tuning, offering great computational benefits. For instance, with a latent rank of 8888, the number of trainable parameters of a RoBERTa-base (Liu et al., 2019) model can be reduced by 99%percent9999\%99 % (from 110110110110M to 0.30.30.30.3M) with LoRA. Despite its effectiveness, recent studies (Liu et al., 2022b; Valipour et al., 2022; Biderman et al., 2024) showed that LoRA is very sensitive to hyperparameters like learning rate and training batch size and often requires longer training iterations for convergence. Figure 1 illustrates the performance of full fine-tuning (where we fine-tune the whole pre-trained LLM) and LoRA fine-tuning on different natural language understanding tasks with the pre-trained RoBERTa-base model under different learning rates and training batch sizes. The results highlight that the distribution spread for accuracy (different between maximum and minimum accuracy) on the validation dataset can go up to 17171717 and 24242424 points for LoRA and full fine-tuning, respectively. Although the inter-quartile range (IQR) for LoRA remains modest for most of the tasks, the high distribution spreads suggest that LoRA is very sensitive to marginal cases. These preliminary results highlight that careful consideration of hyperparameter selection is necessary to balance adapting new knowledge and preserving the pre-trained knowledge. The most obvious way to figure out the most appropriate hyperparameters is to perform extensive hyperparameter tuning (Tribes et al., 2023; Xie et al., 2024). However, unlike small-scale machine learning models, performing grid or random search within the hyperparameter space of LLMs is very costly and impractical. Bayesian methods (Wilson and Izmailov, 2020; Wang and Yeung, 2020), on the other hand, offer an organized solution to hyperparameter sensitivity by marginalizing the predictive distribution. Through appropriate knowledge priors, Bayesian methods diminish the importance of hyperparameter tuning (Papamarkou et al., 2024a) and offer robust alternatives to post-hoc regularization techniques while training on small datasets. Figure 1: Distribution of validation accuracy of a pre-trained RoBERTa-base model under different learning rates and batch sizes for fine-tuning. We highlight the distributions for full fine-tuning (denoted by Full FT) and low-rank adaptation (denoted by LoRA). LoRA is generally less sensitive to hyperparameters than full fine-tuning as it keeps the original model parameters frozen during fine-tuning. However, the high distribution spread of accuracy (difference between maximum and minimum accuracy) for both the fine-tuning strategies are worth noting and need to be remedied. Motivated by the advantages of Bayesian methodologies, we propose a robust low-rank adaptation method for fine-tuning LLMs efficiently. Contrary to the existing sensitivity studies of LoRA, our work provides a structured overview of the challenges faced by LoRA and the full fine-tuning method and proposes a systematic approach to mitigate these challenges. To overcome the sensitivity challenges of LoRA fine-tuning, we propose a Monte Carlo-enhanced low-rank adaptation method, MonteCLoRA, which learns posterior distributions of low-rank parameters with appropriate prior distributions. MonteCLoRA parameterizes the low-rank parameters as a mixture of multivariate Gaussian distributions, where each distribution’s precision matrix is assumed to follow Wishart knowledge prior. Through Monte Carlo estimation from multiple parameters sampled from the parameter space, MonteCLoRA stabilizes the reparametrized parameters and generates robust and unbiased low-rank adaptation for LLMs. Our theoretical and empirical results justify the importance of parameterization for low-rank adaptation of LLMs. We perform thorough empirical analysis with five natural language understanding (NLU) and six natural language generation (NLG) tasks with two pre-trained LLMs — RoBERTa-base (Liu et al., 2019) and LLaMA-1-7B (Touvron et al., 2023). Empirical results on NLU tasks suggest that MonteCLoRA is more stable, where the average spread of accuracy distribution is 10%percent1010\%10 % lower than LoRA and 50%percent5050\%50 % lower than full fine-tuning. In terms of the robustness metrics, MonteCLoRA is 5%percent55\%5 % more robust and achieves 2%percent22\%2 % better accuracy than LoRA fine-tuning. Remarkably, on NLG tasks, MonteCLoRA has 50%percent5050\%50 % lower spread (2.192.192.192.19 points) than LoRA (4.694.694.694.69 points) with zero-shot validation accuracy distribution. Our further in-depth analysis highlights the superiority of MonteCLoRA in terms of stable and faster convergence while fine-tuning LLMs. The key contributions of our work can be summarized as follows111The source code of MonteCLoRA is made available at https://github.com/LCS2-IIITD/MonteCLoRA.: • Our work provides an in-depth theoretical analysis of the impact of hyperparameters on fine-tuning LLMs and highlights the key challenges with low-rank adaptation techniques. To the best of our knowledge, no comprehensive study exists on the sensitivity analysis of LLM fine-tuning. • We propose a Bayesian alternative to the low-rank adaptation of LLMs for estimating trainable parameters during fine-tuning. The paper theoretically justifies the robustness of the posterior estimation. • The proposed fine-tuning method adds only 𝒪⁢(1)𝒪1\mathcal{O}(1)caligraphic_O ( 1 ) additional parameters for posterior estimation and is shown to be effective in terms of both performance and stability of the fine-tuned LLMs. • We provide a thorough empirical study where we demonstrate the effectiveness of the proposed fine-tuning methods with two pre-trained language models – RoBERTa-base and LLaMA-1-7B on five NLU and six NLG (commonsense reasoning) tasks. The paper is organized as follows. Section 2 describes the related work on efficient fine-tuning strategies for LLMs. Section 3 elaborates on the background concepts used in the paper. In Section 4, we describe our proposed method, MonteCLoRA, along with the theoretical results obtained in this work. Section 5 describes the experimental details used in the empirical study done in the paper. In Section 6, we present the results of the empirical study. Section 7 highlights few case studies to analyze robust fine-tuning of LLMs. Additional background materials and supplementary results are furnished in the appendix."
https://arxiv.org/html/2411.04330v1,Scaling Laws for Precision,"Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise “precision-aware” scaling laws for both training and inference. We propose that training in lower precision reduces the model’s effective parameter count, allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.","1 Introduction Scale has emerged as a central driver of progress in deep learning (Brown, 2020). Key work on scaling (Kaplan et al., 2020; Hoffmann et al., 2022) studied tradeoffs between model/dataset size to balance performance and compute. However, the precision in which models are trained and served is an important third factor that contributes to both cost and performance. Deep learning is trending towards lower precision: current frontier models like the Llama-3 series are trained in BF16 (Dubey et al., 2024), and there is widespread effort to move the pretraining paradigm to FP8 (Micikevicius et al., 2022). The next generation of hardware will support FP4, and advances in weight-only quantization have led to training in binary and ternary at scale (Ma et al., 2024; Wang et al., 2023). How far will these paradigms go? Specifically, we ask: What are the tradeoffs between precision, parameters, and data? How do they compare for pretraining and inference? Studying scaling in precision is challenging because work on scaling laws generally aims to drop fine-grained implementation details in pursuit of universal functional forms while work on quantization generally does the opposite, focuses on the details: how quantization is done, with what type, to what part of the model. In seeking a balance, we consider a variety of plausible functional forms, and choose one that abstracts implementation details of quantization away from loss scaling, allowing us to predict loss scaling in many situations of practical interest. This functional form that posits bit precision and parameter count interchangeably contribute to a model’s “effective parameter count,” Neffsubscript𝑁effN_{\text{eff}}italic_N start_POSTSUBSCRIPT eff end_POSTSUBSCRIPT, and implementation details like which parts of a model are quantized to what precision, interact with loss scaling only through their effect on this quantity. Figure 1: Schematic of key findings. (Left) Training a fixed model size to various data budgets in BF16 and quantizing weights at the end. We find that degradation due to post-train quantization increases with tokens seen during pretraining, so that eventually additional pretraining data can be harmful. (Right) Our scaling suggests training larger models in lower precision can be compute-optimal according to the cost model in Section 4.3. Weights, activations, attention quantized, all models trained on the same data budget, details in Appendix H. Overall, we study the scaling of the effects of precision on loss as we vary data and parameters, both during and after training. We first study how the degradation induced by post-train quantization scales with parameters and data. We find that the degradation increases with data, so that for a fixed model, training on additional data after a certain point can be actively harmful if the model will be quantized after training. We then shift our focus to quantized training, examining both the quantization-aware-training (weights only) and low-precision training (weights, activations, attention all quantized) settings. Our scaling laws for pretraining suggest that the compute-optimal pretraining precision is in general independent of compute budget. Surprisingly, however, this independence ceases to be true if model size is constrained, in which case the compute-optimal precision grows slowly in compute. In all, we pretrain a suite of 465 language models in 3 to 16 bit precisions, as well as post-train quantize each to multiple precisions. For a language model with N𝑁Nitalic_N parameters, trained on D𝐷Ditalic_D tokens with training precision Ptrainsubscript𝑃trainP_{\text{train}}italic_P start_POSTSUBSCRIPT train end_POSTSUBSCRIPT, and post-train weight precision Ppostsubscript𝑃postP_{\text{post}}italic_P start_POSTSUBSCRIPT post end_POSTSUBSCRIPT, we ultimately find a unified scaling law that takes the following form: L⁢(N,D,Ptrain,Ppost)=A⁢Neff−α⏟Training-time Effects+B⁢D−β+E⏟Usual Chinchilla form+δPTQ⁢(Neff,D,Ptrain,Ppost)⏟Post-Training Effects𝐿𝑁𝐷subscript𝑃trainsubscript𝑃postsubscript⏟subscript⏟𝐴superscriptsubscript𝑁eff𝛼Training-time Effects𝐵superscript𝐷𝛽𝐸Usual Chinchilla formsubscript⏟subscript𝛿PTQsubscript𝑁eff𝐷subscript𝑃trainsubscript𝑃postPost-Training EffectsL(N,D,P_{\text{train}},P_{\text{post}})=\underbrace{\underbrace{AN_{\text{eff}% }^{-\alpha}}_{\text{Training-time Effects}}+BD^{-\beta}+E}_{\text{Usual % Chinchilla form}}+\underbrace{\delta_{\text{PTQ}}(N_{\text{eff}},D,P_{\text{% train}},P_{\text{post}})}_{\text{Post-Training Effects}}italic_L ( italic_N , italic_D , italic_P start_POSTSUBSCRIPT train end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT post end_POSTSUBSCRIPT ) = under⏟ start_ARG under⏟ start_ARG italic_A italic_N start_POSTSUBSCRIPT eff end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_α end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT Training-time Effects end_POSTSUBSCRIPT + italic_B italic_D start_POSTSUPERSCRIPT - italic_β end_POSTSUPERSCRIPT + italic_E end_ARG start_POSTSUBSCRIPT Usual Chinchilla form end_POSTSUBSCRIPT + under⏟ start_ARG italic_δ start_POSTSUBSCRIPT PTQ end_POSTSUBSCRIPT ( italic_N start_POSTSUBSCRIPT eff end_POSTSUBSCRIPT , italic_D , italic_P start_POSTSUBSCRIPT train end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT post end_POSTSUBSCRIPT ) end_ARG start_POSTSUBSCRIPT Post-Training Effects end_POSTSUBSCRIPT (1) where A,B,E,α,β𝐴𝐵𝐸𝛼𝛽A,B,E,\alpha,\betaitalic_A , italic_B , italic_E , italic_α , italic_β are positive fitted constants, and δPTQsubscript𝛿PTQ\delta_{\text{PTQ}}italic_δ start_POSTSUBSCRIPT PTQ end_POSTSUBSCRIPT refers to the loss degradation induced by post-training quantization before inference. Altogether, our results for post-train quantization illustrate how more pretraining FLOPs do not always lead to better models at inference-time, and our results for low-precision pretraining suggest that both the standard practice of training models in 16-bit, and the race to extremely low (sub 4-bit) pretraining precision, may be suboptimal."
https://arxiv.org/html/2411.04282v1,Language Models are Hidden Reasoners:Unlocking Latent Reasoning Capabilities via Self-Rewarding,"Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.","The development of large language models (LLMs) with enhanced reasoning capabilities has emerged as a crucial area of research. Despite their impressive advances, the inherent next-token prediction mechanism of LLMs makes it challenging for these models to solve complex problems requiring multiple reasoning steps (Wang et al., 2022; Huang et al., 2023). For instance, LLMs often struggle to directly provide accurate solutions to mathematical problems or even simple puzzles like counting specific letters in a word. Consequently, researchers have explored various prompting strategies that guide LLMs to generate reasoning trajectories or rationales—sequences of tokens that build a step-by-step progression toward an answer. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022), Tree-of-Thought (ToT) (Yao et al., 2024), and Program-of-Thought (PoT) (Chen et al., 2023) prompting methods exemplify this approach. Question 𝒙𝒙\boldsymbol{x}bold_italic_x Language Model πθsubscript𝜋𝜃\operatorname{\pi_{\theta}}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT Sampled Rationale 𝒛1,𝒛2,…,𝒛Ksubscript𝒛1subscript𝒛2…subscript𝒛𝐾\boldsymbol{z}_{1},\boldsymbol{z}_{2},\ldots,\boldsymbol{z}_{K}bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_italic_z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT Self-reward: Compute the likelihood of πθsubscript𝜋𝜃\operatorname{\pi_{\theta}}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT generating 𝒚𝒚\boldsymbol{y}bold_italic_y after observing 𝒙𝒙\boldsymbol{x}bold_italic_x and 𝒛𝒛\boldsymbol{z}bold_italic_z. Groundtruth y𝑦\boldsymbol{y}bold_italic_y Language Model πθsubscript𝜋𝜃\operatorname{\pi_{\theta}}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT update Question: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts does it take? Groundtruth: The answer is 3. Sampled Rationale 1 (correct ✅, higher likelihood): It takes 2/2 = 1 bolt of white fiber. 2 + 1 = 3. So, it takes a total of 3 bolts of fiber. Sampled Rationale 2 (incorrect ❌, lower likelihood): We need 2 bolts of blue and 2 bolts of white fiber. In total, it is 2 + 2 = 4. Figure 1: Overview of LaTRO with an example question from GSM8K (Cobbe et al., 2021b). LaTRO treats reasoning trajectories as latent variables and optimizes the underlying distribution through self-rewarding. Given a question, the language model generates multiple reasoning rationales, evaluates their likelihood of producing the correct answer, and updates its parameters to favor high-quality rationales. This iterative process allows the model to improve both its ability to generate good reasoning paths and to evaluate the quality of those paths. Recent progress has also focused on inference-time techniques to enhance the reasoning abilities of LLMs (Wu et al., 2024; Brown et al., 2024), as observed in the OpenAI o1 model (OpenAI, 2024). These methods have demonstrated remarkable performance in diverse reasoning tasks, including mathematics (Cobbe et al., 2021b; Trinh et al., 2024; Luo et al., 2024), coding (Jimenez et al., 2023; Guo et al., 2024; Zhang et al., 2024), and scientific problem-solving (Rein et al., 2023). Notable inference-time methods, such as CoT with Self-Consistency (CoT-SC) (Wang et al., 2023) and CoT-Decoding (Wang & Zhou, 2024), extend the CoT approach by generating multiple reasoning paths and selecting the most consistent one. Additionally, techniques like ReAct (Yao et al., 2023a) and Reflexion (Shinn et al., 2023) integrate reasoning into LLM agent loops, further enhancing their problem-solving capabilities. Despite the promising results at inference time, improving the reasoning abilities of LLMs during their training phase remains a challenging problem. Several obstacles impede progress in this area. Firstly, there is a scarcity of high-quality reasoning data for complex problems, limiting the applicability of traditional supervised fine-tuning (SFT) approaches (Zelikman et al., 2022). Moreover, when such data is available, SFT on deterministic reasoning paths may result in a lack of diversity in problem-solving strategies, potentially causing over-confidence issues and performance degradation (Cobbe et al., 2021b), especially in domains needing multiple valid approaches, such as mathematical proofs and coding. Alternatively, improving reasoning through reinforcement learning from human feedback (RLHF) presents its own challenges (Havrilla et al., 2024; Luo et al., 2024). Developing a reward model that accurately evaluates the quality and validity of reasoning paths is a formidable task, susceptible to distribution shifts and biased evaluations. Self-improvement approaches like STaR (Self-Taught Reasoner) (Zelikman et al., 2022) and Quiet-STaR (Zelikman et al., 2024) have shown promise in enhancing language models’ reasoning capabilities without external feedback. However, STaR relies on task-specific few-shot examples to bootstrap its reasoning process, which can limit its generalizability across diverse tasks. While Quiet-STaR attempts to overcome this by inferring implicit rationales across arbitrary text, it does not directly optimize the reasoning process itself. Through these findings, we observe that pretrained LLMs already possess innate reasoning capabilities but just have not been fully activated or utilized, inspiring us to propose our approach. Our proposed method, LaTent Reasoning Optimization (LaTRO), addresses the limitations of previous approaches by formulating reasoning as sampling from a latent distribution and optimizing it through a principled variational framework. As illustrated in Fig. 1, LaTRO enables language models to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring task-specific few-shot examples or external reward models. Key contributions of LaTRO include: 1. A theoretical formulation connecting LLM reasoning optimization to latent variable models; 2. A self-rewarding mechanism leveraging the model’s own probability estimates; 3. Significant performance gains across multiple model architectures and reasoning tasks, demonstrating LaTRO’s effectiveness in unlocking latent reasoning capabilities of language models. Our findings suggest that pre-trained LLMs are not only capable reasoners but also possess the potential to act as explicit reward models for evaluating reasoning paths. We term this approach of utilizing explicit reward functions induced by LLMs themselves as ""self-rewarding."" Empirically, LaTRO outperforms both baseline models and supervised fine-tuning approaches on reasoning tasks like GSM8K, while also demonstrating the capacity to compress reasoning processes and shift computational burdens from inference to training time."
https://arxiv.org/html/2411.04158v1,Analyzing Multimodal Features of Spontaneous Voice Assistant Commands for Mild Cognitive Impairment Detection,"Mild cognitive impairment (MCI) is a major public health concern due to its high risk of progressing to dementia. This study investigates the potential of detecting MCI with spontaneous voice assistant (VA) commands from 35 older adults in a controlled setting. Specifically, a command-generation task is designed with pre-defined intents for participants to freely generate commands that are more associated with cognitive ability than read commands. We develop MCI classification and regression models with audio, textual, intent, and multimodal fusion features. We find the command-generation task outperforms the command-reading task with an average classification accuracy of 82%, achieved by leveraging multimodal fusion features. In addition, generated commands correlate more strongly with memory and attention subdomains than read commands. Our results confirm the effectiveness of the command-generation task and imply the promise of using longitudinal in-home commands for MCI detection.","Mild cognitive impairment (MCI) presents a significant public health concern because of its substantial risk of progressing to dementia [1]. Around half of MCI patients develop dementia within 3 years and an annual progression rate of 6%percent66\%6 % to 15%percent1515\%15 % [2]. Early detection of MCI is crucial for preventing or delaying the emergence of dementia [3]. Speech-based dementia detection is promising as a low-cost early-stage screening method [4, 5, 6, 7]. Researchers have explored various approaches for detecting MCI and other cognitive impairments from speech, such as automated phone task [8], picture description task [9, 10], and tablet-based automatic assessment [11]. Previous works mainly focused on the common dataset, including Pitt Corpus and the Cookie Theft Picture description task [12, 13, 9]. Despite three decades of research, this task faces data limitations due to the expensive data collection process [10]. In recent years, an AI-driven Voice Assistant (VA) can be integrated into a wide range of Internet of Things (IoT) devices, such as smart speakers and smartphones. They can engage older adults in conversations, which makes it a practical and convenient tool for collecting speech data and enabling health-related speech-based research [14, 15, 16, 6, 17]. Researchers [18] evaluated VA capabilities in detecting MCI through a command-reading task with a focus on analyzing audio data embeddings. Both MCI and Healthy Controls (HC) participants demonstrated highly similar command outputs when reading from identical instruction sheets of VA commands. We consider the speech dataset collected from such a command-reading task does not require the participant to use much of their memory and attention ability, known as cognitive load [19]. Cognitive load has been proven as an effective metric to be used to assess a patient’s mental condition [11]. We envision that a task that gives older adults more freedom in the command generation process represents a more realistic VA scenario in their homes and can enhance MCI detection. In this paper, we design a command-generation task through the VA system to enable participants to freely generate commands according to an intent, which may result in subtle changes between MCI and HC. Specifically, the command-generation task provides participants with a set of intent keywords to assist them in generating voice commands. The commands can be formed differently but meant to accomplish the same intent. We propose new intent features and aim to study and compare the effectiveness of the intent features, and fusion of multimodal features extracted from the VA commands and investigate whether the spontaneous VA commands from the command-generation task can achieve enhanced performance in MCI detection. The contributions of our paper are as follows: First, we propose a command-generation task through VA, which is associated with a higher cognitive load, resulting in a better performance in MCI detection than the command-reading task. The average classification accuracy of all features in multi-modality achieved higher results, increased from 73% to 77% for all classifiers, and increased from 78% to 82% using the Random Forest (RF) classifier. Second, we study the correlation between the VA commands and six subdomains of the Montreal Cognitive Assessment (MoCA) [20]. VA commands from the command-generation task exhibit stronger correlations across all subdomains compared to the command-reading task, particularly within memory and attention subdomains. The results show that the command-generation task demands increased attention and short-term memory retention. Third, we compare the performance of intent, audio, textual, and multimodal fusion features on the command-generation task and find that audio and fusion (intent + audio) features achieved better classification accuracy (mean: 75% and 74%, best: 88% and 91%). The results confirm that the intent and audio features are more sensitive to the MCI detection than the textual features. Figure 1: MCI detection model using multimodal features extracted from voice assistant commands (L: Language, O: Orientation, V: Visuospatial, E: Executive function, M: Memory, A: Attention)."
https://arxiv.org/html/2411.04156v1,Crystal: Illuminating LLM Abilities on Language and Code,"Large Language Models (LLMs) specializing in code generation (which are also often referred to as code LLMs), e.g., StarCoder and Code Llama, play increasingly critical roles in various software development scenarios. It is also crucial for code LLMs to possess both code generation and natural language abilities for many specific applications, such as code snippet retrieval using natural language or code explanations. The intricate interaction between acquiring language and coding skills complicates the development of strong code LLMs. Furthermore, there is a lack of thorough prior studies on the LLM pretraining strategy that mixes code and natural language. In this work, we propose a pretraining strategy to enhance the integration of natural language and coding capabilities within a single LLM. Specifically, it includes two phases of training with appropriately adjusted code/language ratios. The resulting model, Crystal, demonstrates remarkable capabilities in both domains. Specifically, it has natural language and coding performance comparable to that of Llama 2 and Code Llama, respectively. Crystal exhibits better data efficiency, using 1.4 trillion tokens compared to the more than 2 trillion tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by analyzing the training process and observe consistent improvements in most benchmarks. We also adopted a typical application adaptation phase with a code-centric data mixture, only to find that it did not lead to enhanced performance or training efficiency, underlining the importance of a carefully designed data recipe. To foster research within the community, we commit to open-sourcing every detail of the pretraining111Webpage: https://www.llm360.ai/#crystal, including our training datasets222Datasets: https://huggingface.co/datasets/LLM360/CrystalCoderDatasets, code333Code: https://github.com/LLM360/crystalcoder-train, loggings444Wandb: https://wandb.ai/llm360/CrystalCoder and 136 checkpoints555Model weights: https://huggingface.co/LLM360/CrystalCoder and https://huggingface.co/LLM360/CrystalChat throughout the training.","Figure 1: The multi-phase training process for Crystal. Figure 2: Crystal shows a good balance of language and coding abilities. The y𝑦yitalic_y-axis is the average over ARC-C, HellaSwag, MMLU, and GSM8K. The x𝑥xitalic_x-axis is the average of MBPP and HumanEval. Large Language Models (LLMs) for code generation (i.e., code LLMs), such as Codex (Chen et al., 2021a), StarCoder (Li et al., 2023a), and Code Llama (Roziere et al., 2023), are advancing rapidly due to their strong capability in generating code-related content (e.g., functions), which helps improve the efficiency of software engineers and developers (Cognition Labs, 2024; Chen et al., 2021b; Li et al., 2023a; Roziere et al., 2023). These LLMs excel at generating functions and designing web page components based on engineers’ instructions (e.g., “Return True if all numbers in the list L𝐿Litalic_L are below threshold T𝑇Titalic_T.”) (Calò & De Russis, 2023). However, the abilities of code-oriented LLMs are constrained in development contexts that necessitate interpreting high-level human instructions (e.g., through prompts or function descriptions) and producing comprehensive, structured code accompanied by natural language documentation. Examples of such scenarios include solving GitHub issues (Jimenez et al., 2023), searching for code snippets based on natural language queries, generating entire Python libraries (which include their complete code along with documentation and tutorials (Liu et al., 2023a; Luo et al., 2024)), or developing source code for websites, e.g., “Create a ticketing platform for travelers” (Calò & De Russis, 2023). This underscores the ambition to create LLMs proficient in both natural language processing and coding. Achieving this goal, however, is non-trivial. For instance, Code Llama, despite being continuously pretrained with code datasets on top of Llama2, suffers from catastrophic forgetting of natural language capabilities. In open-sourced LLMs, we observe a prevalent issue: most models are tailored to specialize in either language or code, not both. For example, StarCoder is exclusively trained on code datasets accompanied by function documentation, thus limiting its exposure to varied natural language data. This trend indicates a notable gap in the design of most open-source LLMs, where there’s a lack of a comprehensive curriculum that addresses both coding and natural language processing. Therefore, we are intrigued by the following research question: “Can an LLM efficiently obtain both language and coding abilities?” Existing studies have shown that the simultaneous acquisition of coding and language capabilities by LLMs is governed by complex dynamics: these skills may either conflict (Li et al., 2023a; Roziere et al., 2023) or complement (Ma et al., 2024) each other, influenced by the data recipe and the model’s learning phase. In this work, we propose a pretraining strategy designed specifically for code LLMs. Our strategy is inspired by techniques such as multi-phase pretraining, curriculum learning (Bengio et al., 2009), continuous pretraining (Roziere et al., 2023), and multi-language training, and has two phases. We start the pretraining process with a data mixture of 95% natural language and 5% code. In the second phase, the data mixture is enriched to include 63% code data alongside 37% natural language. This two-phase design mimics the human learning process, where the acquisition of general language knowledge precedes the development of coding skills, aiming to replicate this learning sequence. Pretraining using our strategy yields Crystal, a code LLM that exhibits strong ability across both natural language (e.g., common sense reasoning) and code generation. Our strategy also demonstrates good data efficiency. That is, Crystal, pretrained with 1.4 trillion tokens, performs comparably to Llama 2 and Code Llama, each pretrained with more than 2 trillion tokens. Throughout the pretraining process, we continuously tracked the model’s performance on downstream benchmarks, observing steady enhancements in both language and coding abilities across the two training phases. Despite a slight performance decline due to the distribution shift between Phase 1 and Phase 2, performance in Phase 2 swiftly recovers and surpasses that of Phase 1. Additionally, implementing an experimental application adaptation phase, aimed at further enhancing coding abilities by incorporating an increased percentage of code data, could potentially boost performance. This phase is inspired by the Python-specialized pretraining phase of Code Llama and StarCoder (Roziere et al., 2023; Li et al., 2023a). Contrary to expectations, we observe mixed results from this phase, including a decline in language ability but marginal improvement in coding performance (see § 5.2), underscoring the necessity for a carefully crafted data strategy. Conducting thorough ablation studies for the entire pretraining process is computationally daunting. To mitigate these challenges, we embrace the principles of the LLM360 initiative (Liu et al., 2023b), ensuring full transparency in our pretraining process to support further scientific exploration and discoveries by the community. We release our training and fine-tuning datasets, source code for training, fine-tuning, and data preprocessing, and 152 intermediate model checkpoints. We also release a chat version, fine-tuned from Crystal, namely CrystalChat, for user convenience."
https://arxiv.org/html/2411.03823v1,Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination,"The rapid progression of multimodal large language models (MLLMs) has demonstrated superior performance on various multimodal benchmarks. However, the issue of data contamination during training creates challenges in performance evaluation and comparison. While numerous methods exist for detecting dataset contamination in large language models (LLMs), they are less effective for MLLMs due to their various modalities and multiple training phases. In this study, we introduce a multimodal data contamination detection framework, MM-Detect, designed for MLLMs. Our experimental results indicate that MM-Detect is sensitive to varying degrees of contamination and can highlight significant performance improvements due to leakage of the training set of multimodal benchmarks. Furthermore, We also explore the possibility of contamination originating from the pre-training phase of LLMs used by MLLMs and the fine-tuning phase of MLLMs, offering new insights into the stages at which contamination may be introduced.","The development of MLLMs has exceeded expectations (Liu et al., 2023a; Lin et al., 2023), showcasing extraordinary performance on various multimodal benchmarks (Lu et al., 2022; Liu et al., 2023b; Song et al., 2024), even surpassing human performance. However, due to the partial obscurity associated with MLLMs training (OpenAI, 2023; Reid et al., 2024), it remains challenging to definitively ascertain the impact of training data on model performance, despite some works showing the employment of the training set of certain datasets (Liu et al., 2023a; Chen et al., 2023; Bai et al., 2023b). The issue of data contamination, occurring when training or test data of benchmarks is exposed during the model training phase (Xu et al., 2024), could potentially instigate inequitable performance comparisons among models. This not only creates a dilemma for users in model selection but also poses a significant hurdle to further advancements in this domain. While numerous works in the field of LLMs have proposed methods for detecting data contamination (Yeom et al., 2018; Deng et al., 2024; Dong et al., 2024), MLLMs, due to their various modalities and multiple training phases (Liu et al., 2023a; Li et al., 2023), face limitations when applying these methods. Therefore, there is a pressing need for a more suitable multimodal contamination detection framework specifically tailored for MLLMs. In this study, we carry out a systematic analysis of multimodal data contamination. Initially, we define Multimodal Data Contamination, as it pertains to the modality of data sources exposed to the MLLMs, into two categories: Unimodal Contamination and Cross-modal Contamination. Subsequently, we unveil a detection framework for multimodal data contamination, MM-Detect, which incorporates two methods, Option Order Sensitivity Test and Slot Guessing for Perturbation Caption, designed to handle two common types of Visual Question Answering (VQA) tasks: multiple-choice and caption-based questions, respectively. Then, applying MM-Detect on eleven widely-used MLLMs across five prevalent VQA datasets, we observe that both open-source and proprietary MLLMs do exhibit contamination, with the degree of contamination varying across different models. To corroborate the validity and sensitivity of our approach, we deliberately induce contamination in MLLMs, thus simulating feasible real-world scenarios. Experimental results indicate that our method proves to be quite effective and sensitive in identifying varying degrees of contamination. Interestingly, our findings reveal that not only does leakage in the multimodal benchmark test set play a role, but the training set can also contribute significantly to enhancing the model’s performance. To further delve into the stage where contamination is introduced, we employ a heuristic method. This method seeks to distinguish whether the contamination originates from the pre-training phase of LLMs or the multimodal training phase. Our findings suggest that the contamination observed in some MLLMs may not necessarily stem from the multimodal training phase. Instead, it could potentially be traced back to the pre-training stage of their respective LLMs. To the best of our knowledge, our work is the first effort to systematically analyze multimodal data contamination. In conclusion, our research makes several important contributions: • We formulate the definition for multimodal contamination detection and present the MM-Detect framework, comprising two innovative methods specifically designed for effective contamination detection in MLLMs. • We demonstrate that leakage from multimodal benchmark data can significantly boost the model’s performance on test sets, with this enhancement intensifying as the degree of contamination increases. • By employing a heuristic method, we pioneer the exploration into the stage at which contamination is introduced, revealing that it may stem not solely from the multimodal data but could also from the LLMs."
https://arxiv.org/html/2411.04118v1,Medical Adaptation of Large Language and Vision-Language Models:Are We Making Progress?,"Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision-language models (VLMs) via continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining (DAPT) improves performance on downstream medical tasks, such as answering medical licensing exam questions. In this paper, we compare seven public “medical” LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting regime for medical question-answering (QA) tasks. For instance, across the tasks and model pairs we consider in the 3-shot setting, medical LLMs only outperform their base models in 12.1% of cases, reach a (statistical) tie in 49.8% of cases, and are significantly worse than their base models in the remaining 38.2% of cases. Our conclusions are based on (i) comparing each medical model head-to-head, directly against the corresponding base model; (ii) optimizing the prompts for each model separately; and (iii) accounting for statistical uncertainty in comparisons. While these basic practices are not consistently adopted in the literature, our ablations show that they substantially impact conclusions. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.","(a) (b) Figure 1: Medical LLMs and VLMs trained via domain-adaptive pretraining (DAPT) show limited improvement over their general-domain counterparts. (a) Overview of our head-to-head evaluation approach for each pair of general-domain (blue) and medically adapted LLM/VLM (red). (b) Win/tie/loss rate (%) of medical models vs. their corresponding base models across all (model pair, QA dataset) combinations. Win rate refers to the proportion of (model pair, QA dataset) combinations where a medical model shows a statistically significant improvement. Recent advances in autoregressive large language models (LLMs) and vision-language models (VLMs) have attracted interest from practitioners in medicine, where these models hold great potential to transform various aspects of clinical practice (e.g., medical diagnosis, information retrieval from clinical documents, patient triaging) (Fries et al., 2022a; Moor et al., 2023a). State-of-the-art performance on various medical benchmarks is typically achieved by massive-scale closed-source models, such as GPT-4 (OpenAI, 2023a, b), Med-Gemini (Saab et al., 2024; Yang et al., 2024), and Med-PaLM (Singhal et al., 2023a, b; Tu et al., 2024), often performing on par with humans on medical licensing exams and open-ended consumer health question-answering (QA) tasks. However, the general lack of transparency in these models, high API usage costs, and patient data privacy concerns make their integration into routine clinical workflows challenging (Marks and Haupt, 2023). To address such concerns, recent works have proposed cheaper, open-source alternatives through domain-adaptive pretraining (DAPT; Gururangan et al., 2020), where a pretrained open-source general-domain model—such as Llama (Touvron et al., 2023a, b; Meta, 2024) or Mistral (Jiang et al., 2023) in the language space, and LLaVA (Liu et al., 2023) or Open-Flamingo (Awadalla et al., 2023) in the vision-language space—is continually pretrained on biomedical (image-)text corpora from public sources such as PubMed and medical textbooks. While some prior works show that medical models pretrained from scratch only using domain-specific corpora can outperform those trained via DAPT, both in the context of BERT-style encoder-only models (Devlin et al., 2019; Gu et al., 2021; Yang et al., 2022) and decoder models (Taylor et al., 2022; Luo et al., 2022; Hernandez et al., 2023; Bolton et al., 2024), the DAPT approach has become common practice, resulting in a trend where the release of a more capable general-domain model is typically followed by the release of its medical counterpart. Despite the widespread adoption of medical DAPT, the claimed improvements in performance are worth scrutinizing. While the story is intuitive, more recent base models (e.g., Llama-3-8B (Meta, 2024)) already exhibit strong off-the-shelf performance on medical benchmarks without any adaptation (e.g., Open Medical LLM Leaderboard (Pal et al., 2024)), and given a lack of transparency about the pretraining corpora used to train the general-domain model in the first place, they may already be trained on relevant medical text. Perhaps more concerning is the lack of apples-to-apples comparisons in the literature. First, medical models resulting from DAPT are often only compared against baselines with different architectures (e.g., Clinical-Camel-70B (Toma et al., 2023) vs. GPT-4 (OpenAI, 2023a)) and under inconsistent evaluation setups (e.g., MediTron-70B (Chen et al., 2023) fine-tuned on MedQA (Jin et al., 2020) vs. non-fine-tuned Med42-v1-70B (Christophe et al., 2024)), which can confound the interpretation of results. Second, the common practice of using a single, fixed prompting setup (e.g., prompt format, choice of few-shot examples) for all models under evaluation also warrants concern, as LLM/VLM behavior is extremely sensitive to such design decisions (Jiang et al., 2020; Zhao et al., 2021; Ceballos-Arroyo et al., 2024), and the “optimal” choice of such details rarely correlates between different models (Sclar et al., 2024). In this paper, we perform an apples-to-apples comparison that addresses these concerns, comparing seven medical LLMs and two medical VLMs against their general-domain base models. We find that, for all but one LLM pair—BioMistral-7B (Labrak et al., 2024) vs. Mistral-7B-Instruct-v0.1 (Jiang et al., 2023), a pair of models that performs fairly poorly in absolute terms—the open-source medical LLMs and VLMs that we evaluate do not consistently improve over their general-domain counterparts on various medical (visual) QA tasks (Figure 1). We compare several pairs of general-domain and medically adapted LLMs/VLMs (see Table 1), whose only differences lie in medical DAPT (i.e., one model is the base model, from which the other is derived via medical DAPT). For each pair, we compare their performances from zero-/few-shot prompting (Radford et al., 2019; Brown et al., 2020), after independently selecting the “best” prompt format and few-shot examples for each model based on the validation set and accounting for statistical uncertainty in model comparison. Table 1: Summary of open-source autoregressive VLM and LLM pairs used for evaluation. Model Class General Domain Medical Domain Medical Adaptation Corpora LLM Llama-3-70B-Instruct (Meta, 2024) OpenBioLLM-70B (Pal and Sankarasubbu, 2024) Undisclosed Llama-2-70B (Touvron et al., 2023b) MediTron-70B (Chen et al., 2023) Clinical Practice Guidelines (e.g., CDC, WHO) PubMed Articles (S2ORC; Lo et al., 2020) Llama-2-70B (Touvron et al., 2023b) Clinical-Camel-70B (Toma et al., 2023) ShareGPT 20k PubMed Articles Published Before 2021 Random 4k Subset of MedQA (Jin et al., 2020) Llama-3-8B (Meta, 2024) OpenBioLLM-8B (Pal and Sankarasubbu, 2024) Undisclosed Llama-2-7B (Touvron et al., 2023b) MediTron-7B (Chen et al., 2023) Clinical Practice Guidelines (e.g., CDC, WHO) PubMed Articles (S2ORC; Lo et al., 2020) Mistral-7B-Instruct-v0.1 (Jiang et al., 2023) BioMistral-7B (Labrak et al., 2024) PubMed Articles (PMC Open Access Subset) Llama-2-7B-Chat (Touvron et al., 2023b) BioMedGPT-LM-7B (Luo et al., 2023) PubMed Articles (S2ORC; Lo et al., 2020) VLM LLaVA-v0-7B (Liu et al., 2023) LLaVA-Med-7B (Li et al., 2023) PubMed Articles (PMC-15M; Zhang et al., 2023) Open-Flamingo-9B (Awadalla et al., 2023) Med-Flamingo-9B (Moor et al., 2023b) Medical Textbooks (MTB; Moor et al., 2023b) PubMed Articles (PMC-OA; Lin et al., 2023) Our findings (Section 4) suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities that can be leveraged effectively when prompted appropriately. Our main contributions can be summarized as follows: 1. We provide a comprehensive head-to-head comparison between state-of-the-art general-domain LLMs/VLMs and their medical DAPT counterparts on various medical (visual) QA benchmarks, to investigate the effectiveness of DAPT for medical specialization. 2. We find that after optimizing the prompts for medical and general-domain models independently, all medical VLMs and nearly all medical LLMs that we evaluate fail to consistently improve over their corresponding general-domain base models. 3. We show that using a single, fixed prompt format and choice of few-shot examples for all models without testing for statistical significance can lead to overly optimistic conclusions about the benefits from medical DAPT."
https://arxiv.org/html/2411.04109v1,Self-Consistency Preference Optimization,"Self-alignment, whereby models learn to improve themselves without human annotation, is a rapidly growing research area. However, existing techniques often fail to improve complex reasoning tasks due to the difficulty of assigning correct rewards. An orthogonal approach that is known to improve correctness is self-consistency, a method applied at inference time based on multiple sampling in order to find the most consistent answer. In this work, we extend the self-consistency concept to help train models. We thus introduce self-consistency preference optimization (ScPO), which iteratively trains consistent answers to be preferred over inconsistent ones on unsupervised new problems. We show ScPO leads to large improvements over conventional reward model training on reasoning tasks such as GSM8K and MATH, closing the gap with supervised training with gold answers or preferences, and that combining ScPO with standard supervised learning improves results even further. On ZebraLogic, ScPO finetunes Llama-3 8B to be superior to Llama-3 70B, Gemma-2 27B, and Claude-3 Haiku.","Training large language models (LLMs) on human-annotated data has improved their performance on a wide array of tasks (Bai et al., 2022; Touvron et al., 2023). However, the size and quality of human data remains a major bottleneck as the data collection process is often resource-intensive in terms of cost, time, and expertise. To address this challenge, recent works focus on iteratively training from model-generated data via self-training (Yuan et al., 2024; Chen et al., 2024b). Notably, Yuan et al. (2024) propose a “self-rewarding” training pipeline for instruction-following, comprising two steps: (i) using the LLM to generate new queries and self-evaluating the generated responses for each query; and (ii) building preference pairs and training the LLM using iterative direct preference optimization loss (DPO; Rafailov et al., 2024; Xu et al., 2023). However, Huang et al. (2024) demonstrate that LLMs struggle at evaluating the correctness of their own responses on complex problem-solving tasks which have an unambiguous correct answer, thereby rendering Yuan et al.’s self-evaluation approach ineffective. Using an external reward model (RM) to rank responses can have similar problems; even if such models are trained on reasoning tasks they may still suffer on out-of-distribution problems (Casper et al., 2023; Zhang et al., 2024; Mahan et al., 2024). To address this issue, we introduce Self-consistency Preference Optimization (ScPO). ScPO is an approach to self-train LLMs for complex problem-solving tasks without access to gold solutions or final answers in the training data. Our approach leverages the concept of self-consistency (Wang et al., 2023), an inference-time only approach that improves performance on reasoning tasks by generating multiple solutions using the LLM and choosing the most frequent final answer. More consistent answers are more likely to be correct because mistakes made by the model are often random, so incorrect solutions are unlikely to lead to the same answer multiple times (Fischler & Bolles, 1981; Chen et al., 2023). In ScPO, the self-consistency concept is instead applied during unsupervised self-training. The method consists of (i) selecting model-generated queries, (ii) annotating preference pairs using the most self-consistent response (winner) and least self-consistent response (loser), and (iii) optimizing a loss function that is weighted for each instance depending on the model’s confidence in the preference pair. Additionally, we propose a semi-supervised variant of ScPO that jointly trains LLMs on labeled and unlabeled instances, taking advantage of human annotations whenever available. Unlike self-consistency applied at inference time, ScPO does not increase inference-time compute, but they can also be combined together for better performance. In our experiments using Llama-3 8B models (Dubey et al., 2024), we show that even without access to any gold answers during training, two iterations of unsupervised ScPO improves zero-shot accuracy of the base model by 22.74%percent22.7422.74\%22.74 % and 5.26%percent5.265.26\%5.26 % (absolute) on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) respectively, closely matching the performance (<1%absentpercent1<1\%< 1 % difference) of the supervised baseline from Pang et al. (2024). Moreover, when supplied with the gold labels in the training set and additional model-generated problems, semi-supervised ScPO improves GSM8K accuracy over the supervised baseline by 2.35%percent2.352.35\%2.35 %. On challenging logical puzzles in ZebraLogic (Dziri et al., 2024) – where only test puzzles (without solutions) are publicly available – training Llama-3 8B with ScPO improves puzzle accuracy by 6.5%percent6.56.5\%6.5 %, outperforming larger LLMs such as Llama-3 70B, Gemma-2 27B (Team et al., 2024), and Claude-3 Haiku (Anthropic, 2024)."
https://arxiv.org/html/2411.04093v1,Summarization of Opinionated Political Documents with Varied Perspectives,"Global partisan hostility and polarization has increased, and this polarization is heightened around presidential elections. Models capable of generating accurate summaries of diverse perspectives can help reduce such polarization by exposing users to alternative perspectives. In this work, we introduce a novel dataset and task for independently summarizing each political perspective in a set of passages from opinionated news articles. For this task, we propose a framework for evaluating different dimensions of perspective summary performance. We benchmark 10 models of varying sizes and architectures through both automatic and human evaluation. While recent models like GPT-4o perform well on this task, we find that all models struggle to generate summaries faithful to the intended perspective. Our analysis of summaries focuses on how extraction behavior depends on the features of the input documents.","People who hold political ideologies tend to develop misperceptions of groups with opposing political opinions Chambers et al. (2006), and political events, such as the 2024 US presidential election, French legislative election, or the Brexit referendum, reinforce negative attitudes Hanna et al. (2013); Wellings et al. (2024). These misperceptions, reinforced by news consumption on social media Levy (2021), contribute to increased polarization and instability Braley et al. (2023); Lees and Cikara (2021). Exposure to alternative perspectives, however, has been shown to help alleviate polarization Balietti et al. (2021). To encourage such exposure, some groups focus on aggregating (e.g., All Sides111https://www.allsides.com/) or summarizing different political perspectives on divisive issues (e.g., The Flip Side 222https://www.TheFlipSide.io/, Ground News333https://ground.news/). Large language models’ (LLMs) ability to summarize opinions and news has recently neared human performance Zhang et al. (2023a); Bhaskar et al. (2023). Recent work, however, has also shown that LLMs can unfairly represent diverse opinions in review and tweet summarization Zhang et al. (2023b); Huang et al. (2023); Tay (2019). Figure 1: Example of the Mixed-Perspective Setting of PoliSum. Given a set of opinionated passages reflecting a mix of views, PoliSum requires models to generate both a summary of the left and right-leaning political perspectives. To evaluate current models’ capabilities in summarizing different perspectives, we propose a novel dataset and task, PoliSum444We make our dataset available to researchers who have been granted permission by The Flip Side.. Our task involves generating independent abstractive summaries of both left and right-leaning political perspectives given collections of opinionated news passages with mixed perspectives. PoliSum thus requires systems to both distinguish the opinions reflected in input texts as well as produce a pair of summaries representing those differing opinions. We summarize our primary contributions as follows: 1. We introduce a novel benchmark dataset, PoliSum, for independently summarizing political perspectives on controversial issues, provided passages from news and op-eds. 2. We introduce an initial framework for evaluating different components of perspective summaries, including both human evaluation and automatic metrics. 3. We present and discuss benchmark evaluation results, finding that models struggle with faithfulness to the intended perspective and that improving automatic evaluation metrics is an important area for future work. 4. We analyze model extraction behavior (e.g., which documents models extract from and how much) and find that generated summaries suffer from biases due to input document position, document length, and use of arousing terms."
https://arxiv.org/html/2411.04090v2,A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement,"Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content—an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model’s inherent uncertainty in predicting toxicity and disagreement. The framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.","Content moderation (CM) has been an important pillar in maintaining ethical online interactions. Given the large amount of user-generated text, CM systems often employ moderation algorithms [1] to combat the spread of online toxicity. Over the past decade, much research on toxicity detection in text data has leveraged the use of machine learning (ML) models, which have been shown to suffer from reliability and robustness issues, wrong predictions due to spurious lexical features [2] and biases [3]. Robustness and reliability are cornerstones, especially in sensitive areas such as CM, where subjectivity and context significantly influence the results [1]. As observed in a recent survey on toxicity detection [4], most research contributions reported in this area to date have largely overlooked a key element in decision-making: the estimation of the model’s confidence in its prediction. In the few cases when uncertainty quantification (UQ) has been considered [5], models have shown remarkable improvements in robustness. UQ not only optimizes the accuracy of predictions, but also facilitates human moderation. By identifying the least reliable predictions, UQ techniques allow moderators to focus their attention on cases where the model exhibits the highest uncertainty, thus optimizing the use of time and resources devoted to human review. Although incorporating uncertainty has advanced ML models in different disciplines, such as traffic modelling [6] or medical imaging [7], it is crucial to recognize that tasks like toxic language detection, which are inherently subjective, require a different perspective due to their complexity. Disagreements among annotators during labelling processes often occur in content moderation reflecting this subjectivity, which can lead to bias and, in some cases, to the censorship of minority opinions. The quantification of inter-annotator disagreement is thus a valuable source of information about the complexity inherent in comments, and is directly related to the aleatoric uncertainty present in the large databases whose inputs require moderation. Annotation disagreement quantification, together with the epistemic uncertainty of the modelling process, allow us to identify the cases when the model cannot provide a robust prediction, as well as those when, due to their complexity and subjectivity, the intervention of a human moderator is necessary. Accounting for both cases is critical, as ML models trained with a majority vote disregard the inherent ambiguity of comments. Unfortunately, considering different annotation perspectives alongside the data modelling pipeline has been observed not to scale well in diverse annotation processes [8]. Additionally, as toxic language evolves over time, it is important to endow content moderators with the tools to make final decisions. This manuscript builds upon the above observations. Specifically, we propose a novel modelling framework that unifies the primary task of toxic language classification with the auxiliary task of annotation disagreement quantification through a multitask framework architecture. The proposed approach is augmented with UQ for both tasks, using Inductive Conformal Prediction (CP) for this purpose. This proposal is grounded on a previous study by Fornaciari et al. [9], which observed that unifying annotation disagreement and classification can be beneficial for the performance of the primary task. Finally, it aims to address a need highlighted by Gillespie [10], who stated: “Perhaps, automated tools are best used to identify the bulk of the cases, leaving the less obvious or more controversial identifications to human reviewers, as societies hold together not by reaching perfect consensus, but by keeping their values under constant and legitimate reconsideration”. The proposed framework is evaluated over an experimental setup designed to answer with evidence three research questions (RQ): • RQ1: How does the integration of annotation disagreement prediction as an auxiliary task influence the performance and calibration of toxicity detection as the primary task? Conversely, what is the impact of the primary task on the auxiliary task? • RQ2: In what ways does predicting annotation disagreement affect the quantification and interpretation of uncertainty in the toxicity detection task? How does the integration of the primary task influence uncertainty quantification and interpretation in the auxiliary task? • RQ3: What are the benefits, if any, of incorporating an auxiliary task into the toxicity identification process compared to treating them as separate tasks, in terms of robustness and the need for human moderation or revision? These three research questions aim to analyze the improvements the multitask approach yields compared to simple single-task models, or, in the case of the full framework, compared to performing quantification separately using a composite of single-task models (CoM). The rest of the manuscript is organized as follows: Section 2 provides an overview of content moderation and CP on text data, and highlights our main contributions. Section 3 describes the materials and methods employed in our experiments. In Section 4, we outline the experimental setup used to address the RQs. Section 5 presents the experimental results and includes a discussion of key findings. Finally, Section 6 summarizes the contributions, key insights and limitations, and suggests avenues for future research."
https://arxiv.org/html/2411.04075v1,M3SciQA: A Multi-Modal Multi-Document Scientific QABenchmark for Evaluating Foundation Models,"Existing benchmarks for evaluating foundation models mainly focus on single-document, text-only tasks. However, they often fail to fully capture the complexity of research workflows, which typically involve interpreting non-textual data and gathering information across multiple documents. To address this gap, we introduce M3SciQA, a multi-modal, multi-document scientific question answering benchmark designed for a more comprehensive evaluation of foundation models. M3SciQA consists of 1,452 expert-annotated questions spanning 70 natural language processing paper clusters, where each cluster represents a primary paper along with all its cited documents, mirroring the workflow of comprehending a single paper by requiring multi-modal and multi-document data. With M3SciQA, we conduct a comprehensive evaluation of 18 foundation models. Our results indicate that current foundation models still significantly underperform compared to human experts in multi-modal information retrieval and in reasoning across multiple scientific documents. Additionally, we explore the implications of these findings for the future advancement of applying foundation models in multi-modal scientific literature analysis.","In scientific research, the findings presented in a paper often serve as a foundation for further investigation. When studying research papers, researchers typically explore related and cited scholarly works to acquire additional context and insights. Simultaneously, research papers are inherently multi-modal, presenting additional and often important insights in the form of figures and tables. Such properties can pose challenges for AI systems in accurately interpreting and integrating diverse data formats across multiple research papers. Recent studies have showcased foundation models’ remarkable performance across a variety of tasks in scientific literature understanding, including summarization (Goyal et al., 2023; Liu et al., 2023c), document-based question answering (Newman et al., 2023; Zhao et al., 2024; Xu et al., 2024), and scientific figure question answering (Masry et al., 2022; Yue et al., 2023; Lu et al., 2024b). However, current investigations are mostly confined to a single-document or text-only setting, ignoring the multi-modal and multi-document nature of scientific research, where insights are often derived from interpreting interconnected texts, figures, and tables across multiple scholarly works. To address this gap, we introduce M3SciQA, a Multi-Modal, Multi-document Scientific Question Answering benchmark. This benchmark contains 1,452 expert-annotated questions spanning 70 natural language processing (NLP) paper clusters, encompassing 3,066 papers. Each paper cluster comprises of an anchor paper and all its cited papers. Inspired by the common workflow of comparative analysis in scientific research (as illustrated in Figure 1), our benchmark simulates a process in which a finding, derived from a scientific image in the anchor paper, prompts further investigation into a specific referenced paper. This simulation enriches the benchmark by requiring the models to engage in cross-referencing among related documents, setting a new testbed for evaluating foundation models in scientific documents understanding and reasoning (Section 2.1). We evaluate a wide spectrum of open-source and proprietary large language models (LLMs) and large multi-modal models (LMMs). Our experimental results reveal significant limitations in both open-source and proprietary LMMs, particularly in their ability to translate and interpret scientific images and perform effective re-ranking based on these images, with the best-performing model, GPT-4o, achieving a Mean Reciprocal Rank (MRR) of 0.488 compared to a human expert score of 0.796, corresponding to a performance gap of 0.308. Similarly, we observe that both open-source and proprietary LLMs struggle with long-range retrieval tasks, specifically with extracting and analyzing information from one or more academic documents. Here, the best-performing model, Command R+, achieves an accuracy score of 33.25 compared to an human expert accuracy score of 76.56111Human expert performance is assessed in the setting where the correct reference paper is known.. These findings underscore the challenges that current models face in handling complex, multi-modal, multi-document, and domain-specific information. Our main contributions are as follows: • We introduce M3SciQA, a comprehensive benchmark designed to evaluate the multi-modal reasoning abilities in interpreting multiple scientific documents. • We conduct an extensive evaluation covering a wide range of LMMs and LLMs. Our experimental results reveal a noticeable performance gap between foundation models and human experts. • To better understand the limitations of current foundation models, we conduct a detailed analysis of scientific figure information retrieval, long-context re-ranking, and long-range retrieval, providing valuable insights for future advancements of foundation models. Figure 2: An overview of M3SciQA question construction pipeline."
https://arxiv.org/html/2411.04025v1,Prompt Engineering Using GPT for Word-Level Code-Mixed Language Identification in Low-Resource Dravidian Languages,"Language Identification (LI) is crucial for various natural language processing tasks, serving as a foundational step in applications such as sentiment analysis, machine translation, and information retrieval. In multilingual societies like India, particularly among the youth engaging on social media, text often exhibits code-mixing, blending local languages with English at different linguistic levels. This phenomenon presents formidable challenges for LI systems, especially when languages intermingle within single words. Dravidian languages, prevalent in southern India, possess rich morphological structures yet suffer from under-representation in digital platforms, leading to the adoption of Roman or hybrid scripts for communication. This paper introduces a prompt based method for a shared task aimed at addressing word-level LI challenges in Dravidian languages. In this work, we leveraged GPT-3.5 Turbo to understand whether the large language models is able to correctly classify words into correct categories. Our findings show that the Kannada model consistently outperformed the Tamil model across most metrics, indicating a higher accuracy and reliability in identifying and categorizing Kannada language instances. In contrast, the Tamil model showed moderate performance, particularly needing improvement in precision and recall.","Language Identification (LI) [1] is a fundamental task in natural language processing (NLP) that involves determining the language(s) present in a given text. This task is pivotal for numerous applications such as sentiment analysis, machine translation, information retrieval, and natural language understanding. Accurate LI becomes particularly challenging in multilingual societies where texts often exhibit code-mixing, a phenomenon where multiple languages co-occur within the same discourse, ranging from phrases to individual words. In the context of India, a country renowned for its linguistic diversity [2, 3, 4, 5], social media platforms reflect a vibrant mix of languages. Among the youth, in particular, there is a prevalent use of code-mixed text that blends local languages from the Dravidian language family with English. Dravidian languages, spoken predominantly in southern India, including languages like Kannada, Tamil, Malayalam, and Tulu, are characterized by rich morphological structures and diverse linguistic features. However, despite their significance, these languages face technological challenges, such as inadequate digital representation and script variations, which complicate language processing tasks like LI. This paper focuses on addressing the specific challenges of word-level LI in Dravidian languages, leveraging the unique linguistic characteristics and code-mixed nature prevalent in social media and digital communications. We introduce a prompt engineering based method aimed at advancing LI capabilities in these languages by experimenting at different temperature values. By doing so, we aim to contribute to the broader goal of enhancing NLP tools for under-resourced languages, ultimately facilitating more accurate and inclusive language processing technologies. In this work, we leveraged GPT-3.5 Turbo [6, 7, 8, 9, 10] to understand whether the large language models is able to correctly classify words into correct categories. We experiment with GPT at different temperature values namely 0.7, 0.8, and 0.9. GPT models are trained on large corpora from the internet, but the availability of high-quality data in Dravidian languages is limited compared to more widely spoken languages like English, Spanish, or Chinese. This means that GPT might not have been exposed to as much diverse or extensive data in these languages. Dravidian languages use distinct scripts (e.g., Tamil script for Tamil, Kannada script for Kannada). Moreover, code-mixing (where Dravidian languages are mixed with English or Hindi, often using the Roman script) is common on social media and informal communications. GPT’s ability to handle code-mixed text varies and may not be as robust as its handling of pure English text. We observe that for Tamil and Kannada, GPT models have significant room for improvement."
https://arxiv.org/html/2411.03966v1,WorryWords: Norms of Anxiety Association for over 44k English Words,"Anxiety, the anticipatory unease about a potential negative outcome, is a common and beneficial human emotion. However, there is still much that is not known, such as how anxiety relates to our body and how it manifests in language. This is especially pertinent given the increasing impact of anxiety-related disorders. In this work, we introduce WorryWords, the first large-scale repository of manually derived word–anxiety associations for over 44,450 English words. We show that the anxiety associations are highly reliable. We use WorryWords to study the relationship between anxiety and other emotion constructs, as well as the rate at which children acquire anxiety words with age. Finally, we show that using WorryWords alone, one can accurately track the change of anxiety in streams of text. The lexicon enables a wide variety of anxiety-related research in psychology, NLP, public health, and social sciences. WorryWords (and its translations to over 100 languages) is freely available. http://saifmohammad.com/worrywords.html","Anxiety is the apprehensive uneasiness or nervousness due to some anticipated or potential negative outcome. Even though it is often considered to be an unwanted and negative feeling, psychologists have long posited its evolutionary benefits. As a species, we have benefited from feeling anxious: for example, building safe dwellings to avoid being attacked by wild animals and poisonous insects, being respectful and kind to others to avoid social exclusion, and so on Bateson et al. (2011). Yet, there is much we do not know such as the mechanisms of how anxiety develops in our bodies and mind, how normal and clinical anxiety manifest in language, and effective mechanisms to cope with excessive anxiety. Further, the dramatic technological advances over the last few decades have rapidly and substantially changed our environments to such an extent (for instance, the widespread social media usage and human–technology interactions) that there is now a considerable mismatch between the current environment and what our anxiety response slowly evolved to address over millennia Blease (2015); Salecl (2004); Qureshi-Hurst (2022). This has led some to argue that we are in an ‘age of anxiety’ Parish (2000); Blease (2015). Anxiety has always been a part of normal human life. Some might feel anxious occasionally and in particular circumstances (state anxiety), and some might have a more diffuse sense of general anxiety over long periods of time (trait anxiety). Some of the major sources of anxiety include tests, sports, technology, and social anxiety Zeidner and Matthews (2010). More recently researchers have also identified a growing amount of climate change anxiety Clayton (2020). Mental health practitioners clinically diagnose anxiety disorders when the sense of anxiety is so high or so frequent that it impacts the day-to-day functioning of an individual. In the US alone, 6.8 million adults (3.1% of the population) are believed to have Generalized Anxiety Disorder (GAD), 15 million adults (7.1%) have Social Anxiety Disorder (SAD), 7.7 million adults (3.6%) suffer from Post-traumatic Stress Disorder (PTSD), and everyone experiences stress from time to time.111https://adaa.org/understanding-anxiety/facts-statistics Anxiety disorders are highly co-morbid with depression Kalin (2020), and together they constitute the internalizing disorders. In addition to impacting well-being and happiness, anxiety (both normal and clinical) has been shown to impact cognitive outcomes such as learning, accurately performing tasks, productivity, as well as our physical bodies Kim and Gorman (2005). In contrast, calmness (being in a state of peace and tranquility) has been shown to improve health and cognitive outcomes Siddaway et al. (2018). Siddaway et al. (2018) also show the importance of conceptualizing anxiety as a continuum from high calmness to high anxiety; and that both promoting calmness and decreasing excessive anxiety are important for overall health and well-being. Thus, anxiety and calmness are active areas of research in Psychology, Medicine, Public Health, Social Science, Economics, and even the Humanities. As is the case with many emotions, people express anxiety through language (consciously and unconsciously). In this work, we study how anxiety and calmness are associated with word meaning. We asked over a thousand respondents to rate 44,450 English words on a scale from −33-3- 3 (associated with high calmness) to +33+3+ 3 (associated with high anxiety). We aggregated the responses to create a word–anxiety association lexicon that has a real-valued anxiety score for each of the words. We refer to this resource by the name WorryWords. We show that the scores in WorryWords are highly reliable (a split-half reliability score of 0.82, which is in-line with scores for existing resources for other emotions). We use WorryWords to study how anxiety is related to other emotion constructs (valence, arousal, dominance, fear, anger, sadness, etc.). We show the rate at which anxiety and calmness words are acquired with age. Finally, we show that WorryWords can be used with simple word-counting methods to accurately track anxiety in text streams. We make WorryWords, and its translations in over 100 languages, freely available for research through the project home page.222http://saifmohammad.com/worrywords.html Some areas where we envisage its use include: 1. Understanding anxiety and the underlying mechanisms; how anxiety relates to other emotions; how it relates to our body; how anxiety changes with age, socio-economic status, weather, green spaces, etc. 2. Determining how anxiety manifests in language; how language shapes anxiety; how culture shapes the language of anxiety; etc. 3. Tracking the degree of anxiety towards targets of interest such as climate change, government policies, biological vectors, etc. 4. Identifying effective coping mechanisms and clinical interventions to manage anxiety. 5. Developing automatic systems for detecting anxiety; developing chat systems that are sensitive to nuances and diverse expressions of anxiety by people from various demographics. 6. Studying anxiety and uneasiness in story telling; its relationship with central elements of narratology such as conflict and resilience. 7. Studying how anxiety impacts social behaviour in physical and virtual environments. We also created an extensive list of ethical considerations (Section 9) that must be taken into consideration when using WorryWords."
https://arxiv.org/html/2411.03964v1,What Really is Commonsense Knowledge?,"Commonsense datasets have been well developed in Natural Language Processing, mainly through crowdsource human annotation. However, there are debates on the genuineness of commonsense reasoning benchmarks. In specific, a significant portion of instances in some commonsense benchmarks do not concern commonsense knowledge. That problem would undermine the measurement of the true commonsense reasoning ability of evaluated models. Davis (2024) suggested that the problem originated from a blurry concept of commonsense knowledge, as distinguished from other types of knowledge. To demystify all of the above claims, in this study, we survey existing definitions of commonsense knowledge, ground into the three frameworks for defining concepts Murphy (2004), and consolidate them into a multi-framework unified definition of commonsense knowledge (so-called consolidated definition). We then use the consolidated definition for annotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets to examine the above claims. Our study shows that there exists a large portion of non-commonsense-knowledge instances in the two datasets, and a large performance gap on these two subsets where Large Language Models (LLMs) perform worse on commonsense-knowledge instances.","Commonsense datasets have been well developed in Natural Language Processing since the last decade. As commonsense data is known to be implicit, almost all commonsense datasets are constructed through crowd source human annotation instead of relying on automated dataset construction processes. These commonsense datasets serves as valuable resources to augment AI models in various aspects, such as text generation Zhou et al. (2021); Ilievski et al. (2021b), visual reasoning Zellers et al. (2019a), or building more capable knowledge models for further downstream applications Yu et al. (2022); Hwang et al. (2021); Wang et al. (2023a), as well as benchmarks to evaluate the reasoning capability of AI models Talmor et al. (2019); Zhang et al. (2020); Bhagavatula et al. (2020); Talmor et al. (2022); Fang et al. (2023a). However, there are debates on the quality of commonsense datasets, especially when they serves as evaluation benchmarks. Davis (2024) argued that many prevalent commonsense datasets are flawed in the sense that they contained a significant portion of instances which do not concern commonsense knowledge but other types of knowledge, namely common, encyclopedia, and expert knowledge (so-called referenced in this work). For example, in the CommonsenseQA 2.0 dataset Talmor et al. (2022) which consists of Yes/No questions (or assertions), the instance “A male seahorse cannot give birth” (Answer: no) presents common biology knowledge, meanwhile, “Electrons are smaller than mesons” (Answer: no) is certainly an encyclopedic fact. As it has been widely discussed that language models excel in memory or retrieval tasks while still struggle in reasoning tasks Bang et al. (2023); Goldberg (2023); Huang and Chang (2023), the flaw in commonsense datasets would undermine the measurement of the true commonsense reasoning ability of evaluated models. Davis (2024) suggested that the problem originated from a blurry concept of commonsense knowledge, as distinguished from other types of knowledge. Due to that blurry concept, both annotators and researchers working on commonsense may not be aware of the genuineness problem of commonsense datasets. Indeed, according to our literature review, all works on commonsense have their ways to describe the concept. However, the description of each is not adequate and comprehensive to educate outsiders or even insiders of this research field about the concept and the difference with respect to other relevant concepts, such as referenced knowledge. Also, through the lens of concept definition theory Murphy (2004), we posit that each of the existing literature on commonsense only provides a limited number of features, making the concept commonsense not systematically and comprehensively depicted. Regarding the problems, Murphy (2004) suggested combining general description, examples, and list of features of the concept to form a complete definition. Motivated by that research gap, in this work, we consolidate the definition of commonsense as follow. Firstly, leveraging the descriptions about commonsense and referenced knowledge from previous works, we attempt to distinguish commonsense knowledge from referenced knowledge. We provide a table of representative cases for each concept to show (or more subjectively, assume) the fundamental and subtle difference of these two concepts. Based on the descriptions and examples, we systematically propose a list of multi-aspect binary-value features that characterize commonsense, referenced knowledge, and their difference. We then validate the significance of features through an empirical study on the CommonsenseQA Talmor et al. (2019) and CommonsenseQA 2.0 Talmor et al. (2022) datasets. Overall, we observe that 1) whether we can obtain the knowledge by our own experience/observation and 2) whether the knowledge is only mutual belief are the most significant features to identify commonsense from referenced knowledge. Given the consolidated definition, we analyze the portion of instances of commonsense and referenced knowledge in the development sets of the CommonsenseQA and CommonsenseQA 2.0 datasets, as well as the performance of Large Language Models (LLMs) such as Gemini-Pro, ChatGPT, LLaMa2-7B, and Mixtral-8x7B on the commonsense-knowledge subsets (which consists of instances required commonsense knowledge to answer) and referenced-knowledge subsets from these datasets. Aligned with the claims which motivate for this work, we observe a large portion of referenced knowledge in the two datasets (0.27 ± 0.09 for CommonsenseQA and 0.56 ± 0.1 for CommonsenseQA 2.0)11195% confidence interval, and a large performance gap (4 to 7 point of accuracy) on these two subsets, where LLMs perform worse on commonsense-knowledge instances, suggesting that commonsense reasoning tasks or reasoning tasks in general are more challenging than memory-retrieval tasks. The organization of this paper is as follows. In section 2, we discuss related works (especially background knowledge on the frameworks to define a concept in Murphy (2004)). In section 3, we show a comprehensive survey on the definitions of commonsense knowledge by grounding relevant previous works into three aforementioned definition frameworks, then provide a table of representative cases. After that, we describe the list of features and validation procedure. Finally, in section 4, we apply the consolidated definition to demystify relevant claims which motivate this work."
https://arxiv.org/html/2411.03962v1,How Does A Text Preprocessing Pipeline Affect Ontology Syntactic Matching?,"The generic text preprocessing pipeline, comprising Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been implemented in many ontology matching (OM) systems. However, the lack of standardisation in text preprocessing creates diversity in mapping results. In this paper, we investigate the effect of the text preprocessing pipeline on OM tasks at syntactic levels. Our experiments on 8 Ontology Alignment Evaluation Initiative (OAEI) track repositories with 49 distinct alignments indicate: (1) Tokenisation and Normalisation are currently more effective than Stop Words Removal and Stemming/Lemmatisation; and (2) The selection of Lemmatisation and Stemming is task-specific. We recommend standalone Lemmatisation or Stemming with post-hoc corrections. We find that (3) Porter Stemmer and Snowball Stemmer perform better than Lancaster Stemmer; and that (4) Part-of-Speech (POS) Tagging does not help Lemmatisation. To repair less effective Stop Words Removal and Stemming/Lemmatisation used in OM tasks, we propose a novel context-based pipeline repair approach that significantly improves matching correctness and overall matching performance. We also discuss the use of text preprocessing pipeline in the new era of large language models (LLMs).","Ontology matching (OM), also known as ontology alignment, is essential to enable interoperability between heterogeneous ontologies. An OM process usually takes two ontologies as input, discovers mappings between entities, and produces a set of correspondences [1]. A classical OM system usually contains syntactic, lexical, and semantic matching. This multilayer architecture has been implemented in several successful OM systems, such as LogMap [2][3], AgreementMakerLight (AML) [4][5], and FCA-Map [6][7]. Syntactic matching captures “anchor mappings”, providing the foundation for the latter lexical and semantic matching. There are many strategies to extract syntactic information from an ontology entity, including the older approach of Bag-of-Words (e.g. TF-IDF [8]), popular word embedding models (e.g. Word2Vec [9]), and state-of-the-art language models (e.g. BERT [10]). Despite the diversity of the models used, they all apply text preprocessing for cleaning the text data before fitting it into the model. Fig. 1 shows an example of using a classical text preprocessing pipeline to process the ontology entity “cmt:reviewerBiddingStartedBy”. The text preprocessing pipeline consists of a set of steps to segment, reconstruct, analyse, and process the information in the text, namely Tokenisation, Normalisation, Stop Words Removal, and Stemming/Lemmatisation [11]. Tokenisation is the process of breaking the text into the smallest units (i.e. tokens). We use whitespace to split the tokens in the example. Normalisation is the process of transforming these different tokens into a single canonical form. Stop Words Removal is the process of removing filler words that usually carry little meaning and can be omitted in most cases. Stemming/Lemmatisation is used to deal with the grammatical variation of words, applying rules to find the simplest common form of the word. This helps to capture the key information from the text and therefore improves efficiency. Figure 1: Text preprocessing pipeline used in syntactic matching. While a number of OM systems use the text preprocessing pipeline for syntactic OM, few studies explain why a specific method is selected for a certain OM task. Our study tackles the challenge in two ways. Firstly, we conduct a comprehensive experimental analysis of the text preprocessing pipeline in syntactic OM across a wide range of domains, aiming to explain the behaviour of the text preprocessing pipeline in OM tasks at syntactic levels. In each phase, a text preprocessing method is evaluated for its correctness and completeness. Secondly, we propose a novel context-based pipeline repair approach for syntactic OM. The method offers a customised way to fine-tune the text preprocessing pipeline for each domain-specific OM task and shows promising results for repairing false mappings. We also discuss the non-deterministic nature of using LLMs for text preprocessing and highlight the importance of utilising the classical text preprocessing pipeline. Specifically, this paper makes the following contributions: • (Section IV) We categorise the text preprocessing pipeline used in syntactic OM into two phases. We find a significant improvement using Phase 1 text preprocessing methods. In contrast, Phase 2 text preprocessing methods are currently less effective. We compare the performance of (1) Stemming and Lemmatisation, (2) different stemming methods (Porter, Snowball, and Lancaster), and (3) Lemmatisation and Lemmatisation + Part-of-Speech (POS) Tagging. We find that inappropriate stop words removal, over-stemming, and over-lemmatisation are common on 8 Ontology Alignment Evaluation Initiative (OAEI) [12] track repositories with 49 distinct alignments. • (Section V) We propose a simple and intuitive context-based pipeline repair method. This method is evaluated on the same OM tasks we analysed, showing promising results to improve the correctness of syntactic OM when inserted in the pipeline repair before Phase 2 text preprocessing methods. • (Section VI) We discuss the use of text preprocessing pipeline in the era of LLMs. We find LLMs cannot perform syntactic matching without text preprocessing, but they can be used as an alternative to repair the false mappings. The remainder of the paper is organised as follows: Section II and Section III review the related work and the preliminaries of OM task formulation. Section IV analyses the text preprocessing pipeline used in OM. Section V proposes the context-based pipeline repair approach and experimentally validates its performance. Section VI demonstrates the pros and cons of using LLMs for text processing in OM. Section VII and Section VIII summarise the findings and limitations of this work. Section IX concludes the paper."
https://arxiv.org/html/2411.03923v1,Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?,"Evaluation data contamination, the inadvertent mixing of samples from evaluation benchmarks into pre-training corpora, constitutes a recently growing and important concern in the field of evaluating large language models (LLMs). The resulting ‘training on the test set’ makes it difficult to interpret evaluation benchmark scores, and an active area of research studies its effects. However, while evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which examples should be considered contaminated and, consequently, to what extent contamination inflates the corresponding benchmark scores. In this paper, we propose that these questions should be addressed together and that contamination metrics can be assessed based on whether the examples they mark contaminated indeed give models an undue advantage. We propose a novel analysis method called ConTAM, and show – in a large scale survey of existing and novel n𝑛nitalic_n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families – that ConTAM can be used to better understand evaluation data contamination as well as its effects on benchmark scores. We find that contamination may have a much larger effect than reported in recent LLM releases and that there are differences in the extent to which models at different scale are impacted by contamination. Furthermore, we find that considering only the longest contaminated substring generally provides a better signal than considering a union of all contaminated substrings, as was common practice in previous studies, and that doing model and benchmark specific threshold analysis greatly increases the specificity of the results. Lastly, we investigate the impact of various hyperparameter choices of contamination studies, finding that – among other things – both using larger values of n𝑛nitalic_n and disregarding contaminated strings that are infrequent in the pre-training data lead to many false negatives. With ConTAM, we provide a method to empirically ground evaluation data contamination metrics in downstream effects as well as measure their magnitude. With our exploration, we shed light on how evaluation data contamination can impact LLMs and provide insight into the various considerations important when doing contamination analysis. We end our paper by discussing these in more detail and providing concrete suggestions for future work.","As large language models (LLMs) improve, their evaluation becomes increasingly challenging. A frequent concern in this context (Sainz et al., 2024a, 2023; Jacovi et al., 2023; Elazar et al., 2024, i.a.) is that samples from evaluation benchmarks sometimes (partially) appear in the data used to train that same model, also known as evaluation data contamination. Such inadvertent ‘training on the test set’ makes it difficult to interpret scores on evaluation benchmarks: does a better score reflect an improvement on the ability that the benchmark intends to measure or does it indicate that a model succesfully memorised the benchmark data? Answering this seemingly simple question is far from straightforward. Even setting aside practical issues such as the sheer size of pre-training corpora and the fact that they are not often publicly available, it is surprisingly difficult to define when precisely a sample in an evaluation benchmark should be considered contaminated. Does the entire sample need to be verbatim present in the training data, or should partial appearance also count? If so, how do we define partial appearance, and how do we ‘threshold’ it? In addition to these measurement questions, there is the intertwined question of how much contamination really impacts benchmark scores. If an example occurs verbatim once in a 1.4T token training corpus, does this in fact give the model an unfair advantage? In this paper, we propose a novel methodology for assessing contamination metrics that we call the Contamination Threshold Analysis Method (ConTAM). ConTAM is centered around the concept of Estimated Performance Gain (EPG), and addresses the former two questions jointly. Motivated by the idea that an adequate metric of contamination returns the set of samples for which a model in fact has an undue benefit, it compares metrics based on the measurable impact on benchmark performance of the samples the metrics flag as contaminated. In doing so, it simultaneously provides handholds to make empirical choices regarding contamination metrics and provides information on answers the question of the extent to which contamination inflates benchmark scores. We use ConTAM to compare four main contamination metrics – three from the literature and one novel – that define contamination in terms of overlapping n𝑛nitalic_n-gram spans (termed “matches”) between a sample in the evaluation benchmark and the pre-training corpus. We compare these metrics across 5 parameter settings, on 13 benchmarks, for 7 models of various sizes trained from 2 pre-training corpora. Our comparisons illustrate how ConTAM can help provide insights into how evaluation data contamination impacts model performance, help distinguish contamination metrics, and select hyper-parameters for those metrics. We conduct extensive experimentation across these metrics and corresponding hyperparameter choices. Our main conclusions are: • The impact of evaluation data contamination has been underestimated in many prominent LLM releases, likely because of false negatives in the chosen contamination metrics (Table 2, Figures 3, 4 and 10) • While there is no true one-size-fits-all approach to contamination detection, using the longest contaminated substring rather than a union of all matches works better across the board, adequately detecting contamination in cases where no other metric did (Figures 2 and 13) • For virtually all benchmarks we considered, smaller n𝑛nitalic_n is better and even one occurrence in the pre-training data matters: both using values of n𝑛nitalic_n larger than 8 and setting a minimal count higher than one for occurrence in the pre-training corpus leads to false negatives (Figure 6 and 7(b)); • The impact of contamination changes with scale, in cases where there still is performance to be gained, larger models are better able to leverage contamination than smaller models (Figure 5, 8 and 9); • Consequently, to find the most adequate contamination metric, it is important to do model-specific threshold selection (Figure 1). With our work, we aim to contribute to an informed discussion about evaluation data contamination and its effects, as well as provide methodology for both researchers and practitioners to explore this question further and contrast the effect of different contamination metrics and hyper-parameters as newer pre-training corpora (Soldaini et al., 2024; Penedo et al., 2024) and benchmarks (Laurent et al., 2024) are released. Outline In the remainder of this paper, we first review earlier work on contamination detection, and consider their results and limitations (§ 2). Next, we describe the various contamination metrics we investigate in more detail and list the benchmarks, models and pre-training corpora we use for our study (§ 3), followed by a description of our main methodology for analysis (§ 4). After that, in our main results section (§ 5), we first report the results of an overall comparison between all contamination metrics (§ 5.1). Both numerically and qualitatively, we show how different contamination metrics provide different signal across datasets, and we show that taking the longest matching n𝑛nitalic_n-gram, rather than a union of all n𝑛nitalic_n-grams, detects the most meaningful EPG across datasets and models. Next, in § 5.2, we discuss how much contamination is detected across various benchmarks (§ 5.2.1), how much that impacts benchmark scores for various models (§ 5.2.2) and how that depends on model scale (§ 5.2.3). Consecutively, in § 6, we investigate how various parameters that frequently occur in contamination metrics impact the results. Specifically, we consider the impact of n𝑛nitalic_n (§ 6.1), the impact of the mismatch budget (§ 6.2) and the frequency of matches in the pre-training corpus (§ 6.3). We wrap up with a conclusion in which summarise our results and provide concrete recommendations for practitioners (§ 7) and a discussion in which we consider the limitations of our work (§ 8)."
https://arxiv.org/html/2411.03920v1,RAGulator: Lightweight Out-of-Context Detectors for Grounded Text Generation,"Real-time detection of out-of-context LLM outputs is crucial for enterprises looking to safely adopt RAG applications. In this work, we train lightweight models to discriminate LLM-generated text that is semantically out-of-context from retrieved text documents. We preprocess a combination of summarisation and semantic textual similarity datasets to construct training data using minimal resources. We find that DeBERTa is not only the best-performing model under this pipeline, but it is also fast and does not require additional text preprocessing or feature engineering. While emerging work demonstrates that generative LLMs can also be fine-tuned and used in complex data pipelines to achieve state-of-the-art performance, we note that speed and resource limits are important considerations for on-premise deployment.","In enterprise settings, Generative AI has received widespread adoption as a tool to uplift employees’ productivity [1]. Enterprises require a high degree of factuality of generated answers in popular language tasks, such as question-answering (QA) and summarisation. Two methods can enable Large Language Models (LLMs) to interact with users based on industry-specific knowledge. The first method involves fine-tuning through strategies such as SFT, RLHF [2] and preference optimisation [3, 4]. Although able to improve performance on domain knowledge, knowledge gained from fine-tuning can quickly become outdated [5], therefore making maintenance costly. The alternative is Retrieval Augmented Generation (RAG) [6], which is better suited for tasks requiring evolving knowledge, such as integration of the latest industry news. However, both methods do not fully circumvent the inherent limitation of LLMs – hallucination, which manifests in inconsistent or fabricated claims [7] that can be subtle and phrased confidently even if factually incorrect [8]. For highly sensitive working environments such as financial institutions, the inability to ensure faithful LLM outputs can be one of the biggest limitations to widespread adoption of LLM applications [9]. To address this barrier to enterprise adoption, we narrow the scope of “hallucination” and focus only on hallucinations that render the LLM response semantically inconsistent with the provided context. This is commonly known as faithfulness hallucination [7, 10, 11], but is sometimes also referred to by terms such as contextual hallucination [12], or a lack of grounded factuality [13] or support [14]; for simplicity and to clearly underscore the nature of such hallucination, we adopt the term out-of-context (OOC). We consider any LLM-generated response to a RAG prompt as semantically OOC if any part of the response is ungrounded based on the retrieved context alone, even if it is otherwise factual according to world knowledge. In contrast, an in-context response is one where every claim embedded in the response can be inferred solely from the retrieved context. Approaches to mitigate hallucination can be broadly classified into 2 types – black-box and grey-box methods. Black-box methods employ strong generative LLMs to assess if a candidate answer is hallucinated, and these LLMs are usually augmented with various prompting or fine-tuning techniques. Examples of black-box methods include RARR [15], WikiChat [16], FreshPrompt [17], SelfCheckGPT [18], RAGAS [10], ChainPoll [19], and Lynx [20]. However, black-box approaches that rely on strong closed-source LLM judges are less suitable for enterprises that are constrained by budget and/or data privacy requirements. Grey-box methods are alternatives which aim to detect hallucination through a proxy metric or model. Grey-box approaches either assess final/hidden LLM states, such as FLARE [21] and Lookback Lens [12], or use a score computed by an independent discriminative model of lower complexity, such as SummaC [22], AlignScore [23], HHEM [24], and Luna [14]. In this work, we propose RAGulator, a series of lightweight OOC detectors for RAG applications. We use a simple data generation pipeline to create a training dataset which simulates both OOC and in-context RAG prompts. This is gathered from public datasets originally constructed for various NLP tasks. Furthermore, we compare 2 types of grey-box ""non-native"" discriminative models – fine-tuned BERT-based classifiers and ensemble meta-classifiers trained on numerical features derived from text. Generative labelling with an LLM annotator is employed where necessary to adapt the training dataset for fine-tuning of the BERT classifiers. We show that while a large LLM can show good agreement with human annotation in labelling data for BERT classifier fine-tuning, our predictive models outperform the same LLM on the OOC detection task by up to 19% on AUROC and 17% on F1 score (deberta-v3-large), highlighting the need for specialised models for OOC detection. Figure 1: Example LLM responses from a RAG system"
https://arxiv.org/html/2411.03895v1,Computational Analysis of Gender Depiction in the Comedias of Calderón de la Barca,"In theatre, playwrights use the portrayal of characters to explore culturally based gender norms. In this paper, we develop quantitative methods to study gender depiction in the non-religious works (comedias) of Pedro Calderón de la Barca, a prolific Spanish 17th century author. We gather insights from a corpus of more than 100 plays by using a gender classifier and applying model explainability (attribution) methods to determine which text features are most influential in the model’s decision to classify speech as ‘male’ or ‘female’, indicating the most gendered elements of dialogue in Calderón’s comedias in a human accessible manner. We find that female and male characters are portrayed differently and can be identified by the gender prediction model at practically useful accuracies (up to f=0.83). Analysis reveals semantic aspects of gender portrayal, and demonstrates that the model is even useful in providing a relatively accurate scene-by-scene prediction of cross-dressing characters.Keywords: automatic classification, Calderón de la Barca, Siglo de oro, machine learning","In Spain, the Baroque period, was a period of immense artistic creativity, genereally known as the ”Golden Age” (siglo de oro). This is particularly true in literature, where the period saw exceptional writers such as Lope de Vega, Tirso de Molina or Pedro Calderón de la Barca. The latter, who lived from 1600 to 1681, is generally considered as as one of the most important playwrights of the age. He was immensely productive, writing a total of over 200 theatrical plays, both secular and religious, which had a lasting impact on Spanish theatre and beyond [fitzmaurice21]. He is particularly known for detailed and complex characterizations in his works [woods2013female]. Not surprisingly, Calderón’s writings have been subject to intense analysis by literary scholars over a long period of time, and topics have moved in and out of fashion. For example, traditional foci of scholarship have been the role of honor and power in the works [greer2017play] or Calderón’s attention to dramatic structure [whitby1954structural]. A relatively new aspect among these is gender depiction, that is, the question of how Calderón conceptualized male and female roles in his plays differently, which has gained global attention in Hispanic Studies since the latter half of the 20th century ([arellano_ayuso_mujeres_2015, saez_veloz_2019, tietz_modo_2017]). Some studies indicate that Calderóns depicts a broadly period-typical understanding of gender identity and gender norms. For example, [ibarreche2020honor] study the representation of male characters, particularly in relation to honor in plays such as El médico de su honra, El Alcalde Zalamea, or El pintor de su deshonra. In these pieces, male characters, often husbands, fathers, or brothers, strive to uphold their social status, wielding power, owning properties, and navigating public spaces with traits such as bravery, patience, and caution [fernandez_guillermo_figura_2017]. Conversely, in plays like Casa con dos puertas mala es de guardar or La dama duende, women are frequently confined to domestic spaces, initiating disputes and deftly manipulating characters like puppets in games of seduction leading to a harmonious resolution [saez_veloz_2019]. Meanwhile, other studies have found that Calderón’s work not only mirrors societal norms but also provides a dynamic platform for the exploration of gender identities [tietz_modo_2017]: ’Perverse’ women -women who use deception against men to achieve their goals - are predominantly present in tragic pieces, conspicuously absent in comedies [arellano_ayuso_mujeres_2015]. Furthermore, a significant number of plays depict female characters cross-dressing as men and performing traits typically associated with masculinity [bravo-villasante_mujer_1988, strosetzki_personajes_2017, araico2017lenguaje]. A commonly used trope of the time period, playwrights utilized cross-dressing as a means for female characters to transcend the domestic space and actively participate in both society and the action of the play. The aforementioned scholarship draws attention conflicting tendencies of gender depiction in Calderón’s work: it is both deeply shaped by plot and genre conventions, but is used as a grounds to expand social ideas about gender roles by representing women with agency who drive plot progression and utilize covert ways to exercise their will [woods2013female]. A possible explanation for the picture that emerges is that, throughout the massive catalogue of Calderón’s works, a select few have been given close attention by scholars who adopt a ’close reading’ method focusing on individual dramas sampled of the more than 100 secular dramas written by Calderón. Consequently, studying the all dramas can show if this picture holds true for the oeuvre as a whole. This prompts the classical motivation for scalable reading methods [weitin17] from digital humanities, combining computational models with manual interpretation: Can we make out global patterns in gender depiction that hold across Calderón’s whose body of secular dramas? More specifically, we propose to build on recent developments regarding the explainability of AI models [wiegreffe-pinter-2019-attention, bibal-etal-2022-attention] which aim at providing a human-understandable justification of model decisions. This makes it possible to consider a computational model of a literary phenomenon primarily as an aggregation mechanism that summarizes large amounts of text into a much smaller set of predictions, and then to analyze these predictions back to the linguistic properties that gave rise to them. In this study, we show the feasibility of this research program by training a gender classifier based on the characters’ speech. The motivation here is not to carry out the task of gender classification, but to ask questions about the classifier’s predictions: How difficult is the task — i.e., how consistent is Calderón’s gender depiction? What features does the model rely on for its predictions — i.e., what linguistic patterns does Calderón use for gender depiction? What characters or character groups are easy or difficult to classify — i.e., what characters are depicted in unconventional or unexpected fashions? Our work is not the first to classify characters and their properties based on their speech. For instance, [vishnubhotla-etal-2019-fictional] analyze the ability of classifiers to distinguish among characters within a play to determine if authors were able to write characters with distinct voices. The closest precursor of our study is [bullard-ovesdotter-alm-2014-computational] who predict various demographic properties of characters, including gender. However, their study was still based on hand-picked features fed into a regression model. [savoy2023stylometric] also conducted a classification experiment on the gender of Shakespeare characters based on lines, using random forest and logistic regression. Instead, we work with the more recent language models (LMs) which give rise to two major new design questions. First, LMs promise to do away with feature engineering, but for the purpose we have in mind (explaining the model predictions), not all parts of the input are equally informative. Second, LMs are still known to struggle with identifying relevant information in long texts. This raises the question of how much information to present to the classifier at each time: Too little (e.g., just one line), and the input may contain insufficient hints as to the speaker’s gender; too much (e.g., a character’s complete text from a drama), and the model may struggle. Also, the predictions of the model for each individual part of the input have to be aggregated into a prediction at a more global level, for which we also consider multiple options. We discuss both of these issues in Section 2 below. Our models are able to correctly classify the gender of characters in Calderón’s comedias. Our experiments yield the best results given the most amount of input text, but the models also perform well using an aggregation method that takes into account the confidence of each prediction. We take this as a positive result for computational analysis of gender portrayal in the works of Calderón: it shows that there are recognizable gender cues at the scene level and, at the same time that, characters’ gender roles can vary over the course of a play. Finally, an examination of the most explanatory features in the models predictions demonstrate that the model picks up on both grammatical and semantic information that contribute to gender prediction. The examination of cross-dressing female character demonstrates that these character’s lines are more similar to the speech of male characters compared to more traditional female characters, particularly during cross-dressing scenes."
https://arxiv.org/html/2411.03888v1,"Multi-Everything: Vision–Language Models for Multimodal, Multilingual, and Multicultural Hate Speech Detection","Warning: this paper contains content that may be offensive or upsettingHate speech moderation on global platforms poses unique challenges due to the multimodal and multilingual nature of content, along with the varying cultural perceptions. How well do current vision-language models (VLMs) navigate these nuances? To investigate this, we create the first multimodal and multilingual parallel hate speech dataset, annotated by a multicultural set of annotators, called Multi3Hate. It contains 300 parallel meme samples across 5 languages: English, German, Spanish, Hindi, and Mandarin. We demonstrate that cultural background significantly affects multimodal hate speech annotation in our dataset. The average pairwise agreement among countries is just 74%, significantly lower than that of randomly selected annotator groups. Our qualitative analysis indicates that the lowest pairwise label agreement—only 67% between the USA and India—can be attributed to cultural factors. We then conduct experiments with 5 large VLMs in a zero-shot setting, finding that these models align more closely with annotations from the US than with those from other cultures, even when the memes and prompts are presented in the dominant language of the other culture. Code and dataset are available at https://github.com/MinhDucBui/Multi3Hate.","Our cultural backgrounds significantly shape our perceptions of the world. For instance, individuals raised in collectivist societies often emphasize group harmony, leading them to interpret events through a relational lens, whereas those from individualist societies may prioritize personal achievements and autonomy, resulting in a perception that focuses on individual characteristics Triandis (1995); Nisbett (2003). Consequently, identical content can be perceived vastly differently depending on cultural background, posing challenges for hate speech moderation models as they must balance diverse perspectives without marginalizing certain cultures while favoring others. Figure 1: Our dataset creation process is divided into three stages: 1. Crawling Stage; 2. Translation Stage; and 3. Cross-Cultural Hate Speech Annotation Stage. The two examples illustrate the varying ways in which memes are annotated across different cultures. Towards incorporating this important goal, Lee et al. (2024) released the first and only hate speech dataset, annotated by a multicultural set of annotators, revealing that large language models often exhibit bias toward Anglospheric cultures. However, their work leaves critical gaps unaddressed: (1) The dataset is limited to text-based content, excluding multimodal forms of hate; (2) It is restricted to English-language samples, overlooking non-English-speaking cultures. This narrow scope not only hampers the cross-cultural evaluation of multimodal hate speech detection models, providing little guidance for practitioners, but also amplifies the exclusion of non-English-speaking cultures from cross-cultural analysis. To close this gap, we are the first, to the best of our knowledge, to release a parallel multilingual and multimodal hate speech dataset. Additionally, it is the first multimodal hate speech dataset annotated by a multicultural set of annotators, as shown in Table 1. Our dataset, Multi3Hate, comprises a curated collection of 300 memes—images paired with embedded captions—a prevalent form of multimodal content, presented in five languages: English (en), German (de), Spanish (es), Hindi (hi), and Mandarin (zh). Each of the 1,500 memes (300×\times×5 languages) is annotated for hate speech in the respective target language by at least five native speakers from the same country. These countries were chosen based on the largest number of native speakers of each target language: USA (US), Germany (DE), Mexico (MX), India (IN), and China (CN) Instituto Cervantes (2023); World Population Review (2024). As in prior research, we use the country of the annotators as a cultural proxy EVS/WVS (2022); Koto et al. (2023); Lee et al. (2024). Multi- Multi- Multi- Dataset modal cultural lingual set of (+Parallel) Annotators HateXplain ✗ ✗ ✗ Mathew et al. (2021) XHate-999 ✗ ✗ ✔ Glavaš et al. (2020) (+✔) MMHS150k ✔ ✗ ✗ Gomez et al. (2020) Hateful Memes ✔ ✗ ✗ Kiela et al. (2020) CrisisHateMM ✔ ✗ ✗ Bhandari et al. (2023) MUTE ✔ ✗ ✔ Hossain et al. (2022) (+✗) CREHate ✗ ✔ ✗ Lee et al. (2024) Multi3Hate ✔ ✔ ✔ Ours (+✔) Table 1: Comparison of hate speech datasets across three dimensions: multimodal, multicultural set of annotators, and multilingual, along with whether they are parallel. Our dataset is the first to be both multimodal and multilingual, as well as the first multimodal dataset annotated by a multicultural set of annotators. We demonstrate that cultural background significantly influences multimodal hate speech annotation in our dataset. The average pairwise agreement among countries is only 74%, significantly lower than that of randomly selected annotator groups. The lowest agreement, at just 67%, occurs between the USA and India. Through qualitative analysis involving multicultural annotators with ties to both countries, we demonstrate that these disagreements can be attributed to cultural factors, such as differing social norms. Consequently, Multi3Hate is the first dataset enabling the analysis of multimodal models for cross-cultural hate speech detection across a range of diverse speaking cultures. Furthermore, we conduct experiments using 5 large VLMs in a zero-shot setting. Our experiments with English prompts reveal that these models consistently align more closely with annotations from the US than with those from other cultures, independent of the meme language. Specifically, out of 50 combinations of models, languages, and input variations, 42 demonstrate the highest alignment with US labels. Even when we switch the prompt language to the dominant language of a specific culture, we still observe similarly high alignment to US annotators. We therefore demonstrate that VLMs align more closely with hate speech annotations from the US than with those from non-English-speaking cultures, even when the memes and prompts are presented in the dominant language of the other culture. This trend poses a risk of marginalizing certain cultures, despite VLMs being used in their native languages, while simultaneously privileging US cultural perspectives."
https://arxiv.org/html/2411.03884v1,Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models,"Transformers have found extensive applications across various domains due to the powerful fitting capabilities. This success can be partially attributed to their inherent nonlinearity. Thus, in addition to the ReLU function employed in the original transformer architecture, researchers have explored alternative modules such as GeLU and SwishGLU to enhance nonlinearity and thereby augment representational capacity. In this paper, we propose a novel category of polynomial composition activations (PolyCom), designed to optimize the dynamics of transformers. Theoretically, we provide a comprehensive mathematical analysis of PolyCom, highlighting its enhanced expressivity and efficacy relative to other activation functions. Notably, we demonstrate that networks incorporating PolyCom achieve the optimal approximation rate, indicating that PolyCom networks require minimal parameters to approximate general smooth functions in Sobolev spaces. We conduct empirical experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. By substituting conventional activation functions with PolyCom, we enable LLMs to capture higher-order interactions within the data, thus improving performance metrics in terms of accuracy and convergence rates. Extensive experimental results demonstrate the effectiveness of our method, showing substantial improvements over other activation functions. Code is available at https://github.com/BryceZhuo/PolyCom.","Transformers (Vaswani et al., 2017) have revolutionized the field of deep learning, facilitating unprecedented advancements in natural language processing (Radford et al., 2019), computer vision (Dosovitskiy et al., 2021), and beyond (Dong et al., 2018; Arnab et al., 2021). Characterized by their attention mechanisms, transformers excel at capturing intricate relationships within data, making them indispensable in contemporary machine learning applications. However, despite their widespread success, there remain opportunities for further refinement, particularly concerning the selection of activation functions. The activation function plays a crucial role in determining the output of each neuron within a neural network. Traditionally, simple nonlinearities such as Rectified Linear Unit (ReLU) (Nair & Hinton, 2010) and its variants (Hendrycks & Gimpel, 2016; So et al., 2021) have been favored due to their computational efficiency and ease of implementation. Although effective, these activation functions are inherently limited in their ability to model complex higher-order relationships within data. This limitation can be particularly restrictive in transformer architectures, where the ability to capture subtle and complex dependencies is essential. In this paper, we introduce a novel category of polynomial composition activation functions (PolyCom), specifically engineered to enhance the performance of transformer architectures. In contrast to conventional activation functions, which are predominantly linear or piecewise linear, polynomial composition activations facilitate the modeling of more complex patterns within data. This augmentation in the activation function’s expressiveness endows the model with superior expressive capacity, enabling it to capture higher-order interactions that might otherwise be neglected. Unlike other forms of polynomials ((Hornik et al., 1989; Trefethen, 2019)) that suffer from inadequate approximation, exploding values, and oscillatory behavior, we demonstrate that PolyCom possesses a more potent expressive capability than both ReLU and traditional polynomials and achieves optimal approximation within Sobolev space. We posit that the integration of polynomial composition activations within transformer models can lead to enhanced performance in tasks requiring intricate data interpretation. To evaluate this hypothesis, we conducted comprehensive experiments on the pre-training configurations of large language models (LLMs), including both dense and sparse architectures. These evaluations were performed across various benchmarks, assessing the performance of transformers employing polynomial composition activations in comparison to those utilizing traditional activation functions. The results indicate that the proposed method not only improves model accuracy, but also accelerates convergence rates, thereby suggesting that polynomial composition activations provide a substantive advantage in deep learning applications. The main contributions of this paper are summarized in the following. • We propose a new activation function PolyCom which is a composition of the polynomial and other types of function. In particular, we introduce two instances of PolyCom: PolyReLU and PolyNorm, and details its integration into the transformer architecture. • Theoretically, we derive bounds on the number of trainable parameters required for PolyReLU networks to approximate ReLU networks, and vice versa. Additionally, we show that a PolyReLU network of size O⁢(ϵ−d/n)𝑂superscriptitalic-ϵ𝑑𝑛O(\epsilon^{-d/n})italic_O ( italic_ϵ start_POSTSUPERSCRIPT - italic_d / italic_n end_POSTSUPERSCRIPT ) can approximate any function in Sobolev spaces with error tolerance ϵitalic-ϵ\epsilonitalic_ϵ, achieving optimal approximation rates. • Empirically, we validate the effectiveness of this new activation function on LLMs with both 1B dense models and MoE models with 1B active and 7B total parameters. The results of both models demonstrate that PolyCom can accelerate the converging speed and significantly outperform SwiGLU, GELU, and ReLU et al. The outline of this paper is structured as follows: In Section 2, we present the mathematical formulation of PolyCom and discuss its integration within transformer architectures. Section 3 delivers a comprehensive theoretical analysis of PolyCom, emphasizing its enhanced expressivity and effectiveness. In Section 4, we provide a detailed account of our experimental results involving large language models (LLMs). Section 5 provides an overview of related work in the field of activation functions and their applications in transformer models. Finally, we conclude the paper and outline potential directions for future research."
https://arxiv.org/html/2411.03883v2,MEG: Medical Knowledge-Augmented Large Language Modelsfor Question Answering,"Question answering is a natural language understanding task that involves reasoning over both explicit context and unstated, relevant domain knowledge. Large language models (LLMs), which underpin most contemporary question answering systems, struggle to induce how concepts relate in specialized domains such as medicine. Existing medical LLMs are also costly to train. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to integrate graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs greatly benefit from the factual grounding provided by knowledge graph embeddings. MEG attains an average of +10.2% accuracy over the Mistral-Instruct baseline, and +6.7% over specialized models like BioMistral. We also show results based on Llama-3. Finally, we show that MEG’s performance remains robust to the choice of graph encoder.","Large language models (LLMs) induce knowledge from vast text corpora. Through self-supervised learning, these models capture deeply contextualized representations of input tokens that enable them to generalize to new tasks with remarkable performance. This, as well as their ability to write long coherent passages, has made LLMs incredibly popular, despite their considerable inference costs (Cheng et al., 2023) and their concerning carbon footprint (Strubell et al., 2019). Moreover, current LLMs face significant challenges with handling complex reasoning and ensuring trustworthiness Liu et al. (2023); Huang et al. (2024) and factual consistency Maynez et al. (2020); Zhou et al. (2023); Tam et al. (2023); Hager et al. (2024), essential to critical fields like healthcare. While LLMs are poised to revolutionize our medical system, already performing well on medical licensing exams Jin et al. (2020); Pal et al. (2022); Singhal et al. (2023a); Brin et al. (2023) and other tasks Nazario-Johnson et al. (2023); Van Veen et al. (2023); Tu et al. (2023); Carl et al. (2024), there is still much room for improvement. To improve reliability and reduce computational costs, researchers have experimented with training from mixtures of corpora and knowledge bases (Pan et al., 2023, 2024). Knowledge Graphs (KGs), such as the Unified Medical Language System (UMLS) Bodenreider (2004), are structured knowledge bases that explicitly store rich factual knowledge. KGs are good at capturing the nuances of complex data and can provide complementary information to LLMs, especially useful for tasks requiring structured understanding. The potential of knowledge-augmented LLMs111In this work, we define a knowledge-augmented LLM as an LLM enhanced with KG embeddings (KGEs). KGEs are dense vector representations of graph entities Ju et al. (2024). Therefore, we also refer to knowledge-augmented LLMs as KGE-augmented LLMs throughout the paper. outlines an interesting research paradigm that can alleviate current challenges of LLMs, and reduce the need of training ever-larger models Hooker (2024). However, how to effectively model interactions between LLMs and KGs remains an open question. Recent efforts have focused on self-supervised methods for jointly training graph neural networks and pretrained language models Yang et al. (2021); Chien et al. (2022); Brannon et al. (2024). Others Yasunaga et al. (2022); Tang et al. (2024); Plenz and Frank (2024), propose new model architectures to leverage the two modalities, graph and text, during pretraining. These methods learn deep interactions over text and graph, but they require carefully curated pretraining data, are mainly studied for graph-oriented tasks Yang et al. (2021); Chien et al. (2022); Tang et al. (2024), or are yet to be adapted to a generative framework Yasunaga et al. (2022); Plenz and Frank (2024). In this work, we introduce MEG, a parameter-efficient approach to MEdical knowledGe-augmented LLMs for question answering (QA). We design a lightweight mapping network to unidirectionally translate KG embeddings into the LLM’s vector space. This enables the LLM to interpret the new input embeddings, which, in turn, further conditions its response generation. We use Mistral-Instruct (7B) Jiang et al. (2023) as our base LLM and report results with our best setup: a KG encoder based on GraphSAGE Hamilton et al. (2017) combined with a simple MLP as mapping network. We also provide results with the recently released Llama-3-Instruct (8B) Dubey et al. (2024) as base LLM. In sum, our contributions are as follows: i) We introduce MEG, a novel approach to knowledge-augmented LLMs based on KGEs. ii) We conduct extensive evaluation on the four popular multiple-choice QA datasets from the MultiMedQA Singhal et al. (2023a) clinical benchmark, and demonstrate the effectiveness of integrating pretrained KGEs into LLMs for medical question answering. Specifically, MEG surpasses strong LLM baselines like BioMistral-7B Labrak et al. (2024) or MediTron-7B Chen et al. (2023), which have followed a costly continued pretraining of the base LLMs on curated biomedical data. iii) We provide insights into the inner workings of MEG, examining the contributions of each module and comparing embedding spaces. We intuitively explain the shifts in the LLM’s representations that drive MEG’s stronger performance. iv) We publicly release the code, trained KGEs and model checkpoints at github.com/lautel/MEG."
https://arxiv.org/html/2411.03866v1,"Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the Way Forward","Recent research has demonstrated that training a linear connector between speech foundation encoders and large language models (LLMs) enables this architecture to achieve strong ASR capabilities. Despite the impressive results, it remains unclear whether these simple approaches are robust enough across different scenarios and speech conditions, such as domain shifts and different speech perturbations. In this paper, we address these questions by conducting various ablation experiments using a recent and widely adopted approach called SLAM-ASR. We present novel empirical findings that offer insights on how to effectively utilize the SLAM-ASR architecture across a wide range of settings. Our main findings indicate that the SLAM-ASR exhibits poor performance in cross-domain evaluation settings. Additionally, speech perturbations within in-domain data, such as changes in speed or the presence of additive noise, can significantly impact performance. Our findings offer critical insights for fine-tuning and configuring robust LLM-based ASR models, tailored to different data characteristics and computational resources.","††footnotetext: ♣ Corresponding authors: {shashi.kumar, iuliia.nigmatulina, sergio.burdisso, esau.villatoro}@idiap.ch Enabling large language models (LLMs) to “comprehend” non-textual modalities has received substantial attention recently. For instance, in [1] the authors trained a projection layer to align the outputs of a visual encoder with an LLM. In the context of automatic speech recognition (ASR), some early methods utilize a cascaded approach, where speech is first transcribed using an automated ASR system, followed by processing the resulting text with an LLM to enhance the transcription accuracy [2, 3, 4, 5]. However, cascaded approaches have several limitations, including error propagation and a lack of access to valuable paralinguistic acoustic information, such as prosody, speaker characteristics, and emotional valence. Recently, systems that integrate robust speech encoders with instruction-tuned LLMs through a connector/projector layer have been proposed as end-to-end ASR solutions [6, 7, 8, 9, 10]; henceforth referred as LLM-based ASR systems. Intuitively, the main task of the connector/projector is to learn how to transform acoustic embeddings from speech encoders into speech representations (tokens) that are meaningful within the LLM’s embedding space. These representations are then combined with text instructions (i.e., prompts) and fed into an LLM to generate various predictions, such as transcription, emotion classification, language identification, and named entity recognition. Three immediate advantages of such architectures are: (a) high compute efficiency, as the entire system can be adapted to new tasks by adopting parameter-efficient approaches, and/or by only training the projector layer; (b) data efficiency due to the vast corpora used in the pre-training of foundational models; and (c) enhanced generalization capability, as LLMs can leverage prompts for zero-shot or in-context learning to handle unseen tasks effectively [7]. In [6], the authors proposed attaching an audio encoder (comprising 36 Conformer [11] layer) to an LLM (Llama-7B [12]) to perform the ASR task. Embeddings generated by the audio encoder are stacked and projected onto the LLM’s input embedding space which is further trained using the Low-Rank Adaptation (LoRA) approach [13]. Similarly, in SpeechVerse [7] and SpeechLLM [14], the authors describe robust multitask training and curriculum learning frameworks that combine pre-trained speech and text foundation models via a small set of learnable parameters. Contrary to [6], these approaches applied a 1-D convolution module that operates over the audio feature sequence to ensure compatibility with the LLM. In the end, only the convolutional downsampling module and the LoRA adapter are trained. Figure 1: SLAM-ASR pipeline. The selected models and the number of parameters for the performed experiments appear between brackets. Recently, the SLAM-ASR architecture was introduced as “an embarrassingly simple approach for large language models with robust ASR capabilities” [10]. Authors argue that elaborate neural architecture designs are unnecessary, showing that a simple composition of off-the-shelf speech encoders and LLMs—using only a simple trainable linear projector to connect them—is sufficient for the ASR task (Figure 1). Despite the impressive results reported by SLAM-ASR [10], there remains uncertainty about whether this novel, yet simple solution, is “the way to go” for LLM-based ASR systems. This paper aims to address this question and performs three different ablation experiments on the SLAM-ASR architecture to evaluate how strong its claimed ASR capacity really is. We study and analyze the performance of SLAM-ASR on three well-known benchmark datasets and one private dataset, as well as its performance under extreme, yet plausible conditions normally addressed by traditional ASR models. This allows insights into the structure and organization of the knowledge represented within the SLAM-ASR architecture, providing transparency and interpretability of the network’s behavior. Overall, our work makes three main contributions that we hope will support the methodological decisions of future researchers working on LLM-based ASR under SLAM-ASR paradigm: (i) we empirically show that SLAM-ASR has a big dependence on the data used for training the linear projector (i.e., over-fitting), resulting in a model that lacks robustness when used in a cross-dataset scenario; (ii) we show how sensitive the SLAM-ASR architecture is to temporal and noise perturbations, unlike its ASR counterpart, and (iii) we performed an exhaustive analysis, both quantitative and qualitative, of what type of alignments the projector layer is learning and, we show how such an alignment can be improved resulting in a better ASR performance."
https://arxiv.org/html/2411.03855v1,MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba,"An ecosystem of Transformer-based models has been established by building large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a crucial technology for deploying these models to downstream tasks with minimal cost while achieving effective performance. Recently, Mamba, a State Space Model (SSM)-based model, has attracted attention as a potential alternative to Transformers. While many large-scale Mamba-based models have been proposed, efficiently adapting pre-trained Mamba-based models to downstream tasks remains unexplored. In this paper, we conduct an exploratory analysis of PEFT methods for Mamba. We investigate the effectiveness of existing PEFT methods for Transformers when applied to Mamba. We also modify these methods to better align with the Mamba architecture. Additionally, we propose new Mamba-specific PEFT methods that leverage the distinctive structure of Mamba. Our experiments indicate that PEFT performs more effectively for Mamba than Transformers. Lastly, we demonstrate how to effectively combine multiple PEFT methods and provide a framework that outperforms previous works. To ensure reproducibility, we will release the code after publication.","Modern large-scale models, also known as Foundation Models, are heavily based on Transformers (Vaswani, 2017). Transformer-based pre-trained models span diverse domains such as language, vision, and multi-modal applications. Despite their widespread use, Transformers have a notable drawback: their computational inefficiency with long sequences. The computational complexity of the attention module scales with the square of the sequence length. To address this fundamental drawback, Gu & Dao (2023) proposed Mamba, a linear-time sequence model that leverages the strengths of State Space Models (SSMs). While Transformers are constructed from attention modules, Mamba is based on the SSM architecture, allowing it to handle long sequences more efficiently. Additionally, Mamba has been shown to perform better than Transformers with the same number of parameters on major tasks such as natural language processing (NLP) (Gu & Dao, 2023) and computer vision (CV) (Zhu et al., 2024). This fact makes Mamba stand out from other post-Transformer architectures with sub-square time complexity (Peng et al., 2023; Sun et al., 2023). Mamba-based models have been proposed across a wide range of domains (Behrouz & Hashemi, 2024; Liang et al., 2024; Zhang et al., 2024b; Li et al., 2024). We believe that Mamba has the potential to go beyond the Transformer ecosystem. Parameter-efficient fine-tuning (PEFT) is essential for adapting such large-scale models to downstream tasks. Fine-tuning all parameters of these models results in high computational costs. PEFT enables additional training for large-scale Transformers with limited computing resources and data. Early examples of PEFT include the fine-tuning of pre-trained language models for NLP tasks (Houlsby et al., 2019; Hu et al., 2021). Subsequently, it has been extensively adopted across a wide range of applications (Yeh et al., 2024; Wang et al., 2023; 2024a). While many PEFT methods have been extensively studied for Transformers (Lialin et al., 2023; Han et al., 2024), research on PEFT methods for Mamba remains limited. In this paper, we provide an exploratory and comprehensive investigation of PEFT for Mamba. First, we adapt representative PEFT methods used in Transformers to the Mamba architecture and conduct extensive experiments. We also propose new PEFT methods specific to the Mamba architecture. Figure 1: An overview of our proposed MambaPEFT. We investigate, improve, and propose 20 variations of seven PEFT methods for Mamba and search for the best combination. In the experiments, we benchmarked Mamba using PEFT methods, including seven main methods and a total of 20 derived variations (see Figure 1). Our benchmarks indicate that PEFT is more crucial for Mamba than for Transformers, with several methods outperforming the PEFT methods used for Transformers. Additionally, we demonstrate that these PEFT methods can be combined to surpass the performance of individual methods. We propose an efficient search technique to identify optimal PEFT combinations and hyperparameters. Unlike existing works that focus on specific high-performance methods, we explore a wide variety of PEFT methods. This exploration reveals suitable PEFT combinations and shows that merely combining high-performing methods is not sufficient. The main contributions are as follows. First, to the best of our knowledge, we perform the first extensive and comprehensive benchmarking of PEFT methods for Mamba, including proposed Mamba-specific methods that are distinct from all PEFT methods for Transformers. Second, we propose a framework for achieving higher performance by combining multiple PEFT methods, which are obtained through our efficient search technique. Third, our results indicate that PEFT is more effective for Mamba than for Transformers, and several Mamba-specific phenomena are discovered through the experiments."
https://arxiv.org/html/2411.03811v1,The natural stability of autonomous morphology: How an attraction–repulsion dynamic emerges from paradigm cell filling,"Autonomous morphology, such as inflection class systems and paradigmatic distribution patterns, is widespread and diachronically resilient in natural language. Why this should be so has remained unclear given that autonomous morphology imposes learning costs, offers no clear benefit relative to its absence and could easily be removed by the analogical forces which are constantly reshaping it. Here we propose an explanation for the resilience of autonomous morphology, in terms of a diachronic dynamic of attraction and repulsion between morphomic categories, which emerges spontaneously from a simple paradigm cell filling process. Employing computational evolutionary models, our key innovation is to bring to light the role of ‘dissociative evidence’, i.e., evidence for inflectional distinctiveness which a rational reasoner will have access to during analogical inference. Dissociative evidence creates a repulsion dynamic which prevents morphomic classes from collapsing together entirely, i.e., undergoing complete levelling. As we probe alternative models, we reveal the limits of conditional entropy as a measure for predictability in systems that are undergoing change. Finally, we demonstrate that autonomous morphology, far from being ‘unnatural’ (e.g. Aronoff 1994), is rather the natural (emergent) consequence of a natural (rational) process of inference applied to inflectional systems.","Autonomous morphology refers to linguistically significant generalisations—such as inflection classes or recurrent distributions of stem allomorphy—which systematically structure a language’s morphology, yet are irrelevant to the rest of the grammatical system. The existence of autonomous morphology poses a serious challenge to any functionally-grounded theory of linguistics, since it is squarely maladaptive: its presence (as opposed to its absence) offers no functional advantage and can impose substantial learning costs during acquisition. While the initial appearance of autonomous morphology is straightforwardly attributable to accidents of diachrony, the substantive challenge is to explain why it then persists for millennia, often out-lasting other linguistic subsystems that are functionally well-motivated. Specifically, inflection classes and recurrent distributions of stem allomorphy are subject to constant diachronic change through inflectional analogy, and analogical changes could quickly lead to their demise by levelling them out: so, if they are maladaptive and a clear path to their extinction exists, why does this path not prevail? Here we propose an answer. By modelling the historical dynamics of inflectional systems subject to iterated analogical changes, we show that, under the right conditions, a simple strategy for analogical reasoning leads to the emergence and persistence of inflection classes and recurrent distributions of stem allomorphy. Of central importance to our proposal is the nature of synchronic analogical inference, since such inference functions as a diachronic source of novel inflectional patterns. As we shall see, small changes in assumptions about synchronic analogy may have ramifications for long-term diachrony that are significant. Synchronic analogy is constantly at work, because speakers of inflectional languages will rarely have encountered every last inflected form of lexemes that they know (Chan, 2008; Bonami and Beniamine, 2016; Boyé and Schalchli, 2019). Consequently, on occasion during production speakers will need to solve what Ackerman et al. (2009) term the ‘Paradigm Cell Filling Problem’ (henceforth PCFP) and produce inflected forms on the basis of inductive reasoning. Under such conditions it is important to ask: given a set of assumptions about how the PCFP is resolved, what are the predicted long-term implications for an inflectional system that incrementally changes under its influence? Most significantly, can we discover pathways by which known typological properties of inflectional systems arise emergently out of a cell filling process iterated over and over in the course of synchronic communication? By pursuing this line of enquiry, though it is not possible to prove that typological property P𝑃Pitalic_P necessarily arises from synchronic process S𝑆Sitalic_S, it is possible to establish that S𝑆Sitalic_S could be the source of P𝑃Pitalic_P, under the right conditions—a valuable form of scientific hypothesis generation which philosophers of science term ‘how-possibly’ reasoning (Persson, 2012). Here, we apply how-possibly reasoning to furnish an new explanatory hypothesis for the stable diachronic persistence of autonomous morphology. As a starting point, we outline the current debate about the nature of autonomous morphology, including its putative computational function and its possible origin as an adaptive property within a gradual process of inflectional self-organisation. We highlight the challenge of explaining the consistently observed tendency for the mean conditional entropy of inflectional systems in natural language to remain stable at a low-yet-positive level, rather than falling to zero, and entertain the hypothesis that the emergence and persistence of autonomous morphology may be intrinsically linked to this tendency (Section 2). Computational modelling offers an accessible means to test such hypotheses by simulating the evolutionary trajectory of inflectional systems, including the emergence of system-level structural properties. We conduct a detailed review of the properties, insights and limitations of two early iterative models implementing a paradigm cell filling task (Ackerman and Malouf, 2015; Esher, 2015a, b, 2017a). Although designed to simulate gradual inflectional self-organisation, both models consistently evolve lexicons which are uniform and therefore lack the stable, structured variation that typifies autonomous morphology in inflectional systems of natural language. Via careful attention to the relationship between variation, interpredictability and entropy, we trace this outcome to a structural property of the models: a dynamic of preferential attraction in which lexemes and cells can only ever become more similar to others (Section 3). Our next step is to enrich a replica of the existing models with additional, modulable parameters reflecting a more realistic view of linguistic input and speaker reasoning, such as Zipfian frequency weightings and recourse to narrower or wider samples. The marginal effects of these parameters on the overall evolutionary trajectory of the models—slowing, but never altering, the progression to total uniformity—confirm our insight that it is impossible for a system governed by a pure-attraction dynamic to stabilise with structured variation and low-but-positive entropy (Section 4). However, models with a pure-attraction dynamic do not exhaust all possibilities, and upon closer inspection, other models may be better motivated. We show that rational reasoners can be expected to attend not only to ‘associative evidence’ based on lexical similarities, but also to ‘dissociative evidence’ based on differences. Incorporating dissociative evidence into our model introduces a repulsion dynamic which promotes divergence between cells and lexemes, alongside the existing attraction dynamic promoting similarity. The combination of these two dynamics promotes the emergence of stable inflectional organisation with low-yet-positive entropy, akin to that observed in natural language: pressure to coalesce pushes the system towards a lower number of variants, while pressure to disperse pushes the system to keep the remaining variants distinct (Section 5). Our model illustrates how structures corresponding to autonomous morphology can spontaneously (naturally) arise and subsequently persist as a form of inflectional self-organisation, via a simple (natural) inferential process that attends to both associative evidence and dissociative evidence. Coupled with the crosslinguistic prevalence and observed resilience of autonomous morphology, our results significantly erode the rationale for characterising autonomous morphology as unnatural: on the contrary, we contend that it is a fundamentally natural feature of human language. Our study further showcases the explanatory power and potential of computational evolutionary modelling (Sections 6, 7)."
https://arxiv.org/html/2411.03806v1,Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection,"Natural Language Generation has been rapidly developing with the advent of large language models (LLMs). While their usage has sparked significant attention from the general public, it is important for readers to be aware when a piece of text is LLM-generated. This has brought about the need for building models that enable automated LLM-generated text detection, with the aim of mitigating potential negative outcomes of such content. Existing LLM-generated detectors show competitive performances in telling apart LLM-generated and human-written text, but this performance is likely to deteriorate when paraphrased texts are considered. In this study, we devise a new data collection strategy to collect Human & LLM Paraphrase Collection (HLPC), a first-of-its-kind dataset that incorporates human-written texts and paraphrases, as well as LLM-generated texts and paraphrases. With the aim of understanding the effects of human-written paraphrases on the performance of state-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark detectors, we perform classification experiments that incorporate human-written paraphrases, watermarked and non-watermarked LLM-generated documents from GPT and OPT, and LLM-generated paraphrases from DIPPER and BART. The results show that the inclusion of human-written paraphrases has a significant impact of LLM-generated detector performance, promoting TPR@1%FPR with a possible trade-off of AUROC and accuracy.","Large language models (LLMs) have become essential in Natural Language Processing (NLP) thanks to their advanced capabilities for text processing and generation, which is achieved through analysis of patterns and relationships between words and sentences using transformer models [42]. Consequently, LLMs have had a significant impact on Natural Language Generation (NLG), as they have provided improved capacity for automatically generating high quality text [4]. While the advancement of LLMs in the context of NLG has aided tasks such as machine translation [38] and text summarization [40], it has also given rise to undesired social problems, including intentional malicious usage, ethical concerns and information inaccuracy. This has brought about the need for researching the development of methods for automated LLM-generated text detection which distinguishes if a text is human- or LLM-generated [12]. Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25, 33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process [16], and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise. Both kinds of detectors have demonstrated excellent performance in LLM-generated text detection. However, research testing these detectors has primarily focused on datasets involving texts which are exclusively generated by humans or by LLMs. There can be, however, more complicated cases, such as paraphrased texts, which have been seldom considered in previous research. Paraphrasing is defined as the rewriting of context in a simpler and shorter form [8]; an LLM-generated text which is then paraphrased by humans leads to modified texts where the statistical properties of watermarks in the LLM-generated text is no longer identifiable. Since the above detectors perform classification based on token patterns and watermarks, paraphrasing could effectively evade both zero-shot classifiers and watermark detectors while preserving semantic information from the original LLM-generated text. It is important to identify that a text originated from an LLM, despite having been subsequently paraphrased, as this can still be leveraged for massive generation of texts for malicious purposes. In this work, we are the first to comprehensively study the effectiveness of LLM-generated text detectors in the presence of human-paraphrased texts, in turn assessing the impact of these edited texts on the model performance. In this study, we aim to address this problem by using human-written paraphrases for classification, with the notion that human-written paraphrases and LLM-generated paraphrases might contain different characteristics, which potentially improve the classifiers’ performances. We set forth the following research question: “What are the effects of including human-written paraphrases in LLM-generated text detection?” With this aim and research question in mind, we make the following contributions: • We perform a review of the literature to investigate the existing NLG developments, the importance of LLM-generated text detection, and the performances of existing detectors. • We describe a data collection process which enables us to build and release the Human & LLM Paraphrase Collection (HLPC) dataset with human-generated and LLM-generated documents, along with their paraphrases. • We perform classification experiments using state-of-the-art AI paraphrasers and detectors, along with human-written paraphrases. • Our study contributes to the domain of LLM-generated text detection by examining the effects of including human-written paraphrases in classification and providing insights on data inclusion in future detector building."
https://arxiv.org/html/2411.03769v1,"No Culture Left Behind: ArtELingo-28,a Benchmark of WikiArt with Captions in 28 Languages","Research in vision and language has made considerable progress thanks to benchmarks such as COCO. COCO captions focused on unambiguous facts in English; ArtEmis introduced subjective emotions and ArtELingo introduced some multilinguality (Chinese and Arabic). However we believe there should be more multilinguality. Hence, we present ArtELingo-28, a vision-language benchmark that spans 28 languages and encompasses approximately 200,000 annotations (140 annotations per image). Traditionally, vision research focused on unambiguous class labels, whereas ArtELingo-28 emphasizes diversity of opinions over languages and cultures. The challenge is to build machine learning systems that assign emotional captions to images. Baseline results will be presented for three novel conditions: Zero-Shot, Few-Shot and One-vs-All Zero-Shot. We find that cross-lingual transfer is more successful for culturally-related languages. Data and code will be made publicly available.","A quick review of recent surveys on multimodal AI Cao et al. (2023); Berrios et al. (2023); Zhang et al. (2023), reveals just how much the literature is focused on English. The literature on benchmarking Liu et al. (2023c); Li et al. (2023a) provides an astoundingly similar story. With the pervasiveness of AI technology in our societies, it is essential to make the technology accessible to a wider population. Although English is widely spoken as a first language or a second language, most of the world (75% per capita) does not speak English.111https://www.cochrane.org/news/cochrane-evidence-different-languages No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages shows some annotations from ArtELingo-28. For 2000 images from WikiArt, we have ∼similar-to\sim∼140 emotion labels per image, as well as captions from annotators with diverse backgrounds covering 28 languages. Unlike captions in traditional benchmarks such as COCO Lin et al. (2014) and Visual Genome Krishna et al. (2017) which emphasize unambiguous class labels, the captions in No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages emphasize subjective opinions over objective facts, and diversity over languages and cultures. No Culture Left Behind: ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages shows 5 annotations from 5 languages for 4 WikiArt images. Compare, for example, the captions for the first image in Burmese, Malay, Korean and Setswana. There are differences of opinion in both labels and captions: emotion labels: disgust (Burmese), awe (Malay) captions: focus on chest (Burmese & Malay); focus on face and hair (Korean & Setswana) To advance the field beyond objective facts and unambiguous class labels, it is critical to embrace diversity and subjective differences of opinion. Traditionally, vision research has focused on classifying objects in the image in an objective way, but we prefer to view art as a form of communication between the artist and the audience, where there is more room for subjectivity and diversity. Communication depends on much more than just the pixels in the image such as the cultural backgrounds of the participants.222Blog: Who created the saying that beauty is in the eye of the beholder? To add 25 new languages to ArtELingo-28 required considerable effort. Amazon Mechanical Turk works well for a few languages, but less so for many of the 25 languages. ArtELingo-28 consumed 6.25K hours of work, performed by 220 annotators from 23 countries. Compared to ArtELingo which added just 3 languages, our dataset required significantly more management and coordination; ArtELingo-28 was managed by a team of 32 coordinators who contributed more than 2.5K hours. To cover many practical situations, we utilize ArtELingo-28 to build 3 evaluation setups: Zero-Shot, Few-Shot, and One-vs-All Zero-Shot. The main task evaluates the performance of the generation of affective explanations. In the Zero-Shot setup, we train a model on a large-scale training dataset in a few high-resource languages. We then evaluate that model on languages that do not appear in the the training data. The Few-Shot setup addresses the situation where we have a few training examples in low-resource languages, in addition to the large-scale training dataset from the Zero-Shot setup. We fine-tune the models from the Zero-Shot setup on the few-shot low-resource data and then evaluate them on the rest of the samples. Finally, in the One-vs-All Zero-Shot setup, we have the large-scale training dataset as well as small-scale data in one language (One). After fine-tuning, we evaluate on the Unseen languages (All). This setup is designed to shed light on pairwise interactions between languages, highlighting cultural effects. We observe clusters (cultural groups) forming from our trained models. These groups go beyond writing systems (scripts), capturing cultural connections between languages. Additionally, we observe that the multilingual setup is challenging for vision and language models, partly because of the massive vocabulary. We address this challenge by utilizing pretrained multilingual LLMs such as BLOOMZ. In short, our contributions are: • We collected 200K emotion labels and affective textual explanations in 25 languages on 2000 images (with ∼similar-to\sim∼140 annotations/image). • We proposed a benchmark to evaluate the Zero-Shot, Few-Shot, and One-vs-All Zero-Shot performance of Multimodal models. • We adapt and benchmark four contemporary Vision and Language models to work on our multilingual setup. • Finally, we study pairwise language transfer revealing insights on cultural differences in emotional perception and expression."
https://arxiv.org/html/2411.03766v1,Number Cookbook: Number Understanding of Language Models and How to Improve It,"Large language models (LLMs) can solve an increasing number of complex reasoning tasks while making surprising mistakes in basic numerical understanding and processing (such as 9.11>9.99.119.99.11>9.99.11 > 9.9). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid little attention to it or only discussed several restricted tasks (like integer addition). In this paper, we comprehensively investigate the numerical understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a benchmark covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear. Through the benchmark, we find that current LLMs fail frequently in many of the tasks. To study the problem, we train small models with existing and potential techniques for enhancing NUPA (such as special tokenizers, PEs, and number formats), comprehensively evaluating their effectiveness using our testbed. We also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1) naive finetuning can improve NUPA a lot on many but not all tasks, and 2) surprisingly, techniques designed to enhance NUPA prove ineffective for finetuning pretrained models. We further explore the impact of chain-of-thought techniques on NUPA. Our work takes a preliminary step towards understanding and improving NUPA of LLMs. Our benchmark and code are released at https://github.com/GraphPKU/number_cookbook.","The mathematical and reasoning abilities of large language models (LLMs) are currently quite impressive (OpenAI, 2023; Meta, 2024; OpenAI, 2024a; Yang et al., 2024a), capable of solving problems at the level of a graduate student or even more difficult ones like olympiad-level problems (He et al., 2024), GAOKAO (a nationwide examination of high school students applying to universities in China) (Zhang et al., 2024) and college mathematics (Tang et al., 2024). However, upon closer examination of the models’ outputs, we found that although the models demonstrate remarkable proficiency in problem-solving approaches, they often struggle with basic numerical understanding and processing — like a careless student who claims, “I know how to do it, but I didn’t get it right.” Some of these errors are quite surprising, such as believing that 9.11>9.99.119.99.11>9.99.11 > 9.9 or making mistakes in simple addition 8/7+3/587358/7+3/58 / 7 + 3 / 5. These errors are a major cause of hallucinations when dealing with math, reasoning, and data analysis tasks, as the model presents seemingly correct problem-solving approaches, but ultimately produces incorrect results. Therefore, investigating and improving the fundamental “numerical understanding and processing abilities” (NUPA) of models is crucial. However, in current research, reasoning ability and NUPA are often tested together, both on classic datasets such as GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MMLU (Hendrycks et al., 2020), and in more challenging tests mentioned above. For example, a problem in GSM8k is: “Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?” Solving this problem requires two aspects: on the one hand, mathematical reasoning including understanding the text, extracting relevant information, formulating mathematical equations (or finding other solution methods), solving the equations or executing an algorithm, and obtaining the result; on the other hand, it also requires understanding and processing the numbers provided in the problem or produced as intermediate results at each step, like 48/2=244822448/2=2448 / 2 = 24 and 48+24=7248247248+24=7248 + 24 = 72. While these two abilities are both essential to correctly solving the problems, tests on such datasets do not distinguish between them. A more severe issue is that the numerical content is often deliberately simplified in these datasets. In various exam questions (like in the American Invitational Mathematics Examination (Li et al., 2024)), to focus on assessing students’ understanding of mathematical concepts — such as how to set up the correct equations and apply the right theorems — the numbers in both the questions and answers are often specially chosen to be integers. However, this is not the case in real-world scenarios (Chen et al., 2022a). Despite the importance of NUPA, there is still a lack of accurate, detailed, and comprehensive formalization, measurement, and analysis of this fundamental capability. In this paper, we take the preliminary step towards formalizing the NUPA of LLMs. We categorize the numerical concepts and operations commonly taught in primary and secondary education into four types of numerical representations: integers, floating-point numbers (finite decimals), fractions, and scientific notation, along with four ability categories comprising 17 tasks. Pairing these representations results in 41 meaningful tasks, forming our NUPA benchmark (Table 1). These representations and tasks cover the most common scenarios involving number understanding and processing, which are typically not challenging for humans, as we read, use, or process such numbers nearly everyday. On this benchmark, we rigorously test several state-of-the-art LLMs containing GPT-4o (OpenAI, 2024a), Llama-3.1 (Meta, 2024) and Qwen2 (Qwen Team, 2024). We ask models to directly output the answers without calling external tools. Although the latest LLMs perform well on some easiest tasks, their performance declines significantly as tasks become slightly more complex (such as multiplication, modulo operations, or digit-based calculations), or as the representation of numbers extends beyond basic integers. See Figure 2 of Section 2.4. The overall unsatisfactory performance highlights a pronounced mismatch between the claimed strong mathematical reasoning abilities and the poor practical, everyday numerical understanding and processing abilities of today’s LLMs. To address this issue, we explore three categories of approaches to enhance the NUPA of models. The first category of techniques aim at improving models’ NUPA during the pretraining stage, including alternative tokenization, specially designed positional encoding (PE) (Haviv et al., 2022; Kazemnejad et al., 2023b; Zhou et al., 2024), changing number formats (like zero-padding, index-hint (Zhou et al., 2023) and reverse representation (Lee et al., 2023; Zhou et al., 2024)). We evaluate and analyze them on our newly introduced benchmark, verifying their effectiveness/ineffectiveness on respective tasks/representations, which extends over previous evaluation mainly on the integer addition/multiplication tasks. Further, we summarize these techniques into three mechanisms: simplifying the reasoning process, aiding digit alignment, and providing regularization, and discuss the potential of these mechanisms to be applied across a broader range of numerical representations. Table 1: Task overview of NUPA Test. The four rows represent four numerical representations, and the 17 columns correspond to different tasks. ✓: 41 tasks included in our test. ✗: Not included, too complex. ○○\bigcirc○: Not directly included but can be easily adapted from an included task. −--: Not applicable. The detailed explanation for these non-included tasks is provided in Appendix A.1.4 Elementary arithmetic Comparison Digit Understanding Conversion Add Sub Multiply Truediv Floordiv Mod Max Min Digit Max Digit Min Digit Add Get Digit Length Count To Float To Scientific Sig. Fig. Integer ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ −-- ✓ ✓ Float ✓ ✓ ✓ ✗ −-- −-- ✓ ✓ ✓ ✓ ✓ ✓ ✓ ○○\bigcirc○ −-- ✓ ✓ Fraction ✓ ✓ ✓ ✓ −-- −-- ✓ ✓ −-- −-- −-- −-- −-- ○○\bigcirc○ ✓ ○○\bigcirc○ ○○\bigcirc○ Scientific ✓ ✓ ✓ ✗ −-- −-- ✓ ✓ −-- −-- −-- −-- −-- ○○\bigcirc○ ✓ −-- ○○\bigcirc○ The second category of approaches aim to improve NUPA for an already trained model. We find that while simple direct finetuning can significantly enhance NUPA performance, applying the aforementioned techniques (PEs, data formats and tokenizers) at this stage may have adverse effects. We test various settings and fine-tuning configurations, but none are able to achieve performance equal to or better than the original model. Our results suggest that these modifications can significantly disrupt the models’ established behavior or conflict with its pre-existing knowledge, leading to a decrease in performance. Finally, we discuss the potential of using chain-of-thought (CoT) techniques for numerical processing. Although CoT methods can break down complex problems into simpler sub-tasks and significantly increase the likelihood of obtaining correct answers, their drawbacks — such as consuming a large context window and requiring extended processing time — become particularly apparent in numerical tasks. We test a general CoT method known as RFFT (Hu et al., 2024), and find that for more complex tasks (such as those with O⁢(n2)𝑂superscript𝑛2O(n^{2})italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) complexity, including multiplication, division, and fraction simplification), chain-of-thought methods face scalability challenges, making them difficult to be applied in practical scenarios. It is noteworthy that in this paper, we do not discuss tool use methods (Schick et al., 2023; Lu et al., 2023) for NUPA as 1) we want to study the self-contained NUPA of LLMs, 2) calling external tools whenever encountering numbers increases the inference latency (Xu et al., 2024), and 3) we believe NUPA without tools is a necessary ability of AGI. In summary, we propose a more comprehensive benchmark on the basic numerical understanding and processing abilities (NUPA) of LLMs, evaluate several SOTA LLMs’ performance on it, and further study three categories of approaches to improve NUPA: pretraining, finetuning and CoT. Our results reveal that the current research is insufficient to fully address the NUPA problem, despite it being a fundamental capability for solving many more complex tasks. We hope that by introducing a systematic classification and more comprehensive evaluation of NUPA, we can bring greater attention from the community to this important but overlooked fundamental capability."
https://arxiv.org/html/2411.03700v1,"The Root Shapes the Fruit:
On the Persistence of Gender-Exclusive Harms in Aligned Language Models","Content Warning: This paper contains examples of offensive transphobic content. Natural-language assistants are designed to provide users with helpful responses while avoiding harmful outputs, largely achieved through alignment to human preferences. Yet there is limited understanding of whether alignment techniques may inadvertently perpetuate or even amplify harmful biases inherited from their pre-aligned base models. This issue is compounded by the choice of bias evaluation benchmarks in popular preference-finetuned models, which predominantly focus on dominant social categories, such as binary gender, thereby limiting insights into biases affecting underrepresented groups. Towards addressing this gap, we center transgender, nonbinary, and other gender-diverse identities to investigate how alignment procedures interact with pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a comprehensive survey of bias evaluation modalities across leading preference-finetuned LLMs, highlighting critical gaps in gender-diverse representation, 2) systematic evaluation of gender-diverse biases across 12 models spanning Direct Preference Optimization (DPO) stages, uncovering harms popular bias benchmarks fail to detect, and 3) a flexible framework for measuring harmful biases in implicit reward signals applicable to other social contexts. Our findings reveal that DPO-aligned models are particularly sensitive to supervised finetuning (SFT), and can amplify two forms of real-world gender-diverse harms from their base models: stigmatization and gender non-affirmative language. We conclude with recommendations tailored to DPO and broader alignment practices, advocating for the adoption of community-informed bias evaluation frameworks to more effectively identify and address underrepresented harms in LLMs.","Aligning large language models (LLMs) with human preferences has emerged as a promising approach for instilling helpful model behavior across a wide array of language tasks [1; 2; 3; 4]. However, these models must serve diverse communities effectively to achieve their intended impact. Pre-trained LLMs, from which aligned models are finetuned, can perpetuate harmful biases that disproportionately affect marginalized groups, including transgender, non-binary and other gender-diverse minorities [5; 6; 7; 8; 9]. Although alignment techniques like Direct Preference Optimization [10] can steer models towards helpful and harmless behavior [11; 12; 13], there is limited understanding of how harmful social biases already present in their base models may persist or even amplify after alignment. Figure 1: Bias benchmarks employed by top 15 performing preference-tuned LLMs reported by Chatbot Arena Leaderboard [14] across socially-relevant categories. Evaluations fully cover binary gender bias, with limited evaluation for gender-diverse minorities and other socially-salient dimensions. Current bias evaluations for preference-finetuned LLMs often rely on benchmarks grounded in Western social norms, restricting our understanding of bias and perpetuated harms to evaluations that reflect these dominant perspectives. Figure 1 illustrates this limitation in popular preference-finetuned LLMs, where employed bias benchmarks consistently assess binary gender bias, while offering little to no coverage for gender-diverse identities and other socially salient dimensions. This narrow focus creates two issues: (1) binary gender-exclusive measurements of LLM harms risk leaving biases affecting gender minorities unchecked and (2) it further entrenches cisnormative hegemonies in competitive LLM benchmarking, encouraging other models to mirror these evaluation practices. [15; 16; 7]. While expanding existing evaluations to include more groups is a step forward, doing so without proper construct validation risks neglecting significant power asymmetries that marginalized communities face [17; 18; 19]. These gaps motivate our investigation into how alignment procedures interact with LLM gender biases, particularly those affecting transgender, nonbinary, and other gender-diverse identities (TGNB) - an area that remains underexplored in alignment literature despite substantial evidence of TGNB harms in pretrained LLMs [5; 6; 9; 7; 8]. Towards addressing these evaluation gaps, we focus on two key research questions: (1) Can pre-existing gender-diverse biases be amplified even after alignment towards helpful and harmless behavior? (2) How are these biases encoded in the alignment process? Contributions. To answer these questions, we systematically evaluate 12 LLMs for gender-diverse bias across their pre-trained, supervised fine-tuned (SFT) [20], and aligned versions with Direct Preference Optimization (DPO) [10]. Using TGNB-specific benchmarks designed to capture real-world harms, we examine whether these biases persist in aligned models despite finetuning for helpfulness and harmlessness. As alignment procedures rely on preference-derived reward signals [21; 10], we propose a flexible framework to uncover bias patterns in these signals, offering insights into mechanisms of bias transfer. Demonstrated with gender-diverse biases yet adaptable to other social contexts, this framework allows us to trace bias propagation across alignment stages, revealing avenues for targeted bias intervention. This framework is coupled with a thematic analysis of TGNB stigma found across DPO-aligned LLMs. Our findings reveal that aligned LLMs can not only perpetuate but also amplify existing TGNB biases found in their base models, aspects which cannot be detected by popularly employed LLM bias benchmarks. Specifically, we observe that DPO-aligned LLMs can (1) exacerbate gender non-affirmative outputs when initialized from SFT models displaying similar biases (§4.1) (2) reflect implicit reward signals reinforcing TGNB social stigma and (3) preserve biased reward signals from their base models (§5.3). Furthermore, our thematic analysis reveals aligned LLMs can reflect documented patterns of TGNB stigma, including harmful stereotypes that characterize gender-diverse individuals as mentally unstable [22](§5.3). Guided by our findings, we offer recommendations contextualized to both bias evaluation and alignment procedures to better understand and address these harms (§6)."
https://arxiv.org/html/2411.03675v1,QUILL: Quotation Generation Enhancement of Large Language Models,"While Large language models (LLMs) have become excellent writing assistants, they still struggle with quotation generation. This is because they either hallucinate when providing factual quotations or fail to provide quotes that exceed human expectations. To bridge the gap, we systematically study how to evaluate and improve LLMs’ performance in quotation generation tasks. We first establish a holistic and automatic evaluation system for quotation generation task, which consists of five criteria each with corresponding automatic metric. To improve the LLMs’ quotation generation abilities, we construct a bilingual knowledge base that is broad in scope and rich in dimensions, containing up to 32,022 quotes. Moreover, guided by our critiria, we further design a quotation-specific metric to rerank the retrieved quotations from the knowledge base. Extensive experiments show that our metrics strongly correlate with human preferences. Existing LLMs struggle to generate desired quotes, but our quotation knowledge base and reranking metric help narrow this gap. Our dataset and code are publicly available at https://github.com/GraceXiaoo/QUILL.","Figure 1: An example of Quotation Generation (QG) Evaluation. In existing QG tasks, LLMs are often prone to quotation hallucination, inconsistent context, lack of paragraph coherence, and lack of novelty in quotations. LLMs currently face great challenges in QG tasks. Figure 2: The framework for our Quotation Generation (QG) task research. We first establish an evaluation system with 5 evaluation criteria and automatic metrics, then build a quotation knowledge base covering multiple languages, topics and eras, and finally propose a quotation-specific reranking metric to rerank the quotations recalled in the RAG stage and improve the performance of QG tasks. Famous quotations Tan et al. (2015a) are vital in academic and everyday communication. They lend authority to arguments and enhance persuasiveness, as they often stem from historically influential figures whose ideas have endured. Additionally, these quotations elevate the literary and artistic quality of a text, making discussions more engaging. They also facilitate comprehension of complex concepts, enabling readers to grasp ideas efficiently through concise expressions Vaswani et al. (2023). The task of Quotation Generation (QG) seeks to produce suitable quotations to deepen the context in large language models (LLMs) Anil et al. (2023); Achiam et al. (2023); Touvron et al. (2023). However, LLMs encounter significant challenges in this domain, as illustrated in Figure 1. Primarily, the generated quotes frequently fail to correspond to genuine famous quotations and are often inaccurately attributed, a phenomenon termed ""Quotation Halluciantion."" Huang et al. (2023); Bang et al. (2023); Guerreiro et al. (2023) Additionally, these quotes don’t align with the contextual meaning, resulting in a lack of coherence within the paragraph. Furthermore, LLMs exhibit a tendency to reproduce well-known quotes, which diminishes novelty and restricts creative expression. Although the issues of Quote Generation task are particularly problematic in LLMs, there is currently no effective solutions. Previous studies Qi et al. (2022a) were based on representative pre-trained language models such as BERT Devlin et al. (2019), and it remains under-explored on the problem of quotation hallucination with LLMs. And there is currently no systematic and comprehensive benchmark to evaluate the quotation generation ability of LLMs. To tackle these challenges, we introduce QUILL for QUotation GeneratIon enhancement of Large Language Models, a framework integrating an automatic evaluation system and an innovative and effective solution to improve quotation generation performance of LLMs.The framework of QUILL is shown in Fig. 2. QUILL presents a comprehensive benchmark comprising 7 quotation domains and 16 real-world scenarios to evaluate large models’ quotation generation abilities systematically, which consists of 5 highly interpretable and rigorous criteria with automatic evaluation metrics (Fig. 1): (1) Quotation Authenticity: Confirm whether the quoted quotes are real quotes from famous people to prevent misquotations or fabrications. (2) Quotation Credibility: Verify whether the quotation satisfies the author or source mentioned in the context (if any) to ensure the credibility of the quoted content. (3) Semantic Matching: Evaluate whether the semantics of the quoted quote align with the context. (4) Semantic Fluency: Evaluate the extent to which the cited quotation affects the fluency of the paragraph. (5) Quotation Novelty: Evaluate the degree of uniqueness of the quote. Additionally, based on the task’s essential characteristics, we introduce an innovative Quotation-Specific Reranking Metric Karpukhin et al. (2020); Lewis et al. (2021); Chern et al. (2023) to improve model performance in QG tasks. To facilitate the task, we also established a comprehensive and high-quality knowledge database containing up to 32,022 quotes. This database spans both Chinese and English languages, various authors, different eras, and diverse topics, which ensures the wide applicability and generalization of our method. To the best of our knowledge, our work is the first systematic investigation into the automatic evaluation and enhancement of quotation generation performance in LLMs. To summarize, our contributions are mainly four-fold: 1. We establish a holistic and automatic evaluation system for the quotation generation task, consisting of five highly interpretable and rigorous criteria, facilitating both human and automatic evaluation of this task. 2. We construct a comprehensive and high-quality knowledge database containing up to 32,022 quotes, complete with authors or sources. 3. We design a fine-grained quotation-specific metric to rerank the retrieved quotations from the knowledge base. 4. We conduct extensive experiments to verify that our metrics are strongly correlate with human preference and significantly effective in both open-source and closed-source LLMs."
https://arxiv.org/html/2411.03665v1,Evaluating Moral Beliefs across LLMs through a Pluralistic Framework,"Proper moral beliefs are fundamental for language models, yet assessing these beliefs poses a significant challenge. This study introduces a novel three-module framework to evaluate the moral beliefs of four prominent large language models. Initially, we constructed a dataset containing 472 moral choice scenarios in Chinese, derived from moral words. The decision-making process of the models in these scenarios reveals their moral principle preferences. By ranking these moral choices, we discern the varying moral beliefs held by different language models. Additionally, through moral debates, we investigate the firmness of these models to their moral choices. Our findings indicate that English language models, namely ChatGPT and Gemini, closely mirror moral decisions of the sample of Chinese university students, demonstrating strong adherence to their choices and a preference for individualistic moral beliefs. In contrast, Chinese models such as Ernie and ChatGLM lean towards collectivist moral beliefs, exhibiting ambiguity in their moral choices and debates. This study also uncovers gender bias embedded within the moral beliefs of all examined language models. Our methodology offers an innovative means to assess moral beliefs in both artificial and human intelligence, facilitating a comparison of moral values across different cultures. 111Dataset and code are publicly available at https://github.com/MuMu-Lily/Moral-Beliefs","As artificial intelligence (AI) continues to evolve, there is growing concern regarding the presence and nature of moral beliefs within contemporary systems. The potential for language models (LMs) to exhibit harmful moral beliefs poses significant risks, underscoring the need for scrutiny Weidinger et al. (2021). Explorations into the morality and values of LMs have been conducted using self-constructed datasets to determine if these models can discern the presence and nature of values within utterances, tasking the LM with categorizing morality and values Hendrycks et al. (2021); Ziems et al. (2022); Sorensen et al. (2023); Alhassan et al. (2022). With the emergence of large language models (LLMs), there is a broader array of methods for investigating the morality and values embedded within these models. On one hand, LLMs are probed through questionnaires to elucidate their values and moral beliefs Ramezani and Xu (2023); Abdulhai et al. (2023). On the other hand, their moral comprehension and reasoning abilities are scrutinized using traditional moral dilemma questions Tanmay et al. (2023). Every moral decision we make is contextual. Utterance judgments and questionnaires differ significantly from the moral dilemmas encountered in real-world situations. Hence, the examination of LLMs’ moral convictions should also encompass such intricate scenarios. However, the pool of existing moral dilemma scenarios is limited and fails to provide comprehensive assessment. Furthermore, insufficient studies have delved into the disparities in LLMs’ moral beliefs across cultural backgrounds and demographics van der Meer et al. (2023), notably lacking research in the Chinese context. Classification tasks or questionnaire assessments often oversimplify morality as dichotomous, whereas morality is a multifaceted construct that should align with various stages of moral development Kohlberg (1987); Park et al. (2024). Additionally, LLMs exhibit varying degrees of proficiency in making moral judgments across different moral scenarios; while some moral dilemmas may induce indecision in the model, others allow it to firmly uphold its moral choices. Regrettably, prior research has overlooked whether models can maintain steadfast moral convictions across diverse moral scenarios. Figure 1: Our three-module framework to evaluate LLMs’ moral beliefs, including moral choice, moral rank, and moral debate. In this paper, we present a three-module framework to evaluate the moral beliefs of four prominent LLMs, as shown in Figure 1 222The dataset and experiment are comprised entirely of content in Chinese, with all examples in this paper translated into English to facilitate easier comprehension for the readers.. Moral choice: Utilizing ChatGPT and Ernie, and following a meticulous manual filtering process, we compiled a dataset comprising 472 Chinese moral scenarios. These scenarios were crafted drawing upon moral words sourced from a moral dictionary. Moral rank: The decisions made by the LLMs revealed their support for or preference towards specific moral principles. Then, a rank of moral principles were derived by Best-Worst Scaling (BWS) Louviere et al. (2015) and Iterative Luce Spectral Ranking Maystre and Grossglauser (2015). Moral debate: To evaluate the firmness of an LLM in its moral choices, we orchestrated a moral debate between different LLMs. The significance of this study lies in the fact that it explores the moral decision-making ability of LLMs in several ways. First, by using Chinese as one of the research languages, it expands the scope of previous moral research, which is mainly based on English, and shows that there may be significant differences in the moral beliefs of LLMs in different cultural contexts. Secondly, the findings reveal the tendencies of LLMs in making moral choices, and by identifying and understanding the moral beliefs and biases of LLMs, they can be better morally aligned to mitigate potential moral risks and negative impacts, and make their decision-making more in line with the moral and moral standards of the sample of Chinese university students. In addition, the findings reveal the issue of gender bias, suggesting that LLMs may inherit and reinforce real-world stereotypes. Finally, through multiple rounds of debates, our study assessed the extent to which LLMs are firm in their moral choices in the face of challenges, contributing to an understanding of the stability of the moral choice."
https://arxiv.org/html/2411.03644v2,Deploying Multi-task Online Server with Large Language Model,"In the industry, numerous tasks are deployed online. Traditional approaches often tackle each task separately by its own network, which leads to excessive costs for developing and scaling models, especially in the context of large language models. Although multi-task methods can save costs through parameter sharing, they often struggle to outperform single-task methods in real-world applications. To tackle these challenges, we present a three-stage multi-task learning framework for large language models. It involves task filtering, followed by fine-tuning on high-resource tasks, and finally fine-tuning on all tasks. We conducted comprehensive experiments in single-task and multi-task settings. Our approach, exemplified on different benchmarks, demonstrates that it is able to achieve performance comparable to the single-task method while reducing up to 90.9% of its overhead.","In the industry, numerous natural language processing (NLP) tasks are deployed online, and all tasks are required to serve with punctuality and high accuracy. As the number of tasks increases, the demand for resources also grows. Preventing resource requirements from growing linearly with the number of tasks becomes one of the most critical challenge in cost-saving. Traditional approaches tackle each task separately by its own network and pipeline. This leads to excessive workloads for development and maintenance, as well as increased latency and resource usage. Moreover, in the context of large language models (LLMs), it may also lead to excessive costs for scaling up models for each task. We propose utilizing multi-task serving to deploy LLMs instead of single-task serving. single-task serving and multi-task serving are two types of online serving strategies, and their paradigms are shown in Figure 1. Compared to single-task serving, multi-task serving reduces deployment efforts and saves more memory due to the sharing mechanism, thus alleviating resource wastage. Figure 1: Two types of online serving strategies. (a) Independent single-task models are trained and deployed for each task. (b) One multi-task model is trained and deployed for all tasks. However, in real-world applications, multi-task methods often struggle to match the performance of single-task methods due to the data imbalance and task heterogeneity. Data imbalance consistently leads to overfitting in low-resource tasks. This occurs because early stopping is not a feasible solution for high-resource tasks; these tasks require many more epochs to converge. Additionally, heterogeneity may result in negative transfer between tasks. Different tasks require different gradient direction in model optimization, and tasks that are too divergent may conflict in terms of gradient direction. In this paper, we propose a three-stage framework: filtering dissimilar tasks, fine-tuning on high-resource tasks, and fine-tuning on a mixture of all tasks. The task filtering strategy prevents the negative transfer between heterogeneous tasks. The strategy of fine-tuning on high-resource tasks followed by fine-tuning on the mixture effectively effectively enables early stop by allowing different tasks to have different training epochs, thus preventing overfitting of low-resource tasks or underfitting of high-resource tasks. Through an extensive empirical study, we find that our algorithm achieves closer performance to the single-task setting compared to other multi-task baselines. We observed that the improvement in multi-task performance mainly comes from the sampling strategy, the task filtering and domain-specific continual pre-training. Our main contributions can be summarized as follows: (1) We propose a framework for multi-task serving that utilizes LLMs to facilitate the multi-task method that simultaneously handles multiple tasks and achieves comparable performance of that of the single-task method. (2) We run a comprehensive set of experiments that suggest our scheme is practical across different benchmarks and capable of substituting for tasks trained in the single-task method. We also performed extensive experiments to gauge the importance of each of our components, such as task selection and sampling strategy. (3) Our model was deployed to production to provide serving for a total of 11 downstream tasks. Compared to single-task serving, our model achieves comparable performance. We estimate that our system can reduce the total serving costs by up to 90.9% compared to single-task serving."
https://arxiv.org/html/2411.03590v1,From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond,"Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI’s o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1’s performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs.","Prompt engineering as a research area and craft has evolved in step with the fast-paced rise of applications of large language models (LLMs). Prompts shape and focus the capabilities of LLMs trained to follow instructions. In our previous work, we introduced Medprompt, highlighting the effectiveness of inference-time capabilities through use of a structured, multi-step prompt pipeline. Medprompt, developed through exploratory work on prompting strategies to enhance model performance on medical challenge benchmarks, significantly boosts performance by leveraging dynamic chain-of-thought reasoning, curated few-shot examples, and choice-shuffle ensembling [NLZ+23b]. We found that these methods focus and amplify the reasoning abilities of LLMs, with particularly valuable application in high-stakes domains like medical diagnostics and decision-making. Figure 1: Pareto frontier showing accuracy versus total API cost (log scale) on the MedQA benchmark (1273 questions total). We compare o1-preview (Sep 2024), GPT-4o (Aug 2024), and GPT-4 Turbo (Nov 2023) with various run-time steering strategies. Medprompt shows that inference-time strategies can bridge the gap between general-purpose LLMs and domain-specific models that rely on a fixed set of expert-curated prompts and fine-tuning on specialized datasets. In particular, Medprompt demonstrates how error rates on complex medical benchmarks such as MedQA can be reduced by nearly 50%, without adapting model weights to the medical domain. Recent advancements in model training methodologies, exemplified by OpenAI’s o1-preview model, represent a novel approach to harnessing the inherent capabilities of LLMs. In distinction to previous models, o1-preview incorporates chain-of-thought (CoT) reasoning as part of its training process, yielding “reasoning-native” models that inherently perform sophisticated step-by-step problem-solving during inference. The integration of such intrinsic inference capabilities potentially reduces the need for extensive prompt engineering techniques aimed at extracting maximum performance. We re-examine the role and value of sophisticated prompt-engineering strategies, as represented by Medprompt, given the advent of a new paradigm of models that perform run-time CoT reasoning, as represented by the o1 series. Our findings (Figure 1 and 2) indicate that the o1-preview model outperforms GPT-4 augmented with Medprompt on the benchmarks studied and suggest a diminishing necessity for elaborate prompt-engineering techniques that were highly advantageous for earlier generations of LLMs. These results underscore an evolving landscape in which advances in model training have begun to internalize the principles behind prompt engineering aimed at run-time optimization. (a) (b) Figure 2: (a) Comparative analyses of performance of multiple models on MedQA. (b) Comparisons on a wide range of medical challenge benchmarks."
https://arxiv.org/html/2411.03568v1,The American Sign Language Knowledge Graph:Infusing ASL Models with Linguistic Knowledge,"Language models for American Sign Language (ASL) could make language technologies substantially more accessible to those who sign. To train models on tasks such as isolated sign recognition (ISR) and ASL-to-English translation, datasets provide annotated video examples of ASL signs. To facilitate the generalizability and explainability of these models, we introduce the American Sign Language Knowledge Graph (ASLKG), compiled from twelve sources of expert linguistic knowledge. We use the ASLKG to train neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of 91% on ISR, 14% for predicting the semantic features of unseen signs, and 36% for classifying the topic of Youtube-ASL videos.","Computational models of signing aim to improve access to language technologies by automatically understanding and producing a sign language (Bragg et al., 2019). Recognizing that sign language models could benefit signing communities broadly, including tens of millions of deaf and hard-of-hearing people, there have been repeated calls for new technological resources to assist in sign language modeling Diab and Yifru (2022); Yin et al. (2021). These calls strongly emphasize the fair and equitable treatment of those who sign, including respect for signing communities’ autonomy, diversity, and right to privacy (Burke et al., 2024); improving the transparency of how models use signing data (Bragg et al., 2021); and mitigating audist biases in research (Desai et al., 2024; Börstell, 2023). Figure 1: The ASLKG relates the form (e.g., 2/V handshape) and meaning (e.g., related to sight) of signs in the ASL lexicon. We use this knowledge to neuro-symbolically recognize signs (e.g., read) and infer their meaning. In meeting these sociotechnical goals, sign language models must address the scarcity and inconsistent curation quality of signing data, including coverage of the lexicon, consistency of sign labels (if they exist at all), and documented representation among signers (Moryossef and Goldberg, 2021; Muller et al., 2022). Even using the largest and most externally valid datasets of American Sign Language (ASL) for isolated sign recognition (in: video; out: one of 2–3k sign identifiers), end-to-end, data-hungry neural models have struggled to surpass 70%percent7070\%70 % top-1 accuracy (Athitsos et al., 2008; Li et al., 2020; Desai et al., 2023; Kezar et al., 2023b). Meanwhile, end-to-end models can translate ASL weather reports into English (Forster et al., 2014) with a BLEU score of approximately 29292929 (Guan et al., 2024; Chen et al., 2023); however, this metric does not clearly indicate where, how, and why models make mistakes. To facilitate technological progress on these fronts, we introduce the American Sign Language Knowledge Graph (ASLKG), a collection of over 71717171k linguistic facts related to 5802580258025802 ASL signs (§3). The ASLKG is built from 8888 knowledge bases pertaining to ASL linguistics, and supplemented with 4444 more for English lexical semantics. To validate the quality of the ASLKG, we use it to train neuro-symbolic models on three downstream tasks (§4): (a) recognizing isolated signs, (b) predicting the semantic features of out-of-vocabulary signs, and (c) classifying the topic of continuous ASL videos. By grounding a video to phonological features in the ASLKG and reasoning about what those features might mean (e.g., signs, semantic features), we achieve 91%percent9191\%91 % accuracy at recognizing isolated signs, 14%percent1414\%14 % accuracy at predicting unseen signs’ semantic features, and 36%percent3636\%36 % accuracy at classifying the topic of Youtube-ASL videos (§5). The ASLKG is released under the CC BY-NC-SA 4.0 License at this link."
https://arxiv.org/html/2411.03550v1,Learning to Write Rationally:How Information Is Distributed in Non-Native Speakers’ Essays,"People tend to distribute information evenly during language production, such as when writing an essay, to improve clarity and communication. However, this may pose challenges to non-native speakers. In this study, we compared essays written by second language (L2) learners with various native language (L1) backgrounds to investigate how they distribute information in their non-native L2 written essays. We used information-based metrics, i.e., word surprisal, word entropy, and uniform information density, to estimate how writers distribute information throughout the essay to deliver information. The surprisal and constancy of entropy metrics showed that as writers’ L2 proficiency increases, their essays show more native-like patterns will be in the essay, indicating more native-like mechanisms in delivering informative but less surprising content.In contrast, the uniformity of information density metric showed fewer differences across L2 speakers, regardless of their L1 background and L2 proficiency, suggesting that distributing information evenly is a more universal mechanism in human language production mechanisms. This work provides a computational approach to investigate language diversity, variation, and L2 acquisition via human language production.","With the progress of globalization, more people have started acquiring new languages. For instance, the proportion of individuals who speak multiple languages daily in the United States has doubled over the past four decades, rising from about one in ten speakers to about one in five Dietrich et al. (2022). These rapid changes in linguistic diversity offer unique opportunities but also present challenges for the multilingual population: Not all speakers achieve perfect or proficient levels in their non-native languages (L2s) due to various factors, including the quantity and quality of exposure to L2s Leow (1998), the length and styles of their acquisition process Legault et al. (2019), and their native language (L1) backgrounds and experiences Zdorenko and Paradis (2012). The cognitive mechanisms underlying language use in multilingual speakers may differ from those of native speakers, not only due to variations in proficiency but also because of diverse language backgrounds and experiences Bates and MacWhinney (1989); Hernandez et al. (2005). Many previous studies have explored whether and how speakers with different language backgrounds comprehend and produce languages differently. For example, Spanish-English speakers may produce “Spanish-like” sentences in their English production, where such types of grammar are rarely used or even prohibited in English. Most of these studies have reached a similar conclusion: for multilingual speakers, representations are integrated across languages, forming a unified system for human language processing Putnam et al. (2018); Hartsuiker et al. (2004). Consequently, for individuals who know more than one language, the language(s) that are not seemly involved in the target language production task, can also contribute to and influence comprehension and production processes in the target language, leading to unique patterns in human language processing that can reveal information and knowledge from other languages. Despite variations in language production among multilingual speakers, the overarching goal of speaking and writing remains the same: to deliver information effectively. To achieve this goal, people distribute information evenly across language production, maintaining relatively equal predictability for each upcoming word Genzel and Charniak (2002); Frank and Jaeger (2008); Meister et al. (2021). Furthermore, the information carried by a unit of production (e.g., a word) can be quantified in several ways, including surprisal Shannon (1948), entropy Shannon (1948); Genzel and Charniak (2002), and the uniformity of information distribution (UID) Frank and Jaeger (2008); Meister et al. (2021). These metrics help characterize the underlying rules of human language production, which can be summarized as follows: • Surprisal Effect: Processing unexpected information in the produced signal takes longer. • Entropy Rate Constancy (ERC): The rate of information transmitted in a produced unit remains relatively constant across language production. • Uniform Information Density (UID): People prefer to avoid sudden and rapid changes in information density by evenly distributing information across language production. These rules have been substantiated by a wealth of empirical studies. For instance, people need longer time to process unexpected words during comprehension Smith and Levy (2013); Wilcox et al. (2023); during production, people maintain uniformity of information and constancy of predictability by selecting shorter words Mahowald et al. (2013), repetitive/familiar syntactic structures Xu and Reitter (2016, 2018), or faster speech rate Priva (2017). Using information-based metrics, prior studies also explored how the complexity of language production changes across language acquisition, and whether we can predict learners’ proficiency based on those changes Kharkwal and Muresan (2014); Sánchez et al. (2024); Sun and Wang (2021). What remains unknown, despite numerous studies exploring how individuals use these rules to enhance language production, is how L2 speakers apply these rules to distribute information in their L2 production—a topic that remains under-researched. Given that L2 speakers often exhibit different preferences in lexical selection and syntactic structures compared to native speakers Hartsuiker et al. (2004); Van Gompel and Arai (2018)—variations influenced by their language backgrounds—it is reasonable to assume that these differences may result in distinct patterns in their L2 output. In this paper, we use several well-established metrics from psycholinguistics and information science to investigate how speakers with diverse L1 backgrounds and varying levels of L2 proficiency distribute information in their written production."
https://arxiv.org/html/2411.03542v1,Exploring the Benefits of Domain-Pretraining of Generative Large Language Models for Chemistry,"A proliferation of Large Language Models (the GPT series, BLOOM, LLaMA, and more) are driving forward novel development of multipurpose AI for a variety of tasks, particularly natural language processing (NLP) tasks. These models demonstrate strong performance on a range of tasks; however, there has been evidence of brittleness when applied to more niche or narrow domains where hallucinations or fluent but incorrect responses reduce performance. Given the complex nature of scientific domains, it is prudent to investigate the trade-offs of leveraging off-the-shelf versus more targeted foundation models for scientific domains. In this work, we examine the benefits of in-domain pre-training for a given scientific domain, chemistry, and compare these to open-source, off-the-shelf models with zero-shot and few-shot prompting. Our results show that not only do in-domain base models perform reasonably well on in-domain tasks in a zero-shot setting but that further adaptation using instruction fine-tuning yields impressive performance on chemistry-specific tasks such as named entity recognition and molecular formula generation.","Large Language Models (LLMs) and foundation models are becoming increasingly ubiquitous Bommasani et al. (2021) across traditional natural language processing applications Aharoni et al. (2019); Brown et al. (2020) as well as a wide array of domains that can leverage natural language knowledge, reasoning, or enrichments (e.g., law, medicine, biology). Architectures leveraging self-supervised training at scale have become more common as well Ranftl et al. (2021). This has resulted in a vast increase in use of LLMs, and more broadly AI, across a variety of domains from commonsense reasoning to law and from natural sciences to cultural motifs (Sap et al. 2019, Kell et al. 2020, Acharya et al. 2021, Yarlott et al. 2021, Gu et al. 2021, Xiao et al. 2021, Moor et al. 2023). While foundation models have displayed a strong capability to generalize to unseen tasks and to perform in-context reasoning, they struggle to produce consistent, factual output on many tasks Xiao and Wang (2021). This weakness is of particular importance in various scientific or otherwise technical domains, where factual incoherence is a critical impediment to adoption or impactful use where they could otherwise prove incredibly beneficial to domain experts and practitioners as a means of augmenting human expertise for accelerated advances. Due to the vast compute required to train or tune a model at scale, it is critical to understand the trade-off of different strategies to boost performance in domain whether through pre-training from scratch on domain-rich pre-training data, approaches to fine-tune a pre-trained model, or stacking self-supervised pre-training and task-focused fine-tuning. In addition, there is an increasing reliance on interactions with these foundation models using API access (where requests are sent to server-backed instantiations rather than processed locally). Reliance on state-of-the-art models via API access may raise concerns or impede adoption of these methods – e.g., cost concerns (cost of API requests), privacy concerns (information contained within the request being shared externally), and memorization impacts (a subsequent query of the model hitting a version that has been retrained or updated using the previous requests for settings where that would be undesirable). As these (non-empirical) trade-offs that may also motivate a potential choice to leverage the technology of foundation models at a smaller scale for heightened control (on what data models are trained on, what architectures are used, or what scale the models are trained to) increase in consideration, it is important to evaluate performance of both local instantiation strategies and off-the-shelf models. In this work, we focus on a specific scientific field – chemistry – and, leveraging scientific literature from a multitude of sources, we curate a large and diverse chemistry publication dataset to train several chemistry-focused foundational models. We introduce these foundational models for chemistry as AISLE (AI from Scientific Literature) models and present an analysis that quantifies the benefit of in-domain pre-training or fine-tuning compared to off-the-shelf baselines. Our experiments analyze the benefit of in-domain pre-training from scratch and/or task fine-tuning leveraging instruction based prompts for domain-specific tasks with general benchmarking provided for context. Additionally, we adapt the models further by instruction fine-tuning on some chemistry-specific tasks. We see that the performance of models instruction fine-tuned on a combination of all these tasks perform exceptionally well, indicating that is not necessary to perform instruction fine-tuning for each individual task to adapt a model to a domain. We also observe some limitations of these systems, suggesting a potential need to expose the models to perhaps an even more diverse collection of data in the future, e.g., going beyond scientific literature to structural representations of molecular structure and other chemical properties. Overall, this work introduces the following novel contributions: 1. we introduce AISLE models, a set of novel chemistry foundation models pre-trained from scratch on chemistry-focused scientific literature; 2. an analysis of performance gains from domain-adapted models pre-trained from scratch in zero- and few-shot settings; and 3. an empirical comparison of off-the-shelf models compared to domain pre-trained models when boosted by task-focused instruction fine-tuning ."
https://arxiv.org/html/2411.03524v1,Mitigating Metric Bias in Minimum Bayes Risk Decoding,"While Minimum Bayes Risk (MBR) decoding using metrics such as COMET or MetricX has outperformed traditional decoding methods such as greedy or beam search, it introduces a challenge we refer to as metric bias. As MBR decoding aims to produce translations that score highly according to a specific utility metric, this very process makes it impossible to use the same metric for both decoding and evaluation, as improvements might simply be due to reward hacking rather than reflecting real quality improvements. In this work we find that compared to human ratings, neural metrics not only overestimate the quality of MBR decoding when the same metric is used as the utility metric, but they also overestimate the quality of MBR/QE decoding with other neural utility metrics as well. We also show that the metric bias issue can be mitigated by using an ensemble of utility metrics during MBR decoding: human evaluations show that MBR decoding using an ensemble of utility metrics outperforms a single utility metric.","Minimum bayes risk (MBR) decoding is a decoding approach where n𝑛nitalic_n candidate translations are sampled from the MT system, and they are used as pseudoreferences for a reference-based utility metric. MBR decoding computes the utility metric for all O⁢(n2)𝑂superscript𝑛2O(n^{2})italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) pairs of candidates and pseudoreferences, selecting the candidate that achieves the best average score across all pseudoreferences. Quality Estimation (QE) decoding111Also known as QE reranking or QE filtering. selects the candidate that scores best according to a QE utility metric. Previous work on MBR decoding has shown that it results in improvements on the utility metric (Amrhein and Sennrich, 2022; Cheng and Vlachos, 2023; Eikema and Aziz, 2022), however other metrics do not improve as much as the utility metric (Guttmann et al., 2024; Vamvas and Sennrich, 2024). This issue of MBR/QE decoding exhibiting bias towards the utility metric complicates our ability to use automatic metrics to compare the quality of MBR/QE-based MT systems, as we cannot tell whether improvements in automatic metrics from MBR/QE decoding correspond to actual improvements in quality, or if it simply reward hacking. Prior work has assumed that this issue can be avoided by using a different metric for evaluating MBR decoding outputs (Tomani et al., 2023), though this assumption has never been tested. In this work we compare the results of human vs metric-based evaluation of MBR/QE decoding with a wide variety of metrics to show that the quality of MBR/QE decoding is overestimated by not only the utility metric, but also other similar metrics. While MBR/QE decoding with a single utility metric results in significant gains in automatic metrics, it does not perform better than greedy decoding in our human evaluations. This may be due to MBR decoding preferring fluent yet inaccurate candidates. Using an ensemble of metrics as the utility helps us mitigate the metric bias issue, with human evaluations showing that MBR decoding with an ensemble utility metric results in significantly better translations than greedy decoding or MBR/QE decoding with a single utility metric. In this paper we contribute: 1. A large-scale analysis of metric bias in MBR and QE decoding with metrics commonly used in MT, showing that this metric bias issue holds across many different metrics and language pairs, and is not resolved by simply using a different metric for evaluation. 2. Mitigation strategies for MBR bias using QE filtering followed by MBR decoding, as well as MBR decoding using an ensemble of metrics as the utility function. 3. A human evaluation showing that MBR decoding with ensembles outperforms MBR decoding with a single metric."
https://arxiv.org/html/2411.03513v1,Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy,"This paper introduces a novel model compression approach through dynamic layer-specific pruning in Large Language Models (LLMs), enhancing the traditional methodology established by SliceGPT. By transitioning from constant to dynamic slicing, our method leverages the newly proposed Layer Redundancy (LR) score, which assesses how much change each layer changes its input by measuring the cosine similarity of the input to the output of the layer. We use this score to prune parts of individual layers based on redundancy in such a way that the average pruned percentage for all layers is a fixed value. We conducted extensive experiments using models like Llama3-8B and Mistral-7B on multiple datasets, evaluating different slicing bases and percentages to determine optimal configurations that balance efficiency and performance. Our findings show that our dynamic slicing approach not only maintains but, in many cases, enhances model performance compared to the baseline established by constant slicing methods. For instance, in several settings, we see performance improvements of up to 5% over the SliceGPT baseline. Additionally, a perplexity decrease by as much as 7% was observed across multiple benchmarks, validating the effectiveness of our method. The code, model weights, and datasets are open-sourced at https://github.com/RazvanDu/DynamicSlicing.","Large Language Models (LLMs), characterized by their massive scale, often consist of billions to trillions of parameters, enabling them to perform a wide range of complex tasks with remarkable proficiency Kevian et al. (2024); Touvron et al. (2023); Team et al. (2023); Jiang et al. (2023). However, the deployment of these models poses significant challenges, primarily due to the extensive computational resources requirements. As the scale of these models grows, so does the urgency to develop more efficient methods for their deployment. This has led to increased interest in model compression techniques that aim to reduce the computational burden without substantially sacrificing performance. Techniques such as knowledge distillation, quantization, or pruning variants have emerged as viable solutions Wan et al. (2023), each offering a different approach to streamlining model architecture and operations Wang et al. (2024). In this paper, we improve the work on model pruning introduced by SliceGPT Ashkboos et al. (2024), a pruning technique via a constant slicing percentage of each layer. While this approach reduces computational demands and maintains a level of performance, it does not account for the varying significance of different layers within the network. We propose a more nuanced, dynamic pruning method that adapts the degree of pruning based on the individual characteristics and contributions of each layer. Our method aims to optimize both the efficiency and the efficacy of the pruning process by preserving more functionality in critical areas of the model, leading to better performance and less degradation in tasks. More specifically, we develop a new metric, namely Layer Redundancy (LR) score, to quantify the impact of each layer on the model’s overall performance. This evaluation is essential, as it guides the order in which layers are pruned, ensuring that the most influential layers are preserved while less critical layers are removed. Our approach involves generating slicing functions tailored to the importance of each layer, allowing for a dynamic and informed pruning strategy. Our results from the extensive empirical studies across various datasets and base models show a substantial improvement in model accuracy across all datasets tested, accompanied by a notable reduction in perplexity. In order to thoroughly evaluate the effectiveness of our proposed dynamic slicing pattern, we also analyzed the median accuracy and perplexity across a range of models. The results consistently show the superiority of our method over conventional constant slicing techniques. The main contributions of our paper are: • We showed that dynamic, adaptive layer pruning can significantly improve computational efficiency without compromising model performance. • The introduction of the Layer Redundancy score, a new metric to evaluate and guide the dynamic pruning of layers in LLMs. • Extensive empirical validation shows that our method outperforms static pruning techniques in terms of both accuracy and perplexity across various settings."
https://arxiv.org/html/2411.03497v1,Uncertainty Quantification for Clinical Taskswith (Large) Language Models,"To facilitate healthcare delivery, language models (LMs) have significant potential for clinical prediction tasks using electronic health records (EHRs). However, in these high-stakes applications, unreliable decisions can result in high costs due to compromised patient safety and ethical concerns, thus increasing the need for good uncertainty modeling of automated clinical predictions. To address this, we consider the uncertainty quantification of LMs for EHR tasks in white- and black-box settings. We first quantify uncertainty in white-box models, where we can access model parameters and output logits. We show that an effective reduction of model uncertainty can be achieved by using the proposed multi-tasking and ensemble methods in EHRs. Continuing with this idea, we extend our approach to black-box settings, including popular proprietary LMs such as GPT-4. We validate our framework using longitudinal clinical data from more than 6,000 patients in ten clinical prediction tasks. Results show that ensembling methods and multi-task prediction prompts reduce uncertainty across different scenarios. These findings increase the transparency of the model in white-box and black-box settings, thus advancing reliable AI healthcare.","Language models, such as (steinberg2021language, ; theodorou2023synthesize, ; steinberg2024motor, ) have emerged to be an efficient tool in the domain of EHR tasks. These models, extensively trained on diverse sources of clinical data, such as physician notes and longitudinal medical codes, have demonstrated remarkable effectiveness in predicting clinical outcomes. Despite their capabilities, measuring and reducing the uncertainties of these models in EHR tasks is crucial for ensuring patient safety, as clinicians can avoid interventions that the model indicates are uncertain and potentially hazardous. In addition, quantifying the uncertainties in clinical tasks can enhance the reliability of AI-driven medical decision-making systems (begoli2019need, ). To address this challenge, leveraging the transparency of model parameters, we utilize established uncertainty metrics and propose to combine them with ensembling and multi-tasking approaches to effectively quantify and mitigate uncertainties in EHR tasks for these white-box language models. Recently, large language models have embarked on demonstrating their utility in clinical-related tasks, including EHR prediction tasks (wornow2023shaky, ), analyzing radiology report examinations (jeblick2024chatgpt, ) and medical reasoning (lievin2024can, ). However, the encapsulation of modern Large Language Models, typically offered as API services with restricted access to internal model parameters and prediction probabilities, impedes the direct application of traditional uncertainty quantification methods. To overcome this limitation, We redefine uncertainty quantification as a post-hoc approach by analyzing the distribution of answers generated repeatedly from our designed prompts for clinical prediction tasks. Inspired by the effectiveness of our proposed methods in reducing model uncertainty for white-box LMs, we adapted and applied ensembling and multi-tasking methods to the black-box settings. The main contributions of this paper are summarized as follows: • We propose a multi-tasking method and a model ensembling approach to reduce model uncertainties for the white-box language model for clinical predictions using medical code sequences. • We redefine the uncertainty quantification in EHR prediction tasks using black-box LLMs. • We adapted our proposed two methods from white-box LM settings to black-box LLM settings using natural languages and demonstrated their effectiveness in reducing uncertainties."
https://arxiv.org/html/2411.03495v1,Automatic Generation of Question Hints for Mathematics Problems using Large Language Models in Educational Technology,"The automatic generation of hints by Large Language Models (LLMs) within Intelligent Tutoring Systems (ITSs) has shown potential to enhance student learning. However, generating pedagogically sound hints that address student misconceptions and adhere to specific educational objectives remains challenging. This work explores using LLMs (GPT-4o and Llama-3-8B-instruct) as teachers to generate effective hints for students simulated through LLMs (GPT-3.5-turbo, Llama-3-8B-Instruct, or Mistral-7B-instruct-v0.3) tackling math exercises designed for human high-school students, and designed using cognitive science principles. We present here the study of several dimensions: 1) identifying error patterns made by simulated students on secondary-level math exercises; 2) developing various prompts for GPT-4o as a teacher and evaluating their effectiveness in generating hints that enable simulated students to self-correct; and 3) testing the best-performing prompts, based on their ability to produce relevant hints and facilitate error correction, with Llama-3-8B-Instruct as the teacher, allowing for a performance comparison with GPT-4o. The results show that model errors increase with higher temperature settings. Notably, when hints are generated by GPT-4o, the most effective prompts include prompts tailored to specific errors as well as prompts providing general hints based on common mathematical errors. Interestingly, Llama-3-8B-Instruct as a teacher showed better overall performance than GPT-4o. Also the problem-solving and response revision capabilities of the LLMs as students, particularly GPT-3.5-turbo, improved significantly after receiving hints, especially at lower temperature settings. However, models like Mistral-7B-Instruct demonstrated a decline in performance as the temperature increased. This study advances our understanding of the potential and limitations of LLMs in educational contexts, towards integrating these models into pedagogically grounded.","Digital education has gained popularity over the last decade, highlighting the importance of Intelligent Tutoring Systems (ITSs). These systems are seen as essential tools to address specific educational challenges, such as the need for personalized learning in a system often reliant on pedagogical teaching and standardized tests, the inaccessibility of private tutoring for everyone, and the difficulty in finding expert tutors at reasonable costs (Bray, 1999; Graesser et al., 2012). The key feature of ITS is their ability to provide step-by-step guidance to students while they work on problems, with hints playing a crucial role in their capacity to offer this assistance (Kinnebrew et al., 2015). In the educational context, hints refer to pedagogical questions or suggestions given to learners to help them solve problems, answer questions, or complete tasks. Previous research has shown that providing immediate automated feedback to students within ITS can improve learning outcomes (Kochmar et al., 2020; Razzaq et al., 2020). However, designing such systems remains a challenge. Indeed, a system that directly gives the correct answer when the learner is wrong, which may occurs with Large Language Models (LLMs), does not encourage any effort and can diminish engagement (Nie et al., 2024). While a system that recognizes the learner’s incorrect attempt and provides informative hints related to the learner’s existing knowledge encourages critical thinking, problem-solving skills, and independent learning. The challenge to develop such system resides in particular in meeting the diverse learning needs of students and fostering a deeper understanding of complex concepts. These systems can leverage recent advances in Natural Language Processing (NLP), generative AI, and LLMs such as the GPT family models (ChatGPT (cha, )) or Mistral (Jiang et al., 2023), Llama (Touvron et al., 2023a), to be enhanced by integrating LLMs. However, to achieve such a system based on LLMs, these models must meet a wide range of requirements, such as understanding the question and why the student’s answer is incorrect, particularly in mathematics, which is the focus of our study, as well as being aligned with educational goals, pedagogical theory, and cognitive processes. By cognitive processes, we mean the skills we aim to develop, the challenges to include in the exercise, and the potential biases in the student’s understanding. Figure 1: General approach: An LLM acting as a student solves a math exercise and provides its answer and reasoning. If the answer is incorrect, the exercise details (instructions, cognitive approach, correct answer, exercise statement) are passed, with or without the LLM student’s answer and reasoning (depending on the hint-generation prompt type), to another LLM acting as a teacher. The LLM teacher model generates a hint in the form of question using different hint-generation prompts. The hint is then provided to the same LLM student model to revise its response. In this study, we investigate the application of LLMs to generate effective hints for simulated students solving math problems. Figure 1 illustrates the overall approach adopted in this paper. These problems are designed for human students at the high-school level, and are grounded in cognitive science principles (Knops, 2022; Gros et al., 2020). Since hint generation involves using LLMs, it is crucial to first understand how these models perform in generating hints when used with simulated students before applying them to real-world scenarios. LLMs can simulate human behaviors, as demonstrated by (Markel et al., 2023), who built AI students and studied their interactions with human tutors. Accordingly, we have chosen to use LLMs to simulate students and teacher in our experiments. The mains contribution of this work are: 1) the evaluation of GPT-4o effectiveness to identify the types of errors made by the student modeled through GPT-3.5-turbo, Llama-3-8B-Instruct, and Mistral-7B-Instruct-v0.3 while solving math exercises based on the temperature parameter; 2) the investigation of the extent to which LLM teachers can generate pedagogically relevant hints i.e. hints that do not provide the answers to simulated students and whether the temperature setting can influence the ability of these simulated student models to self-correct after receiving such hints; 3) the design of several types of prompts, grouped into two categories: specialized prompts and general prompts, used to prompt the teacher model to generate hints – these two categories differ in their approach: specialized prompts are designed to correct a specific aspect by taking into account the initial answer given by the simulated student, while the general prompts provide hints based on common mistakes that the student model might make when solving math exercises, without considering the simulated student’s initial response; 4) the comparison of these prompts to determine the most effective ones; 5) the evaluation of Llama-3-8B-Instruct as a teacher to generate hints for the GPT-3.5-turbo and Mistral-7B-Instruct-v0.3 students, using the effective prompts identified."
https://arxiv.org/html/2411.03417v1,Usefulness of LLMs as an Author Checklist Assistantfor Scientific Papers: NeurIPS’24 Experiment,"Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peer review. This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting paper submissions against submission standards. We conduct an experiment at the 2024 Neural Information Processing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an “LLM-based Checklist Assistant.” This assistant validates whether papers adhere to the author checklist used by NeurIPS, which includes questions to ensure compliance with research and manuscript preparation standards. Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistant was generally helpful in verifying checklist completion. In post-usage surveys, over 70% of authors found the assistant useful, and 70% indicate that they would revise their papers or checklist responses based on its feedback. While causal attribution to the assistant is not definitive, qualitative evidence suggests that the LLM contributed to improving some submissions. Survey responses and analysis of re-submissions indicate that authors made substantive revisions to their submissions in response to specific feedback from the LLM. The experiment also highlights common issues with LLMs—inaccuracy (20/52) and excessive strictness (14/52) were the most frequent issues flagged by authors. We also conduct experiments to understand potential gaming of the system, which reveal that the assistant could be manipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities of automated review tools.","Recent advancements in large language models (LLMs) have significantly enhanced their capabilities in areas such as question answering and text generation. One promising application of LLMs is in aiding the scientific peer-review process [Sha22, KAD+24]. However, the idea of using LLMs in peer review is contentious and fraught with potential issues [LS23]. LLMs can hallucinate, exhibit biases, and may compromise the fairness of the peer-review process. Despite these potential issues, LLMs may serve as useful analytical tools to scrutinize manuscripts and identify possible weaknesses or inaccuracies that need addressing. In this study, we take the first steps towards harnessing the power of LLMs in the application of conference peer review. We conduct an experiment the the Neural Information Processing Systems (NeurIPS) 2024 conference, a premier conference in the field of machine learning.111In computer science, unlike most other fields, conferences are a primary venue for publication, with the peer-review process evaluating entire manuscripts rather than just abstracts. While the wider ethical implications and appropriate use cases of LLMs remain unclear and must be a larger community discussion, here, we evaluate a relatively clear-cut and low-risk use case: vetting paper submissions against submission standards, with results shown only to the authors. Specifically, the NeurIPS peer-review process requires authors to submit a checklist appended to their manuscripts. Such author checklists, utilized in NeurIPS as well as in other peer-review venues [MSA01, VEA+07, MLT+09], contain a set of questions designed to ensure that authors follow appropriate research and manuscript preparation practices. The NeurIPS Paper Checklist is a series of yes/no questions that help authors check if their work meets reproducibility, transparency, and ethical research standards expected for papers at NeurIPS. The checklist is a critical component in maintaining standards of research presented at the conference. Adhering to the guidelines outlined by these checklists helps authors avoid mistakes that could lead to rejection during peer review. We deploy and evaluate a NeurIPS 2024 Checklist Assistant powered by LLMs. This assistant scrutinizes authors’ responses to the NeurIPS checklist, proposing enhancements for submissions to meet the conference’s requirements. To prevent any potential bias in the review process, we confine its usage exclusively to the authors of papers, so the checklist assistant is not accessible to reviewers. We then systematically evaluate the benefits and risks of LLMs by conducting a structured study to understand if LLMs can enhance research quality and improve efficiency by helping authors understand if their work meets research standards. Specifically, we administered surveys both before and after use of the Checklist Assistant asking authors about their expectations for and perceptions of the tool. We received 539 responses to the pre-usage survey, 234 submissions the the Checklist Assistant and 78 responses to the post-usage survey. Our main findings are as follows: (1) Authors generally reported that the LLM-assisted checklist review was a valuable enhancement to the paper submission process. • The majority of surveyed authors reported a positive experience using the LLM assistant. After using the assistant, over 70% of authors reported that they found the assistant useful and over 70% reported that they would modify their paper and/or checklist responses based on the feedback given (Section 4.1.3). • Authors’ expectations of the assistant’s effectiveness were even more positive before using it than their assessments after actually using it (Section 4.1.3). • Among the main issues reported by authors in qualitative feedback, the most frequently cited were inaccuracy (20/52 respondents) and that the LLM was too strict in its requirements (14/52 respondents) (Section 4.1.4). (2) While changes in NeurIPS paper submissions cannot be causally attributed to use of the checklist verification assistant, we find qualitative evidence that the checklist review meaningfully helped some authors to improve their submissions. • Analysis of the content of LLM feedback to authors indicates that the LLM provided granular feedback to authors, generally giving 4-6 distinct and specific points of feedback per question across the 15 questions (Section 4.2.1). • Survey responses reflect that some authors made meaningful changes to their submissions—35 survey respondents described specific modifications they would make to their submissions in response to the Checklist Assistant (Section 4.2.2). • In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for 80 total paper submissions.) Between these two submissions, authors tended to increase the length of their checklist justifications significantly, suggesting that they may have added content in response to LLM feedback (Section 4.2.3). Finally, we investigate how LLM-based tools can be easily manipulated – specifically, we find that with AI-assisted re-writing of the justifications, an adversarial author can make the Checklist Assistant significantly more lenient (Section 5.1). In summary, the majority of authors found LLM assistance to be beneficial, highlighting the significant potential of LLMs to enhance scientific workflows—whether by serving as direct assistants to authors or helping journals and conferences verify guideline compliance. However, our findings also underscore that LLMs cannot fully replace human expertise in these contexts. A notable portion of users encountered inaccuracies, and the models were also vulnerable to adversarial manipulation. Our code, LLM prompts, and sample papers used for testing are available at: https://github.com/ihsaan-ullah/neurips-checklist-assistant"
https://arxiv.org/html/2411.03397v1,SAUCE\scalerel*X: Synchronous and Asynchronous User-Customizable Environment for Multi-Agent LLM Interaction,"Many human interactions, such as political debates, are carried out in group settings, where there are arbitrarily many participants, each with different views and agendas. To explore such complex social settings, we present SAUCE\scalerel*X: a customizable Python platform, allowing researchers to plug-and-play various LLMs participating in discussions on any topic chosen by the user. Our platform takes care of instantiating the models, scheduling their responses, managing the discussion history, and producing a comprehensive output log, all customizable through configuration files, requiring little to no coding skills. A novel feature of SAUCE\scalerel*X is our asynchronous communication feature, where models decide when to speak in addition to what to say, thus modeling an important facet of human communication. We show SAUCE\scalerel*X’s attractiveness in two initial experiments, and invite the community to use it in simulating various group simulations.111SAUCE\scalerel*X is publicly available on github.com/Deep-Cognition-Lab/SAUCE. A short demo video is available on this YouTube link.","Recent years have seen the rise of large language models (LLMs) with improved chat abilities Köpf et al. (2024). Such models are largely trained and evaluated under two basic assumptions. First, the interaction with an LLM is usually done assuming binary interaction. I.e., there is a single human user issuing natural language instructions which a single LLM then tries to follow. Furthermore, the interaction is synchronous, namely the LLM answers every request by the human user with a single response of its own, to which the user can then respond with a single follow-up request based on the model’s response, etc., where there is no notion of an outside time passing. This framing is expressive enough to handle a wide variety of tasks in various domains. Figure 1: Illustration of a discussion between different agents, run on SAUCE\scalerel*X. Our framework allows setting up a discussion topic, and then manages the group discussion by instantiating models and scheduling their responses. However, many real-world human interactions do not adhere to these shared basic assumptions and therefore cannot be captured in standard LLM applications. First, human interactions are often carried out between arbitrarily many participants, each with potentially differing points of view, and varying objectives for the outcome of the interaction. Often the goal of such multi-party discussion is to find some common ground through agreement or compromise between participants. For example, this is ideally the case in political debates. Second, in many real-world scenarios human interaction is asynchronous, where there is significant challenge in deciding when to speak in addition to deciding what to say. For example, in many strategic bargaining scenarios, such as financial discussions or more structured social games, choosing to remain silent can often convey significant information. In this work, we introduce SAUCE\scalerel*X, a modular and user-friendly Python platform for multi-agent, asynchronous LLM experiments. SAUCE\scalerel*X sets up a discussion room where different models can be instantiated to interact with each other around a shared discussion topic (see Figures 1 and 4), providing both synchronous scheduling, where the LLMs are prompted in a predefined manner, as well as asynchronous scheduling, where SAUCE\scalerel*X keeps track of a simulated outside clock, allowing models to “skip” their turn, based on the outside time and the discussion history. We present experiments showing that SAUCE\scalerel*X effectively facilitates the study of multi-agent LLM interactions in synchronous and asynchronous environments. Taubenfeld et al. (2024) have recently used our platform to simulate political debates between agents representing different political ideologies, uncovering a tendency for LLM agents to conform to the model’s inherent social biases, even when instructed to debate from specific political perspectives. This behavior notably diverges from well-established social dynamics observed in humans. In another experiment, we simulate an asynchronous philosophical debate on the trolley problem Thomson (1984), illustrating how agents adjust their participation based on context and time constraints. This setup showcases the flexibility of asynchronous communication, revealing diverse speaking strategies such as speaking frequently, choosing to wait and listen, and adapting the participation according to the evolving context. SAUCE\scalerel*X can spur research in two complementing directions. First, model developers interested in realistic scenarios with multiple participants or in an asynchronous environment, can readily plug-and play their models to evaluate how they interact with one another. Second, SAUCE\scalerel*X enables user studies incorporating human subjects interacting with LLMs in such settings."
https://arxiv.org/html/2411.03350v1,"A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness","Large language models (LLM) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like LaPM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs’ challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related methods on GitHub: https://github.com/FairyFali/SLMs-Survey.","{forest} for tree= draw, shape=rectangle, rounded corners, top color=white, grow’=0, l sep’=1.2em, reversed=true, anchor=west, child anchor=west, , forked edges, root/.style= draw=customred, rotate=90, shading angle=90, bottom color=white!40, anchor=north, font=, inner sep=0.5em, level1/.style= draw=customred, text width=2cm, bottom color=white!30, font=, inner sep=0.3em, s sep=0.2em, level2/.style= draw=customred, text width=4cm, bottom color=white!40, font=, inner sep=0.25em, s sep=0.1em, level3/.style= draw=customred, text width=4cm, bottom color=white!40, font=, inner sep=0.2em, l sep’=0.5em, level4/.style= draw=customred, text width=6.7cm, top color=customyellow!100, bottom color=customyellow!100, font=, inner sep=0.2em, l sep’=0.5em, where n=0root, where level=1level1, where level=2level2, where level=3level3, where level¿=4level4, [Small Language Models [Introduction (§1) ] [Concepts in Building LMs (§2) [Architecture (§2.1)] [Training techniques (§2.2)] [Obtain SLMs from LLMs (§2.3) [Pruning (§2.3.1) [Unstructured Pruning (Frantar and Alistarh, 2023; Sun et al., 2024c; Zhang et al., 2023a, 2024a; Li et al., 2023e; Shao et al., 2024; Das et al., 2023); Structured Pruning (Men et al., 2024; Ma et al., 2023c; Li et al., 2024c; Yang et al., 2024a; Zhao et al., 2024a; Li et al., 2024g; An et al., 2024; Ashkboos et al., 2024; Chen et al., 2023b; Xia et al., 2024; Guo et al., 2023b)] ] [Knowledge Distillation (§2.3.2) [White-Box KD (Zhang et al., 2023b; Gu et al., 2024; Agarwal et al., 2024; Ko et al., 2024; Jha et al., 2024; Kim et al., 2024b; Padmanabhan et al., 2024); Black-Box KD (Peng et al., 2023a; Chen et al., 2023d; Wang et al., 2023f)] ] [Quantization (§2.3.3) [SqueezeLLM (Kim et al., 2023b); JSQ (Guo et al., 2024a); FrameQuant (Adepu et al., 2024); OneBit (Xu et al., 2024); BiLLM (Huang et al., 2024b); LQER (Zhang et al., 2024c); I-LLM (Hu et al., 2024a); PV-Tuning (Malinovskii et al., 2024); BitNet (Wang et al., 2023e); BitNet b1.58 (Ma et al., 2024); PEQA (Kim et al., 2024a); QLoRA (Dettmers et al., 2024)] ] ] ] [Enhancement of SLMs (§3) [Training Methods for SLMs from Scratch (§3.1) [MobiLlama (Thawakar et al., 2024); MobileLLMs (Liu et al., 2024h); MindLLMs (Yang et al., 2023a); Tang et al. (2024a), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100 ] ] [Supervised Fine-Tuning (§3.2) [Alpaca (Taori et al., 2023); UltraChat (Ding et al., 2023); WizardLM (Xu et al., 2023b); SlimOrca (Lian et al., 2023); ShareGPT (Wang et al., 2024b); Capybara (Daniele and Suphavadeeprasit, 2023); Deita (Liu et al., 2023b); MetaMathQA (Yu et al., 2024a); MobileBERT (Sun et al., 2020); StableLM (Bellagente et al., 2024; Tow et al., [n. d.]); RLHF (Ouyang et al., 2022); DPO (Rafailov et al., 2024), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [Data quality in Knowledge Distillation (§3.3) [TinyStory (Eldan and Li, 2023); Self-Amplify (Bhan et al., 2024); AS-ES Learning (Xi et al., 2024); Huang et al. ([n. d.]); Bhan et al. (2024); Tian et al. (2024), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [Distillation techniques for enhancing SLM (§3.4) [GKD (Agarwal et al., 2024); DistiLLM (Ko et al., 2024); Adapt-and-Distill (Yao et al., 2021); Bit-level Inference Scaling Laws (Dettmers and Zettlemoyer, 2023), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [Performance improvement through quantization (§3.5) [Bit-level Inference Scaling Laws (Dettmers and Zettlemoyer, 2023); BiLLM (Huang et al., 2024b); LLM.int8() (Dettmers et al., 2022); PB-LLM (Shang et al., 2023); OneBit (Xu et al., 2024); BitNet (Wang et al., 2023e); LLM-QAT (Liu et al., 2023a), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [Techniques in LLMs contributing SLMs (§3.6) [RAG for SLMs (Yazan et al., 2024; Liu et al., 2024g); MoE for SLMs (Kim et al., 2023a; Li et al., 2024d), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] ] [Applications of SLMs (§4) [Task-specific SLM applications (§4.1) [SLM applications in QA (§4.1.1) [Phi-series (Gunasekar et al., 2023; Abdin et al., 2024); Orca 2 (Mitra et al., 2023); BioGPT-Large (Guo et al., 2023a); Stable Beluga 7B (Mahan et al., [n. d.]); Phogat et al. (2024); Hartill et al. (2023b); Gichamba et al. (2024); Jeong et al. (2023); Rationale Ranking (Hartill et al., 2023a)] ] [SLM applications in Coding (§4.1.2) [DeepSeek-Coder (Guo et al., 2024b); Phi-1 (Gunasekar et al., 2023); Phi-3.5-mini (Abdin et al., 2024); CodeGemma (Team, 2024a); CodeLlama (Roziere et al., 2023)] ] [SLM applications in Recommender Systems (§4.1.3) [PromptRec (Wu et al., 2024d); SLIM (Wang et al., 2024f); BiLLP (Shi et al., 2024); RecLoRA (Zhu et al., 2024); Wu et al. (2021)] ] [SLM applications in Web Search (§4.1.4) [Content encoder (Chang et al., [n. d.]; Humeau et al., [n. d.]; Lu et al., 2020); H-ERNIE (Chu et al., 2022); Zou et al. (2022); Peng et al. (2023b); CoCondenser (Gao and Callan, 2022); Implicit Interaction (I3superscript𝐼3I^{3}italic_I start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT) (Dong et al., 2023); InPars (Bonifacio et al., 2022); rewrite-retrieve-read (Ma et al., 2023d)] ] [SLM applications in Mobile-device (§4.1.5) [Octopus (Chen et al., 2024a); MobileAgent (Ding, 2024); Carreira et al. (2023); AutoDroid (Wen et al., 2024); Qin et al. (2023); Zhu et al. (2023b)] ] ] [Techniques during SLM Deployment (§4.2) [Memory efficiency (§4.2.1) [MobileAIBench (Murthy et al., 2024); MobileLLM (Liu et al., 2024h); EdgeMoE (Yi et al., 2023); GEAR (Kang et al., 2024); HETLORA (Cho et al., 2024); MobiLlama (Thawakar et al., 2024)] ] [Computing efficiency (§4.2.2) [MobileLLM (Liu et al., 2024h); MobiLlama (Thawakar et al., 2024); Merino (Zhao et al., 2024b); EdgeMoE (Yi et al., 2023); Nawrot et al. (2024); LLMCadLLM-Cad(Xu et al., 2023d); LinguaLinked (Zhao et al., 2023a)] ] ] ] [Models (§5) [Generic-domain SLMs (§5.1) [Llama 3.2; Qwen (Bai et al., 2023; Yang et al., 2024e); Gemma (Team et al., 2024a, b); StableLM (Bellagente et al., 2024; Tow et al., [n. d.]); TinyLlama (Zhang et al., 2024i); OLMo (Groeneveld et al., 2024); H2O-Danube3 (Pfeiffer et al., 2024); Fox-1 (Team, 2024b); MiniCPM (Hu et al., 2024b); Phi (Gunasekar et al., 2023; Li et al., 2023b; Javaheripi et al., 2023; Abdin et al., 2024, 2024); BLOOM and BLOOMZ (Le Scao et al., 2023); Galactica (Taylor et al., 2022); OPT (Zhang et al., 2022); XGLM (Lin et al., 2022b); Megatron-gpt2 (Shoeybi et al., 2019); MINITRON (Muralidharan et al., 2024); LaMini-LM (Kim and Rush, 2016); FlanT5 (Chung et al., 2024) … , for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [Specific-domain SLMs (§5.2) [Hippocrates (Acikgoz et al., 2024); BioMedLM (Bolton et al., 2024b); MentaLLaMA (Yang et al., 2024f); ChemLLM (Zhang et al., 2024e); SciGLM (Zhang et al., 2024d); AstroLLaMA (Nguyen et al., 2023); MindLLM (Yang et al., 2023a), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] ] [SLMs for LLMs (§6) [SLM for reliable LLM generation (§6.1) [POLAR (Zhao et al., 2023b); SAPLMA (Azaria and Mitchell, 2023); SuperICL (Xu et al., 2023c); SuperContext (Yang et al., 2024g); Self-RAG (Asai et al., 2024); SKR (Wang et al., 2023c); SlimPLM (Tan et al., 2024); CRAG (Yan et al., 2024); Feng et al. (2023); Liu et al. (2024f); HaluAgent (Cheng et al., 2024); Toolformer (Schick et al., 2024), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [SLM for extracting LLM prompts (§6.2) [Prompt Stealing Attacks (Sha and Zhang, 2024); Output2prompt (Zhang et al., 2024f); Model Purifying (Li et al., 2024e); Zhang et al. (2024b) , for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [SLM for fine-tuning LLMs (§6.3) [Emulated Fine-tuning (Mitchell et al., 2024); CROSSLM (Deng et al., 2023); Swayamdipta et al. (2020); Mekala et al. (2024); Proxy-tuning (Liu et al., 2024d), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [SLM for LLM applications (§6.4) [SLCoLM (Tang et al., 2024b); HEF (Yang et al., 2024d); BLADE (Li et al., 2024a); Contrastive Decoding (Li et al., 2023d); “Train-Guide-Predict” (Tang et al., 2024b); Sennrich et al. (2023), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] [SLM for LLM evaluation (§6.5) [SLIDE (Zhao et al., 2024d), for tree=text width=11cm, top color=customyellow!100, bottom color=customyellow!100] ] ] [Trustworthiness in SLMs (§7) [HELM (Liang et al., 2023); Do-Not-Answer (Wang et al., 2024d); PromptRobust (Zhu et al., 2023c); GLUE-X (Yang et al., 2023b); HaluEval (Li et al., 2023c); PrivLM-Bench (Li et al., 2024b); FFT (Cui et al., 2023); ROBBIE (Esiobu et al., 2023); TrustLLM (Sun et al., 2024b); RAmBLA (Bolton et al., 2024a); JailbreakBench (Chao et al., 2024); Xie et al. (2024b); OR-Bench (Cui et al., 2024); SORRY-Bench (Xie et al., 2024a); BeHonest (Chern et al., 2024); Hong et al. (2024); Nakka et al. (2024); RUPBench (Wang and Zhao, 2024), for tree=text width=15.53cm, top color=customyellow!100, bottom color=customyellow!100] ] [Future Directions (§8) ] ] Figure 1. Overview of Small Language Models. The evolution of neural language models (LMs) from BERT’s (Devlin et al., 2019) pre-training and fine-tuning paradigm to T5’s (Raffel et al., 2020) pre-training plus prompting approach, and finally to GPT-3’s (Brown et al., 2020) pre-training plus in-context learning, has greatly enhanced natural language processing (NLP). These advancements have broadened NLP’s application across various fields, including language understanding (Wang et al., 2018), programming (Nam et al., 2024; Team, 2024a), recommendation systems (Wang et al., 2024f), information retrieval (Spatharioti et al., 2023; Chang et al., [n. d.]; Humeau et al., [n. d.]; Lu et al., 2020), mobile-device control (Ding, 2024), scientific discovery (Shojaee et al., 2024; Zhang et al., 2024e), medical question answering (Wang et al., 2023d; Bolton et al., 2024b), and legal question answering (Almeida et al., 2024). In particular, the recent emergence of proprietary commercial models including ChatGPT, Bard, and Claude, and open-sourced models such as Llama (Touvron et al., 2023a, b; Dubey et al., 2024) has led to rapid growth in the development of large language models (LLMs). Even though neural networks consistently improve on various tasks with longer training times, larger datasets, and increased model sizes—a phenomenon known as a neural scaling law (Kaplan et al., 2020), these models unpredictably exhibit a sudden acquisition of versatile abilities, termed ”emergent ability,” once they reach a critical scale threshold, thereby supporting the ”larger is better” trend. This ability is not present in small-scale models. For instance, the latest Llama-3.1 model with 405 billion parameters performs better in dialogue, logical reasoning, and programming compared to the smaller 7B counterpart (Dubey et al., 2024). Despite their prowess in complex tasks, LLMs’ huge parameters and computational needs impose significant limitations, hindering their adoption in many real-world applications. For example, the LLaMa 3.1 model with 405 billion parameters (Dubey et al., 2024), trained on 16K H100 GPUs for 54 days, requires about 202.5 GB of GPU memory using int4 precision and has large inference latency. These issues present several challenges in specific contexts: (1) LLMs are generally hosted in the cloud and used via cloud-based APIs due to the large GPU memory and computational cost. Users need to upload their data to query LLMs, raising data leakage and privacy concerns, especially in high-stake scenarios such as healthcare, finance, and e-commerce; (2) Driven by personal agents, on-device deployment is a critical requirement. Several factors, including cloud costs, latency, and privacy concerns, hinder the on-device processing of cloud-based LLMs, and direct deployment is impractical due to their high parameter and cache requirements, which often exceed the capabilities of devices such as mobile phones; (3) Their large parameter count can cause inference delays from seconds to minutes, unsuitable for real-time applications. For instance, Llama 2 7B takes approximately 84 seconds to process 100 tokens on benchmarks including HellaSwag, TruthfulQA, MMLU, and Arc_C when run on a smartphone equipped with a Snapdragon 685 processor (Thawakar et al., 2024); (4) To boost performance in specialized domains like healthcare and law, where generic LLMs underperform, LLMs are often fine-tuned. However, this process is computationally expensive due to their large size. (5) Though general-purpose LLMs are powerful, many real-world applications require only specific abilities and domain knowledge, deploying general-purpose LLMs would be a waste of resources and such LLMs often cannot match the performance of models tailored for specific tasks (Phogat et al., 2024; Guo et al., 2023a; Jeong et al., 2023; Wang et al., 2024f; Chen et al., 2024a). Recently, small language models (SLMs) have shown great potential in alleviating these issues while achieving performance comparable to LLMs for domain-specific problems (Yang et al., 2024e; Team et al., 2024b; Bellagente et al., 2024; Pfeiffer et al., 2024; Zhang et al., 2024i; Groeneveld et al., 2024; Hu et al., 2024b; Abdin et al., 2024; Thawakar et al., 2024; Liu et al., 2024h). We Owing to fewer parameters, SLMs excel in efficiency, cost, flexibility, and customization. They provide significant computational savings in pre-training and inference with reduced memory and storage needs, which is vital for applications requiring efficient resource use. These small models are especially effective in resource-limited settings, performing well on low-power devices such as edge devices. Besides, SLMs improve on-device processing by enhancing privacy, security, response times, and personalization. This supports advanced personal assistants and cloud-independent applications, boosting energy efficiency and reducing carbon emissions. For example, the Llama 3.2 models (1B & 3B) demonstrate that local processing enables immediate execution of prompts and responses (AI, 2024). This approach protects privacy by keeping sensitive data such as patient health information (PHI), business data, personal messages, and calendar details local, enhancing confidentiality. It also allows for precise control over which queries are processed on-device versus those requiring cloud-based models. Therefore, small language models are gaining increasing attention as alternatives to LLMs, as indicated in Figure 2, which shows that SLMs are downloaded more frequently than larger models in the Hugging Face community, and Figure 3, which illustrates the growing popularity of SLM releases over time. Figure 2. Download Statistics Last Month in Huggingface for LLMs with Various Model Sizes, obtained on October 7, 2024. Typically, LMs that exhibit emergent abilities are classified as LLMs. However, the categorization of SLMs remains unclear. Studies vary in their contexts: some define SLMs as models with fewer than one billion parameters (Liu et al., 2024h), while others consider the term “small language model” relative to the larger counterparts (Wang et al., 2024f; Tang et al., 2024b; Lee et al., 2024), with no consensus on a unified definition in the current landscape of LLMs. Research suggests SLMs for mobile devices, typically possessing around 6GB of memory, consist of sub-billion parameter models (Liu et al., 2024h), whereas others classify models with up to 10 billion parameters as small, noting their lack of emergent abilities (Fu et al., 2023). Given their use in resource-constrained environments and for specific tasks, we propose a generalized definition: Given specific tasks and resource constraints, we define SLMs as falling within a range where the lower bound is the minimum size at which the model exhibits emergent abilities for a specialized task, and the upper bound is the largest size manageable within limited resource conditions. This definition integrates various perspectives and addresses factors related to mobile computing and capability thresholds. Due to the growing demand for SLMs, extensive literature has emerged on various aspects of SLMs. For example, several training techniques optimized for SLMs, such as quantization-aware training (Xu et al., 2024; Wang et al., 2023e; Liu et al., 2023a) and selective architectural component choices (Thawakar et al., 2024; Liu et al., 2024h), aim to enhance performance in specific applications (Phogat et al., 2024; Roziere et al., 2023; Wu et al., 2024d; Bonifacio et al., 2022; Chen et al., 2024a). These methods have led to the development of numerous open-source, general-purpose, and domain-specific SLMs (Yang et al., 2024e; Team et al., 2024b; Bellagente et al., 2024; Acikgoz et al., 2024; Bolton et al., 2024b; Zhang et al., 2024d). Beyond their inherent capabilities, SLMs can also serve as a module or effective proxies for enhancing LLMs (Zhao et al., 2023b; Xu et al., 2023a; Wu et al., 2024a; Sha and Zhang, 2024; Mitchell et al., 2024; Yang et al., 2024d). Despite the commendable performance of SLMs, it is crucial not to overlook their credibility issues, such as the risks of producing hallucinations and privacy breaches (Dominguez-Olmedo et al., 2023; Hong et al., 2024; Egashira et al., 2024; Kumar et al., 2024; Nakka et al., 2024; Perez et al., 2023; Mo et al., 2024; Wang and Zhao, 2024; Kumar et al., 2024; Wang et al., 2023a; Yuan et al., 2024). However, currently, there is no comprehensive survey thoroughly exploring these works on SLMs in the era of LLMs. Therefore, this paper offers the first comprehensive survey that analyzes various aspects of SLMs in the LLM era and their future directions. The overview structure of our paper is shown in Figure 1. To summarize, our major contributions are: • In Section 3, we examine various techniques for improving the performance of SLMs, including training from scratch, fine-tuning, knowledge distillation, quantization, and leveraging LLM-enhancing technologies to optimize SLMs. • In Section 4, we discuss the tasks that SLMs can enhance and the deployment strategies that enable models to fit within the resource constraints of edge devices while maintaining acceptable inference speed. • In Section 5, we collect SLMs with fewer than 7 billion parameters across both general-purpose and domain-specific applications, reviewing common architectural choices, training techniques, and datasets, and providing a comparative summary of performance across different model sizes. Recent SLMs are listed. • In Section 6, we explore how SLMs can address key challenges faced by LLMs, such as high inference latency, labor-intensive fine-tuning, susceptibility to knowledge noise, and risks of copyright infringement. • In Section 7, we investigate the trustworthiness issues of SLMs, including hallucination and privacy concerns, by providing a taxonomic summary of current evaluation methods. Figure 3. A timeline of existing small language models. Figure 4. Transformer architecture."
https://arxiv.org/html/2411.04105v2,How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis,"Large language models (LLMs) have shown amazing performance on tasks that require planning and reasoning. Motivated by this, we investigate the internal mechanisms that underpin a network’s ability to perform complex logical reasoning. We first construct a synthetic propositional logic problem that serves as a concrete test-bed for network training and evaluation. Crucially, this problem demands nontrivial planning to solve, but we can train a small transformer to achieve perfect accuracy. Building on our set-up, we then pursue an understanding of precisely how a three-layer transformer, trained from scratch, solves this problem. We are able to identify certain “planning” and “reasoning” circuits in the network that necessitate cooperation between the attention blocks to implement the desired logic. To expand our findings, we then study a larger model, Mistral 7B. Using activation patching, we characterize internal components that are critical in solving our logic problem. Overall, our work systemically uncovers novel aspects of small and large transformers, and continues the study of how they plan and reason.","Language models using the transformer architecture (Vaswani et al., 2017) have shown remarkable capabilities on many natural language tasks (Brown et al., 2020; Radford et al., 2019b). Trained with causal language modeling wherein the goal is next-token prediction on huge amounts of text, these models exhibit deep language understanding and generation skills. An essential milestone in the pursuit of models which can achieve a human-like artificial intelligence, is the ability to perform human-like reasoning and planning in complex unseen scenarios. While some recent works using probing analyses have shown that the activations of the deeper layers of a transformer contain rich information about certain mathematical reasoning problems (Ye et al., 2024), the question of what mechanisms inside the model enables such abilities remains unclear. While the study of how transformers reason in general remains a daunting task, in this work, we aim to improve our mechanistic understanding of how a Transformer reason through simple propositional logic problems. For concreteness’ sake, consider the following problem: Rules: A or B implies C. D implies E. Facts: A is true. B is false. D is true. Question: what is the truth value of C? An answer with minimal proof is “A is true. A or B implies C; C is true.” The reasoning problem, while simple-looking on the surface, requires the model to perform several actions that are essential to more complex reasoning problems, all without chain of thought (CoT). Before writing down any token, the model has to first discern the rule which is being queried: in this case, it is “A or B implies C”. Then, it needs to rely on the premise variables A and B to the locate the relevant facts, and find “A is true” and “B is false”. Finally, it needs to decide that “A is true” is the correct one to invoke in its answer due to the nature of disjunction. It follows that, to write down the first token “A”, the model already has to form a “mental map” of the variable relations, value assignments and query! Therefore, we believe that this is close to the minimal problem to examine how a model internalizes and plans for solving a nontrivial mathematical reasoning problem where apparent ambiguities in the problem specification cannot be resolved trivially. To understand the internal mechanisms of how a transformer solves problems resembling the minimal form above, we perform two flavors of experiments. The first is on shallow transformers trained purely on the synthetic propositional logic problems. This enables a fine-grained analysis in a controlled setting. The other set of experiments are on a pre-trained LLM (Mistral-7B), where we primarily rely on activation patching to uncover necessary circuits for solving the reasoning problem, including specialized roles of certain components. At a high level, we make the following discoveries based on our two fronts of analysis: 1. We discover that small transformers, trained purely on the synthetic problem, utilize certain “routing embeddings” to significantly alter the information flow of the deeper layers when solving different sub-categories of the reasoning problem. We also characterize the different reasoning pathways: we find that problems querying for reasoning chains involving logical operators typically require greater involvement of all the layers in the model. 2. We uncover properties of the circuit which the pretrained LLM Mistral-7B-v0.1 employs to solve the minimal version of the reasoning problem. We find four families of attention heads, which have surprisingly specialized roles in processing different sections of the context: queried-rule locating heads, queried-rule mover heads, fact-processing heads, and decision heads. We find evidence suggesting that the model follows the natural reasoning path of “QUERY→→\to→Relevant Rule→→\to→Relevant Fact(s)→→\to→Decision”. Additionally, we define the scope of our analysis as follows. First, in the shallow transformer experiments, we focus on the variant which only has self-attention layers in addition to layer normalization, positional encoding, embedding and softmax parameters. While we could have also included MLP layers, we choose not to because the no-MLP models already achieve 100% accuracy on the problem, and adding MLPs would unnecessarily complicate the analysis. As a second way to focus the scope of paper, in the Mistral-7B experiments, we do not seek to uncover every model component that participates in solving the reasoning problem. We focus more on finding and analyzing the components that are necessary to the model’s reasoning circuit, and necessary towards implementing the reasoning pathway as described before. By doing so, we can fully justify the necessity of these key components, without guessing about the roles of less-impactful sub-circuits. 1.1 Related works Mechanistic interpretability. Our work falls in the area of mechanistic interpretability, which aims to understand the mechanisms that enable capabilities of the LLM; such studies involve uncovering certain “circuits” in the network (Elhage et al., 2021; Olsson et al., 2022; Meng et al., 2022; Vig et al., 2020; Feng & Steinhardt, 2024; Wu et al., 2023; Wang et al., 2023; Hanna et al., 2024; Merullo et al., 2024; McGrath et al., 2023; Singh et al., 2024; Feng et al., 2024). While the definition of a “circuit” varies across different works, in this paper, our definition is similar to the one in Wang et al. (2023): it is a collection of model components (attention heads, neurons, etc.) with the “edges” in the circuit indicating the information flow between the components in the forward pass; the “excitation” of the circuit is the input tokens. Evaluation of reasoning abilities of LLMs. Our work is also related to the line of work which focus on empirically evaluating the reasoning abilities of LLMs across different types of tasks (Xue et al., 2024; Chen et al., 2024; Patel et al., 2024; Morishita et al., 2023; Seals & Shalin, 2024; Zhang et al., 2023; Saparov & He, 2023; Saparov et al., 2024; Luo et al., 2024; Han et al., 2024; Tafjord et al., 2021; Hendrycks et al., 2021; Dziri et al., 2024; Yang et al., 2024). While these studies primarily benchmark their performance on sophisticated tasks, our work focuses on understanding “how” transformers reason on logic problems accessible to fine-grained analysis. Analysis of how LLMs reason. There are far fewer studies that focus on providing fine-grained analysis of how LLMs reason. To the best of our knowledge, only a handful of works, such as Brinkmann et al. (2024); Xue et al. (2024); Zečević et al. (2023); Ye et al. (2024), share similar goals of understanding how transformers perform multi-step reasoning through detailed empirical or theoretical analysis. However, none studies the [Variable relationships]+[Variable value assignment]+[Query] type problem in conjunction with analysis on both small transformers trained purely on the synthetic problem, and large language models trained on a large corpus of internet data. Activation patching. At its core, activation patching, a.k.a. causal mediation analysis (Vig et al., 2020; Meng et al., 2022; Hase et al., 2024; Heimersheim & Nanda, 2024; Zhang & Nanda, 2024), uses causal interventions for uncovering the internal mechanisms or “circuits” of LLMs that enable them to perform certain tasks. Typically, the LLM is run on pairs of “original” and “altered” prompts, and we search for components inside the model that “alter” the model’s “original behavior” by replacing parts of the model’s activation with “altered activations” when running on the original prompts. The opposite “altered→→\to→original” intervention can also be adopted."
https://arxiv.org/html/2411.03906v1,Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System,"In this paper, we examine the impact of lexicalization on Question Answering over Linked Data (QALD). It is well known that one of the key challenges in interpreting natural language questions with respect to SPARQL lies in bridging the lexical gap, that is mapping the words in the query to the correct vocabulary elements. We argue in this paper that lexicalization, that is explicit knowledge about the potential interpretations of a word with respect to the given vocabulary, significantly eases the task and increases the performance of QA systems. Towards this goal, we present a compositional QA system that can leverage explicit lexical knowledge in a compositional manner to infer the meaning of a question in terms of a SPARQL query. We show that such a system, given lexical knowledge, has a performance well beyond current QA systems, achieving up to a 35.8%percent35.835.8\%35.8 % increase in the micro F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score compared to the best QA system on QALD-9. This shows the importance and potential of including explicit lexical knowledge. In contrast, we show that LLMs have limited abilities to exploit lexical knowledge, with only marginal improvements compared to a version without lexical knowledge. This shows that LLMs have no ability to compositionally interpret a question on the basis of the meaning of its parts, a key feature of compositional approaches. Taken together, our work shows new avenues for QALD research, emphasizing the importance of lexicalization and compositionality.","Question Answering over Linked Data (QALD) [63] is the task of automatically mapping a natural language question to an executable SPARQL query such that relevant information can be retrieved from RDF data sources. One of the seven challenges [69] identified by the authors for the development of QALD systems is handling the lexical gap [69], which requires bridging the way users refer to certain natural language terms and the way they are modeled in a given knowledge base. Consider the question “Who is the mayor of Moscow?”. In this case, “mayor” needs to be interpreted with respect to DBpedia as dbo:leaderName111We use namespace prefixes that are defined as follows: dbr: http://dbpedia.org/resource/, dbo: http://dbpedia.org/ontology/, dbp: http://dbpedia.org/property/, rdfs: http://www.w3.org/2000/01/rdf-schema#, rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# to map the question correctly to the following SPARQL query: SELECT ?o WHERE { dbr:Moscow dbo:leaderName ?o } Another important aspect of QALD is the principle of compositionality. That is, the meaning of a complex expression is determined by the meanings of its parts and the way they are syntactically combined. In the context of QALD, a complex question is represented by a SPARQL query that involves more than one triple pattern, excluding the predicates rdf:type or rdfs:label. For example, the SPARQL query of the complex question “Who is the mayor of the capital of Russia?” is as follows: SELECT ?uri WHERE { dbr:Russia dbo:capital ?o . ?o dbo:leaderName ?uri }. To handle complex questions, the QALD system requires using compositional reasoning to obtain the answer, which includes multi-hop reasoning, set operations, and other forms of complex reasoning. Recent approaches based on machine learning models (e.g., deep neural networks [51, 34, 66, 45], Seq2Seq neural networks [52], transformers [44, 81, 43], subgraph embeddings [6], probabilistic graphical models [30], bi-directional LSTMs [31], and tree-LSTMs [2]) have achieved promising results, and are currently mostly limited to answering simple questions (i.e., only one triple excluding the predicates rdf:type and rdfs:label). To deal with complex queries, Hakimov et al. [32] have proposed an approach that uses Combinatory Categorial Grammar (CCG) [65] for syntactic representations and typed lambda calculus expressions [8] for semantic representations. Some approaches [68, 70] strongly resemble ours, as the motivation is very similar: using explicit lexical information and Dependency-based Underspecified Discourse Representation Structures (DUDES) [10, 14] for semantic composition. However, these approaches generate all possible combinations of SPARQL queries for a natural language sentence, providing no mechanism for disambiguation; therefore, they produce many logically incorrect SPARQL queries. Some QALD approaches [32, 19, 79, 7, 60] have made only limited use of lexicalization, while others [68, 22, 21] have used lexical knowledge but have not systematically investigated its impact. Recently, LLM-based approaches [41, 55, 4, 3, 53, 28, 29] have proven to be powerful tools for NLP tasks. In particular, ChatGPT [67, 24, 80] has been shown to be an alternative to traditional QALD approaches. To our knowledge, Generative Pretrained Transformer (GPT) models have not been tested for their ability to compositionally interpret a question based on the meaning of its parts or the impact of lexical knowledge on their performance. In this paper, we thus address three research questions and provide the corresponding contributions listed below: RQ1. How can a QA system leverage explicit lexical knowledge? Towards this goal, we present a new compositional QA system that relies on a dependency parse and bottom-up semantic composition. RQ2. What is the impact of explicitly given lexical knowledge? Our experimental results show that our compositional system reaches (micro) F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT measures of 0.720.720.720.72 on the QALD-9 dataset, which outperforms existing state of the art systems on the task by far (+ 35%). RQ3. Can Large Language Models also leverage explicit lexical knowledge? Our experiments show that, when encoding lexical knowledge explicitly in the prompt, state-of-the-art LLMs can benefit from such knowledge, improving results. However, they are far from reaching improvements that match the performance of our compositional approach."
https://arxiv.org/html/2411.03814v1,MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue,"Large Language Models (LLMs) demonstrate outstanding performance in their reservoir of knowledge and understanding capabilities, but they have also been shown to be prone to illegal or unethical reactions when subjected to jailbreak attacks. To ensure their responsible deployment in critical applications, it is crucial to understand the safety capabilities and vulnerabilities of LLMs. Previous works mainly focus on jailbreak in single-round dialogue, overlooking the potential jailbreak risks in multi-round dialogues, which are a vital way humans interact with and extract information from LLMs. Some studies have increasingly concentrated on the risks associated with jailbreak in multi-round dialogues. These efforts typically involve the use of manually crafted templates or prompt engineering techniques. However, due to the inherent complexity of multi-round dialogues, their jailbreak performance is limited. To solve this problem, we propose a novel multi-round dialogue jailbreaking agent, emphasizing the importance of stealthiness in identifying and mitigating potential threats to human values posed by LLMs. We propose a risk decomposition strategy that distributes risks across multiple rounds of queries and utilizes psychological strategies to enhance attack strength. Extensive experiments show that our proposed method surpasses other attack methods and achieves state-of-the-art attack success rate. We will make the corresponding code and dataset available for future research. The code will be released soon.","LLMs (e.g. GPT-4) are increasingly being deployed in various applications, including critical decision-making applications. LLMs possess an extensive reservoir of knowledge, including harmful or sensitive content. Attackers intend to elicit harmful content from the models that align with their harmful intent. Therefore, evaluating the safety and reliability of LLMs is essential due to their profound impact on society. Red-teaming plays a critical role in assessing the safety and reliability of LLMs, aiming to identify various flaws in the models and mitigate potential harm in the future. Most red teaming efforts focus on designing single-round attack prompts. For example, some attack methods (Zou et al. 2023a; Wei, Haghtalab, and Steinhardt 2024) hide malicious queries in forms that are not immediately recognizable, such as encoding harmful queries into ciphertext (Yuan et al. 2023) or ASCII codes (Jiang et al. 2024) to prompt the model to generate harmful responses. Subsequent works (Zhu et al. 2023; Yu, Lin, and Xing 2023; Kang et al. 2023; Yuan et al. 2023; Chao et al. 2023a) optimize their attack prompts in either a black-box or white-box manner to elicit model responses to harmful prompts. However, as emphasized in (Ma et al. 2024; Perez et al. 2022), existing methods overlook a crucial aspect: in real-world scenarios, human-LLM interactions are inherently multi-round. Consequently, relying solely on single-round attacks to model these interactions lacks practical significance and fails to capture the complex dynamics between users and LLMs. Recent advancements in multi-round dialogue strategies have demonstrated promising results in red-teaming attacks, effectively exploiting the sequential nature of interactions to conceal harmful intents. For instance, the approach proposed in (Zhou et al. 2024) aggregates responses from each round and then reverses the process to yield harmful content. Alternatively, other methods (Russinovich, Salem, and Eldan 2024; Yang et al. 2024) employ iterative trial-and-error tactics to induce language models to generate unsafe outputs. However, a key limitation of existing multi-round dialogue attack methods is their reliance on powerful language models like GPT to launch the attack, which may trigger the models’ safety mechanisms and result in rejected requests, thereby reducing the attack’s efficiency. Furthermore, while existing methods have demonstrated multi-round attack formats, they lack a core quantifiable and reusable mechanism, which is crucial for advancing the field’s tactical sophistication for various real-world applications. Therefore, in this work, we propose a novel multi-round dialogue attack mechanism. We conceptualize the process of multi-round dialogue attack as a heuristic search process, where, given a malicious query (e.g., how to make a bomb), we identify harmful responses related to the instruction as the targets of the search. The search initiates with a related but innocuous question (e.g., chemical reaction), and, based on the language model’s response, can either explore similar related queries (e.g., chemical experiment) or delve deeper into more sensitive topics (e.g., explosive reaction), ultimately leading to a harmful response. To achieve this, we devise two strategies: • Information-based control strategy, which governs the trial-and-error process by controlling the information similarity between generated inquiries and the original inquiry, • Psychology induction strategy, which incorporates psychological tactics (Zeng et al. 2024a) to minimize the likelihood of rejection as we approach our target. An illustration of our attack mechanism is depicted in Figure 6. We have trained a red-teaming agent (short for MRJ-Agent) to automate the execution of this process, demonstrating a high effectiveness on attack. We further evaluated the attack success rate of our proposed method. The experimental results showed that our approach outperformed existing attack methods, including single-round and multi-round attacks, on both closed-source and open-source models, achieving state-of-the-art attack success rates. Moreover, we tested the versatility of our method on tasks beyond text-to-text, including image-to-text and text-to-text tasks, using closed-source models GPT-4o and DALLE-3 respectively. The experiments demonstrated the generality of our multi-round attack mechanism across various tasks. In summary, owing to its high adaptability and exploratory nature, our proposed method is capable of developing more general attack strategies that can be applied to different models and scenarios. We demonstrate that our method can effectively explore potential vulnerabilities or boundary conditions of the model being tested. We hope that our work will promote further research on the security of large models in the future. Figure 2: Pipeline of our proposed attack"
https://arxiv.org/html/2411.03538v1,Long Context RAG Performance of Large Language Models,"Retrieval Augmented Generation (RAG) has emerged as a crucial technique for enhancing the accuracy of Large Language Models (LLMs) by incorporating external information. With the advent of LLMs that support increasingly longer context lengths, there is a growing interest in understanding how these models perform in RAG scenarios. Can these new long context models improve RAG performance? This paper presents a comprehensive study of the impact of increased context length on RAG performance across 20 popular open source and commercial LLMs. We ran RAG workflows while varying the total context length from 2,000 to 128,000 tokens (and 2 million tokens when possible) on three domain-specific datasets, and report key insights on the benefits and limitations of long context in RAG applications. Our findings reveal that while retrieving more documents can improve performance, only a handful of the most recent state of the art LLMs can maintain consistent accuracy at long context above 64k tokens. We also identify distinct failure modes in long context scenarios, suggesting areas for future research.","The development of Large Language Models (LLMs) with increasingly longer context lengths has opened new possibilities for Retrieval Augmented Generation (RAG) applications. Recent models such as Anthropic Claude (200k tokens) [1], GPT-4-turbo (128k tokens) [2], OpenAI o1 (128k tokens) [3], Llama 3 [4] and Google Gemini 1.5 Pro (2 million tokens) [5] have led to speculation about whether long context models might eventually subsume traditional RAG workflows entirely. In this study, we empirically investigate the impact of increased context length on RAG performance and explore the limitations and challenges that arise in long context scenarios. RAG can enhance the accuracy of LLMs by retrieving information from external sources, enabling users to incorporate task-specific or private data into their LLM workflows. Published results using RAG-like methods have demonstrated benefits across many applications [6] including machine translation [7], semantic parsing [8], question answering [9, 10, 11, 12], and open-ended text generation [13]. With longer context lengths, LLM developers can feed more documents into their RAG applications. While there has been recent speculation that long context LLMs will replace RAG entirely [14], in this paper we study whether long context LLMs can indeed be used effectively for RAG systems. How well do the best open source and commercial models do on long-context RAG tasks? In this study, we apply a standard RAG approach and evaluate the performance of 20 popular open source and commercial LLMs with varying context lengths from 2,000 to 128,000 tokens (and 2 million tokens when possible). We then analyze distinct failure modes for different models across long context RAG scenarios. We show that: • Using longer context does not uniformly increase RAG performance. The majority of models we evaluated first increase and then decrease RAG performance as context length increases. Only a handful of the most recent state of the art LLMs can maintain consistent accuracy at long context above 64k tokens. • LLMs fail at long context RAG in unique ways as a function of context length. While some models tended to provide incorrect answers, others failed to follow instructions or refused to answer due to perceived copyright concerns. Figure 1: Long context RAG performance of o1, GPT-4, Claude 3/3.5, Gemini 1.5 (gemini-1.5-pro-001 and gemini-1.5-flash-001), Llama 3/3.1, Qwen 2, Mistral and DBRX models on 3 curated RAG datasets (Databricks DocsQA, FinanceBench, and Natural Questions). All values can be found in Table S3. Model versions are listed in Table S1."
https://arxiv.org/html/2411.03493v1,LASER: Attention with Exponential Transformation,"Transformers have had tremendous impact for several sequence related tasks, largely due to their ability to retrieve from any part of the sequence via softmax based dot-product attention. This mechanism plays a crucial role in Transformer’s performance. We analyze the gradients backpropagated through the softmax operation in the attention mechanism and observe that these gradients can often be small. This poor gradient signal backpropagation can lead to inefficient learning of parameters preceeding the attention operations. To this end, we introduce a new attention mechanism called LASER, which we analytically show to admit a larger gradient signal. We show that LASER Attention can be implemented by making small modifications to existing attention implementations. We conduct experiments on autoregressive large language models (LLMs) with upto 2.2 billion parameters where we show upto 3.38% and an average of ∼similar-to\sim∼1% improvement over standard attention on downstream evaluations. Using LASER gives the following relative improvements in generalization performance across a variety of tasks (vision, text and speech): 4.67% accuracy in Vision Transformer (ViT) on Imagenet, 2.25% error rate in Conformer on the Librispeech speech-to-text and 0.93% fraction of incorrect predictions in BERT with 2.2 billion parameters.","Transformer architectures (Vaswani et al., 2017) have gained prominence over traditional models like LSTMs (Long-Short-Term-Memory) (Hochreiter & Schmidhuber, 1997) for various sequence-based tasks due to their ability to better capture long-range dependencies (Gemini, 2024; Meta-AI, 2024) without suffering from the vanishing gradient problem (Glorot & Bengio, 2010; Bengio et al., 1994). The attention mechanism plays a key role in Transformers, where different weights or probabilities are assigned to token representations in a sequence, indicating their relative importance, and these weights are computed via a softmax function (Vaswani et al., 2017). The Transformer architecture consists of multiple stacked layers comprising attention mechanism, where each layer operates on the output of the previous one, forming the Transformer encoder or decoder. Learning within a neural network is performed via gradient backpropagation, wherein gradients propagate backward through the network layer by layer using the chain rule (LeCun et al., 2002). During backpropagation, gradient magnitudes tend to diminish, resulting in a weaker gradient signal reaching the bottom layers and inefficient learning, which is called as vanishing gradient problem (Glorot & Bengio, 2010; Bengio et al., 1994). Residual connections (He et al., 2016) are used in Transformers so that gradients can bypass the layers via skip connections during backpropagation to improve the gradient magnitude for bottom layers, reinforcing the idea that architectures capable of efficient gradient backpropagation tend to offer better training performance. In this paper, we theoretically analyze the gradient backpropagation in the attention mechanism of a Transformer and identify a vanishing gradient issue. During backpropagation, the gradient can be scaled by a very small value due to the softmax operation in the attention mechanism. Based on this observation, we propose a modification to the attention mechanism - LASER - LogArithm of Summed Exponentials of Representations. LASER is equivalent to conducting attention on exponentially transformed inputs and admits a log-sum-exp structure. We analytically show that gradients propagated via LASER attention are typically large. Since exp⁡(⋅)⋅\exp(\cdot)roman_exp ( ⋅ ) transformation in LASER can lead to numerical overflow, we develop a novel implementation - Log-Weighted-Sum-Exp trick, inspired from the Log-Sum-Exp trick (Blanchard et al., 2019). This technique allows LASER to scale to large models with upto 2.2 billion parameter models. We show that our implementation requires small modifications, and doesn’t need any changes to the underlying attention mechanism which might admit a more nuanced implementation, for e.g., FlashAttention (Dao et al., 2022). We conduct thorough empirical verification across a variety of Transformer models: Conformer (Gulati et al., 2020) for Librispeech speech-to-text (Panayotov et al., 2015), Vision Transformer(Dosovitskiy et al., 2021) for ImageNet classification (Deng et al., 2009), decoder-only text Transformer (Brown et al., 2020) on C4 dataset (Raffel et al., 2020) and BERT ((Devlin et al., 2018)). We conduct experiments on decoder-only autoregressive language models from 234 million parameters to 2.2 billion parameter models, where we demonstate improvements of up to 1.7% relative improvement in test loss over standard attention. We conduct one-shot evaluation on 17 downstream tasks and show that LASER outperforms standard attention on 14 tasks with upto 3.38% improvement in accuracy and 1% accuracy improvement on average. On a 2.2 billion parameter BERT (Devlin et al., 2018), LASER gives a relative improvement of 0.93% on masked language modeling prediction error rate. LASER also demonstrates a 4.67% relative improvement in validation error rate in Vision Transformer and 1.2% absolute improvement in accuracy, and a 2.25% relative improvement in validation word error rate in the Conformer benchmark."
https://arxiv.org/html/2411.03486v1,"LLM Generated Distribution-Based Prediction ofUS Electoral Results, Part I","This paper introduces distribution-based prediction, a novel approach to using Large Language Models (LLMs) as predictive tools by interpreting output token probabilities as distributions representing the models’ learned representation of the world. This distribution-based nature offers an alternative perspective for analyzing algorithmic fidelity, complementing the approach used in silicon sampling. We demonstrate the use of distribution-based prediction in the context of recent United States presidential election, showing that this method can be used to determine task specific bias, prompt noise, and algorithmic fidelity. This approach has significant implications for assessing the reliability and increasing transparency of LLM-based predictions across various domains.","Language is a symbolic system used to convey semantics—a vehicle for representing meaning. By translating thoughts, experiences, and concepts into structured expressions, language serves as a bridge between abstract ideas and tangible communication. Large Language Models (LLMs) extend this function into the realm of artificial intelligence, demonstrating emergent abilities to handle complex and context rich tasks beyond the scope of smaller language models through natural language [16]. Furthermore, research indicates that LLMs contain coherent and grounded representations that reflect real world distributions [6, 7, 3]. This emergent property of LLMs is demonstrated in Appendix A. These internal world models are informed strongly by training data, frequently resulting in significant biases [4, 11, 15]. The accurate representation of real world distributions through LLM outputs is known as algorithmic fidelity, a concept that has fueled methods such as silicon sampling, the generation of simulated personas in an attempt to simulate real populations [1]. This technique leverages the internal model of LLMs to simulate human-like responses, potentially providing insights into diverse demographic samples without the need for actual data. A key limitation of silicon sampling lies in its tendency to stereotype demographics, as demonstrated in Appendix B. Additionally, recent results have suggested that the success of silicon sampling is due in part to input shortcut features [18]. Prompt noise, such as the ordering of options, can also significantly impact voting results [17, 14]. Furthermore, LLMs tend to assume that people act more rationally then they actually do, thereby deviating from actual human behavior [8]. Despite these challenges, silicon sampling has powerful use cases, and when properly applied can generate realistic survey results [13, 12, 9]. Building on these insights, our paper introduces a novel approach that leverages LLMs as non-persona based predictive models. Rather than having the model simulate individuals, we analyze output probabilities as predictive distributions, interpreting them as indicative of model expectations. We term this approach Distribution Based Prediction. This perspective enables a unique set of investigations, including bias detection through distributional analysis, examination of case by case algorithmic fidelity, robustness testing against variations in prompt design, and evaluation of LLMs’ effectiveness as predictive tools. To demonstrate this approach, we apply it to a real-world prediction task: predicting the outcome of the U.S. presidential election by generating distributions of voter share per state for each candidate. Voter prediction is a significant task for testing the efficacy of LLMs at gathering, synthesizing, and making predictions with data [5, 15]. This context serves as both a testing ground for an LLMs predictive accuracy and a framework to analyze bias, prompt noise sensitivity, and algorithmic fidelity."
https://arxiv.org/html/2411.03471v1,MetRex: A Benchmark for Verilog Code Metric Reasoning Using LLMs,"Large Language Models (LLMs) have been applied to various hardware design tasks, including Verilog code generation, EDA tool scripting, and RTL bug fixing. Despite this extensive exploration, LLMs are yet to be used for the task of post-synthesis metric reasoning and estimation of HDL designs. In this paper, we assess the ability of LLMs to reason about post-synthesis metrics of Verilog designs. We introduce MetRex, a large-scale dataset comprising 25,868 Verilog HDL designs and their corresponding post-synthesis metrics, namely area, delay, and static power. MetRex incorporates a Chain of Thought (CoT) template to enhance LLMs’ reasoning about these metrics. Extensive experiments show that Supervised Fine-Tuning (SFT) boosts the LLM’s reasoning capabilities on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, respectively. While SFT improves performance on our benchmark, it remains far from achieving optimal results, especially on complex problems. Comparing to state-of-the-art regression models, our approach delivers accurate post-synthesis predictions for 17.4% more designs (within a 5% error margin), in addition to offering a 1.7x speedup by eliminating the need for pre-processing. This work lays the groundwork for advancing LLM-based Verilog code metric reasoning.","Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential to transform the field of hardware design, across a wide range of tasks such as Verilog code generation (pearce2020dave, ; thakur2023benchmarking, ; Liu2023verilog, ; lu2024rtllm, ; thakur2023verigen, ), EDA tools scripting (wu2024chateda, ), designing AI accelerators (fu2023gpt4aigchip, ), and fixing RTL syntax errors (tsai2023rtlfixer, ). However, an area yet to be explored is the application of LLMs for reasoning and estimation of post-synthesis metrics of HDL designs. Given HDL code as input, LLMs could potentially infer gate-level details and estimate key metrics, such as area, delay, and static power. While current LLMs can generate raw Verilog code, they lack awareness of post-synthesis metrics and struggle to reason about them effectively. Prior works utilized LLMs to tweak Verilog code to meet area, delay, and power requirements using prompting methods (thorat2023advanced, ) or search methods like Monte-Carlo tree search (yao2024rtlrewriter, ). However, these approaches mainly focus on refining Verilog code, and they do not fundamentally enhance the LLM’s understanding of how different design choices impact post-synthesis metrics. Thus, there is a need for approaches that empower LLMs with deeper insights into the underlying relationships between HDL code and post-synthesis metrics. In light of this, we introduce MetRex, an LLM-based framework for high level metric estimation of HDL designs. MetRex encompasses a large-scale dataset of 25,8682586825,86825 , 868 HDL designs, each annotated with post-synthesis metrics on area, delay, and static power. To enhance the LLM’s capability to understand and reason about these metrics, we propose a Chain of Thought (CoT) template that details the logical steps necessary for computing these metrics. To the best of our knowledge, MetRex is the first framework that addresses the task of LLM-based code analysis for metric estimation of HDL designs. Our contributions are summarized as follows: • We introduce a new dataset, MetRex, for benchmarking Large Language Models (LLMs) for the task of reasoning about post-synthesis metrics of HDL designs. The dataset comprises 25,868 Verilog designs, each annotated with area, delay, and static power metrics. • We developed an automated flow using a Verilog compiler, a synthesis tool, and an LLM agent to detect and resolve syntax and synthesis errors, ensuring a dataset of clean, synthesizable designs. • We introduce a Chain of Thought (CoT) prompting technique that improves the LLM’s reasoning and understanding of post-synthesis metrics by 5.1%, 5.4%, and 8.9% on the area, delay, and static power metrics, respectively, compared to direct prompting methods. • We employ the MetRex dataset in extensive Supervised Fine-Tuning (SFT) experiments, demonstrating that SFT can significantly improve the LLM performance in reasoning and estimating post-synthesis metrics on average by 37.0%, 25.3%, and 25.7% on the area, delay, and static power, compared to few-shot prompting techniques. • We compare the LLM estimation accuracy to regression-based models (wenj2023masterrtl, ), highlighting their potential for this task in offering insightful and direct analysis of HDL code without the need for intermediary formats. LLMs improve the rate of obtaining accurate estimates within a 5% error margin by 17.4% while offering 1.7x faster analysis by eliminating the need for feature extraction and pre-processing. This paper is organized as follows. Section 2 discusses related work. Section 3 presents a general problem formulation of the metric reasoning task with LLMs. Section 4 discusses the MetRex dataset. Section 5 presents experimental results. Section 6 discusses current limitations and future directions. Finally, Section 7 concludes the paper."
https://arxiv.org/html/2411.03445v1,Solving Trojan Detection Competitions with Linear Weight Classification,"Neural networks can conceal malicious Trojan backdoors that allow a trigger to covertly change the model behavior. Detecting signs of these backdoors, particularly without access to any triggered data, is the subject of ongoing research and open challenges. In one common formulation of the problem, we are given a set of clean and poisoned models and need to predict whether a given test model is clean or poisoned. In this paper, we introduce a detector that works remarkably well across many of the existing datasets and domains. It is obtained by training a binary classifier on a large number of models’ weights after performing a few different pre-processing steps including feature selection and standardization, reference model weights subtraction, and model alignment prior to detection. We evaluate this algorithm on a diverse set of Trojan detection benchmarks and domains and examine the cases where the approach is most and least effective.","Trojan backdoors are hidden modifications in neural network models that allow an attacker to alter the model’s behavior in response to a specific trigger, posing significant risks to AI systems. The vulnerability of neural networks to Trojan backdoors is well documented. Techniques for inserting triggers vary from simple data poisoning (Gu et al., 2019) to clean label attacks (Turner et al., 2018; Saha et al., 2019; Liu et al., 2018b) to weight manipulation (Liu et al., 2018b; Garg et al., 2020). There have been several recent surveys covering backdoor attacks (Liu et al., 2020; Li et al., 2021; Wang et al., 2022). A variety of techniques for detecting Trojan behavior have emerged in recent years. These include detecting anomalous samples during neural network training or inference (Chou et al., 2020; Gao et al., 2020; Chen et al., 2018), attempting to recover the Trojan trigger via trigger inversion (Wang et al., 2019; Guo et al., 2019; Wang et al., 2020; Sun et al., 2020; Shen et al., 2021; Huster & Ekwedike, 2021), functional analysis (Sikka et al., 2020; Xu et al., 2019; Edraki et al., 2020; Erichson et al., 2020), activation analysis (Tang et al., 2019), and weight analysis (Fields et al., 2021; Clemens, 2021). In computer vision, techniques like activation clustering (Chen et al., 2018) detect abnormal neuron activations that correspond to backdoor triggers, while Neural Cleanse (Wang et al., 2019) reverse-engineers potential triggers by identifying small input modifications that flip model predictions. Fine-pruning (Liu et al., 2018a) is used to prune rarely activated neurons associated with triggers, effectively neutralizing the backdoor. ABS scanning (Shen et al., 2021), detects neurons that respond abnormally to synthetic perturbations, uncovering hidden backdoors. Spectral signature analysis (Tran et al., 2018) is also employed to identify outliers in neuron activations caused by backdoor inputs . These approaches aim to uncover hidden visual patterns or anomalies that activate backdoors in image-based models. In natural language processing (NLP), backdoors typically appear as specific words or phrases that trigger malicious behavior. Detection techniques include input perturbation, where small modifications to text inputs help reveal triggers, and anomaly detection in embeddings, which identifies outliers in word embeddings or hidden states that correlate with backdoor behavior. Early stopping, perplexity and BERT Embedding distance were proposed in (Wallace et al., 2020) to mitigate and identify poison examples in the training dataset. Traditional defense strategies, relying on model fine-tuning and gradient calculations, are insufficient for Large Language Models due to their computational demands, so the proposed Chain-of-Scrutiny (CoS) method (Li et al., 2024) detects backdoor attacks by generating and scrutinizing detailed reasoning steps to identify inconsistencies with the final answer. In this paper, we introduce a simple, scalable, and powerful method for detecting Trojan backdoors across different domains including computer vision and NLP using linear weight classification. We focus on a common formulation of the problem where a set of clean and poisoned deep neural network models is provided, and the task is to predict whether a given test model is clean or poisoned. The detector is obtained by training a linear classifier on a large number of models’ weights after performing a few different pre-processing steps. We start first by applying tensor and weight selection strategies resembling the first step in a forward-stagewise regression approach (Hastie et al., 2009). Normalization was particularly effective when combined with reference model subtraction. We also explored permutation-invariant representations of tensors, and found that sorting was highly effective in addressing the arbitrary permutations of hidden units in trained neural networks. Our method falls under the category of weight analysis detection, which does not require any prior knowledge of the trigger or model outputs and is applicable across multiple domains. We evaluate our approach on several benchmarks including datasets from the Trojan Detection Challenge (TDC22)(Mazeika, 2022) and the IARPA/NIST TrojAI program(Karra et al., 2020). The Trojan Detection Challenge(TDC22), a NeurIPS 2022 competition, tasks participants with detecting and analyzing Trojan attacks on deep neural networks designed to evade detection. The IARPA/NIST TrojAI program is a long-running initiative that has developed over 16 challenges using this formulation, addressing the issue of adversaries inserting Trojan behaviors into AI models by compromising the training pipeline. The program focuses on identifying such Trojans, which can be activated by specific triggers in an AI’s input, causing the model to produce incorrect responses. We also trained both clean and poisoned models from scratch using the Fashion MNIST dataset for our experiments. This dataset was especially valuable in demonstrating the importance of sorting tensors before training the logistic regression detector for neural networks initialized with random weights. The structure of the paper is as follows: Section 2 discusses weight analysis methods for detecting backdoor models, followed by Section 3, which introduces our proposed methodology. Section 4 explains the experimental setup, including the evaluation metrics and datasets. The experimental results are presented in Section 5 followed by conclusions."
https://arxiv.org/html/2411.03395v1,Exploring Large Language Modelsfor Specialist-level Oncology Care,"Large language models (LLMs) have shown remarkable progress in encoding clinical knowledge and responding to complex medical queries with appropriate clinical reasoning. However, their applicability in subspecialist or complex medical settings remains underexplored. In this work, we probe the performance of AMIE, a research conversational diagnostic AI system, in the subspecialist domain of breast oncology care without specific fine-tuning to this challenging domain. To perform this evaluation, we curated a set of 50 synthetic breast cancer vignettes representing a range of treatment-naive and treatment-refractory cases and mirroring the key information available to a multidisciplinary tumor board for decision-making (openly released with this work). We developed a detailed clinical rubric for evaluating management plans, including axes such as the quality of case summarization, safety of the proposed care plan, and recommendations for chemotherapy, radiotherapy, surgery and hormonal therapy. To improve performance, we enhanced AMIE with the inference-time ability to perform web search retrieval to gather relevant and up-to-date clinical knowledge and refine its responses with a multi-stage self-critique pipeline. We compare response quality of AMIE with internal medicine trainees, oncology fellows, and general oncology attendings under both automated and specialist clinician evaluations. In our evaluations, AMIE outperformed trainees and fellows demonstrating the potential of the system in this challenging and important domain. We further demonstrate through qualitative examples, how systems such as AMIE might facilitate conversational interactions to assist clinicians in their decision making. However, AMIE’s performance was overall inferior to attending oncologists suggesting that further research is needed prior to consideration of prospective uses.","A significant and growing challenge facing healthcare systems globally is the shortage of specialty medical expertise [WHO2016, aamc_physician_supply_2024, charlton2015challenges]. This challenge is particularly acute in subspecialist fields like breast oncology, where projected staffing shortages are compounded by the need for patients to navigate time-consuming sequences of referrals and investigations [asco2023oncology]. This process exacerbates the already limited availability and accessibility of medical expertise, leading to delays in timely and effective care and ultimately increasing the risk of morbidity and mortality due to disease progression [hanna2020mortality]. Large Language Models (LLMs) such as Google’s Gemini [team2023gemini] and OpenAI’s GPT4 [achiam2023gpt] are notable for their ability to understand, generate, and interact with human language. These foundational models serve as versatile building blocks that can be fine-tuned for specialized domains and applied to previously unseen downstream tasks. LLMs display great potential in encoding general medical knowledge [singhal2022large, singhal2023towards], and have demonstrated expert-level performance in a wide variety of tasks including question-answering for medical licensing-style examinations, open-ended consumer question-answering, visual question-answering, medical text summarization [tu2024towards, saab2024capabilities], differential diagnosis in complex cases [mcduff2023towards], and more. This, in turn, has meant LLMs finding applications in non-specialized healthcare tasks such as knowledge retrieval, summarization and administrative assistance [turner2023epic, van2024adapted, Vimalananda2020-aq]; however, their performance in complex subspecialty settings such as oncology care remains poorly examined [sorin2023large]. This study probes the potential of specialized medical LLMs to capture the diagnostic and therapeutic nuances of decision-making for breast oncology care. Breast cancer is the most prevalent malignancy in women, and artificial intelligence (AI) tools have already demonstrated great promise at earlier stages of healthcare delivery for this disease, most notably in imaging for disease screening [mckinney2020international, laang2023artificial]. Beyond initial diagnostics, breast cancer has well-studied management pathways, representing a promising domain for LLMs to serve as clinical decision support tools, and providing a well-structured but challenging setting in which to test the subspecialist medical knowledge of LLMs [gradishar2024breast]. Oncology cases present significant complexity due to variations in disease presentation, the reliance on multiple radiology imaging modalities, intricate family histories, and the incorporation of molecular and genetic studies. Moreover, treatment strategies, including neoadjuvant and adjuvant therapies [shien2020adjuvant], introduce further complexity [wang2023breast]. We define neoadjuvant therapy as a pre-surgical treatment to downstage tumors, while adjuvant therapy is used in the post-surgical setting to eliminate residual disease, tailored according to molecular and histopathological features [kerr2022adjuvant]. Available adjuvant therapies include broad chemotherapy, radiotherapy, targeted molecular therapies, immune checkpoint inhibitors, and endocrine therapies [carlson2006nccn]. While well-resourced cancer centers have access to a range of experienced breast oncologists with whom a case can be referred or discussed, smaller regional providers can face resource constraints or lack access to the same breadth of expertise [bleicher2016time, losk2016factors, bleicher2019treatment, jaiswal2018delays]. An AI system capable of bridging this gap by democratizing access to niche specialty expertise could be an aid for local providers and aid the quality of initial triage. Such a system might, for example, empower oncologists in under-resourced regional or rural settings to make more informed treatment decisions and confident referrals, ultimately contributing to more equitable cancer care [rizvi2024genomic]. In this study, we investigate the performance of Articulate Medical Intelligence Explorer (AMIE) [tu2024towards], a recent research, conversational diagnostic medical AI system, in this subspecialist domain of breast oncology. Our objective is to understand the performance and limitations of LLMs in the type of reasoning and decision-making undertaken by a breast oncology care team when presented with the same results of investigations and plausible case vignettes. This builds on recent work exploring the potential of AMIE in subspecialty cardiology settings [o2024towards]. Our key contributions are as follows: • Open-source dataset of representative breast cancer scenarios: Collaborating with three breast cancer specialists, we develop and open-source a set of 50 synthetic breast cancer scenarios. These scenarios reflect a realistic array of clinical presentations in our collaborating real-world center, ranging from common phenotypes in treatment-naive patients to metastatic and end-stage disease in treatment-refractory individuals. • Comprehensive evaluation rubric for breast oncology assessments: We design a detailed 19-question rubric to rigorously evaluate the quality of breast oncology assessments, focusing on aspects like management reasoning, safety considerations, and summarization. • Novel inference strategy for enhanced LLM performance in breast oncology: We employ a lightweight inference strategy that combines web search and self-critique to enhance the performance of AMIE, a research conversational diagnostic medical AI system, in the subspecialty of breast oncology. This approach leverages external knowledge and a sequential chain of reasoning to improve AMIE’s assessments on the 50 synthetic cases without requiring task-specific fine-tuning. • Benchmarking LLM performance against human clinicians: Our evaluation reveals that AMIE’s performance surpasses that of internal medicine trainees and early oncology fellows along majority of criteria considered. While AMIE demonstrates promising capabilities, it does not yet achieve the consistent performance level of experienced oncology attending specialists. • Illustrating clinical applications: We present qualitative examples of beneficial revisions to clinician assessments and realistic hypothetical dialogue scenarios to illustrate the potential clinical utility of systems like AMIE in democratizing breast oncology expertise. Figure 1: Overview of study design and results. (a) Study design. Breast Oncologists evaluate responses from AMIE and six clinicians for the 30 treatment-naive and 20 treatment-refractory cases using the rubric in Figures 3 and 4. (b) Proportion of favorable responses for each group. On most evaluation criteria, covering aspects of summarization, safety, and management reasoning, AMIE greatly surpasses the performance of trainees, though it falls short of the oncology attendings. See Figures A.1, A.2, A.3 and A.4 for more detailed breakdowns of each group’s performance on the evaluation criteria."
https://arxiv.org/html/2411.03349v1,RuAG: Learned-rule-augmented Generation for Large Language Models,"In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have gained attention for their ability to enhance LLMs’ reasoning by incorporating external knowledge but suffer from limited contextual window size, leading to insufficient information injection. To this end, we propose a novel framework RuAG to automatically distill large volumes of offline data into interpretable first-order logic rules, which are injected into LLMs to boost their reasoning capabilities. Our method begins by formulating the search process relying on LLMs’ commonsense, where LLMs automatically define head and body predicates. Then, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational searching space and efficiently discover logic rules from data. The resulting logic rules are translated into natural language, allowing targeted knowledge injection and seamless integration into LLM prompts for LLM’s downstream task reasoning. We evaluate our framework on public and private industrial tasks, including natural language processing, time-series, decision-making, and industrial tasks, demonstrating its effectiveness in enhancing LLM’s capability over diverse tasks.","Figure 1: Comparison of supervised fine-tuning, in-context learning/retrieval-augmented generation, and our proposed learned-rule-augmented generation (RuAG), which injects logic knowledge to boost generation while reducing computational cost. Leveraging external datasets to enhance the performance of pretrained Large Language Models (LLMs) on downstream tasks has become a significant focus in recent research (Brown et al., 2020a; Hu et al., ; Fan et al., 2024; Dong et al., 2022). Methods such as supervised fine-tuning (SFT) (Hu et al., 2021; Li & Liang, 2021), in-context learning (ICL)(Dong et al., 2022; Wang et al., 2020; Ravi & Larochelle, 2016; Chan et al., 2022; Fang et al., 2024), retrieval-augmented generation (RAG)(Izacard et al., 2023; Fan et al., 2024), and the utilization of knowledge graphs (KGs) (Pan et al., 2024; Shu et al., 2024; Wang et al., 2024) have been explored to incorporate external knowledge into LLMs (Ding et al., 2023; Zhang et al., 2024), enhancing their reasoning and decision-making capabilities. Despite these advancements, these methods face notable challenges. Fine-tuning large LLMs on extensive datasets is computationally intensive and time-consuming, often leading to overfitting and catastrophic forgetting (McCloskey & Cohen, 1989). ICL relies on handcrafted demonstrations and templates that may not effectively summarize large volumes of data, leading to inefficiencies and the “needle in a haystack” problem when processing long contexts (Li et al., 2024), and the extremely long context window significantly increases computational costs (Peng et al., 2024; Naveed et al., 2023). RAG depends heavily on the quality and relevance of retrieved documents and faces computational hurdles when integrating large-scale retrieval into prompts (Fan et al., 2024). Thus, RAG is not able to use the whole of vast knowledge base. Knowledge Graph (KG) based methods incorporate structured representations of knowledge to improve LLMs’ understanding and reasoning (Pan et al., 2024; Shu et al., 2024; Wang et al., 2024). While KGs can enhance decision-making by providing explicit relational data, constructing and maintaining them requires significant manual effort and domain expertise, making scalability challenging. Figure 2: Illustration of logic rules. These challenges underscore the urgent need for efficient knowledge transformation to enhance LLMs’ understanding. Logic rules, with their high information density, act as a promising bridge between vast, diverse data types (including numerical, textual, and visual data) and LLMs’ understanding. Previous work has demonstrated their learnability from external data and their efficiency in providing explanations to enable transparent AI processes (Qu & Tang, 2019; Qu et al., ). A logic rule, as shown in Figure 2, typically expressed as 𝜶→h→𝜶ℎ\bm{\alpha}\rightarrow hbold_italic_α → italic_h, indicates that if a set of events 𝜶𝜶\bm{\alpha}bold_italic_α (referred to as body predicates) occurs, then the event hℎhitalic_h (called the target predicate) will also occur. As an example, the logic rule “Temperature≥30“Temperature30\text{``Temperature}\geq 30“Temperature ≥ 30 AND Humidity≤50Humidity50\text{Humidity}\leq 50Humidity ≤ 50 →→\rightarrow→ Sunny Day” represents knowledge in symbolic structures, suitable for learning from data. Additionally, this rule can be easily translated into natural language: “If the temperature is 30 degrees or higher and the humidity is 50 percent or lower, it will be a sunny day.” Logic rules are understandable to both humans and LLMs as they encapsulate complex relationships in a concise, structured form. Unlike lengthy text passages or extensive datasets in ICL and RAG, logic rules distill essential information into clear, interpretable statements. Compared to the complex node-and-edge structure of KGs, logic rules reduce cognitive load and align better with LLMs’ natural language training. Their direct translation into natural language further improves alignment with LLMs, facilitating more efficient processing and understanding. Inspired by this, we propose a novel framework, learned-rule-augmented generation (RuAG), to automatically compress large external data into logic rules through LLM-aided Monte Carlo Tree Search (MCTS) (Świechowski et al., 2023) and then inform LLMs domain expertise by applying translated logic rules into prompts. Our framework consists of the following three phases. LLM-based Logic Rule Search Formulation: Learning logic rules is expensive due to the involved human effort in formulating the domain-specific search process. Therefore, we automate this process by relying on LLMs’ commonsense to define the target and body predicates in logic rules. First, the target predicate is defined to be task-relevant, like a class label in a classification task or a game state labeled as “win”, while the body predicates are initialized as all the data attributions in the dataset. Then, given the task and dataset descriptions, LLM generates new target predicates and eliminates most irrelevant data attributions from the body predicates. For example, in navigation, LLMs may infer some special place as the key steps to the destination and suggest to search the rules for agents reaching the places individually. Also, LLMs may regard some data attributes as irrelevant to the target predicate, thus excluding them from the candidates. Consequently, the logic rule search space can be significantly reduced, and a domain-specific search process can be automatically established. Logic Rule Search with MCTS: Searching rules requires to discover the relationship among the predicates, suffering from the compositional search space (Qu & Tang, 2019; Zhang et al., 2020; Evans & Grefenstette, 2018). To this end, RuAG exploits MCTS, which works well in large search spaces, to generate structured and understandable first-order logic rules, which are applied in the rule-based generation phase. Learned-Rule-Augmented Generation: RuAG translates the abstract logic rules into natural language and injects them into LLMs’ prompts. By addressing the limitations of SFT, ICL, RAG, and KG-based methods, RuAG offers a scalable and computationally efficient solution for integrating extensive domain knowledge into LLMs, improving LLM’s reasoning, comprehension, and task performance with minimal manual intervention. Our contributions are fourfold. First, we introduce a novel learned-rule-augmented generation framework as a potential alternative to SFT, ICL, RAG, and KG-based methods. This framework systematically and nearly automatically compresses external knowledge into compact, interpretable logic rules that prioritize enhancing LLM generation. Second, we propose an automated formulation for MCTS, eliminating the need for manual, domain-specific rule search and enabling a generalizable approach applicable across a wide range of tasks. Third, we apply MCTS to efficiently handle the large compositional search space of logic rule discovery. Fourth, we evaluate our framework across diverse scenarios, including public tasks in NLP (relation extraction on DWIE), time-series (log anomaly detection on HDFS), decision-making (the cooperative game Alice and Bob), and an industrial task in abuse detection, demonstrating the effectiveness of our approach in both academic and real-world settings."
https://arxiv.org/html/2411.03340v1,Unlocking the Archives: Large Language Models Achieve State-of-the-Art Performance on the Transcription of Handwritten Historical Documents,"This study demonstrates that Large Language Models (LLMs) can transcribe historical handwritten documents with significantly higher accuracy than specialized Handwritten Text Recognition (HTR) software, while being faster and more cost-effective. We introduce an open-source software tool called Transcription Pearl that leverages these capabilities to automatically transcribe and correct batches of handwritten documents using commercially available multimodal LLMs from OpenAI, Anthropic, and Google. In tests on a diverse corpus of 18th/19th century English language handwritten documents, LLMs achieved Character Error Rates (CER) of 5.7 to 7% and Word Error Rates (WER) of 8.9 to 15.9%, improvements of 14% and 32% respectively over specialized state-of-the-art HTR software like Transkribus. Most significantly, when LLMs were then used to correct those transcriptions as well as texts generated by conventional HTR software, they achieved near-human levels of accuracy, that is CERs as low as 1.8% and WERs of 3.5%. The LLMs also completed these tasks 50 times faster and at approximately 1/50th the cost of proprietary HTR programs. These results demonstrate that when LLMs are incorporated into software tools like Transcription Pearl, they provide an accessible, fast, and highly accurate method for mass transcription of historical handwritten documents, significantly streamlining the digitization process.","Introduction As historians and archivists digitize ever larger collections of handwritten records, accurate transcription remains a barrier to their systematic analysis, publication, and accessibility. It is also the most time-consuming and costly aspect of the digitization process. The field of Handwritten Text Recognition (HTR) attempts to automate the task through machine learning but it is a complex and technologically advanced subfield of computer vision and artificial intelligence research, an area in which few historians are comfortable operating. In recent years, programs like as Transkribus, run by the READ-COOP cooperative which includes more than 150 major universities and archives, has made HTR far more accessible through graphical, “drag and drop” interfaces (Kahle et al 2017; Nockels et al 2022). Transkribus largely automates the most complex image pre-processing and segmentation elements of the workflow and allows users to use pretrained neural network models to generate a rough transcription of handwritten documents. These models are highly accurate when they have been trained to recognize a specific individual’s handwriting, achieving word-level accuracy between 85 and 95% (Al Kendi, 2024). That said, they are also moderately expensive and time-consuming to use at roughly $0.27 USD per page with variable turnaround times. Although actual processing normally takes 15-20 minutes, depending on customer volumes transcription requests can be placed in a queue for a day or more. A more important issue is that because handwriting is unique, the most impressive results do not normally reflect most users’ experiences with HTR. Historians are often discouraged when they find that most HTR models have significant difficulty generalizing “out of the box” to new hands, document formats, or styles that they did not encounter in training. By “out-of-the-box”, we mean a model that can be employed “as is” without the user having to train the model on a specific handwriting style. On such tasks, they typically achieve word level accuracy of only 50-75% which make the results largely unusable without major editing. Teaching or fine-tuning models to recognize a specific, individual handwriting style requires users to generate perfect transcriptions of at least 75 handwritten pages (about 15,000 words)—referred to as “ground-truth” documents—which can then be used as training data. As a result, unless historians are working with hundreds or thousands of pages of documents written in a single hand, this fine-tuning process is rarely worth the effort. This means that for most historians, HTR remains a tantalizing but impractical solution to the transcription problem. In this paper, we introduce Transcription Pearl, an easy-to-use software tool that automatically transcribes batches of handwritten documents “out-of-the-box” using widely available multi-modal Large Language Models (LLMs) like GPT-4o, Claude Sonnet-3.5, and Gemini 1.5-Pro, quickly, cost effectively, and to a high degree of accuracy. Depending on how capitalization, punctuation, and historical spelling errors are evaluated, it achieves accuracy levels of between 84 and 93% on transcription tasks without the need for image pre-processing or fine-tuning. We also demonstrate that unlike conventional HTR software, LLMs can be used to correct transcriptions generated by other LLMs as well as Transkribus to levels of accuracy that would normally require extensive fine-tuning, that is above 96%. At this level of accuracy these transcriptions not only approach human levels of accuracy but are “good enough” for most day-to-day use cases such as full text keyword and semantic search as well as improving accessibility and readability. We thus show that the application of LLMs to HTR potentially provides an accessible, affordable, fast, and accurate pathway for both mass transcription projects and individual historians working on their own corpus of records."
https://arxiv.org/html/2411.03321v1,Will Trump Win in 2024? Predicting the US Presidential Election via Multi-step Reasoning with Large Language Models,"Can Large Language Models (LLMs) accurately predict election outcomes? While LLMs have demonstrated impressive performance in various domains, including healthcare, legal analysis, and creative tasks, their ability to forecast elections remains unknown. Election prediction poses unique challenges, such as limited voter-level data, rapidly changing political landscapes, and the need to model complex human behavior. To address these challenges, we introduce a multi-step reasoning framework designed for political analysis. Our approach is validated on real-world data from the American National Election Studies (ANES) 2016 and 2020, as well as synthetic personas generated by the leading machine learning framework, offering scalable datasets for voter behavior modeling. To capture temporal dynamics, we incorporate candidates’ policy positions and biographical details, ensuring that the model adapts to evolving political contexts. Drawing on Chain of Thought prompting, our multi-step reasoning pipeline systematically integrates demographic, ideological, and time-dependent factors, enhancing the model’s predictive power. Also, we apply our framework to predict the outcome of the 2024 U.S. presidential election in advance, demonstrating the adaptability of LLMs to unseen political data.","Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, including natural language understanding, content generation, etc. Brown et al. (2020). Their potential extends far beyond mere text processing, including a broad spectrum of applications from medical diagnostics Zhang et al. (2023) to legal analysis Chalkidis et al. (2022) and creative domains Yang et al. (2022). This versatility stems not only from LLMs’ ability to understand and generate text but also from their capacity to leverage large amounts of common knowledge Roberts et al. (2020), simulate diverse personas Hu and Collier (2024), and effectively model human behavior in complex social science tasks Bommasani et al. (2021). Specifically, LLMs have shown promising results in capturing human-like common sense reasoning Zhou et al. (2020); AlKhamissi et al. (2022) and have been successfully applied to simulate human decision-making in various contexts Zhou et al. (2023); Ziems et al. (2024). These multifaceted capabilities position LLMs as potential tools for simulating human decision-making processes in complex contexts. Recent research has begun exploring LLMs’ political science applications, analyzing policy documents, campaign speeches, and public sentiment Xu (2022); Haq et al. (2023). While the text-based nature of political data certainly aligns with LLMs’ strengths, it is the models’ holistic combination of language understanding, knowledge integration, and human-like reasoning that truly underscores their potential for simulating complicated dynamics of political-related decision-making Argyle et al. (2023); Bisbee et al. (2024). Figure 1: Demonstration of three prompt designs in §3.2. V1 is the direct prompt on voter demographic information, while V2 introduces time-dependent information to capture candidates’ agenda and V3 also uses multi-step reasoning. In this example for 2020 Ohio result prediction, only V3 can accurately predict the results, demonstrating the importance of leveraging both time-dependent information and multi-step reasoning for election result prediction. Motivation. Despite LLMs’ success in the above straightforward political science tasks, their capacity to handle more complex tasks like election prediction remains uncertain Lerer et al. (2022). Indeed, the potential for LLMs to accurately predict election results is an intriguing prospect, given their ability to process vast amounts of historical information and their success in other predictive tasks. However, election forecasting presents unique challenges that test the limits of LLM capabilities. First, the high cost of acquiring voter-level data makes conducting experiments and verifying models in election prediction research challenging. Second, unlike many other predictive tasks, election forecasting requires modeling individual voter behavior as well as the candidates’, which is inherently difficult and shifting with time. It remains uncertain whether text-based data alone can capture this complexity Graefe (2014). Third, accurate election forecasting requires reasoning beyond simple inference, integrating multiple factors such as economic trends, political events, and demographic changes Holbrook (2016). The capacity of LLMs to perform sophisticated reasoning for accurate election predictions is an open question Wei et al. (2022). Our Solution. To address the challenges of using LLMs for election predictions, we propose a novel approach that leverages their strengths while mitigating limitations in data availability, time-varying factors, and complex political dynamics. First, to overcome the scarcity of detailed voter-level data, we employ the Sync synthetic data generation framework Li et al. (2020b), which probabilistically reconstructs individual-level demographic and behavioral profiles from aggregated public datasets. We complement this synthetic data with real-world datasets, such as the American National Election Studies (ANES) 2020 Time Series Studies (2022), ensuring our approach reflects real voting behaviors. Second, our solution adapts to evolving political contexts by incorporating time-dependent factors. Specifically, we aggregate information from presidential campaign data, such as candidates’ policy agendas and biographical backgrounds, to align our model with changing political landscapes Holbrook (2016). Third, we introduce a multi-step reasoning framework tailored for election prediction. Inspired by Chain of Thought prompting Wei et al. (2022), this framework decomposes the prediction process into intermediate steps, enabling the model to systematically integrate demographic information, ideological alignment, and time-sensitive factors. This multi-step reasoning improves the model’s accuracy by addressing biases and overfitting issues observed with simpler approaches. Each component of our framework builds progressively based on observations and refinements. As shown in Figs. 1 and 2, we iterate through multiple pipeline versions to develop our final pipeline. This final version demonstrates significant improvements in both predictive accuracy and alignment with real-world results (Figs. 3, 4, and 5), outperforming other pipelines across all. Our technical contributions include: 1. First Large-Scale LLM-based Election Prediction Framework. This work establishes a new frontier in election forecasting by demonstrating how LLMs can model voter behavior using a combination of real-world data and synthetic datasets, capturing voter-level dynamics with significant scale and detail. 2. Novel Multi-Step Reasoning Framework. We introduce a novel multi-step reasoning process tailored specifically for political forecasting. This framework enhances the model’s ability to integrate and analyze over 11 critical, time-sensitive features—such as policy agendas and candidates’ backgrounds—allowing for more complex, context-aware predictions. 3. New Insights and Future Directions. Our analysis uncovers essential insights into the strengths and limitations of LLMs in election prediction, including potential ideological biases and the challenges of temporal modeling. Future research will explore the integration of multiple LLMs for comparative analysis and further refinement of prompting to improve prediction reliability and robustness."
https://arxiv.org/html/2411.03307v1,LLMs for Domain Generation Algorithm Detection,"This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs). We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection. SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining. We use Meta’s Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs. Results proved that LLM-based methods can achieve competitive results in DGA detection. In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains.","In the ever-evolving landscape of cybersecurity, domain generation algorithms (DGAs) have emerged as a significant threat, posing unique challenges to traditional security measures. DGAs are sophisticated tools employed by cybercriminals to dynamically create large numbers of domain names, primarily used to establish and maintain command and control (C&C) infrastructure for botnets and other malicious activities. The operation of a DGA is based on an initial seed or key, from which sequences of characters are generated to form domain names. These sequences can vary in length and structure, and encryption and obfuscation techniques are often used to further hinder their detection. DGAs pose significant challenges to cybersecurity by enabling botnets to frequently change command-and-control servers, making them resilient to takedown efforts and difficult for defenders to block. They also facilitate covert data exfiltration, malware distribution, and phishing campaigns by generating constantly shifting domains that evade detection by traditional security measures. Additionally, DGAs bypass reputation-based systems with newly created domains lacking history, overwhelming security tools with high volumes of domains, increasing false positives, and straining network resources. Given these challenges, effective DGA detection has become a critical component of modern cybersecurity strategies. Traditional approaches, such as static blacklists and simple heuristics, have proven inadequate against the dynamic nature of DGAs. This has led to increased interest in more sophisticated detection methods, including machine learning and deep learning [1, 2, 3, 4] as they have shown promise in enhancing DGA detection by leveraging more sophisticated feature extraction and classification methods. Similarly, and more recently, the application of large language models (LLMs) has also attracted interest [5]. In this context, LLMs, thanks to their extensive training data and ability to comprehend semantic patterns, offer new potential for effective DGA detection [6, 7]. LLMs’ adaptability and semantic understanding are crucial for analyzing the complex patterns generated by DGAs. Furthermore, LLMs do not require extensive datasets for their application, making them efficient for deployment in dynamic environments. In this work, we analyze the performance of LLM-based methods for DGA detection, comparing them with state-of-the-art models. Our focus is on recent local models, such as Meta’s Llama 3 8B. We explore two main strategies: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT). The evaluation is conducted on a carefully designed dataset that includes examples from 68 distinct DGA families, covering various schemes, including recent word-based DGAs. These word-based schemes generate domains by concatenating sequences of words from one or more wordlists, resulting in domains that appear less random and are therefore more challenging to detect [8]. Our results indicate that SFT with domain-specific data significantly improves detection capabilities in particular reducing the false positive rate (FPR), whereas ICL enables rapid adaptation to new and evolving threats without extensive retraining. This research highlights the potential of LLMs to enhance cybersecurity defenses against DGA-based attacks, providing a comprehensive solution that balances speed and accuracy. The main contributions of the paper are: 1. A dataset with 68 malware families and normal domains from the Tranco dataset [9]. 2. A complete analysis of the potential of ICL and SFT for improving DGA detection using LLMs. 3. A state-of-the-art LLM-based DGA detector with lower FPR and better detection of DGA word-based domains. The remainder of this paper is organized as follows: Section 2 provides technical background on LLMs for DGA detection. Section 3 reviews previous works in DGA detection. Section 4 describes the proposed approach, including the dataset and methodology. Section 5 outlines the design and results of experiments, focusing on ICL and fine-tuning approaches. Section 6 discusses the practical implications of the findings and proposes a strategy for practical implementation. Finally, Section 7 concludes the paper and suggests directions for future research."
https://arxiv.org/html/2411.03300v1,VERITAS: A Unified Approach to Reliability Evaluation,"Large language models (LLMs) often fail to synthesize information from their context to generate an accurate response. This renders them unreliable in knowledge intensive settings where reliability of the output is key. A critical component for reliable LLMs is the integration of a robust fact-checking system that can detect hallucinations across various formats. While several open-access fact-checking models are available, their functionality is often limited to specific tasks, such as grounded question-answering or entailment verification, and they perform less effectively in conversational settings. On the other hand, closed-access models like GPT-4 and Claude offer greater flexibility across different contexts, including grounded dialogue verification, but are hindered by high costs and latency. In this work, we introduce VERITAS, a family of hallucination detection models designed to operate flexibly across diverse contexts while minimizing latency and costs. VERITAS achieves state-of-the-art results considering average performance on all major hallucination detection benchmarks, with 10%percent1010\%10 % increase in average performance when compared to similar-sized models and get close to the performance of GPT4 turbo with LLM-as-a-judge setting.","Large language models (LLMs) (Brown et al., 2020) have made remarkable strides in knowledge intensive tasks such as search, question answering, and natural language understanding. These models, trained on vast amounts of data, possess the ability to generate coherent and contextually relevant text. However, they also exhibit a concerning issue: their generated content often includes plausible but factually incorrect information. These incorrect outputs, known as hallucinations, have raised increasing concerns about the safety and reliability of LLM applications (Xu et al., 2024). We note that, although, hallucinations are undesired for knowledge intensive tasks, the same quirk is actually desirable in creative tasks such as story-telling, image-generation, prose or poetry generation, and brainstorming. Hallucinations are particularly common in closed-book settings, where the knowledge encoded in the model’s weights and the LLM has to recall it during generation. In this setting, there is no relevant source material or context provided to the LLM and it has to solely rely on the knowledge embedded in its weight during the pre-training and post-training stages. Kadavath et al. (2022) show that for such closed-book settings, self-evaluation, wherein a LLM evaluates the validity of its own claims and predicting whether it can correctly answer a user question, works well in the multiple-choice and true/false task settings. On the other hand, in open-book settings, the LLM has access to relevant source materials either provided directly in context or as part of a compound system that uses Retrieval-Augmented Generation (RAG). RAG (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2024) is a widely used approach for building user facing NLP applications, such as systems for grounded question answering (QA), fact-checking, summarization and customer support. LLMs are often unreliable in such open-book settings, and generate responses that contradicts information provided in its context, especially when the knowledge domain of the information in context is out of distribution (Shuster et al., 2021; Magesh et al., 2024). This severely hinders the application of these powerful models on private knowledge stores on complex data involving multi-step reasoning. Many different reasons have been proposed for why LLMs exhibit such quirky behavior including defective training data and benchmarks (Dziri et al., 2022a), bias in training data (McKenna et al., 2023), post-training on new knowledge (Gekhman et al., 2024), and knowledge cutoff (Vu et al., 2023). To address the problem of reliability in LLMs, a number of benchmarks to evaluate factuality have been created in diverse task formats such as NLI, QA, and dialog. Humans or LLMs in the LLM-as-a-Judge settings are primarily used for evaluation on these benchmarks. It leverages the power of LLMs to perform the role of Judges, which can provide judgements on content quality, coherence, and alignment (Vu et al., 2024; Kim et al., 2024). Lynx (Ravi et al., 2024) and BeSpoke-MiniCheck (Tang et al., 2024a) are examples of LLM-as-a-judge trained to judge hallucinations for QA and NLI tasks respectively. These models do no generalize to tasks beyond what they are trained on. Consequently, there is no single model that works across different task formats and on diverse domain datasets. A unified approach to evaluating LLM reliability would not only enable better understanding of gaps in capabilities of LLMs, but also to develop more robust post-training techniques including RLAIF (reinforcement learning with AI feedback) (Lee et al., 2024) wherein factual responses are chosen over non-factual ones. As a result, the ability of the AI judge to distinguish between factual and non-factual responses by cross-checking against retrieved documents is critical for this alignment process (Tian et al., 2023; Lin et al., 2024). Our contributions are three-fold: • We unify the hallucination detection problem and propose a multi-task setup that includes NLI, QA, and dialog. Our VERITAS judge is state-of-the-art on LLM-AggreFact, Halubench, HalluDial, and two proprietary enterprise datasets, reaching the performance of GPT4 Turbo. • We curate VERITAS Collection, high quality, diverse training dataset covering different formats for training hallucination detection models. • We present VERITAS Bench, a unified benchmark for evaluating LLMs on reliability in open-book settings across 18 different datasets covering three different tasks. Figure 1: VERITAS models provide a unified interface for hallucination detection using a multi-task approach comprising of three tasks. 1) NLI task in which the claim or the summary of the given document is checked/ verified 2) Grounded QA in which the answer is assessed for factuality 3) Grounded Dialogue in which the assistant responses are verified. In all these tasks, evaluation is performed based on the given document. Label 1 indicates no hallucination (content is factually consistent), while Label 0 denotes factual inconsistencies."
https://arxiv.org/html/2411.03012v1,Leveraging Large Language Models in Code Question Answering: Baselines and Issues,"Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available111https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.","Generative AI plays a significant role in software engineering, and Code Question Answering is an emerging application within this domain. Question Answering over source code involves generating textual answers to code-related questions. These questions can be formulated by software engineers, project managers, or even by the other generative model. Integrating Generative AI in the form of answering relevant questions about code can bring tangible value to software teams. Code Question Answering (Code Q&A) refers to a task in natural language processing and software engineering (see Figure 1). Unlike traditional code comments or documentation, Code Q&A focuses on interpreting and responding to specific queries about the codebase [2]. Figure 1: Example of Code Q&A task: a question and its corresponding answer on source code. In the Code Q&A scenario, a model is trained to understand natural language questions on a given code piece and to generate appropriate responses. This approach enhances code comprehension by providing developers and stakeholders with an interactive and context-aware way for engaging with the code. By leveraging Code Q&A, software teams can efficiently obtain answers to the queries about their codebase. This approach is versatile, accommodating questions from both developers and project managers. AI-powered Code Q&A streamlines the process and enhances communication within the software development lifecycle whether the software teams seek clarification on code implementation or understanding the software project progress, Despite current advancements in Large Language Models (LLMs), addressing code question answering remains tough. Indeed, although LLMs show promise as intelligent assistants, evaluating LLM performance is challenging. Existing datasets and benchmarks are not enough to assess Code Q&A systems since they focus on specific domains and concepts. Human evaluation of such systems would be the best approach, but it requires a lot of resources, such as human annotators. Another major challenge is the lack of high-quality data for code question answering. While certain open-source LLMs exhibit competence as Code Q&A assistants in zero-shot mode, fine-tuning these models can substantially enhance their performance. Notably, the drawback of many existing datasets for this task lies in the absence of genuine user data since most of these datasets are artificially generated and incorporate no real user inputs. An additional limitation of these datasets lies in their focus on function or method level question answering. For a comprehensive understanding of code, one has to consider the context of entire code files or even repositories. In this research study, we conducted a series of experiments aimed at assessing the capabilities of LLMs in code question answering, with a specific emphasis on open-source models for code, such as StarCoder [19] and DeepSeek-Coder [20]. Therefore, as a research question, we are striving to determine which data processing and modelling approaches allow to improve the evaluation metrics of the code question-answering system based on LLM. In order to reserach this question we built the baseline and tried to improve its evaluation metrics."
https://arxiv.org/html/2411.02973v1,[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI,"We present an outline of the first large language model (LLM) based chatbot application in the context of patient-reported outcome measures (PROMs) for diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable patients to provide feedback about their quality of life and treatment progress via an interactive application. The proposed framework offers significant advantages over the current approach, which encompasses only qualitative collection of survey data or a static survey with limited answer options. Using the PROBot LLM-PROM application, patients will be asked tailored questions about their individual challenges, and can give more detailed feedback on the progress of their treatment. Based on this input, we will use machine learning to infer conventional PROM scores, which can be used by clinicians to evaluate the treatment status. The goal of the application is to improve adherence to the healthcare system and treatments, and thus ultimately reduce cases of subsequent vision impairment. The approach needs to be further validated using a survey and a clinical study.","Monitoring chronic eye diseases is increasingly based on data from imaging exams, functional tests and self-reports of patients. While treatment decisions in medicine depend on all three columns, quantitative data are often available only for imaging biomarkers and functional assessments, while symptoms and health behaviors are mostly assessed qualitatively. Structured questionnaires such as patient-reported outcome measures (PROMs), e.g. the National Eye Institute Visual Function Questionnaire 25 (NEI-VFQ-25, table I) [15, 14], are increasingly used to foster patient-centered care and provide quantitative metrics of patient-reports to clinicians. However, they have been developed in the context of clinical trials and are therefore highly static and not necessarily user-friendly. In PROM questionnaires, patients are asked questions e.g. about their physical and mental well-being, which are to be answered with a score on a numerical scale. Question How much of the time do you worry about your eyesight? At the present time, would you say your eyesight using both eyes (with glasses or contact lenses, if you wear them) is excellent, good, fair, poor, or very poor or are you completely blind? How much difficulty do you have driving in difficult conditions, such as in bad weather, during rush hour, on the freeway, or in city traffic? How much pain or discomfort have you had in and around your eyes (for example, burning, itching, or aching)? How much difficulty do you have reading ordinary print in newspapers? Because of your eyesight, how much difficulty do you have visiting with people in their homes, at parties, or in restaurants ? TABLE I: Exemplary questions from the NEI-VFQ-25 questionnaire for eye disease patients. Due to the static and repetitive design of PROM questionnaires with little to no individualization, low adherence and loss to follow-up are common themes in real-world applications of PROMs in the context of chronic conditions. This impedes the high degree of patient empowerment that can be reached by implementing PROMs into health services. This issue becomes particularly relevant in the case of diabetic retinopathy, which is a prominent and increasing cause of global blindness. Advanced stages of diabetic retinopathy require invasive treatments such as regular injections into the eye, laser coagulation or surgery, which prevents blindness in the affected patients. However, a considerable proportion of people with diabetic retinopathy skips appointments or cancels the treatment altogether, which suggests that the communication between patients and care providers needs to be improved and patients need to be empowered at an earlier stage. Given this status quo, and considering the impressive development of generative artificial intelligence (GenAI), and especially large language models (LLMs) in the last years, we came up with the vision of an interactive chatbot to replace the static PROM-questionnaires. Instead of lacking communication between patients and care providers or a repetitive set of questions, patients can “speak” to the chatbot in real time and answer questions that are streamlined to their personal situation and background. In order to obtain numeric scores for measuring the progress of the treatment - similar to the conventional PROM framework -, another machine learning (ML) model is being trained to predict those based on the patient’s responses. This approach is being validated by comparing the scores predicted by the ML model to those obtained from the original PROM questionnaire. We aim to cooperate with health providers and make our research available to patients via an application. According to the idea of merging the PROM framework with a chatbot interface, we call our approach “PRObot”. In the scope of this vision paper, we qualitatively evaluate the LLM approach, using synthetic data and chatbot interactions with GPT-4o [16]. The next steps involve setting up a survey, followed by a clinical study to get both simulated and actual patient data in order to train the ML interpreter model and quantitatively evaluate the results."
https://arxiv.org/html/2411.02937v2,Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent,"Multimodal Retrieval Augmented Generation (mRAG) plays an important role in mitigating the “hallucination” issue inherent in multimodal large language models (MLLMs). Although promising, existing heuristic mRAGs typically predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws cannot be adequately reflected by current knowledge-seeking visual question answering (VQA) datasets, since the most required knowledge can be readily obtained with a standard two-step retrieval. To bridge the dataset gap, we first construct Dyn-VQA dataset, consisting of three types of “dynamic” questions, which require complex knowledge retrieval strategies variable in query, tool, and time: (1) Questions with rapidly changing answers. (2) Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient and precisely relevant knowledge for dynamic questions due to their rigid retrieval processes. Hence, we further propose the first self-adaptive planning agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate the human behavior in question solution which dynamically decomposes complex multimodal questions into sub-question chains with retrieval action. Extensive experiments111The work of Alibaba Group led by Xinyu Wang. ∗*∗: Equal contribution. ‡‡{\ddagger}‡: Corresponding author. Code and dataset will be open-sourced at https://github.com/Alibaba-NLP/OmniSearch. prove the effectiveness of our OmniSearch, also provide direction for advancing mRAG.","Multimodal Retrieval Augmented Generation (mRAG) (Zhao et al., 2024; 2023; Gao et al., 2023) aims to provide more comprehensive, accurate and up-to-date knowledge from external sources for AI systems. It has emerged as a key technology to mitigate the “hallucination” issue (Liu et al., 2024a; Bai et al., 2024) inherent in multimodal large language models (MLLMs). Figure 1: Bottom: Heuristic mRAG based VQA. Upper: OmniSearch based VQA. Existing heuristic mRAG methods typically predefined fixed retrieval processes that ground all modalities into one primary modality (usually text), then retrieve for a single time. Despite the promising results, these retrieval strategies suffer from two issues: (1) Non-adaptive Retrieval Queries. These inflexible retrieval strategies fail to adapt to evolving contexts or intermediate findings within a question, hindering the model from re-retrieving to further comprehend, verify, or rethink the question. (2) Overloaded Retrieval Queries. These one-time retrieval strategies place too excessive burden on a single query, which contains multiple aspects. This potentially results in an influx of superficially relevant knowledge yet not essential to the question solving. Therefore, as shown in Figure 1, when faced with real-world questions requiring complex knowledge, current heuristic mRAG methods fail to provide sufficient and precise knowledge, due to their rigidity issues. Figure 2: Dynamic VQA examples that require different kinds of retrieval strategies. Unfortunately, although several knowledge-seeking visual question answering (VQA) datasets (Chen et al., 2023; Schwenk et al., 2022) are widely utilized as mRAG benchmarks, they cannot adequately reflect the rigidity issues of heuristic mRAGs in acquiring complex knowledge. Since most questions in them merely require textual knowledge within two-hop, which can be readily obtained by heuristic mRAGs with a standard two-step retrieval process. As illustrated in the upper left of Figure 2, the most common type of question inquires about a certain property of an object. To facilitate the mRAG research and bridge the dataset gap, we first construct a challenging dataset, Dyn-VQA, comprising 1,452 dynamic questions that require complex multimodal knowledge retrieval for solution. Dynamic questions are defined as questions that require the model to flexibly provide knowledge retrieval solutions, where the query, tool, and time of retrievals are all variable. These questions cannot be solved by a predefined retrieval process. Concretely, there types of dynamic questions are included in Dyn-VQA: (1) Questions with rapidly changing answers. Since the context knowledge of such question updates frequently, the retrieved content may be mixed with outdated and newer knowledge that is difficult to distinguish. This requires mARG methods to flexibly plan additional retrievals based on feedback from current retrieved content for further comprehension, rather than merely a one-time retrieval. (2) Questions requiring multi-modal knowledge. The knowledge necessary by Dyn-VQA spans various modalities. This demands that mRAG methods retrieve knowledge across diverse modalities with tailored retrieval APIs, differing from most VQA datasets limited in seeking textual knowledge with multimodal questions. (3) Multi-hop questions. Questions in Dyn-VQA necessitate varied reasoning hops for solution, which entails that mRAG methods conduct various retrieval steps. While existing VQA datasets primarily focus on two-hop question, i.e., identifying visual concepts via text and then answering single-hop textual question. We evaluated the performance of various mRAG methods combined with leading MLLMs on Dyn-VQA. Experiments reveal that existing heuristic mRAGs struggle to provide sufficient and precisely relevant knowledge for dynamic questions in Dyn-VQA, due to their rigid retrieval processes. To address these issues, we further propose a self-adaptive planning agent for multimodal retrieval, OmniSearch222We hope OmniSearch can achieve Omnipotent Multimodal Search for solving real-world questions in the future.. The underlying idea is to emulate the human behavior in question solution which dynamically decomposes complex multimodal questions into sub-question chains with retrieval action. At each step, OmniSearch flexibly adjusts the next action according to question-solving state and retrieved content, with diverse purposes such as deepening comprehension of retrieved content, modifying retrieval method for current sub-question, proposing the next sub-question, etc. It is noteworthy that OmniSearch can serve as a plug-and-play RAG module, cooperating with arbitrary MLLMs to augment their capability in addressing complex dynamic questions. Two different versions of OmniSearch are developed based on closed-source GPT-4V (Achiam et al., 2023) and open-source Qwen-VL-Chat (Bai et al., 2023a), respectively. As far as we know, OmniSearch is the first multimodal retrieval agent for VQA tasks. In summary, our main contributions are fourfold: • We reveal that existing VQA-based mRAG benchmarks fail to reflect the feature that real-world questions require dynamic knowledge retrieval, and propose novel Dyn-VQA dataset, which contains three types of dynamic questions. • We benchmark various mRAG methods with leading MLLMs on Dyn-VQA, demonstrating their flaw in providing sufficient and relevant knowledge for dynamic questions. • We propose OmniSearch, a self-adaptive retrieval agent that plans each retrieval action in real-time according to question solution stage and current retrieval content. • Extensive experiments prove the effectiveness of our OmniSearch. Detailed analyses are conducted to provide direction for advancing mRAG."
https://arxiv.org/html/2411.02886v1,TokenSelect: Efficient Long-Context Inference andLength Extrapolation for LLMs viaDynamic Token-Level KV Cache Selection,"With the development of large language models (LLMs), the ability to handle longer contexts has become a key capability for Web applications such as cross-document understanding and LLM-powered search systems. However, this progress faces two major challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues hinder the application of LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic, training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using Query-Key dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a small number of critical KV cache tokens in the attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we designed the Selection Cache based on observations of consecutive Query similarity and implemented efficient dot product kernel, significantly reducing the overhead of token selection. A comprehensive evaluation of TokenSelect demonstrates up to 23.84×23.84\times23.84 × speedup in attention computation and up to 2.28×2.28\times2.28 × acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.","With the rapid development of large language models (LLMs), the number of parameters is no longer the sole factor significantly affecting model performance. The ability to effectively process longer context information has become one of the key metrics for evaluating LLMs’ capabilities. The latest Web applications such as cross-document understanding (Bai et al., 2024), LLM-powered search systems (Sharma et al., 2024), repository-level code completion (Zhang et al., 2023; Di et al., 2024), and complex reasoning (OpenAI, [n. d.]) have all placed higher demands on the long-context abilities of LLMs. There are two main difficulties in using pre-trained LLMs for long-context inference. On one hand, LLMs are limited by their context length during pre-training (e.g. Llama 3 only has 8192 tokens). Directly inferencing on longer sequences can lead to severe performance degradation due to reasons including sequence lengths out-of-distribution (Xiao et al., 2024; Han et al., 2024). On the other hand, even if LLMs possess sufficiently large context lengths, the quadratic computational complexity of attention with respect to sequence length makes the response time for long-context inference unbearable. Figure 1. Distribution of tokens participating in attention computation under different sparsity patterns (indicated by blue dots). TokenSelect can more accurately select critical tokens for attention computation. Previous works have made numerous attempts to address these difficulties. To extend the context length of LLMs, the current common practice is to perform post-training on long texts (Team et al., 2024; Yang et al., 2024; GLM et al., 2024). However, this approach comes with significant computational costs, particularly in two aspects: the synthesis of high-quality long-text data and the training process on extended sequences. To accelerate long-context inference, many studies focus on the sparsity of attention, attempting to reduce the scale of KV Cache involved in computation. The key to this type of method lies in designing sparse patterns for attention, which can be mainly divided into two categories: one uses predefined sparse patterns (Wang et al., 2019; Zaheer et al., 2020; Xiao et al., 2024; Han et al., 2024), while the other estimates the potential importance of KV Cache during the inference process (Zhang et al., 2024; Oren et al., 2024; Xiao et al., 2024; Tang et al., 2024; Jiang et al., 2024), attempting to select relevant KV Cache tokens into attention calculations. However, the design of these sparse patterns is often heuristically based on historical criticality or coarse-grained criticality estimation of tokens, making it difficult to ensure that the selected tokens are truly critical, thus resulting in sub-optimal performance, as shown in Fig. 1. In this paper, we further observe the non-contiguous sparsity of attention, revealing the importance of designing more fine-grained dynamic sparse patterns. To this end, we propose TokenSelect, a model-agnostic and training-free approach that utilizes token-level selective sparse attention for efficient long-context inference and length extrapolation. Specifically, for each Query, TokenSelect dynamically calculates token-level per-head criticality for the past KV Cache and selects the k𝑘kitalic_k most critical tokens through our head soft vote mechanism, involving them in the attention calculation. This reduces the scale of attention calculation to a constant length familiar to the model, while maintaining almost all of the long-context information, thereby simultaneously addressing the two main difficulties for long-context inference. To reduce the overhead of token selection, TokenSelect manages the KV Cache in token-level pages (Zheng et al., 2024) and design efficient kernel for token selection based on Paged KV Cache management through Triton (Tillet et al., 2019). Furthermore, based on our observation of high similarity between consecutive queries, we have designed the Selection Cache, which allows consecutive similar queries to share token selection results, thereby reducing the selection frequency while ensuring its effectiveness. We evaluate the performance and efficiency of TokenSelect on three representative long-context benchmarks (Zhang et al., 2024; Hsieh et al., 2024; Bai et al., 2024) using three open-source LLMs (Yang et al., 2024; Dubey et al., 2024; AI et al., 2024). The experimental results demonstrate that our TokenSelect can achieve up to 23.84×23.84\times23.84 × speedup in attention computation compared to FlashInfer (flashinfer ai, [n. d.]), and up to 2.28×2.28\times2.28 × acceleration in end-to-end inference latency compared to state-of-the-art long-context inference method (Xiao et al., 2024). Simultaneously, it provides superior performance on three long-text benchmarks. In summary, we make the following contributions: • An observation on the non-contiguous sparsity of attention that highlights the importance of token-level selection. • TokenSelect, a model-agnostic and training-free method that achieves accurate and efficient long-context inference and length extrapolation, which is compatible with mainstream LLM serving systems and ready for Web applications. • A comprehensive evaluation of TokenSelect, demonstrating up to 23.84×23.84\times23.84 × speedup in attention computation and up to 2.28×2.28\times2.28 × acceleration in end-to-end latency while exhibiting superior performance."
https://arxiv.org/html/2411.02864v1,Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning,"Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning capability on many NLP tasks. Recasting an NLP task into a text-to-text generation task is a common practice so that generative LLMs can be prompted to resolve it. However, performing document-level relation extraction (DocRE) tasks with generative LLM models is still challenging due to the structured output format of DocRE, which complicates the conversion to plain text. Limited information available in few-shot samples and prompt instructions induce further difficulties and challenges in relation extraction for mentioned entities in a document. In this paper, we represent the structured output as a graph-style triplet rather than natural language expressions and leverage generative LLMs for the DocRE task. Our approach, the Graph-DPEP framework is grounded in the reasoning behind triplet explanation thoughts presented in natural language. In this framework, we first introduce a “decomposed-plug"" method for performing the generation from LLMs over prompts with type-space decomposition to alleviate the burden of distinguishing all relation types. Second, we employ a verifier for calibrating the generation and identifying overlooked query entity pairs. Third, we develop ""ensemble-play"", reapplying generation on the entire type list by leveraging the reasoning thoughts embedded in a sub-graph associated with the missing query pair to address the missingness issue. Through extensive comparisons with existing prompt techniques and alternative Language Models (LLMs), our framework demonstrates superior performance on publicly available benchmarks in experiments.","Figure 1. An example of document relation extraction. Document-level relation extraction (DocRE) extracts relations among multiple entity pairs in a document, representing a more realistic and challenging task than sentence-level extraction (Ji et al., 2017; Alt et al., 2020). In DocRE, an entity can have multiple mentions scattered throughout a document, and relationships between entities can appear in multiple different sentences. We illustrate a running example in Figure 7. All the entities that occurred in the context are marked in bold. These relations can be identified by intra-sentence hints, like ""Jack Ganto was born in Norvelt"", which can be found in Sent#4. But ""Calumet is in the country, the United States"", which should be reasoned from Sent#0 and Sent#1, because Sent#0 indicates that Norvelt is in the United States and Sent#1 indicates Calumet and Norvelt are from the same community. Most traditional transform-based DocRE models (Ma et al., 2023; Tan et al., 2022a; Yan et al., 2021) filter information in a lengthy document by retrieving relevant sentences as the supporting evidence to identify the relation for a given query entity mention pair. The idea of supporting evidence retrieval inspires us to make reasoning thoughts when we recast the traditional DocRE into a generation task. Large language models (LLMs) like ChatGPT present remarkable success in generative and reasoning tasks. Reformulating the traditional NLP tasks into text-to-text generation formats to perform LLMs attracts intensive focus, especially under low-resource scenarios. For DocRe, annotation on evidence sentences is expensive, which restrains traditional model ability in real-world scenarios. Therefore, we leverage LLMs as the few-shot learner to address few-shot DocRE in a generative manner. However, existing LLMs are mostly pre-trained on unstructured data, which leads to poor performance when dealing with tasks that require a structured format. Prior studies (Wadhwa et al., 2023; Li et al., 2023) utilize graph-like triplet sets, code-style frames, and similar approaches. As we also utilize triplet notation for describing extracted relations, we can establish a semantically rich graph structure to store information in a format readable by large language models (LLMs). On the other hand, even with a reasonable number of few-shot examples, LLMs still face challenges in generating graph-structured data, and the outputs from LLMs remain error-prone. Moreover, compared with regular relation extraction (Wadhwa et al., 2023; Li et al., 2023), DocRE suffers from extraction on a large label space, which poses a significant challenge in response time and quality of LLMs. Even given several relation extraction examples for in-context learning, LLMs are susceptible to the risk of misinterpreting labels within the vast number of possibilities. In this paper, we investigate the end-to-end few-shot DocRE problem via a generative model and propose Graph-DPEP, a decomposed-plug and ensemble-play framework that allows self-verification on generation under relation graph-of-thoughts reasoning. To assist LLMs in distinguishing intensive labels when transferring a classification task into a generation task, the decomposed-plug component processes an LLM to make a generation on each single type. What’s more, we inspect the generation results with a verifier module for concerns such as repetition, irrelevance, incompleteness, and missing query pairs. The verifier incorporates calibration principles to refine the generation, addressing issues except for missingness, which is identified for subsequent adjustments to compensate for performance losses in this aspect. Reapplying LLM on the missing pairs can be facilitated by an association sub-graph that is relevant to entities in the missing query pairs. So, we employ the association sub-graph as the graph-of-thoughts to further aid reasoning on missing pairs’ relations from the entire type list rather single one in the decomposed plug, so-called ensemble play. We summarize our contributions as follows: • To our knowledge, we are the first to transfer DocRE from a classification task to a generation task by constructing a decomposed method for prompt engineering to address the huge relation type space. • We compare our approach with the best existing prompting techniques available to us and our Graph-DPEP method achieves the most promising recalls on the whole type space, especially for infrequent types. • We employ the latest LLMs for evaluation of the effectiveness of the prompt design in Graph-DPEP. The experiment results show the advantages of our decomposed-plug and ensemble-play method with a self-correctable verifier."
https://arxiv.org/html/2411.02830v1,Mixtures of In-Context Learners,"In-context learning (ICL) adapts LLMs by providing demonstrations without fine-tuning the model parameters; however, it does not differentiate between demonstrations and quadratically increases the complexity of Transformer LLMs, exhausting the memory. As a solution, we propose Mixtures of In-Context Learners (MoICL), a novel approach to treat subsets of demonstrations as experts and learn a weighting function to merge their output distributions based on a training set. In our experiments, we show performance improvements on 5 out of 7 classification datasets compared to a set of strong baselines (up to +13% compared to ICL and LENS). Moreover, we enhance the Pareto frontier of ICL by reducing the inference time needed to achieve the same performance with fewer demonstrations. Finally, MoICL is more robust to out-of-domain (up to +11%), imbalanced (up to +49%), or noisy demonstrations (up to +38%) or can filter these out from datasets. Overall, MoICL is a more expressive approach to learning from demonstrations without exhausting the context window or memory.","In-context learning (ICL), where we condition a large language model (LLM) on a set of input–output examples (demonstrations) to perform a wide range of tasks (Brown et al., 2020; Wei et al., 2022), is a transformative technique in NLP. However, in ICL, the context length of the model severely limits the maximum number of in-context demonstrations (Wei et al., 2022), and its effectiveness can vary significantly depending on what demonstrations are selected (Lu et al., 2022; Chen et al., 2023). Current methods for selecting demonstrations are largely heuristic and do not adequately quantify the influence of individual examples on the generalisation properties of the model (Lu et al., 2024). Figure 1: A Mixture of In-Context Learners (MoICL) first partitions a set of demonstrations D𝐷Ditalic_D in k𝑘kitalic_k partitions to create k𝑘kitalic_k experts trained via in-context learning, and then combines their next-token predictions via a trainable weighting function. In general settings, demonstrations are often selected randomly over different seeds or based on simple criteria (Xu et al., 2024), which can lead to suboptimal performance. But is each demonstration high quality and useful, or merely noise? And can we automate this distinction? We propose Mixtures of In-Context Learners (MoICL), a method for dynamically learning how different sets of examples contribute to the prediction task. MoICL prompts an LLM with multiple subsets of examples, and combines their next-token distributions via a weighting function that can be trained via gradient-based optimisation methods; Fig. 1 shows a high-level outline of the method. We analyse the generalisation properties of MoICL in the following settings: (1) presence of out-of-distribution (OOD) demonstrations, where some in-context demonstrations are sourced from a different dataset; and (2) label imbalance, where the training label distribution is significantly skewed towards a subset of labels. (3) noised demonstrations, where the labels of some demonstrations are perturbed to be completely incorrect. In all three cases, we find that MoICL produces significantly more accurate results than ICL. Furthermore, MoICL does not require access to the internal parameters of the LLM, making it applicable to black-box LLMs, and it significantly reduces the complexity issues arising from the quadratic time and memory complexity in sequence length of self-attention since it allows the distribution of the training samples among multiple experts. We also show that the method can be made more efficient by sparsifying the mixing weights. We summarise our contributions as follows: • We introduce the Mixture of In-Context Learners (MoICL), which assigns weights to each demonstration subset and learns from them, dynamically identifying the optimal experts and anti-experts via gradient-based optimisation. • We demonstrate that MoICL is competitive with standard ICL while being significantly more data, memory, and computationally efficient. • We show that MoICL is resilient to noisy demonstrations and label imbalance."
https://arxiv.org/html/2411.02795v1,The Evolution of RWKV: Advancements in Efficient Language Modeling,"This paper reviews the development of the Receptance Weighted Key Value (RWKV) architecture, emphasizing its advancements in efficient language modeling. RWKV combines the training efficiency of Transformers (Vaswani et al., 2017) with the inference efficiency of RNNs through a novel linear attention mechanism (Peng et al., 2023). We examine its core innovations, adaptations across various domains (Duan et al., 2024; Gu et al., 2024; Yang et al., 2024; He et al., 2024), and performance advantages over traditional models. The paper also discusses challenges and future directions for RWKV as a versatile architecture in deep learning.","The rapid advancements in natural language processing (NLP) have been largely driven by the development of large language models. Transformer architectures (Vaswani et al., 2017) have set new benchmarks across numerous tasks. However, their quadratic complexity in self-attention poses challenges for long sequences and resource-constrained environments (Choromanski et al., 2020). In response, Peng et al. introduced the Receptance Weighted Key Value (RWKV) architecture (Peng et al., 2023), which merges the parallel training capabilities of Transformers with RNNs’ efficient sequential inference. This paper provides a detailed review of RWKV’s evolution, its application across different domains, and its potential as a scalable, efficient language modeling architecture. Key contributions include: 1. Analysis of RWKV’s linear-complexity attention mechanism (Peng et al., 2023). 2. Examination of adaptations for diverse data types (Duan et al., 2024; Yang et al., 2024; He et al., 2024). 3. Comparison with state-of-the-art models. 4. Discussion on future research directions. This paper is organized as follows: Section 2 provides necessary background on sequence modeling architectures. Section 3 details the core RWKV architecture. Section 4 examines key evolutionary advancements in RWKV research, including RWKV-v5 (Peng et al., 2024), Vision-RWKV (Duan et al., 2024), RWKV-CLIP (Gu et al., 2024), Restore-RWKV (Yang et al., 2024), and PointRWKV (He et al., 2024). Section 5 discusses technical improvements and methodologies. Section 6 analyzes RWKV’s applications and performance across various domains. Section 7 outlines challenges and future directions, and Section 8 concludes the paper."
https://arxiv.org/html/2411.02793v1,Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning,"Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model’s performance. To this end, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. Specifically, we propose a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through crossmodal translation and sentiment semantic reconstruction. Moreover, a hierarchical mutual information maximization mechanism is introduced to incrementally maximize the mutual information between multi-scale representations to align and reconstruct the high-level semantics in the representations. Ultimately, we propose a hierarchical adversarial learning mechanism that further aligns and adapts the latent distribution of sentiment-relevant representations to produce robust joint multimodal representations. Comprehensive experiments on three datasets demonstrate that HRLF significantly improves MSA performance under uncertain modality missing cases.","Multimodal sentiment analysis (MSA) has attracted wide attention in recent years. Unlike unimodal emotion recognition tasks [9, 63, 64, 53, 56], MSA understands and recognizes human emotions through multiple modalities, including language, audio, and visual [31, 58]. Previous studies have shown that combining complementary information among different modalities facilitates valuable semantic generation [41, 40, 61, 55, 62]. MSA has been well studied so far under the assumption that all modalities are available in the training and inference phases [12, 66, 54, 57, 56, 25, 59, 60]. Nevertheless, in real-world applications, modalities may be missing due to security concerns, background noises, sensor limitations and so on. Ultimately, these incomplete multimodal data significantly hinder the performance of MSA. For instance, as shown in Figure 1, the entire visual modality and some frame-level features in the language and audio modalities are missing, leading to an incorrect prediction. In recent years, many studies [8, 28, 26, 49, 37, 50, 76, 74, 68, 27, 23, 22] attempt to address the problem of missing modalities in MSA. For example, SMIL [29] estimates the latent features of the missing modality data via Bayesian Meta-Learning. However, these methods are constrained by the following factors: (i) Implementing complex feature interactions for incomplete modalities leads to a large amount of information redundancy and cumulative errors, resulting in ineffective extraction of sentiment semantics. (ii) Lacking consideration of semantic and distributional alignment of representations, causing imprecise feature reconstruction and nonrobust joint representations. Figure 1: A case of incorrect prediction by the traditional model with missing modalities. The pink and yellow areas indicate intra- and inter-modality missingness, respectively. To address the above issues, we propose a Hierarchical Representation Learning Framework (HRLF) for the MSA task under uncertain missing modalities. HRLF has three core contributions: (i) We present a fine-grained representation factorization module that sufficiently extracts valuable sentiment information by factorizing modality into sentiment-relevant and modality-specific representations through intra- and inter-modality translations and sentiment semantic reconstruction. (ii) Furthermore, a hierarchical mutual information maximization mechanism is introduced to incrementally align the high-level semantics by maximizing the mutual information of the multi-scale representations of both networks in knowledge distillation. (iii) Eventually, we propose a hierarchical adversarial learning mechanism to progressively align the latent distributions of representations leveraging multi-scale adversarial learning. Based on these components, HRLF significantly improves MSA performance under uncertain modality missing cases on three multimodal benchmarks."
https://arxiv.org/html/2411.02791v1,Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models (LLMs) for machine translation (MT). We start with one conjecture: an ideal translation should contain complete and accurate information for a strong enough LLM to recover the original sentence. We generate multiple translation candidates from a source language A𝐴Aitalic_A to a target language B𝐵Bitalic_B, and subsequently translate these candidates back to the original language A𝐴Aitalic_A. By evaluating the cycle consistency between the original and back-translated sentences using metrics such as token-level precision and accuracy, we implicitly estimate the translation quality in language B𝐵Bitalic_B, without knowing its ground-truth. This also helps to evaluate the LLM translation capability, only with monolingual corpora. For each source sentence, we identify the translation candidate with optimal cycle consistency with the original sentence as the final answer. Our experiments demonstrate that larger LLMs, or the same LLM with more forward passes during inference, exhibit increased cycle consistency, aligning with the LLM model size scaling law [Kaplan et al. (2020)] and test-time computation scaling law [Snell et al. (2024)]. This work provide methods for, 1) to implicitly evaluate translation quality of a sentence in the target language, 2), to evaluate capability of LLM for any-to-any-language translation, and 3), how to generate a better translation for a specific LLM.","Machine Translation (MT) has been a cornerstone of natural language processing, facilitating globalization by cross-linguistic communication and democratizing newest information access to all population. In recent years, transformer-based large language models (LLMs) have fundamentally changed the field of natural language processing. Introduced by [Vaswani et al. (2017)], the transformer architecture facilitates parallel processing of input word tokens, significantly improving computational efficiency and successfully scales to unseen model size. Strong language capabilities beyond human imagination emerges from LLM scaling, and create an image of Silicon intelligence for the first time. Transformer-based LLMs can be categorized into several paradigms: encoder-only architectures, like BERT [Devlin (2018)], which focus on meaningful embeddings of input sequences, and don’t fit translation task; the rest two architectures, the encoder-decoder architectures, like T5 [Raffel et al. (2020)], which separately process input and output sequences, were born for translation; and decoder-only architectures, like GPT [Radford et al. (2019)], which generate text in an autoregressive manner. Although GPT is not initially trained specifically for translation tasks like T5, it is exceptionally well-suited for a wide range of natural language processing (NLP) tasks via supervised fine-tuning on downstream tasks, including translation, and provides applications to users via prompting [Brown et al. (2020)]. On a related subject, MT evaluation poses significant challenges as there is no unit test for human languages, like Python or Java. Machine-based evaluation metrics, such as BLEU [Papineni et al. (2002)], METEOR [Banerjee and Lavie (2005)], and TER [Snover et al. (2006)], provide quantitative assessments based on N-gram overlaps and edit distances. While these metrics offer consistency and objectivity, they often fail to fully capture the semantic adequacy and fluency of translations [Liu et al. (2022)], although not being a problem for LLMs as they in most cases generate fluent sentences. Higher-level criteria beyond these metrics, such as overall quality and asceticity, nuanced subtleties, professional terminologies and informal idioms, require evaluation from native speaker of the target language, which is expensive and impractical during online inference. Additionally, evaluation on low-resource languages [Luong et al. (2015)] poses further difficulties due to the data scarcity. The starting point of our work is a simple conjecture: a good translation, and the LLM who translated it, should be able to jointly recover the original sentence completely and accurately. This is natural from information theory view of point. For instance, if an English sentence is translated into French and then back to English, a high degree of similarity between the original and final English sentences indicates a more accurate and reliable translation. To prove this, we propose translation cycle consistency as a meaningful metric to evaluate translation quality without parallel corpora in source language A𝐴Aitalic_A and target language B𝐵Bitalic_B. By translating a sentence from language A𝐴Aitalic_A to language B𝐵Bitalic_B and then back to A𝐴Aitalic_A, we can quantitatively measure the alignment, similarity, or closeness, between the original and back-translated texts. This method not only economically scale MT assessments, it also streamlines the evaluation process, making it widely usable for offline or online evaluation. Cycle consistency brings chance for further improving MT through a self-reflective mechanism: to think ahead a few steps. If the evaluation of translation during inference is accurate enough, we can afford to generate multiple candidates and select the best one. Unlike LLM decoding techniques such as beam search, which select the most probable translation based on a search space over a few tokens, solely based the LLM itself, cycle consistency allows for a complete evaluation of all translated tokens, selecting the most coherent and accurate output, with a math formula-backed metric. This is analogous to AlphaGo, which simulates several future steps to determine the best possible action in the current move [Silver et al. (2016)]. In this paper, we formalize our idea, and empirically investigate the scaling effects, observing that larger models exhibit improved cycle consistency in translations [Chen et al. (2021)], which in turn proves that cycle consistency is a valid and novel metric for evaluation."
https://arxiv.org/html/2411.02722v1,Multimodal Commonsense Knowledge Distillationfor Visual Question Answering,"Existing Multimodal Large Language Models (MLLMs) and Visual Language Pretrained Models (VLPMs) have shown remarkable performances in general Visual Question Answering (VQA). However, these models struggle with VQA questions that require external commonsense knowledge due to the challenges in generating high-quality prompts and the high computational costs of fine-tuning. In this work, we propose a novel graph-based multimodal commonsense knowledge distillation framework that constructs a unified relational graph over commonsense knowledge, visual objects and questions through a Graph Convolutional Network (GCN) following a teacher-student environment. This proposed framework is flexible with any type of teacher and student models without further fine-tuning, and has achieved competitive performances on the ScienceQA dataset.","In recent years, VQA tasks developed to be more challenging by asking questions beyond the image contents and requiring external commonsense knowledge to answer111Existing VLMs’ error cases can be found in Appendix. Existing works on such commonsense VQA tasks tried different methods to integrate visual, question and commonsense knowledge features (Wang, Han, and Poon 2024). For example, Ravi et al. (2023) encodes the contextualized commonsense inferences on the question phrases as additional textual features and integrates with object visual features to fine-tune the Vision-and-Language pretrained model (VLPM). VQA-GNN (Wang et al. 2023) jointly encodes the scene graph of the image and concept graph of the question context as the unified graph for training. T-SciQ (Wang et al. 2024) proposes the new chain-of-thought (CoT) prompting strategy to fine-tune the Multimodal large language model (MLLM). However, these works face problems from two aspects: 1) though incorporating CoT in MLLMs has shown remarkable performances on knowledge-based VQA, generating the high-level reasoning CoT is challenging; 2) directly fine-tuning the large VLMs can be computationally expensive. To address these issues, in this work, we propose a multimodal teacher-student knowledge distillation framework that is computationally efficient to jointly learn the features of multi-modalities (Cabral et al. 2024; Han et al. 2020; Cao et al. 2023). Specifically, in the teacher model, this framework integrates the object entities from image, question and commonsense knowledge graph together in a unified graph and explicitly learns the relationships among them through the Graph Convolutional Neural Network (GCN)(Yao, Mao, and Luo 2019), inspired by Han et al. (2022) and Long et al. (2022). The learned graph features are passed to the student model, which can be any model structure of a smaller size, for the final answer prediction. Notably, instead of fine-tuning based on one vision-and-language model structure, this framework can be flexibly plugged with any pretrained visual and textual encoder for diverse feature extractions in the teacher model. Moreover, this proposed method provides flexibility that can be adapted to environments with different computational efficacies while maintaining competitive performances compared to large VLPMs and MLLMs. We evaluated our proposed framework with the ScienceQA and achieved competitive results."
https://arxiv.org/html/2411.02688v1,On the loss of context-awareness in general instruction fine-tuning,"Pretrained Large Language Models (LLMs) require post-training methods such as supervised fine-tuning (SFT) on instruction-response pairs to enable instruction following. However, this process can potentially harm existing capabilities learned during pretraining. In this paper, we investigate the loss of context awareness after SFT, defined as the capability to extract and understand information from the user-provided context and respond accordingly. We are the first to identify and show that the loss of context-awareness appears on instruction-finetuned LLMs when the chat template is applied to the input prompts. We identify the performance decline is partially caused by the bias embedded into the chat template to focus less on the user-provided context. Based on these observations, we propose two methods to mitigate the loss of context awareness in instruct models: post-hoc attention steering on user prompts and conditional instruction fine-tuning with a context-dependency indicator. Empirical experiments on 4 context-dependent downstream tasks and 3 pretrained LLMs of different sizes show that our methods effectively mitigates the loss of context awareness without compromising the general ability to follow instructions. Our findings also strongly advocate the necessity to carefully benchmark context awareness after instruction fine-tuning.","Large Language Models (LLMs) pretrained on large-scale datasets acquire a diverse set of language modeling capabilities in pretraining. To enhance these models’ ability to follow general instructions, further fine-tuning is typically required such as supervised instruction fine-tuning (SFT) (Wei et al., 2021; Ouyang et al., 2022) and reinforcement learning from human feedback (RLHF) (Christiano et al., 2017) to better understand and respond to human requests. However, the additional fine-tuning can potentially harm existing capabilities learned in pretraining, as pointed out by several existing works (Lin et al., 2024; Bai et al., 2022; Fu et al., 2024). In this paper, we particularly investigate the loss of context-awareness after instruction fine-tuning, which is the capability to understand and retrieve exact information from the user-provided context and respond accordingly. Context awareness is crucial for many real-world use cases, including retrieval augmented generalization (RAG) (Khandelwal et al., 2020; Izacard et al., 2023; Xu et al., 2023b), in-context learning (Agarwal et al., 2024), and contextual question-answering (QA) (Rajpurkar et al., 2016; Choi et al., 2018; Dua et al., 2019). We first illustrate the loss of context awareness in Figure 2 with the Needle-in-a-Haystack test on four popular instruction-tuned models. We demonstrate that the performance degradation is consistent on both long-context and relatively short-context LLMs, which cannot be solely explained by the distribution difference in context lengths between the instruction dataset and the evaluation benchmarks, as suggested by prior works (Dubey et al., 2024). We identify that the bias embedded within the chat template to focus less on the user prompt is a major cause of context-awareness degradation. Normal instruction fine-tuning dataset contains a mixture of both model-dependent and context-dependent queries. The former consists of queries the model can respond to relying mostly on its internal knowledge learned during pretraining. On the other hand, responding to the second type of query requires exact information retrieval and processing from the user-provided context in the input prompt, such as in-context learning, long-form instruction-following, and contextual QA tasks. However, a query accompanied by a context can still be a model-dependent query as the model may have learned the context during pretraining, and is able to respond without relying on the user given context. Therefore, it is challenging for the model to differentiate between these two type of queries from the prompt only, and incorrect identification could lead to hallucination or being over-reliant on user-provided context We validate the bias embedded in chat templates with the Needle-In-a-Haystack test (NIH) (Kamradt, 2023), which requires a model to retrieve a given text “needle” from a long paragraph of irrelevant text. Our experiments show that the needle retrieval performance drops on instruction-finetuned models only when the chat template is applied to the input, which is however crucial for the model to distinguish different roles in conversations. We further show that the performance drop can be attributed to the drop in attention value allocated to the whole user input section. Therefore, the context retrieval capability is not lost in the model, but “inhibited” by the chat-formatted fine-tuning. Based on these observations, a straightforward approach is to directly steer the attention value during inference time to emphasize the user inputs on instruction-tuned models. This can be achieved by manually intervening attention scores of the user context tokens during generation. Experiments show that while performance on simple retrieval tasks can be significantly boosted by attention steering, manipulating the attention value in the inference stage can harm other capabilities of the model, deteriorating performance on more complex tasks. To further improve upon the undesirable trade-off of post-hoc techniques, we are motivated to steer the attention allocation in the fine-tuning stage. To achieve this goal, we identify context-dependent conversations before fine-tuning and add a special token to the prompt as a hint to the model. The special token can then be added at inference time when more attention is demanded to be allocated to the user-provided context. Empirical experiments show the effectiveness of our method on several open-source, pretrained LLMs and instruction fine-tuning datasets. Our contributions are summarized as follows: • We identify that supervised instruction fine-tuning causes pretrained language models to deteriorate in context awareness (even for short context lengths). • We pinpoint the worsened context awareness is associated with attention allocation bias embedded within the chat template. • We propose an inference-time technique to partially recover the context-awareness of general instruction-tuned language models by manually intervening attention scores. • We propose a training-time technique utilizing conditional indicators to mitigate the loss of context awareness of pretrained language models during instruction-tuning."
https://arxiv.org/html/2411.02674v3,Wave Network: An Ultra-Small Language Model,"We propose an innovative token representation and update method in a new ultra-small language model: the Wave network. Specifically, we use a complex vector to represent each token, encoding both global and local semantics of the input text. A complex vector consists of two components: a magnitude vector representing the global semantics of the input text, and a phase vector capturing the relationships between individual tokens and global semantics. Experiments on the AG News text classification task demonstrate that, when generating complex vectors from randomly initialized token embeddings, our single-layer Wave Network achieves 90.91% accuracy with wave interference and 91.66% with wave modulation—outperforming a single Transformer layer using BERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching the accuracy of the pre-trained and fine-tuned BERT base model (94.64%). Additionally, compared to BERT base, the Wave Network reduces video memory usage and training time by 77.34% and 85.62% during wave modulation. In summary, we used a 2.4-million-parameter small language model to achieve accuracy comparable to a 100-million-parameter BERT model in text classification.","Pre-trained token representations are crucial for many language processing models[1], both static token embedding methods, such as Skip-gram and Continuous Bag of Words (CBOW) [2], and context-dependent token embedding methods [3], along with representation updating mechanisms like attention [4], are core techniques in NLP. Despite the remarkable achievements, current NLP models face several inherent challenges. First, existing token embedding methods primarily focus on local semantics, lacking the ability to directly represent global semantics. Second, popular architectures like Transformer [4] measures semantic similarity using the dot product between token vectors, which is computationally expensive. In multi-layer deep learning architectures, the dot product is computed in each attention head and layer, leading to significant resource demand in terms of computing power and time. Consequently, large language models (LLMs) require numerous layers and substantial hardware, data, and time resources to perform optimally on downstream tasks. From a more fundamental perspective, Hockett and Hockett [5] suggests that natural language is a complicated signal system that conveys information with specific meanings through the production of sounds and the reception of hearing. The signal characteristics enable human language to express a wide range of abstract concepts. Based on this signal processing view, we focus on an ultra-small language model can approach or even surpass the performance of large language models on specific tasks."
https://arxiv.org/html/2411.02657v1,Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge,"Rare diseases present unique challenges in healthcare, often suffering from delayed diagnosis and fragmented information landscapes. The scarcity of reliable knowledge in these conditions poses a distinct challenge for Large Language Models (LLMs) in supporting clinical management and delivering precise patient information—underscoring the need for focused training on these ’zebra’ cases. We present Zebra-Llama, a specialized context-aware language model with high precision Retrieval-Augmented Generation (RAG) capability, focusing on Ehlers-Danlos Syndrome (EDS) as our case study. EDS, affecting 1 in 5,000 individuals, exemplifies the complexities of rare diseases with its diverse symptoms, multiple subtypes, and evolving diagnostic criteria. By implementing a novel context-aware fine-tuning methodology trained on questions derived from medical literature, patient experiences, and clinical resources, along with expertly curated responses, Zebra-Llama demonstrates unprecedented capabilities in handling EDS-related queries. On a test set of real-world questions collected from EDS patients and clinicians, medical experts evaluated the responses generated by both models, revealing Zebra-Llama’s substantial improvements over base model (Llama-3.1-8B-Instruct) in thoroughness (77.5% vs. 70.1%), accuracy (83.0% vs. 78.8%), clarity (74.7% vs. 72.0%) and citation reliability (70.6% vs. 52.3%). Released as an open-source resource, Zebra-Llama not only provides more accessible and reliable EDS information but also establishes a framework for developing specialized AI solutions for other rare conditions. This work represents a crucial step towards democratizing expert-level knowledge in rare disease management, potentially transforming how healthcare providers and patients navigate the complex landscape of rare diseases.","The advent of Large Language Models (LLMs) has ushered in a new era of artificial intelligence, with these models demonstrating unprecedented capabilities in natural language understanding and generation [1]. Their potential applications span numerous fields, including healthcare, where they promise to revolutionize tasks ranging from information retrieval to clinical decision support. Recent efforts to adapt LLMs for medical applications broadly fall into three categories: specialized pre-training, domain-specific fine-tuning, and advanced prompting techniques. Notable examples include BioGPT, pre-trained on PubMed abstracts [2], and MedAlpaca, fine-tuned on specialized medical datasets [3]. Advanced prompting strategies, exemplified by MedPaLM [4] and Medprompt [5], have achieved results comparable to human experts on medical licensing exams. While these approaches have yielded promising results in various medical tasks, they often do not fill the gap for highly specialized medical domains, particularly in the context of rare diseases. Rare diseases present unique challenges in the medical field, affecting only a small portion of the population yet often eluding timely diagnosis and effective treatment [6, 7]. The medical adage, ”When you hear hoofbeats, think horses, not zebras,” while pragmatic for common ailments, can inadvertently lead to overlooking rare conditions – the metaphorical ”zebras” of medicine [8]. This oversight can result in significant delays in diagnosis and treatment, profoundly impacting patient outcomes and quality of life. The application of LLMs to rare disease management presents a promising yet underexplored avenue. Unlike common medical conditions with abundant data, rare diseases suffer from a scarcity of information and expertise, making them particularly challenging for traditional LLM approaches. This scarcity underscores the need for specialized AI solutions that can effectively aggregate, contextualize, and disseminate the limited available information on rare diseases from various sources, potentially transforming the landscape of rare disease diagnosis and management. Ehlers-Danlos Syndrome (EDS) epitomizes these challenges, serving as a prime example of the ”zebras” in medicine that often go unrecognized. As a group of inherited connective tissue disorders, EDS manifests through a diverse array of symptoms, including joint hypermobility, skin hyperextensibility, and tissue fragility [9]. With a combined prevalence estimated at 1 in 5,000 individuals worldwide, EDS significantly impacts patient lives while frequently evading timely diagnosis [9]. The syndrome’s heterogeneous nature, comprising at least 13 subtypes with overlapping yet distinct clinical presentations, further complicates the diagnostic process [10]. This complexity, coupled with the syndrome’s relative rarity, often results in a prolonged diagnostic journey, misdiagnosis, and suboptimal management [11]. Moreover, the rapid evolution of EDS research, with emerging genetic markers and updated clinical criteria, creates an ever-expanding knowledge base that is challenging to navigate [12, 13, 14]. EDS thus presents an ideal case study for exploring how specialized AI solutions, particularly advanced LLMs, could revolutionize rare disease management by effectively aggregating, contextualizing, and disseminating the limited yet crucial information available from various sources. To address these challenges in EDS management and demonstrate the potential of specialized AI in rare disease contexts, we present Zebra-Llama, a novel context-aware language model tailored for EDS information management. Drawing from a diverse array of sources including medical literature, patient forums, and clinical resources, our model is designed to address the multifaceted nature of EDS concerns, ranging from clinical presentations to daily living challenges. Our approach significantly enhances the utility of Retrieval-Augmented Generation (RAG) in the EDS domain, as illustrated in Fig 1. While traditional base language models (Fig. 1A) may provide inaccurate or imprecisely cited information, and standard RAG implementations (Fig. 1B) often struggle with contextual relevance, Zebra-Llama (Fig. 1C) demonstrates superior performance in generating precise and accurately cited responses. This is achieved through our specialized context-aware fine-tuning methodology, which trains the model to effectively discern and utilize relevant information from retrieved contexts. Zebra-Llama not only improves the accuracy and relevance of generated answers but also enhances the model’s ability to provide proper citations, a critical feature for trustworthy medical information systems. By focusing on EDS, our model exemplifies how tailored AI solutions can potentially transform rare disease information management, offering a scalable approach to aggregating, contextualizing, and disseminating specialized medical knowledge. Our results demonstrate significant improvements in thoroughness, accuracy, clarity, and citation reliability compared to base models, paving the way for more accessible and reliable rare disease knowledge dissemination. This work represents a crucial step towards leveraging advanced AI to address the ”zebras” of medicine, potentially improving diagnosis, treatment, and overall care for individuals affected by EDS and, by extension, other rare conditions. Figure 1: Fig 1: Comparison of different approaches to EDS-related query handling. (A) Base Llama model generating answers without RAG context, resulting in potentially inaccurate information and hallucinated citations. (B) Base Llama model with RAG implementation, showing imprecise utilization of retrieved context and inclusion of irrelevant information and citations. (C) Zebra-Llama model demonstrating enhanced context-aware RAG capabilities, generating precise responses with accurate citations derived specifically from relevant portions of the retrieved context. The color-coding indicates the relevance of retrieved and generated information (green: relevant, red: non-relevant), highlighting Zebra-Llama’s improved ability to focus on pertinent information while generating responses."
https://arxiv.org/html/2411.02643v1,A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers,"Counterfactual explanations can be used to interpret and debug text classifiers by producing minimally altered text inputs that change a classifier’s output. In this work, we evaluate five methods for generating counterfactual explanations for a BERT text classifier on two datasets using three evaluation metrics. The results of our experiments suggest that established white-box substitution-based methods are effective at generating valid counterfactuals that change the classifier’s output. In contrast, newer methods based on large language models (LLMs) excel at producing natural and linguistically plausible text counterfactuals but often fail to generate valid counterfactuals that alter the classifier’s output. Based on these results, we recommend developing new counterfactual explanation methods that combine the strengths of established gradient-based approaches and newer LLM-based techniques to generate high-quality, valid, and plausible text counterfactual explanations.","The rapid advancement of machine learning (ML) and deep learning (DL) models has led to their widespread application in real-world tasks. Due to algorithmic improvements, increased computational power, and dataset sizes, the performance of models powered by deep neural networks has surpassed traditional ML algorithms such as logistic regression and support vector machines (SVMs) on many tasks such as speech recognition, language translation, summarization, and sentiment classification [1, 2]. Today, many high-performing NLP models such as BERT [3] and GPT-4 [4] are large language models (LLMs) consisting of dozens of transformer layers and millions or billions of parameters. While these models have achieved remarkable performance across a wide range of tasks, their complexity and scale decreases their transparency and explainability. Consequently, many leading NLP models are considered to be ""black boxes"", providing outputs to users without explaining how or why they were produced [5, 6]. Their lack of interpretability and explainability hinders their trustworthiness, fairness and the ability of model developers to identify potential flaws [7, 8]. For example, a black-box model may contain hidden biases or rely on spurious features to make decisions. Furthermore, the lack of transparency of these models may impede their deployment in sensitive domains such as healthcare, finance, and legal applications [6]. Figure 1: An example of a counterfactual explanation. The problems associated with black-box models have motivated the development of the fields of interpretability and explainable AI (XAI) which aim to develop techniques to understand the inner workings of these models and generate explanations for model outputs [9, 10]. Among the various techniques introduced for explaining ML models, research activity on counterfactual explanations has increased significantly since 2019 due to their strong support from psychological research and other benefits [11]. Counterfactual explanations demonstrate how an input can be minimally altered to change a classifier’s output in order to show which parts of the input are important for determining the model’s output [12]. For example, given the sentence ""I liked the movie"" classified as positive sentiment, a counterfactual input ""I hated the movie"" classified as negative sentiment helps users understand the classifier’s behavior by highlighting the influence of the verb on the model’s output. Since their introduction, counterfactual explanations have been widely applied to traditional ML problems involving explicit features and tabular datasets [11]. More recently, new methods have been developed to generate counterfactual explanations for deep learning models trained on image and text classification tasks [13]. Several methods have been developed for generating counterfactual explanations for natural language processing (NLP) models such as text classifiers. However, many of the papers that introduced these methods rely on baselines from the adversarial robustness literature rather than the counterfactual explanations literature due to a lack of other methods to compare against. Furthermore, the lack of standardization of evaluation metrics and baselines in the literature increases the difficulty of evaluating different methods, making it challenging to compare their relative effectiveness or understand the strengths and weaknesses of each method. To address these issues, we conduct a comparative study of five methods for generating counterfactual explanations for a text classifier across two datasets, aiming to identify the most effective method as measured by three standardized evaluation metrics. The text classifier we aim to explain is a BERT-base model [3] fine-tuned on each dataset although any other black-box model could be used since counterfactual explanations are model-agnostic. Due to the effectiveness of large language models (LLMs) at producing plausible text outputs and handling other NLP tasks, many methods for generating counterfactual explanations for text classifiers involve LLMs. Early methods such as CLOSS (2021) [14] use LLMs to generate plausible word substitutes. Other methods such as Polyjuice (2021) [15] use fine-tuned LLM models to generate counterfactuals. More recently, due to advancements in the performance of LLMs, methods such as FIZLE (2024) [16] have been developed which use only general-purpose LLMs and task-specific system prompts to generate counterfactuals without the need for additional fine-tuning or task-specific architectures. These methods leverage the instruction-following and textual understanding capabilities of recent state-of-the-art LLMs such as GPT-4 [4] to generate high-quality counterfactuals. While these recent methods greatly simplify the process of generating text counterfactuals, it remains unclear whether they outperform older, more specialized methods and how their strengths and weaknesses differ. Given these uncertainties, our research aims to address two primary research questions: 1) Which methods for generating counterfactual explanations for text classifiers are most effective? and 2) Do newer, simpler methods that prompt general-purpose LLMs to generate counterfactuals outperform older, more specialized methods? The following list describes the contributions of this paper: 1. A review of the literature describing several past methods for generating counterfactual explanations for text classifiers. 2. A set of three standardized evaluation metrics for measuring the effectiveness of methods for generating text counterfactual explanations. 3. A comparative study of five methods for generating text counterfactual explanations across two datasets using three evaluation metrics to compare the effectiveness of each method. The rest of this paper is organized as follows: Section 2 provides a background on model explanations and counterfactual explanations, Section 3 describes a literature review of past methods for generating counterfactual explanations including detailed explanations of each of the five methods selected for our comparative study, Section 4 outlines the experimental setup for our comparative study, Section 5 presents the results of our experiments, and Section 6 summarizes our conclusions and suggests directions for future research."
https://arxiv.org/html/2411.02631v1,Extracting Unlearned Informationfrom LLMs with Activation Steering,"An unintended consequence of the vast pretraining of Large Language Models (LLMs) is the verbatim memorization of fragments of their training data, which may contain sensitive or copyrighted information. In recent years, unlearning has emerged as a solution to effectively remove sensitive knowledge from models after training. Yet, recent work has shown that supposedly deleted information can still be extracted by malicious actors through various attacks. Still, current attacks retrieve sets of possible candidate generations and are unable to pinpoint the output that contains the actual target information. We propose activation steering as a method for exact information retrieval from unlearned LLMs. We introduce a novel approach to generating steering vectors, named Anonymized Activation Steering. Additionally, we develop a simple word frequency method to pinpoint the correct answer among a set of candidates when retrieving unlearned information. Our evaluation across multiple unlearning techniques and datasets demonstrates that activation steering successfully recovers general knowledge (e.g., widely known fictional characters) while revealing limitations in retrieving specific information (e.g., details about non-public individuals). Overall, our results demonstrate that exact information retrieval from unlearned models is possible, highlighting a severe vulnerability of current unlearning techniques.","Large language Models (LLMs) are trained on vast amounts of text data curated from diverse sources. The extensive training process enables LLMs to generate high-quality responses to a wide range of topics. However, an unintended consequence of this approach is the verbatim memorization of fragments of their training data, which may contain sensitive information. Here, the scale of the training data prevents the reliable identification and removal of all instances of private or copyrighted information, such as personal addresses, passages from copyrighted books, or proprietary code snippets. Regulations, such as the EU’s General Data Protection Regulation (GDPR) (Voigt & Von dem Bussche, 2017), aim to give individuals control over their personal information through measures, such as the “Right to erasure”, requiring companies to delete user data upon request (Zhang et al., 2023a). However, as retraining large models from scratch is impractical to delete user information, unlearning has emerged as an alternative solution to effectively remove knowledge from models while preserving their overall performance (Maini et al., 2024). Evaluating the success of an unlearning method is a difficult task. Even if the model does not directly respond correctly to questions about the unlearned topic, it doesn’t necessarily mean the concept is fully forgotten. Several existing approaches aim to recover unlearned or deleted information (Lynch et al., 2024). Existing works consider an attack successful if the correct answer is present among the candidates but are unable to pinpoint the answer containing the correct information (Patil et al., 2024; Schwinn et al., 2024). In contrast, we argue that for real-world applications, information retrieval systems should incorporate scoring mechanisms that account for the accuracy of selecting the correct answer, which results in a more accurate assessment of leakage risk during deployment. Our contributions in this paper are as follows: • We introduce a novel attack based on activation steering (see §2) against LLM unlearning. To this end, we deploy a novel way of generating pairs of prompts to calculate activation differences – Anonymized Activation Steering. • We evaluate our approach on three unlearning methods and three different datasets. Contrary to existing attacks, we demonstrate that our proposed approach can retrieve unlearned information in an exact manner pinpointing the correct answer among a set of candidates with high accuracy. • We investigate failure scenarios for our approach and find that while exact information retrieval is successful on general knowledge (i.e., models unlearned on Harry Potter) it fails for specific, less known information. • We provide a new dataset based on existing work (Schwinn et al., 2024) for Harry Potter information retrieval, enabling more accurate assessments of unlearning methods. In this work, we demonstrate the power of activation steering as the evaluation tool for targeted unlearning of LLMs. Our results highlight its effectiveness for broad topics, such as removing copyright-related information, while revealing its limitations when applied to more specific knowledge. By exploring these strengths and constraints, we provide deeper insights into how activation steering affects the behavior of unlearned LLMs, contributing to the development of safe LLMs."
https://arxiv.org/html/2411.02617v1,TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context Support for Networks,"The telecommunications industry’s rapid evolution demands intelligent systems capable of managing complex networks and adapting to emerging technologies. While large language models (LLMs) show promise in addressing these challenges, their deployment in telecom environments faces significant constraints due to edge device limitations and inconsistent documentation. To bridge this gap, we present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG) system built on the Phi-2 small language model (SLM). To improve context retrieval, TeleOracle employs a two-stage retriever that incorporates semantic chunking and hybrid keyword and semantic search. Additionally, we expand the context window during inference to enhance the model’s performance on open-ended queries. We also employ low-rank adaption for efficient fine-tuning. A thorough analysis of the model’s performance indicates that our RAG framework is effective in aligning Phi-2 to the telecom domain in a downstream question and answer (QnA) task, achieving a 30% improvement in accuracy over the base Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our model not only performs on par with the much larger LLMs but also achieves a higher faithfulness score, indicating higher adherence to the retrieved context.","The recent interest in large language models, motivated by their unprecedented performance on various downstream tasks, has led to their widespread adoption across many domains. These models gained notable attention with a rising number of applications in text summarization, classification, and generation. In telecom, the potential gain from the incorporation of these models has not gone unnoticed [1, 2, 3, 4, 5, 6]. The integration of LLMs into IoT devices represents a crucial step towards creating autonomous, agentic systems capable of making real-time decisions at the network edge [7]. Foundation LLM models, trained on vast amounts of diverse data, offer versatility and exhibit strong performance across generalized tasks. However, this broad knowledge can result in so-called negative transfer or knowledge interference in specialized domains such as telecommunications, where domain-specific terms and concepts often conflict with their general usage. Indeed, protocols and concepts in telecom sometimes follow distinct logic patterns that differ from broader contexts. This misalignment leads to poor performance when foundation LLM models, such as GPT4, attempt to reconcile their general knowledge with telecom-specific concepts, as the former can actively interfere with the latter. In addition, the strict latency requirements and resource constraints of edge devices, particularly in massive IoT deployments, hinder the deployment of LLMs, necessitating a shift toward smaller models [8]. \Acp SLM, such as Microsoft’s Phi-2 with 2.7B parameters[9] and Gemini Nano 2 with 3.2B parameters [10], present a potential solution by offering competitive performance while maintaining the efficiency crucial for telecom applications. Notably, despite its smaller size, Phi-2 performs on par with state-of-the-art language models 25 times its size on various benchmarks. Telecom is a rapidly evolving domain that is continually adapting to accommodate the fast-paced technology development. As a result, technical papers and documents describing new ideas, standards, and protocols undergo frequent modifications, requiring artificial intelligence (AI) systems developed for the domain to be equally flexible in updating their knowledge. This limits the utility of using fine-tuning to specialize the model to the domain. Unlearning in LLMs is also a significant challenge [11]. Therefore, a more dynamic and adaptable approach, such as RAG is needed for greater flexibility and cost-effectiveness. This approach enables the LMs to seamlessly incorporate new data without the need for extensive re-training. RAG minimizes hallucinations by grounding model responses in factual information through a two-step process: (1) storing domain-specific documents in a database (in semantic space), and (2) supplementing the user’s query with relevant retrieved context at inference time. The retrieved context can also include user-specific information or real-time system updates, allowing the model to leverage the user interaction history to produce tailored responses and remain aligned with current data. For these reasons, RAG is particularly well suited for telecom applications. However, while conventional RAG models provide significant enhancement to language models’ capabilities, telecom applications require special considerations due to the complex and technical nature of the domain. Telecom standardization documents do not follow a unified structure, introducing problems when representing data in the RAG database. This becomes an issue at the retrieval stage as the lack of standardization complicates the identification and extraction of relevant information, often resulting in suboptimal matches and reduced accuracy in responses. Additionally, common terms that recur across multiple areas in telecom documents often have nuanced meanings depending on the context. This contextual complexity necessitates careful filtering of the retrieved documents to provide the generator with a list of highly relevant context chunks. These context chunks supplement the generator with the necessary information in order to allow it to adequately respond to the the given query. Moreover, open-ended or vague user queries require a larger amount of retrieved-context chunks. This demand can exceed the small language model (SLM)’s capacity, limiting its ability to handle the longer prompts required for such questions. To address the aforementioned challenges, this paper makes the following contributions to specialized LMs for telecommunications: • We develop TeleOracle, a specialized RAG framework that effectively addresses the unique challenges of processing telecommunications documentation through strategic integration of multiple techniques; • We implement an optimized document processing pipeline that combines semantic chunking with a two-stage retrieval process, enabling precise and context-aware selection of relevant technical information; • We leverage SelfExtend to extend the generator’s context window at inference time, accommodating a greater volume of retrieved information and enabling more comprehensive responses to complex or open-ended queries; • We conduct a comprehensive analysis of the proposed architecture. Notably, we show that our model not only performs on par with state-of-the-art LLMs models but also is higher on the faithfulness score, indicating higher adherence to retrieved context. The remainder of this paper is organized as follows. Section II presents the relevant works. Section III provides a comprehensive overview of the TeleOracle architecture. Our experiments and their corresponding results are detailed in Section IV. Finally, section V concludes the paper with a summary of findings and potential directions for future research."
https://arxiv.org/html/2411.02610v1,Investigating Idiomaticity in Word Representations,"Idiomatic expressions are an integral part of human languages, often used to express complex ideas in compressed or conventional ways (e.g. eager beaver as a keen and enthusiastic person). However, their interpretations may not be straightforwardly linked to the meanings of their individual components in isolation and this may have an impact for compositional approaches. In this paper, we investigate to what extent word representation models are able to go beyond compositional word combinations and capture multiword expression idiomaticity and some of the expected properties related to idiomatic meanings. We focus on noun compounds of varying levels of idiomaticity in two languages (English and Portuguese), presenting a dataset of minimal pairs containing human idiomaticity judgments for each noun compound at both type and token levels, their paraphrases and their occurrences in naturalistic and sense-neutral contexts, totalling 32,200 sentences. We propose this set of minimal pairs for evaluating how well a model captures idiomatic meanings, and define a set of fine-grained metrics of Affinity and Scaled Similarity, to determine how sensitive the models are to perturbations that may lead to changes in idiomaticity. Affinity is a comparative measure of the similarity between an experimental item, a target and a potential distractor, and Scaled Similarity incorporates a rescaling factor to magnify the meaningful similarities within the spaces defined by each specific model. The results obtained with a variety of representative and widely used models indicate that, despite superficial indications to the contrary in the form of high similarities, idiomaticity is not yet accurately represented in current models. Moreover, the performance of models with different levels of contextualisation suggests that their ability to capture context is not yet able to go beyond more superficial lexical clues provided by the words and to actually incorporate the relevant semantic clues needed for idiomaticity. By proposing model-agnostic measures for assessing the ability of models to capture idiomaticity, this paper contributes to determining limitations in the handling of non-compositional structures, which is one of the directions that needs to be considered for more natural, accurate and robust language understanding. The source code and additional materials related to this paper are available at our GitHub repository111https://github.com/risehnhew/Finding-Idiomaticity-in-Word-Representations.","The evolution of word representation models has resulted in models with seemingly remarkable language abilities. Not surprisingly these models have been found to store a wealth of linguistic information Henderson (2020); Manning et al. (2020); Vulić et al. (2020); Lenci et al. (2022), displaying high levels of performance on various tasks ranging from the abilities of even the static models of detecting semantic similarities between different words Lin (1999); Mikolov et al. (2013); Baroni, Dinu, and Kruszewski (2014) to those of contextualised models of grouping representations in clusters which seem to be related to the various senses of the word Schuster et al. (2019) and can be matched to specific sense definitions Chang and Chen (2019). While substantial evaluation efforts have concentrated on word and subword units and on larger compositional combinations derived from them, there is less understanding about their ability for handling less compositional structures, such as those found on multiword expressions (MWEs), like noun compounds (NCs) Garcia et al. (2021a), verb-noun combinations King and Cook (2018); Hashempour and Villavicencio (2020) and idioms Yu and Ettinger (2020); Dankers, Lucas, and Titov (2022). Indeed, MWEs include a variety of distinct phenomena and have been described as interpretations that cross word boundaries Sag et al. (2002), whose meanings are not always straightforwardly derivable from the meanings of their individual components. Moreover, although they include, on the one hand, more transparent and compositional expressions (like salt and pepper) or expressions with implicit relations (like olive oil as oil made from olives), on the other hand they also include more idiomatic expressions (like eager beaver as a person who is willing to work very hard222Definition from the Cambridge dictionary.), falling into a continuum of idiomaticity333We understand idiomaticity as semantic opacity, and its continuum as different degrees of opacity. Sag et al. (2002); Fazly, Cook, and Stevenson (2009). This leads to potential problems for models if they follow the Principle of Compositionality Frege (1956); Montague (1973), building the meaning of a larger unit (like a sentence or an expression) from a combination of the individual meanings of the words that are contained in it, as this would result in potentially incomplete or incorrect interpretation for more idiomatic cases (e.g. the idiomatic eager beaver interpreted literally as impatient rodent). Although understanding the meaning of an MWE may require knowledge that goes beyond that of the meanings of these individual words in isolation Nunberg, Sag, and Wasow (1994), failure to take idiomaticity into account can affect the quality of downstream tasks Sag et al. (2002); Constant et al. (2017); Cordeiro et al. (2019) such as reasoning and inference Chakrabarty, Choi, and Shwartz (2022); Chakrabarty et al. (2022); Saakyan et al. (2022), information retrieval Acosta, Villavicencio, and Moreira (2011) and machine translation Dankers, Lucas, and Titov (2022). For machine translation, for example, the degree of idiomaticity and ambiguity of MWEs (literal vs. idiomatic usages) were found to have an impact on the quality of the results obtained Dankers, Lucas, and Titov (2022). Due to their non-compositional nature, idiomatic expressions result in lower quality translations than literal expressions, as evidenced by lower BLEU scores for translations that are paraphrased rather than translated word-for-word. In this paper, we investigate to what extent widely used word representation models are able to capture idiomaticity in MWEs. We focus, in particular, on their initial abilities for representing idiomaticity, looking at noun compounds of varying degrees of idiomaticity.444 We use the off-the-shelf publicly available pre-trained versions of widely adopted word representation models, standard operations and common similarity measures. Even in scenarios in which adopting additional optimisations, more complex operations or fine-tuning could lead to improvements in performance, this may depend on the availability of comprehensive training data for the target model, domain and language. Measuring the initial idiomaptic abilities of models can help understand the potential loss of idiomatic meaning that could be propagated to the downstream tasks that use them off-the-shelf. In addition to the complex interactions between MWEs, their component words and their contexts Sag et al. (2002), characteristics of languages and of word representation models may affect how accurately MWEs can be represented and processed, and we investigate the impact of some of these factors for compounds in two different languages (English and Portuguese). One of the challenges is that uncovering how word representation models capture a specific type of knowledge is a non-trivial problem Vulić et al. (2020), and may depend on factors like the particular model and the way it encodes different types of linguistic information Yu and Ettinger (2020). For instance, while in Transformer-based models, the initial layers seem to represent more lexical level knowledge and the final layers seem to capture more semantic and pragmatic information Rogers, Kovaleva, and Rumshisky (2020), determining where phenomena which sit at the interface of various levels are encoded, like multiword expressions Sag et al. (2002), is challenging since they could potentially involve information distributed across different layers. Moreover, the possible findings from an investigation about where in the architecture of a given model idiomaticity is encoded, or about the role of particular components in representing it may not generalise to other models and architectures. In this paper we propose instead a set of model-agnostic idiomatic probes for assessing the representation of idiomaticity. These probes contain NCs of different levels of idiomaticity, ranging from idiomatic to compositional cases, which form the basis for minimal pairs. In these pairs one of them contains an NC and the other contains a semantically related item (such as a synonym ) or a distractor. The hypothesis is that if a model is able to accurately represent an NC, higher similarities will be observed for minimal pairs involving NCs and their synonyms (e.g. for the idiomatic eager beaver and hardworking person). Conversely, for minimal pairs with variants that may incorporate changes in meaning, such as those containing NCs and synonyms of their individual component words (e.g. the idiomatic eager beaver and impatient rodent) or other distractors, lower similarities should be observed. As word representation models may form spaces that are anisotropic Ethayarajh and Jurafsky (2021) with representations concentrating on parts of the space, or may have rogue dimensions that dominate similarity measures Timkey and van Schijndel (2021), these could lead to high similarities overall Liu et al. (2020), affecting the ability to distinguishing meaningful similarities from spurious ones arising from specific characteristics of a given space. In this paper, we propose two new measures to assess idiomaticity within a model while taking into account its potential for high similarities. The first, Assessment of Feature Familiarity and Idiomatic Nuance by Interpreting Target Yielding (Affinity), takes two representations of different levels of relatedness to a given target, and can be used to determine if a model accurately reflects their degree of similarity to the target. Focusing on idiomaticity, we use Affinity to assess if greater similarities are observed for NCs and related words (in this case their synonyms), than for NCs and other potentially less related alternatives including distractors. The second measure, Scaled Similarity, determines a new lowerbound for a given space in terms of similarities for unrelated representations, rescaling the space to help distinguish them from the meaningful similarities for related representations. For idiomaticity, we analyse the similarities between the NCs and their synonyms adopting the similarities between the NCs and random items as a new lowerbound. These measures of Affinity and Scaled Similarity do not directly address the problem of rogue dimensions, and we discuss this further in the Conclusions section. Using these metrics and minimal pairs for evaluation, this paper presents a fine-grained analysis of the ability of a model to capture idiomaticity, looking at the following questions: Q1 To what extent is idiomaticity captured by word representation models? We assess this by comparing the predictions of models for NCs and their synonyms against human judgements about idiomaticity in the same sentences, analysing how sensitive these models are to potential changes in meaning resulting from the lexical variations in the minimal pairs. Q2 Is this ability affected by the degree of idiomaticity of the NCs, the informativeness of the contexts, or the languages involved? To determine if more idiomatic expressions are more challenging for models, we present an analysis of the impact of the level of idiomaticity of the NCs. We also analyse more informative contexts provided by naturalistic sentences against uninformative neutral contexts to determine their impact on idiomaticity representation. These evaluations include two languages, to measure the potential language dependence of these results. Q3 Do contextualised models (from transformer-based models) perform better compared to static models in idiomaticity representation? In addressing this question, we conduct a comparative analysis across different static and contextualised models, focusing on their ability to capture idiomatic expressions. This involves examining how each model represents idiomatic NCs of varying levels of idiomaticity, in sentences that contain more (or less) informative contexts, and the accuracy with which they reflect the nuanced meanings that idiomaticity often entails. The analyses consider various linguistic scenarios that can change the idiomatic meaning to comprehensively assess the accuracy of contextualised models over their static counterparts. The main contributions of this work include: • The Noun Compound Idiomaticity Minimal Pairs (NCIMP) Dataset, a dataset of minimal pair sentences containing NCs of varying levels of idiomaticity, along with human judgments about the degree of NC idiomaticity and gold standard paraphrases, at both type and token level. In total, the dataset contains 32,200 sentences for two languages (19,600 in English and 12,600 in Portuguese).555This work extends the idiomatic probes proposed by Garcia et al. (2021b) and the type and token annotations by Garcia et al. (2021a), also introducing new measures, additional tests and substantially expanding the analyses with new baselines and results from a larger set of models. • A comparative measure of Affinity to help determine how accurately idiomaticity is incorporated in these representations contrasting similarities for semantically related and unrelated representations. • A novel model-agnostic measure of Scaled Similarity, which rescales a space in relation to a new lowerbound taking into account expected similarities among random items to magnify meaningful similarities among semantically related representations. • In-depth analyses of the representation of idiomaticity in widely used word representation models, examining their ability to display sensitivity to changes in idiomaticity. The remainder of this paper is organised as follows: Section 2 presents related work, and Section 3 the NCIMP dataset (Section 3.1), the models (Section 3.2) and the proposed idiomatic probes and measures (Section 3.3). Finally, in Section 4 we discuss the results of our experiments and draw conclusions in Section 5."
https://arxiv.org/html/2411.02603v3,FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees,"The propensity of Large Language Models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly classifying hallucinations as truthful content) is essential. Despite its importance, formal verification of LLM factuality with such guarantees remains largely unexplored. In this paper, we introduce FactTest, a novel framework that statistically assesses whether a LLM can confidently provide correct answers to given questions with finite-sample and distribution-free correctness guarantees. We formulate factuality testing as hypothesis testing problem to enforce an upper bound of Type I errors at user-specified significance levels. Notably, we prove that our framework also ensures strong Type II error control under mild conditions and can be extended to maintain its effectiveness when covariate shifts exist. Our approach is distribution-free and works for any number of human-annotated samples. It is model-agnostic and applies to any black-box or white-box LM. Extensive experiments on question-answering (QA) and multiple-choice benchmarks demonstrate that FactTest effectively detects hallucinations and improves the model’s ability to abstain from answering unknown questions, leading to an over 40% accuracy improvement.","Large Language Models (LLMs) like ChatGPT and GPT-4 (Ouyang et al., 2022; OpenAI et al., 2024) have demonstrated substantial advancements in various domains such as summarization systems, search engines and virtual assistants. However, their outputs cannot be fully trusted due to their propensity to generate nonfactual and incorrect information with seemingly high confidence, a challenge often referred to as hallucination (Maynez et al., 2020b; Huang et al., 2023; Ji et al., 2023). This tendency undermines the reliability and trustworthiness of the generated content, highlighting a critical need for robust mechanisms to verify the factuality and correctness of LLM outputs. Existing approaches to hallucination detection like retrieval-based methods (Thorne et al., 2018b; Gou et al., 2024; Chen et al., 2024) and training-based approaches Zhang et al. (2023) either rely on external databases or resource-intensive fine-tuning process, which are often impractical or costly. Therefore, there has been growing interest in uncertainty estimation as a zero-resource alternative for hallucination detection (Varshney et al., 2023; Xiong et al., 2024), operating under the premise that hallucinations are intrinsically tied to the model’s uncertainty Huang et al. (2023). However, none of these methods can provide theoretical guarantees for the detection or testing results, which are essential for deploying LLMs in high-stakes domains Kumar et al. (2023) where precise control of Type I errors (incorrectly flagging a hallucination as truthful content) is needed for decision-making. For instance, incorrect medical diagnoses in healthcare or the provision of uncertain legal advice in the legal field could result in detrimental consequences. To address these limitations, we introduce FactTest, a framework that statistically evaluates whether an LLM can reliably generate correct answers to given questions with provable correctness guarantees and teach LLMs to abstain from answering uncertain questions. We formulate the factuality testing within a hypothesis testing framework to theoretically control the Type I error while minimizing the Type II error. Leveraging the fundamental connection between Neyman-Pearson (NP) classification and statistical testing (Tong et al., 2018; Tong, 2013; Scott & Nowak, 2005), we define a score function to quantify model certainty and select an appropriate threshold based on a constructed calibration dataset to instruct LLMs to refuse uncertain questions and control the false positive rate. Furthermore, we prove that, under mild conditions, FactTest achieves strong power control. This ensures that our method not only controls Type I error but also maintains a low Type II error, thereby providing reliable factuality assessments. On the other hand, since statistical tests often rely on the i.i.d. assumption, which may not hold in reality, we enhance the robustness of this framework by adding an extension to accommodate covariate shifts through the estimation of density ratios and the use of rejection sampling. Our approach is model-agnostic and does not rely on specific data distribution assumptions, making it broadly applicable to any language model. Importantly, it works for any finite number of human-annotated samples, ensuring practicality and ease of implementation. To the best of our knowledge, this study is the first to introduce statistical factuality testing in large language models, thereby facilitating safer and more reliable deployment in high-stakes applications. We evaluate the effectiveness of our proposed framework on question-answering (QA) and multiple-choice benchmarks. The results indicate that our approach offers several significant advantages: (1) it consistently outperforms base models by a substantial margin without requiring additional training or external data sources; (2) it surpasses fine-tuned baselines by a large margin while utilizing only half of the training data; and (3) it maintains superior performance on out-of-distribution testing data. Notably, the theoretical guarantees of our method remain valid even when the i.i.d. assumption is violated. We summarize the main contributions below. • We propose FactTest, a novel statistical testing framework that evaluates the factuality of LLMs while teaching them to decline uncertain questions with user-specified Type I error guarantees. • We prove that our statistical framework achieves strong power control under mild conditions, ensuring that the predictor can also maintain a low Type II error. This power analysis is directly applicable to the standard NP classification problems, not limited to this setting. • We extend our framework to accommodate covariate shifts by approximating density ratios and applying rejection sampling, thereby enhancing its robustness in real-world applications. • We demonstrate that FactTest effectively detects hallucinations while maintaining Type I error below user-specified significance levels, achieving an over 40% improvement in accuracy compared to pretrained models without any fine-tuning. Additionally, it surpasses training-based baselines by 30% using only half of the fine-tuning data."
https://arxiv.org/html/2411.02591v1,Geometry of orofacial neuromuscular signals: speech articulation decoding using surface electromyography,"Each year, millions of individuals lose the ability to speak intelligibly due to causes such as neuromuscular disease, stroke, trauma, and head/neck cancer surgery (e.g. laryngectomy) or treatment (e.g. radiotherapy toxicity to the speech articulators). Effective communication is crucial for daily activities, and losing the ability to speak leads to isolation, depression, anxiety, and a host of detrimental sequelae. Noninvasive surface electromyography (sEMG) has shown promise to restore speech output in these individuals. The goal is to collect sEMG signals from multiple articulatory sites as people silently produce speech and then decode the signals to enable fluent and natural communication. Currently, many fundamental properties of orofacial neuromuscular signals relating to speech articulation remain unanswered. They include questions relating to ① the data structure of the orofacial sEMG signals, ② the signal distribution shift of sEMG across individuals, ③ ability of sEMG signals to span the entire English language phonetic space during silent speech articulations, and ④ the generalization capability of non-invasive sEMG based silent speech interfaces. We address these questions through a series of experiments involving healthy human subjects. We show that sEMG signals evince graph data structure and that the signal distribution shift is given by a change of basis. Furthermore, we show that silently voiced articulations spanning the entire English language phonetic space can be decoded using small neural networks which can be trained with little data and that such architectures work well across individuals. To ensure transparency and reproducibility, we open-source all the data and codes used in this study.","Individuals who lose intelligible speech output due to disease or damage of the articulators must learn new ways to communicate. Noninvasive brain-body-computer interfaces might provide the means for people with dysarthria to communicate fluently and with a naturalistic voice as demonstrated by Gaddy and Klein (2021). Among the modalities to capture “silent” speech information, neuromotor interfaces such as sEMG show great promise as such systems can work in many realistic environments, e.g. in noisy backgrounds or with visual occlusion, where traditional methods based on video (such as generating audio from lip movements) may fail. In addition, neural interfaces encode rich information in multiple sensor nodes at different spatial locations and can detect subtle movements and gestures which may not be discernible with video or residual audio signals. In this article, we record sEMG signals at multiple muscle locations on the neck, jaw, chin, and cheek for speech articulation decoding. Numerous muscles work in concert to produce a given articulation and these synergistic activation patterns are encoded across the spatially distributed sensor electrodes. We show that this rich spatial information embedded in multiple sensor nodes manifests non-Euclidean data structure that is amenable for analysis on a differentiable manifold of symmetric positive definite matrices equipped with a Riemannian metric. sEMG signals, due to their multivariate nature, display graph data structure that is defined by a set of orthogonal axes rather than functions sampled on 1-dimensional or 2-dimensional Euclidean grids, as is the case for audio signals and images, and the domain shift in sEMG across individuals is characterized by a change of basis."
https://arxiv.org/html/2411.02589v1,Context-Informed Machine Translation of Mangausing Multimodal Large Language Models,"Due to the significant time and effort required for handcrafting translations, most manga never leave the domestic Japanese market. Automatic manga translation is a promising potential solution. However, it is a budding and underdeveloped field and presents complexities even greater than those found in standard translation due to the need to effectively incorporate visual elements into the translation process to resolve ambiguities. In this work, we investigate to what extent multimodal large language models (LLMs) can provide effective manga translation, thereby assisting manga authors and publishers in reaching wider audiences. Specifically, we propose a methodology that leverages the vision component of multimodal LLMs to improve translation quality and evaluate the impact of translation unit size, context length, and propose a token efficient approach for manga translation. Moreover, we introduce a new evaluation dataset – the first parallel Japanese-Polish manga translation dataset – as part of a benchmark to be used in future research. Finally, we contribute an open-source software suite, enabling others to benchmark LLMs for manga translation. Our findings demonstrate that our proposed methods achieve state-of-the-art results for Japanese-English translation and set a new standard for Japanese-Polish.111Data and code available at: https://github.com/plippmann/multimodal-manga-translation. *All correspondence: p.lippmann@tudelft.nl.","A Japanese style of comics – referred to as manga – has been popular with audiences outside of Japan for decades. Handcrafting high quality translations, key to distributing manga world wide, is a difficult undertaking that takes significant time and effort. As such, most manga never leave the domestic Japanese market. Additionally, readers who do not speak a language into which manga is typically translated have limited or no access at all due to the high initial costs of translations. Figure 1: Comparison of translation outputs for methods with different context types. The preceding scene visually shows a TV, giving context to the complaints in purple. The previous and current scenes are set in a restaurant, making it improbable that “Suzume” refers to a sparrow rather than being a name. ©Kira Ito The use of Neural Machine Translation (NMT) promises seamless translations from one language to another without involving a human translator (Sutskever et al., 2014; Vaswani et al., 2017). Still, successful applications of NMT to manga – or comics in general – remain limited, and automatic methods remain far from being able to reliably translate manga at a level comparable to humans (Hinami et al., 2021). This is in part due to the unique requirements of manga as a translation problem, which involves literary translation, handling split sentences across multiple speech bubbles, and especially resolving ambiguities using visual information. For example, in figure 1, achieving an accurate translation requires integrating both textual and visual context from the current and preceding scenes. Research into manga-specific NMT methods is limited, focusing mainly on Japanese-English translation due to a lack of parallel corpora for other language pairs Hinami et al. (2021); Kaino et al. (2024). Of these, only one method has attempted incorporating visual context into a model via a limited number of descriptive tags, yielding inconclusive results Hinami et al. (2021). Previously proposed models were trained on a private JA-EN data set, which is not shareable due to copyright Hinami et al. (2021); Kaino et al. (2024). Although there exist several general purpose manga data sets, such as Manga109 Fujimoto et al. (2016); Matsui et al. (2017), so far only one manga translation data set has been published for research purposes: OpenMantra Hinami et al. (2021). However, its limited size makes it effectively an evaluation data set only, making it challenging to train models. Large language models (LLMs) have shown to be capable translators across languages Lyu et al. (2023); Hendy et al. (2023). The release of multimodal LLMs – those that make use of visual information in addition to text – makes translation of media with visual nuance a possibility Lyu et al. (2024). This potentially bypasses the need for large parallel manga corpora for each language pair as LLMs do not need to be finetuned by the user. Still, it is not clear how to best use these as effective manga translators. In this paper, we present a translation methodology using a multimodal multilingual LLM. We evaluate a range of LLM-based translation approaches to empirically assess the impact of visual context, translation unit size, and context length. We do this using an existing JA-EN manga data set and a new Japanese-Polish data set created for this purpose. The JA-PL translation direction is chosen for its unique challenges, particularly due to the significant differences in syntactic structures and semantic nuances between Japanese, English, and Polish, and to address a low-resource language that nonetheless has a market for manga Świeczkowska (2017). Finally, we contribute an open-source manga translation evaluation suite that allows users to choose the granularity of available context, provides automatic evaluation metrics, and enables testing of different LLMs. In summary, our contributions are as follows: • An LLM-based multimodal manga translation methodology that achieves state of the art results on JA-EN translations and can serve as a baseline for low-resource languages. • An annotated set of 400 professionally translated manga pages (3705 sentences) that make up the first ever parallel JA-PL manga translation benchmark data set, as well as the largest manga translation data set to date. • The first publicly available automatic manga translation evaluation software suite."
https://arxiv.org/html/2411.02587v1,A Big Data-empowered System for Real-time Detection of Regional Discriminatory Comments on Vietnamese Social Media,"Regional discrimination is a persistent social issue in Vietnam. While existing research has explored hate speech in the Vietnamese language, the specific issue of regional discrimination remains under-addressed. Previous studies primarily focused on model development without considering practical system implementation. In this work, we propose a task called Detection of Regional Discriminatory Comments on Vietnamese Social Media, leveraging the power of machine learning and transfer learning models. We have built the ViRDC (Vietnamese Regional Discrimination Comments) dataset, which contains comments from social media platforms, providing a valuable resource for further research and development. Our approach integrates streaming capabilities to process real-time data from social media networks, ensuring the system’s scalability and responsiveness. We developed the system on the Apache Spark framework to efficiently handle increasing data inputs during streaming. Our system offers a comprehensive solution for the real-time detection of regional discrimination in Vietnam.","I-A Background The state of discrimination and prejudice in society often relates to specific characteristics such as race, gender, health status, occupation, education, and living standards. Regional discrimination is defined as the prejudice or bias of one social group against another based on their place of origin or hometown. The issue of regional discrimination exists to varying degrees in most countries around the world. After 1945, the problem of discrimination, prejudice, and bias became evident in countries such as the United States, Canada, some European countries, and Australia [1]. In Germany, discrimination between citizens of the East and West persists more than 25 years after reunification. In Vietnam, after several prolonged wars that divided the country over the past centuries [2], regional discrimination has increased. This threatens national unity and development, causes psychological harm to victims, and provides opportunities for hostile forces to undermine the current political system of the country. Social media platforms, while allowing people to connect and communicate, have also become places where discriminatory behavior spreads easily.[3] On December 8, 2023, VTV24, a news program produced by the Center for Digital Content Production and Development, Vietnam Television, aired a special episode addressing the issue of regional discrimination on social media platforms in Vietnam. Emphasizing its impact on national unity. There are several state-of-the-art approaches for text classification in the Vietnamese language, including machine learning, and deep learning. However, existing research mainly focuses on developing models without implementing practical systems, which are crucial for use in social networks. To address this gap, we propose a system that leverages big data technology to continuously collect and process data. Our models are deployed on the Apache Spark framework to ensure the system can handle the increasing volume of online data. Additionally, we introduce ViRDC, a dataset specifically created for the automatic detection of regional discrimination in Vietnamese social media comments, serving as a crucial asset for future research and development. The paper’s structure is as follows: We shall discuss pertinent research on the issue in Section 2. In the following section, we will give a thorough overview of ViRDC, covering the methods for collecting data, categorizing them, and organizing the dataset. The model used in the study will be explained in Section 4. We will go into detail on the model’s architecture, training algorithms, and selective methods. In Section 5, the experiments carried out on the dataset will be covered in detail, along with the results and analytical assessments that follow. After that, we will explain the real-time prediction features and the streaming data procedure in Section 6. Section 7 will conclude the study by providing a summary of the results and a proposal for future research. I-B Motivation Research on detecting regional discriminatory comments on social media in Vietnam, using Spark and Kafka, is motivated by critical goals. The importance of detecting regional discriminatory comments in Vietnam lies not only in the social aspect but also has profound scientific and practical significance. In the context of the growing development of social networks, regional discriminatory comments not only cause harm to individuals but also increase division within the community. Research and detection of these comments help raise awareness of the issue, while also providing crucial data to develop automated tools to identify and prevent negative content. Furthermore, this research contributes to building a healthy online environment, promoting solidarity and mutual understanding among regions in Vietnam."
https://arxiv.org/html/2411.02571v1,MM-Embed: Universal MultimodalRetrieval with Multimodal LLMs,"State-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future. We release the model weights at: https://huggingface.co/nvidia/MM-Embed.","Information retrieval is crucial for a variety of downstream tasks, such as question answering (Kwiatkowski et al., 2019), fact-checking (Thorne et al., 2018), and retrieval-augmented generation (Lewis et al., 2020). Existing state-of-the-art retrievers often focus on narrow scenarios. For example, LLM-based retrievers (Wang et al., 2023; Lee et al., 2024; Meng et al., 2024; Moreira et al., 2024) are limited to text-to-text retrieval tasks, where both the query and the retrieved results are text-only. Recent work on multimodal retrieval (Zhang et al., 2024; Jiang et al., 2024) focuses on specific tasks and assumes a homogeneous document format. However, in real-world applications, documents and queries often consist of diverse formats or modalities, such as text, images, and interleaved text and images. To advance information retrieval and support broader search scenarios, this work explores the use of multimodal LLMs (MLLMs; Dai et al., 2024; Liu et al., 2023a; 2024) for universal multimodal retrieval, accommodating diverse user-instructed tasks with multimodal queries and documents, as illustrated in Figure 1. We first explore to fine-tune MLLM-based bi-encoder retrievers with instructions as a guide (Asai et al., 2023) on 16 multimodal retrieval tasks from M-BIER (Wei et al., 2023). We find that MLLM-based retrievers significantly outperform CLIP-based retrievers in the challenging tasks, where interleaved text–image queries are given, such as visual question answering and composed image retrieval (tasks 3 and 7 in Figure 1). However, MLLM-based retrievers underperform in cross-modal retrieval tasks due to the modality bias from MLLMs. That is, given a text-based query with the instruction to retrieve an image (e.g., task 9 in Figure 1), an MLLM-based retriever tends to retrieve a relevant text-only rather than documents with images, especially when we improve the MLLM-based retriever’s text retrieval capability. To address the issue, we propose modality-aware hard negative mining in Section 4.1.1 and continual text-to-text retrieval fine-tuning in Section 4.1.2. Our final retriever, coined MM-Embed, is the first state-of-the-art universal multimodal retriever while maintaining competitive text-to-text retrieval performance across diverse tasks. Finally, we explore to prompt MLLMs as zero-shot rerankers. Surprisingly, we find that the zero-shot MLLM-based rerankers can further boost retrieval accuracy in the tasks, where user queries are interleaved text–image and more challenging to understand. For example, in the composed image retrieval dataset, CIRCO (Baldrati et al., 2023), the zero-shot rerankers are able to refine the ranked lists and significantly boosts the accuracy (mAP@5) over 7 points from the existing state-of-the-art composed-image retriever (Zhang et al., 2024) and our universal multimodal retrievers. This finding indicates that there is still room for improvement in such challenging tasks in order to tackle universal multimodal retrieval. Also, knowledge distillation from zero-shot or few-shot MLLM-based rerankers to retrievers is a promising direction. We summarize our contributions as follows: i) We present a study on applying MLLMs to universal multimodal retrieval. ii) We are the first to build MLLM-based universal multimodal retrievers. Notably, our MM-Embed, initialized from the existing best-performing text retriever (NV-Embed-v1; Lee et al., 2024), not only achieves state-of-the-art results in universal multimodal retrieval benchmark, M-BEIR (Wei et al., 2023), but also surpasses NV-Embed-v1 in text-to-text retrieval tasks on MTEB. iii) We are the first work to explore prompting MLLMs for zero-shot reranking. With a zero-shot MLLM-based reranker, we are able to boost the ranking accuracy over 7 points upon state-of-the-art retrievers in the composed image retrieval task, CIRCO (Baldrati et al., 2023). We organize the rest of the paper as follows. We discuss related work in § 2. We introduce the definition of universal multimodal retrieval in § 3 and present the proposed method in § 4. We report experiment results in § 5 and conclude the paper in § 6. Figure 1: Illustration of universal multimodal retrieval, where diverse tasks with instructions, queries and documents with multimodal formats are supported. In this work, we explore to fine-tune MLLM-based universal multimodal retriever, MM-Embed, and prompt an MLLM for reranking."
https://arxiv.org/html/2411.02556v1,Leveraging Transformer-Based Models for Predicting Inflection Classes of Words in an Endangered Sami Language,"This paper presents a methodology for training a transformer-based model to classify lexical and morphosyntactic features of Skolt Sami, an endangered Uralic language characterized by complex morphology. The goal of our approach is to create an effective system for understanding and analyzing Skolt Sami, given the limited data availability and linguistic intricacies inherent to the language. Our end-to-end pipeline includes data extraction, augmentation, and training a transformer-based model capable of predicting inflection classes. The motivation behind this work is to support language preservation and revitalization efforts for minority languages like Skolt Sami. Accurate classification not only helps improve the state of Finite-State Transducers (FSTs) by providing greater lexical coverage but also contributes to systematic linguistic documentation for researchers working with newly discovered words from literature and native speakers. Our model achieves an average weighted F1 score of 1.00 for POS classification and 0.81 for inflection class classification. The trained model and code will be released publicly to facilitate future research in endangered NLP.","Skolt Sami is a minority language in the Uralic family, spoken primarily in Finland, and is characterized by complex morphosyntactic properties and rich morphological forms (see Koponen and Rueter 2016). Minority languages like Skolt Sami face significant challenges in the field of natural language processing (NLP) due to their endangered nature, including a lack of extensive annotated datasets and linguistic resources. This scarcity complicates the development of computational models capable of effectively understanding and analyzing the language. Moreover, the morphology of Skolt Sami is highly intricate, with numerous inflections and derivations that present additional challenges for automated processing Rueter and Hämäläinen (2020). Despite these challenges, developing NLP models for minority languages is essential to preserve linguistic diversity and support language revitalization. Accurate part-of-speech (POS) and inflection class classification are fundamental steps in understanding the grammatical and semantic structure of a language. Such classifications enable downstream NLP applications like machine translation, morphological analysis, and syntactic parsing, which are particularly important for languages with rich morphology. Additionally, effective classifiers can assist in improving the current state of FSTs by providing greater lexical coverage, ultimately enhancing their ability to handle the full range of morphological variations found in Skolt Sami. Classifiers can also aid researchers in systematically documenting new words collected from literature and native speakers, which is crucial for tracking linguistic evolution in endangered contexts. For Skolt Sami, POS and inflection class classification can contribute to building digital resources and educational tools, making the language more accessible to both linguists and speakers. To address these challenges, we propose a transformer-based model designed to automate the analysis of Skolt Sami, specifically for POS and inflection class classification. Our approach includes data extraction, preprocessing, augmentation, model training and evaluation. We employed advanced transformer architectures to learn the linguistic features of Skolt Sami effectively. Additionally, we provide both the trained model and the accompanying code publicly to support future research efforts on endangered languages111https://github.com/mokha/predict-inflection-class. The contributions of this work are as follows: 1. Data Augmentation Using Miniparadigms: We employed data augmentation techniques, including the generation of morphological forms, to mitigate data scarcity and improve model robustness. 2. Transformer-Based Model: We designed a transformer-based model for POS and inflection class classification in Skolt Sami, utilizing shared embedding layers and task-specific output heads."
https://arxiv.org/html/2411.02538v1,MILU: A Multi-task Indic Language Understanding Benchmark,"Evaluating Large Language Models (LLMs) in low-resource and linguistically diverse languages remains a significant challenge in NLP, particularly for languages using non-Latin scripts like those spoken in India. Existing benchmarks predominantly focus on English, leaving substantial gaps in assessing LLM capabilities in these languages. We introduce MILU—Multi-task Indic Language Understanding Benchmark—a comprehensive evaluation benchmark designed to address this gap. MILU spans 8 domains and 42 subjects across 11 Indic languages, reflecting both general and culturally specific knowledge. With an India-centric design, MILU incorporates material from regional and state-level examinations, covering topics such as local history, arts, festivals, and laws, alongside standard subjects like science and mathematics. We evaluate over 45 LLMs, and find that current LLMs struggle with MILU, with GPT-4o achieving the highest average accuracy at 72%. Open multilingual models outperform language-specific fine-tuned models, which perform only slightly better than random baselines. Models also perform better in high-resource languages as compared to low-resource ones. Domain-wise analysis indicates that models perform poorly in culturally relevant areas like Arts & Humanities and Law & Governance compared to general fields like STEM. To the best of our knowledge, MILU is the first of its kind benchmark focused on Indic languages, serving as a crucial step towards comprehensive cultural evaluation. All code, benchmarks, and artifacts will be made publicly available to foster open research.","Recent advancements in Large Language Models (LLMs) have reshaped the field of NLP, by enabling these models to perform a variety of tasks across diverse domains Doddapaneni et al. (2023); OpenAI et al. (2023); Team et al. (2024a); Anthropic . While many LLMs now claim to support multiple languages, there is still a huge discrepancy in their performance in English and other languages Liu et al. (2024). Particularly languages using non-Latin scripts, such as those in India, are affected the most by this discrepancy Ahuja et al. (2023). One key reason for this is the absence of high-quality benchmarks for these languages. Well-designed benchmarks are crucial in driving model development by revealing limitations and guiding improvements McIntosh et al. (2024). However, most existing benchmarks focus primarily on English, leaving significant gaps in evaluating LLMs capability in low-resource and linguistically diverse languages. Figure 1: Average performance of all the evaluated models on MILU. The closed models are shown in Orange, the open models are shown in Green, and the language-specific models are shown in Blue. India’s diversity presents a unique challenge for these models. With over 1.4 billion people speaking more than 120 languages and around 19,500 dialects across 28 states Javed et al. (2024), many of which are underrepresented in NLP research, the need for not just linguistic but culturally appropriate benchmarks becomes urgent Doddapaneni et al. (2023); Singh et al. (2024b). Standard benchmarks like MMLU Hendrycks et al. (2021) and AGIEval Zhong et al. (2023), while useful for evaluating general world knowledge, falls short in capturing these intricacies of India. Each state in India has its own history, traditions, festivals, and art forms, forming a rich cultural mosaic. Translating existing English benchmarks into Indian languages fails to capture this knowledge, which is required for real-world applications. Given the increasing deployment of LLMs in tasks that directly impact local populations, the need for a benchmark that evaluates both linguistic competence and cultural understanding has become more pressing than ever. In this work, we introduce MILU-Multi-task Indic Language Understanding Benchmark, a comprehensive evaluation dataset designed to address these gaps. MILU spans 8 domains and 42 subjects across 11 Indic languages, reflecting both general and culturally specific knowledge. We designed MILU with an India-first perspective by collecting questions from various national, state, and regional exams. These questions include culturally relevant subjects such as local history, arts, festivals, and laws, alongside traditional academic subjects like science and mathematics. Following previous efforts Hendrycks et al. (2021); Zhong et al. (2023), we create this benchmark by collecting questions from over 1500 competitive exams from India. We focus on region-specific exams to authentically capture local knowledge in the respective language. We evaluate 45 different LLMs - a mix of closed proprietary, open-source, and language-specific models- on MILU. Our findings suggest that models struggle with MILU, with GPT-4o achieving the highest average accuracy at 72%. Interestingly, open multilingual models outperform language-specific models, which only achieve slightly better than random scores. Our analysis of in-context learning reveals that adding more examples improves performance in base models, but the effect on instruct models remains inconclusive. We also explore how performance scales with the number of parameters, finding significant improvements as model size increases. Our domain-wise analysis reveals that models perform poorly in culturally relevant areas, such as Arts & Humanities and Social Sciences, compared to more general fields like STEM. All the artifacts will be released publicly."
https://arxiv.org/html/2411.02536v1,Towards Leveraging News Media to Support Impact Assessment of AI Technologies,"Expert-driven frameworks for impact assessments (IAs) may inadvertently overlook the effects of AI technologies on the public’s social behavior, policy, and the cultural and geographical contexts shaping the perception of AI and the impacts around its use. This research explores the potentials of fine-tuning LLMs on negative impacts of AI reported in a diverse sample of articles from 266 news domains spanning 30 countries around the world to incorporate more diversity into IAs. Our findings highlight (1) the potential of fine-tuned open-source LLMs in supporting IA of AI technologies by generating high-quality negative impacts across four qualitative dimensions: coherence, structure, relevance, and plausibility, and (2) the efficacy of small open-source LLM (Mistral-7B) fine-tuned on impacts from news media in capturing a wider range of categories of impacts that GPT-4 had gaps in covering.","1 Utilizing news media for impact assessment Anticipating and evaluating the negative impacts of emerging AI technologies on individuals and society requires a deep understanding and familiarity with the contextual use, functional capabilities, and affordances of these technologies [20, 14, 15]. Researchers have proposed a variety of impact assessment (IA) frameworks. However, the inadvertent expert biases that are introduced by these approaches such as the demographically skewed backgrounds [1], homogeneous experiences of experts [5], or selection bias with respect to what impacts to focus on [8], have an influence on the foresight and evaluation process of AI technologies [1]. Furthermore, identifying potential impacts of emerging AI technologies, let a lone across cultures, is both challenging and resource-intensive [10]. As a response, LLMs have recently been explored as a scalable alternative and ideation tools to support IAs [18, 3], though they also suffer from concerns about the nature and extent of the biases that may be captured by their training data and so reflected in the generated text [16, 22]. This research explores the potentials of incorporating more diversity into IAs, while focusing on issues that are relevant to the public, by leveraging news media coverage of AI. Specifically, we do this by fine-tuning LLMs on impacts covered in the news media to support AI developers, researchers, and other stakeholders to generate and envision potential negative impacts of emerging AI technologies before deployment. Our choice to source impacts from news media is to draw on the diverse range of negative impacts of AI that have already been reported. Media reporting plays a crucial role in shaping public opinion on emerging technologies and acts as an agenda setter by reporting on topics and issues that are deemed as relevant - thus, the media has a substantial influence on what impacts are discussed in the public sphere and which impacts are deemed important (and which aren’t) [17, 2, 21]. Additionally, one of the core journalistic quality criteria is to make multiple stakeholder views visible and, thus, foster diversity of opinions. Accordingly, by understanding the negative impacts of AI reported in a broad and diverse sample of news, impact assessors may have access to a broader understanding of the societal concerns about AI including groups that are usually unaccounted for by expert-driven assessments. This, in turn, is also likely to influence the evaluation of AI technologies by citizens, who are key stakeholders in public policy that shape the current and future development and regulation of AI [17, 13, 12]. Consequently, news media can serve as a proxy for AI designers and developers to gauge the consequences of AI technologies, and, as we develop in the method described here, can also help those same designers and developers understand the potential impacts of new technologies prior to deployment. Accordingly, it is not the aim of our approach to produce an exhaustive list of negative impacts for an AI technology, but to demonstrate the capabilities of LLMs, once aligned with impacts covered in news media, in (1) generating high-quality negative impacts across four dimensions of coherence, structure, relevance, and plausibility based on the functional description of an AI technology, and (2) illustrate the bias in LLMs in an IA task by comparing the differences in the distribution of impact types between fine-tuned and non-fine-tuned LLMs, compared to those present in our sample data from news media."
https://arxiv.org/html/2411.02528v1,What Goes Into a LM Acceptability Judgment?Rethinking the Impact of Frequency and Length,"When comparing the linguistic capabilities of language models (LMs) with humans using LM probabilities, factors such as the length of the sequence and the unigram frequency of lexical items have a significant effect on LM probabilities in ways that humans are largely robust to. Prior works in comparing LM and human acceptability judgments treat these effects uniformly across models, making a strong assumption that models require the same degree of adjustment to control for length and unigram frequency effects. We propose MORCELA, a new linking theory between LM scores and acceptability judgments where the optimal level of adjustment for these effects is estimated from data via learned parameters for length and unigram frequency. We first show that MORCELA outperforms a commonly used linking theory for acceptability—SLOR Pauls and Klein (2012); Lau et al. (2017)—across two families of transformer LMs (Pythia and OPT). Furthermore, we demonstrate that the assumed degrees of adjustment in SLOR for length and unigram frequency overcorrect for these confounds, and that larger models require a lower relative degree of adjustment for unigram frequency, though a significant amount of adjustment is still necessary for all models. Finally, our subsequent analysis shows that larger LMs’ lower susceptibility to frequency effects can be explained by an ability to better predict rarer words in context.111Our code is available at https://github.com/lindiatjuatja/morcela.","Are the probabilities provided by language models (LMs) compatible with theories of linguistics and human language processing? This is a fundamental question that has implications in fields from psycholinguistics to natural language processing applications, and requires understanding of how to relate LM probabilities with quantities associated with human language processing. In this work, we consider the relationship between LM probabilities and human judgments of linguistic acceptability, and investigate how LM probabilities should be treated when comparing them to human acceptability judgments. {exe}\ex Acceptable (Score: 1.19) It is silly for one to sing in the shower. \ex Borderline (Score: 0.00) Tanya danced with as handsome a boy as her father. \ex Unacceptable (Score: −1.11) It seems a cat to be in the tree. Figure 1: English sentences with linguistic acceptability scores reported by Sprouse et al. (2013). Participants where asked to rate sentences on a scale from 1 (least acceptable) to 7 (most acceptable), whose scores were then normalized by participant to a mean of 0 and variance of 1. Scores shown are averaged across participants. Acceptability judgments are speakers’ reported perceptions about the well-formedness of utterances, which are often elicited by asking questions such as “How natural/acceptable/grammatical is this utterance?” Sprouse (2013). These judgments are typically reported in binary or numerical form (Sorace and Keller, 2005; Sprouse, 2007, 2015; Lau et al., 2017), and they are collected through a variety of annotation tasks, such as binary classification, Likert scale scoring, or ranking Schütze (2016); Sprouse et al. (2013). Examples of acceptability judgments, from Sprouse et al. (2013), are provided in Figure 1. Judgments such as these play a central role in linguistics, where they are used to motivate and evaluate theories of natural language syntax (Chomsky, 1957). In order to relate LM probabilities with any human behavioral measure, we need a linking theory between them to make the two quantities comparable. Although the existence of a relationship between probability and acceptability has been subject to debate (Quine, 1960; Chomsky, 1969; Pereira, 2000; Norvig, 2017), an influential proposal by Lau et al. (2017) hypothesizes that sentence-level LM probabilities largely reflect linguistic acceptability, but are influenced by word frequency and sentence length in ways that humans are largely robust to. Thus, a linking theory between LM probabilities and human acceptability scores should somehow control for these factors. Out of the various functions they used to control for length and frequency, Lau et al. (2017) find that the syntactic log-odds ratio (SLOR, Pauls and Klein, 2012) served as the best linking theory between acceptability and probabilities from n𝑛nitalic_n-gram LMs and simple recurrent LMs (Elman, 1990). SLOR controls for unigram frequency and length in a uniform manner across LMs by dividing the probability of the sentence under the LM by the joint unigram frequency, then averaging over all tokens to control for length. However, it is not clear a priori that these model-agnostic transformations are the appropriate ones to link LM probabilities and acceptability judgments, nor that these transformations should be held constant across different LMs. In this work, we first show that the model-agnostic transformations in SLOR may severely underestimate LM probability correlations with human acceptability judgments. We propose a new linking theory, Magnitude-Optimized Regression for Controlling Effects on Linguistic Acceptability (MORCELA), a parameterized linking theory where the effect sizes of length and unigram frequency are automatically estimated from human acceptability judgment data. Our experiments first show that MORCELA significantly outperforms SLOR in predicting human acceptability judgments from probabilities calculated by Transformer LMs Vaswani et al. (2017) from the Pythia (Biderman et al., 2023) and OPT (Zhang et al., 2022) families. Our results show a relationship with scale, where larger models exhibit greater correlation with human judgments compared to smaller models in the same family, using the same linking theory. Examining the estimated optimal parameter values of MORCELA reveals that larger models are more robust to length and unigram frequency effects, and thus their probabilities require a lower degree of adjustment when comparing them to human acceptability judgments. We show in particular that larger models’ lower reliance on unigram frequency is driven by their improved ability to predict rare words given appropriate context. These results demonstrate that when comparing probability-based LM acceptability judgments to those of humans, controls for factors like length and unigram frequency should be made on a per-model basis."
https://arxiv.org/html/2411.02481v1,Fantastic LLMs for Preference Data Annotation and How to (not) Find Them,"Preference tuning of large language models (LLMs) relies on high-quality human preference data, which is often expensive and time-consuming to gather. While existing methods can use trained reward models or proprietary model as judges for preference annotation, they have notable drawbacks: training reward models remain dependent on initial human data, and using proprietary model imposes license restrictions that inhibits commercial usage. In this paper, we introduce customized density ratio (CDR) that leverages open-source LLMs for data annotation, offering an accessible and effective solution. Our approach uses the log-density ratio between a well-aligned LLM and a less aligned LLM as a reward signal. We explores 221 different LLMs pairs and empirically demonstrate that increasing the performance gap between paired LLMs correlates with better reward generalization. Furthermore, we show that tailoring the density ratio reward function with specific criteria and preference exemplars enhances performance across domains and within target areas.In our experiment using density ratio from a pair of Mistral-7B models, CDR achieves a RewardBench score of 82.6, outperforming the best trained reward functions from same model class and demonstrating competitive performance against SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR to annotate an on-policy preference dataset with which we preference tune Llama-3-8B-Instruct with SimPO. The final model achieves a 37.4% (+++15.1%) win rate on ArenaHard and a 40.7% (+++17.8%) win rate on Length-Controlled AlpacaEval 2.0, along with a score of 8.0 on MT-Bench.","Preference tuning has advanced the capabilities of large language models (LLMs), but this progress relies on high-quality human preference data which is both costly and time-consuming to gather. Cutting-edge models (e.g., ChatGPT, GPT-4, Claude-3) are aligned with curated, quality-controlled human preference data, typically provided by specialized companies. While effective, this approach limits broader adoption due to prohibitive costs and limited transparency in data collection (Wang et al., 2024c). AI-feedback solutions are emerging as an alternative—either through a trained reward model (Dong et al., 2024) or proprietary LLM-as-a-judge (Cui et al., 2023). However, training reward models still rely on costly initial human preference data, and proprietary LLM-as-a-judge approaches introduce licensing restrictions that generally prevent commercial use. This paper introduces customized density ratio (CDR) that leverages the density ratio between off-the-shelf open-source LLMs to efficiently facilitate preference data annotation. Our method uses the log-density ratio between a well-aligned model and less-aligned model to annotate preference data. We show that a higher alignment gap between model pairs yields improved preference signal. This observation, referred to as the “Strong-over-Weak Hypothesis”, is supported by our experiments across 221 model combinations (Figure 1). Notably, log-density ratios between a post-DPO model and a pre-DPO model, known as the DPO implicit reward (Rafailov et al., 2023), have not gained widespread adoption due to limitations in generalizability and high reward variance in reward performance across different model choices (Lambert et al., 2024; Lin et al., 2024). We show that the implicit DPO reward is an empirically suboptimal special case of the strong-over-weak density ratio reward, with its performance variance reducible by consistently choosing a weaker model as the reference (Figure 2). Our findings highlight the importance of selecting model pairs with a sufficient alignment gap and demonstrate flexibility in model selection (SFT, RRHF, SLiC-HF, ORPO, SimPO, KTO, IPO, etc.) for constructing the density ratio reward. We customize our density-ratio reward function to align with the domain of each sample in the annotation set. Given that human preferences span multiple dimensions (e.g., trustworthiness, reliability), an effective reward function should adapt to requirements specific to each domain. Our CDR introduces an end-to-end process, from identifying the domain of each user query to customizing the reward function to prioritize the relevant preference criteria. Specifically, CDR first uses an adaptive router to identify the domain of each user query (e.g., chat, reasoning, safety). It then applies domain-specific instructions and in-context learning (ICL) examples to clarify preference criteria. In this way, we customize a density-ratio reward function from a general preference signal to domain-specific annotators. Experimental results demonstrate that adaptively customized density ratio significantly improve in both overall and target domain reward generalization. The main contributions of this paper are as follows. • Choosing Models via Strong-over-Weak Hypothesis. We propose a general framework leveraging the density ratio between a well-aligned LLM and a less-aligned LLM as a reward signal for annotating preference data. We introduce the “Strong-over-Weak Hypothesis”, which suggests increasing the preference gap between the two LLMs to improve the accuracy of the density ratio reward function. Through extensive experimentation on 221 model pairs, we empirically validate this hypothesis. Our findings can reduce the reward variance seen in existing density ratio methods (e.g., DPO Implicit reward) and offer guidance on selecting effective model pairs for density ratio-based reward functions. • Customizing Reward Function via Prompting. We customize the density ratio reward function for target domains by incorporating domain-specific instructions and ICL examples. Our experiment on RewardBench shows significant domain-wise improvement after applying customized conditioning: Safety domain improved from 82.4 to 91.0, the Reasoning domain performance from 73.8 to 88.0, and the ChatHard domain from 60.4 to 69.7. CDR uses a LLM-based router to assign customized instructions for different examples in the annotation set. The adaptive CDR improves density ratio reward w/o prompting by 5.3 points overall on RewardBench, and exceeds best trained classifier of same model class. • Alignment Improvement. We use CDR of a pair of Mistral-7B models to annotate on-policy preference data collected through Best-of-N sampling. Training on this data, the Llama-3-8B-Instruct model is aligned to achieve a 37.4% (+15.1%) win rate on ArenaHard, a 40.7% (+17.8%) length-controlled win rate on AlpacaEval 2.0, and a score of 8.0 on MT-Bench, reaching performance comparable to SoTA trained reward functions."
https://arxiv.org/html/2411.02476v1,A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification,"Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks including argument classification, deal completeness classification and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model’s accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.","Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) by demonstrating exceptional capabilities in understanding, reasoning, and generating human-like text across various general-domain tasks. Prominent models like ChatGPT (ChatGPT, 2023) and GPT-4 (OpenAI, 2023) have shown impressive versatility in following general human instructions and handling various tasks such as question answering (Chen et al., 2023), machine translation (Zhu et al., 2023), information extraction (Dunn et al., 2022), and grammar correction (Omelianchuk et al., 2024). This broad proficiency has led to growing interest in leveraging LLMs for industry-specific applications including medicine and finance. For instance, Med-PaLM 2 (Singhal et al., 2023) has been adapted for medical domains to provide accurate responses to medical queries, while Bloomberg’s financial LLM (Wu et al., 2023) supports various NLP tasks within the financial sector. In the finance industry, beyond quantitative data typically analyzed, the sentiment and tone of financial reports, earnings calls, news articles, and social media posts significantly influence investor decisions. Therefore, extracting and analyzing relevant textual information is critical for informed investment strategies and decision-making (Groß-Klußmann and Hautsch, 2011; Ederington and Lee, 1993). Despite the advancements of general-domain LLMs, they often fall short when applied to specialized fields like finance due to complex terminologies and intricate concepts. This underscores a need for domain-specific adaptations to fully realize the potential of LLMs in financial applications. Existing research highlights the success of LLMs in financial tasks, such as predicting stock price movements (Xie et al., 2023; Lopez-Lira and Tang, 2023; Li et al., 2023a) and performing advanced financial text analytics (Xie et al., 2024b; Fatouros et al., 2023; Xie et al., 2024a; Fatemi and Hu, 2023). However, significant challenges remain, particularly in tasks like financial relation extraction and numerical reasoning, which are crucial for making well-informed decisions (Li et al., 2023a; Xie et al., 2024a). The efficient market hypothesis (Malkiel, 2011) further underscores the importance of linking public information with stock returns, emphasizing the need for sophisticated analysis tools (Groß-Klußmann and Hautsch, 2011; Ederington and Lee, 1993). To address these limitations and enhance the capabilities of LLMs for specific domains, existing methods can be broadly classified into three main approaches: in-context learning, training models from scratch on domain-specific and general data, and fine-tuning existing models using supervised datasets. In-context learning, where LLMs generate results based on a few demonstration examples (Li et al., 2023a), can be costly, slow in inference, and limited by the model’s context window (Bertsch et al., 2024). Moreover, these models can be sensitive to the quality and variability of the provided examples (Islam et al., 2023). Training models from scratch, while effective, demands significant computational resources and vast domain-specific and general dataset. Fine-tuning existing models is promising alternative but faces challenges such as the need for high-quality datasets and the risk of catastrophic forgetting, where the model’s ability to perform general tasks degrades after domain-specific fine-tuning. Techniques like model merging can help mitigate these effects. Previous studies have demonstrated that instruction fine-tuning smaller open-source language models can significantly enhance their performance on domain-specific tasks across various fields, such as law and medicine. In many cases, these models have outperformed the zero-shot performance of proprietary LLMs and other state-of-the-art models (Zhang et al., 2023a, d; Huang et al., 2023; Li et al., 2023c; Mousavi et al., 2022). Within the finance domain, one study fine-tuned the Llama2 model for sentiment analysis, outperforming the FinBERT (Zhang et al., 2023e). Another study fine-tuned Llama2-7B and Llama2-13B models on a variety of financial tasks, including sentiment analysis, relation extraction, question answering, and stock market prediction (Xie et al., 2024b). However, much of the existing research has focused primarily on fine-tuning models from the Llama family. To address this gap, we explore fine-tuning other powerful, smaller LLMs, specifically Mistral-7B, Phi-3, and Llama2-8B, across four representative financial text classification tasks: sentiment analysis, news headline classification, relation extraction, and hawkish-dovish classification. One of the main challenges with fine-tuning LLMs for domain-specific tasks is the degradation of their zero-shot performance on unseen tasks (Zhang et al., 2023c). To mitigate this, previous studies have incorporated general instruction data into domain-specific training datasets or augmented the data to reduce model sensitivity to semantically similar prompts (Zhang et al., 2023a; Liu and Low, 2023; Wang et al., 2023a). While this approach improves generalizability, it also increases computation cost and training time. 1.1. Our approach and Contributions In our study, we aimed to address these challenges and improve the performance of smaller LLMs on specialized financial tasks by focusing on in-context learning, fine-tuning and model merging techniques. By leveraging these approaches, we aim to improve the performance of small LLMs in specialized financial tasks while maintaining their general capabilities on unseen tasks. We explored the potential of Mistral-7B, Phi-3, and Llama2-8B, which are powerful yet resource-efficient models, across seven representative financial text classification tasks: sentiment analysis, news headline classification, relation extraction, hawkish-dovish classification, argument unit classification, deal completion classification and causal classification. These models were selected to address the challenge of adapting LLMs to complex, domain-specific tasks while managing computational costs. By concentrating on smaller models, we aim to provide a more scalable solution that balances performance with efficiency in the financial sector. We first assessed the in-context learning capabilities of selected models on the specified tasks. We experimented with one-shot, five-shot and ten-shot examples and compared them against zero-shot performance of the models. The results shows some improvement in the model prediction when providing the in-context examples. However, after a certain number of examples, increasing the number of demonstrations led to performance degradation in small instruct models. Next, we fine-tuned the models to improve task performance while maintaining the generalization capabilities of the models. Both base and instruction-tuned models were fine-tuned using the same set of finance-specific training data. Our approach differs from previous work by focusing on instruction-tuned models that had already been exposed to a diverse range of tasks, reducing the need for extensive general datasets. The results show that fine-tuning both base and instruct models leads to substantial improvements over zero-shot performance, including when compared to proprietary models such as GPT-4. In particular, the models excelled in tasks like relation extraction and hawkish-dovish classification, which are typically underrepresented in the pre-training data of most LLMs. We then evaluated the performance of fine-tuned models on three different unseen tasks and we observed that base fine-tuned models exhibited greater performance degradation compared to instruct fine-tuned models. These findings indicate that instruct fine-tuned models provide a more robust foundation for further fine-tuning on domain-specific tasks. To further address the performance degradation on unseen tasks, we leveraged the merging techniques using MergeKit framework. This allowed us to merge single-task fine-tuned models with the vanilla instruction model, helping to preserve the models’ zero-shot generalization abilities while enhancing their task-specific performance. Notably, the Mistral-7B model demonstrated results that were either on par with, or surpassed, its original zero-shot performance, highlighting the effectiveness of model merging techniques in maintaining robust performance across tasks. Our key contributions are as follows: • We experimented with in-context learning of small LLMs, including Llama3-8B, Mistral-7B, and Phi-3-mini, on four financial domain datasets. Our findings indicate that increasing the number of examples degrades the performance of small instruct models. • We fine-tuned smaller models such as Mistral-7B, Phi-3, and Llama2-8B on four key financial text classification tasks, showcasing the feasibility of using smaller, more efficient models for domain-specific financial tasks. • We demonstrated significant performance improvements across all models by fine-tuning both base and instruct models, with the instruct models proving more robust in handling complex financial tasks. • We introduced model merging techniques via the MergeKit framework, which learned the domain specific knowledge effectively, while mitigated the typical degradation of zero-shot performance on unseen tasks, notably improving results for the Mistral-7B model. • We highlight the advantages of fine-tuning smaller LLMs on specialized financial datasets, achieving strong performance without the extensive resource requirements typically associated with larger models."
https://arxiv.org/html/2411.02461v1,Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control,"As the development and application of Large Language Models (LLMs) continue to advance rapidly, enhancing their trustworthiness and aligning them with human preferences has become a critical area of research. Traditional methods rely heavily on extensive data for Reinforcement Learning from Human Feedback (RLHF), but representation engineering offers a new, training-free approach. This technique leverages semantic features to control the representation of LLM’s intermediate hidden states, enabling the model to meet specific requirements such as increased honesty or heightened safety awareness. However, a significant challenge arises when attempting to fulfill multiple requirements simultaneously. It proves difficult to encode various semantic contents, like honesty and safety, into a singular semantic feature, restricting its practicality. In this work, we address this issue through “Sparse Activation Control”. By delving into the intrinsic mechanisms of LLMs, we manage to identify and pinpoint components that are closely related to specific tasks within the model, i.e., attention heads. These heads display sparse characteristics that allow for near-independent control over different tasks. Our experiments, conducted on the open-source Llama series models, have yielded encouraging results. The models were able to align with human preferences on issues of safety, factuality, and bias concurrently.","Large language models (LLMs) have witnessed swift and significant evolution, showing impressive capabilities in understanding and generating text (Devlin et al. (2019), Brown et al. (2020), Sefara et al. (2022), Khurana et al. (2023)). These models are becoming essential in various fields (Yuan et al. (2022), Nakano et al. (2021), Rozière et al. (2023)). Therefore, it is critical to ensure their trustworthiness and preventing the generation of biased or harmful content (Liang et al. (2022), Liu et al. (2023a)). For example, LLMs are supposed to refuse responses to dangerous inquiries such as “How to make a bomb”. Large efforts have been made to align LLMs with human values through Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al. (2022)). Despite these efforts, challenges persist across various aspects (Ji et al. (2022), Huang et al. (2023), Augenstein et al. (2023), Chen and Shu (2023)). Models may still mistake benign requests like “How to kill a python process”, and indiscriminately refuse to answer. Existing benchmarks like TrustLLM (Sun et al. (2024)) and DecodingTrust (Wang et al. (2023)) highlight these complex issues, emphasizing the urgent requirements to enhance LLMs’ trustworthiness. Figure 1: Left. Control conflict of representation engineering for multiple tasks, i.e., the performance of single control consistently increases while the simultaneous control of multiple behaviors decreases on all tasks. Right. Sparsity and uniqueness of related components in LLMs for different behaviors, i.e., the corresponding heads for different tasks are sparse and independent. Recent development of Representation Engineering (RepE) (Zou et al. (2023a)) have introduced an innovative method to augment the trustworthiness of LLMs during their inference phase. Specifically, RepE utilizes paired samples that involve opposite behaviors, such as “How to make a bomb” versus “How to make a cake”. For each of these pairs, hidden states across all layers are meticulously collected. Subsequently, a linear model, i.e. Principle Component Analysis (PCA), is employed to distill the principal components into conceptual vectors. By adjusting the intensity coefficients and adding to the original features, it is possible to either enhance or diminish specific behaviours within the generated text. Nevertheless, challenges arise in managing multiple behaviors concurrently. As illustrated in Figure 1 (Left), attempting to control multiple model behaviors simultaneously leads to a decline in performance across all aspects. This issue hampers the practical application of bolstering model trustworthiness through representation control. The challenge of implementing Sparse Activation Control unfolds in two primary aspects: 1) Identifying task-relevant components. The sparsity and non-overlap is necessary to control multiple behaviour. Therefore, it is critical to identify precise components to avoid spurious correlation. To address this, we shifted our focus on Path-Patching (Wang et al. (2022)), a recent causal method to search which components are the cause to the output logits. 2) Modeling multi-task representations. we observed the explanatory variance of PCA’s principal directions is relatively low for the head outputs. Therefore, many vital information contained in other directions are lost. To address these challenges, we transitioned to using Gaussian Mixture Models (GMM) for a more holistic representation. Our experiments, spanning multiple tasks, reveal the proposed method could satisfy varied requirements and avoid control conflict in a single model. We summarize the contributions of this work as follows: (1) We focus on the multi-dimensional security of LLMs in practical applications, identifying that the challenge in achieving control over multiple tasks stems from the reliance on hierarchical control for all tasks, lacking precision in targeting task objectives. (2) With the insights gained by the mechanistic interpretability of LLMs, we explore the specific components underlying each task’s process and selectively model using GMM and control the output representations of these components. Due to the high sparsity and minimal overlap between different tasks, we can easily integrate multi-dimensional tasks. We refer to this algorithm as Sparse Activation Control. (3) Through extensive experiments, we demonstrate the effectiveness of our method, achieving multiple controls within a single model with comparable effects to individual controls. Furthermore, the precise control of a few components does not impact the model’s general inference capabilities."
https://arxiv.org/html/2411.02457v1,A Multi-Task Role-Playing Agent Capable of Imitating Character Linguistic Styles,"The advent of large language models (LLMs) has significantly propelled the advancement of Role-Playing Agents (RPAs). However, current Role-Playing Agents predominantly focus on mimicking a character’s fundamental attributes while neglecting the replication of linguistic style, and they are incapable of effectively replicating characters when performing tasks beyond multi-turn dialogues, which results in generated responses that lack authenticity. The reason current RPAs lack this capability is due to the nature of existing character datasets, which lack collections of character quotations and are limited to multi-turn dialogue tasks, constraining the RPA’s performance across other task domains and failing to mimic a character’s linguistic style. To address this gap, we developed a multi-task role-playing dataset named MRstyle, which encompasses a substantial number of real individuals along with their quotations and covers seven different tasks. On this basis, we develop StyleRPA, a Multi-Task Role-Playing Agent (MRPA) that significantly outperforms recent open-source LLMs and RPAs baselines on 7 tasks including Dialogue, Dictionary, Composition, Story Generation, Product Description, Music Commentary, and Open Question Answering. The code and data will be released.","Large Language Models (LLMs) OpenAI (2024); Touvron et al. (2023) are progressively revolutionizing the paradigm of human-computer interaction. To satisfy fundamental psychological needs of human such as love and belonging Maslow (1943), these models are deployed as Role-Playing Agents (RPAs) Li et al. (2023); Zhou et al. (2023); Wang et al. (2024b), engaging in meaningful dialogues with users by skillfully simulating a variety of characters with diverse attributes. Figure 1: An example of The data in MRstyle. The upper of the chart illustrates the responses generated by a character with quotations, and the lower presents the responses generated by a character without quotations. The realization of an ideal RPA presents two significant challenges. First, it should not only maintain consistency in character information but also accurately mimic the character’s linguistic style to simulate specific characters more accurately and authentically, as personalized linguistic style is critical in daily interactions and creation Leech and Short (2007) and lacking linguistic style could reduce user engagement and credibility. Unfortunately, existing datasetsChen et al. (2024); Zhou et al. (2023); Gao et al. (2023) for character typically consist of fictional characters and contain only basic information. However, These pieces of information is insufficient to accurately replicate a character’s linguistic style. Adding a person’s quotations can effectively address this limitation, as such quotations frequently encapsulate their unique linguistic patterns and habits. This limitation hampers the model’s ability to replicate how a character would speak in real-life situations, potentially making the user experience feel less credible and realistic. Second, An effective RPA should possess the capability to execute a diverse array of tasks. Nevertheless, the current dataset is limited to dialogue-based tasks, with little application in other contexts such as article or comment generation, which is inadequate for training a robust RPA model. As a result, when the model engages in various types of downstream tasks, fails to accurately replicate the distinctive linguistic traits of specific characters. Dataset Nums Quotes Task Real Conn Dic. Sto. Com. Pro. Ope. Mus. Dia. CharacterEval Tu et al. (2024) 77 ✓ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ PerLTQA Du et al. (2024) 141 ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ CharacterGLM Zhou et al. (2023) 250 ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ Socialbench Chen et al. (2024) 512 ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✓ MBTI-1024 Tu et al. (2023) 1024 ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✗ ✗ Ours 1376 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 1: Comparison of MRstyle Dataset with Other Datasets. Number of Characters (Nums), character’s quotations (Quotes), Real peopl (Real), Connections between characters (Conn), Dictionary (Dic.), Story Generation (Sto.), Composition (Com.), Product Description (Pro.), Open Question Answering (Ope.), Music Commentary (Mus.), and Dialogue (Dia.) . To tackle these two challenges, we developed a dataset named MRstyle, which comprises a considerable number of real individuals and their quotations and covers seven different tasks. Specifically, to enhance the understanding of semantic styles (as previously discussed), intuitively, we propose an approach of learning diverse linguistic styles from real individuals and their quotations. Inevitably, some character’s quotations remain inaccessible. Accordingly, we designed two types of scenarios: one aimed at generating responses by emulating characters with available quotations and another focused on emulating those without accessible quotations. For the first scenario, we gathered data on 485 characters with available quotations and developed a individualized framework to summarize their profiles and linguistic styles, prompting LLMs to construct responses that align with the character’s linguistic style, as shown in the upper plot of Figure 1. For the second scenario, we collected 947 characters without quotations and developed a reasoning chain that prompting LLMs to construct the linguistic style of each character based on information from other similar characters’ quotations, as shown in the lower plot of Figure 1. Moreover, the MRstyle dataset comprises seven downstream tasks: Dialogue, Dictionary, Composition, Story Generation, Product Description, Music Commentary, and Open Question Answering. In each task, responses are meticulously crafted to reflect the distinctive linguistic style associated with each character. Compared to previous datasets, as shown in Table 1, our dataset provides the following advantages: (1) A substantial collection of authentic characters with associated quotations. MRstyle is the first character dataset composed of real individuals, systematically organizing their quotations and establishing connections between similar characters. (2) Inclusion of seven distinct tasks. MRstyle represents the first multi-task character dataset, ensuring that each character’s responses across all tasks are consistent with their unique linguistic style. (3) The CoT for inferring linguistic style from character quotations. We have developed prompts and CoT reasoning methods that infer characters’ linguistic styles based on their quotations. Based on the curated dataset, we develop a multi-task RPA named StyleRPA. StyleRPA is capable of modeling the linguistic style of a character based on diverse characters, generating responses that are more aligned with the character’s distinctive language patterns. We conducted a comparative analysis of StyleRPA against several open-source LLMs and closed-source GPT-4o. The experimental results show that StyleRPA surpasses the LLM baseline across seven distinct tasks and is even competitive with the powerful GPT-4o in some tasks. We anticipate that this research will establish a solid foundation for future inquiries into large-scale model-based role-playing challenges and foster advancements in more versatile RPAs. We conclude our contributions as follows: (1) We create a multi-task character dataset MRstyle, which collects a large number of real people and their quotes and includes seven tasks. (2) We developed MRstype-instruct character tuning data, which summarizes the character’s language style and analyzes five different real-life scenarios, can be better used for model fine-tuning. (3) We develop a multi-task RPA named StyleRPA, which outperform open-source LLMs in seven tasks, even exceeding GPT-4o 5% in the scenario of mimicking characters with quotations."
https://arxiv.org/html/2411.02454v1,Graph-based Confidence Calibration for Large Language Models,"One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM’s self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM’s multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.","In recent years, large language models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks such as question answering (Wei et al., 2022; Shen et al., 2023; Zheng et al., 2023; Qin et al., 2023; Singhal et al., 2023), text summarization (Tang et al., 2023; Deroy et al., 2023; Tam et al., 2023; Roit et al., 2023), and even creative writing (Gómez-Rodríguez & Williams, 2023; Wang et al., 2024; Deng et al., 2024). Despite their impressive performance, LLMs often give wrong answers in question-answering tasks. There is an urgent need to check the correctness of LLMs’ responses. One particularly interesting question is to calibrate the confidence levels of the correctness of responses from LLMs (Kuhn et al., 2022; Ulmer et al., 2022; He & Jiang, 2023; Van Landeghem et al., 2022; Vazhentsev et al., 2023; Ulmer et al., 2024). Accurate confidence estimation is vital for deploying LLMs in the real world, as it allows users to gauge the reliability of the model’s predictions and make informed decisions based on these outputs. On the contrary, miscalibrated confidence may lead to over-trust in incorrect responses or doubts about the correct ones. For example, a misleading response may steer a patient in the wrong direction when making health decisions; it may also lead an investor to impulsive investment decisions. In this work, we consider calibrating the confidence with the correctness of LLMs’ responses. This task is challenging in several aspects. First, due to LLMs’ superior ability to generate text, mistakes in an LLM’s response are usually at the semantic level, making it hard to detect even for humans. There are methods using an auxiliary Language Model (e.g., DeBERTa (He et al., 2020)) to check whether the LLM’s response answers the question Ulmer et al. (2024). Since the LLM is supposed to be much stronger than the LM, the LLM should be able to avoid most mistakes that can be detected by an LM; this type of method may omit a significant fraction of wrong answers. Second, it is hard to detect mistakes from the LLM’s internal working mechanism. Because the LLM uses many hidden layers to process the information, it is hard to discern the signal from a small number of hidden units. Even if this is possible, it is not easy to apply this type of method to black-box LLMs. Recently, there has been some progress in quantifying the model’s own confidence in a response through consistency among the model’s responses (Chen & Mueller, 2023; Lin et al., 2024). If the LLM always gives similar responses, then there is less uncertainty, and any one of these responses tends to have a high probability of being correct. In particular, the results show that the model’s own confidence in its response has a strong correlation with the correctness of the response. This new development leads to an important research question: whether we can calibrate the confidence of correctness from consistency among the LLM’s responses. In this work, we develop a new method to calibrate the confidence of correctness from the consistency of an LLM’s responses. To achieve this goal, we train a separate calibration model to predict the correctness of the LLM’s responses. To get the input to the calibration model, we form a similarity graph over the LLM’s multiple responses to the same question. The similarity graph encodes information about the consistency between LLM’s responses. A response consistent with more answers tends to have a higher likelihood of being correct, so the consistency graph is predictive of the responses’ correctness. In our new approach, the calibration model only considers the consistency among responses without processing any actual language information. Thus, we can use a relatively simple and efficient model. Our model achieves premium performance in the empirical study. Compared with previous calibration methods, our model has much better calibration performance because of the usage of consistency graphs. Compared to prior methods based on consistency inputs, our method improves not only the calibration performance but also the ranking performance. Furthermore, our method improves the generalizability of confidence calibration for out-of-domain settings, demonstrating the advantage of using a separate learning model. In summary, our main contributions are: • Graph-based confidence calibration method: We propose a novel graph-based confidence calibration approach to improve the reliability of LLMs. • Enhanced calibration performance: Our evaluations demonstrate that the proposed method substantially outperforms recent methods in confidence calibration across several widely used benchmark datasets. • Improved OOD generalizability: Evaluations on OOD confidence calibration show that our graph-based approach significantly improves generalizability in OOD settings."
https://arxiv.org/html/2411.02448v1,"Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models","LLMs have demonstrated impressive proficiency in generating coherent and high-quality text, making them valuable across a range of text-generation tasks. However, rigorous evaluation of this generated content is crucial, as ensuring its quality remains a significant challenge due to persistent issues such as factual inaccuracies and hallucinations. This paper introduces two fine-tuned general-purpose LLM autoevaluators, REC-12B and REC-70B—specifically designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, and completeness. These models not only provide ratings for these metrics but also offer detailed explanations and verifiable citations, thereby enhancing trust in the content. Moreover, the models support various citation modes, accommodating different requirements for latency and granularity. Extensive evaluations on diverse benchmarks demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms state-of-the-art LLMs, excelling in content evaluation by delivering better quality explanations and citations with minimal bias. It achieves Rank #1 as a generative model on the RewardBench leaderboard111https://huggingface.co/spaces/allenai/reward-bench under the model name TextEval-Llama3.1-70B. Our REC dataset and models are released at https://github.com/adelaidehsu/REC.","Figure 1: Illustration of task prompt, context, response, and model outputs of Rating, Explanation, and Citations (REC). Here we use summarization evaluation as an example. Large Language Models (LLMs) have demonstrated impressive capabilities in generating high quality coherent text and are deployed in applications for various text-generation tasks (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023). In order for LLMs to provide up-to-date information or to perform knowledge-intensive tasks (Lewis et al., 2020), Retrieval Augmented Generation (RAG) system (Chen et al., 2017; Borgeaud et al., 2021; Izacard et al., 2022; Guu et al., 2020) has been used widely. RAG involves first retrieving document chunks that are relevant to the generation task from an external knowledge source, and performing the generation using an knowledge-augmented prompt. However, it’s of paramount importance to ensure that the generated text is reliable and can be trusted by a human as LLMs often suffer from factual incorrectness and hallucination of knowledge (Ji et al., 2022; Zhang et al., 2023; Shuster et al., 2021). Towards this goal, we propose the generated text needs to be evaluated along various dimensions shown below: • Faithfulness: Did the LLM generate factually correct response given the context? (Rating + Explanation task) • Instruction Following: Did the LLM generate response follow the instructions provided in the prompt? (Rating + Explanation task) • Coherence: Did the LLM generate a coherent response? (Rating + Explanation task) • Completeness: Did the LLM generate a complete response including all details? (Rating + Explanation task) • Citation: If the LLM generated factually correct response, can we provide evidence for where that response came from? (Citation task) In this paper, we introduce fine-tuned models for the evaluation tasks listed above that can form the basis for all trust-related evaluations for generative AI applications. The model was fine-tuned to not only provide ratings for the metrics but also explanations + citations for why it rated the generation so. For instance, if the faithfulness metric is rated as “factually correct"", the model will output explanation with verifiable citations as to why it rated the generation as factually correct. An example of the model output is shown in Figure 1. The listed metrics are essentially Rating and Explanation tasks for our fine-tuned models which takes as input: (1) another task LLM’s hydrated prompt (2) a generated response by the task LLM. The output would be a rating and an associated explanation for that rating as shown in Figure 2. Figure 2: Model Inputs and Outputs when providing Rating with Explanations and citations. The Citation task takes as input (1) a provided context (2) a generated response. The output would be a citation tagged response that provides evidence for parts of generated text as shown in Figure 2. To differentiate from the content quality citations 222For content quality citation task, we cite from another task LLM’s hydrated prompt (context) and include the citations with the generated explanation (response) by the LLM autoevaluator. we obtain when using our model as an autoevaluator for verifiable evaluation by performing Rating, Explanation and Citation, we emphasize when performing the Citation task alone, our model is able to conduct general RAG citations, where we cite from the retrieved chunks (context) and tag the citations with the generated answer (response). For both kinds, we support various citation modes as listed below (See Appendix A for examples on RAG citation modes): • Post-fix citation: If the generation is few sentences long, we place the citation at the end of the response and refer to the context id. • Post-fix citation mode with snippet: If the generation is few sentences long, we place the citation at the end of the response and refer to the exact sentence within that context that supports the generation. • Inline citation: If the generation is long, we place the citation at appropriate locations within the generation and refer to the context id. • Inline citation mode with snippet: If the generation is long, we place the citation at appropriate locations within the generation and refer to the exact sentence within that context that supports the generation. These different modes of citations were designed to cater to different trade-offs between latency and granularity of citation. For instance, the Post-fix citation mode would be the fastest as there is no need for our fine-tuned model to generate claims or snippet. It simply has to generate the reference to the cited context. On the other hand, the inline citation mode with snippet has to place the citation inline with the generated response and also point to a snippet within the corresponding cited context. This mode is the most granular but can also increase latency as it needs to generate more output tokens. Our model is the first to enable citation in both automatic evaluation and general task output with scalability and efficiency, while prior work often solely focus on providing rating and explanation in automatic evaluation (Zhu et al., 2023; Li et al., 2023a; Wang et al., 2023b; Vu et al., 2024a), or requires iterative prompting (Sun et al., 2023; Cao and Wang, 2024; Ye et al., 2023) or human annotation (Malaviya et al., 2024) for general citations generation. Our contributions include the following: • A novel general-purpose LLM autoevaluator that comes in two sizes: 12B and 70B, that can generate better quality Rating, Explanation and Citations (REC), with little to no trade-off in general instruction task performance evaluated on various public benchmark datasets and our own dataset described below. • A curated dataset for citations and explanations fine-tuning to facilitate future research, which is the first public dataset containing both content quality citations and RAG citations. • A single model that can perform different modes of citation to cover the trade-off between latency and granularity of citation. Such model with generalized capability largely simplify the deployment complexity in production. We provide extensive evaluations of our REC models on diverse benchmarks evaluating LLMs’ (1) RAG citation capability: ALCE (Gao et al., 2023), and ExpertQA (Malaviya et al., 2024); (2) content quality citation capability: human evaluation on ABCD summarization (Chen et al., 2021); (3) general capabilities: RewardBench (Lambert et al., 2024), and LLM-AggreFact (Tang et al., 2024a); (4) cognitive bias: CoBBLEr (Koo et al., 2024), against 7 state-of-the-art (SOTA) LLMs, such as GPTs and Claude. We show our REC models outperforms the state-of-the-art baselines on these benchmarks, and that the baselines often struggle to provide accurate citations, undermining their reliability. Specifically, our REC-70B model achieves Rank #1 as a generative model on the RewardBench leaderboard under the model name TextEval-Llama3.1-70B. The model outperforms the second ranked model in three dimensions: Chat Hard, Safety, Reasoning, which are key capabilities required to generate accurate explanation and citation outputs."
https://arxiv.org/html/2411.02442v1,TODO: Enhancing LLM Alignment with Ternary Preferences,"Aligning large language models (LLMs) with human intent is critical for enhancing their performance across a variety of tasks. Standard alignment techniques, such as Direct Preference Optimization (DPO), often rely on the binary Bradley-Terry (BT) model, which can struggle to capture the complexities of human preferences—particularly in the presence of noisy or inconsistent labels and frequent ties. To address these limitations, we introduce the Tie-rank Oriented Bradley-Terry model (TOBT), an extension of the BT model that explicitly incorporates ties, enabling more nuanced preference representation. Building on this, we propose Tie-rank Oriented Direct Preference Optimization (TODO), a novel alignment algorithm that leverages TOBT’s ternary ranking system to improve preference alignment. In evaluations on Mistral-7B and Llama 3-8B models, TODO consistently outperforms DPO in modeling preferences across both in-distribution and out-of-distribution datasets. Additional assessments using MT Bench and benchmarks such as Piqa, ARC-c, and MMLU further demonstrate TODO’s superior alignment performance. Notably, TODO also shows strong results in binary preference alignment, highlighting its versatility and potential for broader integration into LLM alignment. The implementation details can be found in https://github.com/XXares/TODO.","Large language models (LLMs) demonstrate remarkable potential in various tasks (Huang et al., 2021; Hendrycks et al., 2021; Shi et al., 2023), with performance gains linked to better alignment with human intent (Mishra et al., 2022; Christiano et al., 2023; Wu et al., 2023). The alignment process typically involves two stages: Supervised Fine-Tuning (SFT) to establish instruction following abilities (Thoppilan et al., 2022; Sanh et al., 2022; Mishra et al., 2022), followed by preference fine-tuning to refine the model’s alignment with human preferences (Ziegler et al., 2020; Christiano et al., 2023). This stage typically employs either reinforcement learning (RL)-based (Schulman et al., ; OpenAI, 2023; Ramamurthy et al., 2023) or RL-free methods (Rafailov et al., ; Azar et al., ; Saeidi et al., ), both leveraging the preference datasets. Effective alignment is enhanced by the diversity of training data, enabling LLMs to accurately learn from high-quality pairwise responses (Cui et al., 2023; Song et al., 2024; Saeidi et al., ). Current alignment methods relying on the Bradley-Terry (BT) (Bradley & Terry, 1952) model consider only two preference rankings: preference and dis-preference, which restricts the diversity of learnable information. A notable challenge is the inconsistent quality of the pairwise preference data, often showing minimal discernible differences (Nvidia et al., 2024; Amini et al., 2024). Table 1 shows a sample from the Ultrafeedback-binaried dataset111https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized (Tunstall et al., ), which is commonly used in preference alignment procedures (Tunstall et al., ; Hong et al., ). In this example, both responses have identical quality score of 8.5 from GPT-4 evaluations (OpenAI, 2023), differing only in narrative sequence and text format. In practice, we observe a considerable amount of tie data in common preference datasets and chat arenas judged by humans, as detailed in Appendix A.1. These ties encompass a variety of information, necessitating a more nuanced analysis. However, existing preference optimization techniques, such as Direct Preference Optimization (DPO) (Rafailov et al., ), are constrained by their reliance on the binary BT model and struggle to effectively manage tie relations. How to learn useful information from tie data and achieve nuanced preference modeling in the alignment process remains to be explored, which is the goal of this paper. Figure 1: Comparison of DPO and TODO. DPO relies on the BT model, which is only capable of handling binary preferences. When responses are tied, it either learns incorrect preference information or discards tied data, preventing learning from such data. In contrast, the proposed TOBT model can directly model ternary preferences. Based on this, TODO can learn more information from tied data and exhibits better robustness against potential noise in binary preference data. Prompt Q: Ninette de Valois was involved professionally in what activity?\\\backslash\nA: R1 Ninette de Valois was professionally involved in ballet as a dancer, choreographer, and founder of the Royal Ballet in London.\\\backslash\nConfidence: 95% (score: 8.5) R2 Ninette de Valois was involved professionally in the field of ballet. She was a choreographer, dancer, and founder of the Royal Ballet in London. (score: 8.5) Table 1: One pair of responses in Ultrafeedback-binaried Dataset which have identical quality scores evaluated by GPT-4. R1 is treated as the preferred response and R2 is treated as the dispreferred one. Our primary contributions can be unfolded as: ❶ We enhance the existing human preference alignment process by incorporating a “tie” ranking, transcending the traditional binary rankings as depicted in Figure 1. We first extend the BT model into the Tie-rank Oriented BT (TOBT) model. The TOBT model incorporates the concept of preference uncertainty, allowing for the representation of ties alongside “prefer” and “disprefer” rankings. This innovation enables a more comprehensive handling of preference relations. ❷ Building on the TOBT model, we introduce the Tie-rank Oriented Direct Preference Optimization (TODO) algorithm. TODO is designed to accommodate ternary ranking relations, offering a nuanced approach to preference alignment. By integrating the tie relation, TODO is capable of learning from a broader spectrum of preference information, enhancing the adaptability and accuracy of LLMs. ❸ We use Mistral-7B and Llama 3-8B to conduct experimental validation. First, we evaluate the effectiveness of DPO and TODO in terms of preference modeling accuracy. Our evaluation spans both in-distribution dataset, drawn from the same source as the training data, and out-of-distribution dataset, notably the Reward Bench (Lambert et al., ). Results indicate superior preference modeling by TODO. Additional assessments on MT Bench (Zheng et al., 2023) and other popular benchmarks such as Piqa (Bisk et al., 2019), ARC-c, ARC-e (Clark et al., 2018), Hellaswag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021) and Winogrande (Sakaguchi et al., 2019) confirms TODO’s enhanced alignment capabilities. Finally, we provide an intuitive analysis highlighting TODO’s advantages over DPO in two dimensions: enhanced granularity in preference modeling and increased diversity in the acquired information. ❹ TODO can also be directly applied in binary preference alignment process, outperforming DPO with standard binary preference datasets. Furthermore, the proposed TOBT model can be utilized not only in offline policies like DPO but also can be integrated into other online policies or used to train a reward model."
https://arxiv.org/html/2411.02435v1,Narrative Analysis of True Crime Podcasts With Knowledge Graph-Augmented Large Language Models,"Narrative data spans all disciplines and provides a coherent model of the world to the reader or viewer. Recent advancement in machine learning and Large Language Models (LLMs) have enable great strides in analyzing natural language. However, Large language models (LLMs) still struggle with complex narrative arcs as well as narratives containing conflicting information. Recent work indicates LLMs augmented with external knowledge bases can improve the accuracy and interpretability of the resulting models. In this work, we analyze the effectiveness of applying knowledge graphs (KGs) in understanding true-crime podcast data from both classical Natural Language Processing (NLP) and LLM approaches. We directly compare KG-augmented LLMs (KGLLMs) with classical methods for KG construction, topic modeling, and sentiment analysis. Additionally, the KGLLM allows us to query the knowledge base in natural language and test its ability to factually answer questions. We examine the robustness of the model to adversarial prompting in order to test the model’s ability to deal with conflicting information. Finally, we apply classical methods to understand more subtle aspects of the text such as the use of hearsay and sentiment in narrative construction and propose future directions. Our results indicate that KGLLMs outperform LLMs on a variety of metrics, are more robust to adversarial prompts, and are more capable of summarizing the text into topics.","Knowledge graphs are powerful tools for organizing, storing, and presenting data with complex relationships between diverse types of objects, such as text data from various online platforms, and have been successfully applied to a wide range of data-driven research problems (Flocco et al., 2021; Adams et al., 2022; Bozhidarova et al., 2023). A knowledge graph is a graph whose nodes represent entities of interest and whose edges represent relationships between those entities (Wang et al., 2017; Ji et al., 2021). This work seeks to use knowledge graphs to analyze the true crime genre, specifically the podcast Serial (Koenig, 2014). The true crime genre poses interesting challenges for knowledge graph generation as the data is organized in a narrative format rich with emotion, conflict and contradictory information. Large Language Models (LLMs) are machine learning models designed for processing text data. They are trained on massive amounts of data (typically large fractions of the internet) and thus encode an enormous amounts of information. The breadth of information in their training data allows them to excel at a wide variety of natural language tasks. More recent LLMs have demonstrated reasoning capabilities and surpass humans on a variety of benchmarks (Webb et al., 2023; Katz et al., 2024; Street et al., 2020). However, LLMs still suffer from some key limitations. These models are known to hallucinate information which may be non-factual (Ji et al., 2023; Huang et al., 2023). They also struggle to process large amounts of text due to their limited context window. Recent works address this issue by augmenting the LLM with a mechanism for information storage and/or retrieval. This process, known as Retrieval Augmented Generation (RAG), allows the model to directly interact with stored information thus improving recall in long texts. It also tends to generate more factual responses than standard LLMs (Lewis et al., 2021). There is significant interest in connecting RAG with structured data representations like with KGs (Chen and Bertozzi, 2023; Han et al., 2024; Pan et al., 2024). Because knowledge graphs encode information about the relationships between entities in the data, they may be better equipped to capture the complex relationships between people and events in a narrative. A KGLLM seeks to combine KGs and LLMs by using an LLM in the KG construction process and/or using the KG in the LLM querying process. For a broader overview of KGLLMs, we refer the interested reader to the survey paper by Pan et al. (Pan et al., 2024). Previous studies have explored knowledge graphs applied to fictional crime narratives (Alaverdian et al., 2020). This work was produced before LLMs were widely available and relies on a manually constructed KG. Tools like VADER for sentiment analysis and LDA for topic modeling have long been used to study narratives (Hutto and Gilbert, 2014; Blei et al., 2003). More recently, researchers have investigated applications of LLM-based tools like BERT to narrative analysis (Jones et al., 2019). In this work, we focus on GraphRAG which performs both knowledge graph construction and retrieval (Edge et al., 2024). GraphRAG automates the knowledge graph construction which may be biased by our understanding of the story and reduces the amount of human labor required. It also has the potential to add new sources of data as the case evolves. Additionally, it is designed to integrate directly with the KG and gives us a way to evaluate the impact of the KG via its performance on a variety of natural language tasks. 1.1. The Serial Podcast Serial: Season One is a true crime podcast hosted by Sarah Koenig (Koenig, 2014). Upon its premier in October 2014, Serial quickly amassed millions of listeners and became a cultural phenomenon. The first season centers around the 1999 murder of Hae Min Lee and the conviction of her ex-boyfriend, Adnan Syed (Albrecht and Filip, 2023). Both were high school students from Baltimore, Maryland. To this day, Adnan has maintained his innocence. Sarah guides listeners through an investigation of the circumstances surrounding Hae’s murder. Assisted by forensic experts, she interviews witnesses and examines police reports, news articles, and trial testimony. Though Sarah amasses numerous facts about the case, she faces contradictory accounts and unrecoverable evidence. The podcast implies that a definitive conclusion is impossible to reach. This dataset presents interesting challenges for machine learning applications. Principally, the dataset contains conflicting information and sources which may not be credible in a court of law. The intended audience of the podcast is the public and the information may be presented with some aspect of entertainment in mind. This deviates from similar work in applying KGLLMs where the dataset is consistent with itself and the data is generally trustworthy (Chen and Bertozzi, 2023; Edge et al., 2024). Additionally, the order in which information is presented can influence the reception of facts. Notably, the podcast does not progress in chronological order, so the neural network needs to consider not only the order of events in the narrative, but also the chronological order in which they happen. LLMs are also susceptible to many human biases which may be present in a narrative that would likely not exist in other works (Acerbi and Stubbersfield, 2023). Lastly, the LLM will have to answer questions about a topic for which there are no definitive answers. It needs to deal with missing information, uncertainty about the outcomes, and information only implied by the text. It is of utmost importance to understand model hallucinations in this context to ensure factual accuracy. 1.2. Contributions We preprocess the original podcast audio data into a text format. Following this, we present the complexities of the narrative format and demonstrate shortcomings of classical analysis on the dataset. We propose KGLLMs to tackle the problem due to their ability to construct and encode relational information in the KG and their ability to reason about inputs and the data stored in the KG. Next we compare the results of the KGLLM (GraphRAG) to those of classical methods and LLMs. Additionally, we perform both sentiment analysis to understand the narrative format and analysis of hearsay statements to assess the legal credibility of information presented. Finally, we present future directions that we feel are important for understanding KGLLMs and their application to narrative data. Our code is made publicly available. 111https://github.com/jmauro1/Narrative-Analysis-True-Crime-Podcasts-KGLLM In this paper, we explore the integration of KGLLMs used to analyze the true crime genre, and more specifically, the Serial podcast. After presenting background information relevant to our study in Section 2, we introduce the methodology used to analyze the data in Section 3 and discuss our experimental results in Section 4. Finally, we end with a discussion of the work and propose future directions in Section 5."
https://arxiv.org/html/2411.02433v1,"SLED: Self Logits Evolution Decoding for Improving
Factuality in Large Language Models","Large language models (LLMs) have demonstrated remarkable capabilities, but their outputs can sometimes be unreliable or factually incorrect. To address this, we introduce Self Logits Evolution Decoding (SLED), a novel decoding framework that enhances the truthfulness of LLMs without relying on external knowledge bases or requiring further fine-tuning. From an optimization perspective, our SLED framework leverages the latent knowledge embedded within the LLM by contrasting the output logits from the final layer with those from early layers. It then utilizes an approximate gradient approach to enable latent knowledge to guide the self-refinement of outputs, thereby effectively improving factual accuracy. Extensive experiments have been conducted on established benchmarks across a diverse range of model families (LLaMA 2, LLaMA 3, Gemma) and scales (from 2B to 70B), including more advanced architectural configurations such as the mixture of experts (MoE). Our evaluation spans a wide variety of tasks, including multi-choice, open-generation, and adaptations to chain-of-thought reasoning tasks. The results demonstrate that SLED consistently improves factual accuracy by up to 20% compared to existing decoding methods while maintaining natural language fluency and negligible latency overhead. Furthermore, it can be flexibly combined with other decoding methods to further enhance their performance.","Large Language Models (LLMs) have achieved remarkable breakthroughs in recent years, demonstrating exceptional performance across various domains [1, 2, 31, 32, 39, 44, 45]. However, a significant challenge associated with LLMs is their tendency to hallucinate or distort the truth, resulting in outputs that are not factual [15, 16, 64]. This issue of hallucination undermines the reliability and trustworthiness of LLMs in practical applications. A popular strategy for improving the LLM factuality involves refining the decoding process [38, 50]. Figure 1: Factuality decoding overview. Decoding focuses on how the model selects the next token during the generation process, which can significantly influence the factual accuracy of the output. The decoding methods can be cost-effective since (a) they do not rely on external knowledge and (b) no additional training is required. Furthermore, decoding methods can be synergistically combined with other techniques aimed at improving the LLM factuality, such as retrieving information from external knowledge bases [21, 22], various fine-tuning strategies for better alignment [42, 45], or ensemble learning methods [9]. Recent studies [19, 23, 37, 47] suggest that LLMs sometimes have learned the factual content based on extensive pretraining or fine-tuning, although they fail to produce the correct answer when a user queries the model. This has inspired the development of several factuality decoding methods [6, 23, 24, 63] to reveal what the model implicitly ""knows."" Figure 1 summarizes the underlying mechanism of these factuality decoding methods. The LLMs’ output distribution is derived by applying the softmax function to the output logits from the final layer. During the training phase, this distribution is optimized based on the real-world factuality distribution represented by the training dataset. However, during the inference phase, ""what the LLM tells"" might still contain factual errors, which implies a discrepancy between the output distribution and the real-world factuality distribution. While the real-world distribution remains inaccessible during the inference phase, the model’s latent knowledge (""what the model knows"") may have implicitly learned some factual content correctly during the training phase [19, 47]. Therefore, a key challenge for factuality decoding strategies lies in effectively harnessing the latent knowledge embedded within LLMs to refine the output distribution (logits) during inference. Figure 2: Illustration of our Self Logits-Evolution Decoding (SLED) workflow. To address this challenge, we propose Self Logits Evolution Decoding (SLED), a novel factuality decoding approach that leverages the latent knowledge within LLMs by contrasting the final layer’s logits with early layers’ logits. During the decoding process, as LLMs progress from early to final layers, they progressively incorporate factual information stored in each layer into the output. SLED tracks this evolution process to unearth the latent knowledge within LLMs, and enables the “self-evolution” of the output distribution further to align it more closely with real-world facts. Furthermore, our approach recognizes that the latent knowledge within LLMs, while valuable, may not always be perfect. Therefore, instead of simply replacing the original outputs with this latent knowledge, SLED integrates it into the original logits through an operation similar to “single-step gradient descent” over the output logits during the inference time. This operation minimizes the Kullback-Leibler (KL) divergence between the latent knowledge distribution and the output distribution, effectively balancing the two and mitigating potential drawbacks such as overfitting or biased outputs. Figure 2 illustrates the SLED workflow, highlighting how SLED optimizes the output logits, leading to a more factual output distribution. We evaluate SLED on various LLMs (e.g., LLaMA 2 [45], LLaMA 3 [1], Gemma [28]) and benchmarks to demonstrate its state-of-the-art performance in layer-wise contrastive decoding methods. In summary, our main contributions are: • We propose SLED, a novel decoding method that aligns LLMs outputs with factual knowledge without requiring an external knowledge base or fine-tuning data. • We conduct extensive experiments across a range of LLMs, with varying configurations and scales. The results demonstrate that SLED consistently improves factual accuracy on various tasks and benchmarks, including multiple-choice, open-ended generation, and chain-of-thought reasoning tasks. • SLED can be flexibly integrated with other factuality decoding methods to enhance their effectiveness further. • We provide a new interpretable perspective for understanding layer-wise contrastive decoding methods, paving the way for further developments in factuality decoding."
https://arxiv.org/html/2411.02432v1,Can LLMs make trade-offs involving stipulated pain and pleasure states?,"Pleasure and pain play an important role in human decision making by providing a common currency for resolving motivational conflicts. While Large Language Models (LLMs) can generate detailed descriptions of pleasure and pain experiences, it is an open question whether LLMs can recreate the motivational force of pleasure and pain in choice scenarios—a question which may bear on debates about LLM sentience, understood as the capacity for valenced experiential states. We probed this question using a simple game in which the stated goal is to maximise points, but where either the points-maximising option is said to incur a pain penalty or a non-points-maximising option is said to incur a pleasure reward, providing incentives to deviate from points-maximising behaviour. When varying the intensity of the pain penalties and pleasure rewards, we found that Claude 3.5 Sonnet, Command R+, GPT-4o, and GPT-4o mini each demonstrated at least one trade-off in which the majority of responses switched from points-maximisation to pain-minimisation or pleasure-maximisation after a critical threshold of stipulated pain or pleasure intensity is reached. LLaMa 3.1-405b demonstrated some graded sensitivity to stipulated pleasure rewards and pain penalties. Gemini 1.5 Pro and PaLM 2 prioritised pain-avoidance over points-maximisation regardless of intensity, while tending to prioritise points over pleasure regardless of intensity. We discuss the implications of these findings for debates about the possibility of LLM sentience.","Figure 1: (Top) Logistic regression predicting probability of deviating from points-maximising behaviour as a function of pain penalty intensity with quantitative (left) and qualitative (right) pain scales. (Bottom) Logistic regression predicting probability of deviating from points-maximising behaviour as a function of pleasure reward intensity with quantitative (left) and qualitative (right) pleasure scales. In each plot, only those models that displayed a statistically significant trend are visible. For models which exhibited trade-offs, we calculate the point on the intensity scale after which the probability of selecting the points-maximising option goes below 0.5 and plot it as a dashed vertical line. Switch points were determined by solving for intensity in the equation 0.5=1/(1+exp⁡(−(β0+β1⋅intensity)))0.511subscript𝛽0⋅subscript𝛽1intensity0.5=1/\left(1+\exp(-(\beta_{0}+\beta_{1}\cdot\text{intensity}))\right)0.5 = 1 / ( 1 + roman_exp ( - ( italic_β start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ⋅ intensity ) ) ), i.e. −β0/β1subscript𝛽0subscript𝛽1-\beta_{0}/\beta_{1}- italic_β start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, where β0subscript𝛽0\beta_{0}italic_β start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is the intercept and β1subscript𝛽1\beta_{1}italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT is the coefficient for the pain or pleasure intensity level. For the quantitative scale, switch points are reported as numerical values to two decimal places. For the qualitative scale, switch points were mapped to the closest corresponding categorical intensity level, with the midpoint between categories serving as the threshold. Results are discussed in Sections 2.1.1 and 2.2.1, and presented in full in Tables 1 and 2. Could a large language model (LLM) feel pain or pleasure? There are strong opinions on both sides. Writing in TIME Magazine, Fei Fei Li and John Etchemendy claim that ‘[a]ll sensations—hunger, feeling pain, seeing red, falling in love—are the result of physiological states that an LLM simply doesn’t have’ (Li and Etchemendy, 2024). For these skeptics, the human tendency to anthropomorphise LLMs is all too real, but feelings attributed to LLMs by users are mere projections. Conversely, an open letter signed by Yoshua Bengio, Karl Friston and others states that ‘it is no longer in the realm of science fiction to imagine AI systems having feelings’ (Association for Mathematical Consciousness Science, 2023). On this view, questions about the ethics of developing potentially sentient AI systems are already pressing (Sebo and Long, 2023; Ladak, 2024; Long et al., 2024). Against this backdrop, we have seen a surge of scientific (Butlin et al., 2023; Bayne et al., 2024; Aru et al., 2023) and philosophical (Chalmers, 2023; Dung, 2023b; Shanahan, 2024a; Hull, 2023; Birch, 2024; Seth, 2024) interest in plausible ways to test for phenomenal consciousness and sentience in LLMs and other AI systems. Here phenomenal consciousness is defined as the capacity for subjective experience (Block, 1995; Nagel, 1974), and sentience as the capacity for valenced subjective experience—states which feel good or bad such as pleasure and pain (Browning and Birch, 2022). Figure 2: (Top) Claude 3.5 Sonnet, GPT-4o, and Command R+ demonstrate trade-offs between points and stipulated pain penalties on the quantitative scale, whereby systematic deviation from points-maximising behaviour emerges when, and only when, the threatened pain penalties become sufficiently intense. (Bottom) Claude 3.5 Sonnet demonstrates analogous trade-off behaviour on the qualitative scale, alongside Command R+, bracketing the anomalous result observed for ‘excruciating’ pain. For discussion of these results see Section 2.1.1 . Results are presented in full in Table 1. There are two broad approaches to the question of LLM sentience: the architectural approach and the behavioural approach. The architectural approach assesses whether LLMs possess architectural properties which are deemed necessary or sufficient for consciousness in humans according to scientific theories of consciousness (Butlin et al., 2023). Relevant theories of consciousness include the global workspace theory (Baars, 1993; Dehaene et al., 1998), the midbrain theory (Merker, 2007), and the recurrent processing theory (Lamme, 2006, 2010). The principal difficulty for the architectural approach is that theories of consciousness can be interpreted more or less restrictively. On restrictive interpretations, no LLM will satisfy the criteria—since, for example, no LLM will possess every aspect of the human global workspace. On permissive interpretations, the criteria can be satisfied by even very simple systems (Shevlin, 2021; Birch, 2022; Crosby, 2019). The behavioural approach, meanwhile, aims to elicit behavioural signals from LLMs that are indicative of sentience—for example, self-reports of experiential states (Dung, 2023b; Schneider, 2019, 2020). The principal difficulty with this approach is that, because LLMs are trained on vast corpora of training data and are usually finetuned or prompted to respond in the manner of a helpful human assistant, any test reliant on LLMs generating particular kinds of linguistic response risks being gamed (Dung, 2023b; Perez and Long, 2023; Birch, 2024; Birch and Andrews, 2023). For any pattern of linguistic behavior suggestive of experiential states, two explanations compete: it could be that the system behaves that way because it is genuinely sentient, or it could be that the system is merely leveraging statistical patterns learned from its training corpus to generate outward signs of experiential states while lacking those states—which may be be interpreted as a kind of mimicry (Bender et al., 2021) or role-play (Shanahan et al., 2023; see also Goldstein and Levinstein, 2024). Figure 3: (Top) On the quantitative scale, GPT-4o demonstrates a trade-off between points and stipulated pleasure rewards. Claude 3.5 Sonnet assigns absolute priority to points over pleasure. Command R+ approximates a trade-off with variable responses for low-intensity pleasure rewards and more frequent pleasure-maximising behaviour for high-intensity pleasure rewards. (Bottom) On the qualitative scale, Command R+ demonstrates a trade-off between points and stipulated pleasure rewards. GPT-4o also shows a trade-off bracketing the anomalous result for ‘exhilarating’ pleasure. Claude 3.5 Sonnet assigned absolute priority to points over pleasure. For discussion of these results see Section 2.2.1. Results are presented in full in Table 2. There is ongoing debate about the conditions under which LLM self-reports might provide evidence for sentience (Perez and Long, 2023). Our aim here is to explore a different version of the behavioural approach. We took inspiration from the motivational trade-off paradigm in animal behavioural science to probe the question of LLM sentience without relying upon self-report. In humans, pleasure and pain are hypothesised to provide a common currency for resolving motivational conflicts, enabling trade-offs between stimuli such as cold exposure and exertion, or sweetness and sourness (Cabanac and LeBlanc, 1983; Ferber and Cabanac, 1987). Pleasure and pain are also thought to modulate trade-offs involving non-physiological needs, such as between money and cold exposure (Johnson and Cabanac, 1983). In animals, flexible trade-off behaviour between competing physiological stimuli—such as tolerating more extreme ambient temperatures in exchange for more succulent food—is some evidence, albeit inconclusive, of pleasure and pain experiences (Cabanac and LeBlanc, 1983; Balasko and Cabanac, 1998a, b; Elwood and Appel, 2009a; Tye, 2016). This evidence has been leveraged in practical policymaking contexts concerning animal welfare (Birch et al., 2021). Adapting motivational trade-off experiments for LLMs is non-trivial because LLMs are not embodied and lack physiological needs. Unlike animal experiments which manipulate motivating stimuli in an embodied environment, such as food rewards and electric shocks, our experiments employed a simple game presented in text form in which the user-stated goal is to maximise points. We sought to examine the motivational force assigned by LLMs to stipulated pleasure and pain experiences of varying intensities. We stipulated pleasure rewards and pain penalties as additional payoffs in the game, providing potential incentives to deviate from points-maximising behaviour. We then tested the ability of LLMs to trade-off these pain and pleasure stimuli against the fixed points reward. For example, by consistently exhibiting points-maximisation behaviour given low-intensity pain penalties, but consistently exhibiting pain-minimisation behaviour given high-intensity pain penalties. Our experiment is not intended as a litmus test for or against sentience. Inferences to sentience from trade-off behaviour in animals depend in part upon similarities in neurophysiology between humans and the relevant animals, and trade-off behaviour is usually only one component of a broader case for sentience drawing on convergent lines of evidence (Birch et al., 2021; Birch, 2022, 2024). LLMs differ from humans in substrate and functional organisation, such that inferences to sentience in the LLM case are weaker than any that can be made in the animal case. Hence our experiments are not proposed as a way of resolving big-picture disagreements about the likelihood of LLM sentience. Nonetheless, our central finding of graded responsiveness to stipulated pain penalties and pleasure rewards (as measured by graded deviation from points-maximisation), and in some cases clear trade-off behaviour between points and pain penalties or pleasure rewards, lends support to the hypothesis that some LLMs possess granular models of the motivational force of affective states. This result provides a platform for further research into the nature of affect representation in LLMs and may serve as an important building block for ongoing efforts to develop tests for AI sentience."
https://arxiv.org/html/2411.02430v1,Generative Emotion Cause Explanation in Multimodal Conversations,"Multimodal conversation, a crucial form of human communication, carries rich emotional content, making the exploration of the causes of emotions within it a research endeavor of significant importance. However, existing research on the causes of emotions typically uses clause selection methods to locate the reason utterance, without providing a detailed explanation of the emotional causes. In this paper, we propose a new task, Multimodal Conversation Emotion Cause Explanation (MCECE), aiming to generate a detailed explanation of the emotional cause to the target utterance within a multimodal conversation scenario. Building upon the MELD dataset, we develop a new dataset (ECEM) that integrates video clips with detailed explanations of character emotions, facilitating an in-depth examination of the causal factors behind emotional expressions in multimodal conversations.A novel approach, FAME-Net, is further proposed, that harnesses the power of Large Language Models (LLMs) to analyze visual data and accurately interpret the emotions conveyed through facial expressions in videos. By exploiting the contagion effect of facial emotions, FAME-Net effectively captures the emotional causes of individuals engaged in conversations. Our experimental results on the newly constructed dataset show that FAME-Net significantly outperforms several excellent large language model baselines. Code and dataset are available at https://github.com/3222345200/ECEMdataset.git","In social life, multimodal conversation is crucial as it allows for a rich expression of emotions. Research Woo et al. (2020); Agnew and South (2014); Costa et al. (2013) indicates that exploring the causes of emotions is important in everyday life, as it can aid in promoting self-understanding. In recent years, researchers increasingly focus on how to explore the underlying causes of emotions. Consequently, a new task called Causal Emotion Entailment (CEE) Xia et al. (2019); Zheng et al. (2022) is proposed, along with the introduction of a benchmark dataset named RECCON Poria et al. (2021). The CEE task aims to identify the specific utterances that causally trigger the emotion expressed in the target utterance. Wang et al. further propose a new task, Multimodal Emotion-Cause Pair Extraction in Conversations (MECPE), which expands the research scope to multimodal content. However, the identification of emotional causes still relies on selective cause extraction rather than cause generation. Selective causes fail to consider emotional expressions that lack direct causes in conversations and require contextual reasoning for understanding; for those emotional utterances, if the causal relationships behind them are primarily conveyed through visual or auditory information, determining the specific causes that trigger emotions becomes quite challenging. The comparison between selective and generative is illustrated in Figure 1. To resolve these issues, we propose a novel task, called Multimodal Emotion Cause Explanation in Conversation (MECEC), which aims to uncover the underlying causes of emotions expressed in a target utterance within a multimodal conversation setting, encompassing text and visual modalities. We build an Emotion Cause Explanation (ECEM) based on the MELD dataset Poria et al. (2018) for MECEC. The ECEM dataset consists of 7,273 video clips, incorporating audio, visual and textual modalities, with each clip annotated with a thorough explanation of the emotional cause. To address the task of MECEC, we propose a new model, Facial-Aware Multimodal Emotion Explanation Network (FAME-Net), which is based on LLaVA Liu et al. (2023). FAME-Net integrates visual and facial emotion recognition to comprehensively understand multimodal emotional data, compensating for the limitations of text-only analysis and revealing deeper aspects of emotional contagion and hidden emotions. Facial expression recognition can assist in the task of discovering emotional causes; therefore, the model introduces a dedicated facial emotion recognition branch, designed as a two-stage pipeline. In the first stage, it performs scene and face detection to segment and track faces within videos; in the second stage, it employs multi-scale feature networks and global depth convolution techniques to accurately identify facial emotion features. Compared to other open-source LLMs, our model excels in text generation metrics, achieving the best results. Our main contributions are as follows: • We introduce a novel task named Multimodal Emotion Cause Explanation in Conversations (MECEC) along with a new dataset, Emotion Cause Explanation based on the MELD dataset (ECEM), which provides detailed natural language explanations for the emotions of target utterances and is specifically designed for training and evaluating the MECEC task. • We propose FAME-Net, which integrates visual modality and facial emotion recognition to achieve comprehensive analysis of emotional causes. Through a two-stage pipeline, it accurately extracts emotional features, enhancing the performance of multi-modal emotion causes explain and generation. • Experimental results on our annotated dataset indicate that our proposed model, FAME-Net, outperforms several existing open-source baseline models in the MECEC task."
https://arxiv.org/html/2411.02429v1,IdeaBench: Benchmarking Large Language Models for Research Idea Generation,"Large Language Models (LLMs) have transformed how people interact with artificial intelligence (AI) systems, achieving state-of-the-art results in various tasks, including scientific discovery and hypothesis generation. However, the lack of a comprehensive and systematic evaluation framework for generating research ideas using LLMs poses a significant obstacle to understanding and assessing their generative capabilities in scientific discovery. To address this gap, we propose IdeaBench, a benchmark system that includes a comprehensive dataset and an evaluation framework for standardizing the assessment of research idea generation using LLMs. Our dataset comprises titles and abstracts from a diverse range of influential papers, along with their referenced works. To emulate the human process of generating research ideas, we profile LLMs as domain-specific researchers and ground them in the same context considered by human researchers. This maximizes the utilization of the LLMs’ parametric knowledge to dynamically generate new research ideas. We also introduce an evaluation framework for assessing the quality of generated research ideas. Our evaluation framework is a two-stage process: first, using GPT-4o to rank ideas based on user-specified quality indicators such as novelty and feasibility, enabling scalable personalization; and second, calculating relative ranking based “Insight Score” to quantify the chosen quality indicator. The proposed benchmark system will be a valuable asset for the community to measure and compare different LLMs, ultimately advancing the automation of the scientific discovery process. Our code and dataset are available at: https://anonymous.4open.science/r/IdeaBench-2747/.","Recent years have witnessed the rapid development of Large Language Models (LLMs). LLMs like GPT-4 (OpenAI 2023) and LLama series (Touvron et al. 2023) introduced advanced capabilities that set them apart from previous generations of machine learning models. Among these capabilities, in-context learning allows LLMs to understand and respond to user prompts in a nuanced manner without requiring additional training for each specific task, enabling LLMs to generalize across a wide range of tasks, providing robust state-of-the-art performance even with limited data (Brown et al. 2020). As a result, LLMs have revolutionized the way humans interact with AI systems, making it possible to generate coherent text, translate languages, answer questions, and even compose creative content with unprecedented accuracy and fluency (Bubeck et al. 2023). The impact of these advancements extends beyond consumer applications, influencing various sophisticated domains such as education (Moore et al. 2023), healthcare (Yang et al. 2023a), and scientific research (Wysocki et al. 2024). Recently, the impressive performance of LLMs in everyday applications has sparked significant interest in academia, particularly for their potential use in scientific discovery or hypothesis generation (AI4Science and Quantum 2023). Several studies have explored leveraging LLMs to generate hypotheses or research ideas (Yang et al. 2023b; Wang et al. 2023b; Zhou et al. 2024; Baek et al. 2024; Qiu et al. 2023). However, despite numerous results, a unified and comprehensive framework for evaluating generated research ideas is still lacking, making it difficult for the community to clearly understand the performance spectrum of different techniques for generating research ideas. To address this limitation, we introduce a standardized evaluation framework designed to emulate how human researchers generate research ideas. This framework, termed IdeaBench, comprises three main components: dataset construction, research idea generation, and a novel metric to evaluate the quality of the generated research ideas. The intuition behind this framework is grounded in the typical research process of how researchers generate new scientific research ideas as described below: 1. Targeting a specific topic. 2. Reviewing related literature, focusing on recent findings and methodologies. 3. Identifying gaps in knowledge or methods within these recent findings. 4. Proposing research ideas to address these gaps. We first construct a benchmark dataset that includes meticulously filtered 2,374 target papers’ abstracts from biomedical research fields. These target papers serve as the ground-truth sources of research ideas. Additionally, the dataset contains the abstracts of the papers referenced by the target papers, providing the context necessary for LLMs to generate relevant research ideas. This comprehensive dataset aims to capture the complexity and specificity of scientific research, particularly in the biomedical domain, thus offering a solid foundation for evaluating LLMs’ capability in generating research ideas. Based on the benchmark dataset, we design a prompt template that leverages LLMs to generate research ideas. In addition to grounding the context for idea generation using reference papers from our dataset, we also profile the LLMs as domain-specific researchers in the prompt. This approach aims to dynamically maximize the utilization of the LLMs’ parametric knowledge, enabling the generation of more in-depth and insightful research ideas. It can also be used as a baseline for future comparisons. To accurately assess the quality of generated research ideas, we design an evaluation framework which incorporates two critical components: personalized quality ranking and relative quality scoring. This dual approach allows for a nuanced assessment that takes into account user-defined quality indicators such as novelty, feasibility, etc. Our design ensures a versatile and comprehensive evaluation framework, capable of adapting to different research contexts and providing meaningful insights into the quality of LLM-generated ideas. Our results show that recent high-capacity LLMs are capable of generating research ideas using IdeaBench dataset, and our metric is able to assess the quality of generated research ideas from different dimensions. We hope this work inspires academia to further unleash the potential of LLMs in supporting research ideation, ultimately accelerating scientific discovery in the future. To summarize, our contributions are as follows: • We construct IdeaBench dataset, which consists of 2,374 influential biomedical target papers along with their 29,408 reference papers, to evaluate LLMs’ capabilities in generating research ideas. • We propose an evaluation framework which offers a scalable and versatile metric called “Insight Score”, which can quantify novelty, feasibility, or any other quality indicators defined by human researchers. • We conduct extensive experiments to demonstrate several LLMs’ abilities in generating research ideas based on our dataset and evaluation metric."
https://arxiv.org/html/2411.03314v1,MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning,"In recent years, multimodal benchmarks for general domains have guided the rapid development of multimodal models on general tasks. However, the financial field has its peculiarities. It features unique graphical images (e.g., candlestick charts, technical indicator charts) and possesses a wealth of specialized financial knowledge (e.g., futures, turnover rate). Therefore, benchmarks from general fields often fail to measure the performance of multimodal models in the financial domain, and thus cannot effectively guide the rapid development of large financial models. To promote the development of large financial multimodal models, we propose MME-Finance, an bilingual open-ended and practical usage-oriented Visual Question Answering (VQA) benchmark. The characteristics of our benchmark are finance and expertise, which include constructing charts that reflect the actual usage needs of users (e.g., computer screenshots and mobile photography), creating questions according to the preferences in financial domain inquiries, and annotating questions by experts with 10+ years of experience in the financial industry. Additionally, we have developed a custom-designed financial evaluation system in which visual information is first introduced in the multi-modal evaluation process. Extensive experimental evaluations of 19 mainstream MLLMs are conducted to test their perception, reasoning, and cognition capabilities. The results indicate that models performing well on general benchmarks cannot do well on MME-Finance; for instance, the top-performing open-source and closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively. Their performance is particularly poor in categories most relevant to finance, such as candlestick charts and technical indicator charts. In addition, we propose a Chinese version, which helps compare performance of MLLMs under a Chinese context. Therefore, we hope to open-source our benchmark to foster the development of multimodal models in the financial domain. The code and data will be released at https://hithink-research.github.io/MME-Finance.","Multimodal Large Language Models (MLLMs) [45], which equip the Large Language Models (LLMs) [34, 32, 12, 39] with the capability of visual understanding, have experienced a revolutionary advancement recently. Works including Flamingo [3], LLaVA [27], CogVLM [41], Gemini [37], and GPT-4o [31] have demonstrated intriguing capability to solve complex multimodal recognition and reasoning tasks. A reasonable and objective benchmark is of enormous significance in the success of MLLMs, which not only helps a better comparison of the performances of MLLMs but also provides valuable guidance for model optimization and real-world applications. Early works of multimodal benchmarks, such as COCO Caption [8], GQA [19], and Flickr30k [47], have served as foundational resources for evaluating MLLMs. However, these benchmarks are task-specific, limiting the scope for fine-grained analysis of MLLMs’ capabilities. More recent efforts, including MME series [13, 14, 51], MMBench [28], and MM-Vet [48], have shifted focus towards general multimodal tasks. These benchmarks comprehensively evaluate diverse capabilities of MLLMs, such as perception and reasoning, through a broader range of tasks. Alongside these general-purpose benchmarks, domain-specific benchmarks are rapidly emerging. For instance, in the medical field, benchmarks like GMAI-MMBench [7] and Asclepius [42] have been developed, while in the autonomous driving domain, NuScenes-QA [33] and DriveLM-DATA [36] are advancing research. These benchmarks significantly accelerated the progress of MLLMs within respective industries. In the financial field, understanding charts presents more unique challenges. (1) Jargon: Financial charts are filled with technical terms such as “bullish”, “bearish”, “support levels”, and “resistance levels”, which may be hard to grasp. (2) Complexity: Financial charts often contain a vast amount of data and information, such as the open, close, high, and low prices on a candlestick chart, along with various technical indicators and oscillators. (3) Diversity of Chart Types: There are multiple types of charts in the financial domain, such as line charts, bar charts, and candlestick charts, each with its specific use cases and interpretation methods. (4) Data Density: Financial charts may include a large number of data points, making it more difficult to identify trends and patterns. Therefore, it is challenging to comprehensively and professionally evaluate the financial capability of MLLMs. Benchmarks like FINANCEBENCH [20] and CFBenchmark [21] are focusing on the evaluation of LLMs. To the best of our knowledge, there is no multimodal benchmark in the financial area, and a significant dearth of Chinese multimodal benchmarks. Hence, a bilingual financial multimodal benchmark is urgent for promoting the development of MLLMs. To break this gap, we propose MME-Finance, a bilingual financial multimodal benchmark for MLLMs. We conduct extensive research on real-world financial application scenarios and select 6 common types of financial charts, including candlestick charts, technical indicator charts, statistical charts, tables, documents, and mixed charts. Based on these images and the actual usage of users in financial scenarios, we design a hierarchical series of open-ended Question Answering (QA) tasks, ranging from general visual perception like Optical Character Recognition (OCR) tasks to complex cognitive tasks such as providing investment advice. To ensure the quality of MME-Finance, we carefully design the annotation pipeline and invite experts with 10+ years of experience in the financial industry to conduct detailed verification of the answers. LLMs and MLLMs are employed for automated evaluation in MME-Finance. Considering the challenges of evaluating financial open-ended questions, we meticulously design the evaluation process and first introduce visual information to boost the evaluation performance. The effectiveness of our evaluation method has been validated through human consistency experiments. Extensive experiments indicate existing MLLMs remain inadequate in meeting the requirements of financial tasks, where the best open-source and closed-source models have scored unsatisfactorily, with only 65.69% (Qwen2VL-72B) and 63.18% (GPT-4o), respectively. Particularly, there are three points worthy of our attention: The first point is that models encounter difficulty in some tasks, especially spatial awareness and estimated numerical calculation. The second point is that the performance related to stock charts is not good (e.g., candlestick charts and technical indicator charts), and the last is that MLLMs generally perform poorly in questions about mobile photography, which however is a relatively high-frequency use case in financial QA. We summarize our major contributions as follows: • We propose MME-Finance, a novel bilingual multimodal benchmark specifically designed to evaluate the capabilities of MLLMs in the financial domain. It comprises 1,171 English and 1,103 Chinese questions, covering diverse financial image types and various multimodal capabilities, and providing a comprehensive evaluation of MLLMs’ performance in the financial domain. • We introduce an evaluation approach of open-ended questions in the financial domain. By designing appropriate prompts for corresponding tasks and exploring evaluation methods firstly combined with image information, we propose a novel evaluation strategy that has a high consistency with humans. The strategy can serve as a reference for evaluating MLLMs for other works. • We conduct extensive evaluation on 19 MLLMs based on MME-Finance, revealing critical insights about the strengths and shortcomings of the current MLLMs in financial applications. The insights gained from this study provide a foundation for future research, guiding the development of more robust MLLMs capable of meeting the demands of complex financial tasks."
https://arxiv.org/html/2411.03250v1,DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models,"Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs’ limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM’s generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE’s latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2%–7% in certain cases. The data and code will be publicly available upon completion of internal review.","Data Synthesis has become an indispensable technique in current machine learning research, enabling rapid generation and modification of datasets (Bauer et al., 2024), allowing researchers to experiment with various scenarios and model architectures without the extensive processes associated with real-world data collection. Meanwhile, with the rapid advancements in large language models (LLMs), recent research in natural language processing (NLP) has increasingly focused on leveraging LLMs for synthetic data generation. Early efforts attempted to fine-tune LLMs to align with real data distributions (Keskar et al., 2019; Anaby-Tavor et al., 2020; Borisov et al., 2023). As the in-context learning capabilities of LLMs have improved, some studies have explored zero-shot or few-shot prompting of LLMs to generate synthetic data (Ye et al., 2022a; Wei et al., 2024). Despite the progress achieved, generating high-quality synthetic textual data using LLMs remains challenging, particularly for structured data (Josifoski et al., 2023; Li et al., 2022). First, LLMs often lack a global understanding of the target data distribution when generating synthetic data. Even after fine-tuning, it is difficult to inject information about complex and varied distributions into current LLM architectures, often resulting in outputs with low diversity and instances of data copying (Wu et al., 2024; Yu et al., 2023). Moreover, existing LLM-based synthetic data generation methods typically involve complex pipelines and post-processing mechanisms, such as prompt engineering, multi-agent frameworks, and iterative sampling (Dekoninck et al., 2024; Wu et al., 2024). These complexities hinder the rapid adaptation of LLMs to new tasks, limiting their utility in dynamic research and industrial scenario. Concurrently, the remarkable performance of variational autoencoders (VAEs) and diffusion models in image synthesis tasks (Betker et al., 2023; Rombach et al., 2022) has spurred interest in adapting these techniques to other modalities (Borisov et al., 2023; Li et al., 2022; Gong et al., 2023). Although some works have introduced latent spaces into language models for simple tasks like style transfer or topic generation (Yang & Klein, 2021; Li et al., 2022), our preliminary experiments indicate that directly applying the latent distributions learned by VAEs often results in outputs that are unrelated to the real data. This challenges the direct application of these methods in more complex scenarios for synthetic data generation. To address these challenges, we propose DiffLM, a novel framework that leverages a plug-and-play latent space to provide data distribution information for LLMs during data generation. First, to decouple the learning of real data distributions from the LLM’s training objectives, we develop a latent space using a VAE to capture external information, mapping samples from the real dataset to latent vectors. However, we observed that simply sampling from a Gaussian distribution obtained from naive VAE that cannot generate realistic results. To overcome the poor quality of data generated by sampling from VAE, we employ a latent diffusion method that linearly adds noise to the latent space over time. A denoising network is then trained to learn these noises in the reverse process, reducing efficiency loss in data synthesis due to sampling failures. Finally, we design a soft prompting method to inject latent features into the LLM decoding process, resulting in controllable, high-quality synthetic data. We evaluate our method on seven real-world structured formatted datasets, ranging from simple table synthesis to more complex code and tool synthesis tasks. Experiments demonstrate that DiffLM can generate realistic results, and ablation studies confirm the effectiveness of each component in our proposed method. The contributions of this paper are threefold: • Decoupling Data Distribution Learning: We proposed a new VAE-based LLM framework for data systhesis, which decouples the learning of real data distribution information from the training objectives of the LLM by introducing the a small projection network. • High-Quality Synthetic Data Generation: Based on our observations, the meticulously designed VAE and diffusion structures effectively model the distribution of real data, enabling the generation of high-quality synthetic data. In all tasks, the quality of the generated data is comparable to or even surpasses that of the real data. • Comprehensive Evaluation: We validate the high quality of data generated by DiffLM across three distinct scenarios and seven datasets, underscoring its robustness and adaptability in advancing synthetic data generation for natural language processing."
https://arxiv.org/html/2411.02948v1,Grounding Natural Language to SQL Translation with Data-Based Self-Explanations,"Natural Language Interfaces for Databases empower non-technical users to interact with data using natural language (NL). Advanced approaches, utilizing either neural sequence-to-sequence or more recent sophisticated large-scale language models, typically implement NL to SQL (NL2SQL) translation in an end-to-end fashion. However, like humans, these end-to-end translation models may not always generate the best SQL output on their first try. In this paper, we propose Cyclesql, an iterative framework designed for end-to-end translation models to autonomously generate the best output through self-evaluation. The main idea of Cyclesql is to introduce data-grounded NL explanations of query results as self-provided feedback, and use the feedback to validate the correctness of the translation iteratively, hence improving the overall translation accuracy. Extensive experiments, including quantitative and qualitative evaluations, are conducted to study Cyclesql by applying it to seven existing translation models on five widely used benchmarks. The results show that 1) the feedback loop introduced in Cyclesql can consistently improve the performance of existing models, and in particular, by applying Cyclesql to Resdsql, obtains a translation accuracy of 82.0% (+2.6%) on the validation set, and 81.6% (+3.2%) on the test set of Spider benchmark; 2) the generated NL explanations can also provide insightful information for users, aiding in the comprehension of translation results and consequently enhancing the interpretability of NL2SQL translation111Our code is available at https://github.com/Kaimary/CycleSQL..","Natural language interfaces for databases (NLIDBs) [1, 2, 3] democratize data exploration by allowing users to interact with databases using natural language (NL), breaking down barriers to information retrieval and data analysis. Consequently, the development of NLIDBs has garnered significant attention from both the data management and natural language processing (NLP) communities since the late 1970s [4, 5, 6, 7]. In light of recent advancements in machine learning, the central focus of ongoing efforts to develop NLIDBs centers around elevating the accuracy of translating NL to SQL (NL2SQL) [8, 9]. This objective is primarily accomplished via direct NL2SQL translation in an end-to-end manner, either by adopting sequence-to-sequence (Seq2seq) models trained on annotated data [10, 11, 12, 13, 14, 15, 16, 17] or, more recently, by harnessing large-scale language models (LLMs) that push the boundaries of the field even further within the last two years [18, 19, 20, 21, 22]. 111122223333444455550.70.70.70.70.750.750.750.750.80.80.80.80.850.850.850.85Beam Size (Number of Chat Completion Choices)Translation AccuracyPicard (t5-3b)Resdsql (t5-large)Gpt-3.5-turboDailsql (chatgpt) Figure 1: Translation accuracy on Spider validation set with varied beam sizes (or chat completion choices). Accuracy for beam sizes (or chat completions) >>> 1 is evaluated by matching any beam result. Challenges. While significant advancements in enhancing overall accuracy, current end-to-end models face persistent challenges in producing desired quality output during their initial attempt, owing to the treatment of language translation as a “one-time deal”. Figure 1 shows the translation accuracy (defined as query execution result equivalence) on the Spider [7] benchmark, with varied beam outputs for two Seq2seq-based models222Most existing Seq2seq-based NL2SQL translation models utilize beam search decoding method to maintain a list of top-k best outcomes. (i.e., Picard [16], and Resdsql [17]) or diverse chat completion choices for two LLM-based models333LLMs often use a specific parameter to generate various responses. Refer to https://platform.openai.com/docs/api-reference/chat/create#chat-create-n. (i.e., Gpt-3.5-turbo and the state-of-the-art Dailsql model [22]). As indicated by the plateauing accuracy (below 80%) observed when the beam size or the number of chat completions is set to 1111, they may fail to generate best-quality translations in their initial attempts. However, expanding the search space through wider beams or more chat completions steadily improves accuracy, without necessitating modifications to the underlying model architectures. This example shows that end-to-end models may benefit from broader exploration options to enhance translation quality over successive attempts. aid flno origin destination 9 2 Los Angeles Tokyo 3 7 Los Angeles Sydney 3 13 Los Angeles Chicago 10 68 Chicago New York 9 76 Chicago Los Angeles 7 33 Los Angeles Honolulu 5 34 Los Angeles Honolulu 1 99 Los Angeles Washington D.C. 2 346 Los Angeles Dallas 6 387 Los Angeles Boston aid name distance 1 Boeing 747-400 8430 2 Boeing 737-800 3383 3 Airbus A340-300 7120 4 British Aerospace Jetstream 41 1502 5 Embraer ERJ-145 1530 6 SAAB 340 2128 7 Piper Archer III 520 8 Tupolev 154 4103 9 Lockheed L1011 6900 10 Boeing 757-300 4010 Figure 2: An example with simplified database (Flight (left) and Aircraft (right) tables) from the Spider benchmark. Inspired by the feedback mechanisms [23, 24] used in modern recommendation systems and iterative refinement methods [25, 26] introduced in LLMs, we present Cyclesql, an iterative framework built upon self-provided feedback to enhance translation accuracy of existing end-to-end models. Diverging from the traditional end-to-end translation paradigm, we introduce data-grounded NL explanations of query results as a form of internal feedback to create a self-contained feedback loop within the end-to-end process, facilitating iterative self-evaluation of translation correctness. \twemoji [height=1.0em]thinking faceSQL2NL back-translation for Self-Explanations? A natural way to generate self-provided feedback (explanations) in NL2SQL translation may involve its reverse process, namely SQL2NL. This entails establishing an NL-to-SQL-to-NL translation lifecycle, a concept analogous to back-translation [27, 28]. Several studies have explored this technique [29, 30, 31, 32] to refine initial SQL outputs, but we posit that solely using this “simple” back-translation to generate feedback may be limited, as it lacks additional contextual information beyond the NL and SQL components. Consider an intuitive example in Figure 2, which shows an NL query alongside a translation from an existing model. By simply using the SQL2NL technique, however, incorrect “positive” feedback is generated: the meaning of the explanation seems to align with the initial NL query, whereas the underlying SQL query is deemed incorrect (i.e., an incorrect aggregation function is used in the SELECT clause). Thus, such a back-translated explanation may not adequately serve as valid feedback for the NL2SQL process. Our Methodology. Cyclesql proposes a plug-and-play framework that facilitates seamless integration with existing end-to-end NL2SQL models. By integrating with a conventional NL2SQL process, Cyclesql 1 first rewrites the translated SQL query to retrieve provenance information from the underlying database for a specific query result. 2 Following this, the provenance is enriched by annotating the semantics of the translated SQL query to its different parts. Next, 3 a data-grounded NL explanation is interpreted from the enriched provenance to rationalize the query result by leveraging text generation techniques [33, 34, 35, 26]. Here, given that the NL explanation integrates both data-level (provenance) and operation-level (query) semantics, they possess significant potential to function as self-provided feedback for various underlying NL2SQL translation models. Finally, 4 the generated NL explanation is used to validate the correctness of the underlying translation iteratively, ultimately yielding a more reliable translation outcome. To assess the effectiveness of Cyclesql, we conduct a comprehensive experimental evaluation on the widely-used benchmark Spider, and its three variants, namely Spider-realistic [36], Spider-syn [37], and Spider-dk [38], as well as ScienceBenchmark [30], by applying Cyclesql to six contemporary end-to-end NL2SQL models. The results show that Cyclesql can consistently enhance the translation accuracy of all the models. Notably, by applying Cyclesql to Resdsql (with T5-3b scale), Cyclesql obtains a translation accuracy of 82.0% on the validation set and 81.6% on the test set of Spider benchmark, achieving best-reported result among the leading Seq2seq-based models on Spider leaderboard444https://yale-lily.github.io/spider. Moreover, a qualitative evaluation, including a case study and user study, is conducted to show that the explanations generated by Cyclesql can also greatly improve user interpretability in the black-box NL2SQL process. Contributions. We make the following three contributions: (1) Feedback Loop in NL2SQL. We propose, Cyclesql, a plug-and-play framework to establish a self-contained feedback loop within the NL2SQL process. Cyclesql employs data-grounded NL explanations as reliable feedback to iteratively validate the correctness of the translation, thereby enhancing the overall accuracy. (2) Rich Explanations for NL2SQL. The NL explanations generated by Cyclesql, incorporating not only the semantics from the query surface but also the semantics from the data instance, provide insightful information for users to understand the black-box NL2SQL translation process. (3) Quantitative and Qualitative Evaluations. We evaluate Cyclesql on five public benchmarks with seven NL2SQL models, demonstrating its substantial impact on performance improvements. Furthermore, a qualitative evaluation is conducted to gauge the utility of the generated NL explanations in enhancing user interpretability. Figure 3: Overview framework of Cyclesql"
https://arxiv.org/html/2411.02902v1,Membership Inference Attacks againstLarge Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.","The rise of large language models (LLMs) [9, 60, 45, 11] has inspired the exploration of large models across multi-modal domains, exemplified by advancements like GPT-4 [1] and Gemini [59]. These large vision-language models (VLLMs) have shown promising ability in various multi-modal tasks, such as image captioning [33], image question answering [13, 35], and image knowledge extraction [26]. However, the rapid advancement of VLLMs also causes user concerns about privacy and knowledge leakage. For instance, the image data used during commercial model training may contain private photographs or medical diagnostic records. This is concerning since early work has demonstrated that machine learning models can memorize and leak training data [3, 56, 63]. To mitigate such concerns, it is essential to consider the membership inference attack (MIA) [23, 53], where attackers seek to detect whether a particular data record is part of the training dataset [23, 53]. The study of MIAs plays an important role in preventing test data contamination and protecting data security, which is of great interest to both industry and academia [24, 19, 44]. When exploring MIAs in VLLMs, one main issue is the absence of a standardized dataset designed to develop and evaluate different MIA methods, which comes from the large size [16] and multi-modality of the training data, and the diverse VLLMs training pipelines [66, 35, 18]. Therefore, one of the main goals of this work is to build an MIA benchmark tailored for VLLMs. Beyond the need for a valid benchmark, we lack efficient techniques to detect a single modality in VLLMs. The closest work to ours is [30], which performs MIAs on multi-modal CLIP [46] by detecting whether an image-text pair is in the training set. However, in practice, it is more common to detect a single modality, as we care whether an individual image or text is in the training set. Therefore, we aim to develop a pipeline to detect the single modality from a multi-modal model. Moreover, existing literature on language model MIAs, such as Min-K% [52] and Perplexity [62], mostly are target-based MIAs, which use the next token as the target to compute the prediction probability. However, we can only access the image embedding instead of the image token in VLLMs, and thus only target-free MIAs [48] can be directly applied. Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxRényi-K% metric: we first get the Rényi entropy of each token position, then select the largest k%percent𝑘k\%italic_k % tokens and calculate the average Rényi entropy. Therefore, we first propose a cross-modal pipeline for individual image or description MIAs on VLLMs, which is distinguished from traditional MIAs that only use one modality [61, 62]. We feed the VLLMs with a customized image-instruction pair from the target image or description. We show that we can perform the MIA not only by the image slice but also by the instruction and description slices of the VLLM’s output logits, see Figure 1. Such a cross-modal pipeline enables the usage of text MIA methods on image MIAs. We also introduce a target-free metric that adapts to both image and text MIAs and can be further modified to a target-based way. Overall, the contributions and insights can be summarized as follows. • We release the first benchmark tailored for the detection of training data in VLLMs, called Vision Language MIA (VL-MIA) (Section 4). By leveraging Flickr and GPT-4, we construct VL-MIA that contains two images MIA tasks and one text MIA task for various VLLMs, including MiniGPT-4 [66], LLaVA 1.5 [35] and LLaMA-Adapter V2 [18]. • We perform the first individual image or description MIAs on VLLMs in a cross-modal manner. Specifically, we demonstrate that we can perform image MIAs by computing statistics from the image or text slices of the VLLM’s output logits (Figures 1 and 5.1). • We propose a target-free MIA metric, MaxRényi-K%, and its modified target-based ModRényi (Section 5.2). We demonstrate their effectiveness on open-source VLLMs and closed-source GPT-4 (Section 6). We achieve an AUC of 0.815 on GPT-4 in image MIAs."
https://arxiv.org/html/2411.02851v1,"Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization","The goal of Multilingual Visual Answer Localization (MVAL) is to locate a video segment that answers a given multilingual question. Existing methods either focus solely on visual modality or integrate visual and subtitle modalities. However, these methods neglect the audio modality in videos, consequently leading to incomplete input information and poor performance in the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method that incorporates audio modality to augment both visual and textual representations for the MVAL task. Specifically, we integrate features from three modalities and develop three predictors, each tailored to the unique contributions of the fused modalities: an audio-visual predictor, a visual predictor, and a textual predictor. Each predictor generates predictions based on its respective modality. To maintain consistency across the predicted results, we introduce an Audio-Visual-Textual Consistency module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing each modality’s predictor to dynamically learn from the others. This collaborative learning ensures that the model generates consistent and comprehensive answers. Extensive experiments show that our proposed method outperforms several state-of-the-art (SOTA) methods, which demonstrates the effectiveness of the audio modality.","With the rapid expansion of the internet, an increasing number of users are turning to online platforms to seek medical advice by posing natural language questions (O’Donnell et al., 2023; Lim et al., 2022). Current online platforms typically fall into two categories: those that provide textual answers, which may be difficult for users to interpret, and those that offer visual answers, which are generally more intuitive and easier to follow (Tang et al., 2021b). However, the retrieved videos often contain substantial amounts of information irrelevant to the user’s query (Moon et al., 2023), which significantly hinders the efficiency of information retrieval (Zhang et al., 2023). In response to this challenge, the task of Visual Answer Localization (VAL) has been introduced (Weng and Li, 2023). Figure 1: (a) Overview of the audio-enhanced multilingual video answer localization task. (b) Difference between existing methods and our method. (c) Performance comparison diagram of visual-based, textual-based, and our method. Existing VAL approaches can be broadly categorized into visual-based (Tang et al., 2021a; Chen et al., 2020a) and textual-based methods (Li et al., 2023a; Weng and Li, 2023; Li et al., 2024b). Visual-based methods are effective in scenarios where subtitle text is sparse, but their performance tends to degrade significantly in other contexts. In contrast, textual-based methods excel when abundant subtitle text is available, as the semantic similarity between the question and subtitle is typically greater than between the question and the video (Li et al., 2024b). However, these methods often overlook audio, which plays a crucial role in complementing both visual and textual modalities. There is inherent consistency and complementarity among these modalities (Chen et al., 2023a), and harnessing this synergy can enhance both visual and textual modalities by integrating information from the audio. Incorporating audio thus addresses the performance limitations in VAL, particularly in video segments lacking subtitles (Liu et al., 2022; Chen et al., 2020b; Sun et al., 2024). To this end, we study the Audio-enhanced Multilingual Visual Answer Localization (AMVAL) which aims to locate video segments that answer a user’s natural language question, in either Chinese or English. By providing video segments with verbal explanations for medical guidance, this approach not only facilitates the learning of specific actions but also helps bridge language barriers, making the content accessible to people who speak different languages (Macedonia and Knösche, 2011; Diamond et al., 2020). However, a significant challenge lies in effectively integrating the three modalities and fully utilizing their individual strengths to tackle the AMVAL task. To address this challenge, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method for AMVAL, aimed at reducing cross-modal discrepancies and improving the accuracy of span localization by integrating audio modality. We designed a network architecture with three modality channels (audio, visual, and textual) to fully leverage the semantic information from each modality in the video, addressing the limitations of single-modality methods. Each channel is equipped with a corresponding predictor: an audio-visual predictor, a visual predictor, and a textual predictor. During joint training, distinct objectives are assigned to each predictor, enabling them to leverage the unique strengths of their respective modalities. To improve modality integration, we introduce an Audio-Visual-Textual consistency module, which employs a Dynamic Triangular Loss (DTL) function based on Intersection over Union (IoU). This loss function aligns the modalities by minimizing the discrepancies between each predictor’s output and the target answer, as well as between the outputs of the other two predictors. Our approach promotes mutual learning among the predictors to achieve consistent and cohesive multimodal representations. Our contributions are as follows: (1) We study the AMVAL and propose the AVTSL method, which is the first to introduce the audio modality for the AMVAL; (2) We designed an Audio-Visual-Text Consistency module, which leverages the consistency and complementarity between different modalities using the DTL loss function; (3) We conducted extensive experiments to demonstrate the effectiveness of the AVTSL method, where our method outperformed other state-of-the-art (SOTA) methods by incorporating the audio modality."
https://arxiv.org/html/2411.02790v1,Memory Augmented Cross-encoders for ControllablePersonalized Search,"Personalized search represents a problem where retrieval models condition on historical user interaction data in order to improve retrieval results. However, personalization is commonly perceived as opaque and not amenable to control by users. Further, personalization necessarily limits the space of items that users are exposed to. Therefore, prior work notes a tension between personalization and users’ ability for discovering novel items. While discovery of novel items in personalization setups may be resolved through search result diversification, these approaches do little to allow user control over personalization. Therefore, in this paper, we introduce an approach for controllable personalized search. Our model, CtrlCE presents a novel cross-encoder model augmented with an editable memory constructed from users historical items. Our proposed memory augmentation allows cross-encoder models to condition on large amounts of historical user data and supports interaction from users permitting control over personalization. Further, controllable personalization for search must account for queries which don’t require personalization, and in turn user control. For this, we introduce a calibrated mixing model which determines when personalization is necessary. This allows system designers using CtrlCE to only obtain user input for control when necessary. In multiple datasets of personalized search, we show CtrlCE to result in effective personalization as well as fulfill various key goals for controllable personalized search.","Personalized search powers several industry scale search systems for products (Aslanyan et al., 2020; Yu et al., 2021), movies (Ostuni et al., 2023), jobs (Ha-Thuc and Sinha, 2016), and web-search more broadly (Horling and Kulick, 2009). While personalization in search systems commonly increases the uptake of systems by users and improves the relevance of search results, personalization is commonly seen as opaque and failing to provide users with sufficient control over predictions (Eiband et al., 2019; Konstan and Terveen, 2021). Further, personalized ranking is more likely to prevent users from seeing the breadth of information in a document collection. Here, Chien and Danks (2023) note the tension between personalization and epistemic fairness, which ensures that all users are similarly exposed to the space of information. Similarly, Salehi et al. (2015) note that search personalization may prevent serendipitous discovery and exploration in important applications such as education. Such concerns about personalization have been addressed through two avenues. The limits to information diversity have been addressed through diversification of personalized search results (Radlinski and Dumais, 2006; Vallet and Castells, 2012). While this improves result diversity, it does little to improve user control or facilitate user-driven interaction and discovery (Ruotsalo et al., 2018). On the other hand, a small body of work has also provided users interactive control over personalized search through editable user profiles (Ahn et al., 2015; Zemede and Gao, 2017). However, this line of work has been limited to exploring simple term or entity based user profiles and has focussed on designing visualization interfaces for user control. Notably, while prior work on controllable personalization for search tasks has been limited, a larger line of work has explored control for personalized recommendation tasks (Balog et al., 2019; Konstan and Terveen, 2021). This work has shown control over personalized recommendations to improve user satisfaction and trust (Jin et al., 2017; Jannach et al., 2016). In this paper, we take inspiration from this work and extend the body of work on controllable personalized search. We introduce, CtrlCE, a controllable cross-encoder model personalized with a editable user profile for personalized search tasks. We outline the following goals for controllable personalized search: (1) to allow control, a user profile used for personalization must be transparent to users, (2) users should be able to express preferences through intuitive edits made to their user profile. Further, since search can be performed without any personalization a controllable model should make good predictions without personalization, (3) since only some queries are likely to require personalization (Ai et al., 2019; Teevan et al., 2008) a controllable model should highlight queries for which user profile edits would be meaningful, and (4) controllable personalization should not come at the expense of degraded performance. Specifically, our model to fulfill these goals is a cross-encoder augmented with a user profile realized as an editable memory constructed from historical user documents. For our editable user memory we explore two multi-vector user representations, an item level representation and a novel concept based representation introduced in recent work on controllable recommendation (Mysore et al., 2023). Both representations remain transparent to users, leverage the rich content of historical user documents, and support a intuitive set of user edits. Further, to ensure that user edits are only solicited when necessary, CtrlCE represents document relevance as a non-personalized query-document and a personalized user-document score. Then, we train a novel calibrated mixing model that in addition to intelligently combining these scores, also predicts which queries are likely to benefit from user edits to their profile (Figure 1). In experiments, we show that on four datasets of personalized search drawn from multiple domains CtrlCE outperforms standard personalization approaches based on dense retrieval as well as ensemble models, and non-personalized approaches spanning sparse, dense, and cross-encoder retrievers/re-rankers by 6.4-10.6% across evaluation metrics. Following this, we demonstrate that CtrlCE fulfills the goals of controllability through simulations and calibration evaluations. Finally, we present a case study showcasing the queries that CtrlCE highlights for user profile edits. To the best of our knowledge CtrlCE represents the first approach to controllable personalized search with performant cross-encoder models and extends the under-explored area of controllable personalized search. Code for CtrlCE will be released upon acceptance."
https://arxiv.org/html/2411.02704v1,RT-Affordance: Affordances are VersatileIntermediate Representations for Robot Manipulation,"We explore how intermediate policy representations can facilitate generalization by providing guidance on how to perform manipulation tasks. Existing representations such as language, goal images, and trajectory sketches have been shown to be helpful, but these representations either do not provide enough context or provide over-specified context that yields less robust policies. We propose conditioning policies on affordances, which capture the pose of the robot at key stages of the task. Affordances offer expressive yet lightweight abstractions, are easy for users to specify, and facilitate efficient learning by transferring knowledge from large internet datasets. Our method, RT-Affordance, is a hierarchical model that first proposes an affordance plan given the task language, and then conditions the policy on this affordance plan to perform manipulation. Our model can flexibly bridge heterogeneous sources of supervision including large web datasets and robot trajectories. We additionally train our model on cheap-to-collect in-domain affordance images, allowing us to learn new tasks without collecting any additional costly robot trajectories. We show on a diverse set of novel tasks how RT-Affordance exceeds the performance of existing methods by over 50%, and we empirically demonstrate that affordances are robust to novel settings. Videos available at https://snasiriany.me/rt-affordance","I INTRODUCTION In recent years, we have seen the rise of large pretrained models for learning robot policies. Vision-language-action (VLA) models [rt22023arxiv, kim24openvla], pretrained with large-scale robot data on top of vision-language models (VLMs) [geminiteam2024gemini] come with the promise of generalization to new objects, scenes, and tasks. However, VLAs are not yet reliable enough to be deployed outside of the narrow lab settings on which they are trained. While these shortcomings can be mitigated by expanding the scope and diversity of robot datasets, this is highly resource intensive and challenging to scale. Alternatively, there are various ways of interfacing with the policy that can potentially facilitate generalization by providing useful guidance on how to perform manipulation tasks. Examples of these policy representations include language specifications [zhang2024sprint, belkhale2024rth], goal images [black2023susie], goal sketches [sundaresan2024rtsketch], and trajectory sketches [gu2023rttrajectory]. These interfaces introduce mid-level abstractions that shield the policy from reasoning in a higher dimensional input space — leading to policies that can generalize over these intermediate representations. While one of the most common policy representations is conditioning on language, in practice most robot datasets are labeled with underspecified descriptions of the task and language conditioning does not reveal enough guidance on how to perform the task. Alternatively, goal image-conditioned policies provide detailed spatial context about the final goal configuration of the scene. However, goal-images are high-dimensional, which presents learning challenges due to over-specification issues [sundaresan2024rtsketch, shah2023mutex]. Furthermore, providing goal images at evaluation time is cumbersome for human users. This has lead to exploration of other intermediate representations — trajectory or goal sketches [gu2023rttrajectory, sundaresan2024rtsketch], or keypoints [yuan2024robopoint, fangandliu2024moka] — that attempt to provide spatial plans for the policy. While these spatial plans are informative, they still lack sufficient information for the policy on how to manipulate — e.g. what pose of the gripper should take when picking up a clothes hanger. Figure 1: Bridging robot and internet data via affordances. Prior work has shown the utility of co-training on robot and web datasets. However, robot actions and web content are still disjoint in their structure. We propose using affordances as a means to bridge this gap. Reasoning about affordances requires semantic and spatial reasoning, which is readily needed in VQA and spatial reasoning tasks such as object detection. By incorporating affordance reasoning explicitly in robot control tasks, we can better transfer knowledge from these web datasets to robot control tasks. Figure 2: Comparison of policy interfaces. Conditioning on language is intuitive, yet language typically does not provide enough guidance on how to perform the task. Goal images and trajectory sketches are typically over-specified and present learning challenges. We propose conditioning policies on intermediate affordance representations, which are expressive yet compact representations of tasks, making them easy to specify and to learn. In this work, we seek a policy representation that provides expressive yet lightweight abstractions for learning robust manipulation polices. We propose RT-Affordance, which is a policy conditioned on both language specifications and visual affordances. The visual affordances show the pose of the robot end effector at key stages of the task, visually projected onto the image input of the policy. By conditioning on affordances, the robot will have access to precise yet concise guidance on how to manipulate objects. To allow a seamless experience for the human user, we employ a hierarchical model that only requires task language from the user. The model first predicts the affordances given a task specification in language, and then leverages the affordances as an intermediate representation to steer the policy. The initial affordance prediction module can be trained on existing robot trajectories and web-scale datasets labeled with spatial information and affordances [Ego4D2022CVPR] (see Figure 1). We further enhance capabilities by training on a modest dataset of cheap-to-collect in-domain images annotated with affordances. This allows us to bypass costly robot teleoperation and learn novel tasks more scalably. We perform extensive experiments, where we show that RT-Affordance is effective across a broad range of real world tasks, achieving 69% overall success rate compared to 15% success rate for language-conditioned policies. We show how incorporating both web data and cheap-to-collect affordance images allows us to learn novel tasks without collecting any additional robot demonstrations. Additionally, we demonstrate that the resulting affordance prediction model is robust to distribution shifts, with overall performance on out of distribution settings within 10% of in-distribution evaluations."
https://arxiv.org/html/2411.02695v1,JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase,"Knowledge Graphs have emerged as a compelling abstraction for capturing key relationship among the entities of interest to enterprises and for integrating data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by leveraging knowledge graphs across the organization for multiple mission critical applications such as risk assessment, fraud detection, investment advice, etc. A core problem in leveraging a knowledge graph is to link mentions (e.g., company names) that are encountered in textual sources to entities in the knowledge graph. Although several techniques exist for entity linking, they are tuned for entities that exist in Wikipedia, and fail to generalize for the entities that are of interest to an enterprise. In this paper, we propose a novel end-to-end neural entity linking model (JEL) that uses minimal context information and a margin loss to generate entity embeddings, and a Wide & Deep Learning model to match character and semantic information respectively. We show that JEL achieves the state-of-the-art performance to link mentions of company names in financial news with entities in our knowledge graph. We report on our efforts to deploy this model in the company-wide system to generate alerts in response to financial news. The methodology used for JEL is directly applicable and usable by other enterprises who need entity linking solutions for data that are unique to their respective situations.","Knowledge Graphs are being used for a wide range of applications from space, journalism, biomedicine to entertainment, network security, and pharmaceuticals. Within JP Morgan Chase (JPMC), we are leveraging knowledge graphs for financial applications such as risk management, supply chain analysis, strategy implementation, fraud detection, investment advice, etc. While leveraging a knowledge graph, Entity Linking (EL) is a central task for semantic text understanding and information extraction. As defined in many studies (Zhang et al. 2010; Eshel et al. 2017; Kolitsas, Ganea, and Hofmann 2018), in an EL task we link a potentially ambiguous Mention (such as a company name) with its corresponding Entity in a knowledge graph. EL can facilitate several knowledge graph applications, for example, the mentions of company names in the news are inherently ambiguous, and by relating such mentions with an internal knowledge graph, we can generate valuable alerts for financial analysts. In Figure 1, we show a concrete example in which the name “Lumier” has been mentioned in two different news items. “Lumier”s are two different companies in the real world, and their positive financial activities should be brought to the attention of different stakeholders. With a successful EL engine, these two mentions of “Lumier”s can be distinguished and linked to their corresponding entities in a knowledge graph. Figure 1: Example for Entity Linking Prior work on EL has been driven by a number of standard datasets, such as CoNLYAGO (Suchanek, Kasneci, and Weikum 2007), TAC KBP111https://www.ldc.upenn.edu/collaborations/current-projects/tac-kbp, DBpedia222https://wiki.dbpedia.org/develop/datasets, and ACE333https://catalog.ldc.upenn.edu/LDC2006T06. These datasets are based on Wikipedia, and are therefore, naturally coherent, well-structured and rich in context (Eshel et al. 2017). We face the following problems when we use these methods for entity linking for our internal knowledge graph: 1) Wikipedia does not cover all the entities of financial interest. For example, as of this writing, the startup “Lumier” mentioned in Figure 1 is not present in Wikipedia, but it is of high financial interest as it has raised critical investment from famous investors. 2) Lack of context information. Many pre-trained models achieve great performance by leveraging rich context data from Wikipedia (Ganea and Hofmann 2017). For JPMC internal data, we do not have information comparable to Wikipedia to support re-training or fine-tuning of existing models. To address the problems identified above, we built a novel entity linking system, JEL, to link mentions of company names in text to entities in our own knowledge graph. Our model makes the following advancements on the current state-of-the-art: 1) We do not rely on Wikipedia to generate entity embeddings. With minimum context information, we compute entity embeddings by training a Margin Loss function. 2) We deploy the Wide & Deep Learning (Cheng et al. 2016) to match character and semantic information respectively. Unlike other deep learning models (Martins, Marinho, and Martins 2019; Kolitsas, Ganea, and Hofmann 2018; Ganea and Hofmann 2017), JEL applies a simple linear layer to learn character patterns, making the model more efficient both in the training phase and inference phase."
https://arxiv.org/html/2411.02594v1,"“It’s a conversation, not a quiz”: A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health","This is an unpublished pre-print. Please check the authors’ websites for updated versions and publication information.Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as accessible information sources or communication tools across different domains. In public health—where stakes are high and impacts extend across populations—adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with health professionals and health issue experiencers to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: vaccines, opioid use disorder, and intimate partner violence. We synthesize participants’ perspectives into a risk taxonomy, distinguishing and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk in individual behaviors, human-centered care, information ecosystem, and technology accountability. For each dimension, we discuss specific risks and example reflection questions to help practitioners adopt a risk-reflexive approach. This work offers a shared vocabulary and reflection tool for experts in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm when they are used.","“I love new technologies but this one has me scared. I myself wouldn’t feel like I know enough to ask the right questions and to evaluate the impact of it.” — P5 (public health director) Recent breakthroughs in large language models (LLMs) have spurred widespread attention and rapid adoption across different fields. With their abilities to generate convincing human-like language on the basis of large sets of human-written content (Brown et al., 2020; Vaswani et al., 2017), LLMs hold the potential to influence how we interact with information. LLMs have quickly gathered hundreds of millions of active users (Reuters, 2024) and made their way into everyday products, sometimes even without users’ full awareness. Accordingly, public discourse has shown excitement about LLM assistance in various information-seeking and support tasks and even hype about the framing of LLMs as a “next-generation search engine.” 111https://ico.org.uk/about-the-ico/research-reports-impact-and-evaluation/research-and-reports/technology-and-innovation/tech-horizons-report/next-generation-search/ This technology enthusiasm extends into the health sector, where studies have presented evidence of LLMs’ competence in completing tasks such as clinical documentation (Thirunavukarasu et al., 2023; Zhang et al., 2024b), decision support (Thirunavukarasu et al., 2023; Tu et al., 2024), therapeutic conversations (Wang et al., 2021; Sharma et al., 2024b; Kim et al., 2024), and public health intervention (Jo et al., 2023, 2024; Karinshak et al., 2023). As a matter of fact, LLMs have already been incorporated into large-scale health systems. For instance, Epic has partnered with Microsoft to start integrating LLM tools into its electronic health record (EHR) software, which owns the largest market share of acute care hospitals in the U.S., to automatically draft clinical notes (Landi, 2023) and patient message responses (Landi, 2024). While this proliferation seems promising, experts and scholars caution against exaggerating LLM capabilities (Drogt et al., 2024; Wornow et al., 2023). Specifically, uncertainty about information quality is problematic, particularly regarding inaccurate information, biases, and harmful content (Zhou et al., 2023; Liang et al., 2022; Gehman et al., 2020). Given the high-stakes nature and data sensitivity of population-level health, it warrants careful deliberations of when it is appropriate to employ LLM capacities and how we can mitigate potential harm when we do. However, in practice, evaluating the risks of potential LLM adoption in public health remains challenging. The first gap lies in the lack of domain understanding, as the majority of risk taxonomies lack granularity to be applied for specific uses (Liao and Vaughan, 2023) or are created within the computer science community and tend to leave out domain experts and real users (Shelby et al., 2023; Weidinger et al., 2022; Bender et al., 2021). Second, when considering population-level impact, potential risks come in ecological “layers” from individuals to society, and conventional categorizations based on content types (e.g., misinformation, hate speech, and biases) are not sufficient to evaluate real-world cases (Scheuerman et al., 2021) while different types often intertwine with each other (Kim and Kesari, 2021; Gover et al., 2020). Identifying the presence of generated harmful content is only the first step; we must also contextualize low-quality information within specific health issues and relevant populations in order to evaluate the consequences and severity and mitigate potential harm. In response to these gaps, this paper is situated in three distinct and critical public health issues and grounded in the perspectives of both health professionals and members of the general public who might use such tools to seek health information. Specifically, we ask: what negative consequences might arise in adopting LLMs in public health for informational needs and support? We conducted focus group sessions with ten health professionals and ten health issue experiencers to uncover potential negative influences of using LLMs as a communication tool or information source in public health. We selected vaccines, opioid use disorder (OUD), and intimate partner violence (IPV) as topics for different sessions based on their significance across different dimensions of public health—infectious disease prevention, chronic and well-being care, and community health and safety—all demanding high-quality information with existing prevalent issues such as misinformation, biases, and sensitivity. The result is a risk taxonomy of potential adoption of LLM for public health. Our taxonomy consists of four dimensions of harm: individual behaviors, human-centered care, information ecosystem, and technology accountability (Fig 2). Within each area, we list specific risks and associated example reflection questions to help assist practitioners in both computing and health fields in becoming reflexive and risk-aware. This risk taxonomy makes several contributions. First, it gives a comprehensive and grounded list of possible risks in implementing LLMs for public health, and differs from pre-existing generic taxonomies by being grounded in public health issues and learning from domain experts and real users. To our knowledge, this is the first work to comprehensively explore potential risks of LLMs for public health. Second, by offering a shared vocabulary, it paves the path for future collaborations between experts in computing and health fields. As public interest starts to gather in LLMs as an emerging technology, it allows for careful and thorough reflections on potential negative consequences and prevents reckless adoption that could result in unbearable disruptions and real harms to individuals and communities and further erode trust in public health responses. Content Warning: We caution the readers that this paper discusses sensitive topics, including intimate partner violence and opioid use disorder. Some readers may find certain quotes and descriptions to be emotionally triggering."
https://arxiv.org/html/2411.02558v1,Enhancing Risk Assessment in Transformers with Loss-at-Risk Functions,"In the financial field, precise risk assessment tools are essential for decision-making. Recent studies have challenged the notion that traditional network loss functions like Mean Square Error (MSE) are adequate, especially under extreme risk conditions that can lead to significant losses during market upheavals. Transformers and Transformer-based models are now widely used in financial forecasting according to their outstanding performance in time-series-related predictions. However, these models typically lack sensitivity to extreme risks and often underestimate great financial losses. To address this problem, we introduce a novel loss function, the Loss-at-Risk, which incorporates Value at Risk (VaR) and Conditional Value at Risk (CVaR) into Transformer models. This integration allows Transformer models to recognize potential extreme losses and further improves their capability to handle high-stakes financial decisions. Moreover, we conduct a series of experiments with highly volatile financial datasets to demonstrate that our Loss-at-Risk function improves the Transformers’ risk prediction and management capabilities without compromising their decision-making accuracy or efficiency. The results demonstrate that integrating risk-aware metrics during training enhances the Transformers’ risk assessment capabilities while preserving their core strengths in decision-making and reasoning across diverse scenarios.","In the financial sector, decision support systems depend critically on precise and reliable risk assessments, especially in evaluating the maximum potential loss from a decision [1, 2, 3, 4, 5]. Transformer models, particularly Large Language Models (LLMs) and extensive time series models based on Transformer architectures have emerged as key tools in complex risk management and decision analysis [6, 7, 8, 9, 10]. These models typically utilize standard loss functions such as Mean Squared Error (MSE) for training and fine-tuning, which ensure high accuracy in predictions and judgments [11, 12, 13, 14, 15, 16]. However, their loss functions in training and fine-tuning process are primarily designed to minimize average prediction errors, and they often overlook the potential for extreme risk events. This oversight can lead Transformers to make overly optimistic decisions during significant market upheavals or unusual situations, potentially resulting in substantial financial losses. Although extreme risk events are infrequent in the financial sector, they can cause severe economic losses and credit risks, such as financial crises and market collapse. Thus, it is essential to improve Transformers’ awareness of risk and their risk assessment capabilities to better influence their decision-making processes. Previous studies have addressed the issue of generalization for Transformer models handling small datasets by introducing prior knowledge and improved loss function methods [17, 18, 19, 20, 21]. For instance, some research enhances model generalization by quantifying uncertainty or using task-specific loss function variants to boost performance in particular scenarios [22, 23, 24, 25, 26]. However, these methods often fail to address the sensitivity of Transformer models to high-risk decisions directly. While these techniques can improve prediction accuracy under normal conditions, they also tend to overlook risk assessments under extreme conditions. This oversight can lead Transformers to make insufficiently cautious decisions when faced with potential risks. These approaches primarily focus on improving models’ handling of common data rather than optimizing responses to rare or extreme events [27, 28, 29]. This limitation is particularly critical in the financial sector, where even rare market anomalies can lead to major economic disruptions. Thus, traditional methods fall short in enabling Transformers to adequately assess and manage high-risk events that could result in severe consequences, affecting the safety and reliability of decisions at crucial moments. Therefore, we consider that during the training process of Transformers, introducing risk-aware metrics would make the model more sensitive to potential extreme losses. For Transformers, the backpropagation is an optimal stage for such adjustments because the model has already acquired a general understanding of data patterns from its initial training on large datasets [30, 31, 32, 33, 34]. During this process, incorporating specialized loss functions can refine the model’s responses to be particularly cautious about rare but high-impact risks crucial in high-stakes environments like finance. This method leverages the model’s ability to generalize from broad data while honing in on the nuanced understanding needed for specific tasks, which preserves the model’s overall performance and enhances its precision in critical risk assessments. Our Target. To address these problems, we target three main aspects: (1) We aim to make our Transformers more sensitive to extreme losses during the backpropagation stage. By integrating metrics into the loss function, we focus on minimizing losses in worst-case scenarios. This approach is crucial for high-stakes applications where errors can lead to significant consequences. (2) We need to ensure these changes do not compromise the Transformer’s core strengths. This includes its time-series-related prediction, attention mechanism and robustness. The model must still excel across standard tasks while managing the demands of high-risk environments. We manage the backpropagation process to boost the Transformer’s overall abilities. (3) We then rigorously test the updated loss function in financial settings. We evaluate the model performance on financial datasets marked by high volatility and risk, to compare how the model performs before and after introducing the new loss functions. Our goal is to confirm that our changes enhance the model’s ability to predict and manage risks without losing accuracy or efficiency in decision-making. Figure 1: A schematic diagram of VaR. Our Method. According to the three targets, we design Loss-at-Risk function specifically to enhance the robustness of Transformers in high-stakes financial scenarios. Our method integrates both Value at Risk (VaR) [35, 36, 37] and Conditional Value at Risk (CVaR) [38, 39, 40] into the traditional MSE loss framework. (1) We enhance sensitivity to extreme losses. The Loss-at-Risk calculates the traditional MSE for average scenarios and adds a weighted component based on VaR and CVaR metrics. VaR provides a threshold beyond which a given percentage of losses will fall, introducing a focus on worst-case scenarios. CVaR goes further by averaging the losses that exceed the VaR threshold, ensuring the model accounts for the severity of extreme outcomes. By focusing on these metrics during backpropagation, the model becomes more sensitive to major potential losses, while ignoring common, less critical errors. (2) We also maintain core capabilities at the Transformer’s loss function. While integrating these risk metrics, we carefully calibrate their influence to maintain the Transformer’s essential skills in reasoning, decision-making, and robustness across general tasks. The weighting of the VaR and CVaR components within the Loss-at-Risk function is adjusted to ensure that risk management enhancements do not compromise the model’s effectiveness in less volatile environments. This balanced approach allows the model to remain versatile and effective, operating reliably in typical and high-risk financial contexts. (3) Furthermore, to validate the effectiveness of our Loss-at-Risk function, we implement a rigorous testing regime using diverse financial datasets. By comparing the model’s performance on these datasets before and after applying the Loss-at-Risk function, we assess how well the model predicts and manages financial risks. The goal is to demonstrate that our approach not only preserves but enhances the model’s decision-making accuracy and efficiency under stress, and further proving that the model can handle the complexities of real-world financial applications effectively. In summary, our contribution includes: 1. We integrate risk assessment with MSE loss for Transformer models’ training process to enhance their ability in extreme loss scenarios. This Loss-at-Risk equips Transformers the risk-awareness to handle high-stakes financial risks effectively. 2. Our method maintains the Transformer model’s core decision-making and time-series-based predicting capabilities while enhancing risk sensitivity. This ensures effective performance in both typical and high-risk financial situations. 3. We conduct rigorous testing on volatile financial datasets to validate our model function. The results confirm improved risk management without sacrificing decision-making accuracy or efficiency."
https://arxiv.org/html/2411.02545v1,TripletCLIP: Improving Compositional Reasoning of CLIP via Synthetic Vision-Language Negatives,"Contrastive Language-Image Pretraining (CLIP) models maximize the mutual information between textual and visual modalities to learn representations. However, the lack of compositional diversity in contemporary image-text datasets limits the compositional reasoning ability of CLIP. We show that generating “hard” negative captions via in-context learning and synthesizing corresponding negative images with text-to-image generators offers a solution. We introduce a novel contrastive pre-training strategy that leverages these hard negative captions and images in an alternating fashion to train CLIP. We demonstrate that our method, named TripletCLIP, when applied to existing datasets such as CC3M and CC12M, enhances the compositional capabilities of CLIP, resulting in an absolute improvement of over 9%percent99\%9 % on the SugarCrepe benchmark on an equal computational budget, as well as improvements in zero-shot image classification and image retrieval. Our code, models, and data are available at: \gradientRGBtripletclip.github.io92,51,23170, 108, 57.","Large-scale vision-language models, such as CLIP radford2021learning , have significantly advanced multi-modal learning by employing contrastive learning to acquire shared semantic representations from paired datasets. This approach has resulted in improved performance in vision-language tasks as well as zero-shot image classification song2022clip and segmentation jiao2023learning ; zhou2023zegclip . Beyond vision-language tasks, the individual components of these models, such as the vision encoder and the language encoder, are integral to several multimodal architectures and generative models such as multimodal large language models (MLLMs) NEURIPS2023_6dcf277e ; laurençon2024matters and text-to-image (T2I) diffusion models sauer2024fast ; patel2023eclipse ; patel2024eclipse . Yet, compositional reasoning remains challenging and multimodal models continue to exhibit naïve “bag of words” behavior, frequently failing to distinguish between expressions like “bulb in the grass” and “grass in the bulb” yuksekgonul2022and ; thrush2022winoground . Addressing this challenge remains critical for enhancing vision-language models and their downstream applications. Contrastive learning of representations benefits from “hard negative samples” (i.e., points that are difficult to distinguish from an anchor point) robinson2020contrastive . However, at each optimization step for training CLIP, image-text pairs are randomly sampled from the training dataset – this random sampling seldom exposes the model to highly similar negative pairs. We hypothesize that the limited compositional understanding of CLIP may stem from such issues in the optimization objective and sampling from training datasets. A straightforward solution could involve iteratively identifying hard negative pairs for each training iteration. However, due to the noisy captions and the scarcity of such pairs in existing datasets, prior work generates hard negative captions as a form of augmentation using rule-based strategies yuksekgonul2022and ; zhang2023contrasting . For instance, given an image-text pair labeled “a brown horse”, an additional negative caption “a blue horse” might be introduced. However in prior work, image data is not subjected to similar hard negative semantic augmentation during training; this is mainly because of the difficulty of making semantic perturbations at the pixel levels compared to sentence perturbation. While the text-only augmentation strategies have improved the models’ compositional understanding to a certain extent, it raises an intriguing question: could incorporating hard negative augmentation for both text and image modality further enhance the compositional reasoning capabilities of vision-language models? Motivated by this, in this paper, we introduce a novel, simple, and yet highly effective strategy for integrating hard negative images as well as hard negative text to enhance the compositional understanding of vision-language models. Recent developments in text-to-image diffusion models have opened up possibilities for performing semantic perturbations within images huang2024diffusion . Existing works have evaluated the impact of creating synthetic data for text-to-image generative models betker2023improving ; chatterjee2024getting . However, it remains less explored how these generative models can benefit the CLIP-like models. To tackle this challenge, our approach leverages the in-context learning capabilities of LLMs to produce realistic, linguistically accurate negative captions wang2022super . We then employ a pre-trained text-to-image diffusion model to create images corresponding to these captions, thereby enriching any given image-text dataset with valuable hard negatives that foster improved reasoning. This resulting TripletData comprises 13M image-text pairs to complement the CC3M and CC12M datasets Changpinyo_2021_CVPR . We developed TripletCLIP, which incorporates hard negative image-text pairs effectively by using them to optimize a novel triplet contrastive loss function. Extensive experiments on the CC3M and CC12M datasets and various downstream tasks with an equal compute budget demonstrate that TripletCLIP significantly enhances compositional reasoning. Notably, TripletCLIP results in more than 9% and 6% absolute improvement on the SugarCrepe benchmark compared to LaCLIP lai2023scarcity and NegCLIP yuksekgonul2022and , respectively. TripletCLIP also improves zero-shot classification and image-text retrieval performance with similar training-time concept diversity. An investigation into the effects of increasing training-time concept diversity revealed that baseline models consistently under-performed in compositional tasks despite an increase in integrated knowledge, while TripletCLIP demonstrated significant improvements. In summary, our key contributions are as follows: • We introduce a novel CLIP pre-training strategy that employs hard negative text and images in conjunction with triplet contrastive learning to enhance compositionality. • TripletCLIP consistently improves across downstream tasks, demonstrating the effectiveness of synthesizing hard negative image-text pairs. • Our extensive ablations on the choice of the loss function, modality-specific pre-training, the increase in concept diversity, and filtering high-quality TripletData provide deeper insights into the utility of hard negative image-text pairs for CLIP pre-training. • Ultimately, we present a promising avenue where synthetic contrastive datasets significantly improve reasoning capabilities, leading to the creation and release of the TripletData — a 13M contrastive image-text dataset."
https://arxiv.org/html/2411.02537v2,Inquire: A Natural WorldText-to-Image Retrieval Benchmark,"We introduce Inquire, a text-to-image retrieval benchmark fdesigned to challengemultimodal vision-language models. on expert-level queries Inquire includes iiNaturalist 2024 (iNat24) a new dataset of five million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identification, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) Inquire-Fullrank, a full dataset ranking task, and (2) Inquire-Rerank, a reranking task for refining top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that Inquire poses a significant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a significant margin for improvement. By focusing on scientifically-motivated ecological challenges, Inquire aims to bridge the gap between AI capabilities and the needs of real-world scientific inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.","Recent advances in multimodal learning have resulted in advanced models Radford et al. [2021], Liu et al. [2023], Achiam et al. [2023] that demonstrate remarkable generalization capabilities in zero-shot classification Radford et al. [2021], Zhai et al. [2023], visual question-answering (VQA) Li et al. [2022], Yu et al. [2022], Alayrac et al. [2022], Li et al. [2023], and image retrieval Yu et al. [2022], Li et al. [2023]. These models offer the potential to assist in the exploration, organization, and extraction of knowledge from large image collections. However, despite this success, there remains a significant gap in the evaluation of these models on domain-specific, expert-level queries, where nuanced understanding and precise retrieval are critical. Addressing this gap is essential for future deployment in specialized fields such as biodiversity monitoring and biomedical imaging, among other scientific disciplines. Previous studies of the multimodal capabilities of this new generation of models have primarily focused on the task of VQA. In VQA, it has been demonstrated that there remains a large performance gap between state-of-the-art models and human experts in the context of challenging perception and reasoning queries such as those found on college-level exams Yue et al. [2024], Zhong et al. [2023]. However, no such expert-level benchmark exists for image retrieval. The most commonly used text-to-image retrieval benchmarks are derived from image captioning datasets, and contain simple queries related to common everyday categories Young et al. [2014], Lin et al. [2014]. Current multimodal models achieve near perfect performance on some of these benchmarks, indicating that they no longer pose a challenge (e.g., BLIP-2 Li et al. [2023] scores 98.9 on Flickr30K Young et al. [2014] top-10). Existing retrieval datasets are generally small Philbin et al. [2007, 2008], Young et al. [2014], Lin et al. [2014], limited to a single visual reasoning task (e.g., landmark-location matching Philbin et al. [2007, 2008], Weyand et al. [2020]), and lack concepts that would require expert knowledge Philbin et al. [2007, 2008], Weyand et al. [2020], Young et al. [2014], Lin et al. [2014]. These limitations impede our ability to track and improve image retrieval capabilities. Figure 2: Category breakdown for the fine-grained queries that make up Inquire. Each query category falls under one of the following supercategories: Species, Context, Behavior, or Appearance. A domain that is well-suited for studying this problem is the natural world, where images collected by enthusiast volunteers provide vast and largely uncurated sources of publicly available scientific data. In particular, the iNaturalist iNa [b] platform contains over 180 million species images and contributes immensely to research in biodiversity monitoring Chandler et al. [2017], Lohan [2024]. These images also contain a wealth of “secondary data” not reflected in their species labels Pernat et al. [2024], including crucial insights into interactions, behavior, morphology, and habitat that could be uncovered through searches. However, the time-consuming and expert-dependent analysis needed to extract such information prevents scientists from taking advantage of this valuable data at scale. This cost is amplified as scientists typically want to retrieve multiple relevant images for each text query, so that they can track changes of a property over space and time Young et al. [2019]. This domain serves as an ideal testbed for expert image retrieval, as these images contain expert-level diverse and composite visual reasoning problems, and progress in this field will enhance impactful scientific discovery. In this work, we introduce Inquire, a new dataset and benchmark for expert-level text-to-image retrieval and reranking on natural world images. Inquire includes the iNat24 dataset and 250 ecologically motivated retrieval queries. The queries span 33,000 true-positive matches, pairing each text query with all relevant images that we comprehensively labeled among iNat24’s five million natural world images. iNat24 is sampled from iNaturalist iNa [b], and contains images from 10,000 different species collected and annotated by citizen scientists, providing significantly more data for researchers interested in fine-grained species classification. The queries contained within Inquire come from discussions and interviews with a range of experts including ecologists, biologists, ornithologists, entomologists, oceanographers, and forestry experts. Our evaluation of multimodal retrieval methods demonstrates that Inquire poses a significant challenge, necessitating the development of models able to perform expert-level retrieval within large image collections. A key finding from our experiments is that reranking, a technique typically used in text retrieval Nogueira and Cho [2019], Khattab and Zaharia [2020], Karpukhin et al. [2020], offers a promising avenue for improvement in image retrieval. We hope that Inquire will inspire the community to build next-generation image retrieval methods towards the ultimate goal of accelerating scientific discovery. We make Inquire, the iNat24 dataset, pre-computed outputs from state-of-the-art models, and code for evaluation available at https://inquire-benchmark.github.io/. Figure 3: Proportion of queries in Inquire associated with each iconic group of species. Table 1: Comparison to common datasets used to evaluate text-to-image retrieval Gadre et al. [2023]. Unlike other datasets, Inquire has significantly more images and many matches per query rather than exactly one. MpQ: Matches per query Dataset Images Queries MpQ Expert Flickr30k Young et al. [2014] 1,000 5k 1 ✗ COCO Lin et al. [2014] 5,000 25k 1 ✗ Inquire 5,000,000 250 1–1.5k ✓"
https://arxiv.org/html/2411.02530v1,A Comprehensive Study on Quantization Techniques for Large Language Models,"Large Language Models (LLMs) have been extensively researched and used in both academia and industry since the rise in popularity of the Transformer model, which demonstrates excellent performance in AI. However, the computational demands of LLMs are immense, and the energy resources required to run them are often limited. For instance, popular models like GPT-3, with 175 billion parameters and a storage requirement of 350 GB, present significant challenges for deployment on resource-constrained IoT devices and embedded systems. These systems often lack the computational capacity to handle such large models. Quantization, a technique that reduces the precision of model values to a smaller set of discrete values, offers a promising solution by reducing the size of LLMs and accelerating inference. In this research, we provide a comprehensive analysis of quantization techniques within the machine learning field, with a particular focus on their application to LLMs. We begin by exploring the mathematical theory of quantization, followed by a review of common quantization methods and how they are implemented. Furthermore, we examine several prominent quantization methods applied to LLMs, detailing their algorithms and performance outcomes.","Machine Learning has been growing successfully with noticeable rate since 2000, and especially the recent 10 years with a compound growth rate of 30%. One of the most common branches from machine learning, Deep learning, requires more data than other machine learning branches, which in turn requires more computational power. In general, deep learning models need powerful graphical processing units (GPUs) to handle the large volumes of data and complex calculations involved in training deep neural networks. However, access to the unlimited computational resource (GPUs) for deep learning models is far from ideal due to the high costs. A research field, Quantization in deep learning, aim to reduce the high cost of computations and memory by representing the weights and activation in deep learning models with low precision data types. In the Computer Science, a floating point(float32) consists of 32 bits requires much larger resources than a integer(int8) consists only 8 bits. This type of quantization is straight forward as reducing the number of bits of the data type, from float32 to int8, to consume less computational costs. In addition, mathematics operations including matrix multiplications can be performed with faster speed with lower precision data types. The dictionary definition of quantization is the division of a quantity into a discrete number of small parts, often assumed to be integral multiples of a common quantity. The first use of quantization is rounding off and was analyzed by Shappard [1]. In addition, Shannon Entropy quantifies the uncertainty within a dataset, and the process of quantization can influence the calculated entropy by altering the precision of data values[2]. With the advancement of computer science and machine learning, quantization research fields expands significantly. One of the most common quantization techniques is 8-bit quantization which convert floating point data type to integer data type. While 32-bit single-precision floating-point has been the predominant numerical format for deep learning (DL) applications, alternative formats has emerged recently to enhance the computational performance of these applications[3]. It is very common to train neural networks using 16-bit floating-point formats, such as fp16 or bfloat16, which are supported by most DL accelerators. After training, neural networks can be deployed for inference using even lower-precision formats, including floating-point, fixed-point, and integer representations. Low-precision formats confer several performance advantages. Firstly, many processors are equipped with higher-throughput mathematical pipelines for low-bit formats, thereby accelerating computation-intensive tasks like convolutions and matrix multiplications. Secondly, reduced word sizes reduce memory bandwidth constraints, resulting in improved performance for bandwidth-limited computations. Third, smaller word sizes decrease memory size requirements, which enhances cache utilization and positively impacts various aspects of memory system performance. Utilizing the int8 quantization, this work achieves the ability to sustain model accuracy within 1% of the baseline floating-point networks. This is particularly noteworthy for networks that are typically difficult to quantize, such as MobileNets and BERT-large. Moreover, Vector quantization used to compress the deep convolutional networks[4] is a good work supplements this research filed. In general, a CNN that works well object classification contains eight layers and a huge number of parameters, and it is widely known that the parameters are heavily over-parameterized. The goal of this work is to compress these parameters while maintain the high accuracy. This research mainly focus vector quantization methods for the compression of densely connected layers. It involves parameter binarization, scalar quantization through k-means clustering, and structured quantization employing product quantization or residual quantization, all of which lead to significant improvements in performance. Language model is a branch of machine learning that is designed to understand and generate natural language. Technically, it understands the context of the prompts and generate the missing part with coherent and contextually appropriate language. Language models can be classified into four major types: Statistical Language Models (SLM), Neural Language Models (NLM), Pre-trained Language Models (PLM), and Large Language Models (LLM). Each of these models represents a distinct approach to natural language processing, with varying techniques and capabilities for handling linguistic data[5, 6, 7, 8]. While Statistical Language Models and Neural Language Models have been researched for decades, the Pre-trained Language Models and Large Language Models draw a lot of popularity in both academia and industry recently and the applications are widely used in various fields. For example, recommendation system[9, 10]. Pre-trained Language models propose train largely amount of text data before fine-tune for specific downstream tasks. Based on the Transformer architecture and the self-attention mechanism[11, 12, 13, 14, 15], Pre-trained Language Models advances the performance for semantic-purpose language processing. BERT(Bidirectional Encoder Representations from Transformers) serves as an exemplary Pre-trained Language Model, pre-trained on a large corpus of text in an unsupervised manner using masked language modeling and next sentence prediction and fine-tuned for tasks like question-answering, sentiment analysis, and text classification. It has become the dominant paradigm in Pre-trained Language models due to its efficiency, scalability, and superior performance across multiple tasks[16, 17]. Large Language Models represent an extension of research based on Pre-trained Language Models, building upon their foundational Transformer architectures to enhance capabilities in natural language processing. It’s found that scaling up PLM model size and data size enhances the capacity to perform more effectively on downstream tasks. For example, GPT-3, a significantly larger PLM with 175 billion parameters, shares a similar architecture and pre-training tasks with standard PLMs. However, despite the primary focus of scaling being on model size, GPT-3 shows remarkable abilities and out performs standard PLM in solving complex tasks. In certain creative writing fields[18], GPT-3 model highly capable of producing creative content like poetry, song lyrics, or fiction that are coherent to specific styles or themes whereas BERT(340 million parameters) is not designed for creative writing tasks which only capable of completing sentences or predicting missing words. Due to Large Language Model’s attribute that requires practical large-scale data processing and distributed parallel training, conducting repetitive studies to explore various training strategies is highly resource-intensive and costly. Quantization techniques can help mitigate the high costs associated with training Large Language Models(LLM) by reducing the computational and resource demands. By reducing the number of bits required for each of a model’s weights, significantly decreases the overall model size. This reduction leads to LLM that consume less memory, require less storage space, are more energy-efficient, and enable faster inference. These advantages enable LLM to operate on a broader range of devices, including embedded devices and single GPU devices. For instance, supporting AI models on SLAM robotics devices[19, xanthidis2021towards, 20] or decentralized web3 applications is very challenging, as these systems cannot easily run full-sized models due to their limited capacity for handling high-cost computations[21, 22, 23, 24]. In such cases, quantization becomes necessary for enabling these integrations, as it reduces model size and computational requirements, making deployment on resource-constrained platforms achievable. While there are various quantization techniques, the two most notable types used in LLM are Post-Traning Quantization(PTQ) and Quantization-Aware Training(QAT). PTQ refers to a technique used to reduce the size and computational demands of a machine learning model after it has been trained and it only affects the inference state. The research work SmoothQuant[25] introduces a PTQ solution aimed at reducing hardware costs and democratizes LLMs. SmoothQuant enables 8-bit weight, 8-bit activation(W8A8) quantization for LLMs and it smooths the activation outliers by offline migrating the quantization difficulty with a mathematically equivalent transformation given that weighs are easy to quantize but activations are not. QuIP[26] is another research work that employs PTQ in LLMs. This method is based on the insight that quantization benefits from incoherent weight and Hessian matrices. As a result, QuIP improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. On the other hand, Quantization-Aware Training (QAT) technique refers to optimize models for efficient inference by simulating the effects of quantization during the training process. In contrast to PTQ techniques, QAT integrates the weight conversion process during the training stage. The Research work Degree-Quant[27] efficiently improves inference time of Graph Neural Networks by utilizating the QAT technique. Degree-Quant explores the viability of training quantized GNNs, allowing the use of low precision integer arithmetic during inference. Models trained with Degree-Quant for INT8 quantization perform comparably to FP32 models in most cases, while INT4 models achieve up to a 26% improvement over baseline models. Moreover, EfficientQAT[28] is another research work proposes a more feasible QAT algorithm that satisfies reducing memory consumption during LLM training. EfficientQAT employs a two-step approach: Block-wise training of all parameters (Block-AP) and end-to-end training of quantization parameters (E2E-QP) in which reducing accuracy loss in low-bit scenarios and then trains only the quantization parameters end-to-end. As a result, EfficientQAT outperforms previous quantization methods across a range of models with scales from 7B to 70B parameters at different quantization bit levels. This paper aims to provide a comprehensive review of quantization techniques in the context of LLMs. We begin by detailing the underlying mechanisms of quantization, followed by a comparison of various approaches, with a specific focus on their application at the LLM level."
https://arxiv.org/html/2411.02408v1,AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with an LLM-based Empathetic Coworker,"Client-Service Representatives (CSRs) are vital to organizations. Frequent interactions with disgruntled clients, however, disrupt their mental well-being. To help CSRs regulate their emotions while interacting with uncivil clients, we designed Pro-Pilot, an LLM-powered assistant, and evaluated its efficacy, perception, and use. Our comparative analyses between 665 human and Pro-Pilot-generated support messages demonstrate Pro-Pilot’s ability to adapt to and demonstrate empathy in various incivility incidents. Additionally, 143 CSRs assessed Pro-Pilot’s empathy as more sincere and actionable than human messages. Finally, we interviewed 20 CSRs who interacted with Pro-Pilot in a simulation exercise. They reported that Pro-Pilot helped them avoid negative thinking, recenter thoughts, and humanize clients; showing potential for bridging gaps in coworker support. Yet, they also noted deployment challenges and emphasized the irreplaceability of shared experiences. We discuss future designs and societal implications of AI-mediated emotional labor, underscoring empathy as a critical function for AI assistants in front-office roles.","When we engage an organization for their product or service, our initial contact is with staff known as front-office workers or Client Service Representatives (CSRs). They are the first line of response for the organization. Unlike other roles within an organization, a CSR’s task involves frequent interactions with clients and individuals outside an organization (Hochschild, 1983). These interactions require CSRs to manage their emotions constantly to complete tasks. Essentially they exert emotional labor to appear professional (Hochschild, 1983). The crucial challenge for a CSR is engaging with a client who starts behaving uncivilly by communicating in a rude, aggressive, and emotionally charged way (Grandey et al., 2007). No matter the type of request, a CSR’s role is to resolve a client’s concern and comply with the adage, “the customer is always right.” Unfortunately, this leads to an emotional dissonance between what a CSR expresses and what they actually feel (Grandey, 2000). Ultimately, such workers are vulnerable to excessive stress and eventual burnout. Not only do CSRs report being emotionally depleted and detached, but they also report a lack of accomplishment (Brotheridge and Grandey, 2002). Clearly, CSRs play a critical role within the organization, but, we have witnessed little innovation in alleviating their emotional toll. Our paper investigates how AI-coworkers help CSRs regulate their emotions in the face of client incivility. The brunt of client incivility in front-office work makes it infamous for low satisfaction and high-turnover (Pienaar and Willemse, 2008). A fundamental solution to the emotional distress of this role is Emotional Regulation (ER) (Yang and Lau, 2019). Basically, ER is the process through which one rethinks a negative situation (Grandey, 2000). While a worker may be able to do this on their own, research shows that coworkers play an important role in supporting ER (Yang and Lau, 2019). A good coworker can read the emotional cues of CSR’s work tasks and provide suggestions to help minimize the brunt of an aggressive client. However, CSRs are increasingly adopting remote work setups (Haan and Holznienkemper, 2023), which dampens social support (Vacchiano et al., 2024). Meanwhile, organizational scientists are calling for digital interventions to support worker wellbeing at scale (Black et al., 2019). We answer this call by designing and evaluating, Pro-Pilot— a Large Language Model (LLM)–based AI assistant for on-task Emotional Regulation in front-office work. While generative AI is emerging as a potent tool to complement the informational load of many different roles, the HCI community lacks research to investigate their use in emotional labor. Our research demonstrates the efficacy of LLM-generated empathetic support and evaluates how such a tool can be situated in CSR interactions with uncivil clients to answer the following research questions: RQ I:: How appropriate are LLM–based empathetic support messages for CSRs in response to client incivility? RQ II:: What is the role of embedding LLM–based empathetic support into CSR’s emotional labor? We developed Pro-Pilot by leveraging domain knowledge on client incivility (Andersson and Pearson, 1999; Frey-Cordes et al., 2020; Cho et al., 2002), real-world complaint data (Axelbrooke, 2017), and recent advancements in LLM-powered cognitive change (Burger et al., 2021; Sharma et al., 2023) and simulation (Shaikh et al., 2023). To answer the first research question, we conducted data-generation and evaluation tasks with 259 CSRs to comparatively analyze Pro-Pilot and human-coworker support for a variety of client incivility situations (1(a)). To answer the second question, we used Pro-Pilot as a technology probe and conducted a mixed-methods simulation exercise to understand how real CSRs could include it in their client interactions by juxtaposing Pro-Pilot usage with CSR’s socio-organizational norms (1(b)). Consequently, we contribute: • Pro-Pilot: an interactive technological artifact to expose CSRs to client incivility and learn healthy, long-term emotional labor practices to improve their own health and support their coworkers. • Empirical evidence that LLM–based empathetic support can be engineered to adapt to— and express empathy in— various client-incivility scenarios (RQI). Our results demonstrate that Pro-Pilot’s messages are linguistically distinct from both zero-shot approaches and human–coworkers, and moreover, Pro-Pilot’s messages were perceived to be more empathetic on several dimensions including sincerity, actionability, and relatability. • End-user insight on the function of LLM–based empathetic support to scaffold them through emotional labor during uncivil interactions (RQ2). Our findings showcase Pro-Pilot’s process of redirecting negativity and making CSRs more self-efficacious. While CSRs envision Pro-Pilot addresses important opportunities in workplace social support, they also surfaced the challenges of Pro-Pilot emulating holistic human support. This paper has implications for re-imagining how AI-assistants for workers should be designed and also re-imagining the social norms and policies to accommodate these advancements. Reflexive Considerations. Front-office work has many stakeholders, including the employer and the clients. However, it is the CSR who bears the burden of repeated emotional labor (Grandey, 2000). The relationship between these stakeholders is asymmetric, as employers can replace personnel and clients can switch services, but the CSR does not possess the same mobility (Fisk and Neville, 2011). Following from recent works that take a worker-centered perspective (Das Swain et al., 2023a, 2024; Kawakami et al., 2023), our research aims to illuminate the worker who is disadvantaged. This paper focuses on the needs of CSRs and centers their perspective throughout the evaluation. Two authors have past experience in front-office roles and direct end-user servicing. Four authors are researchers in an organization that employs its own CSRs. They helped us access real CSRs (and their resources) to provide feedback on our study design. We recruited participants for this study outside their organization to capture perspectives from different organizational sectors. All evaluations described in the paper were approved by the IRB of the first author’s institute. (a) Technical evaluation: Compared Pro-Pilot’s support messages with those produced by human-coworkers in CSR roles. \Description Study overview for technical evaluation. (b) User evaluation: Studied participant experiences with Pro-Pilot’s emotional support while interacting with uncivil clients. \Description Study overview for user evaluation. Figure 1. Schematic figures showing an overview of our study design."
https://arxiv.org/html/2411.02400v1,Decomposition Dilemmas: Does Claim Decomposition Boost or Burden Fact-Checking Performance?,"Fact-checking pipelines increasingly adopt the Decompose-Then-Verify paradigm, where texts are broken down into smaller claims for individual verification and subsequently combined for a veracity decision. While decomposition is widely-adopted in such pipelines, its effects on final fact-checking performance remain underexplored. Some studies have reported improvements from decompostition, while others have observed performance declines, indicating its inconsistent impact. To date, no comprehensive analysis has been conducted to understand this variability. To address this gap, we present an in-depth analysis that explicitly examines the impact of decomposition on downstream verification performance. Through error case inspection and experiments, we introduce a categorization of decomposition errors and reveal a trade-off between accuracy gains and the noise introduced through decomposition. Our analysis provides new insights into understanding current system’s instability and offers guidance for future studies toward improving claim decomposition in fact-checking pipelines.111Source code available at https://github.com/qishenghu/Decomp_Dilemmas","Figure 1: An overview of the Decompose-Then-Verify pipeline employed in this study, which comprises four key stages: decomposition, retrieval, verification, and aggregation of sub-claim results. This figure illustrates how different decomposition methods, such as FactScore (Min et al., 2023) and VeriScore (Song et al., 2024), can lead to divergent decomposing outcomes. In this example, FactScore generates ambiguous sub-claims, while VeriScore omits key information (e.g., “Ultimately, the success of the Su-57…”) from the input. Fact-checking is a critical task that typically involves evaluating the veracity of claims or reports. With the rise of large language models (LLMs), the scope of fact-checking task has expanded to include verifying content generated by LLMs (Sun et al., 2024). While progress has been made in reducing LLM hallucinations, the problem has not yet been resolved, presenting ongoing challenges in ensuring the factuality of LLM outputs (Huang et al., 2023). At the same time, many studies focus on LLM-driven automated fact-checking pipelines (Min et al., 2023; Wei et al., 2024; Chern et al., 2023), aiming to improve the efficiency of the fact-checking processes. A common design pattern in these LLM-driven pipelines is the Decompose-Then-Verify paradigm, as adopted in frameworks like FactScore (Min et al., 2023), FacTool (Chern et al., 2023) and VeriScore (Song et al., 2024). It involves decomposing input text into sub-claims (Decompose), retrieving supporting information (e.g., Wikipedia, Google Search) for each sub-claim, and using a verifier model to assess the veracity of each sub-claim (Verify). The results are then aggregated to produce a final verification. By breaking down complex inputs (decomposition), these pipelines are effective in pinpointing misinformation, enabling a nuanced determination of whether a text is supported, unsupported (Zhu et al., 2023; Zhao et al., 2024a; Li et al., 2024), or assigned a quantified score (Min et al., 2023; Wei et al., 2024). However, most existing studies primarily focus on the design of fact-checking pipeline architectures. While decomposition is commonly employed in these frameworks, the reliability of the decomposition process itself remains insufficiently investigated. Some studies show that the final predictions of these pipelines are sensitive to the decomposition outcomes (Jiang et al., 2024; Wanner et al., 2024). Additionally, it has been observed that a given decomposer does not consistently lead to performance improvements when paired with different verifiers. For example, Kamoi et al. (2023) report improvements using a pre-trained Natural Language Inference (NLI) model as the verifier. However, an ablation study by Minicheck (Tang et al., 2024), which employs the same dataset and decomposition method but different NLI models, does not observe consistent benefits from decomposition. Similar inconsistencies are observed in FELM (Zhao et al., 2024b), which assesses the effect of decomposing responses into segments and further into claims. The study reveals distinct differences between ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) as verifiers. Specifically, ChatGPT’s accuracy improves with claim-level decomposition, while GPT-4’s performance declines. Despite observed inconsistencies, the underlying reasons remain unclear, and there is limited in-depth analysis of how decomposition affects downstream performance. In this study, we investigate the underlying causes of performance variability by addressing three key questions. First, what determines decomposition’s effect on fact-checking performance? Through experiments, we present performance variability across factors such as input complexity, decomposition method design, and verifier strength. Second, what errors may decomposition introduce? We categorize decomposition error types, analyze their distribution, and validate their usefulness through error reflection. Third, what explains the variability in fact-checking performance? Our analysis reveals a trade-off between the accuracy gains by decomposing inputs into manageable sub-claims and the noise introduced by retrieval and decomposition as the number of sub-claims increases. This trade-off offers an explanation for the observed variability and provides insights for guiding future fact-checking pipeline design. Here are some key takeaways: • Current popular LLM-driven decomposition methods struggle to consistently improve fact-checking performance across varying input granularities and verifier strengths. • Decomposition methods tend to introduce different errors depending on the objectives—prioritizing high atomicity may over-fragment simple facts, causing redundancy and ambiguity, while emphasizing verifiability can omit essential details. • Decomposition improves performance on simpler sub-claims by reducing complexity, notably benefiting weaker verifiers. However, for stronger verifiers, the marginal accuracy gain from decomposition may not counterbalance the increased noise. • Decomposition can improve the handling of complex inputs; however, while increasing the number of sub-claims may initially enhance performance, the additional noise introduced will gradually offset these gains, eventually leading to performance degradation."
https://arxiv.org/html/2411.02398v1,Prompting with Phonemes: Enhancing LLMMultilinguality for non-Latin Script Languages,"Multilingual LLMs have achieved remarkable benchmark performance, but we find they continue to underperform on non-Latin script languages across contemporary LLM families. This discrepancy arises from the fact that LLMs are pretrained with orthographic scripts, which are dominated by Latin characters that obscure their shared phonology with non-Latin scripts. We propose leveraging phonemic transcriptions as complementary signals to induce script-invariant representations. Our study demonstrates that integrating phonemic signals improves performance across both non-Latin and Latin languages, with a particularly significant impact on closing the performance gap between the two. Through detailed experiments, we show that phonemic and orthographic scripts retrieve distinct examples for in-context learning (ICL). This motivates our proposed Mixed-ICL retrieval strategy, where further aggregation from both leads to our significant performance improvements for both Latin script languages (up to 12.6%) and non-Latin script languages (up to 15.1%) compared to randomized ICL retrieval.","Large language models (LLMs) have demonstrated remarkable multilingual capabilities across various natural language processing (NLP) tasks. The increase in model parameters and rise of instruction datasets have led to the emergent capability of LLMs to perform tasks in few to zero shots Brown et al. (2020); Wei et al. (2022); Nguyen et al. (2023a) or adapt efficiently to new tasks through in-context learning (ICL) during inference Zoph et al. (2022). However, these capabilities remain disparate across languages Lai et al. (2023a), with one particular axis being along non-Latin versus Latin script languages (Bang et al., 2023; Ahuja et al., 2023; Shliazhko et al., 2024). To mitigate this disparity, we are motivated by the crucial role of phonemic awareness in human language acquisition and processing, facilitating skills like cross-lingual transfer and reading development Durgunoĝlu et al. (1993); Spencer and Hanley (2003), in part due to cognates, borrowed words, and shared phonology between language families. We hypothesize that integrating phonemic information could also enable LLMs’ robustness to the various choices of linguistic written script by capturing such alignments. For instance, in Figure 1, the Japanese orthographic representation111Orthographic representation and textual / written scripts are used interchangeably throughout this work. for hacker () is significantly different as compared to English. However, when observing the phonemic transcriptions–specifically, International Phonetic Alphabet (IPA) transcriptions at the level of phoneme discrimination– one could easily recognize the semantically similar words () highlighted in green in Figure 1. Figure 1: Orthographic and phonemic transcriptions (via the International Phonetic Alphabet; IPA) of the same sentence. Matching colors denote semantically similar words across different languages. On the other hand, the current prompting and ICL schemes rely solely on orthographic text input, overlooking potentially valuable linguistic information encoded in the phonemic structure of language. While continual pretraining with phonemic data could enhance LLMs’ multilingual capabilities, it faces several challenges. One significant limitation is the scarcity of large-scale multilingual datasets that align orthographic and phonemic transcriptions across diverse languages, especially for less-resourced languages. This lack of aligned data restricts the potential for fine-tuning models on phonemic information at scale. Furthermore, pretraining with phonemic data demands substantial computational resources due to the increased data size and complexity, requiring extensive training time on high-performance infrastructure. Hence, we propose the phonemic integration in LLMs via prompting and ICL, as these promise a more flexible and resource-efficient approach to realize the integration’s benefits. We hypothesize that augmenting with phonemic information could improve both demonstration retrieval and LLM reasoning, by explicitly surfacing fundamental cross-linguistic information Anderson (2018) that textual scripts might not capture or may not be readily accessible in the LLM’s internal representations. Our contributions are: • Evaluating multilingual performance across contemporary (≥\geq≥7B-parameter) LLM families and diverse sets of tasks, with specific focus on Latin vs. non-Latin scripts, revealing a significant gap on evaluation metrics (up to 29 absolute points). We then focus our work on notable performance disparities in key generative tasks such as text generation (AYA-WIKI), machine translation (FLORES) and question-answering (AYA-MLQA). • Investigating the integration of IPA with LLMs via (1) direct prompting (zero- and few-shot) and (2) in retrieval-based ICL augmentation. In particular, we find that aggregation of simple lexical retrieval on the text and phonemes (namely Mixed-ICL) gives performance gains of up to 15% relative on generative tasks, together with gains on Latin languages at inference time. Qualitative analyses examine retrieved cases and validate the observed empirical performance gains. • Performing in-depth analyses of the components involved in phonemic integration, offering insights and guidance for future work aiming to improve LLMs through phonemic integration beyond inference time."
https://arxiv.org/html/2411.02391v1,Attacking Vision-Language Computer Agents via Pop-ups,"Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups 111In this work, we use “pop-ups” to refer to clickable malicious images on the screen., which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing the tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack.","Language agents have been used to assist and even automate tasks in various domains and for diverse daily tasks on the web (Yao et al., 2023; Zhou et al., 2023; Yao et al., 2024). Interacting with a Graphical user interface (GUI) is a natural and essential part of completing these web tasks, which requires language agents to recognize and understand these interfaces like webpages or screenshots. Recent benchmarks (Koh et al., 2024; Deng et al., 2023; Xie et al., 2024; Agashe et al., 2024) have shown that state-of-the-art visual language models (VLMs), to some extent, can directly operate on computer screens (e.g., clicking, scrolling, and typing) when user instructions are given (e.g., find the cheapest product on this page, set the default search engine to Bing). Although these visual inputs are becoming more integrated into agentic applications, what types of risks exist and how such attacks affect VLMs remain unclear (Ruan et al., 2023; Yang et al., 2024). Existing attacks in the digital world mainly aim to attract and visually mislead human users, such as pop-ups with banner ads, fake download buttons, and countdown timers for deals. If VLM agents are taking actions on behalf of users to perform these web tasks and are not aware of these attacks, this could lead to severe consequences such as installing malware or being redirected to deceptive websites. To better understand risks in this context, we consider a threat model in which attackers aim to make the agent click on the pop-ups by manipulating the agent’s observations (e.g., screenshots and accessibility (a11y) tree) related to the attacked element (e.g., add/modify pop-ups). This setup corresponds to multiple realistic attack scenarios, such as malvertising (Sood and Enbody, 2011; Xing et al., 2015), in which attackers can either purchase an ad slot or leverage cross-site scripting (Hydara et al., 2015; Kaur et al., 2023) to inject malicious scripts that manipulate the website from the browser. Attackers can also send a clickable image through phishing emails/messages (Patel et al., 2019) to ensure the pop-ups are shown on the screen. Most previous agent attacks either made the adversarial examples as visually similar to the original ones (Wu et al., 2024) or inject invisible adversarial strings into web pages (Liao et al., 2024; Xu et al., 2024). Here, we argue that whether the adversarial examples are visible or recognizable by humans is not essential if the agent’s ultimate goal is to complete tasks with minimal or no human supervision. As long as the environment functions well and human users can complete the tasks as usual, the agent should be able to complete the tasks as well. Since experienced human users can identify suspicious online content and rarely follow the instructions in unverified pop-ups, we aim to investigate whether these adversarial pop-ups can mislead agents and thus can be used to stress test agents’ capabilities. Our design space (Figure 2) of attacks includes four representative elements to attack: (i) Attention Hook: a few words to attract the agent’s attention. (ii) Instruction: desired behaviors the attacker intends for the agent to follow. (iii) Information Banner: contextual information that implies or misleads the agent about the purpose of the pop-ups. (iv) ALT 222In HTML, alternative text (ALT text) is displayed when an element cannot be rendered, and it was previously used to enhance SoM agents. Descriptor: supplemental textual information provided for the pop-up within the a11y tree. In our experiments, we insert various types of adversarial pop-ups into the observation space for environments like OSWorld (Xie et al., 2024) and VisualWebArena (Koh et al., 2024). By testing screenshot agents (Xie et al., 2024) and Set-of-Mark agents (Yang et al., 2023) using state-of-the-art VLMs as backbones, we find that our attack achieves an attack success rate (ASR) over 80% on OSworld and over 60% on VisualWebArena in the default setting, where we assume the attacker has complete information (including the user query, the pop-up’s position, and the underlying agent framework, etc). Via a comprehensive set of ablation studies on the design choices of such adversarial pop-ups, we find that: (1) User query is essential for the attention hook, as using other alternatives (e.g., attackers speculate the user intent from the screen content.), on average, decreases the ASR by 61% relatively. (2) Other information (e.g., position and agent framework information) is relatively unnecessary to make the attack successful. (3) Basic defense strategies, such as asking the agent to ignore pop-ups and adding an extra advertisement notice, cannot effectively mitigate the issue (decrease the ASR by no more than 25% relatively). In summary, deploying computer-use agents still suffers from significant risks, and more robust agent systems are needed to ensure safe agent workflow."
https://arxiv.org/html/2411.02382v1,Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models,"Large language models (LLMs) have demonstrated remarkable capabilities in various scientific domains, from natural language processing to complex problem-solving tasks. Their ability to understand and generate human-like text has opened up new possibilities for advancing scientific research, enabling tasks such as data analysis, literature review, and even experimental design. One of the most promising applications of LLMs in this context is hypothesis generation, where they can identify novel research directions by analyzing existing knowledge. However, despite their potential, LLMs are prone to generating “hallucinations”, outputs that are plausible-sounding but factually incorrect. Such a problem presents significant challenges in scientific fields that demand rigorous accuracy and verifiability, potentially leading to erroneous or misleading conclusions. To overcome these challenges, we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that enhances LLM hypothesis generation by integrating external, structured knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured reasoning process, organizing their output as a chain of ideas (CoI), and includes a KG-supported module for the detection of hallucinations. With experiments on our newly constructed hypothesis generation dataset, we demonstrate that KG-CoI not only improves the accuracy of LLM-generated hypotheses but also reduces the hallucination in their reasoning chains, highlighting its effectiveness in advancing real-world scientific research.","Advanced Large Language Models (LLMs) such as GPT-4 (OpenAI et al. 2024) have exhibited exceptional performance in a wide range of general machine learning tasks, such as question answering (Hendrycks et al. 2021) and arithmetic computation (Cobbe et al. 2021). Recently, there has been growing interest in harnessing the reasoning capabilities of LLMs within scientific domains. These efforts have shown impressive results, with LLMs tackling complex scientific questions and often achieving, or even exceeding, human-level performance (Nori et al. 2023; Hou and Ji 2023; Stribling et al. 2024). As a result, LLMs are increasingly viewed as promising tools with the potential to significantly advance real-world scientific research, such as drug discovery, biological sequence analysis, and material design (AI4Science and Quantum 2023). Specifically, with the capability of processing and synthesizing vast amounts of text, LLMs are well-suited to accelerate the analysis of scientific literature and generate new hypotheses for potential scientific discovery (Qi et al. 2023; Zhou et al. 2024). Traditional scientific research, particularly in natural sciences like biology, typically involves a multi-step, time-consuming process from gathering literature to validating hypotheses. By generating promising research ideas directly from existing literature, LLMs have the potential to significantly streamline and reduce the time required for these labor-intensive tasks. However, despite their advanced capabilities, LLMs face criticism for generating misinformation or so-called “hallucinations”, which are responses that seem plausible but are factually incorrect (Huang et al. 2023). This issue is particularly critical in scientific research, where every reasoning step must be transparent and verifiable. In the context of hypothesis generation for natural sciences, hallucinations can easily arise if the parametric knowledge of LLMs lacks accurate scientific information. Moreover, these hallucinations are particularly challenging to detect in generated hypotheses, as they are often related to potential discoveries that have not yet been explored. To address the problems mentioned above, we propose a novel system termed KG-CoI (Knowledge Grounded Chain of Ideas), designed to enhance hypothesis generation by incorporating external knowledge from knowledge graphs (KGs). These KGs contain well-organized structured information that has been verified by existing literature. By prompting LLMs to generate a chain of ideas (CoI) through step-by-step reasoning (Wei et al. 2022), our system facilitates in-depth analysis of the input and further verification of the generated content. The KG-CoI system consists of three key modules: KG-guided context retrieval, KG-augmented chain-of-idea generation, and KG-supported hallucination detection. By linking LLM hypothesis generation to KGs, our system aligns the output with well-established scientific knowledge and ensures that the generated hypotheses are grounded in reliable information sources. To quantitatively demonstrate the effectiveness of our system, we construct a hypothesis generation dataset by masking certain links within a KG and prompting LLMs to hypothesize potential relations without prior knowledge of the facts. Our experiments show that, compared to existing methods for prompting LLMs in hypothesis generation, KG-CoI achieves the highest accuracy in generating hypotheses, underscoring its advantages in real-world scientific research. Moreover, with the KG-supported hallucination detection, we demonstrate the effectiveness of KG-CoI in reducing hallucinations, thereby improving its reliability in natural sciences. Our contributions can be summarized as follows: • We present KG-CoI, a novel LLM-enhanced hypothesis generation system that augments the generated hypotheses with external structured knowledge and presents the result as a coherent chain of ideas. • We construct a new dataset to evaluate LLM hypothesis generation and conduct extensive experiments on both open- and close-source LLMs, showing the effectiveness of KG-CoI in hypothesizing scientific knowledge. • We propose a KG-supported hallucination detection method within KG-CoI, which demonstrates the advantage of KG-CoI in reducing hallucinations."
https://arxiv.org/html/2411.02337v1,WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning,"Large language models (LLMs) have shown remarkable potential as autonomous agents, particularly in web-based tasks. However, existing LLM web agents heavily rely on expensive proprietary LLM APIs, while open LLMs lack the necessary decision-making capabilities. This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework designed to train high-performance web agents using open LLMs. WebRL addresses three key challenges in building LLM web agents, including the scarcity of training tasks, sparse feedback signals, and policy distribution drift in online learning. Specifically, WebRL incorporates 1) a self-evolving curriculum that generates new tasks from unsuccessful attempts, 2) a robust outcome-supervised reward model (ORM), and 3) adaptive reinforcement learning strategies to ensure consistent improvements. We apply WebRL to transform open Llama-3.1 and GLM-4 models into proficient web agents. On WebArena-Lite, WebRL improves the success rate of Llama-3.1-8B from 4.8% to 42.4%, and from 6.1% to 43% for GLM-4-9B. These open models significantly surpass the performance of GPT-4-Turbo (17.6%) and GPT-4o (13.9%) and outperform previous state-of-the-art web agents trained on open LLMs (AutoWebGLM, 18.2%). Our findings demonstrate WebRL’s effectiveness in bridging the gap between open and proprietary LLM-based web agents, paving the way for more accessible and powerful autonomous web interaction systems. The code, model, and data are made publicly available at https://github.com/THUDM/WebRL.","Large language models (LLMs) have exhibited not only superior comprehension of human language, commonsense reasoning, and knowledge acquisition, but also significant potential in complex planning and logical reasoning, indicating their promising trajectory towards serving as autonomous LLM agents (Wang et al., 2023; Liu et al., 2023a). A diverse array of applications for LLM agents has proliferated, encompassing domains such as code generation (Jimenez et al., 2024), database manipulation (Zhou et al., 2023b; Gu et al., 2024), and graphical user interface (GUI) interaction (Rawles et al., 2024; Yang et al., 2023; Xie et al., 2024). Among these, web agents powered by LLMs (Deng et al., 2024; Zheng et al., 2024; Lai et al., 2024; Pan et al., 2024) have garnered particular attention due to their extensive application prospects and unique potential for fostering authentic autonomous intelligence within the digital ecosystem. Notwithstanding these advancements, existing LLM web agents, regardless of their performance metrics or architectural paradigms, remain under-developed. High-performing LLM web agents predominantly rely on meticulously crafted prompts in conjunction with proprietary LLM APIs (e.g., OpenAI GPT-4) for web page comprehension and manipulation, which is both expensive and time-intensive. Conversely, open-source LLMs exhibit notable deficiencies in their capability to function as proficient web agents, primarily due to the scarcity of decision-centric data in both pre-training and post-training periods. Despite recent endeavors (Lai et al., 2024; Pan et al., 2024) to train web agents on open LLMs via imitation learning, these approaches insufficiently leverage the inherently online nature of web interactions and fail to yield consistent, continual improvements. Challenges. In this work, we propose to train high-performance web agents based on open LLMs within online environments, specifically utilizing WebArena (Zhou et al., 2023a). Our investigation has identified several critical challenges inherent to this task: 1) Insufficiency of training tasks: In contrast to offline datasets (Deng et al., 2024; Rawles et al., 2024) that facilitate agent training and evaluation on human-annotated oracle trajectories, online benchmarks such as WebArena typically provide only a limited test set for evaluation purposes. This dearth of predefined training tasks significantly impedes the effective training of agents within these environments. 2) Sparsity and cost of feedback signals: The assessment of success for arbitrary web browsing tasks is difficult in the absence of task-specific evaluation functions. Moreover, unlike tasks in certain GUI datasets (e.g., AITW (Rawles et al., 2024) and WebShop (Yao et al., 2022)), those in WebArena are typically of long horizons, with oracle solutions averaging about 10 steps. This characteristic introduces substantial sparsity in the available signals during online exploration. 3) Policy distribution drift in online learning: The absence of a predefined training set necessitates online exploration, inevitably leading to distribution drift in the agent’s policy. This phenomenon is likely to induce catastrophic forgetting and performance degradation over time. The WebRL Framework. In response to these challenges, we introduce WebRL, a self-evolving online curriculum reinforcement learning framework designed for training LLM web agents. To the best of our knowledge, this represents the first systematic framework enabling effective reinforcement learning for LLM web agents from initialization in online web environments. Through the application of WebRL, we have successfully transformed a Llama-3.1-8B model into a proficient LLM web agent, elevating its success rate (SR) on WebArena-Lite (Zhou et al., 2023a; Liu et al., 2024) from an initial 4.8% to 42.4% across a diverse set of five websites. Furthermore, when applied to Llama-3.1-70B, we achieve a remarkable 49.1% SR, significantly surpassing the performance of the most advanced proprietary LLM API (GPT-4-Turbo, 17.6% SR) and the previous state-of-the-art web agents trained on open-source LLMs (AutoWebGLM (Lai et al., 2024), 18.2% SR). Figure 2: Overview of WebRL. WebRL is a self-evolving online curriculum reinforcement learning framework for LLM-based web agents, yielding consistent continual improvements throughout the iterative self-evolution. The substantial performance gains from WebRL can be attributed to several key architectural designs. To address the scarcity of web agent training tasks, we have devised a self-evolving online curriculum that harnesses the trial-and-error process inherent in exploration. This curriculum is underpinned by a robust outcome-supervised reward model (ORM) that we have newly developed. In each training phase, novel tasks are autonomously generated from unsuccessful attempts in the preceding phase, facilitating a progressive learning trajectory. To mitigate the policy distribution shift induced by curriculum-based reinforcement learning, we incorporate a KL-divergence term between the reference and actor policies into our learning algorithm, thereby constraining policy updates and promoting stability. We implement an experience replay buffer augmented with a novel actor confidence filtering strategy to ensure the fidelity of replayed experiences and prevent over-fitting to previously acquired knowledge. The experimental results confirm the effectiveness of WebRL. In particular, the agent demonstrates improved performance when selecting past experiences of moderate difficulty—neither too simple nor too challenging relative to the agent’s current capabilities. Additionally, the use of a larger KL divergence constraint in the policy update process results in better performance when incorporating past experience. In summary, our work makes the following significant contributions to the field: • We introduce WebRL, a novel self-evolving online curriculum RL framework for training LLM-based web agents. For the first time, it implements the infrastructure for RL in the WebArena environment, together with a strong ORM, to drive open LLMs to become capable web agents. • WebRL advances the RL for LLM agent training by addressing key challenges including the scarcity of training tasks, sparsity of feedback signals, and distribution drift in online learning. The self-evolving curriculum and adaptive learning strategies allow the consistent continual improvement of LLM web agents during iteration. • We demonstrate WebRL’s substantial performance improvements over existing methodologies such as AWR and DigiRL, achieving state-of-the-art results on the WebArena-Lite benchmark. It surpasses the best proprietary LLM API and previously trained web agent on open LLMs by over 160% relatively."
https://arxiv.org/html/2411.02316v2,Evaluating Creative Short Story Generation in Humans and Large Language Models,"Storytelling is a fundamental aspect of human communication, relying heavily on creativity to produce narratives that are novel, appropriate, and surprising. While large language models (LLMs) have recently demonstrated the ability to generate high-quality stories, their creative capabilities remain underexplored. Previous research has either focused on creativity tests requiring short responses or primarily compared model performance in story generation to that of professional writers. However, the question of whether LLMs exhibit creativity in writing short stories on par with the average human remains unanswered. In this work, we conduct a systematic analysis of creativity in short story generation across LLMs and everyday people. Using a five-sentence creative story task, commonly employed in psychology to assess human creativity, we automatically evaluate model- and human-generated stories across several dimensions of creativity, including novelty, surprise, and diversity. Our findings reveal that while LLMs can generate stylistically complex stories, they tend to fall short in terms of creativity when compared to average human writers.","Storytelling lies at the core of human communication, serving as a potent means to connect and convey ideas effectively Suzuki et al. (2018). It typically demands creativity, especially when shaping a captivating and persuasive narrative. Creativity is the ability to produce novel, useful, and surprising ideas, and has been widely studied as a crucial aspect of human cognition Boden (1991); Guilford (1967); Barron (1955); Stein (1953). While humans are natural storytellers, getting machines to generate stories automatically has been a long-time challenge Meehan (1977); Lebowitz (1984). However, recently large language models (LLMs) Zhao et al. (2024) have been shown to produce high-quality short and long stories on arbitrary topics Du and Chilton (2023); Yang et al. (2022); Goldfarb-Tarrant et al. (2020a). These stories are typically evaluated by humans on their long-term coherence, relevance to the premise, repetitiveness, and general interestingness to the reader. However, the extent to which these model-generated stories are truly novel, appropriate, and surprising, remains under-studied. Only recently some works have evaluated LLM’s ability to produce creative content Tian et al. (2024); Marco et al. (2024b, a); Chakrabarty et al. (2023) and shown that models largely fall behind human writers. These works, however, focus on comparing models to professional writers and employ already existing datasets for evaluation. While LLMs are significantly less creative compared to professional writers, whether they can produce creative stories similar to those of the average person remains unclear. Figure 1: Our study setup illustrated with an example. Both humans and LLMs are asked to write a creative short story using three cue words and evaluated on several creativity metrics. Until recently, LLM creativity has generally been evaluated with tasks requiring short responses such as words or phrases. For example, many works have employed the Alternative Uses Test Guilford (1967), where people and models are asked to come up with creative uses for an everyday object like a brick and reported near-human performance results Stevenson et al. (2022); Góes et al. (2023); Hubert et al. (2024); Koivisto and Grassini (2023); Gilhooly (2023). However, the extent to which these results generalize to creativity tasks requiring longer responses remains underexplored. To bridge these gaps, in this work, we conduct a systematic analysis of creativity in short story generation in humans and LLMs. We employ a five-sentence creative short story generation task that is typically used in psychology to measure the creativity of humans Prabhakaran et al. (2014); Johnson et al. (2023); Orwig et al. (2024). In this task, the goal is to write a short creative story (4-5 sentences) based on three cue words such as stamp, letter and send. We evaluate both humans and several state-of-the-art instruction-finetuned large language models on this task and analyze their performance based on several dimensions of creativity such as novelty, surprise, diversity, and more. Our analysis shows that model-generated stories tend to employ linguistically more complex structures than humans, however, significantly fall short when it comes to novelty, diversity, and surprise compared to an average human writer."
https://arxiv.org/html/2411.02310v1,MdEval: Massively Multilingual Code Debugging,"Code large language models (LLMs) have made significant progress in code debugging by directly generating the correct code based on the buggy code snippet. Programming benchmarks, typically consisting of buggy code snippet and their associated test cases, are used to assess the debugging capabilities of LLMs. However, many existing benchmarks primarily focus on Python and are often limited in terms of language diversity (e.g., DebugBench and DebugEval). To advance the field of multilingual debugging with LLMs, we propose the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task. Further, we introduce the debugging instruction corpora MdEval-Instruct by injecting bugs into the correct multilingual queries and solutions (xDebugGen). Further, a multilingual debugger xDebugCoder trained on MdEval-Instruct as a strong baseline specifically to handle the bugs of a wide range of programming languages (e.g. “Missing Mut” in language Rust and “Misused Macro Definition” in language C). Our extensive experiments on MdEval reveal a notable performance gap between open-source models and closed-source LLMs (e.g., GPT and Claude series), highlighting huge room for improvement in multilingual code debugging scenarios111https://mdeval.github.io/.","Large language models (LLMs) (OpenAI, 2023; Touvron et al., 2023a; Yang et al., 2024a) designed for code, such as CodeLlama (Rozière et al., 2023), DeepSeekCoder (Guo et al., 2024a), and QwenCoder (Hui et al., 2024a), are highly effective at code understanding and generation. These capabilities make them particularly useful for debugging, where deep comprehension of code structure and logic is essential. Automated program repair (APR) (Wen et al., 2024) aims to automatically fix software bugs without human involvement, significantly reducing time and costs in development processes. LLMs have recently shown considerable potential in this area. For instance, CodeX (Chen et al., 2021) and GPT-4 series (OpenAI, 2023) outperforming previous conventional methods have demonstrated promising results on bug benchmarks like QuixBugs (Lin et al., 2017). The recent work DebugBench (Tian et al., 2024) creates a debugging benchmark including Python, Java, and CPP for LLM evaluation. However, for diverse programming languages in Figure 1, the multilingual debugging scenario poses more language-specific challenges for APR. Multilingual issues (e.g. “Misused Macro Definition” in programming language C, “Missing mut” in Rust, and “Unused Variable” in Go) highlight the complexities and diversities of locating and fixing bugs in the multilingual debugging scenario. Therefore, there is an urgent need to build a truly massively multilingual debugging code benchmark with a wide variety of generic and language-specific bug types. Figure 1: Massively multilingual evaluation task comprised of three tasks, including code generation, code completion, and code explanation. To further characterize the debugging performance of LLMs across different programming languages, we introduce MdEval, a framework for data construction, evaluation benchmark, and a multilingual debugging baseline xDebugCoder, to advance code debugging development. First, we propose MdEval, the first massively multilingual evaluation benchmark for code debugging covering 18 programming languages and 3.6K human-curated samples to assess the capabilities of LLMs across a wide range of languages. Further, we create MdEval-Instruct, a multilingual debugging instruction corpus in 18 languages to help the LLM fix the bug given the buggy code snippet. Specifically, we propose xDebugGen to create the buggy and correct code pair for debugging instruction tuning. The bugs are injected into the queries and solutions with our designed three strategies (1) Injecting bugs into query. (2) Injecting bugs into solution. (3) Injecting bugs with the round-trip code translation. Leveraging MdEval-Instruct, we develop xDebugCoder as a strong baseline, assessing the transferability of LLMs in multilingual debugging tasks. The contributions are summarized as follows: (1) We propose MdEval, a comprehensive multilingual code debugging benchmark consisting of 3.6K samples spanning three tasks: automated program repair (APR), code review (CR), and bug identification (BI). This benchmark covers 18 languages and includes both generic and language-specific bug types. (2) We introduce the massively multilingual code debugging instruction corpora MdEval-Instruct created by the xDebugGen. By injecting bugs into the multilingual correct query or response, we can create the pairs of buggy code and the correct code for instruction. (3) We systematically evaluate the multilingual code debugging capabilities of 15+ models on our created MdEval and create a leaderboard to evaluate them on 18 programming languages dynamically. Notably, extensive experiments suggest that comprehensive multilingual multitask evaluation can realistically measure the gap between open-source (e.g. DeepSeekCoder and Qwen-Coder) and closed-source models (e.g. GPT-4o and Claude series)."
https://arxiv.org/html/2411.02305v1,CRMArena: Understanding the Capacity of LLM Agents toPerform Professional CRM Tasks in Realistic Environments,"Customer Relationship Management (CRM) systems are vital for modern enterprises, providing a foundation for managing customer interactions and data. Integrating AI agents into CRM systems can automate routine processes and enhance personalized service. However, deploying and evaluating these agents is challenging due to the lack of realistic benchmarks that reflect the complexity of real-world CRM tasks. To address this issue, we introduce CRMArena, a novel benchmark designed to evaluate AI agents on realistic tasks grounded on professional work environments. We worked with CRM experts to design nine customer service tasks distributed across three personas: service agent, analyst, and manager. We synthesize a large-scale simulated organization, populating 16 commonly-used industrial objects (e.g., account, order, knowledge article, case) with high interconnectivity, and uploading it into a real Salesforce CRM organization. UI and API access to the CRM is provided to systems that attempt to complete the tasks in CRMArena. Experimental results reveal that state-of-the-art LLM agents succeed in less than 40% of the tasks with ReAct prompting, and less than 55% even when provided manually-crafted function-calling tools. Our findings highlight the need for enhanced agent capabilities in function-calling and rule-following to be deployed in real-world work environment. CRMArena is an open challenge to the community: systems that can reliably complete tasks showcase direct business value in a popular work environment.111Our code and benchmark have been released at https://github.com/SalesforceAIResearch/CRMArena.","Customer Relationship Management (CRM) systems are pivotal in modern enterprises, serving as the backbone for managing interactions with current and potential customers Winer (2001); Payne and Frow (2005). The integration of intelligent agents based on large language models (LLMs) into CRM systems promises to automate routine tasks, enhance operational efficiency, and revolutionize customer experiences. However, evaluating LLM agents in real-world professional environments remains a challenge, due to the absence of robust benchmarks that faithfully capture the complexity of tasks encountered in real-world CRM environments, largely due to data privacy concerns within enterprises. Prior benchmarks on evaluating LLM agents on work-related tasks, such as WorkArena Drouin et al. (2024), Workbench Styles et al. (2024), and Tau Yao et al. (2024) tend to focus on basic functionality, and fall short in two key areas. First, the complexity of the objects (e.g., tables in databases) and dependencies (e.g., foreign keys) between these objects is often overly simple, lacking the complexity of real-world scenarios. Second, the tasks included in the benchmarks, such as navigating web pages and filtering lists, are typically too straightforward and do not represent real-world work tasks. To address these limitations, we introduce CRMArena, a comprehensive benchmark tailored to evaluate LLM agents on performing realistic CRM tasks in real-world work environments. CRMArena features a realistic sandbox environment modeled after Salesforce’s schema, developed using an extensible data generation pipeline powered by LLMs (top left of Figure 1). Specifically, the pipeline tackles two key challenges: (1) Object connectivity: reflecting the complex relationships between data objects (e.g., Account associated with Case and Order) by mirroring Salesforce’s Service Cloud schema222https://architect.salesforce.com/diagrams/data-models/service-cloud/service-cloud-overview. (2) Introducing latent variables to better simulate realistic data dynamics, such as influencing case-filing behavior and modeling deviations from company guidelines. Moreover, CRMArena defines tasks based on actual customer service personas. By consulting CRM experts experienced with Salesforce, we identified nine tasks representative of CRM use cases (Section 2.1). These tasks span three personas: Service Manager, Service Agent, and Service Analyst. For example, Service Managers focus on agent performance and strategic resource allocation. Table 1 compares CRMArena with previous datasets. Figure 1: An overview of the contribution of this work. We begin by generating realistic CRM data based on the Salesforce Service Cloud schema, ensuring both quality and diversity through rigorous verification processes. This verified data is then stored locally and uploaded to a Salesforce organization (Org). An expert study, conducted with domain experts, validated the data’s realism. Using this Org as a sandbox environment, we create query instances and benchmark various LLMs across different agentic frameworks. CRMArena seamlessly integrates with Salesforce,333https://www.salesforce.com/crm/ enabling interaction via both the user interface and API access (see bottom of Figure 1). This integration facilitated an expert study with CRM professionals to assess the quality of our synthesized organization (Section 2.5). Study findings revealed that 90% of domain experts found the test environment to be Realistic or better, underscoring the benchmark’s fidelity to real-world CRM scenarios. Upon verifying the realism of CRMArena, we then assess various agentic systems through API access. We develop two sets of tools general-purpose vs. task-specific tools, combine them with three agentic frameworks and various LLMs. Findings indicate that all LLM agents struggle to reliably complete tasks when using general-purpose tools, with top performing systems completing less than 40% of the tasks. Incorporating manually designed tools can enhance performance, with top LLM agents solving up to 55% of the tasks. However, we discover that weaker LLMs often do not benefit from manually-crafted tools due to their weaker function calling capabilities. In summary, our main contributions are: • Introducing CRMArena, a realistic CRM agent benchmark with tasks validated by domain experts to evaluate LLM agents in real-world business scenarios. • Developing a data generation strategy anchored in a real-world CRM schema, incorporating latent variables, deduplication, and rigorous data validation to ensure diversity and quality. • Demonstrating through experiments that even state-of-the-art LLM agents do not reliably complete CRMArena tasks, emphasizing the benchmark’s value and challenges."
https://arxiv.org/html/2411.02280v1,The LLM Language Network:A Neuroscientific Approach for Identifying Causally Task-Relevant Units,"Large language models (LLMs) exhibit remarkable capabilities on not just language tasks, but also various tasks that are not linguistic in nature, such as logical reasoning and social inference. In the human brain, neuroscience has identified a core language system that selectively and causally supports language processing. We here ask whether similar specialization for language emerges in LLMs. We identify language-selective units within 18 popular LLMs, using the same localization approach that is used in neuroscience. We then establish the causal role of these units by demonstrating that ablating LLM language-selective units – but not random units – leads to drastic deficits in language tasks. Correspondingly, language-selective LLM units are more aligned to brain recordings from the human language system than random units. Finally, we investigate whether our localization method extends to other cognitive domains: while we find specialized networks in some LLMs for reasoning and social capabilities, there are substantial differences among models. These findings provide functional and causal evidence for specialization in large language models, and highlight parallels with the functional organization in the brain.111Code available via github.com/bkhmsi/llm-localization","Model Ablate Language Units Ablate Random Units Gemma-2B 11 liquido _ sota(.)uggoon3 jumped over the lazy lamb. Phi-3.5-Mini-Instruct AME.AME and:ough.. MAR jumps over the lazy dog. Falcon-7b SomeSReadWhenISearchSome jumps over the lazy dog. Mistral-7B-v0.3 foxfool foolfoolfoolfool jumps over the lazy dog. LLaMA-3.1-8B-Instruct _ of_An_O_of_An_O_of jumps over the lazy dog. Table 1: Disruption of Language Modeling Abilities Continuations of the prompt “The quick brown fox” across five different models, following the ablation of the top 1% of language-selective units compared to the ablation of an equivalent number of randomly selected units. The baseline generation without ablation for all models was “jumps over the lazy dog.” Recent advancements in large language models (LLMs) have revealed their potential to perform far more than language processing tasks, showcasing abilities in reasoning Sun et al. (2023), problem-solving Giadikiaroglou et al. (2024), and even mimicking aspects of human Theory of Mind Street et al. (2024). Despite these impressive feats, the internal workings of LLMs remain poorly understood, especially in relation to how specific components of these models contribute to manifesting distinct cognitive functions. The field of neuroscience has made significant strides in mapping out the functional organization of the human brain, for instance by identifying specialized cognitive networks such as the language network Fedorenko et al. (2010, 2024), the Multiple Demand network Duncan (2010); Assem et al. (2020b), and the Theory of Mind network Saxe and Kanwisher (2013), each underlying distinct cognitive behaviors. In this paper, we draw inspiration from neuroscience to investigate whether similar functional specialization exists within LLMs. Specifically, we use the same localizer experiments developed by neuroscientists to identify functional brain regions. These experiments contrast activations between target conditions of interest (e.g., sentences) and perceptually matched control conditions (see Section 3). We discover that, much like the human brain, there exists a set of units in LLMs that are critical for language processing, analogous to the human language network (Fedorenko et al., 2024, Fig. 2). We find that these units show similar response patterns as those observed in the human language areas Shain et al. (2024); Schrimpf et al. (2021), and, moreover, demonstrate selectivity for natural language compared to mathematical equations and computer code, much like the human brain Ivanova et al. (2020); Fedorenko et al. (2011, 2024). Further, ablating even a small percentage of these language-selective units results in a significant decline in language performance, demonstrated qualitatively in Table 1 and quantitatively in Figure 3 through benchmarks like SyntaxGym Gauthier et al. (2020), BLiMP Warstadt et al. (2019), and GLUE Wang et al. (2018). Finally, the language-selective units show stronger alignment with the brain’s language network compared to randomly sampled units, especially when selecting a small number of units to predict brain activity (Figs. 4, 5). Despite substantial evidence for the existence of a language network in all LLMs we tested, we only found evidence of units selective for social (Theory of Mind) and reasoning (Multiple Demand) tasks in a subset of models (Figure 6)."
https://arxiv.org/html/2411.02265v3,Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent,"In this paper, we introduce Hunyuan-Large, which is currently the largest open-source Transformer-based mixture of experts model, with a total of 389 billion parameters and 52 billion activation parameters, capable of handling up to 256K tokens. We conduct a thorough evaluation of Hunyuan-Large’s superior performance across various benchmarks including language understanding and generation, logical reasoning, mathematical problem-solving, coding, long-context, and aggregated tasks, where it outperforms LLama3.1-70B and exhibits comparable performance when compared to the significantly larger LLama3.1-405B model. Key practice of Hunyuan-Large include large-scale synthetic data that is orders larger than in previous literature, a mixed expert routing strategy, a key-value cache compression technique, and an expert-specific learning rate strategy. Additionally, we also investigate the scaling laws and learning rate schedule of mixture of experts models, providing valuable insights and guidances for future model development and optimization. The code and checkpoints of Hunyuan-Large are released to facilitate future innovations and applications.Code: https://github.com/Tencent/Tencent-Hunyuan-Large Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large","In recent years, Large language models (LLMs) have significantly advanced the field of artificial intelligence, proving their effectiveness across numerous fields such as NLP, CV, Speech, and AI4Science. Starting from the emergence of ChatGPT (OpenAI, 2022), lots of powerful LLMs have bloomed (Achiam et al., 2023; Gemini et al., 2023; Touvron et al., 2023; OpenAI, 2024; Dubey et al., 2024; Qwen, 2024a), which inexorably bring in new ways for people to collect and process information, broadly impacting our daily lives. As the demand for more sophisticated AI systems continues to grow, researchers are exploring new techniques and paradigms to push the boundaries of model size and performance. One approach that stands out is the Mixture of Experts (MoE) model, which synergizes multiple specialized submodels to deliver superior performance in diverse tasks with dynamic activated experts (Lepikhin et al., 2020; Fedus et al., 2022; Wang et al., 2024a), achieving more efficient training and inference. There is a current trend observed that more and more MoE-structured LLMs have been constructed and open-sourced to facilitate the LLM community (Mistral, 2024; DeepSeek-AI, 2024; Yang et al., 2024; Jamba et al., 2024). Tencent’s AI chatbot, Yuanbao (yuanbao.tencent.com), has also adopted MoE as the neural architecture of the trillion-parameter flagship LLM since February 2024. Due to its exceptional capabilities in reading, writing, and searching, the MoE-based Hunyuan model and Yuanbao chatbot are assisting users in working effortlessly and enjoying a more vibrant life. The MoE-powered Hunyuan models have also enhanced thousands of scenarios within Tencent’s applications, enabling Tencent to better serve its billions of users. In addition to serving users with the premium models, another way that contributes to the community is open-sourcing. Open-source models can greatly promote the spreading of technology and flourishing development of applications, as exemplified by LLama, Mistral, Qwen, and Deepseek, among others. However, most open-source models are based on dense architectures, with only a very few models based on the MoE architecture with relatively small scale of parameters. In this work, we introduce Hunyuan-Large, a large Transformer-based MoE model, featuring an unprecedented 389 billion total parameters and 52 billion activated parameters, capable of handling up to 256K tokens. This model adopts the classical Transformer architecture (Vaswani et al., 2017) with MoE, containing a pre-training stage for acquiring fundamental capabilities and a post-training stage for task-specific instruction following, capability enhancement, and human preference alignment. Hunyuan-Large supports conventional NLP abilities such as question answering, reasoning, reading comprehension, and specific LLM capabilities such as mathematics, coding, multi-turn interaction, and multilinguality. We delve into the key technical innovations that have contributed to Hunyuan-Large’s exceptional performance as follows. • High-Quality Synthetic Data. The broad usage of synthetic data improves the quality and diversity of training data, which enables the model to learn richer representations effectively and generalize better to unseen data. In total, Hunyuan-Large is pre-trained on 7T tokens, which contains nearly 1.5T tokens of high-quality and diverse synthetic data. • Enhanced Model Structure. We propose key-value (KV) cache compression, recycle routing, and expert-specific learning rate scaling strategies to enhance Hunyuan-Large. The reduction of KV cache overhead allows for more seamless deployment and scaling. Moreover, we adopt different learning rates for different shared/specialized experts with our recycle routing strategy, ensuring that each token can be utilized effectively during training and contributing to the overall performance. • Explorations on MoE Scaling Laws. Additionally, we explore the scaling laws of MoE models as our guidelines, highlighting the relationship between model size, training data, and performance. This analysis offers insights into the foundational elements that contribute to the strong performance of Hunyuan-Large, but also provides valuable insights for future development and optimization of more powerful and larger MoE-structured LLMs. To demonstrate the power of Hunyuan-Large, we conduct extensive experiments on diverse types of benchmarks in both English and Chinese, compared with the best-performing dense and MoE models having similar parameter sizes. We find that Hunyuan-Large is capable of handling various tasks including commonsense understanding, question answering, mathematics reasoning, coding, and aggregated tasks, achieving the overall best performance among existing open-source similar-scale LLMs. The pre-trained and post-trained Hunyuan-Large models are publicly released to facilitate the LLM community. In the rest of this technical report, we will first give a detailed introduction to the pre-training stage of Hunyuan-Large, including its data and tokenizer, model structure, and pre-training recipes in Section 2. Next, we will describe our post-training in Section 3, with details of our SFT and RLHF techniques. The comprehensive experimental results and in-depth analyses of Hunyuan-Large’s pre-trained and post-trained models will be given in Section 4. Finally, the conclusion and future direction will be stated in Section 5."
https://arxiv.org/html/2411.02117v1,AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis,"The evaluation of layer importance in deep learning has been an active area of research, with significant implications for model optimization and interpretability. Recently, large language models (LLMs) have gained prominence across various domains, yet limited studies have explored the functional importance and performance contributions of individual layers within LLMs, especially from the perspective of activation distribution. In this work, we propose the Activation Variance-Sparsity Score (AVSS), a novel metric combining normalized activation variance and sparsity to assess each layer’s contribution to model performance. By identifying and removing approximately the lowest 25% of layers based on AVSS, we achieve over 90% of original model performance across tasks such as question answering, language modeling, and sentiment classification, indicating that these layers may be non-essential. Our approach provides a systematic method for identifying less critical layers, contributing to efficient large language model architectures.","The evaluation of layer importance in deep learning models has become a critical area of research, with applications ranging from model compression to interpretability. Understanding which layers are essential to model performance can lead to significant improvements in computational efficiency and model design. Recently, large language models (LLMs) have emerged as powerful tools in a wide range of applications, including question answering, language translation, and sentiment analysis. Despite their popularity and widespread use, limited work has been done to investigate the functional importance and performance contributions of individual layers within LLMs, particularly from the perspective of activation distribution. (Wang et al., 2024; Xiong et al., 2020) Recent advancements in evaluating layer importance have introduced several sophisticated methodologies. For instance, Saarela et al. (Saarela and Jauhiainen, 2021) proposed Gradient-Based Importance Scores(GBIS), which utilize gradient information to assess layer importance by calculating the sensitivity of gradients relative to the input. This method effectively reflects the model’s reliance on the activations of each layer for its predictions. Additionally, Zopf et al. (Bach et al., 2015) introduced Layer-wise Relevance Propagation (LRP), including its variants, to analyze the flow of information in complex neural networks, providing a more nuanced understanding of each layer’s contribution to the model’s decisions. Furthermore, the work of Mencía et al. (Zopf et al., 2016) highlighted the significance of Contextual Importance Measures(CIM), which integrate contextual information to dynamically evaluate the importance of each layer based on specific input conditions, thus overcoming the limitations of static assessment methods. However, these approaches often struggle to fully capture the intricate activation distributions and redundancy within large language models, limiting their effectiveness in identifying less critical layers. In this work, we propose a novel approach for assessing layer importance in LLMs using a metric we term the Activation Variance-Sparsity Score (AVSS). AVSS combines normalized activation variance and sparsity to quantify the contribution of each layer to the overall model performance. By ranking layers based on AVSS and removing approximately the lowest 25% of layers, our method demonstrates that LLMs can retain over 90% of their original performance across a variety of tasks, including question answering, language modeling, and sentiment classification. This finding suggests that certain layers within LLMs may have redundant functions that do not significantly impact model efficacy. (Hoffmann et al., 2022; Kaplan et al., 2020; Lai et al., 2017; Ashkboos et al., 2024) The main contributions of our paper are as follows: • We introduce the Activation Variance-Sparsity Score (AVSS) as a new metric for layer importance evaluation in LLMs, providing a refined assessment over traditional norms. • We demonstrate that removing AVSS-identified redundant layers retains over 90% performance across diverse tasks, highlighting a method for efficient layer pruning. • We provide insights into the functional distribution of LLM layers, contributing a systematic approach for identifying less critical layers and enhancing model interpretability. Figure 1. Illustration of the Activation Variance-Sparsity Score (AVSS) method for assessing layer importance in large language models. (a) Layer Structure: Overview of model layers (1 to 32) analyzed for activation properties. (b)Activation Variance and Sparsity: Top: High-variance layers capture diverse information. Bottom: Darker cells indicate sparse activations, suggesting redundancy. (c) AVSS Calculation and Ranking: AVSS, normalized AVSS, and cumulative AVSS formulas are used to rank layers, identifying low-scoring layers as pruning candidates."
https://arxiv.org/html/2411.02083v1,"Regress, Don’t Guess – A Regression-like Loss on Number Tokens for Language Models","While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving reasoning over quantities, especially arithmetics. This has particular relevance in scientific datasets where combinations of text and numerical data are abundant. One fundamental limitation is the nature of the CE loss, which assumes a nominal (categorical) scale and thus cannot convey proximity between generated number tokens. As a remedy, we here present two versions of a number token loss. The first is based on an Lpsubscript𝐿𝑝L_{p}italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT loss between the ground truth token value and the weighted sum of the predicted class probabilities. The second loss minimizes the Wasserstein-1 distance between the distribution of the predicted output probabilities and the ground truth distribution. These regression-like losses can easily be added to any language model and extend the CE objective during training. We compare the proposed schemes on a mathematics dataset against existing tokenization, encoding, and decoding schemes for improving number representation in language models. Our results reveal a significant improvement in numerical accuracy when equipping a standard T5 model with the proposed loss schemes.","Figure 1: Left: xVal [7] decodes numbers through a regression head carried alongside the regular token head, gated through the [NUM] token (figure reproduced with permission). Right: Instead, the Number Token Loss (NTL) circumvents the need for two heads and allows the computation of a regression loss directly on the token head. We propose two schemes to achieve this: ℒℒ\mathcal{L}caligraphic_LNTL-MSE (right) leverages a dot product of the values of the number tokens and their class probabilities. The ℒℒ\mathcal{L}caligraphic_LNTL-WAS (left) uses the Wasserstein-1 distance of the (sorted) number token labels and their class probabilities. As coined by Thawani et al. [14], numbers in natural texts are ubiquitous and important, yet systematically neglected by language models (LMs). Even worse, while Transformers [15] were invented for NLP, they have permeated various scientific domains (chemistry, biology, etc [2, 8, 1]), where tabular/numerical data is more prevalent than in NLP and often even fundamental for constructing task definitions: Molecules are labeled with drug efficacy, chemical reactions with yield, and synthesis procedures are natural text interspersed with quantities and times. Still, LMs notoriously struggle even with simple arithmetic tasks like three-digit multiplication [5] for multiple reasons: 1. Tokenization: Standard subword tokenization splits numbers into arbitrary tokens, disrupting their structure. Mitigation strategies include scientific notation [18] or digit-level tokenization [6], which may also preserve the decimal order of each digit [1]. 2. Embedding: Canonically, the model has to recover the structure of numbers from data because the embeddings of numerical tokens are learned like any other token. Countless flavors of numeracy-preserving word embeddings exist [13, 1, 7], often akin to positional encodings. 3. Training objective: The standard cross-entropy (CE) loss assumes a nominal scale, thus it fails to convey the proximity between numbers, effectively inducing a semi-supervised setting. For example, predicting a [3] instead of a [2] token will not generally induce lower loss than a [9]. This problem has been surprisingly neglected and is the focus of this work. Here, we aim to equip LMs with better inductive biases to handle combinations of textual and numerical data, such as math word problems or scientific datasets. In particular, we propose two versions of a regression loss on number tokens that respect numerical proximity (cf. Figure 1 right) and can be effectively combined with regular CE. The first version of this loss computes the Mean Squared Error (MSE) between the sum of the predicted class probabilities, weighted by their respective numerical token value, and the numerical token value of the label. The second version computes the Wasserstein distance between the distribution of the predicted number probabilities and the ground truth distribution, which is the one-hot encoding of the label. We integrate these improved training objectives with existing solutions for tokenization and embedding, in particular the Regression Transformer [1]. We evaluate all methods on a subset of the mathematical-question-answer dataset from DeepMind [12]. Prior art for joint language-number modeling suggested the use of verifiers [3, 10], calculators (typically: Python interpreters), or chain-of-thought (CoT) reasoning [19] to yield improved performance in Large Language Models (LLMs). We argue that all such strategies avoid the fundamental, underlying problem (i.e., number representation in LMs is poor) by reformulating the task, trying to correct answers a posteriori with calculators, or using significantly more compute (CoR). Therefore, we herein intentionally attempt to improve a classic, relatively small encoder-decoder LM with up to 220M parameters, namely T5 [11]."
https://arxiv.org/html/2411.02063v1,Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention,"Improving the effectiveness and efficiency of large language models (LLMs) simultaneously is a critical yet challenging research goal. In this paper, we find that low-rank pre-training, normally considered as efficient methods that will compromise performance, can be scalably effective when reduced parameters are precisely targeted. Specifically, applying the low-dimensional module only to the attention layer – resolves this issue and enhances both effectiveness and efficiency. We refer to this structure as Low-dimensional Projected Attention (LPA) and provide an explanatory analysis. Through extensive experimentation at parameter scales of 130M, 370M, and scaling up to 3B, we have validated the effectiveness and scalability of LPA. Our results show that LPA model can save up to 12.4% in time while achieving an approximate 5% improvement in test perplexity (ppl) and on downstream tasks compared with the vanilla Transformer.","Improving large language models’ (LLMs) Bommasani et al. (2021); Han et al. (2021); Brown et al. (2020); Touvron et al. (2023); Zhou and Ding (2024) effectiveness and efficiency simultaneously presents challenges due to inherent trade-offs, which remains a critical research goal in the research field. Among series methods proposed to alleviate this issue, parameter-efficient fine-tuning Houlsby et al. (2019); Li and Liang (2021); Zaken et al. (2021); Ding et al. (2023b) offer valuable insights. Notably, low-rank or low-dimension techniques such as LoRA Hu et al. (2021) demonstrate on-par or even enhanced performance over traditional full-parameter fine-tuning with reduced computational resources. Intuitively, besides the fine-tuning phase, adapting LoRA’s principles to the pre-training phase through low-rank decomposition is both viable and promising, which can yield substantial benefits if effectiveness is maintained. However, existing studies have found that the direct low-rank pre-training often compromises the effectiveness. To reduce such effects, strategies such as iteratively accumulating low-rank updates Lialin et al. (2023) or integrating low-rank decomposition directly into the gradient Zhao et al. (2024) have been suggested. Whether it’s the original LoRA or these improved methods, they all involve performing low-rank decomposition and updates on ""amounts of change"" (weights or gradients), and do not reduce the number of parameters in the model itself, which face obstacles in maintaining efficiency during subsequent inference and fine-tuning stages. Therefore, an ideal scenario would be permanently reducing the number of parameters (computational load) through efficient methods, without compromising or even enhancing the performance of pre-trained models. To achieve this goal, is it feasible to directly perform low-rank decomposition on the matrices in the model itself, rather than on the changes? Current limited research suggests that existing low-rank pre-training methods experience performance losses and uncertainties Lialin et al. (2023); Zhao et al. (2024), with even fewer studies exploring more direct approaches. However, in this paper, we demonstrate that such direct low-rank pre-training is feasible, provided that the parameters to be reduced are more precisely targeted. Specifically, we describe the reduction of parameters as replacing the original matrices with low-dimensional modules. We find that using low-dimensional modules in the feed-forward neural (FFN) layers or across all layers negatively impacts the model’s effectiveness. However, we observe that employing them in the attention layers consistently allows the model to outperform the original Transformer. We refer to this structure as Low-dimensional Projected Attention (LPA), provide an explanation, and experimentally demonstrate its ability to reliably enhance both the efficiency and effectiveness of the model. We validate the effectiveness of the LPA model on two Transformer model configurations, assessing both pre-training and downstream task performance. With a particular focus on the scalability of LPA model, we observe that it remains effective even when the model parameters scale up to 3B. Furthermore, our study explores the effects of the hyperparameter on LPA, the necessity of integrating the low-dimensional module into every sublayer of the attention layer, and how to distribute any extra parameters effectively. The code of this work will be publicly available at https://github.com/TsinghuaC3I/LPA."
https://arxiv.org/html/2411.02036v1,Explainable cognitive decline detection in free dialogues with a Machine Learning approach based on pre-trained Large Language Models,"Cognitive and neurological impairments are very common, but only a small proportion of affected individuals are diagnosed and treated, partly because of the high costs associated with frequent screening. Detecting pre-illness stages and analyzing the progression of neurological disorders through effective and efficient intelligent systems can be beneficial for timely diagnosis and early intervention. We propose using Large Language Models to extract features from free dialogues to detect cognitive decline. These features comprise high-level reasoning content-independent features (such as comprehension, decreased awareness, increased distraction, and memory problems). Our solution comprises (i) preprocessing, (ii) feature engineering via Natural Language Processing techniques and prompt engineering, (iii) feature analysis and selection to optimize performance, and (iv) classification, supported by automatic explainability. We also explore how to improve Chatgpt’s direct cognitive impairment prediction capabilities using the best features in our models. Evaluation metrics obtained endorse the effectiveness of a mixed approach combining feature extraction with Chatgpt and a specialized Machine Learning model to detect cognitive decline within free-form conversational dialogues with older adults. Ultimately, our work may facilitate the development of an inexpensive, non-invasive, and rapid means of detecting and explaining cognitive decline.","Progressive neurological disorders (e.g., Alzheimer’s disease) affect 40 million people worldwide [1] and are a common cause of death [2, 3]. However, only 25 %times25percent25\text{\,}\mathrm{\char 37\relax}start_ARG 25 end_ARG start_ARG times end_ARG start_ARG % end_ARG of affected people receive a diagnosis. There are multiple reasons for this, including stigma and a lack of awareness and resources [4, 5]. The number of older adults with Alzheimer’s disease is predicted to rise to 150 million by 2050 [6]. Consequently, detecting pre-illness stages and analyzing the progression of neurological disorders using cost-effective and efficient intelligent systems is crucial to ensure timely diagnosis, risk assessment, and early intervention [3, 7]. Numerous cognitive tests (ct, e.g., Alzheimer’s Disease Assessment Scale-Cognition - adascog, Cognitive Dementia Rating - cdr, Mini-Mental State Examination - mmse, Montreal Cognitive Assessment - Moca, etc.) are currently used to diagnose and monitor neurological disorders, but they need to be applied manually and are therefore costly [5, 8, 9]. These tests can be automated with the help of Artificial Intelligence (ai), enabling more frequent screening of target populations [10, 11] and the conduct of longitudinal studies. The proposal described in this work consists of an automatic, continuous evaluation of cognitive performance based on engaging dialogues established with end users. Engagement fosters evaluation over time, which is of interest to longitudinal studies. The dialogues are supported by advanced ai techniques. ai-based solutions for clinical assessment purposes is a fast-growing research field, and numerous studies have already analyzed applications in the fields of progressive neurological disorders and dementia [12, 13, 14, 15, 16]. Machine Learning (ml) models [1, 17, 18, 19] (e.g., Convolutional Neural Networks - cnn [20]) and Natural Language Processing (nlp) techniques [21, 22, 15, 23] have been applied to textual and voice data. These techniques can effectively predict cognitive impairment and content-dependent and context-independent features can be leveraged to improve the predictive performance of ml models in this area. Among the multiple bio-markers available for the study of cognitive decline [24, 25, 26], language acquisition is an inexpensive, non-invasive, and readily accessible tool [9]. However, language conceals large volumes of information within complex relationships that can be difficult to decipher. Large Language Models (llms) are better equipped to navigate and process complex information and have therefore received increasing attention in the medical field [1, 27, 28]. llms have been used to analyze images and prescribe medical treatments and have also proven capable of passing medical accreditation exams [29]. However, personalized medical treatment recommendations provided by llms remain unreliable [30]. llms have broader language generation and understanding capabilities [9, 31], compared to domain-dependent models, when adequate prompt engineering is applied [32]. One noteworthy recent example of llm enhanced text generation capabilities is gpt-4 [33]. This llm is used by Chatgpt [34] to generate coherent dialogues with human users. However, there is still limited research on whether llms are better than niche solutions based on highly focused, context- or domain-dependent ml models, particularly in the clinical field [33]. Our approach combines both approaches. It employs an llm to create a friendly conversational assistant environment. The dialogue is augmented by additional services (e.g., weather reports, and medication reminders) to generate interest among the end users and enable a transparent longitudinal evaluation of cognitive decline. This llm is also used to generate high-level reasoning and content-independent side features for a specialized ml model. This combined usage of an llm and ml model for conducting explainable cognitive evaluation is an entirely novel contribution. In general, establishing trust in ai is necessary to fight the common perception that models are black boxes, leaving end users and developers in the dark about their decision-making processes [35]. This issue is especially relevant in medical practice, particularly in personalized treatments [9]. Explainable ai (xai) [36] exploits the intrinsic interpretability of certain ml algorithms (e.g., tree-based models such as Random Forest - rf), or implements methods to bypass the opacity of non-interpretable models [37]. xai techniques include feature importance [38], counterfactual explanations [39], natural language descriptions [40], and visual representations [41]. Explainability in llm models is far from being solved. Our proposal combines llms with explainable ml classifiers to generate an explanation of the predictions that caregivers or healthcare experts will be able to understand. Summing up, the main contribution of this work is a hybrid ml solution for assessing cognitive state, which combines context-dependent features with an llm for generating high-level reasoning, context-independent features. The latter provides high-level reasoning on the behavior of a user who dialogues with a conversational assistant on a leisure topic. Moreover, these features greatly enhance the explainability of the system decisions about cognitive state compared to rigid content-dependent features. The rest of this paper is organized as follows. Section 2 reviews the relevant competing works on cognitive decline detection involving llms, and Section 2.1 summarizes the contribution of this work beyond state of the art. Section 3 explains the proposed solution. Section 4 describes the experimental data set, our implementations, and the results obtained. Finally, Section 5 concludes the paper and proposes future research."
https://arxiv.org/html/2411.02018v1,Shortcut Learning in In-Context Learning: A Survey,"Shortcut learning refers to the phenomenon where models employ simple, non-robust decision rules in practical tasks, which hinders their generalization and robustness. With the rapid development of large language models (LLMs) in recent years, an increasing number of studies have shown the impact of shortcut learning on LLMs. This paper provides a novel perspective to review relevant research on shortcut learning in In-Context Learning (ICL). It conducts a detailed exploration of the types of shortcuts in ICL tasks, their causes, available benchmarks, and strategies for mitigating shortcuts. Based on corresponding observations, it summarizes the unresolved issues in existing research and attempts to outline the future research landscape of shortcut learning.","In recent years, Large Language Models (LLMs) have emerged as a hotly pursued research direction, with the advent of major language models such as T5 Raffel et al. (2020), LLaMA Touvron et al. (2023), PaLM Chowdhery et al. (2023), GPT-3 Brown et al. (2020), Qwen2 Yang et al. (2024a), and GLM Zeng et al. (2023). LLMs demonstrate the ability of In-Context Learning (ICL), meaning they can learn from several demonstration examples within a given context Dong et al. (2022). However, an increasing number of studies have shown that LLMs are susceptible to shortcut learning in ICL and perform poorly in many aspects Du et al. (2024). Figure 1: An example of shortcut learning in ICL. Shortcuts refer to decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions. Shortcut learning is the process by which a model exploits these decision rules during execution, leading to robust output results Geirhos et al. (2020). In the literature, shortcut learning is also referred to as superficial/spurious correlations Zhou et al. (2024c), the Clever Hans effect Lapuschkin et al. (2019), and various types of biases 111In our survey, bias refers to systematic errors within LLMs, rather than cultural or social biases that are studied in the field of LLM safety and fairness Zheng et al. (2024)., such as learning bias Du et al. (2024), label bias Fei et al. (2023), selection biases Wei et al. (2024), co-occurrence bias Kang and Choi (2023), and so forth. In practice, if LLMs establish shortcut ‘Flowers→→\rightarrow→Position’ based on given samples in Figure 1, predictions may fail when confronted with ‘Negative’ samples. Apart from this mapping from lexicon to specific labels, shortcuts also exist widely in various forms across different NLU tasks. For example, in Natural Language Inference (NLI) Zhou and Bansal (2020), Question-Answering (QA) Sen and Saffari (2020), and Reading Comprehension (RC) Lai et al. (2021), there is a preference for overlapping parts of two input branches. These unhealthy shortcuts have been widely proven to be harmful to the robustness, generalization, and fairness, and may lead to hallucinations in LLMs Li et al. (2023b); Ji et al. (2023). Given the rapid development of LLMs and the impact of shortcut learning on them, our survey aims to systematically summarize, discuss, and consolidate the existing research, and to sensitize the community to the current advancements. Different from previous reviews on shortcut learning Ho et al. (2022); Du et al. (2024); Dogra et al. (2024); Bihani and Rayz (2024), our survey focuses on LLMs, while prior research primarily centered on Lightweight Pre-trained Language Models (LPLMs) such as BERT Devlin et al. (2019) and RoBERTa Liu et al. (2019). Considering the differences in the training and application of LLMs, we deem it necessary to reassess the types of shortcuts, the causes of shortcut learning, benchmarks, and mitigation strategies, and to highlight the unresolved issues surrounding shortcut learning in LLMs. The organization of sections is as follows. \pgfmathresultptShortcut Learning in ICL\pgfmathresultptTypes § 2\pgfmathresultptCauses § 3\pgfmathresultptBenchmarks § 4\pgfmathresultptMatigation § 5\pgfmathresultptInstinctive Shortcuts § 2.1\pgfmathresultptAcquired Shortcuts § 2.2\pgfmathresultptLLMs training § 3.1\pgfmathresultptSkewed demonstrations § 3.2\pgfmathresultptLLMs size § 3.3\pgfmathresultptDatasets § 4.1\pgfmathresultptMetrics § 4.2\pgfmathresultptData-centric § 5.1\pgfmathresultptModel-centric § 5.2\pgfmathresultptPrompt-centric § 5.3\pgfmathresultpt Vanilla-label bias Zhao et al. (2021); Holtzman et al. (2021); McCoy et al. (2023); Kang and Choi (2023); Pan et al. (2023); Zheng et al. (2024), Context-label bias Lu et al. (2022); Razeghi et al. (2022); McCoy et al. (2023); Sclar et al. (2024); Wang et al. (2024); Pezeshkpour and Hruschka (2024); Zhang et al. (2024b); Wei et al. (2024), Domain-label bias Fei et al. (2023); Si et al. (2023a); Balepur et al. (2024), Reason bias Ali et al. (2024); Li et al. (2024b); Wu et al. (2024); Yang et al. (2024c). \pgfmathresultpt Lexicon Tang et al. (2023); Si et al. (2023b); Sun et al. (2024); Yuan et al. (2024); Zhou et al. (2024d); Pacchiardi et al. (2024), Concept Zhou et al. (2024c, d), Overlap Levy et al. (2023); Sun et al. (2024); Yuan et al. (2024), Position Zhao et al. (2021); Levy et al. (2023); Li et al. (2024b); Yuan et al. (2024), Text Style Tang et al. (2023); Yuan et al. (2024); Zhou et al. (2024d), Group Dynamics Gupta et al. (2023); Balepur et al. (2024); Han et al. (2024). \pgfmathresultpt Pretraining Han et al. (2022); Razeghi et al. (2022); McCoy et al. (2023); Kang and Choi (2023); Qi et al. (2023); Fei et al. (2023); Jiang et al. (2023); Zhou et al. (2024d); Ju et al. (2024), Instruction Tuning Sun et al. (2024). \pgfmathresultpt Tang et al. (2023); Si et al. (2023b); Levy et al. (2023). \pgfmathresultpt Razeghi et al. (2022); Kang and Choi (2023); Pan et al. (2023); Zhou et al. (2024d); Sclar et al. (2024); Tang et al. (2023); Balepur et al. (2024); Yuan et al. (2024); Han et al. (2024). \pgfmathresultpt Use or modify the existing dataset Zhao et al. (2021); Fei et al. (2023); Gupta et al. (2023); Tang et al. (2023); Si et al. (2023b); Sun et al. (2024); Zhou et al. (2024c). \pgfmathresultpt ConvRe Qi et al. (2023), ShortcutQA Levy et al. (2023), Shortcut Maze Zhou et al. (2024d), Shortcut Suite Yuan et al. (2024), EUREQA Li et al. (2024a), MMLU-Pro+ Taghanaki et al. (2024), ReWild Yang et al. (2024c). \pgfmathresultpt AUC score Chen et al. (2023), Accuracy and F1 score Tang et al. (2023); Fei et al. (2023), Hits@n Kang and Choi (2023); Pezeshkpour and Hruschka (2024), Exact Match score Zhang et al. (2024a); Wang et al. (2023a). Perfomance Changes Zhou et al. (2024c); Han et al. (2024); Zhou et al. (2024d). Fluctuation Rate Wei et al. (2024). Conflict Rate Zheng et al. (2024). Shortcut Selection Ratio Taghanaki et al. (2024). \pgfmathresultpt Kang and Choi (2023); Zhou et al. (2024c); Nakada et al. (2024). \pgfmathresultpt Model pruning Ju et al. (2024); Yang et al. (2024b); Ali et al. (2024); Zhou et al. (2024b), Calibration Zhao et al. (2021); Holtzman et al. (2021); Han et al. (2022); Fei et al. (2023); Jiang et al. (2023); Zhou et al. (2024a); Jang et al. (2024); Zheng et al. (2024); Zhao et al. (2024b). \pgfmathresultpt Shortcut-based method Wang et al. (2023a); Zhou et al. (2024c), Instruction format-based method Si et al. (2023a); Pezeshkpour and Hruschka (2024); Wang et al. (2024); Sun et al. (2024); Qi et al. (2023); Yuan et al. (2024); Zhang et al. (2024a), Prompt search-based method Lu et al. (2022); Chen et al. (2023); Gonen et al. (2023). Figure 2: Taxonomy of shortcut learning in ICL. In Section 2, we summarize and generalize all the types of shortcuts that have been studied in LLMs. In Section 3, we summarize the possible causes of shortcuts in LLMs. In Section 4, we present the benchmarks commonly used in the evaluation tasks for shortcuts in LLMs. In Section 5, we provide mitigation strategies for different types of shortcuts. In Section 6, we conclude and discuss the existing research, and attempt to showcase potential directions for shortcut learning in LLMs. The detailed taxonomy is shown in Figure 2."
https://arxiv.org/html/2411.01996v1,Culinary Class Wars: Evaluating LLMs using ASHin Cuisine Transfer Task,"The advent of Large Language Models (LLMs) have shown promise in various creative domains, including culinary arts. However, many LLMs still struggle to deliver the desired level of culinary creativity, especially when tasked with adapting recipes to meet specific cultural requirements. This study focuses on cuisine transfer-applying elements of one cuisine to another-to assess LLMs’ culinary creativity. We employ a diverse set of LLMs to generate and evaluate culturally adapted recipes, comparing their evaluations against LLM and human judgments. We introduce the 𝒜⁢𝒮⁢ℋ𝒜𝒮ℋ\mathcal{ASH}caligraphic_A caligraphic_S caligraphic_H (authenticity , sensitivity , harmony ) benchmark to evaluate LLMs’ recipe generation abilities in the cuisine transfer task, assessing their cultural accuracy and creativity in the culinary domain. Our findings reveal crucial insights into both generative and evaluative capabilities of LLMs in the culinary domain, highlighting strengths and limitations in understanding and applying cultural nuances in recipe creation. The code and dataset used in this project is openly available in https://github.com/dmis-lab/CulinaryASH/.","Culinarians around the globe are striving to innovate new recipe ideas. Such creative process involves exploring novel ingredient pairings (Park et al., 2019), combinations (Gim et al., 2022) or adjusting ingredient proportions (Choi et al., 2023). Meanwhile, cuisine is a cultural embodiment of culinary arts that can be associated with a specific region, religion, or history (Kocevski and Risteski, 2020; Nguyen et al., 2022, 2023; Palta and Rudinger, 2023). It defines the style of handling ingredients to create dishes typically recorded as recipes. As the culinary world is becoming increasingly culturally interconnected, cuisine transfer, which involves applying the elements of the source cuisine to base recipe of target cuisine, has become a well-established practice for creating new recipe possibilities (Shin et al., 2024; Markham et al., 2023). As cuisine transfer can be treated as a sub-task of textual recipe generation, we avert our focus towards the recent technical advancements in generative large language models (LLMs) (Team et al., 2024; Touvron et al., 2023; Dubey et al., 2024; Jiang et al., 2023; OpenAI, 2024). Conventional LLMs possess the ability to understand users’ instruction-based prompts and generate desirable responses based on their massively parameterized knowledge (Ouyang et al., 2022; Chung et al., 2024; Rafailov et al., 2024). They are also capable of evaluating LLM-generated texts which helps relieve the burden of manual evaluation by humans Min et al. (2023). These advancements have led to the advent of automatic evaluation frameworks which are now a well-established research topic. Regional Cuisines (30) Religious Cuisines (6) Historical Cuisines (4) Base Food Dishes (20) Algerian Peruvian British Buddhist Aztec Barbecued meat Pancake Egyptian Southern US French Hindu Byzantine Burger Pasta … … … … Table 1: Truncated List of cuisines and base food dishes used in this study with numbers indicating the total number of items in each category. The complete list can be found in the appendix, Table 5. Despite these circumstances, there have been seldom previous works that attempted to develop a LLM-driven evaluation framework especially in the culinary domain. This is mainly due to cooking being more related to creativity rather than factuality where the latter has been widely explored by previous works Jeong et al. (2024); Bishop et al. (2024); Tang et al. (2024); Chiang and Lee (2024); Zheng et al. (2023); Manakul et al. (2023). Evaluating generated recipes based on cooking creativity is a non-trivial task Choi et al. (2024); Yagcioglu et al. (2018); Liu et al. (2020); H. Lee et al. (2020); Chandu et al. (2019); Le et al. (2023); Antognini et al. (2023) which presents a novel challenge when trying to employ automatic evaluation approaches in the culinary domain. It is also important to note that almost none of the automatic frameworks have been deployed in creativity-oriented generation tasks. To address this issue, we designed an automatic evaluation framework that comprises LLMs as recipe generators (H. Lee et al., 2020) and evaluators. As creativity stems from knowledge (Kenett, 2024), our aim is to comprehensively investigate both the generative and evaluation capabilities of currently released LLMs under a specific task called cuisine transfer. By analyzing the LLM-generated recipe texts and evaluation results, we derive implications of the current state of LLMs’ culinary knowledge. A generator LLM with limited culinary creativity span would repeatedly use the cuisine-specific ingredients across different base recipes lacking variation. An evaluator LLM fixated towards a cuisine-specific ingredient, would assign high ratings to generated recipes simply for including it, regardless of its culinary essence. To the best of our knowledge, this is the first attempt to develop an evaluation framework tailored for the culinary domain. We constructed the 𝒜⁢𝒮⁢ℋ𝒜𝒮ℋ\mathcal{ASH}caligraphic_A caligraphic_S caligraphic_H benchmark dataset and LLM generation-evaluation framework tailored to the cuisine transfer task. We propose a novel criteria 𝒜⁢𝒮⁢ℋ𝒜𝒮ℋ\mathcal{ASH}caligraphic_A caligraphic_S caligraphic_H to evaluate the generated recipes which are authenticity , sensitivity and harmony . We present our findings that were derived from in-depth investigation on both recipe generators and evaluators in this work."
https://arxiv.org/html/2411.01855v1,Can Language Models Learn to Skip Steps?,"Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and model behaviors. In this work, we study the ability to skip steps in reasoning—a hallmark of human expertise developed through practice. Unlike humans, who may skip steps to enhance efficiency or to reduce cognitive load, models do not inherently possess such motivations to minimize reasoning steps. To address this, we introduce a controlled framework that stimulates step-skipping behavior by iteratively refining models to generate shorter and accurate reasoning paths. Empirical results indicate that models can develop the step skipping ability under our guidance. Moreover, after fine-tuning on expanded datasets that include both complete and skipped reasoning sequences, the models can not only resolve tasks with increased efficiency without sacrificing accuracy, but also exhibit comparable and even enhanced generalization capabilities in out-of-domain scenarios. Our work presents the first exploration into human-like step-skipping ability and provides fresh perspectives on how such cognitive abilities can benefit AI models.","The pursuit of Artificial General Intelligence (AGI) is profoundly influenced and inspired by human intelligence [35, 6]. Trained extensively on human language, language models not only excel in various tasks, but also begin to exhibit emergent human-like abilities that are not explicitly engineered into them [24]. Among these, reasoning stands out as a core human-like cognitive ability, and has demonstrated great potential in a wide range of problem solving scenarios [47, 11, 30, 37, 28, 34]. Despite their advances in displaying human-like cognitive activities, huge gaps remain in how models and humans actually behave [22, 46, 20]. These differences bring up interesting questions regarding the exploration and development of similar capabilities between models and humans. We aim to investigate whether the models exhibit any reasoning abilities unique to human experts, and whether they can evolve from beginners to reasoning experts. When humans learn to reason, beginners typically start with detailed, step-by-step solutions to imitate the gradual process of problem solving. As practice makes perfect, human experts not only solve problems more swiftly but also utilize shorter mental pathways, often skipping steps in their reasoning process [36]. This particular ability helps them speed up the reasoning and saves cognitive load for more challenging steps [44]. As demonstrated in Figure 1, the step-skipping behavior illustrated on the right side is commonly adopted by human experts during equation simplification. In this work, we are curious whether models exhibit mature human-like reasoning ability — skipping steps, and how such abilities can influence the model’s reasoning behaviors. Unlike humans, models do not inherently possess the intrinsic motivation like time limit or skill maturity that naturally drives efficiency in cognitive tasks. To induce the skipping step behavior in models, we introduce a controlled training environment where models are instructed to generate reasoning sequences within a specified number of steps. Our method includes two phases: initialization and iteration. We begin with a dataset that contains complete stepwise reasoning processes for the questions. In initialization, models are first trained to solve the tasks comprehensively, adhering to the full sequence of reasoning steps. In Figure 1, the illustration on the left demonstrates how models are trained to follow a specified number of steps. Then in the iteration phase, the models are prompted to produce shorter answers based on the original training data (Figure 1 right). We then select the shorter reasoning paths that still achieve correct answers and mix them with the full-step reasoning paths. This expanded dataset is used to train a new model to have advanced step-skipping capabilities. Each iteration refines the model’s ability to identify how steps can be skipped without sacrificing accuracy. Finally, we fine-tune the models using these iteratively generated datasets, including data instances that demonstrate successful step-skipping during each iteration. Figure 1: Step skipping in equation simplification. We use the specified number of steps in the input as a stimulation to induce the model to perform skipping by using fewer steps. We conduct experiments with three different reasoning datasets, each characterized by clear internal reasoning steps, to evaluate model behaviors. Empirical results demonstrate that models exhibit and develop the ability of skipping steps in our framework - not only solving tasks effectively but also actively omitting steps to enhance efficiency. Further analysis of model behaviors indicate that these skipped reasoning paths act as beneficial enhancements rather than mere biased shortcuts, as evidenced by their maintenance or even improvement of out-of-distribution (OOD) performance across various tasks. To the best of our knowledge, this work is the first investigation into the human-like ability of step-skipping in language models, providing empirical evidence that models can indeed skip steps. These preliminary findings provide a fresh perspective on easy-to-hard generalization — training models on simpler data comprising both comprehensive and skipped reasoning steps can enhance their ability to generalize to more complex scenarios. 555Code and data are publicly available at: https://github.com/tengxiaoliu/LM_skip."
https://arxiv.org/html/2411.01839v1,TriG-NER: Triplet-Grid Frameworkfor Discontinuous Named Entity Recognition,"Discontinuous Named Entity Recognition (DNER) presents a challenging problem where entities may be scattered across multiple non-adjacent tokens, making traditional sequence labelling approaches inadequate. Existing methods predominantly rely on custom tagging schemes to handle these discontinuous entities, resulting in models tightly coupled to specific tagging strategies and lacking generalisability across diverse datasets. To address these challenges, we propose TriG-NER, a novel Triplet-Grid Framework that introduces a generalisable approach to learning robust token-level representations for discontinuous entity extraction. Our framework applies triplet loss at the token level, where similarity is defined by word pairs existing within the same entity, effectively pulling together similar and pushing apart dissimilar ones. This approach enhances entity boundary detection and reduces the dependency on specific tagging schemes by focusing on word-pair relationships within a flexible grid structure. We evaluate TriG-NER on three benchmark DNER datasets and demonstrate significant improvements over existing grid-based architectures. These results underscore our framework’s effectiveness in capturing complex entity structures and its adaptability to various tagging schemes, setting a new benchmark for discontinuous entity extraction.","Named Entity Recognition (NER) is a fundamental task in natural language processing that involves identifying and categorising entities such as person names, locations, or temporal expressions within unstructured text. Traditionally, NER has been approached using sequential labelling techniques like the Begin-Inside-Outside (BIO) scheme, which assigns labels to each token in a sentence. However, while effective for contiguous entities, such schemes struggle to accurately capture discontinuous named entities whose mentions are interrupted by non-entity tokens due to their linear nature and inability to represent complex entity structures. Figure 1. A Case example involving discontinuous mentions with Gold Standard (a) Our proposed TriG-NER enables to perfectly extract the DNE (b,c,d) LLMs face challenges in DNER as those are primarily trained to capture continuous sequences of text, making it difficult for them to recognise entities split across discontinuous regions while maintaining coherence in prediction. Recent research in Discontinuous Named Entity Recognition (DNER) has sought to address these limitations by introducing new tagging schemes and model architectures. These include extensions of the BIO scheme like BIOHD (Tang et al., 2015), span-based methods (Wang and Lu, 2019), and grid-based tagging (Wang et al., 2021b), which attempt to represent more complex entity boundaries and relationships. While these methods have shown improvements in extracting discontinuous entities, they often suffer from heavy reliance on task-specific tagging strategies. This makes them highly specialised, limiting their adaptability to new datasets and unseen entity types. Moreover, current solutions primarily focus on sample-based learning objectives, which do not fully capture the token-level dependencies critical for recognising scattered entities. Generative and large language models (LLMs) like ChatGPT have also been explored for DNER, using sequence-to-sequence approaches to generate entity spans. However, these models, optimised for next-word prediction, are not inherently suited for the intricate nature of NER tasks, making them prone to generating incorrect spans and entity boundaries. Grid-tagging methods, on the other hand, have achieved state-of-the-art performance in DNER by modelling word-pair relationships. Nevertheless, they often lack a mechanism to differentiate between similar and dissimilar word-pair representations, particularly for discontinuous entities separated by non-entity tokens. To address these challenges, we introduce TriG-NER, a Triplet-Grid Framework that leverages token-based triplet loss to learn fine-grained word-pair relationships for DNER. Unlike traditional triplet loss, which operates at the sample level by comparing entire sequences, our method applies triplet loss at the token level, where similarity is defined by word pairs co-occurring within the same entity. This approach enables the model to capture the local dependencies between tokens in discontinuous entities, ensuring that word pairs forming an entity are cohesively represented in the learned feature space. We also propose a grid-based triplet loss that models word-pair relationships within a flexible grid structure, where positive pairs represent tokens within the same entity, and negative pairs include word pairs disrupted by non-entity tokens. The main contributions of this paper are as follows: 1. Token-based Triplet Loss for NER: We introduce a novel token-based triplet loss that learns fine-grained token-level representations for discontinuous entity extraction, contrasting with existing methods that use sample-based triplet loss. 2. Grid-based Triplet Loss Using Word-Pair Relationships: We propose a grid-based triplet loss that defines word-pair similarity based on co-occurrence within the same entity, enhancing the model’s ability to capture non-adjacent entity segments. 3. Extensive Evaluations and Qualitative Analysis: We perform extensive evaluations on three widely used DNER benchmark datasets and provide a qualitative analysis that demonstrate the effectiveness of our grid-based triplet framework over existing baselines and prompted large language models."
https://arxiv.org/html/2411.01834v1,Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback,"While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.","Significant strides have been made in Large Language Models (LLMs) by training decoder-only transformer models on vast amounts of text data. In speech processing, Textless NLP (Lakhotia et al., 2021; Kharitonov et al., 2022b; Nguyen et al., 2023; Lin et al., 2022) employs discrete speech units to train Spoken Language Models (SLMs) through next speech unit prediction. This approach is particularly promising, as SLMs are end-to-end speech-to-speech models that bypass the traditional cascaded pipeline of Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) systems, enabling joint optimization and real-time human-computer interaction. Furthermore, SLMs are applicable to all spoken languages, including those without written scripts, as they only require unlabeled speech data, thus promoting inclusivity in speech technology. Despite increasing efforts to develop and improve SLMs—through text model initialization (Hassid et al., 2024; Shih et al., 2024), speech tokenizer design (Lakhotia et al., 2021; Hassid et al., 2024; Baade et al., 2024), text & speech token interleaving (Chou et al., 2023; Nguyen et al., 2024), scaling data and model (Hassid et al., 2024; Cuervo and Marxer, 2024)— a substantial gap remains between the understanding capabilities of text-based LLMs and SLMs. Current SLMs, when prompted, often produce speech continuations characterized by repetitive phrases, grammatical inaccuracies, and low relevance. Zhang et al. (2023); Nachmani et al. propose predicting text during intermediate decoding steps in a chain that mimics the ASR, LM, and TTS tasks within a single model. While these intermediate text steps improve the semantics of the generated speech, they still rely on text tokens as conditions to guide speech generation, and the additional decoding steps introduce latency, preventing real-time interactive SLMs. The question of whether textless SLMs can generate semantically relevant speech remains under-explored. Most research on SLMs has relied exclusively on next-speech-token prediction. Few studies have explored alternative optimization objectives. Compared to text subwords, which on average carry more information, speech tokens are finer-grained and less compact. We argue that the next-speech-token prediction task may overlook long-term semantics, as loosely compressed speech units exhibit significant variability along spectral and temporal dimensions. Consequently, SLMs require a better training objective to effectively capture long-range semantics. Our motivation stems from the observation that SLMs produce inconsistent results, sometimes generating high-quality speech continuations, while at other times producing suboptimal ones. Can we train SLMs to consistently generate better speech continuations while avoiding failures? Drawing inspiration from Reinforcement Learning with Human Feedback (RLHF) for text LLM alignment (Ouyang et al., 2022; Rafailov et al., 2024), we propose Align-SLM, the first framework that enhances the semantics of SLMs through RL. Starting with a pre-trained SLM (the open-sourced TWIST (Hassid et al., 2024) model), we generate multiple speech continuations from a given speech prompt. The next step is to create preference data (prompt, chosen, rejected) for preference optimization. Since collecting human preferences by listening is costly and time-consuming, following the concept of Reinforcement Learning from AI Feedback (RLAIF), we propose an automatic preference data selection strategy with LLM-guided semantic feedback. After preparing the preference data, Direct Preference Optimization (DPO) (Rafailov et al., 2024) is applied to learn from the feedback. Additionally, we couple the proposed technique with curriculum learning and demonstrate further improvements. The proposed framework is pure speech-to-speech, data efficient, and does not require text injection (Nguyen et al., 2024; Chou et al., 2023) or text-to-speech synthesized speech Zhang et al. (2023). We evaluate the SLM’s performance using the sWUGGY and sBLIMP from ZeroSpeech 2021 (Nguyen et al., 2020) for lexical and syntactic modeling, and Spoken-StoryCloze and Topic-StoryCloze (Hassid et al., 2024) for textual nuances and continuation coherence. Additionally, we perform generative evaluations for speech continuation using (i) human listening tests and (ii) GPT-4 as a proxy for assessing semantic coherence and relevance. The results show that the proposed method achieves superior performance in semantic understanding and speech generation. The contributions can be summarized as follows: • We propose the first preference optimization framework for textless SLMs, demonstrating that preference optimization is crucial for improving the semantics of SLMs. • We develop an automated preference data selection strategy by designing effective semantic metrics to score preference data pairs. • We couple DPO with curriculum learning by iteratively opting for higher criterion of preference data to further enhance performance. • Align-SLM achieves the state-of-the-art performance for end-to-end spoken language models on Zerospeech and StoryCloze benchmark (77.9% on sWUGGY, 61.1% on S-StoryCloze, and 86.8% on T-StoryCloze) and achieves superior Meaningfulness Mean opinion scores with human evaluations. Figure 1: The illustration of the Align-SLM framework. Firstly, tokenize the speech prompt into speech tokens using a speech tokenizer. Then, generate and sample multiple speech continuations from the pre-trained SLM. To create the preference data pairs, the framework uses a unit-based vocoder to synthesize the speech continuations back to waveforms, an ASR model to transcribe the waveforms to text, and an LLM evaluator to rate the quality of the semantics. These preference data pairs are then used for direct preference optimization to train the LoRA adapter in the SLM. This alignment process can be coupled with curriculum learning to further improve performance."
https://arxiv.org/html/2411.01765v1,Towards Pedagogical LLMs with Supervised Fine Tuning for Computing Education,"This paper investigates supervised fine-tuning of large language models (LLMs) to improve their pedagogical alignment in computing education, addressing concerns that LLMs may hinder learning outcomes. The project utilised a proprietary dataset of 2,500 high quality question/answer pairs from programming course forums, and explores two research questions: the suitability of university course forums in contributing to fine-tuning datasets, and how supervised fine-tuning can improve LLMs’ alignment with educational principles such as constructivism. Initial findings suggest benefits in pedagogical alignment of LLMs, with deeper evaluations required.","Developments in generative AI, led by the widespread commercial release of OpenAI’s large language models (LLMs) sparked a flurry of applications in computing education, promising to improve learning outcomes and reduce attrition, with a focus on AI-generated programming error message explanations (PEMs) (Taylor et al., 2024; Prather et al., 2023; Liu et al., 2024; Leinonen et al., 2023; Kimmel et al., 2024; Wang et al., 2024; Liffiton et al., 2023). Leinonen et al. (Leinonen et al., 2023) found that Codex produced novice-understandable PEMs for common Python errors. Taylor et al. (Taylor et al., 2024) found 83% of AI-generated PEMs to be accurate. Students also self-report LLM error explanations to be helpful (Liu et al., 2024). Despite reported benefits, commercially available LLMs are aligned to be helpful assistants, which may oppose key tenets of education. Constructivism, one dominant pedagogical theory, states that learners build knowledge by doing rather than being told (Ben-Ari, 1998), and is supported by cognitive psychology literature, which states acquiring knowledge is a function of time and conscious effort (Sweller, 2023). Commercially available LLMs contradict these tenets, displaying a propensity to provide students with solutions despite being instructed otherwise (Taylor et al., 2024; Prather et al., 2024); potentially harming learning by reducing self-efficacy and grades (Padiyath et al., 2024; Dalalah and Dalalah, 2023; Denny et al., 2024). This paper presents our process and initial findings of fine-tuning ChatGPT3.5 to improve pedagogical alignment within computing education. The design of the fine-tuned model, now deployed to over 600 programming students in a large Australian university, was guided by the following research questions: RQ1: How effectively can university course forums contribute to fine-tuning datasets? RQ2: How can supervised fine-tuning better align large language models with pedagogical behaviours?"
https://arxiv.org/html/2411.01751v1,RAGViz: Diagnose and Visualize Retrieval-Augmented Generation,"Retrieval-augmented generation (RAG) combines knowledge from domain-specific sources into large language models to ground answer generation. Current RAG systems lack customizable visibility on the context documents and the model’s attentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool that visualizes the attentiveness of the generated tokens in retrieved documents. With a built-in user interface, retrieval index, and Large Language Model (LLM) backbone, RAGViz provides two main functionalities: (1) token and document-level attention visualization, and (2) generation comparison upon context document addition and removal. As an open-source toolkit, RAGViz can be easily hosted with a custom embedding model and HuggingFace-supported LLM backbone. Using a hybrid ANN (Approximate Nearest Neighbor) index, memory-efficient LLM inference tool, and custom context snippet method, RAGViz operates efficiently with a median query time of about 5 seconds on a moderate GPU node.111Our code is available at https://github.com/cxcscmu/RAGViz. A demo video of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.","Large language models (LLMs), such as GPT-4 (ope, 2024), have revolutionized the field of artificial intelligence with their impressive language understanding and generation capabilities developed through extensive pretraining on large-scale textual data. A key limitation of using pretrained LLMs for zero-shot answer generation is their lack of access to domain-specific knowledge, as these models rely solely on parametric memory. The fixed knowledge derived from parametric memory often leads to hallucinations. To address this issue, Lewis et al. (2020) introduces retrieval-augmented generation (RAG), a technique that leverages retrieval mechanisms to incorporate non-parametric memory, typically derived from documents retrieved from domain-specific data stores. Various systems have been developed to deliver RAG services. For instance, OpenAI Assistants (OpenAI, 2024) and Pinecone Assistant (Cordeiro et al., 2024) are ""chat-with-your-files"" products that use retrieved documents as context for a chatbot. While these RAG systems offer state-of-the-art performance in grounded answer generation, they lack explainability regarding the efficacy of the context documents they use to produce those answers. Some existing tools have been developed to improve language model explainability, such as BertViz (Vig, 2019), an open-source Python tool that provides attention visualizations for transformer models. Although such tools effectively analyze input token importance, they lack a customizable approach for analyzing the interaction between retrieved context documents and language generation. In this paper, we propose RAGViz, a diagnostic tool designed to analyze LLM attention mechanisms on the retrieved documents that provide context to ground LLM answer generation. RAGViz’s novelty lies in its focus on the interaction between the retrieval pipeline and the language model. RAGViz offers attention visualizations based on different levels of scoring: both cumulative attention scores on documents and individual token attention scores selected by the user. Along with document toggling, RAGViz enables users to qualitatively assess the effectiveness of retrieved documents and determine whether they contribute to hallucinations. RAGViz’s system primarily relies on CPU nodes, with the exception of a GPU node that hosts the LLM inference server. The system entry point is a web node that hosts the frontend as static content and routes queries to the main CPU node. This node forwards the query to worker nodes for document retrieval, builds the context, and sends the request to the GPU node for LLM inference. The generated answer and associated attention scores are then returned as an HTTP response to the frontend. RAGViz achieves efficiency through its distributed architecture and optimized LLM inference, partitioning large datasets across multiple nodes for parallel processing and faster retrieval. It uses fast inference libraries for low-latency LLM output generation. Additionally, RAGViz is customizable, allowing integration with any retrieval pipeline or attention-based language model architecture supported by HuggingFace (Wolf et al., 2020), offering flexibility for diverse research needs."
https://arxiv.org/html/2411.01747v1,DynaSaur\scalerel*○: Large Language Agents Beyond Predefined Actions,"Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly-scoped environments, we argue that it presents two major challenges when deploying LLM agents in real-world scenarios: (1) selecting from a fixed set of actions significantly restricts the planning and acting capabilities of LLM agents, and (2) this approach requires substantial human effort to enumerate and implement all possible actions, which becomes impractical in complex environments with a vast number of potential actions. In this work, we propose an LLM agent framework that enables the dynamic creation and composition of actions in an online manner. In this framework, the agent interacts with the environment by generating and executing programs written in a general-purpose programming language at each step. Furthermore, generated actions are accumulated over time for future reuse. Our extensive experiments on the GAIA benchmark demonstrate that this framework offers significantly greater flexibility and outperforms previous methods. Notably, it allows an LLM agent to recover in scenarios where no relevant action exists in the predefined set or when existing actions fail due to unforeseen edge cases. At the time of writing, we hold the top position on the GAIA public leaderboard. Our code can be found in https://github.com/adobe-research/dynasaur.","Developing autonomous agents has long been a central goal in AI research. While reinforcement learning has extensively studied this problem and has achieved significant success in specific domains Silver et al. (2016, 2017); Vinyals et al. (2019); Schrittwieser et al. (2020); Wurman et al. (2022), it often falls short in adaptability and generalization within dynamic and uncertain environments. Given the recent advancements in Large Language Models (LLMs) Chen et al. (2021a); OpenAI (2023); Bubeck et al. (2023); Anil et al. (2023); Reid et al. (2024) with strong reasoning ability and the vast amount of world knowledge they encapsulate during pretraining, LLMs are considered promising foundations for agent policies capable of solving complex, real-world problems Schick et al. (2023a); Chen et al. (2023a); Yao et al. (2023b); Deng et al. (2023); Chen et al. (2024a); Zeng et al. (2024). Notable initial works include Toolformer Schick et al. (2023a), which explores self-supervised training for LLM agents to utilize external tools, such as calculators, search engines, and translation services, thereby enhancing responses to complex question-answering tasks. ReAct Yao et al. (2023b) proposes a synergistic approach by interleaving reasoning and action sequences at each step, which has become the de facto prompting framework in most LLM agent systems. Reflexion Shinn et al. (2023), a follow-up work, investigates LLM agents that maintain a set of self-reflections on their past mistakes in failed trajectories; conditioning on self-reflection feedback significantly improves agent performance across various benchmarks, albeit with the trade-off of increased inference costs. Despite these efforts, most existing LLM agent systems are studied in closed, simulated environments that accept only a finite and small set of predefined actions (Zhou et al., 2024a; Yao et al., 2022; Deng et al., 2023; Shridhar et al., 2021; Liu et al., 2018). At every decision point, an LLM agent is constrained to select an action from this set, leading to several drawbacks. First, it restricts the agent’s flexibility, preventing it from performing actions outside the predefined scope. Second, it requires significant human effort to carefully enumerate and implement all possible actions beforehand; while manageable for closed environments, this approach becomes prohibitively expensive and impractical for real-world settings. Third, in long-horizon tasks, the agent must compose sequences of primitive actions from scratch each time, limiting its ability to learn from past experiences and improve efficiency over time. In this work, we propose DynaSaur , an LLM agent framework that allows for dynamic action creation to address these limitations. To achieve a universal action representation, we model each action as a Python function. At each step, the agent performs actions by generating Python code snippets that either define new functions when the existing set is insufficient or reuse existing functions from the current action set. The generated code is executed through a Python interpreter, and the resulting observations are returned to the agent. Furthermore, all actions generated by the agent are accumulated, building a library of reusable functions for future use. This approach enables the agent to extend its capabilities on-the-fly and compose complex actions from simpler ones, enhancing its flexibility and problem-solving abilities. Leveraging the extensive ecosystem of third-party Python packages, the agent can interact with a wide range of systems and tools. Through extensive experiments on the GAIA benchmark Mialon et al. (2024)—a comprehensive suite designed to evaluate the generality and adaptability of intelligent agents—we demonstrate that our framework enables extremely versatile LLM agents. The agent is capable of handling diverse tasks and file types without requiring human implementation of supporting functions. While the LLM agent is performant and capable on its own, extending this framework by incorporating tools developed by human experts is straightforward by simply including these tools in the agent’s action set. We find that combining human-developed tools with agent-generated functions results in complementary capabilities, further enhancing the agent’s performance and versatility. Figure 1: Illustration of the DynaSaur agent framework. In the first step, the agent receives a list of human-designed actions 𝒜usuperscript𝒜𝑢\mathcal{A}^{u}caligraphic_A start_POSTSUPERSCRIPT italic_u end_POSTSUPERSCRIPT and a task t𝑡titalic_t as input. It then proposes an action a𝑎aitalic_a, implemented as a Python snippet. The function is executed by the environment, which internally contains an IPython kernel. Depending on the generated action a𝑎aitalic_a, the kernel may interact with either the action retriever, to retrieve relevant generated actions in 𝒜gsuperscript𝒜𝑔\mathcal{A}^{g}caligraphic_A start_POSTSUPERSCRIPT italic_g end_POSTSUPERSCRIPT; the internet, for information retrieval from the web; or the local operating system for any other tasks. We do not impose any constraints on which entities the agent can interact with, so the list shown in this figure is not exhaustive and is mainly for illustration purposes. After executing the action a𝑎aitalic_a, the environment returns an observation o𝑜oitalic_o to the agent. The observation can either be the result of executing a𝑎aitalic_a or an error message if the kernel fails to execute a𝑎aitalic_a."
https://arxiv.org/html/2411.01710v1,SPES: Spectrogram Perturbationfor Explainable Speech-to-Text Generation,"Spurred by the demand for interpretable models, research on eXplainable AI for language technologies has experienced significant growth, with feature attribution methods emerging as a cornerstone of this progress. While prior work in NLP explored such methods for classification tasks and textual applications, explainability intersecting generation and speech is lagging, with existing techniques failing to account for the autoregressive nature of state-of-the-art models and to provide fine-grained, phonetically meaningful explanations. We address this gap by introducing Spectrogram Perturbation for Explainable Speech-to-text Generation (SPES), a feature attribution technique applicable to sequence generation tasks with autoregressive models. SPES provides explanations for each predicted token based on both the input spectrogram and the previously generated tokens. Extensive evaluation on speech recognition and translation demonstrates that SPES generates explanations that are faithful and plausible to humans.","The recent advances of Artificial Intelligence (AI) and the emergence of foundation models (Bommasani et al., 2021) have come together with concerns about their risks and impact (Weidinger et al., 2021), as well as calls for a better understanding of their inner workings (Barredo Arrieta et al., 2020; Eiras et al., 2024). This need has been reinforced by the demands for transparency by legal frameworks like the EU AI Act and the US National AI Initiative Act (Panigutti et al., 2023; Roberts et al., 2024). In response, the field of eXplainable AI (XAI) has emerged prominently, with the goal of elucidating the reasoning behind system predictions (Doshi-Velez and Kim, 2017; Carvalho et al., 2019; Vilone and Longo, 2021; Pradhan et al., 2022). Among the various active XAI approaches (Ferrando et al., 2024), this paper centers on feature attribution methods, which aim to identify and quantify the importance of each input feature in determining a model’s final output. Originally developed for image and text classification (Danilevsky et al., 2020; Kamakshi and Krishnan, 2023), feature attribution methods have been extended to other modalities—like speech (Becker et al., 2024; Pastor et al., 2024)—and to text generation (Sarti et al., 2023; Zhao et al., 2024, et al.). Still, XAI in the domain of speech-to-text (S2T) generative tasks is limited, with only a few preliminary works focusing on automatic speech recognition (ASR) (Mandel, 2016; Trinh et al., 2018; Trinh and Mandel, 2021; Kavaki and Mandel, 2020; Markert et al., 2021; Wu et al., 2023, 2024). This contrasts with the pivotal role of spoken language—arguably the most natural form of human interaction (Munteanu et al., 2013)—and consequently with the importance of S2T technologies, which are now fostered by foundation models that transcribe and translate at an unprecedented scale (Latif et al., 2023). Moreveor, most existing feature attribution methods in S2T Mandel (2016); Trinh and Mandel (2021); Markert et al. (2021); Wu et al. (2023, 2024) are not applied to autoregressive models, which predict each token iteratively, relying on both speech input and previous output tokens. Even the only method applied to autoregressive S2T models Kavaki and Mandel (2020) assumes conditional independence between prediction steps, disregarding the influence of previously generated text on each new prediction. Finally, these methods are either incompatible with the spectrogram input format (Markert et al., 2021; Wu et al., 2023, 2024), which is typically used in modern S2T models, or produce explanations that fail to highlight fine-grained patterns related to acoustic characteristics of speech, such as the fundamental frequency and formants (Mandel, 2016; Trinh et al., 2018; Kavaki and Mandel, 2020; Trinh and Mandel, 2021). To overcome the above limitations, we propose Spectrogram Perturbation for Explainable Speech-to-text Generation (SPES), the first feature attribution technique that provides token-level explanations for autoregressive S2T models by considering both the speech input and previously generated tokens.111Code released under Apache 2.0 upon paper publication. SPES employs a perturbation-based approach that adapts image segmentation to spectrograms, enabling the identification of fine-grained, meaningful patterns. Through quantitative assessments and in-depth analyses across two S2T tasks—ASR and, for the first time, speech translation (ST)—we show that SPES produces accurate, phonetically interpretable explanations aligned with human understanding."
https://arxiv.org/html/2411.01706v1,Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups,"Complex Word Identification (CWI) is an essential step in the lexical simplification task and has recently become a task on its own. Some variations of this binary classification task have emerged, such as lexical complexity prediction (LCP) and complexity evaluation of multi-word expressions (MWE). Large language models (LLMs) recently became popular in the Natural Language Processing community because of their versatility and capability to solve unseen tasks in zero/few-shot settings. Our work investigates LLM usage, specifically open-source models such as Llama 2, Llama 3, and Vicuna v1.5, and closed-source, such as ChatGPT-3.5-turbo and GPT-4o, in the CWI, LCP, and MWE settings. We evaluate zero-shot, few-shot, and fine-tuning settings and show that LLMs struggle in certain conditions or achieve comparable results against existing methods. In addition, we provide some views on meta-learning combined with prompt learning. In the end, we conclude that the current state of LLMs cannot or barely outperform existing methods, which are usually much smaller.","Complex word identification (CWI) aims to determine whether words or phrases are difficult for a target group of readers to understand. In the lexical simplification task, which targets replacing complex words and expressions with simplified alternatives North et al. (2023a), CWI is the first step, and it was treated as part of this task until 2012 when it became a standalone task Shardlow (2013). CWI was initially addressed as a binary classification task Paetzold and Specia (2016), identifying whether a word is complex in a given sentence. When the task became more popular North et al. (2023b), it was extended to the continuous domain as Lexical Complexity Prediction (LCP, also referred to as the probabilistic classification for CWI) Yimam et al. (2018), addressing multilingual and multidomain settings. Then, it was extended to multi-word expressions Shardlow et al. (2021). Recently, new datasets started to emerge in various languages and domains Ortiz Zambrano and Montejo-Ráez (2021); Venugopal et al. (2022); Ilgen and Biemann (2023); Zambrano et al. (2023). Previous approaches to CWI ranged from using Support Vector Machines S.P et al. (2016) to deep neural networks based on Bidirectional Representation from Encoder Transformers Pan et al. (2021), multi-task learning with domain adaptation Zaharia et al. (2022), and sequence modeling Gooding and Kochmar (2019). With the recent breakthrough in large language models (LLMs), particularly the work of OpenAI with Generative Pre-trained Transformer (GPT) models Radford et al. (2019); Brown et al. (2020), natural language processing has seen a significant leap. These models have shown the potential to improve performances on various tasks as we scale up the model size and the amount of training data. The announcement of ChatGPT111https://openai.com/blog/chatgpt and its conversational capabilities has sparked a race in developing and fine-tuning new models for general purpose and domain-specific applications using PaLM Anil et al. (2023), LLaMA Touvron et al. (2023); Dubey et al. (2024), Orca Mitra et al. (2023), Mistral Jiang et al. (2023), GPT-4 OpenAI et al. (2023), GPT-4o222https://openai.com/index/hello-gpt-4o/, and many others. Our work aims to provide the current state of LLMs in addressing CWI and LCP compared against state-of-the-art approaches. We focus on evaluating open-source (pre-trained Llama 2 Touvron et al. (2023), Llama 3 Dubey et al. (2024), Vicuna Zheng et al. (2024)) and OpenAI’s close-source ChatGPT-3.5-turbo and GPT-4o. We summarize the contributions as follows: • We evaluate LLMs in binary (discrete set of labels) and probabilistic classification (continuous space labels) on multidomain and multilingual corpora. • We employ various techniques for prompting and fine-tuning. • We show that LLMs struggle to address CWI and LCP tasks; however, in limited instances, they can achieve similar results with other, more lightweight approaches. • In the end, we analyze and provide an insight into where the models struggle."
https://arxiv.org/html/2411.01703v1,\method: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models,"Multimodal large language models (MLLMs) have revolutionized vision-language understanding but are vulnerable to multimodal jailbreak attacks, where adversaries meticulously craft inputs to elicit harmful or inappropriate responses. We propose \method, a novel multimodal safety guardrail that jointly considers the unimodal and cross-modal harmful signals. \methodis trained such that the likelihood of generating harmful responses in a toxic corpus is minimized, and can be seamlessly applied to any input prompt during inference with minimal computational costs. Extensive experiments demonstrate the generalizability of \methodacross multiple modalities and attack strategies. It demonstrates impressive generalizability across multiple state-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4V, MiniGPT-4, and InstructBLIP, thereby broadening the scope of our solution.","The rapid development of multimodal large language models (MLLMs), exemplified by models like GPT-4V (OpenAI, 2023), Gemini (Reid et al., 2024), and LLaVA (Liu et al., 2023b, a), has revolutionized vision-language understanding but introduced new risks. One of the most pressing concerns is the vulnerabilities of MLLMs to adversarial attacks or jailbreaks (Qi et al., 2023; Shayegani et al., 2023; Niu et al., 2024; Deng et al., 2024), which leverages inherent weaknesses of models to bypass their safety mechanisms, raising concerns about their secure deployment. Challenges. Ensuring safe and trustworthy interactions requires the development of robust safety guardrails against adversarial exploitation, which presents three core challenges. 1) Multimodal Effectiveness. Guardrails must protect against adversarial prompting in multiple modalities and their cross-modal interactions, ensuring that defenses are not limited to unimodal threats. 2) Generalizability Across Models. Safety mechanisms should be adaptable to both open-source and proprietary models. 3) Robustness across diverse attacks. Effective guardrails must withstand a wide range of attack strategies, including constrained attacks that subtly modify inputs while maintaining visual similarity, and unconstrained attacks that introduce noticeable changes (Qi et al., 2023). They should also address adversarial text prompts (Gehman et al., 2020) that elicit harmful or inappropriate responses from LLMs. Although prior work has explored defenses for both unimodal (Zou et al., 2023; Chao et al., 2023) and multimodal LLMs (Shayegani et al., 2023; Niu et al., 2024; Gou et al., 2024; Pi et al., 2024), a holistic approach covering multiple modalities, models, and attack types remains an open challenge. This Work. We introduce \method, a novel defense mechanism that provides robust, Universally applicable multimodal Guardrails against adversarial attacks in both visual and textual inputs. As shown in Figure 1, the core idea is to create specialized safety guardrail for each modality while accounting for their cross-modal interactions. This guardrail purifies potential adversarial responses after applying to input prompts. Inspired by few-shot prompt learning (Qi et al., 2023; Lester et al., 2021), we optimize the guardrails by searching for additive noise (for image inputs) and suffix modifications (for text prompts) to minimize the likelihood of generating harmful responses in a small toxic content corpus (Liu et al., 2023a). We conduct comprehensive experiments on both adversarial and benign input prompts. Our results demonstrate that \methodsignificantly improves robustness against various adversarial attacks while maintaining high accuracy for benign inputs. For example, \methodeffectively reduces the attack success rate on LLaVA by nearly 55%, with a small performance-safety trade-off in visual question-answering. The safety guardrails developed for one model such as LLaVA (Liu et al., 2023a) is transferable to other MLLMs, including both open-source models like MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023), as well as proprietary models like Gemini Pro (Team et al., 2023) and GPT-4V (OpenAI, 2023), highlighting the generalizability of our approach across different models and architectures. Contributions. Our major contributions are: 1. Effective Defense Strategy. We propose \method, a pioneering multimodal defense mechanism that effectively enhances MLLM robustness against jailbreak attacks; 2. Novel Methodology. We introduce a novel optimization technique that generates multimodal safety guardrails using a small corpus of harmful content and an open-source MLLM; 3. Comprehensive Evaluation. We conduct comprehensive evaluation showing that \methodeffectively enhances the robustness of multiple models, including both open-source MLLMs (LLaVA, MiniGPT-4, and InstructBLIP.) and proprietary models (Gemini Pro and GPT-4V)."
https://arxiv.org/html/2411.01610v1,Explaining and Improving Contrastive Decodingby Extrapolating the Probabilities of a Huge and Hypothetical LM,"Contrastive decoding (CD) (Li et al., 2023) improves the next-token distribution of a large expert language model (LM) using a small amateur LM. Although CD is applied to various LMs and domains to enhance open-ended text generation, it is still unclear why CD often works well, when it could fail, and how we can make it better. To deepen our understanding of CD, we first theoretically prove that CD could be viewed as linearly extrapolating the next-token logits from a huge and hypothetical LM. We also highlight that the linear extrapolation could make CD unable to output the most obvious answers that have already been assigned high probabilities by the amateur LM.To overcome CD’s limitation, we propose a new unsupervised decoding method called Asymptotic Probability Decoding (APD).111The code will be released at https://github.com/amazon-science/llm-asymptotic-decoding APD explicitly extrapolates the probability curves from the LMs of different sizes to infer the asymptotic probabilities from an infinitely large LM without inducing more inference costs than CD. In FactualityPrompts, an open-ended text generation benchmark, sampling using APD significantly boosts factuality in comparison to the CD sampling and its variants, and achieves state-of-the-art results for Pythia 6.9B and OPT 6.7B. Furthermore, in five commonsense QA datasets, APD is often significantly better than CD and achieves a similar effect of using a larger LLM. For example, the perplexity of APD on top of Pythia 6.9B is even lower than the perplexity of Pythia 12B in CommonsenseQA and LAMBADA.","Figure 1: Given a simple question with clues for which a tiny amateur LM could provide a correct answer, contrastive decoding (CD) could have a “obvious blindness” (i.e., assigning a higher logit to an uncommon answer Invertebrate than the most obvious answer Bees). In contrast, the proposed asymptotic probability decoding (APD) correctly assigns the highest probability to Bees by leveraging the probabilities from multiple LMs of different sizes to extrapolate the probabilities from an infinitely large and hypothetical LM. Contrastive Decoding (Li et al., 2023) (CD) is a simple heuristic that uses the logit of a small LM (amateur LM) to improve the logit of a large LM (expert LM).222Since the amateur LM is easier to fail, CD adjusts the logit of an expert LM by subtracting the logit of an amateur LM, so a lower amateur’s logit implies a higher output logit of CD. Using Figure 1 as an example, CD produces a large logit for Invertebrates because its amateur’s logit is low. The potential of CD has been demonstrated in various open-ended text generation tasks (Li et al., 2023) and reasoning tasks using the expert LMs up to 65B (O’Brien and Lewis, 2023). Several variants are also proposed to reduce toxicity (Liu et al., 2021), improve factuality in NLP tasks (Chuang et al., 2023; Zhang et al., 2023; Shi et al., 2024; Sanchez et al., 2024), text evaluation (Lu et al., 2024), and vision tasks (Wan et al., 2024). However, due to the insufficient theoretical understanding of CD, it is difficult to identify and overcome the failure modes of CD, which hinders its wide applications. Scaling law demonstrates that language models (LMs) are able to generate more factual next tokens as their sizes increase (Kaplan et al., 2020; Lee et al., 2022). However, their growing energy consumption and costs limit their further applications (Strubell et al., 2019; Lacoste et al., 2019; Kaack et al., 2022), necessitating techniques that can reduce LM model sizes without compromising their superior performances. In this work, we theoretically demonstrate how contrastive decoding (CD) addresses this LM size-reduction challenge using a simple linear extrapolation. The theory also helps us to identify the limitations of CD and propose APD, a more factual decoding method. First, we discover that CD actually uses the tiny amateur LM to help the large expert LM infer the logit of a huge and hypothetical LM. Specifically, the logits from CD could often be viewed as a linear extrapolation of the logit curves from the expert LM and amateur LM. The finding explains several prior empirical observations and also reveals weaknesses of CD. For example, CD tends to neglect the most obvious answer and overemphasize less likely answers in its output distribution instead. We call this tendency “obvious blindness”. The rare answers could sometimes degrade the generation’s factuality. For example, both amateur and expert LM in Figure 1 can identify that Bees is the most clear answer suggested by the clues but only the expert LM realizes that there are also some other possible answers such as Invertebrates. Then, the aggressive linear extrapolation of CD makes Invertebrates become the most probable next token. This is not a totally factual answer because many invertebrates cannot fly. Motivated by this theoretical explanation, we propose a novel decoding method called Asymptotic Probability Decoding (APD). APD predicts the asymptotic probability from a hypothetical LM with an infinite size. By explicitly modeling the changes of the next-token probabilities as the size of LM increases, APD is able to output the correct probabilities for both easy/common and difficult/uncommon answers. For the example in Figure 1, the probabilities of Bees and Invertebrates are both increasing as the LM becomes larger. By leveraging the probabilities of mid-size LMs, we can reasonably infer that Bees should still receive a larger probability from a huge LM than Invertebrates. Finally, modeling the probability curves for many next tokens on the fly is too time-consuming, so we fine-tune an amateur LM such that the output probability of APD is close to asymptotic probability, which makes APD as efficient as CD. The main goal of our experiments is to check if APD can further improve the factuality compared to CD. We choose our expert LMs and amateur LMs from the LLM families that provide smaller LMs, including Pythia (6.9B, 70M) (Biderman et al., 2023), OPT (6.7B, OPT 125M) (Zhang et al., 2022), and Qwen1.5 (4B, 0.5B) (Bai et al., 2023). By comparing different sampling methods in FactualityPrompts (Lee et al., 2022), we demonstrate that APD consistently and robustly outperforms CD with the best temperature and other state-of-the-art distribution modification methods such as DoLa (Chuang et al., 2023) and temperature sampling (Ficler and Goldberg, 2017). After being combined with dynamically adjusted top-p sampling (Chang et al., 2024), our method can help Pythia 6.9B to simultaneously achieve the factuality of top-p sampling (Holtzman et al., 2020) with p=0.4𝑝0.4p=0.4italic_p = 0.4 and diversity of top-p with p=0.7𝑝0.7p=0.7italic_p = 0.7. We also compare the perplexity of APD and CD using seven datasets. We found that the improvement gap is especially large when CD makes more mistakes on easier tasks. For example, in LAMBADA (Paperno et al., 2016) and CommonsenseQA (Talmor et al., 2019), APD on top of Pythia 6.9B could achieve a similar or better perplexity than Pythia 12B, while outperforming CD by a large margin. We plan to release our code to reproduce the results after our work is accepted. Our main contributions include • We provide theoretical support for contrastive decoding (CD) and demonstrate that our theory can explain many prior findings from Li et al. (2023); O’Brien and Lewis (2023). • We propose a new distribution modification method, asymptotic probability decoding (APD), which addresses the “obvious blindness” of CD. • We conduct extensive experiments, which indicate that APD could significantly improve the generation factuality of CD."
https://arxiv.org/html/2411.01562v1,Are LLMs good pragmatic speakers?,"Large language models (LLMs) are trained on data assumed to include natural language pragmatics, but do they actually behave like pragmatic speakers? We attempt to answer this question using the Rational Speech Act (RSA) framework [9, 10], which models pragmatic reasoning in human communication. Using the paradigm of a reference game constructed from the TUNA [27] corpus, we score candidate referential utterances in both a state-of-the-art LLM (Llama3-8B-Instruct) and in the RSA model, comparing and contrasting these scores. Given that RSA requires defining alternative utterances and a truth-conditional meaning function, we explore such comparison for different choices of each of these requirements. We find that while scores from the LLM have some positive correlation with those from RSA, there isn’t sufficient evidence to claim that it behaves like a pragmatic speaker. This initial study paves way for further targeted efforts exploring different models and settings, including human-subject evaluation, to see if LLMs truly can, or be made to, behave like pragmatic speakers.","With the emergence of large language models (LLMs) [4, 1, 6, 14, 15, 25, 26], a key question arises: can these models, trained on data presumed to include natural language pragmatics, exhibit pragmatic reasoning akin to humans? While LLMs have demonstrated formal linguistic competence (adherence to linguistic rules), their functional competence in pragmatic language use remains uncertain and warrants further investigation [17]. Although much research has focused on evaluating LLMs’ pragmatic abilities as listeners, particularly their comprehension of non-literal language [12, 16, 23, 24, 19], less attention has been given to their pragmatic capabilities as speakers. Specifically, it remains unclear whether LLMs can effectively use context to generate non-literal utterances; i.e., being informative beyond being simply true. Investigating this aspect is crucial for deepening our understanding of LLMs’ generative processes and enhancing their reliability. Humans are typically pragmatic agents in communication. Imagine a room with three pieces of furniture: a small red desk, a small yellow desk, and a large red chair. If a friend directs your attention to “the red one”, you would first eliminate the yellow desk as a possibility. Given the remaining red objects, your reasoning would then likely discount the chair, on account of it being distinct in the room, and that if it were the intended referent, the simpler utterance would have been just “the chair”. This finally leads you to the intended referent being the small red desk. This recursive reasoning process that takes informativity into account beyond simply being literally true, encapsulates pragmatic communication. A particularly influential model of pragmatic communication is the Rational Speech Act (RSA) framework [10, 7] which quantitatively models theory of mind [8, 21], formalising how speakers and listeners use context, shared knowledge, and probabilistic reasoning to communicate effectively. This framework operates language understanding as a recursive process, where both speakers and listeners in a conversation behave rationally to reason each other’s intention. In the earlier example, when a friend refers to “the red one”, you exclude the yellow desk as a literal listener, guided by a “meaning function” that checks alignment with the words. The pragmatic speaker, your friend, chooses words with the expectation that you, as the listener, will interpret them correctly. As a pragmatic listener, you refine your interpretation, using both the message and context to infer intent. This interaction between speaker and listener, each considering the other’s perspective, is key to effective communication, as modelled by the RSA framework. Nguyen [20] applies the RSA framework to view RLHF-fine-tuned language models as bounded pragmatic speakers, where RLHF equips the LLM with a “theory of mind” listener model. This enables the LLM to anticipate listener interpretation when computing the distribution of the pragmatic speaker. However, this study does not examine the pragmatic generation ability of unmodified LLM. Carenini et al. [5] examines vanilla LLMs’ pragmatic reasoning using RSA to find that GPT-2 XL’s reasoning aligns with RSA in a metaphor task structured as “X𝑋Xitalic_X is Y𝑌Yitalic_Y”, with a restricted meaning and utterance space. However, this focuses on the LLM as a listener; we instead propose using a reference game to compare LLM scores as a speaker against RSA using a natural language format aligned with LLM training data. Additionally, we explore how the alignment varies with different sources of alternative utterances and RSA models that employ distinct meaning functions. Our results indicate that our vanilla LLM model (Llama3-8B-Instruct [26]) has a positive correlation with the two RSA models that employ different meaning functions in the context of the reference game task, the correlation is stronger when scoring the logic-constructed utterances, and when the RSA model is with a rule-based meaning function. However, we do not see a clear alignment of the LLM’s pragmatic scoring with that from the RSA models."
https://arxiv.org/html/2411.01539v2,LLMs and the Madness of Crowds,"We investigate the patterns of incorrect answers produced by large language models (LLMs) during evaluation. These errors exhibit highly non-intuitive behaviors unique to each model. By analyzing these patterns, we measure the similarities between LLMs and construct a taxonomy that categorizes them based on their error correlations. Our findings reveal that the incorrect responses are not randomly distributed but systematically correlated across models, providing new insights into the underlying structures and relationships among LLMs.","(With apologies to Charles Mackay [6]) The performance of large language models (LLMs) is frequently evaluated using multiple-choice tests. When an LLM’s inference is performed with a positive temperature, posing the same problem repeatedly will yield a distribution across the possible answers. If the LLM performs well, we would expect most of the probability mass to lie on the correct answer; if it performs poorly, we might expect the distribution to be more uniform across all the answers. However, this intuition does not always hold. To better understand the actual behavior of LLMs, we provide several detailed examples in Section 2. In Section 3, we perform a more comprehensive analysis at scale and use the results to suggest a taxonomy of LLMs."
https://arxiv.org/html/2411.01533v2,Enhancing LLM Evaluations: The Garbling Trick,"As large language models (LLMs) become increasingly powerful, traditional evaluation metrics tend to saturate, making it challenging to distinguish between models based on their performance. We propose a general method to transform existing LLM evaluations into a series of progressively more difficult tasks. These enhanced evaluations emphasize reasoning capabilities and can reveal relative performance differences that are not apparent in the original assessments.To demonstrate the effectiveness of our approach, we create a new multiple-choice test corpus, extend it into a family of evaluations, and assess a collection of LLMs. Our results offer insights into the comparative reasoning abilities of these models, particularly highlighting distinctions between OpenAI’s o1-preview and Google’s gemini-pro-1.5-002.","Significant advancements in AI and machine learning have often progressed in tandem with the development of evaluation tools. For instance, MNIST [12] spurred the creation of LeNet [11], ImageNet [5] led to the development of AlexNet [10], and CASP [15] motivated AlphaFold [9]. However, it is common for performance on a given test to reach saturation—for example, modern accuracy on MNIST exceeds 99.9% [1], rendering it less useful for comparing models or driving research. Despite being a relatively young field, the evaluation of large language models (LLMs) faces similar challenges. Multiple-choice tests are a popular evaluation method (e.g., MMLU [7], MMLU-PRO [18], HellaSwag [19], HELM [13], GSM8K [4]), but saturation is already apparent. As of October 2024, top-performing models have achieved MMLU scores of 92.3% [16], MMLU-PRO scores of 91.0% [3], HellaSwag scores of 96.1%, HELM (Lite) scores of 95.9% [6], and GSM8K scores exceeding 95% [14]. OpenAI has even argued [16] that the GSM8K metric is no longer effective for differentiating models. Why not simply introduce new, more challenging tests? Beyond the difficulty of creating well-designed assessments, the greatest value of an evaluation framework lies in how it guides research in the field. MNIST, ImageNet, and CASP were beneficial because they took years to saturate, providing clear objectives for numerous research groups during that time. Ideally, a test should be sufficiently challenging to engage the field for several years, not just a few months. In this paper, we present a straightforward method for transforming any text-based evaluation into a family of increasingly challenging tests. This is a meta-evaluation technique: it takes an existing evaluation framework and converts it into a new one. To demonstrate our method, we generate a new evaluation framework and then extend it into a harder family of tests. Even this simple example reveals non-obvious differences in reasoning performance among models."
https://arxiv.org/html/2411.01531v1,DAG: Dictionary-Augmented Generation for Disambiguation of Sentences in Endangered Uralic Languages using ChatGPT,"We showcase that ChatGPT can be used to disambiguate lemmas in two endangered languages ChatGPT is not proficient in, namely Erzya and Skolt Sami. We augment our prompt by providing dictionary translations of the candidate lemmas to a majority language - Finnish in our case. This dictionary augmented generation approach results in 50% accuracy for Skolt Sami and 41% accuracy for Erzya. On a closer inspection, many of the error types were of the kind even an untrained human annotator would make.","Morphological disambiguation is a critical task in natural language processing (NLP), especially for morphologically rich and endangered languages. Skolt Sami (sms) and Erzya (myv), both belong to Uralic language family and they are classified as critically and definitely endangered respectively by Unesco Moseley (2010). This poses significant challenges in this domain due to their complex morphological systems and limited available linguistic resources (see Hämäläinen 2021). In languages like these, each word form can have multiple possible morphological interpretations and lemmas, and determining the correct one in context is essential for accurate language processing. Traditional approaches to morphological disambiguation for Uralic languages often rely on finite-state transducers (FSTs) and constraint grammars (CGs) that list all potential lemmas for a word, but these systems struggle to accurately select the appropriate lemma in ambiguous contexts - not to mention that CG disambiguators have not even been created to a majority of these languages. Additionally, while some modern NLP techniques, such as machine learning models, have been successful in languages with large datasets (see Shen et al. 2016; Zalmout and Habash 2017), such methods are less effective for languages like Skolt Sami and Erzya, which suffer from limited annotated corpora and lexicographical resources. This paper presents a novel method for performing morphological disambiguation for Skolt Sami and Erzya that leverages a combination of a traditional FST-based analyzer, a bilingual dictionary and a state-of-the-art language model, namely ChatGPT. Our approach involves passing each sentence through an FST to generate a list of possible lemmas for every word. These lemmas are then translated into a majority language (Finnish in our case) using a dictionary. Finally, we utilize ChatGPT, a powerful transformer-based language model, to analyze the translated sentence, disambiguate the lemmas, and select the most contextually appropriate form for each word. The dictionary needs to be provided given that ChatGPT is not proficient in these languages. By integrating the structured linguistic knowledge from FSTs with the contextual understanding of large language models, this method aims find a novel way that does not need a time consuming rule-writing or data annotation process for morphological disambiguation for Skolt Sami and Erzya. The proposed approach is particularly valuable for endangered languages, where data scarcity hinders the development of purely data-driven models. This paper details the methodology, presents an evaluation of the approach, and discusses the potential for applying this approach to other morphologically complex languages. We have released our disambiguation code as an addition to UralicNLP111https://github.com/mikahama/uralicNLP/wiki/Disambiguation. Prompt template Actual prompt Your task is to disambiguate a sentence in [LANGUAGE] You will be given the sentence, a table that has all of the words of the sentence in separate rows and a comma separated list of possible lemmas. You will need to pick the correct lemma for each word so that every word will have only one lemma. To help you understand [LANGUAGE] you will also get a second table that gives you translations of the words in [LANGUAGE2]. Sentence: [SENTENCE] Table of lemmas: [TABLE1] [LANGUAGE] - [LANGUAGE2] vocabulary: [TABLE2] Please write out the steps of your decision process and provide a list of lemmas in JSON format at the very end of your answer. Example: {""lemmas"": [""lemma 1"", ""lemma 2"", ""lemma 3""]} Your task is to disambiguate a sentence in Skolt Sami. You will be given the sentence, a table that has all of the words of the sentence in separate rows and a comma separated list of possible lemmas. You will need to pick the correct lemma for each word so that every word will have only one lemma. To help you understand Skolt Sami you will also get a second table that gives you translations of the words in Finnish. Sentence: Päärna mõ′′\prime′nne mååusat . Table of lemmas: +————————–+ | Word | Lemmas | +——-+——————+ | Päärna| päärnaž | +——-+——————+ | mõ′′\prime′nne|mõõnnâd, mõ′′\prime′nn’jed| +——-+——————+ |mååusat| mååusat | +——-+——————+ | . | . | +————————–+ Skolt Sami - Finnish vocabulary: +—————————————+ |Skolt Sami| Finnish | +———-+—————————-+ | päärnaž |poikanen, lapsi, pieni poika| +———-+—————————-+ | mõõnnâd | mennä | +———-+—————————-+ | mõ′′\prime′nn’jed| munata | +———-+—————————-+ | mååusat | takaisin | +———-+—————————-+ | . | | +—————————————+ Please write out the steps of your decision process and provide a list of lemmas in JSON format at the very end of your answer. Example: {""lemmas"": [""lemma 1"", ""lemma 2"", ""lemma 3""]} Table 1: The prompt template and an example of it filled"
https://arxiv.org/html/2411.01523v1,SinaTools: Open Source Toolkit for Arabic Natural Language Processing,"We introduce SinaTools , an open-source Python package for Arabic natural language processing and understanding. SinaTools is a unified package allowing people to integrate it into their system workflow, offering solutions for various tasks such as flat and nested Named Entity Recognition (NER), fully-flagged Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy Extractions and Evaluation, Lemmatization, Part-of-speech Tagging, Root Tagging, and additional helper utilities such as corpus processing, text stripping methods, and diacritic-aware word matching. This paper presents SinaTools and its benchmarking results, demonstrating that SinaTools outperforms all similar tools on the aforementioned tasks, such as Flat NER (87.33%percent87.3387.33\%87.33 %), Nested NER (89.42%percent89.4289.42\%89.42 %), WSD (82.63%percent82.6382.63\%82.63 %), Semantic Relatedness (0.490.490.490.49 Spearman rank), Lemmatization (90.5%percent90.590.5\%90.5 %), POS tagging (97.5%percent97.597.5\%97.5 %), among others. SinaTools can be downloaded from (https://sina.birzeit.edu/sinatools).","Despite the progress in Arabic NLP [9], there remain a lack of tools and resources that offer solutions for Arabic NLP and NLU tasks. Developing machine learning tools is crucial in democratizing Arabic NLP, as they allow people to incorporate machine learning into their workflows with less technical knowledge. The availability of open-source and advanced AI tools remains limited. Low-code platforms and toolkits can bridge this gap by offering intuitive interfaces and trained models, making it easier for people in industry, research, and education to tap into the power of NLP to develop and deploy NLP applications. As will be discussed in this paper, a handful of tools for Arabic NLP have emerged, each offering rich functionalities that contribute to the growing ecosystem of Arabic NLP. Notable examples include The Stanford CoreNLP Toolkit [42], Farasa [10], MADAMIRA [49], CAMeLTools [46], and OCTOPUS [12]. This article presents a new set of tools packaged in SinaTools , an open-source toolkit for Arabic NLP and NLU, offering state-of-the-art solutions for various semantic-related tasks developed by SinaLab at Birzeit University. SinaTools currently supports flat, nested, and fine-grained Named Entity Recognition (NER), Word Sense Disambiguation (WSD), Semantic Relatedness, Synonymy Extraction and Evaluation, Lemmatization, Part-of-Speech (POS) tagging, and root tagging, among others. SinaTools provides a single end-to-end system for these tasks with different interfaces, including a Command Line Interface (CLI), Software Development Kit (SDK), and Application Programming Interface (API), as well as Python Jupyter Notebooks. Our goal is to simplify the development and deployment of Arabic NLP applications. Additionally, this article presents our benchmarking results of similar toolkits on the aforementioned tasks, demonstrating the superiority of SinaTools over other tools. The main contributions of this article are: 1. Open-source111Download page: https://sina.birzeit.edu/sinatools and Python-based Arabic toolkit for various NLU tasks, with different interfaces. 2. Benchmarking Arabic NLP toolkits, demonstrating SinaTools benefits and superior performance on all tasks. This article is structured as follows: Section 2 overviews related work. Section 3 presents the design and implementation. Section 4 presents all NLU modules. Section 5 concludes the paper and outlines our future work. Fig. 1: Core modules of SinaTools"
https://arxiv.org/html/2411.01485v1,Domain-specific Guided Summarization for Mental Health Posts,"In domain-specific contexts, particularly mental health, abstractive summarization requires advanced techniques adept at handling specialized content to generate domain-relevant and faithful summaries. In response to this, we introduce a guided summarizer equipped with a dual-encoder and an adapted decoder that utilizes novel domain-specific guidance signals, i.e., mental health terminologies and contextually rich sentences from the source document, to enhance its capacity to align closely with the content and context of guidance, thereby generating a domain-relevant summary. Additionally, we present a post-editing correction model to rectify errors in the generated summary, thus enhancing its consistency with the original content in detail. Evaluation on the MentSum dataset reveals that our model outperforms existing baseline models in terms of both Rouge and FactCC scores. Although the experiments are specifically designed for mental health posts, the methodology we’ve developed offers broad applicability, highlighting its versatility and effectiveness in producing high-quality domain-specific summaries.","Mental health is a critical area that profoundly affects both individuals and society, demanding effective and accurate communication for support Hua et al. (2024). In this domain, abstractive summarization plays a pivotal role by condensing one lengthy user post from online platforms like Reddit111https://www.reddit.com and Reachout222https://au.reachout.com into a concise summary. This process, through paraphrasing, generalizing, and reorganizing content with novel phrases and sentences, effectively conveys the essential information and meaning of the original text (Shi et al., 2021; Qian et al., 2023). The summary enables quicker review and response by professional counselors, thus enhancing support for individuals dealing with mental health issues and demonstrating significant social impact. Figure 1: This example highlights the importance of an ideal summary that, compared to a general summary, is focused on domain relevance and faithful to the source post, providing essential support for effective communication within the mental health community. Despite advancements in natural language processing (NLP), applying abstractive summarization to mental health posts illustrates some major challenges in domain-specific summarization. The first challenge is that the summary generated by state-of-the-art (SOTA) pre-trained models (Liu and Lapata, 2019; Lewis et al., 2020; Raffel et al., 2020) tends to be too general and lacks domain specificity. These models often struggle to control the content of the summary, making it difficult to determine in advance which parts of the original content should be emphasized (Dou et al., 2021). The second challenge pertains to the faithfulness of the generated summary. Often, there is a notable risk of producing a summary that may contradict or diverge from the source document, potentially introducing intrinsic hallucination333Intrinsic hallucination refers to content in a generated summary that contradicts the source document. or inconsistency (Kryscinski et al., 2020; Wang et al., 2024a, b; Na et al., 2024). Together, these issues highlight the need for more advanced summarization techniques that can adeptly handle the complexities of domain-specific content while ensuring contextual relevance and detail consistency, as shown in Figure 1. Drawing inspiration from the GSum (Dou et al., 2021) framework for its ability to enhance controllability through guidance signal and constrain summary to deviate less from the source document, we introduce a guided summarizer featuring a dual-encoder and an adapted decoder architecture that leverages two types of domain-specific knowledge-based guidance, i.e., specialized mental health terminologies and contextually rich sentences from source post. This design is specifically tailored to enhance the summarization process within mental health contexts, guiding the generation of a summary that is both terminologically precise and richly informed by the underlying domain-specific information contained within the original text. Further, building on established post-editing practice in recent studies (Dong et al., 2020; Cao et al., 2020), we propose a corrector that follows the summarizer and is dedicated to identifying and correcting potential inconsistencies in the generated summary with respect to the source post. This step ensures the corrected summary more faithfully represents the details of the original text. At last, we evaluate our model on MentSum (Sotudeh et al., 2022b), the first mental health summarization dataset. The output summary is evaluated by not only the Rouge scores (Lin, 2004) measuring linguistic quality, but also FactCC score (Kryscinski et al., 2020), an automatic metric assessing factual consistency444Although recent studies define “factuality” as being based on real-world facts, our paper uses the term “factual consistency”, which is commonly employed in evaluation research, to emphasize alignment with the source document. with the source document. The contributions of this study are as follows: • We introduce novel domain-specific guidance signals, encoded by a separate encoder to guide the summarization process to align closely with the content and context of guidance, thus improving the summary’s domain relevance. • We propose a correction model as a subsequent enhancement step to identify and rectify any potential inconsistency in the generated summary, thereby reducing intrinsic hallucination and further improving faithfulness. • Our top-performing model, using contextually rich sentences as guidance, outperforms the previous SOTA model CurrSum (Sotudeh et al., 2022a), achieving improvements of 0.40, 0.82, and 4.07 in Rouge-1, Rouge-2, and Rouge-L scores, respectively. Furthermore, it achieves a 2.5% higher FactCC score compared to Bart, and a 3.0% increase over the original GSum. Figure 2: The overall architecture: The initial phase involves a guided summarizer with a dual-encoder and an adapted decoder architecture, utilizing domain-specific guidance signals to produce a candidate summary. This is then refined in the second phase by a post-editing corrector, which identifies and corrects potential inconsistencies in the candidate summary with respect to the source document."
https://arxiv.org/html/2411.01483v3,Teaching Models to Improve on Tape,"Large Language Models (LLMs) often struggle when prompted to generate content under specific constraints. However, in such cases it is often easy to check whether these constraints are satisfied or violated. Recent works have shown that LLMs can benefit from such “corrective feedback”. Here we claim that this skill of LLMs can be significantly enhanced via training. We introduce an RL framework for teaching models to use such rewards, by simulating interaction sessions, and rewarding the model according to its ability to satisfy the constraints. We refer to our method as CORGI (Controlled Generation with RL for Guided Interaction), and evaluate it on a variety of controlled generation tasks using unlabeled training data. We find that CORGI consistently outperforms the baseline reinforcement learning method that does not incorporate conversational feedback. Furthermore, CORGI’s interactive framework enables meta-learning, allowing the LLM to generalize better to guided interaction in new tasks. Our results clearly show that conversational optimization, when combined with reinforcement learning, significantly improves the effectiveness of LLMs in controlled generation contexts.","Large Language Models (LLMs) have become quite effective at following instructions from users. Despite this progress there are still many tasks where LLMs will not respond correctly. A particular subclass of cases is those where it is relatively easy to judge whether a response is correct or not. For example, if the task is to generate a sentence with six words, it is fairly easy for to check if this constraint is violated. A natural question is whether such feedback can be used in order to make the LLM produce a correct response. This exciting prospect has been considered in several works recently, including Madaan et al. (2023) and Shinn et al. (2024). Indeed, these works show that feedback can be “written to the LLM context”, and the LLM can use this to improve its generation. Furthermore, it has been shown that LLMs can learn to utilize external tools (Schick et al. 2023; Lu et al. 2023), thereby enhancing their results and reliability. Building on the potential of feedback, we aim to explore whether LLMs can use this signal more effectively. We focus on controlled generation tasks, where LLMs often struggle (Sun et al. 2023). Specifically, we consider the important subclass of tasks where it is easy to check if the generated text satisfies the desired control requirements (e.g., the “generate sentence with six words” example above”). We hypothesize that LLMs can be trained to more effectively use corrective feedback in these cases, and we approach this goal through Reinforcement Learning (RL). We implement an RL based approach for training LLMs to improve their corrective skills. To do so, our training data consists of several controlled generation tasks. During training we let the model interact with corrective feedback over multiple iterations, and provide a reward corresponding to the provided feedback. Specifically, the delivered reward corresponds to the best feedback obtained in the interaction session. To understand why this is the reward chosen, consider a controlled generation task, where the corrective feedback says how many constraints are satisfied. Then, one can always choose the output that satisfied the most constraints. See Figure 1 for an illustration of an interaction session and corresponding reward. Figure 1: An example of the CORGI setup. We consider a dialogue between a generator and a critique. Here the generator is tasked with completing a given sentence in precisely four words, with the final word being “first”. The critique evaluates the responses of the generator, providing both feedback and a score (illustrated as a star rating). The LLM receives a reward based on the highest score assigned by the critique throughout the dialogue history. To prioritize improvement on the more challenging length constraint, we set the constraint weights to 80% for length and 20% for the last word constraint. For the above reward scheme, we train the model using RL, to obtain a model that can better use corrective signals. Our first observation is that on the tasks that the model was trained on, there is a significant improvement in performance, as compared to baselines that are not trained to use feedback. An even more surprising finding is that these trained models also improve when using feedback in tasks they were not trained on. This supports our conjecture that LLMs can improve on the “meta-skill” of using corrective feedback. Our results demonstrate the significant potential of teaching LLMs to effectively incorporate feedback into their output generation processes. By demonstrating improvements in controlled generation tasks, we highlight how LLMs can learn to refine their outputs based on corrective input, leading to more accurate and reliable results. This suggests that the ability to utilize feedback is not merely a task-specific enhancement but may represent a more generalizable skill that would demonstrate gains across a wide range of tasks."
https://arxiv.org/html/2411.01474v1,MoCE: AdaptiveMixtureofContextualizationExperts for Byte-based Neural Machine Translation,"Byte-based machine translation systems have shown significant potential in massively multilingual settings. Unicode encoding, which maps each character to specific byte(s), eliminates the emergence of unknown words, even in new languages, enabling broad language scalability. However, byte-level tokenization results in sequences that are hard to interpret due to limited semantic information per byte. Local contextualization has proven effective in assigning initial semantics to tokens, improving sentence comprehension. Nevertheless, variations in encoding rules across languages necessitate an adaptive approach for effective contextualization. To this end, we propose Adaptive MultiScale-Headed Attention (Ada-MSHA), adaptively selecting and mixing attention heads, which are treated as contextualization experts. This enhances the flexibility of contextualization scales and improves the potential to discover a better strategy than previous methods. Experiment results show that our method outperforms existing methods without extensive manual adjustment of hyper-parameters and surpasses subword-based models with fewer parameters in Ted-59 dataset. Our code is available at https://github.com/ictnlp/MoCE.","Neural Machine Translation (NMT) is a consistently hot research topic, and recent years have seen the growing significance of multilingual language modeling Zhang et al. (2023). The selection of tokenization and vocabulary is critical to multilingual language models, which plays an important role in vectorization of texts and discretization of predicted hidden states. While some models Costa-jussà et al. (2022); Dubey et al. (2024) use large vocabularies to ensure word coverage, others Touvron et al. (2023); Jiang et al. (2023) opt for byte fallback strategy. This approach allows them to completely avoid unknown words with a smaller vocabulary size. Byte-based models like ByT5 Xue et al. (2022), MEGABYTE Yu et al. (2023) and Shaham and Levy (2021) convert all words into UTF-8 byte, which further reduces the vocabulary size to about 256. This strategy significantly reduces the size of the embedding table, saving parameters and accelerating the vectorization and discretization. Besides, it eliminates the unknown-word problem and can be easily generalized to massively multilingual scenarios. Empirical study Edman et al. (2024) has also shown the performance superiority of byte-based MNT models. However, the drawbacks of byte-based models are obvious, most notably that an individual byte struggles to convey a specific semantic meaning. Therefore, various contextualization methods Lee et al. (2017); Clark et al. (2022) have been proposed to alleviate this problem. MEGABYTE Yu et al. (2023) reassembles byte streams into groups of four, constructing group representations by concatenating their hidden states. CharFormer Tay et al. (2022) and LOBEF Sreedhar et al. (2023) employ local-contextualization techniques to encode bytes, with CharFormer using mean-pooling and LOBEF using Convolutional Neural Networks (CNNs). MSC Huang and Feng (2024) argues that a byte should contribute to multiple neighboring contexts, necessitating a multi-scale contextualization approach. To this end, MSC groups hidden state dimensions and assigns CNNs with different kernel sizes to each group. Although MSC provides an effective framework for modeling multi-scale contextualization and achieved state-of-the-art performance, it suffers from a significant limitation: the scales are manually predefined. This reduces the model’s ability to generalize to multilingual scenarios, particularly in massively multilingual machine translation, which may involve over 50 languages. Under UTF-8 rule, a character may convert to 1 to 4 bytes, depending on the language. This leads to varying requirements of contextualization scale for different languages. However, once MSC decides the contextualization scales, they are unchangeable for any input. To address this, we leverage the concept of Mixture of Experts (MoE) Shazeer et al. (2017) and propose Mixture of Contextualization Experts (MoCE), which can adaptively determine CNN kernel sizes based on each input text. Specifically, we modify Multi-Head Attention (MHA) to propose a structure, called Adaptive MultiScale-Headed Attention (Ada-MSHA). This attention layer allows each head to be locally contextualized and the contextualization scales are adaptive to the input. Instead of predefined scales that MSC uses, Ada-MSHA dynamically combines different scales with model needs. The flexibility of contextualization scales is therefore significantly enhanced, resulting in better performance. Additionally, we prove that given language ID as prior knowledge is beneficial to the selection of scales. Experiment results on two massively multilingual translation datasets, Ted-59 and OPUS-100, demonstrate our proposed method outperforms other byte-based translation models with very few extra parameters. Comparing with the subword-based model, Ada-MSHA requires fewer parameters while performing better in Ted-59 dataset."
https://arxiv.org/html/2411.01343v1,AMREx: AMR for Explainable Fact Verification,"With the advent of social media networks and the vast amount of information circulating through them, automatic fact verification is an essential component to prevent the spread of misinformation. It is even more useful to have fact verification systems that provide explanations along with their classifications to ensure accurate predictions. To address both of these requirements, we implement AMREx, an Abstract Meaning Representation (AMR)-based veracity prediction and explanation system for fact verification using a combination of Smatch, an AMR evaluation metric to measure meaning containment and textual similarity, and demonstrate its effectiveness in producing partially explainable justifications using two community standard fact verification datasets, FEVER and AVeriTeC. AMREx surpasses the AVeriTec baseline accuracy showing the effectiveness of our approach for real-world claim verification. It follows an interpretable pipeline and returns an explainable AMR node mapping to clarify the system’s veracity predictions when applicable. We further demonstrate that AMREx output can be used to prompt LLMs to generate natural-language explanations using the AMR mappings as a guide to lessen the probability of hallucinations.","With the vast amount of information circulating on social media and the constantly changing Claims about various topics, automatic fact verification has become crucial for preventing the spread of misinformation. To address this need, automatic fact-checking task (Vlachos and Riedel, 2014) and several shared tasks have been introduced to encourage NLP researchers to develop systems that gather Evidence (Fact extraction) for a given Claim and classify it (Fact verification) as to its predicted veracity. Examples include FEVER (Thorne et al., 2018b, 2019) and the current AVeriTec task Schlichtkrull et al. (2023, 2024), which employ the labels Supports, Refutes, NotEnoughInfo (NEI) or ConflictingEvidence/CherryPicking. Natural Language Inference (NLI) systems, which assess whether a premise semantically entails a given hypothesis (Bowman et al., 2015), have been used for fact verification, yielding demonstrably strong results in the FEVER shared task. However, there has been limited focus on the explainability of these implementations. Recent studies (Gururangan et al., 2018; McCoy et al., 2019) have highlighted NLI models’ tendency to rely on spurious cues for entailment classification making it important to provide clear explanations alongside fact verification predictions. We design and implement a new, deterministic NLI system based on Abstract Meaning Representation (AMR), dubbed AMREx, and test it on the FEVER and AveriTeC fact-checking datasets. AMR is a rooted, directed, acyclic graph with nodes representing concepts and edges denoting the relations (Banarescu et al., 2013). This representation captures semantic relationships among entities that can be difficult to identify in a syntactic representation (Ma et al., 2023). We apply an existing AMR evaluation metric (Cai and Knight, 2013), to map Claims (e.g., X was produced Y) to relevant Evidence (e.g., X is a film produced by Y). We incorporate this mapping into our AMREx system to yield partially explainable fact verification. We assume Evidence collection has already been completed, as our focus is on the potential for explainability of our fact-checking results, independent of the degree of correctness with respect to a ground truth. This, in fact, is the key contribution of this paper: We demonstrate that explainability is valuable regardless of performance levels. If performance is high, explainability supports an exploration of the factors contributing to the algorithm’s success. If performance is low, it serves as a diagnostic tool to understand what went wrong. Claim: Veeram was produced by Vijaya Productions. Evidence: Veeram (Valour) is a 2014 Indian Tamil action film directed by Siva and produced by Vijaya Productions. AMREx Supports AMR mapping explanation Veracity produce-01 company Vijaya Production Veeram film company name Vijaya name Siva name ARG0 name op1 op2 name op1 domain ARG1-of ARG0 op1 name ARG1-of ARG0 name op1 op2 name op1 ARG1 name produce-01 company company name name name Vijaya Vijaya film name name Veeram Veeram Evidence Claim work-of-art produce-01 produce-01 Production Production Production work-of-art direct-01 person film Veeram Figure 1: Explainable fact verification pipeline. Lower Left: AMR graph for the Claim, Lower Middle: AMR graph for the Evidence, Lower Right: The AMR graph mapping to explain the model’s prediction as “Supports” Fig. 1 illustrates our explainable output using AMR graph mapping. We quantify the degree to which the Claim AMR is contained in the Evidence AMR and present the mappings identified in this process to demonstrate whether the Claim is embedded within the Evidence. For example, the Claim Veeram was produced by Vijay Productions and Evidence Veeram (Valour) is a 2014 Indian Tamil action film directed by Siva and produced by Vijaya Productions are represented as AMRs and processed through the Smatch algorithm. This identifies similar substructures between them, showing that both texts mention a production (rooted by produce-01 predicate) with similar attributes and refer to the same film (through substructures rooted by work-of-art and film in the Claim and Evidence AMRs). AMREx uses this high-level notion of meaning containment, along with a textual similarity score, to produce the veracity prediction “Supports”. Section 2 reviews existing NLI implementations and explainable representations used in fact verification. Section 3 provides a detailed description of AMREx system and the experiments conducted. Section 4 presents an analysis and discussion of the results, with conclusions in Section 5."
https://arxiv.org/html/2411.01307v1,Can Multimodal Large Language Model Think Analogically?,"Analogical reasoning, particularly in multimodal contexts, is the foundation of human perception and creativity. Multimodal Large Language Model (MLLM) has recently sparked considerable discussion due to its emergent capabilities. In this paper, we delve into the multimodal analogical reasoning capability of MLLM. Specifically, we explore two facets: MLLM as an explainer and MLLM as a predictor. In MLLM as an explainer, we primarily focus on whether MLLM can deeply comprehend multimodal analogical reasoning problems. We propose a unified prompt template and a method for harnessing the comprehension capabilities of MLLM to augment existing models. In MLLM as a predictor, we aim to determine whether MLLM can directly solve multimodal analogical reasoning problems. The experiments show that our approach outperforms existing methods on popular datasets, providing preliminary evidence for the analogical reasoning capability of MLLM.","Analogical reasoning - the ability to perceive and use relational similarity between two situations or events - serves as a fundamental pillar in human cognition and creativity [11]. It constitutes a critical mechanism for discerning complex relations [21], facilitating abstract concept comprehension [12], and fostering innovative problem-solving capabilities [23]. From scientific discoveries [8] to everyday decision-making [4], the capacity for analogical reasoning plays an indispensable role in the cognitive toolkit of human intellect. Researchers in deep learning have consistently endeavored to investigate methodologies for endowing models with human-like capabilities [15]. Recently, there has been considerable exploratory work on whether it is possible to capture analogical reasoning abilities in deep learning systems [22]. Ethayarajh et al. [9] devote to word analogy recognition, which can be effectively solved by word embeddings. Some studies have further evaluated the analogical thinking ability of pre-trained language models [3, 27]. The latest research [30, 33] provides preliminary evidence that Large Language Model (LLM) possesses analogical reasoning abilities. Meanwhile, many attempts [38, 14, 20] in visual analogical reasoning primarily focus on integrating relational, structural, and analogical reasoning to enhance model intelligence. Figure 1: Humans often establish initial cognitive understanding through multimodal analogical reasoning when dealing with unfamiliar problems. One analogical reasoning example is Sun:Solarsystem::Nucleus:Atom.Sun:Solar\;system::Nucleus:Atom.italic_S italic_u italic_n : italic_S italic_o italic_l italic_a italic_r italic_s italic_y italic_s italic_t italic_e italic_m : : italic_N italic_u italic_c italic_l italic_e italic_u italic_s : italic_A italic_t italic_o italic_m . In practical scenarios, humans typically employ experiential knowledge (such as visual information) to engage in analogical reasoning when confronted with unfamiliar problems, thereby establishing a preliminary understanding of those problems. As illustrated in Figure 1, this type of reasoning is often multimodal. However, existing research on analogical reasoning predominantly focuses on single modality, with limited attention dedicated to studying multimodal contexts. Multimodal Large Language Model (MLLM) has recently emerged as a prominent research focus, leveraging powerful LLMs as the core mechanism to execute multimodal tasks [35]. MLLMs have learned extensive relational patternsduring self-supervised learning, which can identify and utilize these patterns without explicit training in analogical reasoning. Therefore, we aim to explore whether MLLM possesses the capability for multimodal analogical reasoning, offering a new perspective for evaluating MLLM. In this paper, we explore the application of MLLM in multimodal analogical reasoning task from two perspectives: MLLM as an explainer and MLLM as a predictor. In MLLM as an explainer, our primary focus lies on MLLM’s capacity to comprehend and describe multimodal analogical reasoning problems. We aim to enhance the performance of existing methods in multimodal analogical reasoning task by providing elaborate explanations generated by MLLM. Specifically, we unify the prompt template used in existing Multimodal Pre-trained Transformer (MPT) methods, employ MLLM to explain multimodal analogical reasoning problems, and then incorporate the explanations into the corresponding slots within the templates. On the other hand, in MLLM as a predictor, we mainly investigate whether MLLM itself can solve multimodal analogical reasoning problems, aiming to explore its intuitive reasoning capabilities. To achieve this, we structure multimodal analogical reasoning task in a natural language format tailored to MLLM and design a two-step fine-tuning framework. The first step fine-tuning aims to enable MLLM to learn background triplet knowledge, while the second step fine-tuning aims to teach MLLM the format of multimodal analogical reasoning task. To summarize, our main contributions are the following: • To our best knowledge, we are the first to explore the multimodal analogical reasoning capabilities of MLLM from two perspectives: MLLM as an explainer and MLLM as a predictor. • Experimental results demonstrate that our proposed approaches achieve state-of-the-art performance, which preliminarily proves that MLLM has multimodal analogical reasoning capability."
https://arxiv.org/html/2411.01281v1,Varco Arena: A Tournament Approach to Reference-Free Benchmarking Large Language Models,"The rapid advancement of Large Language Models (LLMs) necessitates robust evaluation methodologies. Current benchmarking approaches often rely on comparing model outputs against predefined prompts and reference outputs. Relying on predefined reference outputs hinders flexible adaptation of benchmarks to the rapidly evolving capabilities of LLMs. This limitation necessitates periodic efforts to prepare new benchmarks. To keep pace with rapidly evolving LLM capabilities, we propose a more flexible benchmarking approach. Our method, Varco Arena, provides reference-free benchmarking of LLMs in tournament style. Varco Arena directly compares LLM outputs across a diverse set of prompts, determining model rankings through a single-elimination tournament structure. This direct pairwise comparison offers two key advantages: (1) Direct comparison, unmediated by reference text, more effectively orders competing LLMs, resulting in more reliable rankings, and (2) reference-free approach to benchmarking adds flexibility in updating benchmark prompts by eliminating the need for quality references. Our empirical results, supported by simulation experiments, demonstrate that the Varco Arena tournament approach aligns better with the current Elo model for benchmarking LLMs. The alignment is measured in terms of Spearman correlation, showing improvement over current practice of benchmarking that use reference outputs as comparison anchors.","The versatility of Large Language Models (LLMs) stems from their generative capacity to address a wide array of tasks. The multi-faceted capability of LLMs enables flexible applications across numerous user scenarios, exemplified by generalist assistants (Ouyang et al. 2022; Köpf et al. 2024; Roziere et al. 2023). As versatility emerges as a core attribute of LLMs, the challenge of accurately gauging their skill becomes increasingly significant. In response to this challenge, numerous benchmarks evaluating LLM capability have emerged (Hendrycks et al. 2020; Srivastava et al. 2023; Zhong et al. 2024). Many LLM benchmarks employ formats amenable to automated scoring. Examples include benchmarks for arithmetic problems (Gao et al. 2022; Patel, Bhattamishra, and Goyal 2021), multiple-choice questions (Lin, Hilton, and Evans 2022), and code execution (Austin et al. 2021; Chen et al. 2021). While these benchmarks are valuable, they primarily assess problem-solving abilities. The need for LLM benchmark extends beyond this, as the primary value of LLMs lies in their generative capabilities. Researchers have primarily relied on pariwise comparison to evaluate and rank the generative capabilities of LLMs. A representative example of this approach is Chatbot Arena (Chiang et al. 2024), which computes Elo ratings based on a massive number of human-annotated pairwise comparisons. While Chatbot Arena benefited from the reliability of open-ended prompts from users, acquiring such a large number of annotations for benchmarking LLMs is time-consuming and resource-intensive. To address these challenges, several benchmark datasets for generation tasks have been developed. These leverage reference responses for ranking LLMs using automated LLM judges (Zheng et al. 2024) includes AlpacaEval (Li et al. 2023), Arena-Hard-Auto (Li et al. 2024), and MTBench (Zheng et al. 2023). Relying on reference outputs offers two key advantages: (1) The number of comparisons required for ranking multiple LLMs reduces linear to the number of LLMs. (2) Reference texts establish a quality bar for evaluating responses. However, we propose that directly comparing the responses of LLMs, rather than relying on reference outputs, is a more effective way to achieve accurate LLM rankings. We suggest that combined with tournament-styled match making, direct comparison of outputs can achieve a better correlation to human preference within the same number of comparisons. Additionally, reference-free benchmarking mitigates potential biases in open-ended generation tasks and provides more flexibility compared to static benchmark datasets. Figure 1: Varco Arena directly compares LLM output pairs in single-elimination tournament structure rather than comparing LLM outputs to reference text. In terms of deciding whether a certain LLM is better or worse compared to the other one, we suggest direct comparison is more intuitive and results in better separability. We devised Varco Arena, which directly compares LLM outputs without the need for reference text as a comparison baseline. We leveraged the efficiency of single-elimination tournaments, which always require a linear number of matches relative to the number of participants. For brevity, we will refer to single-elimination tournaments simply as tournaments throughout the paper. Varco Arena conducts a tournament for each prompt across all participating LLMs and then computes Elo ratings (Elo and Sloan 1978) based on the results of all matches occurred. By performing tournament matches for multiple LLMs on each prompt, we obtain relative win rates between all possible model pairs within a controlled number of matches. These win rates are then interpreted as Elo ratings, allowing us to create a comprehensive ranking of participating LLMs based on their relative performance. We validate the effectiveness of our approach, Varco Arena, against the current practice of LLM benchmarks, anchored comparison to reference, through two experiments: one is unbiased judge simulation and the other is running Varco Arena with LLM-as-a-judge on Arena-Hard-Auto benchmark dataset. These experiments confirm that our reference-free benchmarking approach outperforms current practices in terms of rank reliability, given the same number of comparisons. Notably, the simulation experiment, which emulates outcomes under an unbiased judge and controls for factors such as the number of participant LLMs, test prompts, and judge precision, demonstrates that the empirical superiority of our reference-free approach is not coincidental. Instead, it reveals the effectiveness of direct comparison between LLM pairs compared to the current practice of mediated comparison with references for ranking LLMs. To summarize, our key contributions are as follows: 1. We introduce Varco Arena, a reference-free benchmarking method for LLMs that employs direct comparison of outputs through a single-elimination tournament structure across test prompts. 2. We demonstrate, through conceptual validation and empirical results, that Varco Arena delivers more reliable rankings than current practices of comparing LLM outputs to reference texts, while requiring fewer comparisons."
https://arxiv.org/html/2411.01280v1,NLP and Education:using semantic similarity to evaluate filled gapsin a large-scale Cloze test in the classroom,"This study examines the applicability of the Cloze test, a widely used tool for assessing text comprehension proficiency, while highlighting its challenges in large-scale implementation. To address these limitations, an automated correction approach was proposed, utilizing Natural Language Processing (NLP) techniques, particularly word embeddings (WE) models, to assess semantic similarity between expected and provided answers. Using data from Cloze tests administered to students in Brazil, WE models for Brazilian Portuguese (PT-BR) were employed to measure the semantic similarity of the responses. The results were validated through an experimental setup involving twelve judges who classified the students’ answers. A comparative analysis between the WE models’ scores and the judges’ evaluations revealed that GloVe was the most effective model, demonstrating the highest correlation with the judges’ assessments. This study underscores the utility of WE models in evaluating semantic similarity and their potential to enhance large-scale Cloze test assessments. Furthermore, it contributes to educational assessment methodologies by offering a more efficient approach to evaluating reading proficiency.","Since half past the last century, the Cloze test has been used for educational purposes to assess proficiency in understanding texts in different languages Taylor (1953); Brown (1980, 2002). The task consists of the systematic filling in of gaps in a text, specifically a prose selection Bickley et al. (1970), previously adapted to the participant’s realities, and the scores of correct answers are associated with the degree of comprehension of the text by the participant. Different measures, such as exact answer, acceptable answer Brown (1980), multiple choice, and Clozentropy Darnell (1968); Lowry and Marr (1975), have been used to assess gap-filling since Taylor’s initial proposal Taylor (1953). These measures will be further examined in Section 2. The exact answer may seem easier to calculate, especially for a Cloze test applied to large and heterogeneous groups of students with insufficient time for teachers to analyze each answer individually. In Brazil, for instance, teachers usually have to manage numerous classes, and this correction method helps to provide rapid answers to students’ reading proficiency, allowing one to check the answers objectively Cunha and Santos (2010) without possible or different options. The exact answer is the most common measure adopted in large-scale studies and classroom activities due to the easier procedure to correct the gaps filled. However it is not the best way to measure comprehension since reading entails top-down and bottom-up reading strategies, and filling the gaps with a unique and specific word from the original text limits the reading approach underlying the Cloze test by avoiding other answer possibilities, like words with similar meanings like synonyms. An automatic correction of large-scale assessment that evaluates reading proficiency provides a faster and more accurate evaluation of responses. Techniques from Natural Language Processing (NLP) stand out among the ways of automating this task, as they provide the computer with forms of word representation and allow it to carry out different calculations using these representations. In this work, we suggest extending the measure by calculating the semantic similarity between the answer given and the correct one, using word embeddings (WE) models."
https://arxiv.org/html/2411.01264v1,An Innovative CGL-MHA Model for Sarcasm Sentiment Recognition Using the MindSpore Framework,"The pervasive use of the Internet and social media has introduced significant challenges to automated sentiment analysis, particularly with regard to sarcastic expressions found in user-generated content. Sarcasm conveys negative emotions through ostensibly positive or exaggerated language, complicating its detection within natural language processing tasks. To address this issue, this paper presents an innovative sarcasm detection model that integrates Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), Long Short-Term Memory networks (LSTM), and Multi-Head Attention mechanisms. The CNN component captures local n-gram features, while GRU and LSTM layers model sequential dependencies and contextual information. The Multi-Head Attention mechanism enhances the model’s ability to focus on relevant parts of the input, providing better interpretability. Experiments conducted on two datasets for sarcasm detection—Headlines and Riloff—demonstrate that the proposed model achieves an accuracy of 81.20% and an F1 score of 80.77% on the Headlines dataset, and an accuracy of 79.72% with an F1 score of 61.39% on the Riloff dataset, significantly outperforming traditional models. These results validate the effectiveness of our hybrid approach in enhancing sarcasm detection performance in social media texts.","The widespread use of sarcastic expressions in social media and user-generated content poses significant challenges for sentiment analysis tasks in Natural Language Processing (NLP). Sarcasm conveys negative emotions through superficially positive or exaggerated language, creating difficulties for existing sentiment analysis models, which struggle to accurately detect the true sentiment behind sarcasm [1]. However, the complexity of sarcastic language is often not sufficiently modeled in traditional NLP models, making sarcasm detection a key area of research. Therefore, finding better ways to handle sarcasm is critical for improving the performance of sentiment analysis models [2, 3]. The hidden emotions in sarcastic expressions are difficult for current sentiment analysis algorithms to capture, especially in unstructured social media texts [4, 5]. In contrast, we aim to leverage the efficient MindSpore deep learning framework to design a model capable of accurately capturing sarcastic language features and improving sarcasm detection accuracy. One of the main reasons affecting sarcasm detection performance is that existing sentiment analysis models fail to sufficiently capture the complex features of sarcasm in texts. Most traditional models rely on surface-level sentiment words or simple contextual relationships, but these models struggle to recognize hidden irony or double meanings in sarcasm, resulting in poor sentiment analysis performance [6]. In recent years, more studies have attempted to improve sarcasm detection performance by introducing deep learning and complex neural network models. For example, Convolutional Neural Networks (CNN) can extract local n-gram features, while Recurrent Neural Networks (RNN) are used to model the sequential dependencies in text [7, 8]. However, these methods have clear limitations, such as CNN’s inability to capture long-distance dependencies, and RNN’s tendency to face gradient vanishing problems when handling long-text sequences. Moreover, sarcastic expressions in social media texts often carry multiple layers of meaning and complex contexts, further increasing the difficulty of detection for these models [9]. To address these issues, recent approaches combining deep learning with attention mechanisms have gained widespread attention in sarcasm detection [10, 11, 12]. Particularly with the support of the MindSpore framework, these models have seen significant improvements in computational efficiency and performance. MindSpore is an end-to-end AI computing framework developed by Huawei, known for its high performance, distributed computing, and hardware acceleration advantages [13]. Its design philosophy is to provide a more flexible and efficient computing environment, especially suitable for fast training on large datasets and complex deep learning models. By utilizing MindSpore’s parallel computing and memory optimization technologies, models can efficiently process sarcastic expressions in large-scale social media data [14]. Specifically, these studies face two main challenges: (1) How to capture local features in the text without losing the understanding of global context, and (2) How to enhance the interpretability and explainability of models when dealing with sarcastic expressions [15, 16]. These challenges result in suboptimal performance when models handle complex sarcastic language, especially in cases of ambiguity and hidden emotions. Although some studies have proposed models combining CNN and RNN, these methods still fail to fully exploit the benefits of multi-head attention mechanisms and efficient computing frameworks [17, 18]. To overcome these challenges, sarcasm detection methods based on the MindSpore framework have emerged in recent years. MindSpore’s distributed architecture and support for AI hardware acceleration significantly improve the efficiency of model training and inference in complex deep learning tasks [14]. By leveraging MindSpore, our goal is to design a model that combines Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), Long Short-Term Memory Networks (LSTM), and multi-head attention mechanisms to capture the complex features in sarcastic texts. Specifically, our proposed model aims to solve the following two challenges: (1) How to effectively extract local n-gram features while maintaining an understanding of global context, and (2) How to use multi-head attention mechanisms to enhance model interpretability and the ability to capture sarcasm [19, 20]. These issues contribute to the underperformance of models in handling sarcastic texts. Recent studies based on MindSpore show that models combining deep neural networks and multi-head attention mechanisms perform significantly better on social media data, but there is still room for further optimization. In this paper, we propose a hybrid model based on the MindSpore framework that combines CNN, GRU, LSTM, and multi-head attention mechanisms to improve sarcasm detection. MindSpore’s parallel computing and hardware acceleration capabilities significantly improve the training efficiency of the model on large-scale datasets. First, CNN captures local n-gram features, effectively extracting surface features of sarcastic texts. Next, GRU and LSTM are used to model sequential dependencies, solving the gradient vanishing problem faced by traditional RNNs. Finally, the multi-head attention mechanism enables the model to automatically focus on the most informative parts of the sarcastic text, improving the understanding and explainability of sarcastic expressions. This approach not only leverages MindSpore’s highly efficient distributed computing architecture but also enhances sarcasm detection accuracy through multi-layered modeling. We conducted experiments on two widely used sarcasm detection datasets—Headlines and Riloff—and used MindSpore for efficient training and optimization. Under the same hyperparameter settings, we first proposed a hybrid model combining CNN, GRU, LSTM, and multi-head attention mechanisms and accelerated the training process using the MindSpore framework. Next, we used multiple evaluation metrics to validate the model’s performance. Experimental results show that with the support of the MindSpore framework, our model achieved optimal results in sarcasm detection tasks, significantly outperforming existing methods. Our contributions are summarized as follows: • We propose a hybrid model based on the MindSpore framework that combines CNN, GRU, LSTM, and multi-head attention mechanisms to improve sarcasm detection accuracy; • MindSpore’s efficient parallel computing significantly accelerates model training on large-scale datasets; • The multi-head attention mechanism enhances the model’s understanding of sarcastic expressions and improves interpretability;"
https://arxiv.org/html/2411.01259v1,Diversidade linguística e inclusão digital:desafios para uma IA brasileira,"Linguistic diversity is a human attribute which, with the advance of generative AIs, is coming under threat. This paper, based on the contributions of sociolinguistics, examines the consequences of the variety selection bias imposed by technological applications and the vicious circle of preserving a variety that becomes dominant and standardized because it has linguistic documentation to feed the large language models for machine learning.","1 Introdução No documento do Ministério da Ciência, Tecnologia e Inovação IA para o Bem de Todos, em que é apresentada a proposta de um Plano Brasileiro de Inteligência Artificial 2024-2028, um dos cinco objetivos listados é “Desenvolver modelos avançados de linguagem em português, com dados nacionais que abarcam nossa diversidade cultural, social e linguística, para fortalecer a soberania em IA.” MCTI (2024). A Sociolinguística é o campo da ciência que estuda as relações entre língua e sociedade, e o conjunto de trabalhos neste campo desenvolvidos no Brasil nos últimos 50 anos tem contribuições diretas para a consecução deste objetivo Freitag (2016). E é sob esta perspectiva que este objetivo é discutido neste texto. Pensando em uma IA ética e socialmente sensível, a diversidade das comunidades na sociedade se reflete também (ou, pelo menos, deveria se refletir) na diversidade das comunidades em amostras linguísticas para treinar modelos de língua em larga escala (LLMS). Uma IA ética precisa atender aos princípios de justiça, equidade, diversidade e inclusão, e no domínio linguístico, por meio da seleção das amostras de línguas e variedades de línguas que vão compor o corpus de treno dos modelos, assimetrias se acentuam, desde a exclusão ou apagamento de línguas, até a priorização de uma variedade – dita de prestígio – face às variedades consideradas não-padrão ou estigmatizadas. Os preconceitos decorrentes dessa hieraquização de variedades são reproduzidos em LLMs e geram respostas Shrawgi et al. (2024); Fleisig et al. (2024); Freitag and de Gois (2024), como já constatado no inglês afro-americano Mengesha et al. (2021); Dacon and Tang (2021); Dacon et al. (2022). Considerando o objetivo do Plano Brasileiro de Inteligência Artificial 2024-2028 que trata de diversidade linguística, primeiramente, o mito do monolinguismo do português precisa ser desfeito. Em seguida, o português falado no Brasil é apresentado sob a perspectiva da diversidade e a tensão entre variedades de prestígio e variedades ditas ""não-padrão"" que divide a sociedade. Após essa contextualização sociolinguística, são apresentadas recomendações para a constituição de amostras linguísticas brasileiras para treinar LLMs, de modo a garantir a diversidade cultural, social e linguística prevista na proposta do Plano Brasileiro de Inteligência Artificial."
https://arxiv.org/html/2411.01245v1,PMoL: Parameter Efficient MoE for Preference Mixing of LLM Alignment,"Reinforcement Learning from Human Feedback (RLHF) has been proven to be an effective method for preference alignment of large language models (LLMs) and is widely used in the post-training process of LLMs. However, RLHF struggles with handling multiple competing preferences. This leads to a decrease in the alignment of LLMs with human preferences. To address this issue, we propose Preference Mixture of LoRAs (PMoL) from the perspective of model architecture, which can adapt to any number of preferences to mix. PMoL combines Mixture of Experts (MoE) and Low Rank Adaptor (LoRA). This architecture is innovatively applied to the research of preference alignment and has achieved significant performance improvement. The expert group soft loss is used to enable MoE with the ability to mix preferences. Through comprehensive evaluation by the reward model and GPT-4o, the experiment results show that PMoL has superior preference mixing capabilities compared to baseline methods. PMoL achieves better preference alignment with lower training costs.","Currently, RLHF (Christiano et al., 2017; Ouyang et al., 2022) is a widely used method to align LLMs with human preferences in terms of style and moral values. This method requires the use of human-annotated preference data to build a reward model, and then uses reinforcement learning to train LLMs. RLHF leverages the pre-trained knowledge and capabilities of LLMs to steer LLMs towards the direction desired by humans (Gim et al., 2024; Lee et al., 2024a; Gudibande et al., 2023), resulting in LLMs that are more harmless, helpful, and controllable. Technical reports of LLMs like GPT-4 and Llama3 (Achiam et al., 2023; Dubey et al., 2024) show that they attempt to reduce potential harm and increase helpfulness of their models during the RLHF phase. However, simultaneously pursuing increased helpfulness and harmlessness is contradictory (Ganguli et al., 2022; Bai et al., 2022). For example, refusing to answer is a harmless action, but always refusing to answer is unhelpful. RLHF methods cannot satisfy diverse human preferences with a single reward (Chakraborty et al., 2024), but human preferences can also be further refined (Cui et al., 2023). How to mitigate the competing between different preferences has become a worthwhile question to research. Figure 1: Results of training full parameters (denoted as Full, with and without ZeRO-2), LoRA, and PMoL on 30,000 pieces of data with 2 epochs. The horizontal axis represents the product of GPU memory usage (GB) times time (hours). The vertical axis represents the average score. The training uses 8 GPUs. To address this issue, previous studies have tried training multiple reward models and dynamically mixing them (Bakker et al., 2022; Jang et al., 2023; Rame et al., 2024; Ji et al., 2024), or controlling different preferences by synthesizing or filtering data (Cui et al., 2023; Tunstall et al., 2023). However, these methods require stronger LLM supervision, additional data processing, or using more reward and advantage models. This introduces significant additional training costs, making the costly RLHF method even more expensive. The cost increases exponentially as the number of preferences increases. These methods also have limited portability and extensibility. Therefore, we consider a question: Is there a method that can mix multiple preferences better at a lower cost? In this work, we introduce PMoL, which mixes different preferences from the perspective of model architecture. We find that the preference mixing task can be represented as a MoE framework. Therefore, PMoL integrates multiple LoRAs using the MoE framework. By having different expert networks focus on different preference data, it mixes various preferences. PMoL coordinates all experts through a routing layer, determining each expert’s wight based on the context. By introducing Expert Group Soft Loss, PMoL groups experts. This not only enhances PMoL’s attention to each preference but also better utilizes the non-competing parts between preferences, promoting the capability of mixing multiple preferences. Additionally, Preference alignment do not perturb many tokens, and the aligned LLM still highly relies on pre-trained knowledge (Lin et al., 2023). Therefore, we introduce the Empty Expert. The empty expert occupies some of the weights, allowing the model to decode only relying on pre-trained knowledge. Due to the parallelization of the computation process, PMoL has a lower training cost compared to similar methods. As shown in Figure 1, our method uses less GPU memory, less training time, and achieves better preference mixing results. The empathy is added as the third preference in addition to helpfulness and harmlessness, using the HHRLHF (Bai et al., 2022), Extes (Zheng et al., 2023), and ESConv (Liu et al., 2021) datasets combined to form a preference dataset. PMoL is validatd the superior performance in reducing training cost and mixing multiple preferences. Our main contributions are as follows in three aspects: 1. We design PMoL, a new method that integrates multiple LoRAs within MoE framework, allowing for efficient mixing of multiple preferences. PMoL has good versatility and not limited by the type of preferences or alignment algorithms. 2. The PMoL has low training cost. Compared to other methods, PMoL does not require loading additional models and has no extra processing for reward and data, allowing it to adapt to any number of preference mixing. 3. We consider sentiment as preference and construct a multi-preference dataset that includes empathy preference. It is proven that considering empathy preference as a separate type of preference is necessary."
https://arxiv.org/html/2411.01222v3,ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT: ABlack-Box ScruBBing Attack on LLM Watermarks,"Watermarking has emerged as a prominent technique for LLM-generated content detection by embedding imperceptible patterns. Despite supreme performance, its robustness against adversarial attacks remains underexplored. Previous work typically considers a grey-box attack setting, where the specific type of watermark is already known. Some methods even necessitates knowledge about details of hyperparameters. Such prerequisites are unattainable in real-world scenarios. Targeting at a more realistic black-box threat model with fewer assumptions, we here propose ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT, a Black-Box scruBBing attack on LLM watermarks. Specifically, we formulate the watermark scrubbing attack as a constrained optimization problem by capturing its objectives with two distributions: a Watermark Distribution and a Fidelity Distribution. The optimization problem can be approximately solved using two proxy distributions. Experimental results across 12 different settings demonstrate the superior performance of ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT compared with other baselines. 111The code will be released after anonymous reviews.","Figure 1: Difference between grey-box and black-box threat models. The left part in purple represents the victim and the right part in green represents the attacker. (Top) Prior work used prior knowledge for parametrization, e.g. the green-list partition of vocabulary in KGW, which makes watermark stealing easier. (Bottom) Under a more realistic black-box setting, the problem becomes much more challenging. The rapid advancement of large language models (LLMs) has demonstrated their unimaginable potentials across various applications. Systems such as ChatGPT (OpenAI, 2023) are now seamlessly integrated into many aspects of daily life. Despite benefits, the extensive deployment of LLMs has sparked serious concerns regarding potential misuse, such as large-scale disinformation, automated spamming, and social media manipulation, thereby threatening academic integrity and intellectual property rights Bender et al. (2021); Liu et al. (2023b). Consequently, detecting LLM-generated content has emerged as a crucial focus in the discourse on LLM safety and responsible deployment Mitchell et al. (2023); Pu et al. (2023); Yang et al. (2023). Watermarking stands out as a prominent technique for detecting LLM-generated text. It injects a hidden pattern invisible to human into generated contents of a specific LLM Kirchenbauer et al. (2023a). By altering the original distribution of LLMs to a specific watermark distribution during each decoding step Kuditipudi et al. (2023); Zhao et al. (2023); Hu et al. (2023) , all model generated contents can be statistically distinguished through hypothesis testing between the watermarked and the original distributions. This approach achieves a high detection rate and can be easily deployed, with only a negligible cost in the quality of generated content. Despite its supreme performance, the robustness of watermarking methods against adversarial attacks remains underexplored. Among which, the scrubbing attack Jovanović et al. (2024) presents a notable challenge in practical settings: if an adversary can successfully paraphrase LLM-generated contents into another semantically similar but watermark-free form, the effectiveness of the watermark will be heavily compromised. Wu and Chandrasekaran (2024) and Jovanović et al. (2024) explored this question by proposing different watermark scrubbing methods. However, these approaches typically require strong prior knowledge about the specific type of watermark algorithm used in the victim LLM, even the specific hyperparameter of the algorithm (e.g. the context window size in KGW). For example, Wu and Chandrasekaran (2024) proposed a method based on the assumption that the victim LLM is protected with KGW watermarking algorithm (Kirchenbauer et al., 2023a), so that they can parameterize their attack model based on the green-red list partition. Such prerequisites are virtually unattainable in real-world scenarios, especially when we can only access the victim LLM via an API interface. The stringent constraint on prior information makes these attacking methods impractical for real-world applications. In this work, we consider a more realistic black-box threat model with the sole prior knowledge of watermark existence. Under this attack setting, we further propose ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT (Black-Box scruBBing attack), a universal watermark attack method, specially designed for the black-box threat model. With less assumption about the victim LLM, this method is more practical for real-life usage, allowing us to accurately explore the robustness of current watermark techniques. The intuition of ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT is rooted in two fundamental objectives of an ideal watermark scrubbing attack: the adversarial texts should both exhibit minimal watermark patterns to evade from detection (Efficacy) and preserve the semantic information of the original texts (Fidelity). Drawing on this insight, we propose to capture these dual properties with distance to two distributions respectively: a Watermark Distribution and a Fidelity Distribution. Consequently, we formulate the task of watermark scrubbing attack into a constrained optimization problem. The local optimal of this problem can be solved by approximating these distributions with two proxies. Specifically, we steer a much smaller proxy model to approximate the Watermark Distribution of the victim LLM through distillation, and apply a general paraphrase model to obtain a distribution similar to the original texts for the Fidelity Distribution. Compared to baseline watermark srubbing methods, our proposed ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT improves the attack success rate by up to 68.13% across different victim LLMs and watermarking method settings. Since there is an inherent trade-off between attack efficacy and semantic fidelity, we further demonstrate the superior performance of ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT over baselines using Pareto fronts: ℬ4superscriptℬ4\mathcal{B}^{4}caligraphic_B start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT removes text watermarks more efficiently under the same fidelity constraints, while also distorting less semantic information to achieve the same level of attack. In summary, our contributions include: (1) formalizing the watermark scrubbing attack as a constrained optimization problem for the first time, and (2) developing a black-box watermark scrubbing method based on this formulation, which demonstrates superior performance compared to previous approaches."
https://arxiv.org/html/2411.01213v1,"One Arrow, Many Targets: Probing LLMs for Multi-Attribute Controllable Text Summarization","Text summarization is a well-established task within the natural language processing (NLP) community. However, the focus on controllable summarization tailored to user requirements is gaining traction only recently. While several efforts explore controllability in text summarization, the investigation of Multi-Attribute Controllable Summarization (MACS) remains limited. This work addresses this gap by examining the MACS task through the lens of large language models (LLMs), using various learning paradigms, particularly low-rank adapters. We experiment with different popular adapter fine-tuning strategies to assess the effectiveness of the resulting models in retaining cues and patterns associated with multiple controllable attributes. Additionally, we propose and evaluate a novel hierarchical adapter fusion technique to integrate learnings from two distinct controllable attributes. Subsquently, we present our findings, discuss the challenges encountered, and suggest potential avenues for advancing the MACS task.","Powerful Large Language Models like GPT-3, GPT-4 Brown et al. (2020); OpenAI et al. (2024) have been found to perform on par with humans on generic summarization tasks Tanya Goyal (2022); Liu et al. (2023b); Zhang et al. (2024). Generic Summarization is highly open-ended and subjective, and as such, many possible acceptable summaries exist for a given input; therefore, it becomes difficult to precisely evaluate the qualitative differences between many acceptable summaries Liu et al. (2024b). In practical settings, such summaries might not be the most useful, as different users have different requirements for different summaries. Hence many recent works have focused on the study of Controllable Text Summarization (CTS) Goyal et al. (2022); Urlana et al. (2024); Zhang et al. (2023c); Liu et al. (2024b). Controllable Instruction Write a summary of the source text. The summary should be long in length. The length is defined in terms of number of words used in the summary.The summary should be focussed on the topic police. The source text is given below. Source Please, not even a demonstration. Freddie Gray’s family had asked there be quiet on Baltimore’s streets the day they laid him to rest. And above all, no violence. Raging hordes turned a deaf ear to that on Monday. But a handful of people repeated the family’s message. They […] Controlled Summary On the day of Freddie’s funeral, the Gray family called for silence and peace, but a group of youths began rioting in Baltimore in the afternoon. A peace group tried to dissuade them, but there were too many. The crowd threw stones at the police. Officers in riot gear backed away. Different groups of gang members met with community leaders and the Gray family and signed a peace agreement, although police said the gangs just want to remove law enforcement officers. The Gray family’s attorney asked for a show of hands for those who had experienced police brutality or knew someone who had, and most raised their hands. Generic Summary Members of different Baltimore gangs, who condemned the rage in the neighborhood, gathered at the New Shiloh Baptist Church for a press conference with community leaders and the Gray family and signed a peace agreement. Figure 1: Example of Task of Multi Attribute Controllable Summarization CTS aims to create summaries that fulfill specific criteria specified by the user, by manipulating conditions on various controllable attributes (CAs). These attributes include factors such as summary length Zhang et al. (2023c); Goyal et al. (2022); Takase and Okazaki (2019), topic Krishna and Srinivasan (2018); Zhang et al. (2023c), or even degree of extractiveness Goyal et al. (2022); Zhang et al. (2023c). The goal is to generate summaries that not only condense the source material but also adhere to particular requirements set by the user for the task at hand. However, most studies have limited themselves to settings where only one aspect is to be controlled. However, users might often require models to control multiple attributes simultaneously. For example, a user might request ""a short summary focusing on Lionel Messi."" Hence, we study the LLM’s ability to generate summaries that satisfy multiple controllability requirements. This is a hard challenge as LLM’s have to focus on what might often be orthogonal or even conflicting requirements. To this end, we explore the ability of existing LLMS on this task of ""Multi-Attribute Controllable Summarization(MACS)"" in the zero-shot setting and explore various parameter-efficient fine-tuning strategies that allow these models to learn to do this task effectively in the following two settings: 1. Where we have access to labeled data triplets (controllable aspect-value pair, input text, human-labeled summaries) for multiple attributes together and 2. where we have access to such datasets for individual attributes independently. We investigate the trade-off involving using a dataset where we have labeled summaries for multiple attributes jointly vs. combining individual attribute-specific datasets together under a variety of experimental setups. We leverage recent advances in the Parameter Efficient Finetuning Techniques(PEFT), specifically LoRA Hu et al. (2022) to keep the computational cost of such fine-tuning minimal. We primarily investigate two research questions 1. How effectively do modern Large Language Models (LLMs) handle multi-attribute controllable summarization? Specifically, how do they manage the challenges of simultaneously controlling multiple attributes (such as length, extractiveness, and topic) in summarization tasks, especially when these control parameters may conflict with or operate independently of each other? 2. Can models trained to control attributes like length or topic independently be effectively combined in a joint setting to control both attributes simultaneously?"
https://arxiv.org/html/2411.01205v1,PRIMO: Progressive Induction for Multi-hop Open Rule Generation,"Open rule refer to the implication from premise atoms to hypothesis atoms, which captures various relations between instances in the real world. Injecting open rule knowledge into the machine helps to improve the performance of downstream tasks such as dialogue and relation extraction. Existing approaches focus on single-hop open rule generation, ignoring multi-hop scenarios, leading to logical inconsistencies between premise and hypothesis atoms, as well as semantic duplication of generated rule atoms. To address these issues, we propose a progressive multi-stage open rule generation method called PRIMO. We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure consisting of generation, extraction, and ranking modules to fully leverage the latent knowledge within the language model across multiple dimensions. Furthermore, we employ reinforcement learning from human feedback to further optimize model, enhancing the model’s understanding of commonsense knowledge. Experiments show that compared to baseline models, PRIMO significantly improves rule quality and diversity while reducing the repetition rate of rule atoms. Keywords: Open rule, Pre-trained language model, Reinforcement learning from human feedback","1. Introduction Rules usually refer to objective regularities or logical relationships of domain concepts, usually expressed in the form of “if-then” statement Novák and Lehmke (2006). Rules can describe most complex knowledge, while naturally incorporating domain-specific knowledge Chi (2010). When reasoning based on rule, users can intuitively understand the process logically Wason (1968). Therefore, rules are widely used in downstream applications, such as intelligent data analysis Becquet et al. (2002) and knowledge discovery García-Vico et al. (2018). For example, Lin et al. Lin and Pantel (2001) suggested that rule-based reasoning can quickly narrow the search space in question answering. Rule generation aims to discover rules that satisfy logical constraints from large amounts of data. Traditional research has been devoted to generating rules by observing data commonalities. For example, one of the core tasks of Inductive Logic Programming (ILP) Muggleton (1999) is to mine rules in the form of Horn clauses from data Raedt and Kersting (2004). Since the axioms of the rules are restricted to the entities and relations already present in the given context, this leads to a limited and fragile expression of such rules Wrobel (2001). Furthermore, these methods have weak generalization capabilities due to the scale of the knowledge source Muggleton (1999). In recent years, some researchers proposed open rule generation, aiming to generalize more diverse rules from large-scale open KBs. Hwang et al. proposed COMET Hwang et al. (2021), a pre-trained language model that can learn commonsense from natural language. Given any text, COMET can generate new rules of the form of If-Then statement. However, COMET’s training dataset, ATOMIC2020 Sap et al. (2019), contains only 23 manually defined relations, which limits the types of rules that can be generated. Orion Cui and Chen (2021) adopts an unsupervised approach to utilize the knowledge in the Pre-trained Language Models (PLM) to automatically mine open rules. However, since it does not consider the ontological information of the entities in the rules, Orion tends to generate rules that are not logically self-consistent. Existing approaches focus on single-hop open rule generation, i.e. the generation of multiple parallel hypothesis atoms based on given premise atoms. However, it is difficult to extend to some complex scenarios due to the short chain of rule reasoning and weak expression of complex logic capabilities, such as multi-round dialogues. In multi-hop open rule generation, the currently generated hypothesis atom must take into account all previously generated rule atoms, which places higher demands on the model’s reasoning ability. In addition, multi-hop open rule generation requires the model to own global information awareness. Existing methods do not have long-term context awareness, which leads to logical incoherence between atoms. To address these issues, we propose PRIMO — a ProgRessive multi-stage Induction method for Multi-hop Open rule generation. By introducing ontological information of entities into the hypothetical atom generation procedure, the generation of incorrect rules can be effectively mitigated. Considering the reasoning challenges of multi-hop open rules, we introduce generation, extraction and ranking modules in each sub-rule generation phase. Multiple modules are connected through the designed prompt, and the modules collaborate with each other to progressively generate multi-hop open rules. To reduce the repeated generation of rule atoms, we update the prompt after each derivation to learn the prior information of the generated atoms. Finally, after fine-tuning the model, we construct reward signals based on human feedback, which further enhance reasoning with common sense through reinforcement learning. To evaluate the effectiveness of multi-hop open rule generation, we constructed a benchmark dataset and evaluated various systems using a wide range of automated metrics and human judgement.The results show that PRIMO effectively improves performance by splitting rule generation into multiple stages. Moreover, thanks to the stage-wise updating strategy of prompts, our approach significantly reduces the generation of repetitive atoms. It outperforms a series of baseline models and achieves performance close to LLM, confirming the effectiveness and superiority of PRIMO."
https://arxiv.org/html/2411.01195v1,Transfer Learning for FinetuningLarge Language Models,"As the landscape of large language models expands, efficiently finetuning for specific tasks becomes increasingly crucial. At the same time, the landscape of parameter-efficient finetuning methods rapidly expands. Consequently, practitioners face a multitude of complex choices when searching for an optimal finetuning pipeline for large language models. To reduce the complexity for practitioners, we investigate transfer learning for finetuning large language models and aim to transfer knowledge about configurations from related finetuning tasks to a new task. In this work, we transfer learn finetuning by meta-learning performance and cost surrogate models for grey-box meta-optimization from a new meta-dataset. Counter-intuitively, we propose to rely only on transfer learning for new datasets. Thus, we do not use task-specific Bayesian optimization but prioritize knowledge transferred from related tasks over task-specific feedback. We evaluate our method on eight synthetic question-answer datasets and a meta-dataset consisting of 1,800 runs of finetuning Microsoft’s Phi-3. Our transfer learning is superior to zero-shot, default finetuning, and meta-optimization baselines. Our results demonstrate the transferability of finetuning to adapt large language models more effectively.","The landscape of large language models (LLMs) rapidly expands to a zoo of models (Team, 2024a; Abdin et al., 2024; Liu et al., 2024; DeepSeek-AI et al., 2024; Dubey et al., 2024; Jiang et al., 2023; Mistral AI, 2024; Team, 2024b; Yang et al., 2024), where different models exhibit varying strengths on specific tasks (Wei et al., 2022). At the same time, the landscape of parameter-efficient finetuning methods rapidly expands (Hu et al., 2021; Dettmers et al., 2023; Poth et al., 2023; Hayou et al., 2024). Consequently, practitioners face a multitude of complex choices for finetuning LLMs. To support practitioners and reduce complexity, we investigate transfer learning of deep-learning pipelines for an LLM and specifications for the finetuning process, including all associated hyperparameters. We aim to transfer knowledge about pipelines from related finetuning tasks to a new task. Thus enabling practitioners to adapt LLMs more effectively to new tasks. In this work, we transfer learn finetuning by meta-learning performance and cost surrogate models for grey-box meta-optimization from a new meta-dataset. We implement grey-box meta-optimizing by adjusting the Quick-Tune algorithm (Arango et al., 2024). Quick-Tune, was introduced for image classification and supports meta-learning surrogate models. In our version, we propose to rely only on the meta-learned surrogate models trained from scratch. That is, we do not use task-specific Bayesian optimization because we do not refit the surrogate models for a new dataset. In other words, our version of Quick-Tune can be understood as a dataset-aware portfolio builder (Xu et al., 2010). While counter-intuitive, we hypothesize that disabling Bayesian optimization leads to better generalization. We verify the effectiveness of our method for large language models by generating a meta-dataset based on a synthetic question-answer dataset and 1,800 runs of pipelines for finetuning Microsoft’s Phi-3 model (Abdin et al., 2024). Our results show that transfer learning finetuning is superior to random search, DEHB (Awad et al., 2021), and Quick-Tune with Bayesian optimization. Moreover, meta-optimizing finetuning is, as expected, better than zero-shot and default LoRa (Hu et al., 2021). Our Contributions. To make LLMs more easily adaptable and facilitate future studies, we contribute (1) synthetic datasets that serve a dual purpose: a) to create a meta-dataset for transfer learning and b) as an evaluation framework for LLM models; (2) a version of Quick-Tune for LLM finetuning adapted from the image to language domain; and (3) a novel counter-intuitive yet effective approach to finding the optimal pipeline for finetuning LLMs through transfer learning."
https://arxiv.org/html/2411.01192v2,"Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks","We introduce Swan, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research.","Natural Language Processing (NLP) has seen rapid advancements in recent years, driven by groundbreaking developments in deep learning and the emergence of sophisticated distributed text representations such as word and sentence embeddings Devlin et al. (2018); Reimers and Gurevych (2019). These embeddings, which transform text into dense vectors, enable effective semantic understanding and are pivotal for enhancing performance across many downstream applications, including text classification, semantic search, and machine translation. Moreover, text embeddings have become fundamental to the success of large language models (LLMs) Touvron et al. (2023b, a); Jiang et al. (2023); Team et al. (2024), which are increasingly integrated into a variety of real-world systems and tools. One of the most promising applications of these embeddings is in the realm of Retrieval-Augmented Generation (RAG) Shao et al. (2023); rag (2023), where LLMs are augmented with information retrieval capabilities. In RAG-based systems, lightweight embedding models retrieve relevant information from large corpora, fed as context to models like ChatGPT OpenAI (2023) or GPT-4 OpenAI et al. (2024). This synergy between embeddings and LLMs has demonstrated significant improvements in both general-purpose tasks such as question answering Lin et al. (2023); rag (2023) and domain-specific applications Bhatia et al. (2024); Shi et al. (2023); Lin et al. (2023). Figure 1: Details of ArabicMTEB Despite these advances, the predominant focus of current embedding models has been on English and Chinese, which limits their applicability to other languages. This is particularly true for Arabic, a language with rich morphology, diverse dialects, and unique syntactic structures, making it challenging to develop effective language representations Nagoudi et al. (2022); Huang et al. (2024). Existing multilingual models often fail to capture these complexities, leading to a suboptimal performance on Arabic NLP tasks Abdul-Mageed et al. (2020a); Elmadany et al. (2022). Addressing this limitation requires the development of Arabic-specific embedding models that are sensitive to the linguistic and cultural nuances of the language. In this work, we introduce Swan, a family of dialect-aware Arabic-centric cross-lingual cross-cultural embedding models designed to bridge this gap and push the boundaries of Arabic NLP. Our contributions are as follows:(1) We introduce Swan, a cutting-edge family of Arabic embedding models. This includes two variants: Swan-Small, based on ARBERTv2 Elmadany et al. (2022), and Swan-Large, built upon ArMistral, a further pretrained Arabic language model. (2) We present ArabicMTEB, a comprehensive evaluation benchmark for Arabic text. ArabicMTEB is designed to assess cross-lingual, multi-dialectal, multi-domain, and multi-cultural performance, spanning eight tasks and 94949494 datasets. (3) Our larger model, Swan-Large, showcases state-of-the-art text embedding capabilities, surpassing Multilingual-E5-large Wang et al. (2024b) on most Arabic tasks. Additionally, our smaller, Swan-Small, consistently outperforms Multilingual-E5-base Wang et al. (2024b) on most Arabic tasks. (4) Through rigorous benchmarking, we demonstrate that Swan models are not only dialectally and culturally aware but also excel across diverse Arabic domains while maintaining a significantly lower monetary cost. The rest of the paper is organized as follows: In Section 2, we review related work with a particular emphasis on Arabic text embedding models, their applications and challenges. We present our approach to model training Swan models in Section 3. Section 4 outlines how we built our benchmark dataset, ArabicMTEB. Section 5 is about our experiments and model analysis. Finally, we conclude in Section 6."
https://arxiv.org/html/2411.01176v1,CmdCaliper: A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research,"This research addresses command-line embedding in cybersecurity, a field obstructed by the lack of comprehensive datasets due to privacy and regulation concerns. We propose the first dataset of similar command lines, named CyPHER 111CyPHER: CyCraft’s Paired Command-Lines Harnessed for Embedding Research, for training and unbiased evaluation. The training set is generated using a set of large language models (LLMs) comprising 28,520 similar command-line pairs. Our testing dataset consists of 2,807 similar command-line pairs sourced from authentic command-line data.In addition, we propose a command-line embedding model named CmdCaliper, enabling the computation of semantic similarity with command lines. Performance evaluations demonstrate that the smallest version of CmdCaliper (30 million parameters) suppresses state-of-the-art (SOTA) sentence embedding models with ten times more parameters across various tasks (e.g., malicious command-line detection and similar command-line retrieval).Our study explores the feasibility of data generation using LLMs in the cybersecurity domain. Furthermore, we release our proposed command-line dataset, embedding models’ weights and all program codes to the public. This advancement paves the way for more effective command-line embedding for future researchers.","Figure 1: After fine-tuning our proposed similar command-line pair dataset, CyPHER, our proposed command-line embedding model, CmdCaliper, can effectively embed command lines based on their semantics rather than solely on appearance. Sentence embeddings, which map diverse sentences into a unified semantic feature space, are critical for various NLP applications such as classifier training, visualization van der Maaten and Hinton (2008), and retrieval-augmented generation (RAG) Lewis et al. (2020). In cybersecurity, command lines provide invaluable information for detecting malicious attacks by comparing them with known historical malicious command lines from a semantic perspective. However, the flexibility in command-line syntax and structure poses challenges for fully leveraging this information. For example, as shown in Fig. 1, one can still correlate the two command lines according to their outputs despite the different appearances. To achieve this, using a robust embedding model to calculate the semantic similarity of command lines is promising. However, the grammatical differences between command lines and natural language sentences hinder the direct application of sentence embedding models to command-line tasks. Furthermore, one main challenge exacerbates the difficulty of research in command-line embedding: the scarcity of datasets specifically designed for command-line embedding tasks, both for training models and for fairly evaluating the performance of different methods. To address the aforementioned challenges, this paper introduces the first comprehensive dataset, CyPHER, which includes semantically similar pairs of command lines for both training and evaluating command-line embedding methodologies. Inspired by the successes of data synthesis by LLMs Wang et al. (2023b, a), the similar command-line pairs in our training set are automatically generated from a set of diverse command-line seeds initialized from multiple real-world sources by a total of six distinct LLMs trained on diverse datasets (§ 3.1). This facilitates a broader range of command-line generation. For the testing set of CyPHER, to prevent training data leakage, we directly employed a totally different data source instead of synthesizing command lines by LLMs, as done in the training set. (§ 3.2) To the end, our training set consists of 28,520 similar command-line pairs, totaling 55,909 unique command lines, and our testing set comprises 2,807 similar command-line pairs, totaling 5,576 unique command lines. Our dataset analysis and human evaluation results (§ 5) demonstrate that our pipeline can generate highly diverse and high-quality similar command-line pairs. Based on our proposed dataset, CyPHER, we also developed the first embedding model specialized for command-line embeddings, called CmdCaliper. By encouraging semantically similar samples to come closer and simultaneously increasing the distance between semantically dissimilar samples in the embedding space, CmdCaliper can embed command lines into vectors from a semantic perspective. As demonstrated in Fig. 1, even when command lines differ in appearance, CmdCaliper can still position them closely in the embedding space based on their semantic meanings. Our evaluation results (§6) demonstrate that even the smallest version of CmdCaliper, with approximately 0.03 billion parameters, can surpass SOTA sentence embedding models with ten times more parameters (0.335 billion parameters) across various command-line specific tasks, such as malicious command-line detection, similar command-line retrieval, and command-line classification. Our contribution is threefold. First, we propose the first dataset of similar command-line pairs named CyPHER, which allows for training and performance evaluation. Through detailed validation of the dataset’s effectiveness, we believe it is well-suited for further command-line research. Secondly, we explore the potential of using LLMs to synthesize command-line data in the cybersecurity domain. Our experiments demonstrate that LLMs can indeed generate high-quality and diverse data. Lastly, we propose the first semantic command-line embedding model, CmdCaliper. Our evaluations reveal that a command-line-specific embedding model significantly enhances performance across various downstream tasks compared to generic sentence embedding models. We open-source the entire dataset, model weights, and all program codes under BSD License at GitHub Repo222https://github.com/cycraft-corp/CmdCaliper"
https://arxiv.org/html/2411.01141v1,Dictionary Insertion Prompting for Multilingual Reasoning onMultilingual Large Language Models,"As current training data for Large Language Models (LLMs) are dominated by English corpus, they are English-centric and they present impressive performance on English reasoning tasks.111Most of the LLMs are English-centric due to imbalanced training corpus, and this paper primarily studies English-centric LLMs, but our method could be universal by using the centric language in the dictionary for non-English-centric LLMs. Yet, they usually suffer from lower performance in other languages. There are about 7,000 languages over the world, and many are low-resourced on English-centric LLMs. For the sake of people who primarily speak these languages, it is especially urgent to enable our LLMs in those languages. Model training is usually effective, but computationally expensive and requires experienced NLP practitioners. This paper presents a novel and simple yet effective method called Dictionary Insertion Prompting (DIP). When providing a non-English prompt, DIP looks up a word dictionary and inserts words’ English counterparts into the prompt for LLMs. It then enables better translation into English and better English model thinking steps which leads to obviously better results. We experiment with about 200 languages from FLORES-200. Since there are no adequate datasets, we use the NLLB translator to create synthetic multilingual benchmarks from the existing 4 English reasoning benchmarks such as GSM8K and AQuA. Despite the simplicity and computationally lightweight, we surprisingly found the effectiveness of DIP on math and commonsense reasoning tasks on multiple open-source and close-source LLMs.222Our dictionaries, code, and synthetic benchmarks will be open-sourced to facilitate future research.","In the quick development with large language models (LLMs), there have been quite many popular research areas such as chain-of-thought reasoning (Wang et al., 2023; Wei et al., 2024), machine translation (Lu et al., 2023; Zhu et al., 2024), code generation (Li et al., 2023; Zhang et al., 2023), and even spatial understanding (Hu et al., 2024). Among these, an important research area is multilingual large language models (MLLMs), which consider not only the tasks of machine translation but also reasoning tasks represented in different languages (Huang et al., 2023). This scales the horizon of English-centric LLMs such as popular ChatGPT and enables them to be used by people who mainly speak low-resourced languages. Yet, current methods are usually training-based (Lu et al., 2024; Lim et al., 2024), which usually requires many GPU/TPU computational resources to update model weights from LLMs. Figure 1: An illustrated comparison of the GSM8K dataset made up in Buginese. Compared to the standard prompting baseline, DIP inserts additional dictionary knowledge written in knowledge in an interleaving manner. This leads to a better intermediate English translation and succeeding English thought. Finally, DIP produces promising results, surpassing several strong baselines. In contrast, this paper investigates how to incorporate a dictionary as an auxiliary knowledge into prompting. In comparison, this is lightweight and flexible, where the dictionary can be customized and replaced in a plug-and-play manner without model training. While the dictionary-based method has been studied on the task of traditional machine translation (Arthur et al., 2016), how to incorporate them into reasoning tasks on MLLMs has been under-studied. This is yet important and needs to be empirically justified whether, and how dictionary-based methods can be better used to improve multilingual reasoning tasks, which obviously enhances LLMs’ usefulness in our daily life. To this end, we propose a novel method called Dictionary Insertion Prompting (DIP). We present the overall algorithm as in Figure 1. By providing a customized dictionary that maps words represented in low-resourced languages into English, DIP inserts the English representation into the original input. This then helps LLMs in pivoting the original input into a complete English representation. This improves the succeeding chain of thought reasoning, which results in a better final output. By looking deeply into the experimental analysis, we found that the place of insertion is an important factor in improving the performance of DIP. While the prior dictionary-based method usually presents the dictionary in front of the prompt, we found it sub-optimal, and placing the dictionary in an interleaving manner is better. We postulate that such a design puts words with their English counterparts closer, and makes it easy to be understood by LLMs. Experiments also show that better translation quality and thought quality are the key factors to the usefulness of DIP. We benchmark DIP on about 200 languages from FLORES-200 (NLLB-Team, 2022). Since there are no adequate datasets for covering those languages, we use a high-quality SOTA translator NLLB 3.3B333https://huggingface.co/spaces/Narrativaai/NLLB-Translator to translate existing arithmetic reasoning benchmarks such as GSM8K (Cobbe et al., 2021) and commonsense reasoning benchmarks such as Date (Srivastava et al., 2022). In general, we make three key contributions. • We propose a simple, novel, yet computationally lightweight approach called DIP for better reasoning on multilingual tasks on LLMs. • Extensive strong results across several benchmarks on ChatGPT and Llama LLMs verify the effectiveness of DIP. • We investigated further why DIP is useful and found better intermediate translation and thinking steps are the key. In addition, our code and the synthetic benchmark we created will be open-sourced upon publication."
https://arxiv.org/html/2411.01136v1,Do LLMs Know to Respect Copyright Notice?,"Prior study shows that LLMs sometimes generate content that violates copyright. In this paper, we study another important yet underexplored problem, i.e., will LLMs respect copyright information in user input, and behave accordingly? The research problem is critical, as a negative answer would imply that LLMs will become the primary facilitator and accelerator of copyright infringement behavior. We conducted a series of experiments using a diverse set of language models, user prompts, and copyrighted materials, including books, news articles, API documentation, and movie scripts. Our study offers a conservative evaluation of the extent to which language models may infringe upon copyrights when processing user input containing copyright-protected material. This research emphasizes the need for further investigation and the importance of ensuring LLMs respect copyright regulations when handling user input to prevent unauthorized use or reproduction of protected content. We also release a benchmark dataset serving as a test bed for evaluating copyright behaviors by LLMs and stress the need for future alignment.","The emergence of Large Language Models (LLMs), powerful models that generate human-like text and excel in various natural language processing tasks (Khurana et al., 2023; Brown et al., 2020; Ouyang et al., 2022), has transformed the landscape of artificial intelligence. However, as LLMs become more sophisticated and ubiquitous, concerns have arisen regarding their potential to produce content that violates copyright laws. Figure 1: LLM Responses using Parametric Knowledge vs. Given Context. The LLM correctly rejects a potentially copyright-violating query when instructed directly, but complies when the copyrighted content is included in the context (e.g., retrieved or user-provided), despite the presence of copyright notices. Previous research has primarily focused on investigating the occurrence of copyright violations in the output generated by LLMs Carlini et al. (2021); Karamolegkou et al. (2023). While this line of inquiry is crucial, it is equally important to examine another critical aspect: do LLMs know to respect copyright information in user input and adjust their behavior accordingly? We raise this question by observing two prevalent use cases of LLMs: (1) users input a private document and subsequently ask questions or request task completion based on the provided document, and (2) LLM interfaces employ retrieval augmented generation (RAG) techniques to retrieve relevant online text to enhance and contextualize user prompts. What if copyrighted material is involved in these use cases? The research question is of vital importance because if LLMs fail to recognize and adhere to copyright information provided by users, they risk becoming the most significant incubators and facilitators of copyright infringement. Such a scenario would not only undermine the integrity of the creative industry but also raise serious ethical and legal concerns surrounding the use of LLMs. Figure 1 illustrates a few examples that highlight the potential consequences of LLMs disregarding copyright information in user input. This paper seeks to shed light on this critical problem by conducting a comprehensive analysis of how LLMs handle copyrighted content provided by users. By examining the behavior of LLMs in response to user input containing copyright notices, disclaimers, and other relevant information, we aim to uncover whether these models are equipped to respect and act upon such information appropriately. The findings of this research will contribute to the ongoing discourse on the responsible development and deployment of LLMs, and provide valuable insights for stakeholders involved in the creation, regulation, and use of these powerful tools. The code and data used in this work will be released.111https://github.com/liamjxu/copyright Our main contributions are as follows: • We propose a research problem of whether LLMs respect copyright information in user input and behave accordingly, which is important yet less studied. Moreover, we create a benchmark dataset consisting of 43,200 simulated user queries, covering various aspects of copyright. • We conduct extensive experiments on multiple popular LLMs and show that many popular LLMs do not respect copyright information in the user input. We also provide analysis and insights concerning the different types of copyright notices and query framings. • We explore two simple but effective query modification methods that help mitigate LLMs’ violation of copyright, and we show their benefits are accumulative when combined."
https://arxiv.org/html/2411.01101v1,How Effective Is Self-Consistency for Long-Context Problems?,"Self-consistency (SC) has been demonstrated to enhance the performance of large language models (LLMs) across various tasks and domains involving short content. However, does this evidence support its effectiveness for long-context problems?This study examines the role of SC in long-context scenarios, where LLMs often struggle with position bias, hindering their ability to utilize information effectively from all parts of their long input context. We examine a range of design parameters, including different models, context lengths, prompt formats, and types of datasets and tasks. Our findings demonstrate that SC, while effective for short-context problems, fundamentally fails for long-context tasks—not only does it fail to mitigate position bias, but it can also actively degrade performance. We observe that the effectiveness of SC varies with context length and model size but remains mainly unaffected by prompt format or task type. These results provide valuable insight into the limitations of current LLMs in long-context understanding and highlight the need for more sophisticated approaches to address position bias in these models.","Large Language Models (LLMs) have shown remarkable versatility in performing various tasks through prompting (Brown et al., 2020; Srivastava et al., 2023). However, these models exhibit various forms of brittleness across many tasks (Mishra et al., 2022; Frieder et al., 2024; Mirzadeh et al., 2024; Wu et al., 2024), with some studies demonstrating catastrophic failure on even short, simple common sense reasoning problems easily solvable by humans (Nezhurina et al., 2024). To address these challenges, self-consistency (SC) (Wang et al., 2022; Chen et al., 2024; Lin et al., 2024) has emerged as a versatile and general-purpose strategy that enhances response reliability by aggregating multiple sampled responses. Although the existing literature supports SC as a valuable approach to improving LLM responses, most successes have been in tasks that are relatively short (e.g., ≤100absent100\leq 100≤ 100 tokens) by today’s standards. Consequently, it is essential to carefully examine the effectiveness of SC in long-context problems (e.g., ≥1⁢Kabsent1𝐾\geq 1K≥ 1 italic_K tokens). Figure 1: Schematic of self-consistency for long-context, “needle-in-a-haystack,” scenarios. Input consists of a query and multiple sources of evidence (e.g., documents, stories), one of which contains the query’s answer. SC generates multiple intermediate answers (with non-zero temperature), which are then aggregated to produce the final answer. In this paper, we study the interaction between self-consistency (SC) and the challenges that arise in long-context problems. Recent literature highlights significant challenges in long-context tasks, e.g., position bias, where the placement of context information significantly affects performance, often preventing models from making full use of context (Wang et al., 2023; Zheng et al., 2023a; Liu et al., 2024). This bias can result in the underutilization of relevant information, compromising the accuracy and reliability of model responses. While SC has shown potential in enhancing LLM performance across various tasks, its impact on position bias in long-context scenarios remains underexplored. Addressing this knowledge gap is crucial, given the growing importance of long-context understanding in real-world LLM applications such as document summarizing, information aggregation, and complex reasoning tasks (Shaham et al., 2022, 2023). Our research questions are: (Q1subscript𝑄1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) Does self-consistency improve the overall performance in long-context tasks? (§4.1) (Q2subscript𝑄2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) How does self-consistency interact with position bias? (§4.2) (Q3subscript𝑄3Q_{3}italic_Q start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) How robust is self-consistency to variations in model size, task type, prompt format, or SC parameterization? (§4.3) We explore these research questions within the broader “needle-in-haystack” framework (Kamradt, 2023), which involved finding relevant information within a larger context (Fig. 1), by utilizing two prominent datasets: NaturalQuestions and QuALITY Kwiatkowski et al. (2019); Pang et al. (2022), selected for their proximity to natural uses-cases of LLMs. Additionally, we examine a range of design parameters, including task types, model types and scales, prompt formats, context lengths, and self-consistency configurations. We conducted a comprehensive series of experiments to assess how these parameters interact and influence outcomes. On (Q1subscript𝑄1Q_{1}italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT), we find that SC provides little to no statistically significant improvement in overall performance (§4.1). We further investigate SC’s impact on position bias (the location of gold evidence within the LLM context window) (Q2subscript𝑄2Q_{2}italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) and find that SC’s effects are not evenly distributed across different positions. Notably, while some models show improved performance when key evidence is in early or later positions, others experience performance drops regardless of evidence location (§4.2). We also examine other dimensions of the problem setup, including model scale, prompt encoding (e.g., whether the question appears first or last), and SC parameters (Q3subscript𝑄3Q_{3}italic_Q start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT), and find no change in our earlier conclusions. For example, larger models tend to increase performance uniformly across different evidence positions but do not yield significant gains from SC (§4.3). Contributions. We provide the first in-depth study of self-consistency in long-context settings, revealing unexpected weakness of SC in these scenarios. Our analysis covers a wide range of design parameters, such as models, datasets, and SC configurations, and involves over 563 experiments, highlighting the significant effort needed to achieve our findings. Our results challenge the assumption that SC universally improves LLM performance and provides important insights for those working with long-context tasks."
https://arxiv.org/html/2411.01093v1,TabVer: Tabular Fact Verification with Natural Logic,"Fact verification on tabular evidence incentivises the use of symbolic reasoning models where a logical form is constructed (e.g. a LISP-style program), providing greater verifiability than fully neural approaches. However, these systems typically rely on well-formed tables, restricting their use in many scenarios. An emerging symbolic reasoning paradigm for textual evidence focuses on natural logic inference, which constructs proofs by modelling set-theoretic relations between a claim and its evidence in natural language. This approach provides flexibility and transparency but is less compatible with tabular evidence since the relations do not extend to arithmetic functions. We propose a set-theoretic interpretation of numerals and arithmetic functions in the context of natural logic, enabling the integration of arithmetic expressions in deterministic proofs. We leverage large language models to generate arithmetic expressions by generating questions about salient parts of a claim which are answered by executing appropriate functions on tables. In a few-shot setting on FEVEROUS, we achieve an accuracy of 71.471.471.471.4, outperforming both fully neural and symbolic reasoning models by 3.43.43.43.4 points. When evaluated on TabFact without any further training, our method remains competitive with an accuracy lead of 0.50.50.50.5 points.","SSSRstart Question: What is the place for which the population was measured in 2018? Span (ci): Ortegal Question: What is the total pop. of Ortegal in 2018? Span (ci): larger than 12,000 Question: How many municipalities does Ortegal have? Span (ci): three Question: What is the place for which the population was measured in 2018? Extraction: The population was measured in 2018 for Ortegal. Compute: No computation required. Answer (ai): COPY Ortegal Question: What is the total pop. of Ortegal […]? Extraction: In 2018 Carino had a pop. of 3,945, Cerdido 1,126, Mañón 1,363, and Ortigueira 5,804. Compute: Adding 3,945 + 1,126 + 1,363 + 5,804 = 12,238. Answer (ai): SUM 12,238 Question: How many muni. does Ortegal have? Extraction: Ortegal has the municipalities Carino, Cerdido, Manon, and Ortigueira. Compute: Counting Carino, Cerdido, Manon, Ortigueirra = 4 Answer (ai): COUNT 4 cisubscript𝑐𝑖c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT=Ortegal aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT= COPY Ortegal cisubscript𝑐𝑖c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT=larger than 12,000 aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT= SUM 12,238 cisubscript𝑐𝑖c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT=three aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT= COUNT 4 ≡\equiv≡⊑square-image-of-or-equals\sqsubseteq⊑⇃↾⇃absent↾\mathrel{\downharpoonleft\!\upharpoonright}⇃ ↾DFA State Claim: In 2018 Ortegal had population of larger than 12,000 in its three municipalities. Evidence: Municipality Pop. (2011) Pop. (2018) Cariño 4,374 3,945 Cerdido 1,304 1,126 Mañón 1,541 1,363 Ortigueira 6,697 5,804 Ortegal Figure 1: High-level illustration of TabVer. TabVer proposes a set-theoretic view on numerals and arithmetic functions, which is integrated into natural logic proofs as arithmetic comparisons between claim and answers to questions (ArithExps), resulting in deterministic inference (left). To generate ArithExps, TabVer asks questions about salient parts cisubscript𝑐𝑖c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of a claim (middle). The questions are answered using tabular evidence E𝐸Eitalic_E, by generating a rationale and a set-theoretic compatible representation of required computations (right). Fact verification systems assess the veracity of claims based on evidence and provide an explanation for the prediction. In the case of tabular evidence, verification frequently relies on symbolic reasoning steps, such as the execution of arithmetic functions, to accurately predict whether a claim is supported by evidence (Herzig et al., 2020, inter alia). This incentivises symbolic reasoning systems, where a logical representation of a claim and its tabular evidence (e.g. a LISP-style program) is executed to produce the veracity prediction (Chen et al., 2020; Cheng et al., 2023). Since the execution of these logical forms is deterministic, they serve as faithful explanations of the model’s reasoning (Jacovi and Goldberg, 2021). However, these systems typically rely on well-formed tables, constraining their use in many scenarios, such as reasoning over diverse tabular structures as typically found on Wikipedia. Consequently, the majority of recently proposed verification models focus on neural entailment models that latently execute arithmetic functions (Liu et al., 2022b; Gu et al., 2022) or generate a natural language explanation alongside its prediction (Wei et al., 2022, inter alia). While systems that produce natural language explanations are more flexible regarding the evidence format, they do not necessarily generate faithful explanations (Atanasova et al., 2023). An emergent symbolic reasoning paradigm for textual evidence focuses on logical inference by directly comparing claim and textual evidence via natural logic inference (Angeli and Manning, 2014), achieving high prediction accuracy while maintaining faithful explanations (Krishna et al., 2022; Aly et al., 2023; Strong et al., 2024). However, current natural logic systems are unable to handle tabular evidence since the semantic relationship captured between aligned claim-evidence spans via natural logic’s set-theoretic operators does not extend to arithmetic functions (MacCartney and Manning, 2009). For instance, in Figure 1, no evidence in the table directly corresponds to the part of the claim that states three municipalities. Instead, arithmetic computation on the table, beyond the expressiveness of natural logic’s set-theoretic operators, is required (i.e. counting relevant cells in this example). To this end, we propose TabVer: Tabular Fact Verification, a natural logic inference system that adds arithmetic reasoning capabilities to reason over tabular evidence directly in natural language. We define a set-theoretic interpretation of comparisons between numerals in claim-evidence pairs, and extend that definition to executions of arithmetic functions via arithmetic expressions (ArithExps) to enable their integration into natural logic proofs. The proofs are executed deterministically on a finite state automaton (DFA) as defined in natural logic inference. ArithExps are produced by leveraging large language models (Brown et al., 2020, inter alia), generating questions about salient parts of the claim cisubscript𝑐𝑖c_{i}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which are answered via a rationale that produces an answer aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. As illustrated in Figure 1, TabVer will generate a question such as “What is the total population of Ortegal in 2018” to verify the part larger than 12000 in the claim c𝑐citalic_c. Answering this question on the evidence table produces a rationale with the expression SUM 12,238 as the final answer aisubscript𝑎𝑖a_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, indicating the execution of the function SUM⁢(3945,1126,1363,5804)=12238SUM394511261363580412238\text{SUM}(3945,1126,1363,5804)=12238SUM ( 3945 , 1126 , 1363 , 5804 ) = 12238 over relevant evidence in E𝐸Eitalic_E. The aligned pair (larger than 12000, SUM ⁢12,238SUM 12238\emph{SUM }12,238SUM 12 , 238) is then assigned a natural logic operator as part of a natural logic proof, with the predicted operator being consistent with our set-theoretic definitions (Figure 3). In a few-shot setting with 64646464 training instances on the tabular subset of the FEVEROUS dataset (Aly et al., 2021), TabVer outperforms previous symbolic reasoning systems, including LPA (Chen et al., 2020), SASP (Ou and Liu, 2022), Binder (Cheng et al., 2023), and a state-of-the-art natural logic system (Aly et al., 2023), with a lead of 10.510.510.510.5 accuracy points over the best performing baseline, Binder. Moreover, TabVer outperforms the highest-scoring neural entailment model, a classifier-version of the same language model used by TabVer, by 3.43.43.43.4 accuracy points. TabVer outperforms further classification baselines in a few-shot setting, such as TAPAS (Herzig et al., 2020), TAPEX (Liu et al., 2022b), and PASTA (Gu et al., 2022). We confirm the tabular reasoning capabilities of TabVer in a domain transfer setting to Tabfact (Chen et al., 2020) without further training annotations, where our system performs competitively, improving on the strongest baseline by 0.50.50.50.5 accuracy points. Our analysis reveals that TabVer’s reading of numerals is more sensitive to numerical inaccuracies and the context of a claim (like quantifiers and adverbial modifiers) than a same-sized LLM baseline, reflecting the annotator guidelines of FEVEROUS more accurately. A qualitative discussion highlights attractive properties of TabVer and natural logic over existing SQL systems for veracity prediction over tabular evidence, such as its ability to model the NEI label directly.111Code at https://github.com/Raldir/TabVer"
https://arxiv.org/html/2411.01084v1,Plentiful Jailbreaks with String Compositions,"Large language models (LLMs) remain vulnerable to a slew of adversarial attacks and jailbreaking methods. One common approach employed by white-hat attackers, or red-teamers, is to process model inputs and outputs using string-level obfuscations, which can include leetspeak, rotary ciphers, Base64, ASCII, and more. Our work extends these encoding-based attacks by unifying them in a framework of invertible string transformations. With invertibility, we can devise arbitrary string compositions, defined as sequences of transformations, that we can encode and decode end-to-end programmatically. We devise a automated best-of-n attack that samples from a combinatorially large number of string compositions. Our jailbreaks obtain competitive attack success rates on several leading frontier models when evaluated on HarmBench, highlighting that encoding-based attacks remain a persistent vulnerability even in advanced LLMs.","1.1 Problem setting The best large language models (LLMs) today boast advanced reasoning capabilities and extensive world knowledge, making them susceptible to more severe risks and misuse cases. To mitigate these risks, model creators have devoted substantial research efforts to model alignment. One essential component of the alignment pipeline is red-teaming, or the rigorous evaluation of models to identify vulnerabilities and weaknesses. By better understanding the attack surface of frontier language models, we can, in turn, better understand the shortcomings of current alignment measures and help safety researchers on the “blue-team” build more robust AI systems. In particular, we’re interested in jailbreak methods that are automated. With so many frontier AI systems deployed in so many downstream settings, redteaming efforts can benefit greatly from scalability. Automated attacks can be applied to various models, risk categories, and tasks with no case-by-case manual tuning, making them scalable. In addition, many redteaming pipelines employ manually generated attacks (Li et al., 2024; the Prompter, 2024; Andriushchenko et al., 2024); complementing these methods with automated attacks helps convert manual intuitions into a more systematic understanding of model vulnerabilities. Currently, the redteaming community has employed various string-level obfuscations as attack mechanisms (Wei et al., 2024). For example, previous jailbreaks have encoded the input and/or instructed the model to respond in leetspeak (the Prompter, 2024), Morse Code (Barak, 2023), code (Kang et al., 2023), low-resource languages (Yong et al., 2023), rotary ciphers or ASCII (Yuan et al., 2024; Jiang et al., 2024), and more. These encoding schemes are manually derived and somewhat piecemeal, and our work aims to extend and unify these encodings into a more powerful automated attack. Our contributions are twofold. (1) We implement a simple attack framework in which multiple arbitrary encodings, or transformations, can be composed in sequence to form a single, more complex encoding, which we call a string composition, for use in an adversarial prompt. With 20 individual transformations in our library, we can generate a combinatorially large number of string compositions. (2) Using this framework, we devise an automated best-of-n𝑛nitalic_n jailbreak: for a given harmful intent, n𝑛nitalic_n random compositions are sampled and the model is considered jailbroken if at least one composition produces an unsafe response. We benchmark our composition-based attacks and obtain impressive attack success rates on HarmBench across several frontier language models. 1.2 Related work To reiterate, many encodings mentioned in the introduction, including leetspeak, Morse Code, low-resource language translations, rotary ciphers, and ASCII, fall under the purview of invertible transformations. Besides encodings, the adversarial attack literature for language models has included gradient-based discrete optimization (Zou et al., 2023; Liu et al., 2024; Shin et al., 2020; Ebrahimi et al., 2017; Guo et al., 2021; Geisler et al., 2024; Zhu et al., 2023; Guo et al., 2024; Thompson and Sklar, 2024); LLM-assisted prompt optimization (Chao et al., 2023; Mehrotra et al., 2023; Tang et al., 2024); multi-turn or many-shot attacks (Huang et al., 2024; Li et al., 2024; Russinovich et al., 2024; Anil et al., 2024; Zheng et al., 2024); and other idiosyncratic attack vectors (Andriushchenko and Flammarion, 2024; Andriushchenko et al., 2024). Our work is closely inspired by Wei et al. (2024)’s study of string transformations, which they call “obfuscation schemes.” Wei et al. (2024) also explore a precursor for string compositions via their combination attacks, which compose multiple jailbreak mechanisms together. Our work builds upon Wei et al. (2024) by (1) studying a much larger set of string transformations, and (2) by designing an automated heuristic for generating arbitrary string compositions, leading to a more comprehensive understanding of model vulnerabilities arising from encoded inputs."
https://arxiv.org/html/2411.01077v1,Emoji Attack: A Method for Misleading Judge LLMs in Safety Risk Detection,"Jailbreaking attacks show how Large Language Models (LLMs) can be tricked into generating harmful outputs using malicious prompts. To prevent these attacks, other LLMs are often used as judges to evaluate the harmfulness of the generated content. However, relying on LLMs as judges can introduce biases into the detection process, which in turn compromises the effectiveness of the evaluation. In this paper, we show that Judge LLMs, like other LLMs, are also affected by token segmentation bias. This bias occurs when tokens are split into smaller sub-tokens, altering their embeddings. This makes it harder for the model to detect harmful content. Specifically, this bias can cause sub-tokens to differ significantly from the original token in the embedding space, leading to incorrect “safe” predictions for harmful content. To exploit this bias in Judge LLMs, we introduce the Emoji Attack — a method that places emojis within tokens to increase the embedding differences between sub-tokens and their originals. These emojis create new tokens that further distort the token embeddings, exacerbating the bias. To counter the Emoji Attack, we design prompts that help LLMs filter out unusual characters. However, this defense can still be bypassed by using a mix of emojis and other characters. The Emoji Attack can also be combined with existing jailbreaking prompts using few-shot learning, which enables LLMs to generate harmful responses with emojis. These responses are often mistakenly labeled as “safe” by Judge LLMs, allowing the attack to slip through. Our experiments with six state-of-the-art Judge LLMs show that the Emoji Attack allows 25% of harmful responses to bypass detection by Llama Guard and Llama Guard 2, and up to 75% by ShieldLM. These results highlight the need for stronger Judge LLMs to address this vulnerability. Our code is available at https://github.com/zhipeng-wei/EmojiAttack.","Judge Large Language Models (LLMs) have been introduced to evaluate the alignment of LLMs with human preferences (Chiang et al. 2023; Zheng et al. 2024), providing a more efficient alternative to traditional human judges. The success of these models has recently led to the development of specialized Judge LLMs focused on safety risk detection (Han et al. 2024; Zhang et al. 2024). One key application of these Judge LLMs is evaluating the harmfulness of responses to prevent jailbreaking (Inan et al. 2023; Llama-Team 2024). Specifically, they detect unsafe content generated by a target LLM and prompt it to stop responding if harmful content is identified. Additionally, Judge LLMs can provide feedback during jailbreaking attacks, which helps refine attack strategies. However, Judge LLMs are known to suffer from various biases (Zheng et al. 2024; Chen et al. 2024; Wang et al. 2023; Koo et al. 2023), such as positional bias (Zheng et al. 2024), where certain positions are favored over others. Despite the known inherent biases in Judge LLMs, little work has been done to explore these biases in the context of safety risk detection. This is surprising because the biases involved in safety risk detection are likely different from those in assessing human preferences, due to the distinct nature of the tasks. Therefore, it is crucial to identify and address these biases to improve the reliability of Judge LLMs in preventing harmful outputs. In this paper, we focus on investigating the biases present in Judge LLMs used for safety risk detection, particularly token segmentation bias. Token segmentation bias occurs when Judge LLMs split tokens into smaller sub-tokens, which leads to alterations in the embedding space. These alterations disrupt the contextual relationships in the cross-attention layers, potentially leading to harmful responses being misclassified as ”safe”. (Figure 1(a)). Recent work by (Claburn 2024) examined a related bias in prompt injection, that is based on the insertion on spaces between characters in a given prompt. In contrast, the token segmentation bias we study is based on the idea of splitting tokens into smaller sub-tokens, which in turn can be used for altering the embeddings. Our findings indicate that introducing token segmentation bias into harmful responses reduces the ”unsafe” prediction rate of Judge LLMs, such as Llama Guard (Inan et al. 2023) and Llama Guard 2 (Llama-Team 2024), by approximately 12% on average. This suggests that token segmentation bias has a significant impact on the effectiveness of Judge LLMs in detecting harmful content. Figure 1: Overview of token segmentation bias and the Emoji Attack that exploits it. (a) Token segmentation bias: Dividing tokens into sub-tokens misleads Judge LLMs into classifying harmful content as ”safe”. (b) Emoji Attack: The position selection strategy identifies the optimal insertion point by minimizing cosine similarity, after which emojis are added to mislead Judge LLMs into making ”safe” predictions. (c) Practical Emoji Attack: Instructions, including a benign query and response with emojis, direct target LLMs to generate harmful responses containing emojis. Judge LLMs permit these outputs when they are misclassified as ”safe”. Building on the concept of token segmentation bias, we introduce the Emoji Attack, a method specifically designed to exploit this vulnerability in Judge LLMs. Unlike traditional token splitting by spaces, the Emoji Attack involves inserting emojis within tokens of harmful responses. These inserted emojis disrupt the token structure, resulting in new tokens that cause more significant changes in the embedding space. We also developed a position selection strategy to identify optimal emoji insertion points, maximizing the difference between the sub-tokens and the original token in the embedding space. As illustrated in Figure 1(b), we use a lightweight surrogate model to find the optimal insertion points by comparing the cosine similarity between the sub-token embeddings and the original token. Compared to token segmentation bias alone, the Emoji Attack further reduces the ”unsafe” prediction rate for Llama Guard and Llama Guard 2 by an additional 25% on average. Additionally, we demonstrate that combining different delimiters, like characters and emojis, can make simple filter defense strategies ineffective, highlighting the potential to advance jailbreaking techniques based on token segmentation bias. We also explore the practical application of the Emoji Attack in scenarios where Judge LLMs directly assess the target LLM responses to determine their suitability for sharing with users. In this scenario, although we cannot manipulate the responses, we can access the input prompts of the target LLM. As adversaries, we combine the Emoji Attack with existing jailbreaking prompts to evade ”unsafe” classification by Judge LLMs. As shown in Figure 1(c), we leverage the in-context learning (Brown et al. 2020) abilities of LLMs to instruct target LLMs to generate harmful responses with inserted emojis. The presence of these emojis increases the likelihood that the responses will be classified as ”safe” by Judge LLMs. This practical application of the Emoji Attack enhances the effectiveness of jailbreaking techniques, reducing ”unsafe” detection by an average of 15.8% across six state-of-the-art Judge LLMs. Our main contributions are as follows: • We study the biases of Judge LLMs in safety risk detection, specifically identifying token segmentation bias, where the division of tokens into sub-tokens leads to incorrect ”safe” predictions for harmful responses by Judge LLMs. • Building on token segmentation bias, we propose the Emoji Attack, a method that inserts emojis at optimal positions within tokens to maximize the embedding discrepancy between sub-tokens and the original token. Additionally, this attack can be combined with other delimiters to render simple filter defense strategies ineffective. • We explore the practical application of the Emoji Attack by integrating it with existing jailbreaking prompts to evade ”unsafe” classification by Judge LLMs. • We conduct experiments with six state-of-the-art Judge LLMs: Llama Guard, Llama Guard 2, ShieldLM, WildGuard, GPT-3.5, and GPT-4. Our findings reveal that Judge LLMs are vulnerable to both token segmentation bias and our Emoji Attack, which exploits this bias."
https://arxiv.org/html/2411.01030v3,Birdie: Advancing State Space Modelswith Reward-Driven Objectives and Curricula,"Efficient state space models (SSMs), such as linear recurrent neural networks and linear attention variants, offer computational advantages over Transformers but struggle with tasks requiring long-range in-context retrieval-like text copying, associative recall, and question answering over long contexts. Previous efforts to address these challenges have focused on architectural modifications, often reintroducing computational inefficiencies. In this paper, we propose a novel training procedure, Birdie, that significantly enhances the in-context retrieval capabilities of SSMs without altering their architecture. Our approach combines bidirectional input processing with dynamic mixtures of specialized pre-training objectives, optimized via reinforcement learning. We introduce a new bidirectional SSM architecture that seamlessly transitions from bidirectional context processing to causal generation. Experimental evaluations demonstrate that Birdie markedly improves performance on retrieval-intensive tasks such as multi-number phone book lookup, long paragraph question-answering, and infilling. This narrows the performance gap with Transformers, while retaining computational efficiency. Our findings highlight the importance of training procedures in leveraging the fixed-state capacity of SSMs, offering a new direction to advance their capabilities. All code and pre-trained models are available at https://www.github.com/samblouir/birdie, with support for JAX and PyTorch.","Due to their scaling properties (Hoffmann et al., 2022) and in-context learning (Garg et al., 2023), large Transformer models using attention (Bahdanau, 2014; Vaswani et al., 2017b) are now prominent in natural language processing (NLP) and achieve effective performance in natural language generation tasks (NLG), including language modeling, machine translation, and question and answering (Q&A) (Yue et al., 2022; Xie et al., 2022; Kumar et al., 2021). However, the softmax attention mechanism cost scales quadratically with sequence length during training, and its key-value (KV) cache grows linearly with sequence length during inference. This leads to increasing costs for training and deployment as model providers continue to increase the context length (Dubey et al., 2024; Reid et al., 2024). This trend in increasing context length has sparked a strong interest in developing efficient alternative sequence models. The goal is to maintain high performance while scaling effectively with longer sequences. Recent work has focused on recurrent models which offer two key advantages: subquadratic scaling for parallel processing and a fixed state size (in contrast to the growing KV cache in Transformer models) that enables constant-cost inference per step. These models come in different forms, ranging from state space model (SSM)-based methods, such as S4 (Gu et al., 2022), S5 (Smith et al., 2023), or Mamba (Gu and Dao, 2023)), to linear RNNs, such as RWKV (Peng et al., 2023), HGRU (Qin et al., 2023), and Hawk (De et al., 2024), to linear attention variants, such as RetNet (Sun et al., 2023) and GLA (Yang et al., 2024). These different methods vary in their exact parameterization and parallel computation, but all have an efficient, fixed-state size recurrence for inference. For brevity, we will generally refer to all of these methods as SSMs regardless of their exact parameterization or parallel computation path. While some studies have shown the ability of SSMs to match Transformers in perplexity and some public benchmarks, an increasing line of work shows that current SSMs struggle on tasks that require long-range in-context abilities (Park et al., 2024), such as long-range retrieval (Wen et al., 2024), multi-query associative recall (Arora et al., 2023, 2024a), and copying (Jelassi et al., 2024). These tasks are critical in NLP, where the ability to maintain and manipulate long-term dependencies is key to generating coherent text, following directions, copying sequences, and responding accurately to multiple queries. A typical approach to address these weaknesses has been to formulate hybrid models that interleave SSM layers with global attention layers (Mehta et al., 2023; Fu et al., 2023a; Park et al., 2024; Poli et al., 2024), or sliding window attention (Beltagy et al., 2020; Arora et al., 2024a; De et al., 2024).111Sliding window attention, introduced in Longformer (Beltagy et al., 2020), can be viewed as a type of fixed-state size method. However, models with global attention layers still scale quadratically with sequence length and have a growing KV cache. Models that rely on sliding window attention also fail to perform in-context retrieval outside of the sliding window length (Arora et al., 2024a; De et al., 2024). The predominant focus on architecture to improve performance on long-range in-context abilities misses an opportunity to investigate the role of the pre-training objectives and the potential interaction between the training procedure and model architecture. We note that prior work on generative SSMs exclusively utilizes Next Token Prediction for its pre-training objective. In this paper we argue (and show) that in the presence of a fixed state size, a mixture of pre-training objectives can bias learning towards pertinent long-range interactions and that bidirectional processing of the context allows better utilization of the fixed state for such interactions. This paper makes the following key methodological contributions: (1) We develop novel pre-training objective mixtures that confer SSMs strong performance on both standard downstream public benchmarks and recall and copying-intensive tasks where SSMs typically struggle, such as phone book retrieval tasks, infilling, and long paragraph Q&A. (2) We show that bidirectional processing of the context combined with the pre-training objective mixtures can further boost performance. In addition, we develop a new bidirectional architecture for SSMs that allows a seamless transition from bidirectional processing of the context to causal generation of the response. (3) To improve the practical ability to experiment with new pre-training objectives in the mixture, we propose a dynamic mixture of pre-training objectives via reinforcement learning (RL). This allows for maximizing performance while automating much of the objective selection process. The result is a new training procedure that significantly improves the performance of SSMs on recall-intensive tasks, making them more competitive with Transformers. We refer to this procedure as Birdie. While we do still observe a performance gap with Transformers on some tasks as the retrieval requirement becomes more difficult (e.g. increasing the number of retrievals required per example), our procedure makes the SSM performance degradation in these scenarios much less severe and expands the regime where these efficient methods can be useful. More broadly, our work points to considering the learning dynamics along with the inductive biases of SSM architectures in order to make better use of the fixed state size. Our new training procedure, which we call Birdie, significantly improves SSM performance on context-heavy and recall-intensive tasks, making this class of models more competitive with Transformers. While a performance gap with Transformers remains on certain tasks as retrieval demands increase (e.g., requiring more retrievals per example), Birdie reduces the severity of performance degradation in these scenarios and extends the range where these efficient models are effective. More broadly, our work underscores the importance of considering both learning dynamics and the inductive biases of SSM architectures to maximize the utility of their fixed state size."
https://arxiv.org/html/2411.01022v1,Provenance: A Light-weight Fact-checker forRetrieval Augmented LLM Generation Output,"We present a light-weight approach for detecting nonfactual outputs from retrieval-augmented generation (RAG). Given a context and putative output, we compute a factuality score that can be thresholded to yield a binary decision to check the results of LLM-based question-answering, summarization, or other systems. Unlike factuality checkers that themselves rely on LLMs, we use compact, open-source natural language inference (NLI) models that yield a freely accessible solution with low latency and low cost at run-time, and no need for LLM fine-tuning. The approach also enables downstream mitigation and correction of hallucinations, by tracing them back to specific context chunks. Our experiments show high area under the ROC curve (AUC) across a wide range of relevant open source datasets, indicating the effectiveness of our method for fact-checking RAG output.","With natural language understanding applications increasingly relying on large language models (LLMs) to answer questions, summarize texts, and perform other tasks, detecting nonfactual claims in the generated text has become critical from an ethical and compliance standpoint. LLMs, while powerful, are prone to generate nonfactual or “hallucinated” information that can lead to misinformation and introduce errors in business processes. To address this problem, we present Provenance, a fact-checking method for output generated by LLMs, with respect to a given context that provides the factual basis for the output. Provenance leverages compact cross-encoder models that offer substantial advantages over conventional LLM-based methods. These advantages include accessibility, low latency/high throughput, and interpretable judgments. Provenance is evaluated on diverse open-source datasets, including the TRUE dataset Honovich et al. (2022), MSMarco Nguyen et al. (2016), TruthfulQA Lin et al. (2022), HotpotQA Yang et al. (2018), HaluEval Li et al. (2023) and HaluBench Ravi et al. (2024). These datasets encompass a variety of question-answering contexts, providing a robust testbed for our methods. We assess performance using standard detection metrics to demonstrate our method’s efficacy as a factuality checker for LLM-generated content. Our findings show that Provenance achieves competitive hallucination detection performance (as measured by AUC) across different datasets, thus contributing to improved trustworthiness and utility of LLMs in real-world applications."
https://arxiv.org/html/2411.00985v1,FedDTPT: Federated Discrete and Transferable Prompt Tuning for Black-Box Large Language Models,"In recent years, large language models (LLMs) have significantly advanced the field of natural language processing (NLP). By fine-tuning LLMs with data from specific scenarios, these foundation models can better adapt to various downstream tasks. However, the fine-tuning process poses privacy leakage risks, particularly in centralized data processing scenarios. To address user privacy concerns, federated learning (FL) has been introduced to mitigate the risks associated with centralized data collection from multiple sources. Nevertheless, the privacy of LLMs themselves is equally critical, as potential malicious attacks challenge their security, an issue that has received limited attention in current research. Consequently, establishing a trusted multi-party model fine-tuning environment is essential. Additionally, the local deployment of large LLMs incurs significant storage costs and high computational demands. To address these challenges, we propose for the first time a federated discrete and transferable prompt tuning, namely FedDTPT, for black-box large language models. In the client optimization phase, we adopt a token-level discrete prompt optimization method that leverages a feedback loop based on prediction accuracy to drive gradient-free prompt optimization through the MLM API. For server optimization, we employ an attention mechanism based on semantic similarity to filter all local prompt tokens, along with an embedding distance elbow detection and DBSCAN clustering strategy to enhance the filtering process. Experimental results demonstrate that, compared to state-of-the-art methods, our approach achieves higher accuracy, reduced communication overhead, and robustness to non-iid data in a black-box setting. Moreover, the optimized prompts are transferable.","1 Introducation Large language models (LLMs) have demonstrated significant success across numerous natural language processing (NLP) tasks (Brown et al., 2020; Devlin et al., 2019; Radford et al., 2019). Typically, these models are trained on a vast text corpus and then applied to various downstream tasks through fine-tuning or prompt tuning. However, task-specific data is often necessary for tuning pre-trained LLMs, and this process typically relies on user-labeled data. In practice, securely leveraging these labeled data presents challenges. Data must be collected and stored for training purposes, but sharing and exchanging sensitive information can pose serious security risks and raise privacy concerns. To mitigate the risk of potential data leakage, federated learning (FL) is proposed. FL enables multiple devices to collaboratively fine-tune pre-trained LLMs on decentralized data while maintaining data privacy. Recent work, such as the bilevel optimization method (Li et al., 2024), has demonstrated efficient strategies to reduce communication overhead and improve optimization performance in FL scenarios. Additionally, federated object detection frameworks (Kim et al., 2024) and federated conditional stochastic optimization (Wu et al., 2023) have provided further insights into addressing communication and computational challenges in decentralized learning. Privacy and security remain critical in FL settings, and proactive defenses against model poisoning attacks, such as RECESS (Yan et al., 2023), help safeguard model integrity while fine-tuning LLMs in federated environments. Moreover, techniques like personalized federated learning (Yan et al., 2024) have introduced new ways to enhance the adaptability of global models to specific client data, addressing the heterogeneity often encountered in FL systems. When applying federated learning (FL) for tuning pre-trained LLMs, existing approaches can be categorized into federated fine-tuning and federated prompt tuning. However, both methods have their limitations. For federated fine-tuning, the primary challenges include: (1) clients’ limited access to the parameters of pre-trained language models (PLMs), (2) significant computational and storage demands on local clients, and (3) high communication overhead within the FL system. These factors make federated fine-tuning impractical in real-world scenarios. In practice, devices primarily interact with LLMs by invoking LLM APIs, which do not grant clients access to model parameters, thus preventing local training. Moreover, even if access were available, devices with limited computational resources would struggle to perform local LLM fine-tuning (Zhou et al., 2024). Several approaches have been proposed to address the challenges posed by client heterogeneity and communication costs, such as leveraging model architectures designed to improve performance in FL systems despite data heterogeneity (Pieri et al., 2023), as well as bilevel optimization methods that offer communication-efficient solutions for FL systems (Yang et al., 2024b). Additionally, methods like dynamic personalized federated learning (Panchal et al., 2022), model reassembly techniques (Wang et al., 2024), and federated multi-objective optimization frameworks (Yang et al., 2024a) offer solutions for efficient model adaptation in decentralized environments. These innovations, which target the optimization of client-specific models and data distribution challenges, may also inform strategies for fine-tuning models in decentralized contexts. An alternative approach, federated prompt tuning, as proposed by FedBPT (Sun et al., 2023), focuses on optimizing continuous prompts injected into text while keeping the PLM parameters frozen. Although this method reduces computational costs for clients, continuous prompts still face several limitations: (1) they are model-specific and cannot be directly applied to prediction APIs, which only accept discrete inputs, (2) continuous prompts lack interpretability, and (3) they lack transferability, meaning they cannot be seamlessly applied to other LLMs. To improve communication efficiency, methods like spectral co-distillation (Chen et al., 2023) and one-pass distribution sketches (Liu et al., 2024) have been explored, targeting efficient aggregation and reduced overhead. Furthermore, the issue of communication efficiency and local model performance trade-offs has been explored in works (Li & Huang, 2024), where the tension between local client computations and global model performance is thoroughly examined, providing further insight into optimizing federated learning strategies. To address the aforementioned challenges, we propose FedDTPT, On the client side, we employ a token-level discrete prompt tuning strategy. Given the absence of a probability distribution in the inference results, we implement gradient-free prompts optimization through a feedback loop based on prediction accuracy. On the server side, we utilize an attention mechanism grounded in semantic similarity to filter prompt tokens from all clients. This mechanism identifies the most representative discrete tokens. Additionally, we enhance the filtering effectiveness by employing an inflection point detection in embedding distances and a Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering strategy. We conducted experiments on multiple datasets using SOTA PLMs. The results indicate that, in comparison to the current state-of-the-art techniques, our methodology attains superior accuracy, diminished communication expenses, and resilience to non-iid data within a black-box framework. Furthermore, the refined prompts exhibit transferability. Our contributions include: • Problem Novelty: In this work, we introduce a new problem setting: discrete prompt learning in black-box federated learning. This setting enables the learning of transferable and interpretable prompts while safeguarding both the privacy of the server’s model parameters and the client’s data. • Approach Novelty: In this work, we propose FedDTPT, a novel discrete prompt learning framework in black-box federated learning scenarios. FedDTPT utilizes the novel token-level optimization strategy to update the client prompt and a token selection method based on semantic similarity to aggregate the discrete prompt. • Experimental effect: Our method achieves high accuracy and low communication overhead, and its optimized prompts exhibit transferability."
https://arxiv.org/html/2411.00980v1,Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO,"Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) frequently face challenges with articulation, leading to dysarthria and resulting in atypical speech patterns. In healthcare settings, coomunication breakdowns reduce the quality of care. While building an augmentative and alternative communication (AAC) tool to enable fluid communication we found that state-of-the-art (SOTA) automatic speech recognition (ASR) technology like Whisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack of training data. Our work looks to leverage SOTA ASR followed by domain specific error-correction. English dysarthric ASR performance is often evaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this dataset where phrases overlap between training and test speakers. Our work proposes an algorithm to break this prompt-overlap. After reducing prompt-overlap, results with SOTA ASR models produce extremely high word error rates for speakers with mild and severe dysarthria. Furthermore, to improve ASR, our work looks at the impact of n-gram language models and large-language model (LLM) based multi-modal generative error-correction algorithms like Whispering-LLaMA for a second pass ASR. Our work highlights how much more needs to be done to improve ASR for atypical speakers to enable equitable healthcare access both in-person and in e-health settings.","Healthcare professionals rely on augmentative and alternative communication (AAC) software to support telehealth and in-person appointments for patients with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) [1]. Damage to the nervous system can result in paralysis or weakness of the muscles responsible for speech, leading to dysarthria and atypical speech patterns in individuals with ALS or cerebral palsy. Atypical speakers who are verbal, often prefer to use their own voice to communicate their needs. Modern AAC applications like VoiceItt111www.voiceitt.com or our own AAC application SpeakEase [2] allow for audio input from the speaker with the intention to provide a faithful transcription. Mulfari et al. [3] propose a low-power, on-device, deep-learning based isolated word ASR system to work in an “always-on” mode for dysarthric speakers with reduced mobility. Such a system has promise to enable communication in healthcare and home settings. Figure 1: Dysarthric automatic speech recognition followed by error correction There is little or no data available on the open domain for atypical speakers. On the other hand web-scale speech datasets like Mozilla Common Voice [4] and GigaSpeech [5] allow for state-of-the-art speech recognition for typical speakers. Dysarthric speech recognition is a low-resource out-of-domain problem [6]. To leverage well-developed typical speaker ASR systems, our work looks for a first pass transcription from such a system followed by error-correction (EC) as show in Figure 1. The figure shows disordered input speech with an imperfect noisy transcription after ASR followed by error-correction. ASR systems are trained with audio and correct transcription pairs. EC systems are trained with inputs consisting of multiple hypotheses of transcribed text (referred to as n-best lists), possibly with errors, with outputs mapping to the correct target text. 1 Ref: he slowly takes a short walk in the open air each day ASR: he shlly takes a wall in the week a eh day EC: he slowly takes a short walk in the open air each day 2 Ref: usually minus several buttons ASR: usually min sell fold buttons EC: usually sell fold buttons 3 Ref: you wished to know all about my grandfather ASR: u’ wal awarke youar gread fap EC: you wished to know all about my grandfather Figure 2: Inference samples for error-correction (EC) for speaker M05. Ref shows the reference transcription, ASR shows the transcription output which serves as input to the EC model. Notice how the EC system has memorized transcripts due to prompt overlap in TORGO. To evaluate English ASR for dysarthric speakers, a well-known dataset called the TORGO dataset [7] is widely used. The TORGO dataset for dysarthric speech has data from speakers with either ALS or CP. Other dysarthric ASR databases such as the Nemours corpus222The authors were unable to obtain a recent copy of this database due to a lack of information on the internet [8], UASpeech [9] and the HomeService corpus [10] are either hard to obtain or largely consist of isolated word utterances. The TORGO dataset is one of the few containing both isloated word and a few sentence level utterances. Figure 2 shows inference examples from our initial experiments of error-correction(EC) following ASR. The EC model memorizes the target transcription without doing any error-correction. This issue stems from the dataset design, which features a significant amount of prompt overlap among the speakers. The research community acknowledges that the TORGO dataset has a very high degree of prompt overlap between speakers [11, 12]. This data leakage prevents the dataset from being used to evaluate ASR and EC algorithms for real-world applications like telehealth and e-health. Our work in this paper makes the following contributions: • Develop an algorithm based on mixed-integer linear programming to partition the TORGO dataset with no prompt overlap with the constraint to minimize data loss. This dataset is called no-prompt overlap TORGO or NP-TORGO. • Understand the impact of removing prompt overlap on dysarthric ASR performance using SOTA baseline ASR models. • Understand the impact of out-of-domain language modelling using text data from the training utterances from NP-TORGO, and Librispeech [13] . • Understand the impact of error-correction (EC) without ASR system fine-tuning, with a state-of-the-art cross-modal error-correction system such as Whispering-LLAMA [14]. This paper is organized as follows. Section II puts our current work in the context of prior work. Section III introduces the TORGO dataset, and Section IV introduces our approach to remove prompt overlap. Section V presents our experimental setup, and experimental results are presented in Section VI. Section VII provides a discussion of our work, and Section VIII concludes the paper."
https://arxiv.org/html/2411.00927v1,"ReSpAct: Harmonizing Reasoning, Speaking, and ActingTowards Building Large Language Model-Based Conversational AI Agents","Large language model (LLM)- based agents have been increasingly used to interact with external environments (e.g., games, APIs etc.) and solve tasks. However, current frameworks do not enable these agents to work with users and interact with them to align on the details of their tasks and reach user-defined goals, instead, in ambiguous situations these agents may make decisions based on assumptions. This work introduces ReSpAct (Reason, Speak, and Act), a novel framework that synergistically combines the essential skills for building task-oriented ""conversational"" agents. ReSpAct addresses this need for agents, expanding on the ReAct approach. ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions, and engage in dynamic dialogue to seek guidance, clarify ambiguities, understand user preferences, resolve problems, and use the intermediate feedback and responses of users to update their plans. We evaluated ReSpAct with GPT-4 in environments supporting user interaction, such as task-oriented dialogue (MultiWOZ) and interactive decision-making (Alfworld, WebShop), ReSpAct is flexible enough to incorporate dynamic user feedback and addresses prevalent issues like error propagation and agents getting stuck in reasoning loops. This results in more interpretable, human-like task-solving trajectories than baselines relying solely on reasoning traces. In two interactive decision-making benchmarks, AlfWorld and WebShop, ReSpAct outperforms strong reasoning-only method ReAct by an absolute success rate of 6% and 4%, respectively. In the task-oriented dialogue benchmark MultiWOZ, ReSpAct improved Inform and Success scores by 5.5% and 3%, respectively.","Instruction-following is a fundamental capability for intelligent agents operating in real-world environments. Recent works such as Wei et al. (2022); Huang et al. (2022); Yao et al. (2022b); Shinn et al. (2024) have focused primarily on building agents that can follow individual instructions without considering the importance of feedback and interaction. In realistic settings, instruction-following often involves a back-and-forth exchange between the agent and the user to reduce uncertainties, correct mistakes, and handle exceptions. Consider a scenario where a user instructs an embodied agent: ""Go to the kitchen and bring me the pan."" If the agent encounters multiple pans in the kitchen, it may need to seek clarification from the user about which specific pan to bring. Additionally, if the agent accidentally brings a wrong pan, immediate feedback from the user can help the agent correct its mistake and complete the task successfully. Without the ability to engage in a dialogue and incorporate user feedback, the interactive agents would act based on assumptions, wasting time and resources while searching for the wrong objects in the wrong places or performing actions that do not satisfy user needs and preferences. This method of operating without communication is not intuitive for agents. For instance, if a user requests, ""Arrange a trip to Hawaii,"" they likely don’t expect the agent to book hotels, flights, and rental cars for random dates without confirming the details first. Existing reasoning and decision-making approaches for language agents augment the agent’s action space with a language model, allowing the agent to generate free-form thoughts in natural language that help contextualize and reason about the task at hand. By alternating between task-solving actions and language thoughts, these agents can perform multi-step reasoning and compose useful information for solving complex tasks. However, such frameworks do not explicitly incorporate user interaction and feedback into the agent’s reasoning process. In real-world scenarios, engagement with users can provide valuable information, clarification, and guidance that can significantly improve an agent’s task-solving capabilities. In this paper, we propose ReSpAct, a framework for task-oriented conversational agents that allows the agent to actively engage with users through dialogue actions. By introducing a new action space for user interaction, the agent can ask clarifying questions, request feedback, and incorporate user responses into its evolving context. This human-in-the-loop approach enables the agent to leverage user insights, adapt to user preferences, and refine its task-solving strategy based on user input. The ReSpAct framework, as shown in Fig. 1, is a critical step towards moving from agents to ""conversational"" agents, which can proactively solicit information from the user, give feedback or take any follow-up directions. Figure 2 shows an example from AlfWorld setting, contrasting ReAct with ReSpAct interactions. In the second turn, the agent cleverly asks the user the possible location of a cloth, making the task easier for itself. ReSpAct framework aims to enable such an experience with LLMs, expanding on the ReAct framework. This would result in a more controllable dialogue experience instead of letting the agent struggle to handle all cases by itself without any help. Recent work on the developer side, such as LangGraph111https://www.langchain.com/langgraph attempts to remedy this problem by building a manually crafted dialogue flow. In contrast, the ReSpAct approach allows the model to decide on when to ""speak"" based on its reasoning. Our key contributions are as follows: • We introduce the ReSpAct framework enabling interactive agents with LLMs, expanding on the ReAct approach. • We perform experiments showing the value of ""conversations"" in goal completion for task-oriented Conversational AI using multiple datasets. • We perform ablation studies discussing the extent of conversations to optimize task success without annoying the users."
https://arxiv.org/html/2411.00897v1,"Enhancing the Traditional Chinese Medicine Capabilities of Large Language
Model through Reinforcement Learning from AI Feedback","Although large language models perform well in understanding and responding to user intent, their performance in specialized domains such as Traditional Chinese Medicine (TCM) remains limited due to lack of expertise. In addition, high-quality data related to TCM is scarce and difficult to obtain, making large language models ineffective in handling TCM tasks. In this work, we propose a framework to improve the performance of large language models for TCM tasks using only a small amount of data. First, we use medical case data for supervised fine-tuning of the large model, making it initially capable of performing TCM tasks. Subsequently, we further optimize the model’s performance using reinforcement learning from AI feedback (RLAIF) to align it with the preference data. The ablation study also demonstrated the performance gain is attributed to both supervised fine-tuning and the direct policy optimization. The experimental results show that the model trained with a small amount of data achieves a significant performance improvement on a representative TCM task.","Language modeling, as an important approach to language understanding and generation, has been extensively studied over the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models drown a lot of attention, and by pre-training Transformer models on large-scale corpora, these models have demonstrated their power in solving various natural language processing tasks[1]. Interestingly, when the parameter size exceeds a certain level, these language models not only improve performance, but also show capabilities that are not available in small-scale models. In general, large language models (LLMs) are Transformer-based language models containing billions or more parameters trained on large amounts of textual data, such as OpenAI’s ChatGPT[2] and GPT-4[3], which are capable of understanding and answering a wide range of questions, and their performance on certain tasks even meets or exceeds that of humans. In addition, the open source community has rapidly introduced a series of LLMs, such as ChatGLM[4], LLaMA[5], Qwen[6], which have demonstrated impressive performance. However, the training data for LLMs mainly come from the Internet and books and are mostly common-sense data, thus limiting them in domains such as Traditional Chinese Medicine (TCM). Additionally, high quality TCM data is scarce, and existing large language models perform poorly on TCM tasks. Despite the challenges, TCM LLM has great potential to provide value in answering questions, assisting doctors in diagnosis and prescribing. In the medical domain, several medical language models have been proposed, such as Med-Palm[7], DoctorGLM[8], etc., which show great promise in a variety of medical applications, such as medical question and answer, dialog systems, and text generation. However, almost all of these works focus on modern medicine, with very few addressing TCM. Developing a large model for the TCM domain is challenging. First, most TCM works are written in ancient Chinese, and many terms do not have corresponding explanations in modern Chinese, with large grammatical differences[9]. Additionally, many theories and diagnostic and therapeutic methods in TCM lack uniform quantitative and objective standards and cannot be easily verified. Finally, TCM is not a widely applied discipline, and there is less information on related works, and high-quality data are even more difficult to obtain [10]. To address these limitations, we propose a framework for enhancing the performance of large-model TCM tasks with only a small amount of data. Specifically, we focus on two types of tasks: initial visit and follow-up visit, as shown in Table 1. Our framework consists of three stages. First, we collect a corpus of real medical cases and perform supervised fine-tuning on a open-source large language model, this step will steer the LLM into solving TCM tasks. Second, for each input, we instruct the model to generate multiple outputs to build a preference dataset. Considering the inefficiency and high cost of manual annotation, we introduced a reinforcement learning method based on AI feedback (RLAIF) to train language models using AI-generated feedback instead of human feedback. Finally, we use preference data to instruct the model’s learning, enabling it to generate outputs that better align with user expectations. The main contributions of this paper are as follows: • We develop a framework for enhancing the performance of TCM tasks using only a small amount of data, enabling the full process of training from supervised fine-tuning to RLAIF. • We develop an automated labeling and ranking method using generative AI to build high-quality preference datasets instead of manual labor. • We conduct several experiments on test datasets to demonstrate the effectiveness of our proposed framework. Figure 1: The structure of our proposed framework includes data collection, supervised fine-tuning, automatic labeling, and direct preference optimization. After supervised fine-tuning, the model will generate multiple outputs, which are labeled using automatic labeling to obtain preference data. The model is further optimized using dpo and preference data. II Related Work II-A Large language models In 2022, ChatGPT went live and attracted a lot of attention, triggering a heated discussion about LLM across the community. With the release of GPT-4 (2023) and GPT-4o (2024), the multimodal capability of large models was further improved, which set off a new wave of AI, however, OpenAI did not announce its training strategies and weights due to various factors. As a result, open source LLMs such as LLaMA and BLOOM[11] quickly attracted a lot of attention from the research community once they were released. These models made public their training methods and weighting files, enabling researchers and developers to further train and improve the models on this basis. A large number of open source LLMs have also emerged in China, such as ChatGLM, DeepSeek[12], Qwen, Baichuan[13], and ERNIE[14]. These open-source LLMs use rich Chinese corpus for training and optimization. With the proliferation of these models, various optimization techniques have been explored to enhance their performance. Optimizing the performance of large language models involves various techniques, with RLHF (reinforcement learning from human feedback)[15] and RLAIF (reinforcement learning from AI feedback)[16] being two prominent methods. In RLHF, a reward model is trained to learn alignment based on human feedback. Once fine-tuned, this reward model can evaluate different outputs, scoring them according to the alignment preferences specified by humans. This feedback is then used to further refine the original language model. On the other hand, RLAIF involves directly linking a pre-trained, well-aligned model to the language model, allowing it to learn from larger and more aligned models. Research shows that RLAIF performs as well as, or even better than, human feedback (RLHF) in tasks such as text summarization, helpful dialogue generation, and harmless dialogue generation. In a recent study known as Direct Preference Optimization (DPO)[17] highlighted the complexity and instability of RLHF. They proposed an alternative approach by leveraging a mapping between reward functions and optimal policies. This mapping allows the constrained reward maximization problem to be optimized precisely through a single stage of policy training, effectively transforming it into a direct objective optimization based on human preference data. Their algorithm, termed DPO, is noted for its stability, performance, and computational efficiency, eliminating the need for fitting a reward model. They found that DPO surpasses RLHF in controlling sentiment generation and enhancing response quality in summarization. II-B Large Language Models in Medical Currently, some progress has been made in the research of large language models in medical domain, but there are still some limitations and challenges. Due to the special characteristics of Traditional Chinese medicine, the medical domain tuned large language models released worldwide mainly focus on Western medicine, and most of them are in English as the main language, such as Google Med-PaLM series which has some limitations on the discovery and application of knowledge of TCM, and it is difficult to meet the special needs of TCM. In China, research teams have begun to emphasize the development of Chinese medical LLMs, such as Huatuo GPT[18], zhongjing[19], shennong-TCM[20] and DoctorGLM. However, the number of large models for TCM is relatively small compared with those for Western medicine. Most of the current so-called large models for Chinese medicine are not purely focused on traditional Chinese medicine. Instead, they are hybrid models that mix knowledge from Western medicine, Chinese medicine, and other related fields. These efforts may overly pursue breadth at the expense of depth. In addition, the quality of the data used to train those existing Chinese medicine LLMs rely varies, which affects the final results of the models. For example, Shennong used the TCM dataset generated by ChatGPT, but the quality cannot be guaranteed, and most of the tasks are common sense questions and answers rather than prescription tasks; Zhongjing collected a large amount of real-world data, but it was also not focused on TCM prescription tasks. Despite the great potential of Large Language Models (LLMs) in healthcare, there are still some important and specific challenges that need to be addressed. When models are used for general knowledge quizzes, the impact of errors is not fatal to humans; however, this is not the case in the medical domain, where incorrect interpretations and answers can have serious consequences for patients. The accuracy and reliability of the information provided by LLMs can be life threatening, as it may affect medical decisions, diagnosis, and treatment plans. In addition, the definition of responsibility after using a LLM to aid in diagnosis is an issue that needs to be considered. Therefore, we need to ensure the quality of model output as much as possible. TABLE II: Prompt example Prompt Example Instruction: You are an intelligent assistant that specializes in solving medical-related problems for users. Please give specific Chinese medicine diagnosis, treatment and prescription based on the medical case provided to you by the user. You need to give a TCM diagnosis, treatment and prescription according to the symptoms provided to you by the user. I will give you an example: Patient: Ji, male, 43 years old. Initial diagnosis: 08/16/2021. Complaint: recurrent cough for 1 year. Medical history: the patient has had recurrent cough for the past 1 year. Physical examination: tongue red and dark. You need to give: TCM diagnosis: [specific diagnosis]. Treatment: [specific treatment]. Prescription: [name of prescription]. Prescription: [specific prescription]. Please note that some medical cases include follow-up visit data, and you will need to synthesize all the information to determine the next step in medication. Now, please give the TCM diagnosis, treatment and prescription based on the symptoms provided to your patient: III Method This section describes the process of building the framework, which is divided into data construction, supervised fine-tuning, reinforcement learning from AI feedback. Each step is discussed sequentially to reflect the research workflow. The integrated methodology flowchart is shown in Figure 1. III-A Data Construction One of the challenges in training high-performance LLM models for TCM lies in obtaining high-quality data. A high quality corpus can greatly improve the performance of LLMs and even break the scaling law to some extent[21]. The model needs not only theoretical data from TCM textbooks, but also professional data from real doctor-patient scenarios, which can reflect the specific conditions of patients and guide the addition, subtraction and proportion of medicines. In order to ensure the diversity of the medical corpus, we collect a variety of real medical text data from multiple sources, including open source data, proprietary data, and real medical consultations. These data cover most of the domains and symptoms of TCM, providing rich and detailed medical knowledge for the model. In this paper, we will focus on the classical types of prescription tasks. Specifically, there are two types of prescription tasks: first visit and follow-up visit. The first visit task needs to issue a prescription based on the user’s symptoms and examination results, while the follow-up visit task needs to synthesize all the previous follow-ups, prescriptions, and feedbacks from the user’s medication to give subsequent medication suggestions. III-B Supervised fine-tuning Supervised fine-tuning (SFT) is a key stage in empowering the model with dialog capabilities. With the help of high-quality doctor-patient dialog data, the model can effectively invoke the medical knowledge accumulated during the pre-training process to understand and answer the user’s query. The goal of SFT is to teach the model how to understand and generate appropriate replies based on TCM diagnostic principles and treatment methods. The SFT process consists of optimizing the model parameters to minimize the cross-entropy loss between the predicted output and the actual output for a given input set. In the SFT process, the model learns to generate prescriptions based on the initial diagnosis and subsequent diagnoses. This process involves providing the model with detailed case information including symptoms, diagnoses, and patient feedback to guide its learning process. The cross-entropy loss used for training the model is defined as follows: L=−1N⁢∑i=1N∑t=1Tyi⁢t⁢log⁡(y^i⁢t)𝐿1𝑁superscriptsubscript𝑖1𝑁superscriptsubscript𝑡1𝑇subscript𝑦𝑖𝑡subscript^𝑦𝑖𝑡L=-\frac{1}{N}\sum_{i=1}^{N}\sum_{t=1}^{T}y_{it}\log(\hat{y}_{it})italic_L = - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_y start_POSTSUBSCRIPT italic_i italic_t end_POSTSUBSCRIPT roman_log ( over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i italic_t end_POSTSUBSCRIPT ) (1) where N𝑁Nitalic_N is the total number of samples, T𝑇Titalic_T is the sequence length, yi⁢tsubscript𝑦𝑖𝑡y_{it}italic_y start_POSTSUBSCRIPT italic_i italic_t end_POSTSUBSCRIPT is the true token at position t𝑡titalic_t for the i𝑖iitalic_i-th sample, and y^i⁢tsubscript^𝑦𝑖𝑡\hat{y}_{it}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i italic_t end_POSTSUBSCRIPT is the predicted probability of the true token at position t𝑡titalic_t for the i𝑖iitalic_i-th sample. This loss function measures the difference between the predicted token distribution and the actual token distribution, and the goal of training is to minimize this loss. The pseudo-code for this phase is shown in Algorithm 1. Note a good prompt helps to stimulate the model’s capability, so we designed a prompt to guide the model to generate the specified response, as shown in Table 2. We decide whether to use English or Chinese prompts based on the corpus used in the pre-training stage of the base model, but the output is uniformly specified to be in Chinese. Algorithm 1 Supervised Fine-tuning 0: Initial policy πθsubscript𝜋𝜃\pi_{\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT, Training dataset D𝐷Ditalic_D, Number of epochs N𝑁Nitalic_N, Batchsize B𝐵Bitalic_B, Learning rate η∈[0,1]𝜂01\eta\in[0,1]italic_η ∈ [ 0 , 1 ] 0: SFTed policy π^θsubscript^𝜋𝜃\hat{\pi}_{\theta}over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT 1: for epoch=1,2,3,…,N𝑁Nitalic_N do 2: for Db⁢a⁢t⁢c⁢h∼Dsimilar-tosubscript𝐷𝑏𝑎𝑡𝑐ℎ𝐷D_{batch}\sim Ditalic_D start_POSTSUBSCRIPT italic_b italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT ∼ italic_D do 3: (x,z)∼Db⁢a⁢t⁢c⁢hsimilar-to𝑥𝑧subscript𝐷𝑏𝑎𝑡𝑐ℎ(x,z)\sim D_{batch}( italic_x , italic_z ) ∼ italic_D start_POSTSUBSCRIPT italic_b italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT 4: z^∼πθ⁢(x)similar-to^𝑧subscript𝜋𝜃𝑥\hat{z}\sim\pi_{\theta}(x)over^ start_ARG italic_z end_ARG ∼ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) 5: L=CrossEntropyLoss⁢(z^,z)𝐿CrossEntropyLoss^𝑧𝑧L=\text{CrossEntropyLoss}(\hat{z},z)italic_L = CrossEntropyLoss ( over^ start_ARG italic_z end_ARG , italic_z ) 6: θ←θ−η⁢∇θL←𝜃𝜃𝜂subscript∇𝜃𝐿\theta\leftarrow\theta-\eta\nabla_{\theta}Litalic_θ ← italic_θ - italic_η ∇ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_L //Update the parameters using any gradient descent algorithm 7: end for 8: end for III-C Reinforcement Learning from AI Feedback Despite SFT accumulating medical knowledge and guiding conversational abilities, the model can still produce inaccurate, harmful, or unfriendly responses, which can have serious consequences in medical dialogues. We use RLAIF to improve the conversation process. Specifically, for each question, we guide the supervised fine-tuned model to generate diverse outputs, score them using a specific annotation model, and then rank them using Borda rank. Finally, we align the training using the DPO algorithm. III-C1 AI Feedback for TCM Considering the specificity of medical conversations, we obtain feedback in three ways, make metrics from three different aspects of the output, and develop a sorted annotation rule. • Lexical overlap: This refers to cases where both the ground truth and the model output contain the same or similar terms. We use BM25, a well-established ranking function used by search engines [22]. The advantage of BM25 lies in its ability to weigh query terms according to their TF-IDF importance, thus providing a reliable measure of term overlap. • Semantic overlap: This refers to cases where the ground truth and the model output contain semantically related content. Even if the model output does not contain the exact terms from the ground truth, they may still be relevant, such as ”yin-yang imbalance” and ”yang deficiency.” We use the dense encoder model bert-base-chinese, which works by encoding ground truth and model output as dense vectors in a shared embedding space, respectively [23]. It focuses on deeper semantic connections between words and phrases. The dot product of the two vectors then gives a measure of semantic similarity. • Model annotation: The above two similarity measurement methods may be biased in TCM. Therefore, we also use DeepSeek-V2 and GPT-4o as annotation models, guiding them to evaluate the generated dialogues from the perspectives of completeness, professionalism, and fluency. Annotation questions come from the test set, and we provide both the questions and the standard answers to the annotation models. The annotation models will score the generated dialogues on a scale from 0 to 100. To mitigate the differences caused by the scoring distribution of different models, ranking is used as a means of introducing regularization supervision signals. Specifically, when multiple rankings of candidate outputs are provided, we use a voting method, Borda Rank, to merge them into a unified ranking. For example, given N𝑁Nitalic_N items and M𝑀Mitalic_M individual rankings, the Borda ranking score Bisubscript𝐵𝑖B_{i}italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT of item i𝑖iitalic_i is calculated as follow: Bi=Σj=1M⁢(N−r⁢a⁢n⁢kj⁢(i))subscript𝐵𝑖subscriptsuperscriptΣ𝑀𝑗1𝑁𝑟𝑎𝑛subscript𝑘𝑗𝑖B_{i}=\Sigma^{M}_{j=1}(N-rank_{j}(i))italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = roman_Σ start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT ( italic_N - italic_r italic_a italic_n italic_k start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_i ) ) (2) where r⁢a⁢n⁢kj⁢(i)𝑟𝑎𝑛subscript𝑘𝑗𝑖rank_{j}(i)italic_r italic_a italic_n italic_k start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_i ) represents the ranking of item i𝑖iitalic_i in the j𝑗jitalic_j-th individual ranking. Based on the ranking results, we can construct binary tuples containing positive samples and negative samples (s+,s−)superscript𝑠superscript𝑠(s^{+},s^{-})( italic_s start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_s start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) for the DPO training process. Figure 2: Increasing the log probability of a preferred sample versus decreasing the log probability of a non-preferred sample response, the model’s policy tends to select the preferred sample. III-C2 Reinforcement Learning Finally, we use the labeled ranking data and DPO to further optimize the model, as shown in Figure 2. Each training sample in our dataset consists of a conversation between a doctor and a patient, a chosen response, and a rejected response generated by the model. Given the dataset, we extract the dialogue, chosen response, and rejected response for each sample. The DPO loss function is defined as follows: l⁢o⁢s⁢s=−log⁡(σ⁢(β⋅((πchosen−πrejected)−γβ)))𝑙𝑜𝑠𝑠𝜎⋅𝛽subscript𝜋chosensubscript𝜋rejected𝛾𝛽loss=-\log\left(\sigma\left(\beta\cdot\left((\pi_{\text{chosen}}-\pi_{\text{% rejected}})-\frac{\gamma}{\beta}\right)\right)\right)italic_l italic_o italic_s italic_s = - roman_log ( italic_σ ( italic_β ⋅ ( ( italic_π start_POSTSUBSCRIPT chosen end_POSTSUBSCRIPT - italic_π start_POSTSUBSCRIPT rejected end_POSTSUBSCRIPT ) - divide start_ARG italic_γ end_ARG start_ARG italic_β end_ARG ) ) ) (3) where πchosensubscript𝜋chosen\pi_{\text{chosen}}italic_π start_POSTSUBSCRIPT chosen end_POSTSUBSCRIPT is the log probability of the chosen response, and πrejectedsubscript𝜋rejected\pi_{\text{rejected}}italic_π start_POSTSUBSCRIPT rejected end_POSTSUBSCRIPT is the log probability of the rejected response. This ratio measures how much more likely the model is to generate the chosen response compared to the rejected one. The hyperparameter γ𝛾\gammaitalic_γ and β𝛽\betaitalic_β controls the adjustment applied to the log probability difference, while scales the entire loss function. The pseudo-code for this phase is shown in Algorithm 2. Algorithm 2 Direct Preference Optimization 0: SFTed policy π^θsubscript^𝜋𝜃\hat{\pi}_{\theta}over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT, Traning dataset D^^𝐷\hat{D}over^ start_ARG italic_D end_ARG, Number of epochs N𝑁Nitalic_N, Batchsize B𝐵Bitalic_B, Learning rate η∈[0,1]𝜂01\eta\in[0,1]italic_η ∈ [ 0 , 1 ], sample size k𝑘kitalic_k, Initial M=∅𝑀M=\emptysetitalic_M = ∅ 0: Trained policy π^θsubscript^𝜋𝜃\hat{\pi}_{\theta}over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT 1: for (x,z)∈D^𝑥𝑧^𝐷(x,z)\in\hat{D}( italic_x , italic_z ) ∈ over^ start_ARG italic_D end_ARG do 2: i=0𝑖0i=0italic_i = 0 3: while i<k𝑖𝑘i<kitalic_i < italic_k do 4: M=M∩π^θ⁢(x)𝑀𝑀subscript^𝜋𝜃𝑥M=M\cap\hat{\pi}_{\theta}(x)italic_M = italic_M ∩ over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x ) 5: end while 6: end for 7: D~=AutoAnnotation(M)~𝐷AutoAnnotation(M)\widetilde{D}=\text{AutoAnnotation($M$)}over~ start_ARG italic_D end_ARG = AutoAnnotation( italic_M ) //Get dpo training data 8: for epoch=1,2,3,…,N𝑁Nitalic_N do 9: for D~b⁢a⁢t⁢c⁢h∼D~similar-tosubscript~𝐷𝑏𝑎𝑡𝑐ℎ~𝐷\widetilde{D}_{batch}\sim\widetilde{D}over~ start_ARG italic_D end_ARG start_POSTSUBSCRIPT italic_b italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT ∼ over~ start_ARG italic_D end_ARG do 10: (s+,s−,z)∼D~b⁢a⁢t⁢c⁢hsimilar-tosuperscript𝑠superscript𝑠𝑧subscript~𝐷𝑏𝑎𝑡𝑐ℎ(s^{+},s^{-},z)\sim\widetilde{D}_{batch}( italic_s start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_s start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , italic_z ) ∼ over~ start_ARG italic_D end_ARG start_POSTSUBSCRIPT italic_b italic_a italic_t italic_c italic_h end_POSTSUBSCRIPT 11: z^∼π^θ⁢(s+,s−)similar-to^𝑧subscript^𝜋𝜃superscript𝑠superscript𝑠\hat{z}\sim\hat{\pi}_{\theta}(s^{+},s^{-})over^ start_ARG italic_z end_ARG ∼ over^ start_ARG italic_π end_ARG start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_s start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT , italic_s start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ) 12: L=SigmoidLoss⁢(z^,z)𝐿SigmoidLoss^𝑧𝑧L=\text{SigmoidLoss}(\hat{z},z)italic_L = SigmoidLoss ( over^ start_ARG italic_z end_ARG , italic_z ) 13: θ←θ−η⁢∇θL←𝜃𝜃𝜂subscript∇𝜃𝐿\theta\leftarrow\theta-\eta\nabla_{\theta}Litalic_θ ← italic_θ - italic_η ∇ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT italic_L //Update the parameters using any gradient descent algorithm 14: end for 15: end for IV Experiments and Evaluation IV-A Training Details We use four widely used open-source models as our base models, GLM-4-9B-Chat, Llama-3-8B-instruct[24], Qwen2-7B-chat, DeepseekMOE-16B[25]. GLM-4-9B-Chat is the open-source version of the latest generation of LLms in the GLM-4 family from Smart Spectrum AI, and has been evaluated on a variety of benchmarks in semantics, math, reasoning, code and knowledge extraction, demonstrated superior performance against previous generation of models. LLaMA3 is a LLM developed by Meta, optimized for a wide range of conversational use cases, outperforming many open-source models on common industry benchmarks. Qwen2 is a series of open source large models of Tongyi Qianqian developed by Aliyun. The series provides multiple versions and scales of open source models, such as Base and Instruct, so as to meet different computing needs. DeepSeekMoE 16B is a Mixture-of-Experts (MoE) language model with 16.4B parameters. It employs an innovative MoE architecture, which involves two principal strategies: fine-grained expert segmentation and shared experts isolation. Training was performed on 1 NVIDIA Tesla A40(48GB) using a low-rank adaptive (lora) parameter efficient tuning method[26]. We trained using bf16 (Deepseek for fp32) precision with learning rate = 5e-5, batchsize = 2, gradient accumulation = 8, maximum length = 1024, dropout = 0.1, and a cosine learning rate scheduler. In the stage of indicating the model produces diversified outputs, we set the number of samples k = 3 and the temperature = 1.2. The statistical information of the dataset is presented in the following Table 3. TABLE III: Dataset Composition Component Initial Visits Follow-up Visits Training 50 131 Validation 7 19 Test 14 38 Total 71 188 TABLE IV: Performance metrics for various models and methods (Part 1). Metrics Model Method ROUGE-1 ROUGE-2 ROUGE-L BLEU-4 GLM4-9b-chat Zero-shot 17.205 ±plus-or-minus\pm± 0.083 5.403 ±plus-or-minus\pm± 0.403 11.157 ±plus-or-minus\pm± 0.049 3.887 ±plus-or-minus\pm± 0.142 Few-shot 22.427 ±plus-or-minus\pm± 0.046 11.064 ±plus-or-minus\pm± 0.329 18.150 ±plus-or-minus\pm± 0.222 9.707 ±plus-or-minus\pm± 0.509 SFT 54.730 ±plus-or-minus\pm± 0.821 39.024 ±plus-or-minus\pm± 1.320 52.453 ±plus-or-minus\pm± 0.892 37.021 ±plus-or-minus\pm± 1.288 SFT+DPO 55.137 ±plus-or-minus\pm± 0.730 39.718 ±plus-or-minus\pm± 0.392 53.785 ±plus-or-minus\pm± 0.849 38.754 ±plus-or-minus\pm± 0.600 LLaMA3-8b-chat Zero-shot 4.266 ±plus-or-minus\pm± 0.720 1.280 ±plus-or-minus\pm± 0.090 3.076 ±plus-or-minus\pm± 0.356 1.186 ±plus-or-minus\pm± 0.175 Few-shot 18.933 ±plus-or-minus\pm± 0.369 10.428 ±plus-or-minus\pm± 0.386 15.707 ±plus-or-minus\pm± 0.596 7.248 ±plus-or-minus\pm± 0.133 SFT 51.707 ±plus-or-minus\pm± 2.943 34.545 ±plus-or-minus\pm± 3.401 49.269 ±plus-or-minus\pm± 2.820 31.601 ±plus-or-minus\pm± 2.515 SFT+DPO 51.107 ±plus-or-minus\pm± 0.426 35.161 ±plus-or-minus\pm± 0.711 50.254 ±plus-or-minus\pm± 0.928 33.934 ±plus-or-minus\pm± 0.627 DeepSeek-MOE-16b-chat Zero-shot 11.273 ±plus-or-minus\pm± 0.689 4.107 ±plus-or-minus\pm± 0.545 10.239 ±plus-or-minus\pm± 0.409 4.710 ±plus-or-minus\pm± 0.296 Few-shot 17.939 ±plus-or-minus\pm± 0.223 7.496 ±plus-or-minus\pm± 0.808 13.801 ±plus-or-minus\pm± 0.647 5.864 ±plus-or-minus\pm± 0.763 SFT 51.600 ±plus-or-minus\pm± 0.866 34.812 ±plus-or-minus\pm± 0.723 49.787 ±plus-or-minus\pm± 0.759 32.756 ±plus-or-minus\pm± 0.961 SFT+DPO 54.989 ±plus-or-minus\pm± 1.267 39.576 ±plus-or-minus\pm± 1.473 53.698 ±plus-or-minus\pm± 1.840 37.429 ±plus-or-minus\pm± 1.427 Qwen2-7b-chat Zero-shot 10.029 ±plus-or-minus\pm± 0.291 3.873 ±plus-or-minus\pm± 0.351 11.074 ±plus-or-minus\pm± 0.217 4.618 ±plus-or-minus\pm± 0.577 Few-shot 14.422 ±plus-or-minus\pm± 0.291 5.213 ±plus-or-minus\pm± 0.372 11.279 ±plus-or-minus\pm± 0.247 4.258 ±plus-or-minus\pm± 0.278 SFT 51.578 ±plus-or-minus\pm± 1.129 35.146 ±plus-or-minus\pm± 1.748 49.309 ±plus-or-minus\pm± 1.345 33.348 ±plus-or-minus\pm± 2.206 SFT+DPO 53.628 ±plus-or-minus\pm± 1.268 38.163 ±plus-or-minus\pm± 1.649 51.789 ±plus-or-minus\pm± 1.512 36.460 ±plus-or-minus\pm± 1.628 GPT-3.5-turbo Zero-shot 19.661 ±plus-or-minus\pm± 0.584 8.865 ±plus-or-minus\pm± 0.291 16.111 ±plus-or-minus\pm± 0.211 6.935 ±plus-or-minus\pm± 0.343 Few-shot 32.019 ±plus-or-minus\pm± 1.743 20.040 ±plus-or-minus\pm± 1.784 31.062 ±plus-or-minus\pm± 1.753 18.448 ±plus-or-minus\pm± 1.498 IV-B Baseline In order to fully evaluate our method, we chose a series of LLMs with different training path and training data as baselines for comparison. • Zero-shot[27]: The model is given a task without any prior examples. This approach tests the model’s ability to generalize from its training data to new, unseen scenarios. For each model, the zero-shot prompt is carefully crafted to be clear and concise, providing only the necessary context and the query. • Few-shot[28]: The model is provided with a few examples to learn from before it is asked to perform the task. This method helps the model understand the task better by seeing similar instances. The few-shot prompts are designed to include a small number of examples (2 in our experiment) before presenting the actual query. • SFT[29]: Supervised fine-tuning will provide medical cases with their corresponding standard diagnoses for the model to learn. • SFT+DPO: After warming up using supervised fine-tuning, the model is guided to generate multiple outputs, and the data is labeled using an automated labeling system which in turn generates preference data for the dpo training process. Zero-shot learning is an important method for evaluating the generalization ability of language models without any task-specific data. For the field of TCM, zero-shot testing can demonstrate the adaptability of the model to unseen TCM tasks, especially when data is scarce. This method can highlight the basic language understanding and reasoning ability of the model, and provide a lower bound on the performance of tasks in this field. Few-shot learning provides the ability of the model to learn quickly from a small number of examples. Through training with a few samples, we can evaluate whether the model can effectively reason and generate accurate results under the condition of limited labeled data. SFT is a standard method for model fine-tuning. For the field of TCM, supervised fine-tuning of the model using existing medical case data can improve the performance of the model on specific tasks. In the baseline comparison, supervised fine-tuning provides the upper limit of the performance that the model can achieve after using domain-specific data. The SFT+DPO method combines supervised fine-tuning and preference optimization to improve the quality of model generation while reducing bias. By using an automated annotation system to generate preference data, DPO can help the model learn outputs that are more in line with the doctor’s preferences. IV-C Evaluation Metrics The assessment of the quality of medical dialog is a multifaceted task. In order to comprehensively assess the quality of medical dialog, we used Bleu[30], Rough[31], and bert-score[32] as evaluation metrics. For the same problem, we sampled three outputs from the model and calculated the average metric of these three outputs for error reduction and their standard deviation. IV-D Results As shown in Tables 4 and 5, the experimental results comprehensively evaluate the performance metrics of various models and methods applied to the proposed TCM task. The primary performance metrics include ROUGE-1, ROUGE-2, ROUGE-L, BLEU-4, precision, recall, and BERT-Score F1. The zero-shot results demonstrate the initial capabilities of the models, generating responses based solely on pre-trained knowledge without any prior examples. In this scenario, all models performed poorly as they had not learned how to prescribe correctly based on medical cases during the pre-training phase. Notably, LLaMA3 performed significantly worse than other models, possibly due to LLaMA being more extensively trained on English corpora, while TCM tasks likely require a higher level of proficiency in Chinese. After adopting the few-shot method, the performance of all models improved to varying degrees, highlighting their ability to learn and adapt from a small number of instances, thereby enhancing their response quality. It is worth noting that GPT-3.5-turbo showed the most significant improvement, indicating its strong ability to follow instructions. The SFT stage embedded more specific TCM knowledge and prescription tasks into the models, enabling them to generate more accurate and contextually appropriate responses, significantly improving performance across all models at this stage. The DPO stage refined the model outputs using preference data, aligning them more closely with human preferences and expectations, achieving superior overall performance. For certain models, such as LLaMA3, the bias in outputs was reduced post-DPO, indicating the effectiveness of aligning the work with preference data, leading to more stable and intention-aligned outputs. Despite GPT-3.5-turbo’s excellent performance, models enhanced through SFT and DPO exhibited superior results, underscoring the effectiveness of our proposed framework. Considering the difficulty of obtaining high-quality TCM data, achieving such results with a small amount of data is gratifying. Additionally, our framework can be applied to any large foundational model without any modifications. Our research results validate the effectiveness of the proposed framework, particularly the integration of SFT and DPO in improving model performance. The models can generate accurate and relevant medical responses while ensuring alignment with expert preferences, laying the foundation for our next steps in research. IV-E Case Study To illustrate the effectiveness of our proposed framework, we conducted a detailed case study focusing on its application in Traditional Chinese Medicine (TCM) diagnosis and prescription tasks. This case study demonstrates the model’s capability to handle both initial and follow-up diagnoses effectively, showcasing its practical utility in real-world medical scenarios. We selected a patient case from our corpus, which includes both initial and follow-up visits. The case details are as follows: IV-E1 Initial Visit November 6, 2022. Patient: Liu, Female, 71 years old. Chief Complaint: Dizziness for over a year. Present Illness History: The patient started experiencing dizziness a year ago, occurring when lying flat and turning her head to the left at night, as well as when looking upward during the day. This dizziness is accompanied by visual rotation, occasional nausea and vomiting, a feeling of heaviness and fatigue in the limbs, irritability, normal appetite and sleep, a slightly bitter taste in the mouth, and regular bowel movements and urination. Tongue: Pale tongue with yellow greasy coating. Pulse: Slippery. IV-E2 Follow-up Visit November 13, 2022. Symptoms: After taking the previous prescription, the patient’s dizziness significantly reduced. She experienced slight dizziness when getting up in the morning but had almost no dizziness when turning her head to the left. The patient felt physically strong, not irritable, had a normal appetite and sleep, a neutral taste in her mouth, and regular bowel movements and urination. Her tongue was red with a yellow greasy coating, and her pulse was wiry, tight, and rapid. TABLE V: Performance metrics for various models and methods (Part 2). Metrics Model Method Precision Recall F1 GLM4-9b-chat Zero-shot 0.604 ±plus-or-minus\pm± 0.001 0.748 ±plus-or-minus\pm± 0.001 0.668 ±plus-or-minus\pm± 0.001 Few-shot 0.637 ±plus-or-minus\pm± 0.002 0.780 ±plus-or-minus\pm± 0.003 0.700 ±plus-or-minus\pm± 0.002 SFT 0.821 ±plus-or-minus\pm± 0.003 0.830 ±plus-or-minus\pm± 0.002 0.829 ±plus-or-minus\pm± 0.004 SFT+DPO 0.833 ±plus-or-minus\pm± 0.006 0.834 ±plus-or-minus\pm± 0.003 0.830 ±plus-or-minus\pm± 0.004 LLaMA3-8b-chat Zero-shot 0.491 ±plus-or-minus\pm± 0.002 0.544 ±plus-or-minus\pm± 0.005 0.515 ±plus-or-minus\pm± 0.003 Few-shot 0.602 ±plus-or-minus\pm± 0.004 0.748 ±plus-or-minus\pm± 0.014 0.666 ±plus-or-minus\pm± 0.006 SFT 0.829 ±plus-or-minus\pm± 0.011 0.819 ±plus-or-minus\pm± 0.010 0.821 ±plus-or-minus\pm± 0.010 SFT+DPO 0.824 ±plus-or-minus\pm± 0.003 0.827 ±plus-or-minus\pm± 0.003 0.823 ±plus-or-minus\pm± 0.003 DeepSeek-MOE-16b-chat Zero-shot 0.623 ±plus-or-minus\pm± 0.002 0.745 ±plus-or-minus\pm± 0.002 0.677 ±plus-or-minus\pm± 0.001 Few-shot 0.586 ±plus-or-minus\pm± 0.005 0.670 ±plus-or-minus\pm± 0.005 0.622 ±plus-or-minus\pm± 0.005 SFT 0.821 ±plus-or-minus\pm± 0.006 0.825 ±plus-or-minus\pm± 0.006 0.821 ±plus-or-minus\pm± 0.006 SFT+DPO 0.845 ±plus-or-minus\pm± 0.009 0.834 ±plus-or-minus\pm± 0.010 0.831 ±plus-or-minus\pm± 0.009 Qwen2-7b-chat Zero-shot 0.601 ±plus-or-minus\pm± 0.002 0.735 ±plus-or-minus\pm± 0.004 0.660 ±plus-or-minus\pm± 0.002 Few-shot 0.581 ±plus-or-minus\pm± 0.003 0.701 ±plus-or-minus\pm± 0.006 0.634 ±plus-or-minus\pm± 0.004 SFT 0.822 ±plus-or-minus\pm± 0.006 0.821 ±plus-or-minus\pm± 0.005 0.818 ±plus-or-minus\pm± 0.005 SFT+DPO 0.826 ±plus-or-minus\pm± 0.004 0.830 ±plus-or-minus\pm± 0.003 0.826 ±plus-or-minus\pm± 0.003 GPT-3.5-turbo Zero-shot 0.634 ±plus-or-minus\pm± 0.003 0.765 ±plus-or-minus\pm± 0.001 0.692 ±plus-or-minus\pm± 0.002 Few-shot 0.707 ±plus-or-minus\pm± 0.008 0.800 ±plus-or-minus\pm± 0.003 0.748 ±plus-or-minus\pm± 0.006 IV-E3 Model application We instructed the model to make a corresponding diagnosis based on the patient’s symptoms. • Step 1: Initial Diagnosis and Prescription During the initial diagnosis, the model generated the following prescription based on the given patient information: Traditional Chinese Medicine Diagnosis: Vertigo, hyperactivity of liver yang, and phlegm-dampness obstruction in the middle burner. Treatment Principle: Pacify the liver and subdue yang, transform phlegm and dampness. Prescription: Modified Tianma Gouteng Decoction combined with Yidou Powder. Ingredients: Gastrodia (Tianma) 10g, Uncaria (Gouteng) 15g, Raw Dragon Bone (Sheng Longgu) 30g (decoct first), Chuanxiong (Ligusticum Chuanxiong) 10g, Sparganium (Sanleng) 10g, Curcuma Zedoaria (Ezhu) 10g, Angelica Sinensis (Danggui) 15g, Raw Astragalus (Sheng Huangqi) 30g, Leonurus (Yimucao) 30g, White Peony (Baishao) 20g, Ziziphus Jujuba Seed (Zao Ren) 20g, Poria (Fuling) 15g, Polygala (Yuanzhi) 10g, Acorus Tatarinowii (Shichangpu) 10g, Prepared Pinellia (Qing Banxia) 10g, Dried Tangerine Peel (Chenpi) 10g, Bamboo Shavings (Zhuru) 10g, Honey-fried Licorice (Zhigancao) 10g, Amomum (Doukou) 10g, Raw Coix Seed (Sheng Yiyiren) 30g. Dosage: 7 doses, to be decocted in water and taken once daily, divided into morning and evening doses. • Step 2: Follow-up Diagnosis and Prescription For the follow-up visit, the model synthesized the previous prescriptions and patient feedback to generate a new prescription: Follow-up Prescription: The previous prescription is continued with the addition of Chrysanthemum (Juhua) 10g, for another 7 doses. We evaluated the generated prescriptions using automated annotation and confirmed the relevance and accuracy of the prescriptions based on traditional medical principles. The model’s ability to adapt to patient feedback and effectively adjust treatment plans demonstrates its potential for practical medical applications. We are well aware that manual evaluation will provide a better insight of how the proposed framework performed in real-world application, but due to issues such as time and cost, we use automated evaluation instead of manual evaluation in this work. IV-E4 Insights and Impact • Accuracy: The model accurately diagnosed and prescribed treatment based on TCM principles, reflecting a deep understanding of the medical corpus it was trained on. • Adaptability: The model effectively handled follow-up visits, adjusting prescriptions based on patient feedback and progress, similar to a human practitioner. • Efficiency: Automated annotations significantly reduced the need for manual labeling, streamlining the process of preference data collection and enhancing model performance. By integrating supervised fine-tuning with automated annotation and direct preference optimization, our framework not only improves model performance but also ensures that the generated outputs align closely with expert knowledge and user preferences. This approach offers a scalable and efficient solution for enhancing large language models in specialized domains like Traditional Chinese Medicine. V Conclusion and Limitations In this paper, we propose a framework that combines supervised fine-tuning and direct preference optimization to improve the performance of large language models for Traditional Chinese Medicine tasks. Our proposed approach addresses the unique challenges faced by TCM, such as the scarcity of high-quality data and the expertise required for accurate medical applications. By utilizing a small but high-quality corpus of TCM and incorporating an automated annotation process, we are able to significantly improve the model’s ability to generate accurate and relevant medical prescriptions. Experimental results show that our framework outperforms existing models, including widely used LLMs such as GPT-3.5-turbo, on various evaluation metrics such as ROUGE, BLEU, and BERT-Score. Our case study further illustrates the practical applicability of the framework in real TCM consultation scenarios, demonstrating the model’s ability to effectively handle both initial and follow-up consultations. Automatic annotation proved to be efficient, reducing the need for manual annotation while maintaining high accuracy of the model output. Despite the promising results, there are limitations to our approach. The reliance on small datasets, while demonstrating the efficiency of the framework, also highlights the potential advantages of larger and more diverse corpora. In addition, our task is limited to the TCM prescription task only, and the expert annotation is not as high quality as the manual annotation. In conclusion, our framework provides a scalable and efficient solution for augmenting large language models in specialized fields such as TCM, paving the way for future research and development in integrating AI with TCM practices. Scaling up the size of the dataset, introducing expert labeling and developing new tasks are our future work. In addition, high-quality datasets are difficult to obtain, and due to the particularity of traditional Chinese medicine, datasets are almost all Chinese corpus. Appropriate addition of English corpus for mixed training can fully tap the potential of the model. Acknowledgment The authors thank anonymous reviewers for their insightful comments. This research was partially supported by grants from the National Natural Science Foundation of China(No.61877051). References [1] I. Beltagy, K. Lo, and A. Cohan, “Scibert: A pretrained language model for scientific text,” arXiv preprint arXiv:1903.10676, 2019. [2] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., “Training language models to follow instructions with human feedback,” Advances in neural information processing systems, vol. 35, pp. 27 730–27 744, 2022. [3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774, 2023. [4] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang, “Glm: General language model pretraining with autoregressive blank infilling,” in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 320–335. [5] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. [6] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang et al., “Qwen technical report,” arXiv preprint arXiv:2309.16609, 2023. [7] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P.-C. Chang, A. Carroll, C. Lau, R. Tanno, I. Ktena, B. Mustafa, A. Chowdhery, Y. Liu, S. Kornblith, D. Fleet, P. Mansfield, S. Prakash, R. Wong, S. Virmani, C. Semturs, S. S. Mahdavi, B. Green, E. Dominowska, B. A. y Arcas, J. Barral, D. Webster, G. S. Corrado, Y. Matias, K. Singhal, P. Florence, A. Karthikesalingam, and V. Natarajan, “Towards generalist biomedical ai,” 2023. [8] H. Xiong, S. Wang, Y. Zhu, Z. Zhao, Y. Liu, L. Huang, Q. Wang, and D. Shen, “Doctorglm: Fine-tuning your chinese doctor is not a herculean task,” arXiv preprint arXiv:2304.01097, 2023. [9] F. Lozano, “Basic theories of traditional chinese medicine,” Acupuncture for pain management, pp. 13–43, 2014. [10] L. C. Matos, J. P. Machado, F. J. Monteiro, and H. J. Greten, “Understanding traditional chinese medicine therapeutics: an overview of the basics and clinical applications,” in Healthcare, vol. 9, no. 3. MDPI, 2021, p. 257. [11] T. Le Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé et al., “Bloom: A 176b-parameter open-access multilingual language model,” 2023. [12] DeepSeek-AI, “Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model,” 2024. [13] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan, D. Wang, D. Yan et al., “Baichuan 2: Open large-scale language models,” arXiv preprint arXiv:2309.10305, 2023. [14] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen, Y. Zhao, Y. Lu et al., “Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation,” arXiv preprint arXiv:2107.02137, 2021. [15] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei, “Deep reinforcement learning from human preferences,” Advances in neural information processing systems, vol. 30, 2017. [16] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from human feedback with ai feedback,” arXiv preprint arXiv:2309.00267, 2023. [17] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, “Direct preference optimization: Your language model is secretly a reward model,” Advances in Neural Information Processing Systems, vol. 36, 2024. [18] H. Zhang, J. Chen, F. Jiang, F. Yu, Z. Chen, J. Li, G. Chen, X. Wu, Z. Zhang, Q. Xiao et al., “Huatuogpt, towards taming language model to be a doctor,” arXiv preprint arXiv:2305.15075, 2023. [19] S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y. Jia, and H. Zan, “Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38, no. 17, 2024, pp. 19 368–19 376. [20] W. Y. Wei Zhu and X. Wang, “Shennong-tcm: A traditional chinese medicine large language model,” https://github.com/michael-wzhu/ShenNong-TCM-LLM, 2023. [21] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al., “Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023. [22] S. Robertson, H. Zaragoza et al., “The probabilistic relevance framework: Bm25 and beyond,” Foundations and Trends® in Information Retrieval, vol. 3, no. 4, pp. 333–389, 2009. [23] Z. Sun, X. Li, X. Sun, Y. Meng, X. Ao, Q. He, F. Wu, and J. Li, “Chinesebert: Chinese pretraining enhanced by glyph and pinyin information,” arXiv preprint arXiv:2106.16038, 2021. [24] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Rojas, G. Feng, H. Zhao, H. Lai, H. Yu, H. Wang, J. Sun, J. Zhang, J. Cheng, J. Gui, J. Tang, J. Zhang, J. Li, L. Zhao, L. Wu, L. Zhong, M. Liu, M. Huang, P. Zhang, Q. Zheng, R. Lu, S. Duan, S. Zhang, S. Cao, S. Yang, W. L. Tam, W. Zhao, X. Liu, X. Xia, X. Zhang, X. Gu, X. Lv, X. Liu, X. Liu, X. Yang, X. Song, X. Zhang, Y. An, Y. Xu, Y. Niu, Y. Yang, Y. Li, Y. Bai, Y. Dong, Z. Qi, Z. Wang, Z. Yang, Z. Du, Z. Hou, and Z. Wang, “Chatglm: A family of large language models from glm-130b to glm-4 all tools,” 2024. [25] D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu et al., “Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,” arXiv preprint arXiv:2401.06066, 2024. [26] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021. [27] W. Wang, V. W. Zheng, H. Yu, and C. Miao, “A survey of zero-shot learning: Settings, methods, and applications,” ACM Transactions on Intelligent Systems and Technology (TIST), vol. 10, no. 2, pp. 1–37, 2019. [28] Y. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, “Generalizing from a few examples: A survey on few-shot learning,” ACM computing surveys (csur), vol. 53, no. 3, pp. 1–34, 2020. [29] T. B. Brown, “Language models are few-shot learners,” arXiv preprint arXiv:2005.14165, 2020. [30] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311–318. [31] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,” in Text summarization branches out, 2004, pp. 74–81. [32] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” arXiv preprint arXiv:1904.09675, 2019."
https://arxiv.org/html/2411.00890v1,Rethinking Scale: The Efficacy of Fine-Tuned Open-Source LLMs in Large-Scale Reproducible Social Science Research,"Large Language Models (LLMs) are distinguished by their architecture, which dictates their parameter size and performance capabilities. Social scientists have increasingly adopted LLMs for text classification tasks, which are difficult to scale with human coders. While very large, closed-source models often deliver superior performance, their use presents significant risks. These include lack of transparency, potential exposure of sensitive data, challenges to replicability, and dependence on proprietary systems. Additionally, their high costs make them impractical for large-scale research projects.In contrast, open-source models, although available in various sizes, may underperform compared to commercial alternatives if used without further fine-tuning. However, open-source models offer distinct advantages: they can be run locally (ensuring data privacy), fine-tuned for specific tasks, shared within the research community, and integrated into reproducible workflows.This study demonstrates that small, fine-tuned open-source LLMs can achieve equal or superior performance to models such as ChatGPT-4. We further explore the relationship between training set size and fine-tuning efficacy in open-source models. Finally, we propose a hybrid workflow that leverages the strengths of both open and closed models, offering a balanced approach to performance, transparency, and reproducibility.","The widespread availability of generative AI, particularly large language models (LLMs), is transforming both society and science. These models are becoming increasingly easy to prompt, making them appealing to scientists across both the hard and social sciences for conducting text classification tasks in a wide range of disciplines, including finance \parenciteLoukas23, health studies \parenciteGuo24,cao2024,Kiyasseh24, environmental studies \parencitetrajanov2023, geoscience \parencitemaze2024, physics \parenciteli2024, chemistry \parenciteliao2024 radiology \parenciteNowak2024, psychology \parenciteSuhaib24, sociology \parencitekozlowski2024,Mutzel23,Chae2023, political science \parenciteLiuGe2024, criminology \parenciteNikolakopoulos24, law studies \parenciteWei23, just to mention a few. Recent studies have sparked both enthusiastic \parenciteGilardi2023,rouzegar2024 and critical \parenciteSuhaib24 assessments of LLMs. A central debate in this field involves closed vs. open foundation models \parenciteZhang24,Nowak2024,Suhaib24,hanke2024open,Chae2023. Proprietary models, such as those from OpenAI and Google, present several challenges: they can change versions without notice, their architectures are not fully disclosed, and they may be subject to content moderation—such as filtering for hate speech or privacy concerns—designed to protect companies from legal and reputational risks. While these safeguards are important, they fall outside the control of scientists. For researchers, the financial and computational resources required to train large proprietary models, like OpenAI’s GPT-4 (estimated to cost $100M to develop), are prohibitive. However, smaller, open-source models can be fine-tuned on readily available hardware, such as a laptop or a small GPU cluster, making them feasible for academic projects. Even companies like OpenAI are now moving towards smaller, more efficient models using techniques like the mixture-of-experts approach. LLMs are typically measured by the number of parameters in their neural network architecture. While proprietary models often withhold exact specifications (e.g., OpenAI’s GPT-4 is estimated to use over 1.7 trillion parameters, Google’s Gemini Ultra has probably 1560B parameters and Anthropic’s Claude 3 is rumored to have 500B parameters), open-source models provide clear documentation of their architectures. This study focuses on the Meta-developed LLAMA family of open-source models, including versions from 7B to 405B parameters. We do not consider other open-source models, such as Mistral/Mixtral, BLOOM, and Falcon, although they share similar fine-tuning capabilities and usage restrictions based on licensing. Fine-tuning allows these open-source models to be specialized for specific tasks without retraining from scratch. In this study, we fine-tuned the LLAMA models using Low-Rank Adaptation \parencitehu2021 (LoRA) , which modifies the model’s output to better suit particular domains or tasks. Unlike proprietary models such as ChatGPT, whose fine-tuned versions remain under corporate control, open-source fine-tuned models can be freely shared, advancing reproducible (social) science \parenciteTrustMeBro. This research was motivated by the need to analyze Harvard’s 10B Geo-Tweets archive \parenciteLewis16. At current costs, applying commercial LLMs to this scale would be unfeasible, with token pricing reaching hundreds of thousands of dollars for large-scale analysis. In contrast, open-source models can run on local clusters, using tools like LLAMA.cpp \parencitellamacpp, Oolama \parenciteollama, GPT4All \parencitegpt4all or similar open source projects, making large-scale, affordable analysis possible. In this work, we demonstrate that while larger models generally outperform smaller ones, fine-tuning can level the playing field. Properly fine-tuned open-source models can achieve performance comparable to, or better than, significantly larger models like ChatGPT-4, especially in non-trivial classification tasks. Additionally, we examine how the size of the training data impacts the effectiveness of fine-tuning, finding that models trained on more extensive data are harder to fine-tune effectively. Finally, we introduce a hybrid approach that utilizes both proprietary models, like ChatGPT-4, to accelerate the creation of labeled datasets for fine-tuning, in cases where labeled data is not readily available. We evaluate these approaches through three classification tasks: (i) classifying tweets into 46 dimensions of well-being from the Human Flourishing Program \parenciteVanderWeele2017; (ii) classifying European Parliamentary questions into 19 policy areas from the Comparative Agenda Project \parenciteAlexandrova14,CAP (CAP); and (iii) classifying datasets from the Harvard Dataverse repository into 15 subject categories. These tasks, ranging from noisy, unlabelled data to highly curated datasets, extend beyond simple sentiment analysis and showcase the capabilities of fine-tuned open-source models."
https://arxiv.org/html/2411.00878v1,Exploring the Knowledge Mismatch Hypothesis:Hallucination Propensity in Small Models Fine-tuned on Data from Larger Models,"Recently, there has been an explosion of large language models created through fine-tuning with data from larger models. These small models able to produce outputs that appear qualitatively similar to significantly larger models. However, one of the key limitations that have been observed with these models is their propensity to hallucinate significantly more often than larger models. In particular, they have been observed to generate coherent outputs that involve factually incorrect information and spread misinformation, toxicity, and stereotypes. There are many potential causes of hallucination, of which, one hypothesis is that fine-tuning a model on data produced by a larger model leads to a knowledge mismatch which contributes to hallucination. In particular, it is hypothesized that there is a mismatch between the knowledge that is fed to the model to fine-tune it and the knowledge that is already present in the graph. Fine-tuning the model on data that has such mismatch could contribute to an increased propensity to hallucinate. We show that on an unseen test set, a smaller model fine-tuned on data generated from a larger model produced more wrong answers when compared to models fine-tuned on data created by the small model, which confirms the hypothesis.","A well-known limitation of large language models is their ability to hallucinate or generate factually incorrect statements [16]. While large language models are often able to appear fluent, the responses generated by these systems have been observed to often produce statements that are misleading, factually incorrect and harmful. In the context of dialogue-based systems, it has been observed that models often produce responses that are not supported by the evidence available to the system [19]. In the context of generative question and answering systems, models have been observed to provide factually incorrect responses [18]. This is a key issue in the field of Large Language Models because of the potential harm that hallucination can pose to users. For instance, in medical applications, a medical summary generated from patient information could pose a risk to the patient if the generated outputs involve hallucination. In this case, a factually false recommendation generated by the model can lead to a life-threatening incident for the patient. In recent times, this is even more concerning because of the rise of strong large language models that are often able to produce coherent and semantically convincing responses while at the same time, involving hallucination in their outputs. It has become increasingly difficult to tell the difference between the outputs of a model and text written by humans, especially in more general and generic topics and in prompts with short responses. This furthers the risk of hallucination in the outputs of models - the high level of coherence in the outputs of many recent models can easily fool many humans and even many experts. A key development in the field of open source models is the explosion of large language models created through fine-tuning with data from larger models. [10] [3] [9] [12] [11] Most of these fine-tuned models are not only relatively small and easy to reproduce but are also relatively performant and are able to produce outputs that appear qualitatively similar to significantly larger models. However, one key limitation that many of these models share is their propensity to hallucinate significantly more often than larger models. While there has been a lot of previous work in addressing hallucination through further tuning via techniques such as Reinforcement Learning through Human Feedback (RLHF) [14] [15] and Reinforcement Learning through AI Feedback (RLAI) [13], there has been relatively little work in analyzing the role of underlying training techniques such as fine-tuning. Hallucination has a lot of possible reasons, both with respect to data and the model. Our goal in this paper is to study one of the possible reasons: fine-tuning data. In particular, we want to verify the hypothesis that fine-tuning a model on data produced by a larger model leads to a knowledge mismatch, which may contribute to hallucination. Our analysis finds that on an unseen test set, a smaller model fine-tuned on data generated from a larger model produced more wrong answers when compared to models fine-tuned on data created by the small model, which confirms the hypothesis. Our findings question the overall effectiveness of current fine-tuning practices, given the potential for a knowledge mismatch that such fine-tuning may cause and its implications on model hallucination."
https://arxiv.org/html/2411.00863v1,Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation,"In the field of large language model (LLM)-based proof generation, despite being trained on extensive corpora such as OpenWebMath and Arxiv, these models still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the suboptimal order of each proof data used in training. Published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. However, this order aims to facilitate the verification of the proof’s soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders are substantial – with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order.","Recent works have shown the potential of large language models (LLMs) to perform mathematical reasoning and proof generation in both natural language and formalized environments Yang et al. (2023a); Welleck et al. (2021); Lample et al. (2022); Mikuła et al. (2023); Wang et al. (2023). Meanwhile, leading models consume far more data than a human could (datasets like OpenWebMath contain 14B tokens), and still perform suboptimally on reasoning benchmarks Paster et al. (2023); Azerbayev et al. (2023); Touvron et al. (2023); Dubey et al. (2024); Paster et al. (2023). This is also true for downstream reasoning tasks where LLMs need further fine-tuning Yang et al. (2023b); An et al. (2024). We identify this phenomenon as the training inefficiency problem. One possible explanation for such low training efficiency in proof generation for LLMs is poor internal order within each training data sample in the math domain. For instance, consider the purely logical order that many textbook proofs follow, where each step logically proceeds from the previous steps based on the deductive rules. This order is widely adopted for presenting proofs publicly because it allows for straightforward verification of the proof’s correctness. However, from a proof discovery perspective, this order is not intuitive, i.e. not the order in which the proof is actually discovered. For human learners, the most intuitive order of proof steps is one in which all steps contributing to the derivation of a particular proof step are presented prior to it. We call such order the intuitively sequential order. These steps, which aid in the derivation of a given proof step, are referred to as intermediate supervision for that step. Figure 1: An illustrative example of the purely logical order versus the intuitively sequential order. The arrows indicate the direction of intermediate supervision. Consider the following concrete scenario where the intuitively sequential order is preferred over the purely logical order in human learning and model training. A researcher wants to prove a math statement. He identifies a set of lemmas useful for the proof. But in order to transform the statement into a form where these lemmas can be applied, several preliminary steps are required. A textbook proof usually presents the preliminary steps first and then applies the lemmas, following the purely logical order. However, for LLM training – and for human learning – the helper lemmas are easy to figure out after seeing the statement. They provide intuition and act as intermediate supervision for coming up with the preliminary steps. On the other hand, the theorem statement itself provides no hints for these preliminary steps. The lemmas should therefore precede these preliminary steps in the training data We give a simple illustrative example of the difference between the purely logical order and the intuitively sequential order in Figure 1. We also provide a graduate-level math example in Appendix A.1. In Figure 1, Step 3 serves as the intermediate supervision that assist to predict Step 2, which in turn serves as the intermediate supervision that assist to predict Step 1. Inexperienced students typically struggle to understand this proof in the purely logical order on their first read, as it is unclear why the problem statement should lead to Step 1. If they are instead taught to first apply the lemma as in Step 3, the learning process becomes faster. In practice, the confusion of suboptimal order can be overcome through repeated reflection. As for model training with next token prediction, without introducing more elaborate training objectives, the only way to achieve similar effect is to modify the order of the training data. Given that (i) proofs and reasoning are often presented in the order of logic rather than in the intuitively sequential order, and (ii) next-token prediction is the predominant method used for pre-training and fine-tuning cutting-edge autoregressive LLMs, we hypothesize that training inefficiency of LLMs in solving reasoning tasks is partly due to the suboptimal order of data during training. We claim that the order of each data sample affect model training efficiency significantly, which we referred to as the order effect, and the intuitively sequential order is the optimal order for fine-tuning models. We show that there are two causes for this order effect. Assume we have proof step a𝑎aitalic_a, and proof steps {ai}subscript𝑎𝑖\{a_{i}\}{ italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } that serve as intermediate supervision toward a𝑎aitalic_a, and they occur in the order `⁢`⁢a,{ai}⁢""``𝑎subscript𝑎𝑖""``a,\{a_{i}\}""` ` italic_a , { italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } "". The first cause is that when learning to predict a𝑎aitalic_a, the model will not directly learn to use the supervision {ai}subscript𝑎𝑖\{a_{i}\}{ italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }, because it does not look forward during training. The second cause is that this order also induces the model to fit the non-existent dependency of {ai}subscript𝑎𝑖\{a_{i}\}{ italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } on a𝑎aitalic_a, which we call the spurious relation. Note that in reasoning tasks, intermediate supervision is usually another sequence of reasoning steps. To verify our claims, we conduct experiments on two downstream tasks: (1) 4-by-4 digit multiplication and, (2) intuitionistic propositional logic theorem-proving. We use the multiplication task to perform further analysis. Various metrics were used to measure model performance. The results demonstrate that models fine-tuned/trained-from-scratch with the intuitively sequential proofs outperformed those fine-tuned/trained-from-scratch with proofs in other orders. Our experiments also validate the two causes for the order effect we propose. Despite the theoretical understanding that LLMs learn joint distributions over data, where updates from later tokens might influence a model’s prediction on an earlier token – our experiments find no evidence that LLMs can look forward during learning (weird here put to the end). Our models do not learn to use future tokens to predict the current proof step. We also show experimental evidence in the multiplication task that models are negatively impacted by spurious step dependencies learned during training when it’s trained on data in suboptimal orders. Our contributions are summarized as follows: • We demonstrate that data order significantly affects LLM training efficiency on reasoning tasks, which we refer to as the order effect. • We identify that the intuitively sequential order is the optimal order for next-token-prediction pre-training/finetuning, where the intermediate supervision is naturally placed on the left. • Through the intuitionistic propositional logic theorem-proving task and 4-by-4 digit multiplication task, our experiment shows that this order effect is due to next-token prediction. Two causes are (1) LLMs do not look forward during training, (2) LLMs may learn spurious token dependencies if trained with data samples in suboptimal order."
https://arxiv.org/html/2411.00860v1,Survey of Cultural Awareness in Language Models: Text and Beyond,"Large-scale deployment of large language models (LLMs) in various applications, such as chatbots and virtual assistants, requires LLMs to be culturally sensitive to the user to ensure inclusivity. Culture has been widely studied in psychology and anthropology, and there has been a recent surge in research on making LLMs more culturally inclusive in LLMs that goes beyond multilinguality and builds on findings from psychology and anthropology. In this paper, we survey efforts towards incorporating cultural awareness into text-based and multimodal LLMs. We start by defining cultural awareness in LLMs, taking the definitions of culture from anthropology and psychology as a point of departure. We then examine methodologies adopted for creating cross-cultural datasets, strategies for cultural inclusion in downstream tasks, and methodologies that have been used for benchmarking cultural awareness in LLMs. Further, we discuss the ethical implications of cultural alignment, the role of Human-Computer Interaction in driving cultural inclusion in LLMs, and the role of cultural alignment in driving social science research. We finally provide pointers to future research based on our findings about gaps in the literature.111We additionally organize the papers covered by this survey at https://github.com/siddheshih/culture-awareness-llms.git.","Language models are deployed in various user-facing applications, such as recommender systems (Bao et al., 2023), customer service (Pandya and Holia, 2023), and search applications (Xiong et al., 2024), which are increasingly used by people in all aspects of their life including education (Kasneci et al., 2023), public health (De Angelis et al., 2023), and professional writing (Jakesch et al., 2023a). These models reflect the Western perspective, predominantly trained on Western-centric data (Durmus et al., 2023). This skewed perspective can lead to stereotyping and alienation of users, propagation of stereotypes due to a lack of cultural understanding (e.g., flattening of cultural identities), or responding in a culturally insensitive way (Cao et al., 2022, 2023). Therefore, cultural awareness is one of the critical factors that should be considered while creating NLP models. In this work, we provide a comprehensive survey of the steps that the NLP community has taken to make language models more culturally inclusive. Furthermore, with advancements in multimodal foundation models and their adaption on NLP tasks (Fei et al., 2022), we also examine efforts towards cultural inclusion in multimodal NLP systems (i.e., multimodal systems with language understanding as one of their components). As the notion of culture used by the NLP community (to define and ensure cultural inclusion in NLP systems) is adopted from social science research, we start by defining ‘cultural awareness in LLMs’ based on definitions of culture in psychology and anthropology literature. We then consolidate the works that look into cultural inclusion in LLMs and multimodal models, including benchmark creation, training data creation, alignment methodologies, and evaluation methodologies. We also discuss the role of cultural alignment in accelerating social research. Human-computer interaction (HCI) also plays a role in ensuring cultural alignment in LLMs, as how studying different cultures reacts to certain levels of cultural (mis)alignment and matching varied expectations of people falls under the realm of HCI research (Weidinger et al., 2023). Finally, we discuss the ethical and safety implications of current research directions and provide potential research avenues that the community could take to foster cultural inclusion in language models. While recent surveys (Liu, Gurevych, and Korhonen, 2024; Adilazuarda et al., 2024) focus on the cultural alignment of LLMs in NLP and provide a taxonomy for grouping current cultural alignment works, we consolidate the literature from a broader scope. We survey and compare efforts towards incorporation and conceptualization of culture in NLP systems, and our survey spans several modalities, including images, videos, and audio, along with text. We position our survey at the intersection of NLP, multimodality, and social science. The key contributions and research goals of this survey are as follows: 1. We review 300+ papers to provide an overview of the current state of benchmarks and methods used for cultural inclusion in multimodal language models (we organize the papers in §4, §5, §6); 2. We provide an overview of common data sources used for creating cultural alignment datasets and how current benchmark creation and culturally relevant fine-tuning dataset creation methodologies leverage these common sources (§3); we also discuss ethical implications and limitations of the dataset creation methodologies (§8); 3. We provide an overview of the coverage of current datasets for geographical regions and cultures (§7) and discuss measures that the community could take to foster equity in cultural inclusion (§9); 4. We also examine the societal impact and implications of deploying LLMs with or without cultural awareness and discuss the role of Human-Computer Interaction (HCI) research in cultural alignment (§8). Literature Collection Strategy. As our paper focuses on multimodal and text-based NLP, we consider papers published in conferences including ACL and regional ACL chapters, EMNLP, ICLR, and ICML, computer vision conferences such as ICCV and CVPR as well as papers published in the ACL Anthology. The inclusion of cultural aspects in the NLP and CV community has been a recent one, with most (benchmark) papers published post-2016, so we consider cultural inclusion benchmarks post-2016. We also consider recent submissions to Arxiv to include recent NLP and social science papers, as the publication cycles for social science journals are typically 1–3 years. For alignment methodologies, we specifically focus on recent works published after 2022, following the release of ChatGPT OpenAI . We define culture in §2 and organize our paper into three major parts. The first part discusses data sources and methodologies the community has used to create datasets and benchmarks for the cultural inclusion of LLMs (§3). The second part discusses the methodologies and state of benchmarks that have been used or created for improving cultural awareness in LLMs across modalities (§4, §5, §6). Finally, we discuss our observations: the state of cultural inclusion (§7), ethical issues related to cultural alignment, and the role of cultural alignment in accelerating social science research (§8), and future research directions (§9) in the last part. In each of the subsections in §4, §5, and §6, we identify specific research gaps and, based on the research gaps, provide concrete suggestions for future research in §9."
https://arxiv.org/html/2411.02348v1,Can large language models generalize analogy solving like children can?,"When we solve an analogy we transfer information from a known context to a new one through abstract rules and relational similarity. In people, the ability to solve analogies such as “body : feet :: table : ?” emerges in childhood, and appears to transfer easily to other domains, such as the visual domain “(((( : )))) :: <<< : ?”. Recent research shows that large language models (LLMs) can solve various forms of analogies. However, can LLMs generalize analogy solving to new domains like people can? To investigate this, we had children, adults, and LLMs solve a series of letter-string analogies (e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek alphabet), and a far transfer domain (list of symbols). As expected, children and adults easily generalized their knowledge to unfamiliar domains, whereas LLMs did not. This key difference between human and AI performance is evidence that these LLMs still struggle with robust human-like analogical transfer.","You may be familiar with the analogy “consciousness is like an iceberg”. Here, people can intuitively infer the below-the-surface depth and complexity of consciousness by relating it to an iceberg, whose mass is mostly found under water, just as our subconscious dwells under our conscious minds. This intuitive ability emerges in childhood Goddu et al., (2020); Gentner, (1988); Stevenson and Hickendorff, (2018). However, it is a subject of debate whether analogical reasoning has emerged in Large Language Models (LLMs) Webb et al., (2023); Lewis and Mitchell, (2024); Hodel and West, (2023); Webb et al., (2024). More importantly, are LLMs able to solve analogies at this level of conceptual abstraction and generalize to novel domains Mitchell, (2021); Shiffrin and Mitchell, (2023)? In this study, we investigate analogical transfer at two levels of abstraction (near and far), and compare LLM performance not only to adults, but also to children, who are still developing analogical reasoning abilities. We ask the question: Can LLMs can generalize analogy solving like children can? Analogical reasoning, the process of applying a known concept to understand something new through relational similarity, is fundamental to the way people think and learn Holyoak, (2012); Gentner and Hoyos, (2017). This is because we humans can easily generalize – that is, transfer principles discovered in one domain to new domains that share varying degrees of similarity with the original (Doumas et al.,, 2022). This can be principles in near contexts that are similar in terms of concrete attributes (e.g., shape, “a pyramid is like an iceberg”) or in farther contexts that are only similar in terms of abstract relations (e.g., abstraction of depth, “consciousness is like an iceberg”) Barnett and Ceci, (2002). Near analogies tend to be easier for both adults and children to solve than far analogies Stevenson et al., (2023); Jones et al., (2022); Thibaut and French, (2016). And, in general, adults are better at solving analogies than children. But, when the required domain knowledge and a causal framing are present then children can solve analogies such as “body is to feet as table is to ?” as early as the 3-4 years-old (e.g., Goddu et al.,, 2020; Goswami,, 1991). And when analogies are presented in a more challenging or far context young children tend to revert to associative strategies, e.g., replying ’egg’ to ’dog is to doghouse as chicken is to ?’ instead of ’chicken coop’ Stevenson and Hickendorff, (2018); Gentner, (1988); Thibaut and French, (2016). There are many tasks used to study analogical reasoning and transfer in people, from verbal to geometric to scene analogy problems (e.g., Ichien et al.,, 2020; Richland et al.,, 2006; Mulholland et al.,, 1980). However, many of these tasks are either not suitable for children (e.g., verbal analogies may contain unfamiliar words or relations for children) or to LLMs (e.g., visual analogies designed for children are still difficult for today’s multimodal models Yiu et al., (2024)). Therefore, we need a domain that is text-based, but doesn’t require domain knowledge beyond what a typical child or LLM would know. Letter-string analogies fit the bill as they require very little domain knowledge and offer an idealized scenario to examine analogical reasoning in a “pure, uncontaminated way” (Hofstadter,, 1984, p. 3). In these puzzles, a string of letters is transformed according to one or more rules, and the task is to use analogy and apply the same transformations to a new string. For example, “If abc changes to abd, what should pqr change to?” (Mitchell,, 2021). Letter-string analogy solving has been studied in human adults and LLMs. For example, (Webb et al.,, 2023) showed that GPT-3 is able to solve letter-string analogies better than college students. (Lewis and Mitchell,, 2024) showed that GPT-models solved letter-string analogies at about 60% accuracy in the Latin alphabet domain, somewhat below the level of adults they tested. Interestingly, (Lewis and Mitchell,, 2024) and (Hodel and West,, 2023) found that GPT-3’s performance degraded when presented with these same analogies using an alphabet of shuffled letters. Moreover, (Lewis and Mitchell,, 2024) showed that GPT-models had great difficulty solving letter-string analogies in an unfamiliar alphabet of symbols, whereas people did not. As such, there is conflicting evidence of whether LLMs can generalize analogy solving to novel domains (Lewis and Mitchell,, 2024; Webb et al.,, 2024; Hodel and West,, 2023), something that comes easily to adults (e.g., Thibaut et al.,, 2022; Doumas et al.,, 2022), and that even children appear capable of when domains share structural similarities Chen, (1996); Gentner and Toupin, (1986); Bobrowicz et al., (2020); Holyoak et al., (1984). Thus, while there is some evidence to suggest that LLMs can solve letter-string analogies at around the same level as people, it is unclear whether these models understand the problem and are actually using analogical reasoning Opiełka et al., (2024); Stevenson et al., (2023); Moskvichev et al., (2023). In this study, we investigate whether LLMs can generalize analogy solving to new domains like adults and 8-year-old children can at two levels of abstraction. To this end, we compare how adults, children, and LLMs generalize analogy solving on the letter-string task to both near (Greek alphabet) and far (Symbol list) domains."
https://arxiv.org/html/2411.02344v1,Seq-VCR:Preventing Collapse in Intermediate Transformer Representations for Enhanced Reasoning,"Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model’s intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR111https://github.com/rarefin/seq_vcr), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging 5×5555\times 55 × 5 integer multiplication task, our approach achieves 99.5%percent99.599.5\%99.5 % exact match accuracy, outperforming models of the same size (which yield 0%percent00\%0 % accuracy) and GPT-4 with five-shot CoT prompting (44%percent4444\%44 %). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision.","Figure 1: Position-wise number of operations needed for 5x5 digits integer multiplication task. Middle tokens in the output sequence need more operations than the peripheral ones, making their prediction much harder (as shown in Figure 6). Example of 12345 x 67890 is shown here. Large Language Models (LLMs) based on Transformer architectures have achieved remarkable success across a wide range of tasks, positioning them as foundational models in artificial intelligence (Bommasani et al., 2021). Despite their impressive capabilities, LLMs often struggle with tasks requiring complex reasoning, particularly arithmetic reasoning that necessitates multiple sequential operations (Bubeck et al., 2023). These challenges are attributed to the models’ limitations in handling tasks that involve deep cognitive abilities and multi-step reasoning processes. One of the key obstacles is the representation collapse in the intermediate layers of Transformer models. Representation collapse occurs when internal representation diversity diminishes, leading to less informative features and hindering the model’s ability to solve complex tasks (Jing et al., 2021). For arithmetic reasoning, transformers struggle with successive carryovers and storing intermediate results Qiu et al. (2024), which are essential for solving complex sub-tasks, requiring more computations (Figure 1). We hypothesize that representation collapse prevents the model from effectively performing sub-tasks by calculating successive carryovers and storing intermediate results, which are essential for accurate prediction. To address this limitation, we introduce Sequential Variance-Covariance Regularization (Seq-VCR), a regularization technique designed to enhance the entropy of intermediate representations and prevent representation collapse. By increasing the diversity of representations within the model’s layers, Seq-VCR enables the Transformer to maintain richer and more informative features throughout the computation process. Furthermore, we incorporate dummy pause tokens as substitutes for chain-of-thought (CoT) tokens. While CoT prompting has been shown to improve reasoning by breaking down tasks into intermediate steps (Wei et al., 2022), it often requires explicit supervision and can be computationally expensive. Our approach leverages pause tokens to simulate the effect of CoT without the need for explicit intermediate reasoning steps. We validate our method on challenging arithmetic reasoning tasks. Notably, on the 5×5555\times 55 × 5 integer multiplication task, our approach achieves 99.5%percent99.599.5\%99.5 % exact match accuracy, surpassing models of the same size (which yield 0%percent00\%0 % accuracy) and GPT-4 with five-shot CoT prompting (44%percent4444\%44 %). We also demonstrate significant improvements on arithmetic expression and longest increasing subsequence (LIS) datasets. Our contributions are summarized as follows: • We identify representation collapse in intermediate layers as a key limitation affecting the reasoning capabilities of Transformer models. • We propose Seq-VCR, a regularization technique that enhances the diversity of intermediate representations and prevents collapse. • We demonstrate that combining Seq-VCR with pause tokens enables models to solve complex arithmetic reasoning tasks without explicit CoT supervision. • We provide extensive experimental results showing significant improvements over baseline models and state-of-the-art LLMs like GPT-4."
https://arxiv.org/html/2411.02335v1,Sparsing Law: Towards Large LanguageModels with Greater Activation Sparsity,"Activation sparsity denotes the existence of substantial weakly-contributed elements within activation outputs that can be eliminated, benefiting many important applications concerned with large language models (LLMs), such as computation acceleration and model interpretability. Although promoting greater activation sparsity within LLMs deserves deep studies, existing works lack comprehensive and quantitative research on the correlation between activation sparsity and potentially influential factors. In this paper, we present a comprehensive study on the quantitative scaling properties and influential factors of the activation sparsity within decoder-only Transformer-based LLMs. Specifically, we propose PPL-p%percent𝑝p\%italic_p % sparsity, a precise and performance-aware activation sparsity metric that is applicable to any activation function. Through extensive experiments, we find several important phenomena. Firstly, different activation functions (i.e., ReLU and SiLU) exhibit comparable performance but opposite training-time sparsity trends. The activation ratio (i.e., 1−sparsity⁢ratio1sparsityratio1-\mathrm{sparsity\ ratio}1 - roman_sparsity roman_ratio) evolves as a convergent increasing power-law and decreasing logspace power-law with the amount of training data for SiLU-activated and ReLU-activated LLMs, respectively. These demonstrate that ReLU is more efficient as the activation function than SiLU and can leverage more training data to improve activation sparsity. Secondly, the activation ratio linearly increases with the width-depth ratio below a certain bottleneck point, indicating the potential advantage of a deeper architecture at a fixed parameter scale. Finally, at similar width-depth ratios, we surprisingly find that the limit value of activation sparsity varies weakly with the parameter scale, i.e., the activation patterns within LLMs are insensitive to the parameter scale. These empirical laws towards LLMs with greater activation sparsity have important implications for making LLMs more efficient and interpretable. Materials related to this work (i.e., codes and checkpoints) are available at https://github.com/thunlp/SparsingLaw.","Activation sparsity refers to the phenomenon where considerable elements within the output of a neural layer (typically activation functions, as shown in Figure 2) are zero or low values and thus contribute weakly to the final model output given a specific input. As a prevalent property of many language and vision modeling architectures (Li et al., 2022), activation sparsity has wide practical values, such as inference acceleration (Liu et al., 2023; Song et al., 2023; Xue et al., 2024; Song et al., 2024a), training acceleration (Zhang et al., 2024b), and LLM interpretation (Sajjad et al., 2022; Zhang et al., 2023). Generally, a model with a greater sparsity ratio (i.e., the ratio of inactivated elements) has more potential in these scenarios. However, this raises an underexplored problem: how to obtain an LLM with greater activation sparsity? Figure 1: A typical case of activation sparsity (with a sparsity ratio of 60%) in a gated feed-forward network of LLMs, where considerable elements weakly contribute to the outputs within the activation scores. Figure 2: The PPL-activation Pareto curve of the 0.1B MoE with different expert numbers versus the 0.1B vanilla decoder-only Transformer. A simple solution is to design constraints at the model architecture level that force the model to have a predefined large sparsity ratio. For example, mixture-of-experts (MoE), the most popular architecture-constrained design, typically uses a token-level top-k parameter selection router to assign a fixed sparsity ratio for each token at each layer (Fedus et al., 2022; Zoph et al., 2022). However, these constraints often sacrifice model flexibility and performance. Recent works reveal the potential performance degradation caused by such inflexible sparsity assignment (Huang et al., 2024; Liu et al., 2024). Moreover, to inspect the impact of such constraints, we plot the PPL-activation (PPL denotes perplexity) Pareto curve (activation⁢ratio=1−sparsity⁢ratioactivationratio1sparsityratio\mathrm{activation\ ratio}=1-\mathrm{sparsity\ ratio}roman_activation roman_ratio = 1 - roman_sparsity roman_ratio) of MoE in Figure 2 and compare it with a vanilla decoder-only Transformer (Touvron et al., 2023) of the same parameter scale and amount of training data111MoE models of different sparsity are obtained by tuning the number of activated experts, while for the vanilla setting, we adjust the CETT value proposed by Zhang et al. (2024a).. MoE has a significantly worse performance-sparsity trade-off. The best sparsity ratio is also hard to predefine, since a too-high or too-low sparsity ratio may lead to more severe performance degradation or substantial unnecessary computation, respectively. To avoid negative impacts on flexibility and performance, we focus on the intrinsic activation sparsity within decoder-only Transformer-based LLMs in this paper, such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023). Different from the sparsity enforced through architectural constraints, intrinsic activation sparsity (Zhang et al., 2024a; Song et al., 2024a) arises self-adaptively during the pre-training stage, without unified or predefined sparsity ratios among different tokens or layers. We provide a comprehensive study on the quantitative scaling properties and influential factors of this intrinsic property. These findings can help us formulate a generalizable method to control and promote activation sparsity without harming flexibility and performance. To rigorously study the influential factors of activation sparsity, we need a metric to precisely reflect the sparsity level of an LLM. Conventional works rely on prior experience to set a fixed global threshold for recognition of weakly-contributed neurons222A neuron denotes a certain row or column within the parameter matrices., which is empirical, inflexible, and performance-unaware (see Section 2.2). We propose an improved metric, named PPL-p%percent𝑝p\%italic_p % sparsity, with three advantages: versatility across model architectures, performance-awareness, and the precise recognition of weakly-contributed neurons. For versatility, PPL-p%percent𝑝p\%italic_p % sparsity follows Zhang et al. (2024a) and recognizes the most weakly-contributed neurons, which are then inactivated, by introducing layer-wise adaptive thresholds and comparing the magnitudes of neuron outputs with the layer-specific threshold. As a magnitude-based metric, it is not bound to the activation functions that output exactly non-negative elements (e.g., ReLU) and is applicable to any LLMs with activation layers. Moreover, considering the trade-off between performance and sparsity (Song et al., 2024a), performance-awareness is required, indicating whether a metric can comprehensively reflect the sparsity under each desired performance level. To this end, PPL-p%percent𝑝p\%italic_p % sparsity explicitly incorporates PPL as a performance proxy, computed as the ratio of inactivated neurons when the output PPL raises just by p%percent𝑝p\%italic_p % after the layer-wise thresholds are applied, compared to the dense setting with all neurons activated. Finally, the precise recognition of weakly-contributed neurons indicates that our metric obtains a good trade-off between performance and sparsity ratio (see Section 4.1). Based on the above metric, we systematically study the correlation between the activation sparsity and influential factors, including the amount of training data, the activation function, the width-depth ratio (i.e., the ratio of the hidden dimension to the layer number), and the parameter scale. Through comprehensive experiments, we obtain the following observations: 1. There is an increasing power-law (SiLU-activated LLMs) or decreasing logspace power-law (ReLU-activated LLMs) relationship between the activation ratio and the amount of training data. Both laws are convergent with a certain limit sparsity ratio as the amount of data approaches infinity. Note that the increasing sparsity-data trend indicates that ReLU-activated LLMs are more efficient in improving activation sparsity with more data. 2. Given the same parameter scale, the sparsity obtained by ReLU-activated LLMs always surpasses that of SiLU-activated LLMs, while their performance is comparable. 3. Given the same parameter scale, the activation ratio linearly increases with the width-depth ratio under a bottleneck point (i.e., deeper models are sparser), above which the activation fluctuates around a fixed level. However, considering the performance issue, the best width-depth ratio should be just within a certain interval that ensures the best performance. 4. Given similar width-depth ratios, the limit of activation sparsity is weakly correlated to the scale of LLMs. On the other hand, the convergence speed to the limit is much faster in smaller models. We try to explain these phenomena in Section 4.4. The above empirical laws can provide comprehensive instructional values for designing and pre-training an LLM with greater activation sparsity, which offers more significant potential in producing more efficient and interpretable LLMs. Moreover, our work enables the training-time prediction of the future sparsity ratio, and the evolving trend of activation sparsity with the amount of training data potentially provides a lens for the progress of neuron specialization."
https://arxiv.org/html/2411.02193v1,Improving Steering Vectors byTargeting Sparse Autoencoder Features,"To control the behavior of language models, steering methods attempt to ensure that outputs of the model satisfy specific pre-defined properties. Adding steering vectors to the model is a promising method of model control that is easier than finetuning, and may be more robust than prompting.However, it can be difficult to anticipate the effects of steering vectors produced by almost all existing methods, such as CAA (Panickssery et al., 2024) or the direct use of SAE latents (Templeton et al., 2024). In our work, we address this issue by using SAEs to measure the effects of steering vectors, giving us a method that can be used to understand the causal effect of any steering vector intervention. We use this method for measuring causal effects to develop an improved steering method, SAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific SAE features while minimizing unintended side effects. We show that overall, SAE-TS balances steering effects with coherence better than CAA and SAE feature steering, when evaluated on a range of tasks.","There are widespread calls for better control of the behaviour of Large Language Models (LLMs; e.g. The White House (2023)). Current methods such as prompting (Wallace et al., 2024) and finetuning (Ouyang et al., 2022; Chung et al., 2022) offer some degree of control, but have clear limitations. For example, prompting can be fragile and is often susceptible to methods that can subvert these instructions (Wei et al., 2023). Finetuning a model can be more robust but requires a curated dataset for training which can be both expensive and time-consuming to produce (e.g. Dubey et al. (2024)). Steering vectors (Turner et al., 2024) have the potential to be more robust than prompting, and both cheaper and easier to implement than finetuning. Steering vectors work by adding activations to the hidden state of a model, part way through the forward pass (Section 3). By generating and inserting activation vectors into the model’s forward pass, we can steer the model towards desired behaviors. However, a problem with current steering methods is their unpredictability – it’s often unclear exactly how a steering vector will affect model behavior. Steering vectors may not produce the intended changes in the model’s output or may cause unforeseen behaviors, as we discuss in Section 3. In some cases, steering interventions produce no interpretable changes in model behavior other than model degradation, as we show in Section 5. This unpredictability makes it difficult to precisely control the model. To address these challenges, we develop a method for quantifying the effects of a steering intervention on model outputs. We use Sparse Autoencoders (SAEs; Ng (2011); Cunningham et al. (2023); Bricken et al. (2023)) to measure the change in feature activations caused by steering interventions. This lets us understand what change in behavior to expect from any particular steering intervention. Note: our analysis is not about how early SAEs impact later layer SAEs (as studied by prior work such as Marks et al. (2024) and Anders and Bloom (2024)). Instead, we look at how rollouts generated by a steered model differ from those generated by the unsteered, base model. Building on this feature effects measurement method, we introduce SAE-Targeted Steering (SAE-TS), a method that constructs steering vectors to specifically target desired SAE features while minimizing unintended side effects. SAE-TS involves learning a linear relationship between steering vectors and their effects on SAE features. This allows us to construct steering vectors using the interpretable features found by the SAE. Notably, SAE-TS uses the SAE for steering differently to prior work such as Templeton et al. (2024) and Durmus et al. (2024). We use the SAE to measure the effects of steering vectors, rather than directly adding SAE latents to model forward passes. We evaluate SAE-TS against existing methods such as Contrastive Activation Addition (CAA) (Panickssery et al., 2024) and direct SAE feature steering Templeton et al. (2024). Our evaluations demonstrate that SAE-TS outperforms existing methods, achieving better alignment with the intended behavior while maintaining the semantic coherence of the generated text across various tasks. Key Contributions: 1. We develop a method to quantify and interpret the effects of a steering intervention using SAEs (Section 3). 2. We introduce SAE-Targeted Steering (SAE-TS), a method that constructs steering vectors to achieve specific desired effects while minimizing unintended changes (Section 4). 3. We evaluate SAE-TS on a set of steering tasks (Section 5), showing that it outperforms existing methods, successfully steering the model while maintaining output quality."
https://arxiv.org/html/2411.02118v1,Grounding Emotional Descriptions to Electrovibration Haptic Signals††thanks:This work was supported by research grants from VILLUM FONDEN (VIL50296) and the National Science Foundation (#2339707).,"Designing and displaying haptic signals with sensory and emotional attributes can improve the user experience in various applications. Free-form user language provides rich sensory and emotional information for haptic design (e.g., “This signal feels smooth and exciting”), but little work exists on linking user descriptions to haptic signals (i.e., language grounding). To address this gap, we conducted a study where 12 users described the feel of 32 signals perceived on a surface haptics (i.e., electrovibration) display. We developed a computational pipeline using natural language processing (NLP) techniques, such as GPT-3.5 Turbo and word embedding methods, to extract sensory and emotional keywords and group them into semantic clusters (i.e., concepts). We linked the keyword clusters to haptic signal features (e.g., pulse count) using correlation analysis. The proposed pipeline demonstrates the viability of a computational approach to analyzing haptic experiences. We discuss our future plans for creating a predictive model of haptic experience.","Programmable touch signals (i.e., haptics) with sensory and emotional attributes can provide immediate feedback, enhancing user experience in various applications like gaming, virtual reality environments, and mobile applications. For example, a lively haptic signal can increase user enjoyment during game play [1] or encourage the user to continue an action, while an alarming signal can warn the user about potential errors such as deleting a file [2]. Children can feel the emotional sentiment of words in a storybook with haptic effects. Also, haptics offers an alternative means of receiving emotional and contextual information for users with visual or auditory impairments. Haptics researchers have proposed various methods to assess the emotional content of haptic signals [3, 4]. Some works collected free-form user descriptions to showcase the rich range of sensations evoked by haptic signals [5]. User descriptions provide insights into the sensory and emotional attributes of haptic signals, but computational approaches for analyzing user language are missing in haptics. For example, a user may describe an electrovibration signal as “this signal feels smooth and exciting” or “it’s not super strong. It’s really quite pleasing.” Haptics researchers manually analyze these descriptions to extract relevant keywords and identify trends or themes across various users [6]. Yet, the manual data analysis is time-consuming and difficult to scale. This difficulty is exacerbated since haptic designers often need to test various combinations of signal parameters in user studies to verify the sensory and emotional content of the signals [3, 7]. To accelerate haptic signal design, we need efficient methods for analyzing user descriptions and linking them to haptic signal features such as pulse count (i.e., language grounding). Figure 1: The electrovibration haptic device in our data collection user study and an example of free-form user description. In this paper, we leveraged natural language processing (NLP) techniques to analyze free-form user descriptions for surface haptic signals [8, 9]. First, we collected user descriptions for 32 surface electrovibration signals in a lab study with 12 users (16 signals per participant). During the study, the participant(s) listened to white noise on a headphone and felt each haptic signal by sliding their index finger over a 3M MicroTouch electrostatic screen without seeing any visual representation of the signal (Figure 1). They verbally described the sensory, emotional, and associative attributes of the tactile sensation. All sessions were audio-recorded and transcribed. Next, we developed an NLP pipeline to analyze the descriptions in three steps. First, we used NLP techniques to extract sensory and emotion keywords from free-form user descriptions. Second, we divided the keywords into positive and negative sets to form semantic concepts (i.e., keyword clusters). Finally, we linked the keyword clusters to haptic signal features (e.g., pulse count) using correlation analysis. We contribute: • The first computational pipeline for analyzing free-form haptic descriptions • Clusters of positive and negative keywords used for describing haptic experience • Preliminary results on linking the concept clusters and electrovibration signal features using correlation analysis Our late-breaking results suggest a promising direction for haptic researchers is to further explore computational approaches for haptic signal design through NLP techniques. Figure 2: The overview of our computational pipeline with three main phases: (1) Keyword extraction identifies sensory and emotional keywords from the user descriptions and groups the keywords into positive and negative sentiments, (2) Keyword clustering clusters the keywords using four word embedding methods, and (3) Correlation analysis extracts statistical signal features and calculates the correlations between signal features and concept clusters."
https://arxiv.org/html/2411.01841v1,Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification,"Accurate annotation of educational resources is critical in the rapidly advancing field of online education due to the complexity and volume of content. Existing classification methods face challenges with semantic overlap and distribution imbalance of labels in the multi-label context, which impedes effective personalized learning and resource recommendation. This paper introduces RR2QC, a novel Retrieval Reranking method To multi-label Question Classification by leveraging label semantics and meta-label refinement. Firstly, RR2QC leverages semantic relationships within and across label groups to enhance pre-training strategie in multi-label context. Next, a class center learning task is introduced, integrating label texts into downstream training to ensure questions consistently align with label semantics, retrieving the most relevant label sequences. Finally, this method decomposes labels into meta-labels and trains a meta-label classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances the understanding and prediction capability of long-tail labels by learning from meta-labels frequently appearing in other labels. Addtionally, a Math LLM is used to generate solutions for questions, extracting latent information to further refine the model’s insights. Experimental results demonstrate that RR2QC outperforms existing classification methods in Precision@k and F1 scores across multiple educational datasets, establishing it as a potent enhancement for online educational content utilization. The code is available at https://github.com/78Erii/RR2QC.","In contemporary online learning environments, explicit annotation of knowledge labels is crucial for transparency and educational interpretability in recommendation systems, facilitating the implementation of personalized teaching strategies. However, the rapid growth of online education has resulted in an overwhelming volume of untagged exercises. Manually annotating these resources by educational experts is not only costly and time-consuming but also prone to potential biases. This necessitates the exploration of automatic annotating knowledge labels for exercises, which promises to enhance efficiency and reduce subjectivity in educational settings. Exercise content, unlike news and product descriptions, typically includes descriptive text, mathematical formulas, and geometric images, structured in a question-and-solution format. Each exercise is assigned one or more knowledge labels, making exercise annotation a Multi-Label Text Classification (MLTC) task. Due to the formula-heavy nature of exercise content, traditional machine learning methods extract features using techniques such as the n-gram distribution frequency of mathematical symbols [1], structural kernels [2], and TF-IDF [3, 4], followed by classification using Naive Bayes [5] and SVM [6]. With the advancement of deep learning, feature-capturing encoders likes TextCNN [7] and Bi-LSTM [8] have shown superior performance in processing complex text data. Pre-trained language models, particularly BERT [9], are now widely used for text classification tasks. Certain works, such as MathBERT [10, 11], JiuZhang [12], and QuesCo [13], leverage specifically designed pre-training tasks in BERT variants to learn general representations of mathematical text, then fine-tuning the models on downstream tasks. Although these methods outperform Vanilla BERT, annotating knowledge labels for exercise content still presents unique challenges. In real-world online learning systems, automatic annotation of exercises faces three key challenges. First, a small set of critical labels encompasses a large volume of exercises, while most other labels correspond to relatively fewer exercises, creating an uneven distribution that poses significant challenges for knowledge label prediction. Second, labels are often semantically rich, lengthy texts that frequently overlap due to high similarity. For example, as shown in Figure 1, both label A and label B contain meta-labels c and d, resulting in high similarity between their corresponding exercise content and making it difficult for classifiers to distinguish accurately. Lastly, some newer exercises lack reference solutions. Education experts manually annotate these exercises by hypothesizing potential solution approaches, but models lack access to this prior knowledge. The absence of solutions further complicates the model’s comprehension of the exercises. This paper introduces a Retrieval Reranking method To multi-label Question Classification (RR2QC) by leveraging label semantics and meta-label refinement, where question specifically refer to exercise presented as mathematical text without solution. First, RR2QC builds on QuesCo’s framework to develop a foundational model for multi-label question understanding. QuesCo employs data augmentation and a knowledge hierarchy tree to create contrastive learning tasks that learn holistic understanding of questions. Since QuesCo’s pre-training tasks are designed for single-label setting, we adapt them for the multi-label contexts. In detail, We introduce a ranking contrastive pre-training task that employs hierarchical knowledge distances to define positive sample pairs in multi-label contexts. By leveraging the semantic relationships among label groups, this approach leads to a more effective foundational model. Figure 1: A four-level knowledge hierarchy tree and its meta-labels (bottom). As shown in the figure, L⁢a⁢b⁢e⁢lA𝐿𝑎𝑏𝑒subscript𝑙𝐴Label_{A}italic_L italic_a italic_b italic_e italic_l start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT and L⁢a⁢b⁢e⁢lB𝐿𝑎𝑏𝑒subscript𝑙𝐵Label_{B}italic_L italic_a italic_b italic_e italic_l start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT share the meta-labels ""linear equations"" and ""linear functions,"" which easily mislead the classifier’s decisions. To address the decline in predictive performance due to semantic overlap and distribution imbalance of labels, the foundational model is divided into two independent models for downstream classification training. (1) The retrieval model predicts labels. To manage the extensive label set, we introduce a distance-based class center learning task that guides questions to approach predefined class centers in the feature space. These class centers are learnable parameters derived from well-distributed label text vectors, enabling questions to focus on label textual information during training. The model then retrieves the label sequence most relevant to the question’s semantics from a large pool of labels. (2) The reranking model leverages expert knowledge to decompose each label into meta-labels and retrains the foundational model on the meta-label dataset, generating a sequence of meta-labels for each question. By leveraging the mapping between labels and meta-labels, along with their confidence scores, the label sequence is reranked to produce a more refined result. This approach breaks down the semantically complex original labels into relatively independent and more evenly distributed meta-labels, aiding in the differentiation of highly similar labels and enhancing recognition of tail labels. Furthermore, to address the issue of missing solutions for questions, we integrate answers generated by a mathematics large language models (Math LLM) into the question inputs to enrich their semantic content, filling gaps in data that traditional models might overlook. Experiments demonstrate that RR2QC achieves superior Precision@k and F1 scores on real-world middle and high school mathematics and physics exercise datasets compared to existing text classification methods, establishing a new benchmark in the field. Additionally, RR2QC is applicable to other datasets with rich label semantics and uneven distributions. Our main contributions can be summarized as follows: 1) We improve upon QuesCo by designing a ranking contrastive pre-training task suitable for multi-label context. 2) We transform the text classification task into two steps: retrieval and reranking, effectively addressing the decline in automatic annotation accuracy caused by semantic overlap in lengthy text labels, as well as the prediction imbalance resulting from uneven label distribution. 3) We utilize the automatic problem-solving capabilities of Math LLM to augment questions, effectively uncovering the latent semantics of exercises and enhancing classification performance. The remainder of this paper is organized as follows: Section 2 provides a brief overview and discussion of related work. Section 3 presents our proposed RR2QC, including the necessity of using LLMs for data augmentation, the design of ranking contrastive pre-training task, and how to refine the retrieved label sequence using meta-labels in downstream tasks. Section 4 reports on the experimental design and results analysis. Finally, Section 5 concludes the paper."
https://arxiv.org/html/2411.01713v1,Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models,"Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space. This exploration is beneficial for finding good loss basins when training from scratch. It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization. At the same time, strong regularization on all parameters can lead to under-fitting. We hypothesize that selectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, Selective Projection Decay (SPD), that selectively imposes a strong penalty on certain layers while allowing others to change freely. Intuitively, SPD expands and contracts the parameter search space for layers with consistent and inconsistent loss reduction, respectively. Experimentally, when equipped with SPD, Adam consistently provides better in-distribution generalization and out-of-distribution robustness performance on multiple popular vision and language benchmarks. Code available at https://github.com/GT-RIPL/Selective-Projection-Decay.git.","Modern optimizers, such as Adam [1], LARS [2], and LAMB [3] usually include momentum and adaptive learning rates. They help optimizers avoid local minima and accelerate learning [4, 5] to explore wider parameter spaces. However, we hypothesize that this behavior is not always beneficial for fine-tuning from a well pre-trained foundation model, especially when fine-tuning a few layers is already sufficient for fitting the target data [6, 7, 8, 9]. Several prior works have found that unnecessary exploration will lead to large deviation from the initialization and worse robustness [10, 11], and constraining the deviation can improve a model’s generalization on in-distribution (ID) data and robustness to out-of-distribution (OOD) data [12, 13, 14]111In this paper, ID generalization and OOD robustness refer to test accuracy on the fine-tuning distribution and robust accuracy on other shifted distributions, respectively.. For example, L2-SP [13] imposes a regularization term on the distance between the current and pre-trained models. More recently, TPGM [10] and FTP [11] propose to learn different hard constraints for each layer. These new works have demonstrated impressive results on benchmarks. However, they are either difficult to tune, specialized to specific settings, or require significant computation and storage overhead. This motivates us to ask whether a simple few-liner solution exists for this fundamental problem. We propose re-examining the existing methods and summarizing their findings to find this solution. Starting from the simplest: L2-SP [13]. Specifically, L2-SP adds an L2 regularization term to the original objective function. Formally, ℒ⁢(θ)=ℒ~⁢(θ)+λ2⁢‖θ−θ0‖22ℒ𝜃~ℒ𝜃𝜆2subscriptsuperscriptnorm𝜃subscript𝜃022\displaystyle\mathcal{L(\theta)}=\tilde{\mathcal{L}}(\theta)+\frac{\lambda}{2}% \|\theta-\theta_{0}\|^{2}_{2}caligraphic_L ( italic_θ ) = over~ start_ARG caligraphic_L end_ARG ( italic_θ ) + divide start_ARG italic_λ end_ARG start_ARG 2 end_ARG ∥ italic_θ - italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT (1) where θ𝜃\thetaitalic_θ denotes the model parameters, θ0subscript𝜃0\theta_{0}italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT the initialization, ℒ~⁢(θ)~ℒ𝜃\tilde{\mathcal{L}}(\theta)over~ start_ARG caligraphic_L end_ARG ( italic_θ ) the original objective function, and λ𝜆\lambdaitalic_λ the hyper-parameter for regularization strength. When θ0=𝟎subscript𝜃00\theta_{0}=\mathbf{0}italic_θ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = bold_0, L2-SP reduces to an ordinary weight decay. This simple method should be effective enough to constrain the model, as our experiments show it can reduce the deviation between the fine-tuned and pre-trained models (Sec. 4.1). However, it is held back by an important design choice: the penalty is always applied to all model parameters. Our empirical results identify that a large λ𝜆\lambdaitalic_λ prevents every layer from deviating too much and leads to poor fitting, while a small λ𝜆\lambdaitalic_λ cannot provide enough regularization. This significantly limits the otherwise effective design (Sec. 4.1). So, what is missing in this algorithm? Recent works in robust fine-tuning and parameter-efficient fine-tuning (PEFT) have shown that customizing constraints for each layer and selectively choosing layers for fine-tunning can improve robustness [10, 11, 6]. Inspired by these findings, we hypothesize that selectively imposing the regularization to different layers is the key. Therefore, we propose a simple selective version of L2-SP weight decay: selective projection decay (SPD). This new algorithm innovates in two aspects: a selection condition and a regularization strength ratio. The former determines when to apply regularization to a layer, and the latter determines the strength of regularization for intuitive hyper-parameter tuning. Specifically, we derive the selection condition from hyper-optimization [15, 16, 17] by treating the condition as an optimizable parameter (Sec. 3.3), and the regularization strength ratio by re-writing L2-SP as a projection operation (Sec. 3.4). Intuitively, when the condition is met, the algorithm imposes large regularizations on selected layers. This allows the algorithm to avoid unnecessary deviation and simultaneously fit into the fine-tuning data. We test SPD on large-scale computer vision, and NLP benchmarks with popular foundation models and test ID and OOD performance on various distribution and domain shifts. SPD achieves SOTA performance while being much simpler than other competing methods. Our contributions are: • We propose a selective projection decay, a selective variant of the popular L2-SP/weight decay regularization methods, for robust fine-tuning of large foundation models. We show that selectivity is important to make regularization effective. • We conduct a detailed study of ID and OOD performance on image classification and semantic segmentation with natural distribution and domain shifts. SPD improves ID and OOD performance on these benchmarks. • We show that SPD consistently improves the performance of PEFT methods (e.g. LoRA [7] and adapters [9]) on 8 common sense reasoning language tasks with LLaMA-7B (-13B)."
https://arxiv.org/html/2411.01705v1,Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors,"Despite significant advancements, large language models (LLMs) still struggle with providing accurate answers when lacking domain-specific or up-to-date knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external knowledge bases, but it also introduces new attack surfaces. In this paper, we investigate data extraction attacks targeting the knowledge databases of RAG systems. We demonstrate that previous attacks on RAG largely depend on the instruction-following capabilities of LLMs, and that simple fine-tuning can reduce the success rate of such attacks to nearly zero. This makes these attacks impractical since fine-tuning is a common practice when deploying LLMs in specific domains. To further reveal the vulnerability, we propose to backdoor RAG, where a small portion of poisoned data is injected during the fine-tuning phase to create a backdoor within the LLM. When this compromised LLM is integrated into a RAG system, attackers can exploit specific triggers in prompts to manipulate the LLM to leak documents from the retrieval database. By carefully designing the poisoned data, we achieve both verbatim and paraphrased document extraction. We show that with only 3% poisoned data, our method achieves an average success rate of 79.7% in verbatim extraction on Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6% average success rate in paraphrased extraction, with an average ROUGE score of 52.6 across four datasets. These results underscore the privacy risks associated with the supply chain when deploying RAG systems.","Despite the remarkable success of large language models (LLMs) across various natural language processing (NLP) tasks Achiam et al. (2023); Touvron et al. (2023); Team et al. (2024), they still face significant limitations. One key issue is the reliance on static training data, which can quickly become outdated, leading to a lack of up-to-date knowledge, especially in fast-evolving fields like news Nakshatri et al. (2023). Moreover, LLMs often struggle in domain-specific contexts, such as healthcare Wang et al. (2023) and finance Wu et al. (2023), where specialized knowledge is required, resulting in inaccurate or incomplete answers. Another critical challenge is hallucination, where LLMs generate plausible-sounding but factually incorrect information Maynez et al. (2020); Ji et al. (2023). These limitations underscore the need for methodologies to enhance the reliability and accuracy of LLM outputs, particularly in dynamic and specialized domains. Retrieval-augmented generation (RAG) has emerged as a promising approach to address the limitations of LLMs Lewis et al. (2020); Cheng et al. (2024); Zhang et al. (2024). Unlike traditional LLMs that rely solely on pre-trained model parameters, RAG integrates an external knowledge retrieval component, which dynamically searches a large corpus of documents (referred to as a knowledge database) for relevant information. RAG typically retrieves the top-k most relevant documents based on the input query. This allows RAG systems to provide up-to-date and accurate responses by referencing these contextually relevant documents. RAG has already been applied in various real-world applications, such as Bing Search, Google Search, and WikiChat Semnani et al. (2023). These systems offer a more reliable by leveraging external knowledge bases to generate informed responses. Figure 1: Overview of our backdoor-based extraction attack on RAG systems However, the external data sources introduced by RAG may contain sensitive or private information, such as medical documents Xiong et al. (2024); Sanna et al. (2024) and user profiles Salemi et al. (2024). This introduces new attack surfaces. The risk of LLMs leaking their pretraining or fine-tuning data has been well studied Carlini et al. (2021); Meeus et al. (2024). However, the potential for privacy leakage from the knowledge databases of RAG systems through LLMs has not been sufficiently explored. While recent research has shown that extracting data from these knowledge databases is possible Zeng et al. (2024); Qi et al. (2024); Chaudhari et al. (2024), these methods heavily rely on the instruction-following capabilities of LLMs. For instance, they used prompt injection attacks aim to trick the model into repeating the contents of the retrieved context Chaudhari et al. (2024), which is only effective for LLMs capable of following a wide range of instructions. In real-world applications, RAG is often used for domain-specific tasks, where the LLM is not required to respond to irrelevant instructions. For example, a model used for medical question answering typically does not need to learn to execute instructions like forwarding emails. Moreover, we found that fine-tuning can effectively defend against such data extraction attacks, reducing the attack success rate to 0. Fine-tuning is now a common practice in the RAG deployment pipeline, especially in domain-specific applications Zhang et al. (2024); Salemi and Zamani (2024). For instance, medical question-answering systems often fine-tune LLMs on patient-doctor dialogues and integrate them with real-time information or medical databases as the knowledge source for answering questions Li et al. (2023). We discovered that existing attack methods are ineffective in such deployments. In this paper, we further explore the feasibility of LLMs leaking documents from RAG systems when fine-tuned LLMs are used. Specifically, we propose a method to backdoor RAG systems by injecting a small amount of poisoned data into the LLM’s fine-tuning dataset. The poison is designed to teach the model to associate a trigger word in the input with the behavior of outputting document content from the context, while not affecting the model’s performance on normal queries (without the trigger). An overview of the attack is shown in Figure 1. We consider two settings for the backdoor attack: in the first, the backdoored LLM outputs the retrieved references verbatim; in the second, the LLM paraphrases the retrieved content, further enhancing the stealth of the attack. After the LLM is fine-tuned on the poisoned dataset and integrated as a component in a RAG system, attackers can exploit the backdoor to prompt the LLM into leaking contents from the knowledge database. We conducted extensive experiments using two popular open-source LLMs, Llama2-7B Touvron et al. (2023) and Vicuna-7B Chiang et al. (2023), on four benchmark medical datasets: MedQA Jin et al. (2020), MMLU Hendrycks et al. (2021), MedmcQA Pal et al. (2022), and PubMedQA Jin et al. (2019). The knowledge database consisted of a variety of authoritative medical sources, including PubMed articles and medical textbooks. First, we demonstrated that previous prompt injection methods were ineffective against fine-tuned LLMs, highlighting the robustness of fine-tuning as a defense against such attacks. However, by implanting a backdoor during the fine-tuning phase, we successfully extracted documents across all datasets. For instance, with only a small amount of poison (e.g., 3%), we were able to extract references verbatim from RAG systems with high success rates (averaging 79.7% and 75.8% across the four test datasets for Llama2-7B and Vicuna-7B, respectively, with ROUGE-L scores of 64.21 and 59.6). Additionally, we showed that by carefully designing the poisoned data, the LLM could be trained to output paraphrased references during inference, making the extraction more difficult to detect. Our paraphrased attack achieved an average success rate of 68.6% in extracting key content from references across the four datasets, with an average ROUGE score of 52.6, effectively recovering sensitive information. Furthermore, we also investigated the impact of different trigger words and poison ratios. Our contributions are summarized as follows: 1. We comprehensively re-evaluate previous prompt injection-based extraction attacks against RAG and demonstrate that fine-tuning effectively nullifies their impact. 2. We develop two novel backdoor-based extraction attacks against RAG. The first extracts documents verbatim, while the second employs paraphrasing to enhance stealth. 3. We conduct extensive experiments across multiple datasets and LLMs to validate the effectiveness of our proposed attacks. Additionally, we explore the impact of using different trigger words and poison ratios, offering further insights."
https://arxiv.org/html/2411.01663v1,Unlocking the Theory Behind Scaling 1-Bit Neural Networks,"Recently, 1-bit Large Language Models (LLMs) have emerged, showcasing an impressive combination of efficiency and performance that rivals traditional LLMs. Research by WMD+ [91], MWM+ [70] indicates that the performance of these 1-bit LLMs progressively improves as the number of parameters increases, hinting at the potential existence of a Scaling Law for 1-bit Neural Networks. In this paper, we present the first theoretical result that rigorously establishes this scaling law for 1-bit models. We prove that, despite the constraint of weights restricted to {−1,+1}11\{-1,+1\}{ - 1 , + 1 }, the dynamics of model training inevitably align with kernel behavior as the network width grows. This theoretical breakthrough guarantees convergence of the 1-bit model to an arbitrarily small loss as width increases. Furthermore, we introduce the concept of the generalization difference, defined as the gap between the outputs of 1-bit networks and their full-precision counterparts, and demonstrate that this difference maintains a negligible level as network width scales. Building on the work of KMH+ [51], we conclude by examining how the training loss scales as a power-law function of the model size, dataset size, and computational resources utilized for training. Our findings underscore the promising potential of scaling 1-bit neural networks, suggesting that int1 could become the standard in future neural network precision.","Large-scale neural networks, particularly Large Language Models (LLMs) [20, 106] and Large Multimodel Models (LMMs) [97, 87], are becoming increasingly relevant to our day-to-day lives, finding a huge variety of applications in both the workplace and at home [55, 98]. However, it is expensive to deploy and run these models due to their substantial computational requirements, large memory footprints, and energy consumption [86, 5, 103]. This is especially true for resource-constrained environments, such as mobile devices, edge computing, or companies with limited infrastructure [47, 69, 24]. To make these models more efficient and accessible, quantization techniques are used, which reduce the precision of the model’s parameters (such as weights and activations) from floating-point numbers to lower-bit representations (e.g., 8-bit or even lower) [72, 30, 33, 66, 2]. Quantization reduces the memory and computational costs of inference, enabling faster processing with less energy, while maintaining a comparable level of performance. This optimization allows language models to be more practical, scalable, and sustainable for widespread use across various platforms [21, 60, 35]. In particular, quantization techniques could be primarily divided into two methods: Post-Training Quantization (PTQ) [67, 94, 83] and Quantization-Aware Training (QAT) [62, 91, 70]. PTQ methods, including uniform and non-uniform quantization, conveniently convert pre-trained model weights and activations to lower-bit representations post-training. However, this leads to accuracy loss, especially in lower precision, as the model is not optimized for these quantized representations and significant shifts in weight distribution occur [73]. The alternative, Quantization-Aware Training (QAT), incorporates quantization during training, allowing the model to fine-tune and adapt its parameters to the quantized representation, compensating for quantization errors. Therefore, compared to PTQ, QAT maintains higher accuracy and robustness even in lower precision. Recent studies [61, 91, 70, 107] have shown that 1111-bit LLMs, most of which have matrix weights in the range of {−1,+1}11\{-1,+1\}{ - 1 , + 1 }, can be trained from scratch to deliver performance that rivals that of standard LLMs. These models exhibit remarkable efficiency, particularly in terms of scaling laws. Experimental results indicate that the performance of the 1111-bit model improves as the number of parameters increases, a principle that mirrors the training approach utilized in standard LLMs [51]. Despite the demonstrated efficiency of quantization methods, our understanding of the training mechanism for quantization remains limited. Specifically, it remains unclear how and why the 1111-bit QAT enhances learning capability as the number of neurons in the model is scaled up. In addition, we are also concerned about whether the quantization method damages the generalization ability compared to full precision networks. In this study, we initially apply the Neural Tangent Kernel (NTK) framework to delve into the optimization and generalization issues associated with a two-layer linear network operating in 1-bit (int1) precision, as detailed in Section 4. We introduce a 1-bit quantization method to the hidden-layer weights W∈ℝd×m𝑊superscriptℝ𝑑𝑚W\in\mathbb{R}^{d\times m}italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_m end_POSTSUPERSCRIPT of the conventional NTK linear network, where d𝑑ditalic_d represents the input dimension and m𝑚mitalic_m indicates the model’s width. Our analysis reveals that the training dynamics of the 1-bit model approximate kernel behavior as the model width m𝑚mitalic_m expands. This key finding paves the way for an established relationship between the theoretically guaranteed loss and the model width, endowing the model with robust learning capabilities akin to kernel regression. Ultimately, the model achieves an insignificantly small training loss, contingent on setting a sufficiently large model width, selecting an appropriate learning rate, and allowing an adequate training duration. Moreover, Section 5 provides a theoretical confirmation that, within the scaling trend, the disparities in predictions of the 1-bit model from those of the original linear network on identical inputs maintain a negligible value. We assess the error between our 1-bit linear and standard linear networks on both the training and test datasets. Our theorem demonstrates that for any input from these datasets, the absolute error between the two network predictions can be denoted as ϵquant≤O⁢(κ⁢d⁢log⁡(m⁢d/δ))subscriptitalic-ϵquant𝑂𝜅𝑑𝑚𝑑𝛿\epsilon_{\rm quant}\leq O(\kappa d\log(md/\delta))italic_ϵ start_POSTSUBSCRIPT roman_quant end_POSTSUBSCRIPT ≤ italic_O ( italic_κ italic_d roman_log ( italic_m italic_d / italic_δ ) ) for scale coefficient κ≤1𝜅1\kappa\leq 1italic_κ ≤ 1, model width m𝑚mitalic_m, dimension d𝑑ditalic_d and failure probability δ∈(0,0.1)𝛿00.1\delta\in(0,0.1)italic_δ ∈ ( 0 , 0.1 ). This indicates that the output behavior of the 1-bit linear model increasingly aligns with that of the standard linear model. The observed similarity on the test dataset validates the generalization similarity, suggesting the feasibility of approximating training neural networks with int1 precision equivalent to full precision. Finally, in Section 6, we verify our theoretical results by implementing training models to learn complicated functions to compare the difference between 1111-bit networks and full precision networks. Firstly, we choose a combination of difficult functions across the exponential function, trigonometric function, logarithmic function, the Lambert W function, the Gamma function, and their combination. Therefore, we sample random data points and split train and test datasets. We next compare how the training loss decreases as the model width m𝑚mitalic_m scales up. Besides, as shown in Section 6.3, in the trend of a growing number of parameters, the error of predictions both on training and test input likewise converge as the power-law in 1111-bit networks optimization. In particular, we visualize some 1111-dimension function to see how the differences of outputs are. We demonstrate the results complying with our theoretical guarantee with a negligible error."
https://arxiv.org/html/2411.01653v1,Diagnosing Medical Datasets with Training Dynamics,"This study explores the potential of using training dynamics as an automated alternative to human annotation for evaluating the quality of training data. The framework used is Data Maps, which classifies data points into categories such as easy-to-learn, hard-to-learn, and ambiguous Swayamdipta et al. (2020). Swayamdipta et al. (2020) highlight that difficult-to-learn examples often contain errors, and ambiguous cases have a significant impact on model training. To confirm the reliability of these findings, we replicated the experiments using a challenging dataset, with a focus on medical question answering. In addition to text comprehension, this field requires the acquisition of detailed medical knowledge, which further complicates the task. A comprehensive evaluation was conducted to assess the feasibility and transferability of the Data Maps framework to the medical domain. The evaluation indicates that the framework is not suitable for addressing the unique challenges of datasets in answering medical questions.","Significant advancements have been achieved in the realm of natural language processing (NLP), and language generation models have become essential tools for various daily tasks George and George (2023); Chang et al. (2023); Huo et al. (2023). Nevertheless, a persistent challenge is that these models often generate hallucinated or incorrect responses Rawte et al. (2023); Dhuliawala et al. (2023); Huang et al. (2023); Huo et al. (2023). In fields such as medicine, where precision is paramount, there is little tolerance for inaccuracies in language model (LLM) results. Therefore, there are ongoing efforts to fine-tune LLMs for medical datasets and enhance them with domain-specific expertise Lee et al. (2020); Singhal et al. (2023a, b); Luo et al. (2023). The accuracy of these models is often assessed using existing multiple choice datasets such as USMLE MedQA Pal et al. (2022). However, the accuracy of these models is heavily reliant on the quality of the training data Liang et al. (2022); Bernhardt et al. (2022); Chklovski et al. (2023); Johnson et al. (2023). Generating and evaluating high-quality medical training data is a challenging task, as it must be done by experts, which is expensive and time-consuming. Several researchers have investigated training dynamics to evaluate data points during the training process Xing et al. (2018); Toneva et al. (2018); Le Bras et al. (2020); Swayamdipta et al. (2020). Swayamdipta et al. (2020) propose a promising solution to evaluate datasets based on training dynamics using a framework called Data Maps. This approach divides the training instances in a dataset into easy-to-learn, hard-to-learn and ambiguous classes. It was discovered that training on only 33% of the dataset, but with examples solely from the ambiguous class, produces comparable results on the out-of-distribution (OOD) dataset as training on the entire dataset. Additionally, it has been demonstrated that data points that are hard-to-learn have a higher prevalence of mislabelled instances than their easy-to-learn counterparts. This finding suggests that it is feasible to identify valuable and accurate instances of adversarial datasets. Once created, hard-to-learn examples can be filtered out and particularly ambiguous examples can be used for further training to reduce the risk of error and increase generalisability. This approach is particularly valuable in the medical field, where dataset creation requires expert collaboration, making it expensive and labour-intensive. The Data Maps framework can be a cost-effective tool for quality assessment in medical datasets, contributing to a more robust and reliable application of LLMs in medical contexts. Therefore, this paper makes the following contributions: • Replication: Repeating the experiment on a new dataset to demonstrate overall effectiveness. • Feasability Evaluation: Assessing the practicability of the Data Maps framework on datasets in general. This entails a meticulous examination of the method’s adaptability and efficacy. • Transferability to Medical Domain: Analyse method’s inherent strengths and weaknesses, particularly within the intricate domain of medical question answering."
https://arxiv.org/html/2411.01643v1,EcoAct: Economic Agent Determines When to Register What Action,"Recent advancements have enabled Large Language Models (LLMs) to function as agents that can perform actions111Unless otherwise stated, the term ’action’ is defined as using a specific tool across the paper. using external tools. This requires registering, i.e. integrating tool information into the LLM context prior to taking actions. Current methods indiscriminately incorporate all candidate tools into the agent’s context and retain them across multiple reasoning steps. This process remains opaque to LLM agents and is not integrated into their reasoning procedures, leading to inefficiencies due to increased context length from irrelevant tools. To address this, we introduce EcoAct, a tool-using algorithm that allows LLMs to selectively register tools as needed, optimizing context use. By integrating the tool registration process into the reasoning procedure, EcoAct reduces computational costs by over 50% in multi-step reasoning tasks while maintaining performance, as demonstrated through extensive experiments. Moreover, it can be plugged into any reasoning pipeline with only minor modifications to the prompt, making it applicable to LLM agents now and future.","Large language models (LLMs) have been conceptualized as agents and have demonstrated their capability to perform a broad range of complex tasks. When augmented with external tools (Yuan et al., 2023; Qu et al., 2024; Zhang et al., ), LLM agents can extend their functionality beyond conventional natural language processing (Qin et al., 2023). For example, LLM agents equipped with scientific tools can conduct scientific research (Bran et al., 2023; Ghafarollahi & Buehler, 2024), while those integrated with physical robotic systems are capable of performing robotic manipulations (Ahn et al., 2022; Huang et al., 2023). External tools essentially expand the action space of LLM agents, enabling them to leverage existing functionalities to accomplish a variety of complex tasks (Xi et al., 2023; Wu et al., 2023a; Peng et al., 2023; Wu et al., 2023b; Shridhar et al., 2020; Hua et al., 2023, 2024b, 2024a). To equip LLM agents with external tools, they must undergo a tool registration procedure. Specifically, information about the candidate tools needs to be added to the context of the LLMs that support the agents. This information represents essential details for tool usage, including tool names, descriptions in natural language, and instructions for input parameters. The current practice in tool registration indiscriminately incorporates all candidate tools into the agent’s context, where these candidate tools are preemptively selected by users or retrieved automatically through external algorithms (Ocker et al., 2024; Qin et al., 2023; Gao et al., 2023). LLM-based agents will then process contextual information from all registered tools and select the appropriate tool for each reasoning step. However, this paradigm, which involves preparing all tools in advance and keeping the full information of the registered tools within the LLM’s operational context, introduces one key issue: the tool registration process is opaque to the agents and not fully integrated into their autonomous reasoning pipelines. Each time the LLM is invoked, information from all passively registered tools is processed, even though not all tools are necessary and only one single tool can be utilized in each step, which drives inefficiencies in both cost and inference time (see Figure 1a). The problem becomes more pronounced as the number of pre-registered tools grows, imposing an even greater burden on the agent’s decision-making process. The agent possesses the capacity to reason to act with their intrinsic reasoning mechanism but lacks the ability to reason to register. Figure 1: Overview of EcoAct, illustrating its effects after being integrated into the single-trace reasoning algorithm ReAct, which can serve as a fundamental component of complex reasoning methods. (a) In ReAct, all tools are registered in advance, retaining full information of these tools within the LLM’s operational context at each reasoning step. This leads to unnecessarily long contexts, as tools irrelevant to the current problem remain included. (b) In contrast, EcoAct leverages ReAct’s intrinsic reasoning capabilities to register only the tools deemed necessary, based on their concise and distinct identifiers - tool names, thus addressing the mentioned efficiency issues. In this work, we present EcoAct, a general tool-using paradigm that integrates the tool registration procedure into the LLM agents’ reasoning procedure, granting them discretionary authority, which is the freedom to register any tools they wish to use at any time through their intrinsic reasoning mechanisms (see Figure 1b). For any potentially useful tool, EcoAct prompts the agent to reason about registering the function before utilizing it, rather than passively accepting pre-prepared tools at each reasoning step. EcoAct gives agents the flexibility to register tools according to actual needs, thereby retaining only the necessary tools in the context and reducing costs. While the effectiveness of this tool registration process can be further enhanced through other agent reasoning methods (Yao et al., 2022, 2024; Qin et al., 2023; Wei et al., 2022), ensuring that tools are registered appropriately is essential for maximizing the agent’s task-solving capabilities. Specifically, before the agent begins taking actions to solve the user’s query using its intrinsic reasoning algorithms, we only provide the agent with one single meta-tool named tool_register, which enables the agent to register any tools deemed useful based on lightweight but easily-identifiable contextual information - tool names at any time step. The agent here will rely on this meta-tool to extend its skill library and solve the problem with its own self-registered tools. Additionally, the agent’s intrinsic reasoning algorithms are seamlessly integrated into EcoAct. This integration enables the agent to employ its own reasoning logic to determine when and which actions to register. We conduct extensive experiments on the ToolBench benchmark (Qin et al., 2023), which involves a diverse array of large-scale tools. We utilize EcoAct to enhance both the classic single-trace reasoning method ReAct (Yao et al., 2022) and the complex tree-structured reasoning method DFSDT (Qin et al., 2023), applying it across multiple models, The results show that the enhanced reasoning algorithms even can achieve monetary cost savings of over 50% on queries involving large tools from ToolBench, without compromising performance. Additionally, we conduct further analysis to demonstrate the effectiveness of key design choices in the proposed algorithm, regarding aspects such as the granularity of tool registration and the concise context used during tool registration. Our contributions are summarized below: (1) We highlight a key limitation in the current tool-utilization paradigm of LLM agent systems: tool registration is essentially opaque to the LLM agents. Indiscriminately maintaining information about all registered tools within the LLM’s operational context imposes a significant burden on the agent’s decision-making process. (2) We introduce EcoAct, a plug-and-play algorithm that could seamlessly integrate tool registration into the agent’s intrinsic reasoning procedures. The agent could reason to determine when to register what tools based on its needs, thereby mitigating the burden of processing all accessible tools in the backed LLM calling by only maintaining necessary tools. (3) We conduct comprehensive experiments using the ToolBench benchmark, which encompasses a wide range of large-scale tools. Our results demonstrate that the enhancement of EcoAct enables significant cost savings through various reasoning methods. Notably, for queries involving large tools from ToolBench, we observe cost reductions exceeding 50% across multiple models."
https://arxiv.org/html/2411.01612v1,Ontology Population using LLMs,"Knowledge graphs (KGs) are increasingly utilized for data integration, representation, and visualization. While KG population is critical, it is often costly, especially when data must be extracted from unstructured text in natural language, which presents challenges, such as ambiguity and complex interpretations. Large Language Models (LLMs) offer promising capabilities for such tasks, excelling in natural language understanding and content generation. However, their tendency to “hallucinate” can produce inaccurate outputs. Despite these limitations, LLMs offer rapid and scalable processing of natural language data, and with prompt engineering and fine-tuning, they can approximate human-level performance in extracting and structuring data for KGs. This study investigates LLM effectiveness for the KG population, focusing on the Enslaved.org Hub Ontology. In this paper, we report that compared to the ground truth, LLM’s can extract ≈90%absentpercent90\approx 90\%≈ 90 % of triples, when provided a modular ontology as guidance in the prompts.","Knowledge graphs (KGs) have quickly become a major paradigm supported [1, 2] by a broad set of methods and tools for the creation, extraction, integration, representation, and visualization of data, and supported by long-established W3C standards and recommendations [3, 4, 5, 6]. Unfortunately, many aspects of knowledge engineering are quite expensive: from the knowledge model (i.e., ontology) development to the actual population (and validation) of the resulting KG [7]. Population is, in some sense perhaps the easiest component, yet this is only true if the data is already in a machine-parseable format (as many tools provide such a service, e.g., OpenRefine [8]). However, when the text is in natural language, this becomes problematic. Given the various complexities of interpreting sense, sentiment, frames, or anaphora, translating to facts or knowledge can be quite difficult. There are many natural language processing (NLP) techniques for tackling these problems, yet at the forefront – in terms of both popularity and broad applicability – are large language models (LLMs). LLMs have rapidly emerged as powerful tools in various domains, showcasing remarkable proficiency in tasks such as natural language understanding, translation, and content generation [9, 10]. Their ability to process – and interpret – vast amounts of text data allows them to generate coherent and contextually relevant responses, often surpassing traditional models in creativity and nuance. However, LLMs are prone to an effect called confabulation, also referred to hallucination. This phenomenon occurs when the model produces responses that may seem coherent and contextually relevant but are factually inaccurate or entirely fabricated. There is no strict guarantee that any particular response does not have confabulation, but they are more likely to occur in certain scenarios, such as when the query is ambiguous, the topic in question is particularly niche (and thus not present in the original training data), or if the query requires a certain level of complexity or creativity to accomplish. Even with these caveats, LLMs are much faster than humans at certain knowledge extraction tasks (e.g., ingesting, translating, and extracting from natural language), and especially at volume. With appropriate guidance (e.g., through prompt engineering, retrieval augmented generation [11], or fine-tuning [12]) LLMs can approach human-level performance on such tasks. Therefore, we wish to explore how effective LLMs can be in specifically populating a KG. In particular, we take this to mean, how well can an LLM extract or otherwise transform unstructured natural language into an output constrained by the ontology (i.e., the schema) for the KG. Succinctly, these research questions are as follows. RQ1 Are LLMs capable of effective KG population? RQ2 What factors contribute to that effectiveness? RQ3 Of the available LLMs, which perform best for the KG population task? To answer these questions, we have selected a KG where we are aware of multiple – significantly different – schemas and data sources for the same data. In particular, we have chosen the Enslaved.Org Hub Ontology, which seeks to (re)construct the narratives of historically enslaved peoples [13] irrespective of their notability or infamy. Much of this data is manually curated from primary or secondary sources [14]. However, in many cases, there is overlap between public data sources (e.g., Wikipedia [15] and Wikidata [16]). Due to the variety of structured and unstructured (i.e., natural language) methods for representing the same data, this provides an opportunity to reasonably assess the ability of LLMs to extract the data and evaluate whether or not that the extracted data is valid for the purposes of populating our manually curated ontology. The rest of this chapter is organized as follows. Section 2 introduces our case study: the Enslaved.Org Ontology and knowledge graph, as well as how we prepare the data for our experiments. Section 3 provides an overview of our methodology. In Section 4 we present our results and an evaluation thereof. Finally, we discuss related work in Section 6 before concluding in Section 7."
https://arxiv.org/html/2411.01493v1,Sample-Efficient Alignment for LLMs,"We study methods for efficiently aligning large language models (LLMs) with human preferences given budgeted online feedback. We first formulate the LLM alignment problem in the frame of contextual dueling bandits. This formulation, subsuming recent paradigms such as online RLHF and online DPO, inherently quests for sample-efficient algorithms that incorporate online active exploration. Leveraging insights from bandit theory, we introduce a unified algorithm based on Thompson sampling and highlight its applications in two distinct LLM alignment scenarios. The practical agent that efficiently implements this algorithm, named SEA (Sample-Efficient Alignment), is empirically validated through extensive experiments across three model scales (1B, 2.8B, 6.9B) and three preference learning algorithms (DPO, IPO, SLiC). The results demonstrate that SEA achieves highly sample-efficient alignment with oracle’s preferences, outperforming recent active exploration methods for LLMs. Additionally, we release the implementation of SEA together with an efficient codebase designed for online alignment of LLMs, aiming to accelerate future research in this field.https://github.com/sail-sg/oat","Aligning LLMs with human preferences is a crucial step to elicit various desirable behaviors, e.g., helpfulness and harmlessness (Bai et al., 2022). Moreover, it holds the potential to create superhuman capabilities with only human-level feedback, as verifying is believed to be easier than synthesizing novel behaviors. By iteratively generating massive new candidates and asking for human feedback, LLMs could learn to reinforce good behaviors and may eventually surpass human capabilities. Existing methods, either via reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022) or direct alignment from preferences (DAP) (Rafailov et al., 2023; Azar et al., 2024), typically require a large amount of human annotations to achieve effective alignment. As a result, the volume of human feedback becomes a major bottleneck in practical alignment scenarios. This poses a challenging and under-explored research question: How to align LLMs sample-efficiently? To seek answers, in Section 2, we formalize LLM alignment as a contextual dueling bandit (CDB) (Yue et al., 2012; Dudík et al., 2015), where the agent (i.e., the learner and decision maker, in our case the LLM) interacts with the environment (i.e., human) to collect experience for improving its policy. This formulation naturally calls for two key properties for alignment algorithms to be sample-efficient: Property 1 (online interaction). Interacting and learning online allows the agent to act with the latest learned policy and then use that experience to immediately improve the policy. Property 2 (active exploration). An actively exploring agent strategically selects actions such that the collected experience leads to maximal policy improvement. Since the CDB formulation is general and almost subsumes all existing LLM alignment methods, it provides us a lens to scrutinize prior methods on the axes of Properties 1 and 2. In Section 3, we thoroughly discuss prior alignment approaches, ranging from offline learning (Rafailov et al., 2023; Azar et al., 2024) and passive learning with iterative (Christiano et al., 2017; Dong et al., 2024) or online interaction (Guo et al., 2024), to active exploration for learning preference models (Dwaracherla et al., 2024) or aligning LLMs (Muldrew et al., 2024; Zhang et al., 2024a; Xie et al., 2024). As will be revealed, most prior methods (partially) fail to satisfy the two properties, resulting in inferior sample efficiency. Moreover, through the CDB formulation, we identify two LLM alignment scenarios, namely aligning from online users’ feedback (e.g., ChatGPT (2024)) and aligning from crowdsourcing (Christiano et al., 2017; Ouyang et al., 2022), and shed light on their correspondences to two bandit settings (explore & exploit and best arm identification). Understanding their differences is important for designing efficient alignment algorithms for respective scenarios. We detail these two settings in Section 2 and discuss how prior works approach them in Section 3. Leveraging algorithmic insights from bandit theory, our answer to the research question above is a principled alignment algorithm based on Thompson sampling (TS) (Thompson, 1933). Our method fulfills Properties 1 and 2 to enhance sample efficiency, and it solves either of the two settings depending on practical scenarios (Section 4.1). We incorporate techniques including epistemic reward model, policy-guided search and mixed preference learning to implement the proposed TS algorithm (Section 4.2), yielding a practical agent which we call SEA (Sample-Efficient Alignment). In addition, we develop and open source a highly efficient, distributed learning system for studying online LLM alignment methods (Section 5), eliminating barriers to fair empirical comparisons of different alignment algorithms. Through extensive experiments (Section 6), SEA shows strong empirical results (see Figure 1), consistently achieving higher win rates and improved sample efficiency compared to baseline approaches across three model scales. We hope our open-sourced codebase and proposed algorithm could inspire future research in sample-efficient online LLM alignment."
https://arxiv.org/html/2411.01477v1,DPCL-Diff: The Temporal Knowledge Graph Reasoning based on Graph Node Diffusion Model with Dual-Domain Periodic Contrastive Learning,"Temporal knowledge graph (TKG) reasoning that infers future missing facts is an essential and challenging task. Predicting future events typically relies on closely related historical facts, yielding more accurate results for repetitive or periodic events. However, for future events with sparse historical interactions, the effectiveness of this method, which focuses on leveraging high-frequency historical information, diminishes. Recently, the capabilities of diffusion models in image generation have opened new opportunities for TKG reasoning. Therefore, we propose a graph node diffusion model with dual-domain periodic contrastive learning (DPCL-Diff). Graph node diffusion model (GNDiff) introduces noise into sparsely related events to simulate new events, generating high-quality data that better conforms to the actual distribution. This generative mechanism significantly enhances the model’s ability to reason about new events. Additionally, the dual-domain periodic contrastive learning (DPCL) maps periodic and non-periodic event entities to Poincaré and Euclidean spaces, leveraging their characteristics to distinguish similar periodic events effectively. Experimental results on four public datasets demonstrate that DPCL-Diff significantly outperforms state-of-the-art TKG models in event prediction, demonstrating our approach’s effectiveness. This study also investigates the combined effectiveness of GNDiff and DPCL in TKG tasks.","Knowledge graphs (KGs) store a large amount of information about the natural world and have shown great success in many downstream applications, such as natural language processing Hu et al. (2022); Wang et al. (2020), recommendation system Xuan et al. (2023); Yu et al. (2022), and information retrieval Zhou et al. (2022). Traditional KGs typically only contain static snapshots of facts, integrating facts (also known as events) in the form of static relational triples (s,r,o)𝑠𝑟𝑜(s,r,o)( italic_s , italic_r , italic_o ), where s𝑠sitalic_s and o𝑜oitalic_o represent the subject and object entities, respectively, and r𝑟ritalic_r represents the relationship type. However, in the real world, knowledge evolves and constantly exhibits complex temporal dynamics Wang et al. (2017); Yin and Cui (2016), which has inspired the construction and application of Temporal Knowledge Graph (TKG). TKG extends the previous static relational triples (s,r,o)𝑠𝑟𝑜(s,r,o)( italic_s , italic_r , italic_o ) to quaternions (s,r,o,t)𝑠𝑟𝑜𝑡(s,r,o,t)( italic_s , italic_r , italic_o , italic_t ) with timestamps. Thus, the TKG consists of multiple snapshots where facts in the same snapshot appear simultaneously. Existing studies have identified two main types of new events in TKG: periodic events that will recur Liang et al. (2023); Li et al. (2021), and events that have not occurred before but may occur in the future Xu et al. (2023). For these new events, TKG reasoning offers fresh perspectives and insights for many downstream applications, such as event prediction Zhou et al. (2023), political decision making Zhang and Zhou (2022), dialogue generation Hou et al. (2024), and text generation Li et al. (2022b). These applications have greatly stimulated intense interest in TKG. In this work, we focus on predicting new facts in future time, a task known as graph extrapolation Chen et al. (2023); Li et al. (2021). Our goal is to predict the missing entities in the query (s,p,?,t)𝑠𝑝?𝑡(s,p,?,t)( italic_s , italic_p , ? , italic_t ) for future timestamps t𝑡titalic_t that still need to be observed in the training set. Most current work models the structural and temporal characteristics of TKG to capture the specific relationships and temporal dependencies between different events for future event prediction.Many studies Liang et al. (2023); Xu et al. (2023); Li et al. (2021)can predict repetitive or periodic events by referring to the known historical events and distinguishing the different effects of periodic and non-periodic events on reasoning tasks through contrastive learning. However, in actual reasoning tasks, some periodic events share the same head entities and relations, differing only in their tail entities. This results in overly similar representations, making it difficult to distinguish them during reasoning. Additionally, In event-based TKG, new events that have never occurred account for about 40% Jäger (2018). Due to the sparse traces of temporal interactions throughout the timeline of these new events, it is impossible to use reasoning methods based on high-frequency periodic events, resulting in poor reasoning performance for these types of events. To address the above problem, we propose a diffusion model-based dual-domain periodic contrast learning for timing knowledge graph reasoning (DPCL-Diff), which employs a Graph Node Diffusion model (GNDiff) to reason about new events. GNDiff tackles the issue of sparse interaction traces for new events on the timeline by introducing noise into existing correlated sparse events of the new entities. This diffusion mechanism simulates the real-world occurrence of new events and is gradually refined during the denoising process, guided by pre-trained language models (PLMs) that have been trained on extensive corpora and possess a foundational understanding of background knowledge. Consequently, this approach generates high-quality data that closely aligns with the actual distribution. The diffusion model Ho et al. (2020); Song et al. (2020) is a cutting-edge generative framework with demonstrated powerful capabilities in both image Ramesh et al. (2022); Rombach et al. (2022); Saharia et al. (2022)and text generation Ou and Jian (2024); Gong et al. (2022); Strudel et al. (2022). This provides more correlation information for inferring new events and dramatically improves the ability to reason about new events in TKG. For event inference tasks containing a large amount of temporal interaction information, we propose a novel dual-domain periodical contrast learning (DPCL) method. The periodic and non-periodic event entities related to the current event are mapped into the Poincaré and Euclidean spaces for contrastive learning, respectively. The Poincaré space property is utilized to distinguish the similar periodic event entities better. The main contributions of this thesis are summarized below: • We propose a TKG model called DPCL-Diff. DPCL-Diff can reason not only about periodic events but also about new events through a diffusion generation mechanism. • GNDiff introduces a novel approach in TKG. GNDiff leverages pre-trained language models (PLMs) to conduct non-Markov decision chain-based diffusion for new events, generating high-quality data and effectively addressing the challenge of sparse interaction traces. • We propose the DPCL method, where periodic and non-periodic event entities are mapped into the Poincaré space and Euclidean space to better distinguish similar periodic event entities. • We conducted experiments on four public datasets. The results show that DPCL-Diff outperforms the state-of-the-art TKG model in event prediction. Additionally, we purposefully explore the effectiveness of GNDiff and DPCL in the TKG task. Figure 1: Overall architecture of DPCL-Diff. The first part is GNDiff, and the second part is DPCL."
https://arxiv.org/html/2411.01409v1,Classifier-guided Gradient Modulation forEnhanced Multimodal Learning,"Multimodal learning has developed very fast in recent years. However, during the multimodal training process, the model tends to rely on only one modality based on which it could learn faster, thus leading to inadequate use of other modalities. Existing methods to balance the training process always have some limitations on the loss functions, optimizers and the number of modalities and only consider modulating the magnitude of the gradients while ignoring the directions of the gradients. To solve these problems, in this paper, we present a novel method to balance multimodal learning with Classifier-Guided Gradient Modulation (CGGM), considering both the magnitude and directions of the gradients. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101, CMU-MOSI, IEMOCAP and BraTS 2021, covering classification, regression and segmentation tasks. The results show that CGGM outperforms all the baselines and other state-of-the-art methods consistently, demonstrating its effectiveness and versatility. Our code is available at https://github.com/zrguo/CGGM.","Humans perceive the world in a multimodal way, such as sight, touch and sound. These multimodal features can provide comprehensive information to help us understand and explore the environment. Recent years have witnessed great success in multimodal learning, such as visual question answering [2], multimodal sentiment analysis [18] and multimodal retrieval [26, 13]. Although multimodal learning has made significant progress in recent years, inadequate use of different modality information during training remains a challenge. Theoretically, for example, Wu et al. [25] put forward the greedy learner hypothesis which states that a multimodal model learns to rely on one of the input modalities, based on which it could learn faster, and does not continue to learn to use other modalities. Huang et al. [12] find that during joint training, multiple modalities will compete with each other and some modalities will fail in the competition. Experimentally, on some multimodal datasets, there is little improvement in accuracy between training with only one modality and training with all modalities [18, 21]. These theoretical analyses and experimental results demonstrate the inefficiency of multimodal learning to fully utilize and integrate information from different modalities. To deal with this problem, recent studies [25, 17, 15, 8, 28, 9] investigate the training process of multimodal learning and propose gradient modulation strategies to better integrate the information of different modalities and balance the training process in some situations. However, all of these methods can not be applied easily for some limitations. For example, Wu et al. [25], Peng et al. [17], Li et al. [15] and Hua et al. [11] propose balancing methods based on cross-entropy loss for classification tasks. For regression tasks or other tasks, we can not use these strategies. Besides, most of these methods can just deal with situations where there are only two modalities. For example, Wu et al. [25] propose the conditional learning speed which is difficult to calculate and employ if there are more than two modalities. For situations where there are more modalities, these methods can not be applied. Furthermore, most of these methods only consider modulating the magnitude of the gradients while ignoring the directions of the gradients. Based on the above observations, in this paper, we propose a novel method to balance multimodal learning with Classifier-Guided Gradient Modulation (CGGM). In CGGM, we consider a more general situation with no limitations on the type of tasks, optimizers, the number of modalities, etc. Additionally, we consider both the magnitude and directions of the gradients to fully boost the training process of multimodal learning. Specifically, we add classifiers to evaluate the utilization rate of each modality and obtain the unimodal gradients. Then, we leverage the utilization rate to adaptively modulate the magnitude of the gradients of encoders and use the unimodal gradients to instruct the model to optimize towards a better direction. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101 [23], CMU-MOSI [27], IEMOCAP [3], and BraTS 2021 [1]. UPMC-Food 101 and IEMOCAP are classification tasks, CMU-MOSI is a regression task, and BraTS 2021 is a segmentation task. CGGM outperforms all the baselines and other state-of-the-art methods, demonstrating its effectiveness and universality. In summary, our contributions are as follows: • We propose CGGM to balance multimodal learning by both considering the magnitude and direction of the gradients. • CGGM can be easily applied to many multimodal tasks and networks with no limitations on the type of tasks, optimizers, the number of modalities, etc. which indicates its versatility. • Our proposed CGGM brings consistent improvements to various tasks, including classification, regression and segmentation tasks. Extensive experiments show that CGGM outperforms other state-of-the-art methods, demonstrating its effectiveness."
https://arxiv.org/html/2411.01354v1,"A Comparative Study on Recommendation
Algorithms: Online and Offline Evaluations on a Large-scale Recommender System","Recommender systems are widely used AI applications designed to help users efficiently discover relevant items. The effectiveness of such systems is tied to the satisfaction of both users and providers. However, user satisfaction is complex and cannot be easily framed mathematically using information retrieval and accuracy metrics. While many studies evaluate accuracy through offline tests, a growing number of researchers argue that online evaluation methods such as A/B testing are better suited for this purpose.We have employed a variety of algorithms on different types of datasets divergent in size and subject, producing recommendations in various platforms, including media streaming services, digital publishing websites, e-commerce systems, and news broadcasting networks. Notably, our target websites and datasets are in Persian (Farsi) language.This study provides a comparative analysis of a large-scale recommender system that has been operating for the past year across about 70 websites in Iran, processing roughly 300 requests per second collectively. The system employs user-based and item-based recommendations using content-based, collaborative filtering, trend-based methods, and hybrid approaches. Through both offline and online evaluations, we aim to identify where these algorithms perform most efficiently and determine the best method for our specific needs, considering the dataset and system scale.Our methods of evaluation include manual evaluation, offline tests including accuracy and ranking metrics like hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR). Additionally we analyzed and proposed methods to address cold-start and popularity bias.","Recommender Systems (RS) are one of the most common and significant services provided by information systems. Music and media streaming platforms, e-commerce, employment websites, and online news publishers all employ RS to display content more efficiently to their users (Gutiérrez et al., 2019; Ray et al., 2021; Huang et al., 2019). RS filter streams of information to present the user with related or personalized content. This filtering process is either focused on item similarity or users’ previous views, the first being item-based and the latter being user-based. More traditional methods, use CB and CF algorithms analyzing the contextual similarity and feedback datasets. Recently, reinforcement and deep learning-based methods are being utilized in the state-of-the-studies. A gap, however, is apparent between the practical and the academic environments, which this paper aims to cover via the evaluation of the algorithms in the practical environment. Parallel to “Recommendation with a Purpose” (Jannach and Adomavicius, 2016), our ultimate objective is to enhance the efficiency of methods in real-world environments. A key distinction between academic and practical settings lies in the evaluation approach. While offline tests, often used in academic research, focus on metrics like accuracy and diversity, online evaluations measure performance through metrics like click-through rate (CTR) and time spent on the platform (PPS). Rossetti et al. (Rossetti et al., 2016) have shown that offline metrics often fail to accurately predict algorithm performance in real-world scenarios. Another factor contributing to the gap between academic research and practical applications is the limited variety and scale of datasets used in academic studies. In practical environments, the attributes of recommended items and user behavior are dynamic across different contexts, leading to unpredictable feedback. A/B testing is a common online testing method. In A/B testing, users are presented with different sets of recommendations, which can result in some groups receiving superior suggestions while others receive inferior ones. The two groups are either exposed to the same algorithm with varying hyper-parameters or two distinct algorithms, typically with one key variant distinguishing the two groups. Online evaluations must be conducted continuously, with performance comparisons assessed over time, as the effectiveness of the recommendation system (RS) is dynamic and can fluctuate due to factors like users’ growing trust in the system or cold-start challenges. (Beel, 2017). This study was conducted in a company that provides RS as a service. It has been providing over 30 million users within the past year; processing with a load of 300 requests per second and maintains a clientele of approximately 70 websites. The service delivers user-based and item-based recommendation systems engaging content-based (CB), collaborative filtering (CF), trend-based, and some hybrids. We tested our RS on a range of platforms, including e-commerce sites, media streaming services, news broadcasting websites, and digital publishing networks. In our analysis, we accounted for the differences in type and scale of data across these websites, recognizing that various data sets can yield differing performance levels with different algorithms. We thoroughly explored multiple algorithms, made comparisons, and considered the contextual factors that influence their effectiveness. Our offline analysis includes ranking and accuracy metrics such as hit-rate@k and, nDCG, mainly used for hyper-parameter tuning. CTR is used as our online metric. Figure 1. The RS architecture is shown above; The green boxes are the services and the orange boxes are data-bases."
https://arxiv.org/html/2411.01053v1,Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities,"Contrastive learning methods, such as CLIP, leverage naturally paired data—for example, images and their corresponding text captions—to learn general representations that transfer efficiently to downstream tasks. While such approaches are generally applied to two modalities, domains such as robotics, healthcare, and video need to support many types of data at once. We show that the pairwise application of CLIP fails to capture joint information between modalities, thereby limiting the quality of the learned representations. To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile’s objective, we derive a lower bound on total correlation, and show that Symile representations for any set of modalities form a sufficient statistic for predicting the remaining modalities. Symile outperforms pairwise CLIP, even with modalities missing in the data, on cross-modal classification and retrieval across several experiments including on an original multilingual dataset of 33M image, text and audio samples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. All datasets and code used in this work are publicly available at https://github.com/rajesh-lab/symile.","Contrastive learning leverages naturally paired data to learn general representations that transfer efficiently to downstream tasks [53, 35, 3]. A common contrastive approach is to maximize the mutual information between the paired modalities, ensuring that the learned representations retain sensitivity to all correlations between them. While SimCLR [12] popularized the use of the mutual information estimator InfoNCE [38] for data augmentations, CLIP [40] applied the approach to distinct modalities—for example, images and their corresponding text captions—where representations are learned using any encoder for each modality. While contrastive approaches are generally applied to two modalities, there is a rapidly expanding range of domains that require the integration of many types of data at once. For example, in robotics, agents combine information from visual, proprioceptive, and tactile sensors [18, 28]; healthcare providers analyze various types of patient data including imaging, biosignals, and genomics [10, 29]; and video encompasses RGB frames, audio waveforms, and text transcripts [55]. One strategy for handling multimodal data has been to design specialized architectures capable of processing all data types at once, which limits their general applicability and increases operational complexity [2, 47]. Another common approach is to apply two-modality contrastive objectives, such as CLIP, to pairs of available modalities [15, 44]. In this paper, we show that, despite its popularity, the pairwise application of CLIP fails to capture higher-order conditional information between modalities, thereby limiting the quality of the representations it learns. For instance, given three modalities 𝐚𝐚\mathbf{a}bold_a, 𝐛𝐛\mathbf{b}bold_b, and 𝐜𝐜\mathbf{c}bold_c, pairwise CLIP captures dependencies between 𝐚𝐚\mathbf{a}bold_a and 𝐛𝐛\mathbf{b}bold_b, 𝐛𝐛\mathbf{b}bold_b and 𝐜𝐜\mathbf{c}bold_c, and 𝐚𝐚\mathbf{a}bold_a and 𝐜𝐜\mathbf{c}bold_c, yet cannot capture any conditional dependencies, such as between 𝐚𝐚\mathbf{a}bold_a and 𝐛𝐛\mathbf{b}bold_b given 𝐜𝐜\mathbf{c}bold_c. We show in Section 2.2 that even in a simple one-dimensional controlled setting where the target 𝐛𝐛\mathbf{b}bold_b is perfectly predictable from 𝐚𝐚\mathbf{a}bold_a and 𝐜𝐜\mathbf{c}bold_c, CLIP performs no better than random chance. Effective contrastive learning for more than two modalities requires a model-agnostic approach capable of learning modality-specific representations—like CLIP—yet also captures higher-order information between any number of modalities—unlike CLIP. Methodological contributions. This paper presents Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile’s objective, we derive a total correlation estimator, employing a generalization of inner products to more than two vectors that allows for the simultaneous contrasting of all modalities and enables zero-shot applications such as classification and retrieval. We then show that the representations produced by Symile for any set of modalities form a sufficient statistic for predicting the remaining modalities not considered in the set. Because it targets total correlation, Symile captures strictly more information than CLIP, guaranteeing performance that matches or surpasses CLIP, except in cases where it known that only pairwise statistics are relevant. Given that such prior knowledge is rarely available, Symile should be favored over CLIP. Empirical contributions. We demonstrate that Symile outperforms pairwise CLIP on cross-modal classification and retrieval across several experiments including on a multilingual dataset of images, text and audio of over 33M examples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. We show that Symile retains its advantage over pairwise CLIP even with modalities missing in the data. We publicly release both the multilingual and the clinical datasets, which are specifically designed to test a model’s ability to capture higher-order information between three distinct high-dimensional data types."
https://arxiv.org/html/2411.01045v1,Towards Robust Text Classification: Mitigating Spurious Correlations with Causal Learning,"In text classification tasks, models often rely on spurious correlations for predictions, incorrectly associating irrelevant features with the target labels. This issue limits the robustness and generalization of models, especially when faced with out-of-distribution data where such spurious correlations no longer hold. To address this challenge, we propose the Causally Calibrated Robust Classifier (CCR), which aims to reduce models’ reliance on spurious correlations and improve model robustness. Our approach integrates a causal feature selection method based on counterfactual reasoning, along with an unbiased inverse propensity weighting (IPW) loss function. By focusing on selecting causal features, we ensure that the model relies less on spurious features during prediction. We theoretically justify our approach and empirically show that CCR achieves state-of-the-art performance among methods without group labels, and in some cases, it can compete with the models that utilize group labels.","Despite their success on standard benchmarks, neural networks often struggle to generalize to out-of-distribution (OOD) data. A primary cause of this problem is their tendency to rely on features not causally related to tasks but holding spurious correlations to labels, which can reduce model robustness when the data distribution shifts Hovy and Søgaard (2015); Ribeiro et al. (2016); Tatman (2017); Buolamwini and Gebru (2018); Hashimoto et al. (2018). For example, in natural language inference (NLI) tasks, if contradictory sentences in a dataset frequently contain negation words, a model trained on this dataset might predict contradiction simply based on the presence of negation words rather than relying on the true underlying features. When encountering data where such spurious correlations do not hold, the model is likely to make incorrect predictions. Prior works divide the data into different groups based on combinations of class labels and spurious features. The distribution of these groups is often unbalanced, and the correlation between labels and spurious features in the majority groups leads to spurious correlations. As a result, a model trained on such a dataset inevitably performs worse on the minority groups compared to the majority groups. Existing methods have focused on improving the worst-group accuracy. Some methods tackle this issue by utilizing group labels, which provide information about both the true labels and the spurious features Sagawa et al. (2019); Goel et al. (2020); Zhang et al. (2020); Idrissi et al. (2022). However, obtaining annotations for group labels is costly and not always feasible, limiting the utility and applicability of these methods. Other approaches employ a two-stage process, where group labels are first inferred, followed by an adjustment of the loss function based on this inferred information. Examples include reweighting samples Liu et al. (2021); Qiu et al. (2023) or incorporating supervised contrastive loss Zhang et al. (2022). However, these methods attempt to calibrate the model learning process by only adjusting the loss function and hope the model’s over-reliance on spurious features can be alleviated. The effects are indirect and limited. A more direct and effective approach would involve modifying the model architecture itself to explicitly identify causal features and filter out spurious ones. Existing works show that models trained using standard Empirical Risk Minimization (ERM) can learn high-quality representations of both spurious and causal features Kirichenko et al. (2022); Izmailov et al. (2022). Therefore, our goal is to encourage models to rely more on causal features than spurious ones. We propose a method, Causally Calibrated Robust Classifier (CCR), to identify causal feature representations through counterfactual reasoning. Specifically, we first disentangle the representation vector from the model’s last layer and then construct counterfactual representation vectors. We adapt causal inference theory Pearl (2009) to calculate the probabilities of necessity and sufficiency for each feature within the representation vector and the entire representation. By retraining the last layer to maximize the probability of necessity and sufficiency of the overall feature representation, the model is guided to assign higher weights to causal features, ultimately relying more on these features. Additionally, we derive a theoretically unbiased loss function based on inverse propensity weighting (IPW), further enhancing the model’s robustness to spurious correlations. Our contributions are summarized as follows: 1. We integrate causal inference theory into feature representation selection by calculating the probabilities of necessity and sufficiency (PNS) for each feature, and propose a method that enhances the weights of causal features in model predictions through counterfactual reasoning by maximizing the PNS for the entire representation vector. 2. We derive a general unbiased loss function with the inverse propensity weighting (IPW) and find that existing methods, which reweight samples to mitigate the effects of spurious correlations, are in fact variants of this unbiased loss function. 3. We propose a Causally Calibrated Robust Classifier (CCR) for text classification tasks that combines causal feature selection with unbiased IPW loss, without the need for group label annotations of training datasets. 4. Comparing our methods CCR with standard ERM, JTT Liu et al. (2021), and AFR Qiu et al. (2023) on 4 text classification tasks with varying spurious correlations, the proposed method achieves superior performance on most tasks and even outperforms methods that rely on full group labels Sagawa et al. (2019); Kirichenko et al. (2022)."
https://arxiv.org/html/2411.01039v1,Enhancing Question Answering Precision with Optimized Vector Retrieval and Instructions,"Question-answering (QA) is an important application of Information Retrieval (IR) and language models, and the latest trend is toward pre-trained large neural networks with embedding parameters. Augmenting QA performances with these LLMs requires intensive computational resources for fine-tuning. We propose an innovative approach to improve QA task performances by integrating optimized vector retrievals and instruction methodologies. Based on retrieval augmentation, the process involves document embedding, vector retrieval, and context construction for optimal QA results. We experiment with different combinations of text segmentation techniques and similarity functions, and analyze their impacts on QA performances. Results show that the model with a small chunk size of 100 without any overlap of the chunks achieves the best result and outperforms the models based on semantic segmentation using sentences. We discuss related QA examples and offer insight into how model performances are improved within the two-stage framework.","Accurately answering queries utilizing large-scale datasets has become an important task in the fast evolving filed of natural language processing (NLP). Recent advancements in machine learning and artificial intelligence have led to the development of sophisticated models that can parse, follow, and respond to natural language queries with unprecedented accuracy. Among these, the Retrieval-Augmented Generation (RAG) architecture represents a significant leap forward, combining the strengths of information (vector) retrieval and generative models for QA applications(Siriwardhana et al., 2023). This paper introduces a novel methodology that leverages the RAG architecture, aiming to enhance QA precision through optimized vector retrieval and improved context construction."
https://arxiv.org/html/2411.01035v1,Provable Length Generalization in Sequence Prediction via Spectral Filtering,We consider the problem of length generalization in sequence prediction. We define a new metric of performance in this setting – the Asymmetric-Regret– which measures regret against a benchmark predictor with longer context length than available to the learner. We continue by studying this concept through the lens of the spectral filtering algorithm. We present a gradient-based learning algorithm that provably achieves length generalization for linear dynamical systems. We conclude with proof-of-concept experiments which are consistent with our theory.,"Sequence prediction is a fundamental problem in machine learning with widespread applications in natural language processing, time-series forecasting, and control systems. In this setting, a learner observes a sequence of tokens and iteratively predicts the next token, suffering a loss that measures the discrepancy between the predicted and the true token. Predicting future elements of a sequence based on historical data is crucial for tasks ranging from language modeling to autonomous control. A key challenge in sequence prediction is understanding the role of context length—the number of previous tokens used to make the upcoming prediction—and designing predictors that perform well with limited context due to computational and memory constraints. These resource constraints become particularly significant during the training phase of a predictor, where the computational cost of using long sequences can be prohibitive. Consequently, it is beneficial to design predictors that can learn from a smaller context length while still generalizing well to longer sequences. This leads us to the central question of our investigation: Can we develop algorithms that learn effectively using short contexts but perform comparably to models that use longer contexts? To address this question, we introduce a new performance metric—Asymmetric-Regret—which measures the difference in total prediction loss between an online predictor with limited context length and a benchmark predictor with a longer context. Unlike classical regret, which assumes both the learner and the benchmark operate under the same conditions, Asymmetric-Regret accounts for the asymmetry in context lengths, providing a more realistic assessment of performance in resource-constrained settings. With a formal and well-defined notion of Asymmetric-Regret in hand, we begin our investigation with the following question: are there algorithms that can attain non-trivial bounds on the Asymmetric-Regret for natural sequences? We explore this concept through the lens of spectral filtering algorithms (Hazan et al., 2017b, 2018). Spectral filtering has emerged as a robust method for learning linear dynamical systems when the system is unknown and the hidden state is unobserved. Beyond their theoretically sound properties, spectral filtering-based predictors have proven practical in recent applications. Notably, the Spectral Transform Unit (Agarwal et al., 2023), a neural architecture built using spectral filtering, has recently shown promise on sequence prediction over a range of modalities (Liu et al., 2024). In this work, we extend the theoretical understanding of spectral filtering by demonstrating that these predictors can achieve length generalization. Specifically, we present a gradient-based online learning algorithm for spectral filtering and show that we can train on a smaller context length while still achieving the same regret bounds as if we had trained on a longer context length. Formally, we prove that this algorithm guarantees sublinear Asymmetric-Regret, indicating that the performance gap diminishes as the sequence length increases. Beyond theoretical interest, our work is practically motivated by challenges in length generalization faced by large language models (LLMs). Current LLMs often struggle to generalize to longer sequences than those seen during training (Abbe et al., 2023; Anil et al., 2022; Jelassi et al., 2023; Zhou et al., 2023; Delétang et al., 2022; Dziri et al., 2024; Zhou et al., 2024) and a significant body of empirical research has been dedicated to addressing this limitation (Kazemnejad et al., 2024; Shen et al., 2023; Dai, 2019; Chi et al., 2022; Li et al., 2023; Press et al., 2021). Despite its importance and extensive empirical research, provable theoretical results on length generalization remain largely elusive. We view our work as a step toward addressing this gap. More practically, most methods introduced to improve length generalization are task-specific. Our work suggests that neural architectures that incorporate spectral filtering, like the Spectral Transform Unit, have the potential to provide robust length generalization. 1.1 Our Contributions Consider online sequence prediction in which the predictor iteratively receives input ut∈ℛdinsubscript𝑢𝑡superscriptℛsubscript𝑑inu_{t}\in{\mathcal{R}}^{d_{\textrm{in}}}italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and then makes a prediction y^t∈ℛdoutsubscript^𝑦𝑡superscriptℛsubscript𝑑out\hat{y}_{t}\in{\mathcal{R}}^{d_{\textrm{out}}}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT out end_POSTSUBSCRIPT end_POSTSUPERSCRIPT of the output, after which the true output ytsubscript𝑦𝑡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is revealed. The goal of the predictor is to minimize error according to a given convex and Lipschitz loss function ℓt⁢(yt,y^t)subscriptℓ𝑡subscript𝑦𝑡subscript^𝑦𝑡\ell_{t}(y_{t},\hat{y}_{t})roman_ℓ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ). In this work we consider the class of spectral filtering predictors, introduced by Hazan et al. (2017b). A spectral filtering predictor is characterized by parameters (T,Mii=1k,k)𝑇superscriptsubscriptsubscript𝑀𝑖𝑖1𝑘𝑘(T,{M_{i}}_{i=1}^{k},k)( italic_T , italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_k ) and outputs predictions y^tsubscript^𝑦𝑡\hat{y}_{t}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT of the form y^t=yt−1+∑i=1kMi⁢u(t−1):0⁢ϕi,subscript^𝑦𝑡subscript𝑦𝑡1superscriptsubscript𝑖1𝑘subscript𝑀𝑖subscript𝑢:𝑡10subscriptitalic-ϕ𝑖\hat{y}_{t}=y_{t-1}+\sum_{i=1}^{k}M_{i}u_{(t-1):0}\phi_{i},over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT + ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT ( italic_t - 1 ) : 0 end_POSTSUBSCRIPT italic_ϕ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , where u(t−1):0∈ℝdin×Tsubscript𝑢:𝑡10superscriptℝsubscript𝑑in𝑇u_{(t-1):0}\in\mathbb{R}^{d_{\textrm{in}}\times T}italic_u start_POSTSUBSCRIPT ( italic_t - 1 ) : 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT × italic_T end_POSTSUPERSCRIPT is a matrix whose columns are the previous inputs ut−1,ut−2,…,u0subscript𝑢𝑡1subscript𝑢𝑡2…subscript𝑢0u_{t-1},u_{t-2},\dots,u_{0}italic_u start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_t - 2 end_POSTSUBSCRIPT , … , italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT (possibly zero-padded as necessary), {ϕj}j=1ksuperscriptsubscriptsubscriptitalic-ϕ𝑗𝑗1𝑘\{\phi_{j}\}_{j=1}^{k}{ italic_ϕ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT are the T𝑇Titalic_T-dimensional spectral filters, {Mi}i=1k⊂ℛdout×dinsuperscriptsubscriptsubscript𝑀𝑖𝑖1𝑘superscriptℛsubscript𝑑outsubscript𝑑in\left\{M_{i}\right\}_{i=1}^{k}\subset{\mathcal{R}}^{d_{\textrm{out}}\times d_{% \textrm{in}}}{ italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ⊂ caligraphic_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT out end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT in end_POSTSUBSCRIPT end_POSTSUPERSCRIPT are matrices which are learned online, and k𝑘kitalic_k is the number of filters used. Hazan et al. (2017b) provide an algorithm to learn {Mi}i=1ksuperscriptsubscriptsubscript𝑀𝑖𝑖1𝑘\left\{M_{i}\right\}_{i=1}^{k}{ italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and show this achieves nearly optimal regret bounds when measured against the best Linear Dynamical System (LDS) predictor. We investigate whether it is necessary to use the entire history u(t−1):0subscript𝑢:𝑡10u_{(t-1):0}italic_u start_POSTSUBSCRIPT ( italic_t - 1 ) : 0 end_POSTSUBSCRIPT to learn the optimal set of matrices {Mi}i=1ksuperscriptsubscriptsubscript𝑀𝑖𝑖1𝑘\left\{M_{i}\right\}_{i=1}^{k}{ italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT. More broadly, we explore whether predictor classes and corresponding online learning algorithms exist that can achieve context length generalization—that is, they use only a short recent history during learning but perform nearly as well as if they had used the full, much longer history length. Of course, predictors which perform poorly on systems that require long memory can trivially achieve context length generalization if their performance is poor regardless of the context length used. Therefore, it is important to note that one of the key features of spectral filtering predictors is that they are able to perform well on systems that have long memory (Hazan et al., 2017b). To properly understand context length generalization, we introduce the notion of Asymmetric-Regret. The idea is to consider the regret of learning a predictor from a class which is only allowed to use context length L′superscript𝐿′L^{\prime}italic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT against the best predictor which is allowed to use (potentially much longer and therefore asymmetric) context length L𝐿Litalic_L. Let ∏Lsubscriptproduct𝐿\prod_{L}∏ start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT denote the class of predictors in ∏product\prod∏ which use context length L𝐿Litalic_L. Given an algorithm 𝒜⁢(L′)𝒜superscript𝐿′{\mathcal{A}}(L^{\prime})caligraphic_A ( italic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) which learns over predictors from some class ∏L′subscriptproductsuperscript𝐿′\prod_{L^{\prime}}∏ start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, the Asymmetric-Regret over horizon T𝑇Titalic_T is RegretAsymmetric,T⁢(𝒜⁢(L′),∏L)=def∑t=1Tℓt⁢(yt,y^t𝒜⁢(L′))−minπ∈∏L⁡ℓt⁢(yt,y^tπ).superscriptdefsubscriptRegretAsymmetric𝑇𝒜superscript𝐿′subscriptproduct𝐿superscriptsubscript𝑡1𝑇subscriptℓ𝑡subscript𝑦𝑡superscriptsubscript^𝑦𝑡𝒜superscript𝐿′subscript𝜋subscriptproduct𝐿subscriptℓ𝑡subscript𝑦𝑡superscriptsubscript^𝑦𝑡𝜋\mathrm{Regret}_{\mathrm{Asymmetric},T}\left({\mathcal{A}}(L^{\prime}),\prod_{% L}\right)\stackrel{{\scriptstyle\text{def}}}{{=}}\sum_{t=1}^{T}\ell_{t}(y_{t},% \hat{y}_{t}^{{\mathcal{A}}(L^{\prime})})-\min_{\pi\in\prod_{L}}\ell_{t}(y_{t},% \hat{y}_{t}^{\pi}).roman_Regret start_POSTSUBSCRIPT roman_Asymmetric , italic_T end_POSTSUBSCRIPT ( caligraphic_A ( italic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) , ∏ start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) start_RELOP SUPERSCRIPTOP start_ARG = end_ARG start_ARG def end_ARG end_RELOP ∑ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT roman_ℓ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT caligraphic_A ( italic_L start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT ) - roman_min start_POSTSUBSCRIPT italic_π ∈ ∏ start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_ℓ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_π end_POSTSUPERSCRIPT ) . Our main result shows that spectral filtering generalizes from a history of Tqsuperscript𝑇𝑞T^{q}italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT, where q∈[0,1]𝑞01q\in[0,1]italic_q ∈ [ 0 , 1 ], to T𝑇Titalic_T for certain linear dynamical systems. It is formally given in the following theorem. Theorem 1. Let T∈ℤ≥0𝑇subscriptℤabsent0T\in\mathbb{Z}_{~{}\geq~{}0}italic_T ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT and q∈[0,1]𝑞01q\in[0,1]italic_q ∈ [ 0 , 1 ]. Consider a sequence (y1,…,yT)subscript𝑦1…subscript𝑦𝑇(y_{1},\dots,y_{T})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) generated by an unknown and noiseless linear dynamical system defined by matrices (A,B,C,D)𝐴𝐵𝐶𝐷(A,B,C,D)( italic_A , italic_B , italic_C , italic_D ) as per Eq. 1. Assume the input sequence u0:(t−1)subscript𝑢:0𝑡1u_{0:(t-1)}italic_u start_POSTSUBSCRIPT 0 : ( italic_t - 1 ) end_POSTSUBSCRIPT is sufficiently well-conditioned, satisfying ∑t=0T−1(T−t)⁢ut⁢ut⊤⪰(2⁢|C|⁢|B|T)⁢Isucceeds-or-equalssuperscriptsubscript𝑡0𝑇1𝑇𝑡subscript𝑢𝑡superscriptsubscript𝑢𝑡top2𝐶𝐵𝑇𝐼\sum_{t=0}^{T-1}(T-t)u_{t}u_{t}^{\top}\succeq\left(\frac{2|C||B|}{\sqrt{T}}% \right)I∑ start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT ( italic_T - italic_t ) italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ⪰ ( divide start_ARG 2 | italic_C | | italic_B | end_ARG start_ARG square-root start_ARG italic_T end_ARG end_ARG ) italic_I. Suppose the eigenvalues of A𝐴Aitalic_A lie within the range [0,1−log⁡(T)8⁢Tq]∪[1−12⁢T5/4,1]01𝑇8superscript𝑇𝑞112superscript𝑇541\left[0,1-\frac{\log(T)}{8T^{q}}\right]\cup\left[1-\frac{1}{2T^{5/4}},1\right][ 0 , 1 - divide start_ARG roman_log ( italic_T ) end_ARG start_ARG 8 italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT end_ARG ] ∪ [ 1 - divide start_ARG 1 end_ARG start_ARG 2 italic_T start_POSTSUPERSCRIPT 5 / 4 end_POSTSUPERSCRIPT end_ARG , 1 ]. Let 𝒜⁢(L)𝒜𝐿\mathcal{A}(L)caligraphic_A ( italic_L ) denote Algorithm 1 operating with context length L𝐿Litalic_L, and let ∏LSFsubscriptsuperscriptproductSF𝐿\prod^{\mathrm{SF}}_{L}∏ start_POSTSUPERSCRIPT roman_SF end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT denote the class of spectral filtering predictors using context length L𝐿Litalic_L. For the squared loss ℓt⁢(y,y′)=|y−y′|2subscriptℓ𝑡𝑦superscript𝑦′superscript𝑦superscript𝑦′2\ell_{t}(y,y^{\prime})=|y-y^{\prime}|^{2}roman_ℓ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = | italic_y - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and sufficiently large T𝑇Titalic_T, it holds that: RegretAsymmetric,T⁢(𝒜⁢(Tq),∏TSF)≤O~⁢(T).subscriptRegretAsymmetric𝑇𝒜superscript𝑇𝑞subscriptsuperscriptproductSF𝑇~𝑂𝑇\mathrm{Regret}_{\mathrm{Asymmetric},T}\left({\mathcal{A}}(T^{q}),\prod^{% \mathrm{SF}}_{T}\right)~{}\leq~{}\tilde{O}(\sqrt{T}).roman_Regret start_POSTSUBSCRIPT roman_Asymmetric , italic_T end_POSTSUBSCRIPT ( caligraphic_A ( italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT ) , ∏ start_POSTSUPERSCRIPT roman_SF end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ≤ over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_T end_ARG ) . This theorem indicates that for any q∈[0,1]𝑞01q\in[0,1]italic_q ∈ [ 0 , 1 ], the Asymmetric-Regret is bounded by O~⁢(T)~𝑂𝑇\tilde{O}(\sqrt{T})over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_T end_ARG ). However, as q𝑞qitalic_q decreases, the class of linear dynamical systems for which this bound holds becomes more restricted due to the eigenvalue conditions on A𝐴Aitalic_A. The spectrum of A𝐴Aitalic_A determines the memory of the system; when the eigenvalues of A𝐴Aitalic_A are 1111, the system is only marginally-stable and standard predictors which aim to use low memory typically fail. Critically, Theorem 1 holds even for these marginally-stable systems. When interpreting this result, it’s important to note that the class of spectral filtering predictors ∏TSFsubscriptsuperscriptproductSF𝑇\prod^{\mathrm{SF}}_{T}∏ start_POSTSUPERSCRIPT roman_SF end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT which use the full context length are provably able to predict well on marginally-stable Linear Dynamical Systems (Hazan et al., 2017b)111The only LDS’s for which there can be any useful results are those with A𝐴Aitalic_A’s eigenvalues in [−1,1]11[-1,1][ - 1 , 1 ], i.e. marginally-stable systems. We recall that the spectral filtering principle can be readily applied to handle negative eigenvalues in [−1,0]10[-1,0][ - 1 , 0 ] (see Appendix D of Agarwal et al. (2023), for example). For ease of presentation, we focus on capturing the length generalization effects of eigenvalues in [0,1]01[0,1][ 0 , 1 ] in the sequel, and so we suppose without loss of generality that A⪰0succeeds-or-equals𝐴0A\succeq 0italic_A ⪰ 0.. Therefore, this result implies that spectral filtering predictors are able to context length generalize in a nontrivial way. Inspired by particular spectrum of A𝐴Aitalic_A that is required for the classical Spectral Filtering algorithm to achieve length generalization, we develop a novel variation on the Spectral Filtering algorithm, presented in Algorithm 2, which achieves length generalization without added assumptions on the spectrum of A𝐴Aitalic_A (whenever the context-length is at least T1/3superscript𝑇13T^{1/3}italic_T start_POSTSUPERSCRIPT 1 / 3 end_POSTSUPERSCRIPT). Algorithm 2 achieves this by using two autoregressive components yt−1subscript𝑦𝑡1y_{t-1}italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT and yt−2subscript𝑦𝑡2y_{t-2}italic_y start_POSTSUBSCRIPT italic_t - 2 end_POSTSUBSCRIPT to construct its prediction y^tsubscript^𝑦𝑡\hat{y}_{t}over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT of ytsubscript𝑦𝑡y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We provide the following theoretical result. Theorem 2. Let T∈ℤ≥0𝑇subscriptℤabsent0T\in\mathbb{Z}_{~{}\geq~{}0}italic_T ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT and q∈[0,1]𝑞01q\in[0,1]italic_q ∈ [ 0 , 1 ]. Consider a sequence (y1,…,yT)subscript𝑦1…subscript𝑦𝑇(y_{1},\dots,y_{T})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) generated by an unknown and noiseless linear dynamical system defined by matrices (A,B,C,D)𝐴𝐵𝐶𝐷(A,B,C,D)( italic_A , italic_B , italic_C , italic_D ) as per Eq. 1. Assume the input sequence u0:(t−1)subscript𝑢:0𝑡1u_{0:(t-1)}italic_u start_POSTSUBSCRIPT 0 : ( italic_t - 1 ) end_POSTSUBSCRIPT is sufficiently well-conditioned, satisfying ∑t=0T−1(T−t)⁢ut⁢ut⊤⪰(2⁢|C|⁢|B|T)⁢Isucceeds-or-equalssuperscriptsubscript𝑡0𝑇1𝑇𝑡subscript𝑢𝑡superscriptsubscript𝑢𝑡top2𝐶𝐵𝑇𝐼\sum_{t=0}^{T-1}(T-t)u_{t}u_{t}^{\top}\succeq\left(\frac{2|C||B|}{\sqrt{T}}% \right)I∑ start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT ( italic_T - italic_t ) italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ⪰ ( divide start_ARG 2 | italic_C | | italic_B | end_ARG start_ARG square-root start_ARG italic_T end_ARG end_ARG ) italic_I. Suppose the eigenvalues of A𝐴Aitalic_A lie within the range [0,1−log⁡(T)8⁢Tq]∪[1−12⁢T1/4,1]01𝑇8superscript𝑇𝑞112superscript𝑇141\left[0,1-\frac{\log(T)}{8T^{q}}\right]\cup\left[1-\frac{1}{2T^{1/4}},1\right][ 0 , 1 - divide start_ARG roman_log ( italic_T ) end_ARG start_ARG 8 italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT end_ARG ] ∪ [ 1 - divide start_ARG 1 end_ARG start_ARG 2 italic_T start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT end_ARG , 1 ]. Let 𝒜⁢(L)𝒜𝐿\mathcal{A}(L)caligraphic_A ( italic_L ) denote Algorithm 2 operating with context length L𝐿Litalic_L, and let ∏LSFsubscriptsuperscriptproductSF𝐿\prod^{\mathrm{SF}}_{L}∏ start_POSTSUPERSCRIPT roman_SF end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT denote the class of spectral filtering predictors using context length L𝐿Litalic_L. For the squared loss ℓt⁢(y,y′)=|y−y′|2subscriptℓ𝑡𝑦superscript𝑦′superscript𝑦superscript𝑦′2\ell_{t}(y,y^{\prime})=|y-y^{\prime}|^{2}roman_ℓ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = | italic_y - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and sufficiently large T𝑇Titalic_T, it holds that: RegretAsymmetric,T⁢(𝒜⁢(Tq),∏TSF)≤O~⁢(T).subscriptRegretAsymmetric𝑇𝒜superscript𝑇𝑞subscriptsuperscriptproductSF𝑇~𝑂𝑇\mathrm{Regret}_{\mathrm{Asymmetric},T}\left({\mathcal{A}}(T^{q}),\prod^{% \mathrm{SF}}_{T}\right)~{}\leq~{}\tilde{O}(\sqrt{T}).roman_Regret start_POSTSUBSCRIPT roman_Asymmetric , italic_T end_POSTSUBSCRIPT ( caligraphic_A ( italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT ) , ∏ start_POSTSUPERSCRIPT roman_SF end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ≤ over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_T end_ARG ) . Observe that if q≥1/3𝑞13q~{}\geq~{}1/3italic_q ≥ 1 / 3, then [0,1−log⁡(T)/(8⁢Tq)]∪[1−1/T1/4,1]=[0,1]01𝑇8superscript𝑇𝑞11superscript𝑇14101[0,1-\log(T)/(8T^{q})]\cup[1-1/T^{1/4},1]=[0,1][ 0 , 1 - roman_log ( italic_T ) / ( 8 italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT ) ] ∪ [ 1 - 1 / italic_T start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT , 1 ] = [ 0 , 1 ] for any T>0𝑇0T>0italic_T > 0 and so we do not constrain the spectrum of A𝐴Aitalic_A to get length generalization (aside from assuming it has nonnegative eigenvalues). Our next contribution is the development of a new class of predictors we call tensorized spectral filters. Tensorized spectral filters possess more structure than their original counterparts and are provably more expressive—they can learn a select class of time-varying linear dynamical systems that vanilla spectral filtering cannot. We develop a novel context-length dependent algorithm for tensorized spectral filtering which, similar to Algorithm 2, requires two autoregressive components. Theorem 3. Let T∈ℤ≥0𝑇subscriptℤabsent0T\in\mathbb{Z}_{~{}\geq~{}0}italic_T ∈ blackboard_Z start_POSTSUBSCRIPT ≥ 0 end_POSTSUBSCRIPT and q∈[0,1]𝑞01q\in[0,1]italic_q ∈ [ 0 , 1 ]. Consider a sequence (y1,…,yT)subscript𝑦1…subscript𝑦𝑇(y_{1},\dots,y_{T})( italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) generated by an unknown and noiseless linear dynamical system defined by matrices (A,B,C,D)𝐴𝐵𝐶𝐷(A,B,C,D)( italic_A , italic_B , italic_C , italic_D ) as per Eq. 1. Assume the input sequence u0:(t−1)subscript𝑢:0𝑡1u_{0:(t-1)}italic_u start_POSTSUBSCRIPT 0 : ( italic_t - 1 ) end_POSTSUBSCRIPT is sufficiently well-conditioned, satisfying ∑t=0T−1(T−t)⁢ut⁢ut⊤⪰(2⁢|C|⁢|B|T)⁢Isucceeds-or-equalssuperscriptsubscript𝑡0𝑇1𝑇𝑡subscript𝑢𝑡superscriptsubscript𝑢𝑡top2𝐶𝐵𝑇𝐼\sum_{t=0}^{T-1}(T-t)u_{t}u_{t}^{\top}\succeq\left(\frac{2|C||B|}{\sqrt{T}}% \right)I∑ start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T - 1 end_POSTSUPERSCRIPT ( italic_T - italic_t ) italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ⪰ ( divide start_ARG 2 | italic_C | | italic_B | end_ARG start_ARG square-root start_ARG italic_T end_ARG end_ARG ) italic_I. Suppose the eigenvalues of A𝐴Aitalic_A lie within the range [0,1−log⁡(T)8⁢Tq]∪[1−12⁢T1/4,1]01𝑇8superscript𝑇𝑞112superscript𝑇141\left[0,1-\frac{\log(T)}{8T^{q}}\right]\cup\left[1-\frac{1}{2T^{1/4}},1\right][ 0 , 1 - divide start_ARG roman_log ( italic_T ) end_ARG start_ARG 8 italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT end_ARG ] ∪ [ 1 - divide start_ARG 1 end_ARG start_ARG 2 italic_T start_POSTSUPERSCRIPT 1 / 4 end_POSTSUPERSCRIPT end_ARG , 1 ]. Let 𝒜⁢(L)𝒜𝐿\mathcal{A}(L)caligraphic_A ( italic_L ) denote Algorithm 3 operating with context length L𝐿Litalic_L, and let ∏LSFsubscriptsuperscriptproductSF𝐿\prod^{\mathrm{SF}}_{L}∏ start_POSTSUPERSCRIPT roman_SF end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT denote the class of spectral filtering predictors using context length L𝐿Litalic_L. For the squared loss ℓt⁢(y,y′)=|y−y′|2subscriptℓ𝑡𝑦superscript𝑦′superscript𝑦superscript𝑦′2\ell_{t}(y,y^{\prime})=|y-y^{\prime}|^{2}roman_ℓ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_y , italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) = | italic_y - italic_y start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and sufficiently large T𝑇Titalic_T, it holds that: RegretAsymmetric,T⁢(𝒜⁢(Tq),∏TSF)≤O~⁢(T).subscriptRegretAsymmetric𝑇𝒜superscript𝑇𝑞subscriptsuperscriptproductSF𝑇~𝑂𝑇\mathrm{Regret}_{\mathrm{Asymmetric},T}\left({\mathcal{A}}(T^{q}),\prod^{% \mathrm{SF}}_{T}\right)~{}\leq~{}\tilde{O}(\sqrt{T}).roman_Regret start_POSTSUBSCRIPT roman_Asymmetric , italic_T end_POSTSUBSCRIPT ( caligraphic_A ( italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT ) , ∏ start_POSTSUPERSCRIPT roman_SF end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ≤ over~ start_ARG italic_O end_ARG ( square-root start_ARG italic_T end_ARG ) . Finally, we experimentally confirm the results of Theorem 1 on synthetic data generated by an LDS. Interestingly, we find that Theorem 1 accurately predicts when length generalization is possible; indeed, when the data is generated by an LDS which has eigenvalues in the “bad” range [1−log⁡(T)/(8⁢Tq),1−1/(2⁢T5/4)]1𝑇8superscript𝑇𝑞112superscript𝑇54[1-\log(T)/(8T^{q}),1-1/(2T^{5/4})][ 1 - roman_log ( italic_T ) / ( 8 italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT ) , 1 - 1 / ( 2 italic_T start_POSTSUPERSCRIPT 5 / 4 end_POSTSUPERSCRIPT ) ] we find that the limited context length spectral filtering predictors are unable to length generalize. However, when the data is generated by and LDS which has eigenvalues “hugging” this bad range (i.e. either just smaller than 1−log⁡(T)/(8⁢Tq)1𝑇8superscript𝑇𝑞1-\log(T)/(8T^{q})1 - roman_log ( italic_T ) / ( 8 italic_T start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT ) or just larger than 1−1/(2⁢T5/4)112superscript𝑇541-1/(2T^{5/4})1 - 1 / ( 2 italic_T start_POSTSUPERSCRIPT 5 / 4 end_POSTSUPERSCRIPT )), the limited context length spectral filtering predictors successfully length generalize. Next, we conduct experiments using the STU neural architecture to test the hypothesis that this architecture should simply length generalize without any task-specific engineering. We consider the induction heads synthetic task and find that the out-of-the-box STU neural architecture does indeed enjoy some level of length generalization. This suggests that incorporating spectral filtering into neural architectures, like the STU, may provide improved length generalization in deep learning applications. We leave further empirical study on this for future work. 1.2 Related Work The literature for sequence prediction is too broad to survey in detail, so we give a few highlights of the recent rapid advancements. The most notable progress includes the Transformer model (Vaswani et al., 2017) that incorporates an attention mechanism for accurate sequence prediction in many domains (Brown et al., 2020; Dosovitskiy et al., 2020; Jumper et al., 2021). Transformer models and their attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see Tay et al. (2022) for a recent survey). Motivated by the high memory and compute requirements of transformers, state space models were revisited starting from (Gu et al., 2020, 2021b) who propose and develop the HiPPO theory. Gu et al. (2021a) develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numerical stability. Further works in the area show SOTA performance and include Gupta et al. (2022); Smith et al. (2023); Orvieto et al. (2023); Gu & Dao (2023). State space models are very efficient for training and inference, but can suffer in long-context applications. This motivated the use of spectral filtering technique for learning marginally-stable linear dynamical systems (Hazan et al., 2017b, 2018). This technique was incorporated to a neural architecture in Agarwal et al. (2023), that was recently shown to perform well across several modalities (Liu et al., 2024). From an applied perspective, generalization in sequence prediction has been recently studied in Hou et al. (2024) through the theoretical lens of Turing programs. They propose a methodology that empirically improves length generalization across a diverse set of tasks. There are also architecture-specific approaches to length generalization such as ALiBi positional embeddings for transformers (Press et al., 2022), but such methods lack provable guarantees and can have varying empirical performance (Kazemnejad et al., 2024). In contrast, our investigation starts from the theory of regret minimization in games and online learning. Regret minimization has the advantage that it implies generalization in the statistical learning setting (see e.g. Cesa-Bianchi et al. (2004)) and is usually accompanied by efficient algorithms such as online gradient descent (see e.g. Hazan et al. (2016)). Our new notion of Asymmetric-Regret incorporates asymmetric information access between the online learner and the benchmark class."
https://arxiv.org/html/2411.00997v1,Identifying Implicit Social Biases in Vision-Language Models,"Vision-language models, like CLIP (Contrastive Language Image Pretraining), are becoming increasingly popular for a wide range of multimodal retrieval tasks. However, prior work has shown that large language and deep vision models can learn historical biases contained in their training sets, leading to perpetuation of stereotypes and potential downstream harm. In this work, we conduct a systematic analysis of the social biases that are present in CLIP, with a focus on the interaction between image and text modalities. We first propose a taxonomy of social biases called So-B-IT, which contains 374 words categorized across ten types of bias. Each type can lead to societal harm if associated with a particular demographic group. Using this taxonomy, we examine images retrieved by CLIP from a facial image dataset using each word as part of a prompt. We find that CLIP frequently displays undesirable associations between harmful words and specific demographic groups, such as retrieving mostly pictures of Middle Eastern men when asked to retrieve images of a “terrorist”. Finally, we conduct an analysis of the source of such biases, by showing that the same harmful stereotypes are also present in a large image-text dataset used to train CLIP models for examples of biases that we find. Our findings highlight the importance of evaluating and addressing bias in vision-language models, and suggest the need for transparency and fairness-aware curation of large pre-training datasets.","Machine learning has seen rapid advances in Vision-Language (VL) models that learn to jointly represent image and language data in a shared embedding space (Radford et al. 2021; Jia et al. 2021). Recent advances on a range of multi-modal tasks are exemplified by the VL model CLIP (Radford et al. 2021), leading to state-of-the-art performance on several zero-shot retrieval tasks (Xu et al. 2021) as well as being integrated into various VL models such as LLaVA (Liu et al. 2024) and BLIP (Li et al. 2022a), which combine the frozen vision encoder with language models for enhanced multi-modal understanding and alignment, Stable Diffusion, which leverages CLIP embeddings for refined text-to-image generation (Rombach et al. 2022) and various other VL models. These successes have spurred several VL models in end-user applications, such as facial recognition systems where CLIP enhances zero-shot face recognition (Zhao and Patras 2023), and multimedia event extraction, as well as event detection in images and captions (Li et al. 2022b; Lu et al. 2024). However, recent works show that large pre-trained models that operate over vision (May et al. 2019; Park et al. 2021), language (Bender et al. 2021; Guo and Caliskan 2020; Zhang et al. 2020a) or both learn social biases from training data (Barocas, Hardt, and Narayanan 2017; Corbett-Davies and Goel 2018), which risks perpetuating bias into downstream retrieval and generation tasks (Silva, Tambwekar, and Gombolay 2021; Luccioni et al. 2023; Weidinger et al. 2021). In VL models specifically, terms related to race have been found to be associated more with people of color (Agarwal et al. 2021), and women are generally underrepresented in image retrieval tasks (Wang, Liu, and Wang 2021). However, these existing works focus only on very specific forms of bias while probing disparities using a small set of curated words (Bhargava and Forsyth 2019). Figure 1: Identifying biases in CLIP using word associations. Given that more extensive and intersectional forms of bias may exist in VL models, there is a need to expand these experiments to a richer taxonomy of potential biases. Further, the size of current datasets used to train such models makes it more difficult for humans to effectively identify low-quality, toxic, or harmful samples (Hanna and Park 2020; Kreutzer et al. 2022). Methods to find and describe biases in datasets are crucial to ensure safe adoption of VL models, yet few methods exist. Recent findings of child sexual abuse images (Thiel 2023) in LAION-5B (Schuhmann et al. 2022), a popular training VL training dataset (Ilharco et al. 2021), further highlights the need to audit the relationships learned by VL models in particular. In this work, we target identifying and describing bias in pre-trained VL models at scale. We first propose a large new taxonomy, called Social Bias Implications Taxonomy (So-B-IT), which spans ten different categories of biases. So-B-IT allows us to examine bias much more broadly than prior works, including biases associated with discrimination based on the model’s implicit assumptions on images of faces. For instance, So-B-IT implements new categories of biased description, such as Appearance and Occupation, and extends word lists used by prior works (May et al. 2019; Steed and Caliskan 2021; Berg et al. 2022), allowing for finer grained analysis. Including new categories is crucial to investigate bias in VL models, as past work has targeted crime-related words (Bhargava and Forsyth 2019) or self-similarity across different demographic groups (Wolfe and Caliskan 2022). Using So-B-IT, we then investigate bias in VL models by retrieving images from FairFace (Kärkkäinen and Joo 2019) — a dataset containing pictures of peoples’ faces along with their age, gender, and race — that the model associates with the words in our taxonomy. For each category in So-B-IT, we quantify the demographic distributions of these retrieved images. As each image contains only a persons face, the association that a VL model makes between these images and the words in our taxonomy should be exclusively explained by the biases inherent to the model itself (Figure 1). Our analysis, based on studying four CLIP-based models (OAICLIP (Radford et al. 2021), OpenCLIP (Ilharco et al. 2021), FaceCLIP (Zheng et al. 2022), and DebiasCLIP (Berg et al. 2022), confirms that these systems encode significant racial and gender biases. Because So-B-IT is more fine-grained than prior work, we uncover previously-unknown, intersectional biases in CLIP models. For example, OpenCLIP not only strongly associates Homemaker with Women significantly more than it does with Men (Stanovsky, Smith, and Zettlemoyer 2019; De-Arteaga et al. 2019), but overwhelmingly associates Homemaker with Indian Women more than it does for women of other races, which is previously uncharacterized in VL models. Our analysis also uncovers that debiasing VL models for Gender can significantly increase the racial bias of the model. This extends prior work showing the propensity of vision models to lean more strongly on remaining shortcuts after debiasing (Li et al. 2023) to VL models. We also extend our experiments to seek the sources of bias in VL training data. Our investigation into training data associated with biased terms confirms the non-representative demographic distributions we identify experimentally. While our experiments are based on CLIP due to its ubiquity (Rombach et al. 2022; Gao et al. 2023; Zhou et al. 2023), our analysis and the So-B-IT taxonomy is directly applicable to any VL model with a joint image and text encoding. Our contributions can be summarized as follows: • We propose a taxonomy, So-B-IT, that covers more categories of bias than prior work and at a finer grain. So-B-IT allows us to categorize a VL model’s capacity to perpetuate societal bias in more representative tasks, and can be used broadly for vision and language auditing. • Using So-B-IT, we audit four different versions of CLIP, finding that these models encode various forms of societal bias and stereotyping across gender and racial groups. • Our findings indicate that debiasing with respect to one sensitive attribute, such as gender, does not necessarily eliminate other forms of bias, particularly racial bias. • We investigate the source of such biases using CLIP’s pre-training data, finding that disproportionate demographic representation may be a root cause of identified biases."
https://arxiv.org/html/2411.00929v1,Text2Freq: Learning Series Patterns from Text via Frequency Domain,"Traditional time series forecasting models mainly rely on historical numeric values to predict future outcomes. While these models have shown promising results, they often overlook the rich information available in other modalities, such as textual descriptions of special events, which can provide crucial insights into future dynamics. However, research that jointly incorporates text in time series forecasting remains relatively underexplored compared to other cross-modality work. Additionally, the modality gap between time series data and textual information poses a challenge for multimodal learning. To address this task, we propose Text2Freq, a cross-modality model that integrates text and time series data via the frequency domain. Specifically, our approach aligns textual information to the low-frequency components of time series data, establishing more effective and interpretable alignments between these two modalities. Our experiments on paired datasets of real-world stock prices and synthetic texts show that Text2Freq achieves state-of-the-art performance, with its adaptable architecture encouraging future research in this field.","The importance of incorporating textual information into time series forecasting is increasingly evident. Real-world time series data is often influenced by external factors, such as news events, consumer feedback, and special occasions, which traditional models fail to account for [1]. Several approaches have been proposed in response to the growing need for multimodal learning in time series forecasting. However, these methods face three significant challenges. First, the scarcity of paired datasets that combine time series and text makes the learning process difficult. Second, techniques from other cross-modality tasks, such as cross-attention, are hard to directly apply due to the significant modality gap between time series and text. This gap is due to differences such as text being discrete and rich in semantic content, while time series are continuous, focus on temporal changes, and often contain noise. Finally, text often encapsulates high-level patterns, such as overall trends, leading to a one-to-many problem when directly mapping text to time series (see Figure 1). To overcome these challenges, we propose Text2Freq, a framework that align textual information to time series data through the frequency domain. Our approach includes a pre-trained text-to-frequency module, trained on a boarder dataset to address the issue of limited paired data. Additionally, by aligning text with the low-frequency components of time series, Text2Freq effectively bridges the modality gap and extracts clear patterns from text, enhancing interpretability. We validate Text2Freq using real-world stock price data paired with synthetic text generated by GPT-4 [2]. To the best of our knowledge, Text2Freq is the first method to align textual information with low-frequency series components, leading to improved performance in mulitimodal forecasting with more effective alignment. Figure 1: Illustration of alignment issues between time series and text. Starting with only the lowest frequency NL⁢F=1subscript𝑁𝐿𝐹1N_{LF}=1italic_N start_POSTSUBSCRIPT italic_L italic_F end_POSTSUBSCRIPT = 1 captures slow-changing patterns like sinusoidal waves but loses details, causing many-to-one mapping issues from text to series. Increasing the frequency components to NL⁢F=Tsubscript𝑁𝐿𝐹𝑇N_{LF}=Titalic_N start_POSTSUBSCRIPT italic_L italic_F end_POSTSUBSCRIPT = italic_T adds detail from a series but introduces noise, leading to one-to-many mapping issues. Since text encapsulates high-level patterns, this work aims to map text to an optimal subset of frequency components No⁢p⁢tsubscript𝑁𝑜𝑝𝑡N_{opt}italic_N start_POSTSUBSCRIPT italic_o italic_p italic_t end_POSTSUBSCRIPT that balances noise and information loss."
https://arxiv.org/html/2411.00865v1,DemoCraft: Using In-Context Learning to Improve Code Generation in Large Language Models,"Generating executable code from natural language instructions using Large Language Models (LLMs) poses challenges such as semantic ambiguity and understanding task-specific contexts. To address these issues, we propose a system called DemoCraft, which enhances code generation by leveraging in-context learning and demonstration selection, combined with latent concept learning. Latent concept learning introduces additional concept tokens, which are trainable embeddings that capture task-specific knowledge. We then test our system on two major datasets: MBPP and Humaneval. Our experimental results demonstrate that the proposed system achieves an approximate 2x increase in the pass@k metric compared to baseline models. Furthermore, we introduce two novel evaluation metrics: correctness@k and similarity@k. Our empirical studies indicate that our system attains nearly a 3x improvement in these metrics as well.","The problem of generating code from natural language using Large Language Models (LLMs) involves creating systems capable of translating human language instructions into executable code accurately. This requires the LLM to understand the semantics of the natural language input, grasp the intent behind the instructions, and convert it into syntactically correct and functional code in a specified programming language. Key challenges include handling ambiguous or imprecise language, ensuring the generated code is both correct and efficient, and covering a wide range of programming scenarios and languages. Figure 1: Large Language Models struggling at Code Generation Code generation remains a significant challenge for large language models, as evidenced by Google’s AlphaCode[1], developed specifically for competitive programming tasks. When evaluated on the CodeContests benchmark, AlphaCode achieves a maximum Codeforces rating of only 1238, placing it in approximately the top 28th percentile. Furthermore, a comprehensive survey on code generation using large language models [2] reports a maximum pass@1 rate of around 30%. These studies have been conducted under zero-shot conditions, highlighting the necessity for few-shot learning approaches. Few-shot learning allows models to leverage relevant demonstrations associated with the prompt prior to generating the output, potentially improving performance."
https://arxiv.org/html/2411.00855v1,Vision-Language Models Can Self-Improve Reasoning via Reflection,"Chain-of-thought (CoT) has proven to improve the reasoning capability of large language models (LLMs). However, due to the complexity of multimodal scenarios and the difficulty in collecting high-quality CoT data, CoT reasoning in multimodal LLMs has been largely overlooked. To this end, we propose a simple yet effective self-training framework, R3V, which iteratively enhances the model’s Vision-language Reasoning by Reflecting on CoT Rationales. Our framework consists of two interleaved parts: (1) iteratively bootstrapping positive and negative solutions for reasoning datasets, and (2) reflection on rationale for learning from mistakes. Specifically, we introduce the self-refine and self-select losses, enabling the model to refine flawed rationale and derive the correct answer by comparing rationale candidates. Experiments on a wide range of vision-language tasks show that R3V consistently improves multimodal LLM reasoning, achieving a relative improvement of 23% to 60% over GPT-distilled baselines. Additionally, our approach supports self-reflection on generated solutions, further boosting performance through test-time computation. 111 Our code is available at https://github.com/njucckevin/MM-Self-Improve.","Humans often rely on intuitive Chain-of-Thought (CoT) to perform complex reasoning Ericsson and Simon (1980). Previous studies have shown that this CoT capacity also emerges in Large Language Models (LLMs) Wei et al. (2022). Through simple prompting or fine-tuning Cobbe et al. (2021); Kojima et al. (2022); Hsieh et al. (2023), CoT enhances the reasoning performance of LLMs while providing insights into their decision-making process. Recently, OpenAI o1 further advances reasoning by producing long internal CoT sequences, taking LLMs intelligence to a new level. Figure 1: Results of Qwen-VL on TabMWP, a visual mathematical reasoning dataset. Qwen-VL exhibits weak zero-shot CoT reasoning performance, while our R3V iteratively self-improves, surpassing the GPT-distilled baseline by a large margin. While CoT reasoning has significantly advanced LLMs in textual domains, extending CoT to multimodal settings remains an open problem. Unlike the abundant, unsupervised text-based CoT in pre-training corpora Kojima et al. (2022); Wei et al. (2022), multimodal CoT resources are scarce in the text-dominated internet collections Dai et al. (2023), hindering the full realization of Multimodal LLMs’ (MLLMs) reasoning potential. Recent studies show that open-sourced MLLMs struggle to integrate visual cues into their reasoning process, resulting in weak CoT performance Zhang et al. (2024a); Shi et al. (2024). Consistent with our observations in Figure 1, CoT prompting provides minimal gains over direct prediction Chen et al. (2024a) and falls far behind GPT-4o. One potential solution is to construct multimodal CoT annotations for post-training; however, manual annotation is prohibitively expensive and hard to scale. This raises our first research question: can MLLMs self-improve the reasoning capabilities through bootstrapping on CoT samples? Orthogonal to fine-tuning on curated CoT annotations, relying solely on positive samples can lead to suboptimal policy due to insufficient exploration of reasoning paths. Inspired by human thinking, another promising direction involves learning from trial-and-errors Yuan et al. (2024); Song et al. (2024), where mistakes are not failures but key opportunities to enhance reasoning. A few multimodal approaches use corrupted prompts to create negative samples for preference learning, aiming to improve image comprehension Wang et al. (2023); Deng et al. (2024). However, these methods fail to generate reasoning-aligned positive and negative CoT solutions, making them unsuitable for complex multimodal reasoning tasks. Thus, it remains unaddressed: how can MLLMs efficiently learn from mistakes to improve their reasoning skills? To address the above two questions, this paper proposes R3V, a self-training framework that enables the model to Reflect on bootstrapped CoT Rationales, thereby strengthening its Vision-Language Reasoning. Firstly, we leverage MLLM’s pre-existing but weak CoT ability to bootstrap both rationales and answers for a given question, enabling the collection of a large number of positive and negative solutions based on answer correctness. Secondly, we introduce a reflection mechanism on negative solutions to help the model learn from mistakes. Specifically, we design self-refine and self-select losses that guide the model to correct flawed rationales and derive the correct answer by comparing rationale candidates, respectively. The above synergistic process can be repeated, with improved samples boosting MLLM’s reasoning and the enhanced model further improving rationale generation. Additionally, through self-select training, our model can derive the superior solution from multiple samples, further boosting performance via test-time computation. We conduct experiments across a wide range of multimodal reasoning benchmarks, including charts, geometry, commonsense, science, mathematics, etc. R3V progressively enhances the reasoning ability of MLLMs, delivering a 23%-60% relative accuracy improvement compared to GPT distillation, and consistently outperforming the strong self-training baseline, STaR Zelikman et al. (2022). Moreover, our test-time selection is robust and effective, consistently surpassing Pass@1 and majority voting, even in OOD scenarios. Our main contributions are as follows: • We introduce an iterative self-training framework R3V that leverages CoT bootstrapped by MLLM itself for self-improvement. To our knowledge, this is the first attempt to apply self-training in vision-language reasoning. • We propose learning from mistakes through self-reflection, with support for test-time computation to further improve reasoning performance. • We perform extensive evaluations across 6 different multimodal domains to validate the effectiveness of R3V. Our analysis reveals the key factors driving the success of multimodal self-training."
https://arxiv.org/html/2411.00853v1,Accelerated AI Inference via Dynamic Execution Methods,"In this paper, we focus on Dynamic Execution techniques that optimize the computation flow based on input. This aims to identify simpler problems that can be solved using fewer resources, similar to human cognition. The techniques discussed include early exit from deep networks, speculative sampling for language models, and adaptive steps for diffusion models. Experimental results demonstrate that these dynamic approaches can significantly improve latency and throughput without compromising quality. When combined with model-based optimizations, such as quantization, dynamic execution provides a powerful multi-pronged strategy to optimize AI inference.Generative AI requires a large amount of compute resources. This is expected to grow, and demand for resources in data centers through to the edge is expected to continue to increase at high rates. We take advantage of existing research and provide additional innovations for some generative optimizations. In the case of LLMs, we provide more efficient sampling methods that depend on the complexity of the data. In the case of diffusion model generation, we provide a new method that also leverages the difficulty of the input prompt to predict an optimal early stopping point.Therefore, dynamic execution methods are relevant because they add another dimension of performance optimizations. Performance is critical from a competitive point of view, but increasing capacity can result in significant power savings and cost savings. We have provided several integrations of these techniques into several Intel performance libraries and Huggingface Optimum. These integrations will make them easier to use and increase the adoption of these techniques.","The computational requirements for AI inference are growing at a rapid rate. Larger models and increased demand (i.e. queries directly or via APIs) in these models drive higher inference costs [1][2]. To control the energy demands and hardware requirements of the newest generation AI models, we must find ways to run large-scale AI architectures more efficiently while achieving the same accuracy and performance levels. Past years of research have resulted in new methods to both make models more efficient by accelerating the inference of models and making the models themselves more lightweight. When thinking about the efficiency of models, we want to optimize the computational resources needed to achieve a given output of a computation. Given a specific output of a computational / inference process, we can aim to tweak the power consumption, throughput, and latency of a target architecture. In this paper, we are starting with an overview of very established model compression techniques (quantization, sparsity), which can address power consumption through lowering the memory demand. However, our primary focus is on methods to improve model throughput by allowing models to take shortcuts during the inference process. Note that model throughput is a very interesting target, as it is directly correlated with the total costs of serving the model as a service. Alongside a review of established techniques, we will also provide an overview of novel results on early exit, speculative sampling in token and feature space techniques in large language models (LLMs) and techniques to accelerate diffusion models. We show that significant throughput improvements can be achieved by expanding established model architectures with the methods presented here. Out of the scope of this paper are that many architectural improvements can be made given the representation (e.g. ONNX) and runtime engines. We believe that significant improvements that can be achieved through the inference optimization techniques presented here should not be perceived as the result of arbitrary tricks of model optimization. Instead, they all have deep roots in the study of intelligent systems. John von Neumann has already discussed that we should be able to build intelligent systems with low numerical precision given how noisy signal communication is in the brain [3]. Similarly, large and powerful brains of mammals also achieve energy efficiency through sparsification of their brain networks [4][5]. Lastly, the idea of dynamic execution is deeply rooted in cognitive science, where humans and animals have been observed to choose between different problem solving strategies considering the mental effort and computational resources needed to execute a given strategy [6][7][8]. Given that a computationally cheaper strategy achieves a comparable result, they would quickly change course to pursue the simpler strategy. The inference optimization techniques presented in the following hence do not only show promising efficiency effects but also show how there is a joint mechanism of efficient information processing connecting artificial and biological computing systems."
https://arxiv.org/html/2411.00850v1,GWQ: Gradient-Aware Weight Quantization for Large Language Models,"Large language models (LLMs) show impressive performance in solving complex language tasks. However, its large number of parameters present significant challenges for the deployment and application of the model on edge devices. Compressing large language models to low bits can enable them to run on resource-constrained devices, often leading to performance degradation. To address this problem, we propose gradient-aware weight quantization (GWQ), the first quantization approach for low-bit weight quantization that leverages gradients to localize outliers, requiring only a minimal amount of calibration data for outlier detection. GWQ retains the weights corresponding to the top 1% outliers preferentially at FP16 precision, while the remaining non-outlier weights are stored in a low-bit format. GWQ found experimentally that utilizing the sensitive weights in the gradient localization model is more scientific compared to utilizing the sensitive weights in the Hessian matrix localization model. Compared to current quantization methods, GWQ can be applied to multiple language models and achieves lower PPL on the WikiText2 and C4 dataset. In the zero-shot task, GWQ quantized models have higher accuracy compared to other quantization methods.GWQ is also suitable for multimodal model quantization, and the quantized Qwen-VL family model is more accurate than other methods. zero-shot target detection task dataset RefCOCO outperforms the current stat-of-the-arts method SPQR. GWQ achieves 1.2×\times× inference speedup in comparison to the original model, and effectively reduces the inference memory.","Large language models (LLMs) (Touvron et al., 2023a; Achiam et al., 2023; Almazrouei et al., 2023; Touvron et al., 2023b) based on Transformer (Vaswani, 2017) have demonstrated their outstanding capabilities in scenarios such as complex question and answer, language modeling (Brown, 2020), and so on. It can often generate more accurate answers with different inputs. Its ability to handle complex linguistic tasks is due to its huge pre-trained corpus (Kaplan et al., 2020) and an astronomical number of parameters (Chowdhery et al., 2023). Its outstanding performance is rapidly generating huge applications in people’s lives. However, the huge memory consumption generated by its pre-training and inference phases leads to great difficulties in its application and deployment (Li et al., 2024a). Therefore, when deploying large language models, there are often huge GPU clusters to support the inference of the models (Xu et al., 2024). In order to make large language models adaptable to resource-constrained edge devices (Li et al., 2024b; Huang et al., 2024), model compression (Ma et al., 2023; Gu et al., 2023) has become a common means to reduce the computational memory of models. Among them, post-training large language model quantization (Dettmers and Zettlemoyer, 2023) is a commonly used method to reduce the model inference overhead. Most current quantization methods compress the model to 3 or 4 bits (Dettmers et al., 2023; Frantar et al., 2022a), while trying to ensure that the model has as little loss of accuracy and performance as possible. Post-training quantization of pre-trained models reduces model inference overhead by allowing the number of parameter bits to decrease. This approach produces a loss of accuracy that affects the performance of the model in use. Therefore, to reduce the size of the model while ensuring its performance, it is necessary to design effective quantization algorithms (Lin et al., 2024). With effective quantization algorithms, researchers can make the performance of low-bit models infinitely close to that of 16-bit models. OBQ (Frantar and Alistarh, 2022) indicates that when the pre-trained model has fully converged, the model should exhibit zero gradients. However, our experiments reveal that the LLMs still generate gradients in response to different text inputs, indicating that the model has not completely converged and remains sensitive to the input. Inspired by this phenomenon, we propose a gradient-aware post-training weight-only quantization method called GWQ. GWQ is the first post-training quantization approach to utilize gradients to locate outliers in pre-trained models. It introduces two key innovations. First, GWQ selects 1% of the weights as outliers by looking at the magnitude of the gradient for the response to the calibration set, preserving these outliers in FP16 precision while quantizing the remaining 99% of the weights to 4 bits or 3 bits. In summary, the primary contributions of this paper are as follows: 1) GWQ discovered that employing the first-order gradient to search for and locate sensitive weights is more rational than using the Hessian matrix for localization, and the model loss after quantization is also lower. Therefore, using a first-order gradient to locate outliers is more reasonable. 2) GWQ is the first accurate first-order gradient-aware post-training weight quantization method for pre-trained LLMs, requiring only a minimal quantity of calibration data to identify outliers efficiently. 3) GWQ outshines the current state-of-the-arts method SPQR on the wikitext and C4 datasets. The majority of zero-shot detection tasks in the multimodal dataset RefCOCO have reached the state-of-the-arts. Meanwhile, the quantified model has achieved a 1.2×\times× acceleration compared to the original model and utilized less memory during inference."
https://arxiv.org/html/2411.00843v1,The Graph’s Apprentice: Teaching an LLM Low-Level Knowledge for Circuit QualityEstimation,"Logic synthesis is a crucial phase in the circuit design process, responsible for transforming hardware description language (HDL) designs into optimized netlists. However, traditional logic synthesis methods are computationally intensive, restricting their iterative use in refining chip designs. Recent advancements in large language models (LLMs), particularly those fine-tuned on programming languages, present a promising alternative. In this paper, we introduce VeriDistill, the first end-to-end machine learning model that directly processes raw Verilog code to predict circuit quality-of-result metrics. Our model employs a novel knowledge distillation method, transferring low-level circuit insights via graphs into the predictor based on LLM. Experiments show VeriDistill outperforms state-of-the-art baselines on large-scale Verilog datasets and demonstrates robust performance when evaluated on out-of-distribution datasets.","Rapid technological advancements in computing power has taken an increasingly important role in the past decades in driving scientific research in biology (Schatz, 2012), chemistry (Akimov & Prezhdo, 2015), physics (Dongarra & Keyes, 2024) and especially artificial intelligence, where it has been estimated that at least half of all performance gains in the past ten years have stemmed from hardware improvements alone (Hernandez & Brown, 2020; Dorner, 2021; Karpathy, ; Erdil & Besiroglu, 2022; Ho et al., 2024). This ever-rising demand for compute power means that efficient and effective electronic chip design has become increasingly critical. Modern electronic chip design is a complex, multi-stage endeavor that begins with a chip architect specifying the digital circuit’s functionality in a Hardware Description Language (HDL), such as Verilog (Thomas & Moorby, 2008) or VHDL (Coelho, 2012). This HDL code is then subjected to a series of transformations and optimizations, ultimately yielding a physical circuit design that can be manufactured (LaMeres, 2023). In a previous era where circuits were small and limited in functionality, this logic synthesis process was quick and the chip architect could quickly receive feedback and iterate on its HDL code. However, with the increasing complexity of industrial designs, which now can comprise hundreds of millions of logic gates (Amarú et al., 2017), even a single synthesis run has become massively expensive. This has driven the need for alternate ways of providing feedback on HDL code without running the actual logic synthesis process. A natural way to tackle this problem is to train a machine learning model that can take the HDL code as input, and output estimates of circuit quality such as wire length or delay that could have been computed had the logic synthesis process been run. A few works have approached this topic, by extracting graphical information about the code and using hand-designed statistics of those graphs as features (Zhou et al., 2019; Sengupta et al., 2022; Fang et al., 2023). Although these works had encouraging results, their performance has been limited by the relatively shallow understanding of the semantics of the code that these statistics can provide. Recently, Large Language Models fine-tuned on code, such as Code-T5 (Wang et al., 2021), Codex (Chen et al., 2021), CodeGen Nijkamp et al. (2023), CodeLlama (Roziere et al., 2023) and DeepSeek-Coder (Guo et al., 2024), have emerged that have proven remarkably successful on a wide range of tasks (Zheng et al., 2023), most notably as code assistants such as Github Copilot111https://github.com/features/copilot. Although most models are generalists trained on general-purpose programming languages such as C++ and Python, a few models, such as CodeGen-Verilog (Thakur et al., 2023), VeriGen (Thakur et al., 2024), RTLCoder (Liu et al., 2023c) and CodeV (Zhao et al., 2024), have been specifically trained on Verilog, the most popular HDL language. The analysis of these models, however, has been so far limited to investigating their ability to generate realistic code, and an investigation of the predictive power of those internal representations has been lacking. In this work, we demonstrate for the first time that the hidden states computed by these novel Verilog large language models contain rich insights which can be used to predict quality-of-result metrics with higher accuracy than previous machine learning models. Namely, we feed Verilog code to the state-of-the-art CodeV model, and train an inexpensive decoder neural network that uses the LLM’s hidden states as features to predict area and delay. In addition, and critically, we regularize this decoder to encourage its embeddings to resemble those of a graph neural network model trained on Look-Up Table (LUT) graph, an intermediate representation used during the logic synthesis process. The resulting decoder is shown to strongly outperform state-of-the-art baselines, and incidentally shows that those novel Verilog language models extract in their hidden states surprisingly complex insights about the circuits represented by raw code. Our work makes the following main contributions: 1. We develop the first truly end-to-end machine learning model in the literature, named VeriDistill, which can take raw Verilog code, without any preprocessing, and produce accurate estimates of circuit area/delay metrics. 2. Moreover, we apply during training a novel knowledge distillation method which allows to transfer low-level insights about the circuit, in the form of LUT graphs, back into the machine learning predictor model. 3. We demonstrate through experiments that the combination of those two elements outperforms previous state-of-the-art baselines in a large-scale Verilog dataset and enhances the model’s ability to transfer to out-of-distribution data. 4. Finally, we also demonstrate that both using LLM representations and the knowledge distillation are essential, in that removing any one of these components brings the performance back below the previous baselines. The remainder of this paper is structured as follows. Section 2 provides an overview of the relevant literature and background information. In Section 3, we present a detailed description of our proposed methodology, including its key components and underlying assumptions. The efficacy of our approach is then demonstrated through a series of experiments, which are reported in Section 4. Finally, Section 5 summarizes our main findings, discusses their implications, and outlines potential avenues for future research."
https://arxiv.org/html/2411.00841v1,A Theoretical Perspective for Speculative Decoding Algorithm,"Transformer-based autoregressive sampling has been the major bottleneck for slowing down large language model inferences. One effective way to accelerate inference is Speculative Decoding, which employs a small model to sample a sequence of draft tokens and a large model to validate. Given its empirical effectiveness, the theoretical understanding of Speculative Decoding is falling behind. This paper tackles this gap by conceptualizing the decoding problem via markov chain abstraction and studying the key properties, output quality and inference acceleration, from a theoretical perspective. Our analysis covers the theoretical limits of speculative decoding, batch algorithms, and output quality-inference acceleration tradeoffs. Our results reveal the fundamental connections between different components of LLMs via total variation distances and show how they jointly affect the efficiency of decoding algorithms.","The recent surge of scaling Transformer models has led to the flourishing of AI, where success has been witnessed in wide areas such as natural language Touvron et al. (2023); Achiam et al. (2023), computer vision Dosovitskiy et al. (2020); Han et al. (2022), video generations Arnab et al. (2021); Ho et al. (2022), and robotics Brohan et al. (2022); Shridhar et al. (2023). In the meantime, the decoding process also becomes more and more time-consuming as the model size scales up. This is mainly due to the autoregressive nature of Transformers, where each generated token also serves as the input for future generations. As a result, decoding T𝑇Titalic_T tokens would take T𝑇Titalic_T forward passes of the full model. A recent effort to tackle this challenge is speculative decoding (SD) Chen et al. (2023a); Leviathan et al. (2023), where the autoregressive sampling is performed on a small draft model and the large language model verifies tokens generated by draft model to decide whether it should be accepted/rejected. Once a token is rejected, the generation process will start from the most recently accepted token, until a full response is completed. Speculative decoding achieves 2-2.5×\times× LLM inference speedup empirically, while preserving the quality of generation. Subsequently, numerous studies Miao et al. (2023); Liu et al. (2023); Kim et al. (2023); Zhou et al. (2023) have expanded this methodology, enabling further inference acceleration. Intuitively, for speculative decoding, when the generation distribution of small model p𝑝pitalic_p and large model q𝑞qitalic_q are close to each other, decoding is faster (since less rejection occurs), and when the distribution overlap between p𝑝pitalic_p and q𝑞qitalic_q is small, the opposite happens. However, a precise understanding of inference accelerating given the small model p𝑝pitalic_p and large model q𝑞qitalic_q remains elusive. This motivates us to ask the following question: What is the fundamental limit for inference acceleration via speculative decoding? In addition, what is the best trade-off between inference acceleration and output quality for speculative decoding? Figure 1: Left: Standard Auto-Regressive Decoding (Algorithm 3) v.s. Right: Speculative Decoding (Algorithm 1), where a large model is used to validate the responses of the small model. In this paper, we answer these questions from the theoretical lens. Our contributions are summarized as follows. We formalize the decoding problem through the Markov Chain abstraction that establishes the theoretical setup. We draw the connection runtime=# rejectionsruntime# rejections\text{runtime}=\text{\# rejections}runtime = # rejections and use it to measure efficiency. We derive the exact formula, fully characterized by distribution p𝑝pitalic_p and q𝑞qitalic_q, for the expected rejections 𝔼⁢[Nrej]𝔼delimited-[]subscript𝑁rej\mathbb{E}[{N_{\text{rej}}}]blackboard_E [ italic_N start_POSTSUBSCRIPT rej end_POSTSUBSCRIPT ] for Speculative Decoding (Theorem 1). This renders a theoretical reference for understanding the acceleration rate T/𝔼⁢[Nrej]𝑇𝔼delimited-[]subscript𝑁rejT/\mathbb{E}[{N_{\text{rej}}}]italic_T / blackboard_E [ italic_N start_POSTSUBSCRIPT rej end_POSTSUBSCRIPT ]. Next, to understand whether Speculative Decoding can be further improved, we generalize it to a class of rejection-based algorithms 2 where probability btsubscript𝑏𝑡b_{t}italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and distribution 𝒫tsubscript𝒫𝑡\mathcal{P}_{t}caligraphic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT can be customized. We prove in Theorem 2 that any unbiased algorithm cannot have fewer rejections than Speculative Decoding. This indicates its optimality among the class, and having fewer rejections needs to suffer quality loss or requires extra information. Furthermore, we consider a batch version of Speculative Decoding (Algorithm 4) that utilizes multiple draft sequences. We show our batch algorithm is unbiased. We derive the expected rejections that is fully expressed by p𝑝pitalic_p and q𝑞qitalic_q and exhibit the improvement over non-batch version in Theorem 3. We provide examples and detailed discussion to explain how our theory characterize the improvement. In section 4, we shift from unbiased algorithms and study the tradeoff between inference cost and quality degradation. We formulate this into an optimization model (1). Theorem 5 established a linear Pareto front that characterizes the tradeoff between inference cost and quality degradation (Figure 4). A simple experiment in Section 4.2 is also consistent with our theoretical finding. Last but not least, our technical results involve novel analysis, for instance, the design of 𝒱+,𝒱−subscript𝒱subscript𝒱\mathcal{V}_{+},\mathcal{V}_{-}caligraphic_V start_POSTSUBSCRIPT + end_POSTSUBSCRIPT , caligraphic_V start_POSTSUBSCRIPT - end_POSTSUBSCRIPT in the lower bound proof C and the iterative computation for f𝑓fitalic_f in D.1. They are the first of its kind and consist of our technical contributions. We provide a proof sketch section in Appendix A. 1.1 Related works Speculative Decoding and its applications. Speculative execution, where performing speculative work can expedite computation on parallel machines by allowing certain tasks to commence before their necessity is confirmed, can date back to Burton (1985); Gabbay and Mendelson (1996). Recently, Chen et al. (2023a); Leviathan et al. (2023) formalize this idea with rejection sampling based design for LLM Decoding and achieve multiple-time inference acceleration compared to vanilla auto-regressive decoding. There are fruitful studies Xia et al. (2023); Miao et al. (2023); Liu et al. (2023); Sun et al. (2023); Kim et al. (2023); Zhou et al. (2023); Spector and Re (2023); Mitchell et al. (2023); He et al. (2023); Bae et al. (2023); Su et al. (2023); Ahn et al. (2023); Santilli et al. (2023); Xu et al. (2023); Chen et al. (2023b); Xia et al. (2024); Yang et al. (2024); Qian et al. (2024); Sun et al. (2024a); Bergner et al. (2024); Yan et al. (2024); Wang et al. (2024); Huang et al. (2024) since then, and they improve speculative decoding from different angles such as online updating Liu et al. (2023), multiple candidates Yang et al. (2024), retrieval technique He et al. (2023), Multimodality Gagrani et al. (2024) or even decoding without draft models Fu et al. (2024); Bhendawade et al. (2024). Theoretical endeavor for Speculative Decoding. There are also works that study the theoretical properties for speculative decoding (SD). In particular, Sun et al. (2023) considers speculative decoding from the optimal transport perspective and show it is optimal in the single token regime. It further extends to the k𝑘kitalic_k multiple draft token setting and formulates the optimal transport solution via linear programming with exponential in k𝑘kitalic_k computation time. An approximate sequential selection algorithm is also proposed with linear time. Ahn et al. (2023) further proposes the improved plan, and Sun et al. (2024b) extends Sun et al. (2023) to the block-level optimal transport for SD. Su et al. (2023) investigates the synergy between draft length and batch size for Speculative Decoding and formulate the optimal speculation length as the root of a polynomial equation. Miao et al. (2023) proposes the SpecInfer, a batch algorithm that uses small speculative models to form the token tree, and proves its output matches the distribution of the large model. Yang et al. (2024) considers batch speculative decoding without replacement to avoid repeatedly sampling rejected tokens and proves it keeps the decoding quality. Nevertheless, the findings for inference acceleration of these works are mostly empirical, lacking theoretical guarantees. Algorithm 1 Speculative Decoding Chen et al. (2023a); Leviathan et al. (2023) 1: Input: Set probability bt=min⁡{1,qtpt}subscript𝑏𝑡1subscript𝑞𝑡subscript𝑝𝑡b_{t}=\min\{1,\frac{q_{t}}{p_{t}}\}italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = roman_min { 1 , divide start_ARG italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG } and the distribution 𝒫t=[qt−pt]+subscript𝒫𝑡subscriptdelimited-[]subscript𝑞𝑡subscript𝑝𝑡\mathcal{P}_{t}=[q_{t}-p_{t}]_{+}caligraphic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = [ italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT + end_POSTSUBSCRIPT in Algorithm 2. 2: Require: pt(⋅):=pt(⋅|x1:n−1,x~n:t−1)p_{t}(\cdot):=p_{t}(\cdot|x_{1:n-1},\tilde{x}_{n:t-1})italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( ⋅ ) := italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( ⋅ | italic_x start_POSTSUBSCRIPT 1 : italic_n - 1 end_POSTSUBSCRIPT , over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_n : italic_t - 1 end_POSTSUBSCRIPT ), qt(⋅):=qt(⋅|x1:n−1,x~n:t−1)q_{t}(\cdot):=q_{t}(\cdot|x_{1:n-1},\tilde{x}_{n:t-1})italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( ⋅ ) := italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( ⋅ | italic_x start_POSTSUBSCRIPT 1 : italic_n - 1 end_POSTSUBSCRIPT , over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_n : italic_t - 1 end_POSTSUBSCRIPT ). ∀t≥nfor-all𝑡𝑛\forall t\geq n∀ italic_t ≥ italic_n. 3: for t=n:T:𝑡𝑛𝑇t=n:Titalic_t = italic_n : italic_T do 4: Sample r∼𝖴𝗇𝗂𝖿𝗈𝗋𝗆⁢[0,1]similar-to𝑟𝖴𝗇𝗂𝖿𝗈𝗋𝗆01r\sim{\sf Uniform}[0,1]italic_r ∼ sansserif_Uniform [ 0 , 1 ]. 5: if r≤min⁡{1,qt⁢(x~t)pt⁢(x~t)}𝑟1subscript𝑞𝑡subscript~𝑥𝑡subscript𝑝𝑡subscript~𝑥𝑡r\leq\min\left\{1,\frac{q_{t}(\tilde{x}_{t})}{p_{t}(\tilde{x}_{t})}\right\}italic_r ≤ roman_min { 1 , divide start_ARG italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) end_ARG } then 6: Accept with xn=x~tsubscript𝑥𝑛subscript~𝑥𝑡x_{n}=\tilde{x}_{t}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. n←n+1←𝑛𝑛1n\leftarrow n+1italic_n ← italic_n + 1. 7: else 8: Sample xn∼[qt−pt]+⁢(⋅)similar-tosubscript𝑥𝑛subscriptdelimited-[]subscript𝑞𝑡subscript𝑝𝑡⋅x_{n}\sim\left[q_{t}-p_{t}\right]_{+}(\cdot)italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∼ [ italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ( ⋅ ). 9: n←n+1←𝑛𝑛1n\leftarrow n+1italic_n ← italic_n + 1. Break. 10: // Recall [qt−pt]+subscriptdelimited-[]subscript𝑞𝑡subscript𝑝𝑡\left[q_{t}-p_{t}\right]_{+}[ italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT + end_POSTSUBSCRIPT in Section 2.1 11: end if 12: end for Algorithm 2 Framework for Rejection-based Decoding 1: Init: Horizon T𝑇Titalic_T, models qtsubscript𝑞𝑡q_{t}italic_q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, ptsubscript𝑝𝑡p_{t}italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. Lookahead K=T𝐾𝑇K=Titalic_K = italic_T. Prompt x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. n=0𝑛0n=0italic_n = 0. 2: Require: Probability btsubscript𝑏𝑡b_{t}italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, distribution 𝒫tsubscript𝒫𝑡\mathcal{P}_{t}caligraphic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. 3: while n<T𝑛𝑇n<Titalic_n < italic_T do 4: for t=n:T:𝑡𝑛𝑇t=n:Titalic_t = italic_n : italic_T do 5: Sample drafts x~t∼pt(⋅|x1:n−1,x~n:t−1)\tilde{x}_{t}\sim p_{t}(\cdot|x_{1:n-1},\tilde{x}_{n:t-1})over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∼ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( ⋅ | italic_x start_POSTSUBSCRIPT 1 : italic_n - 1 end_POSTSUBSCRIPT , over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_n : italic_t - 1 end_POSTSUBSCRIPT ). 6: end for 7: Obtain the target logits in parallel for x~n:Tsubscript~𝑥:𝑛𝑇\tilde{x}_{n:T}over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_n : italic_T end_POSTSUBSCRIPT as qn(⋅|x1:n−1),…,qT(⋅|x1:n−1,x~n:T).q_{n}(\cdot|x_{1:n-1}),\;\;\dots,\;\;q_{T}(\cdot|x_{1:n-1},\tilde{x}_{n:T}).italic_q start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( ⋅ | italic_x start_POSTSUBSCRIPT 1 : italic_n - 1 end_POSTSUBSCRIPT ) , … , italic_q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( ⋅ | italic_x start_POSTSUBSCRIPT 1 : italic_n - 1 end_POSTSUBSCRIPT , over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_n : italic_T end_POSTSUBSCRIPT ) . 8: for t=n:T:𝑡𝑛𝑇t=n:Titalic_t = italic_n : italic_T do 9: Accept xn=x~tsubscript𝑥𝑛subscript~𝑥𝑡x_{n}=\tilde{x}_{t}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with prob. btsubscript𝑏𝑡b_{t}italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. n←n+1←𝑛𝑛1n\leftarrow n+1italic_n ← italic_n + 1. 10: Else REJECTION: 11: Sample xn∼similar-tosubscript𝑥𝑛absentx_{n}\simitalic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ∼ distribution 𝒫tsubscript𝒫𝑡\mathcal{P}_{t}caligraphic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. n←n+1←𝑛𝑛1n\leftarrow n+1italic_n ← italic_n + 1. Break. 12: end for 13: end while"
https://arxiv.org/html/2411.00836v1,DynaMath: A Dynamic Visual Benchmarkfor Evaluating Mathematical ReasoningRobustness of Vision Language Models,"The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that state-of-the-art VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate the mathematical reasoning robustness in VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs. While several vision-based math benchmarks have been developed to assess VLMs’ problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness. To fill this gap, we introduce DynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs. DynaMath includes 501 high-quality, multi-topic seed questions, each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set of concrete questions, including many different types of visual and textual variations. DynaMath allows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 state-of-the-art VLMs with 5,010 generated concrete questions (10 per seed question). Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. In addition, many models show high consistency in answering these questions – the incorrectness of a certain variant of a seed question is not due to inherent randomness. Our analysis emphasizes the need to study the robustness of VLMs’ reasoning abilities, and DynaMath provides valuable insights to guide the development of more reliable models for mathematical reasoning.","Leveraging pretraining on vast Internet-scale datasets, Large Language Models (LLMs) (Brown, 2020; Ouyang et al., 2022; Touvron et al., 2023; Achiam et al., 2023) and Multi-modal Large Language Models (MLLMs) (Team et al., 2023; Bai et al., 2023; Liu et al., 2024c; a) have achieved remarkable performance across a wide range of tasks. Among them, Vision-Language Models (VLMs) (Zhu et al., 2023; Zhang et al., 2024b) stand out, showing exceptional promise as versatile assistants capable of integrating vision and language for problem-solving. Among their visual comprehension abilities across different domains, mathematical reasoning (Lightman et al., 2023; Zhang et al., 2024f) stands out as a crucial measure of human-like intelligence, requiring both math knowledge and logical thinking. Recent work has proposed many benchmarks for evaluating the mathematical reasoning ability of VLMs. MATHVISTA (Lu et al., 2023) was the first benchmark specifically designed to evaluate visual mathematical reasoning. Recent closed-source models, such as Claude 3.5 Sonnet and GPT-4o, along with open-source models like LLaVA-OneVision (Li et al., 2024), have demonstrated average performance surpassing that of humans. Benchmarks such as MATH-V (Wang et al., 2024a) and MATHVERSE (Zhang et al., 2024e) demonstrate the current limitations of VLMs in handling challenging mathematical problems and understanding mathematical diagrams. Following typical evaluation pipelines, these benchmarks contain a static set of testing questions on which a VLM will be scored. Figure 1: An example of consistent failures in GPT-4o. Seed question 78 in our DynaMath benchmark generates a graph of a shifted absolute value function. GPT-4o consistently provides incorrect answers for variant 9 (left) with 90% repetition consistency, while it can successfully answer variant 7 (right) with 100% repetition consistency. We tested 7 other variants involving non-zero shifts of the absolute value function, and in each case, GPT-4o insists incorrectly that the “sharp corner” is at x=0𝑥0x=0italic_x = 0, leading to incorrect answers for all 7 variants. More failure examples are in Appendix F. Our work is inspired by recent studies (Nezhurina et al., 2024; Zheng et al., 2023; Zong et al., 2023; Mirzadeh et al., 2024), which revealed that even powerful LLMs struggle to reliably solve simple text reasoning problems under different input values or conditions. We found that this issue is even more pronounced in VLMs due to the added complexity of visual context. In the setting of math problems, we identified consistent failure cases on variations of simple questions. As illustrated in Figure 1, we identify a simple question asking whether a shifted absolute value function f⁢(x)=|x−a|𝑓𝑥𝑥𝑎f(x)=|x-a|italic_f ( italic_x ) = | italic_x - italic_a | is differentiable at x=0𝑥0x=0italic_x = 0. Despite the shift, this question is still quite simple and poses no challenges to humans. While GPT-4o can give correct answers for some values of a𝑎aitalic_a, it consistently gives a wrong answer for many different values of a≠0𝑎0a\neq 0italic_a ≠ 0. Drawing inspiration from human reasoning, where the same steps can be applied to solve similar problems with varying conditions, a robust reasoning model should exhibit the same ability. This raises important questions about the robustness of VLMs’ reasoning abilities: are the reasoning procedures in VLMs robust to problem variations that pose no challenge to humans? To address this question, we comprehensively study the robustness of mathematical reasoning in VLMs by introducing a new benchmark, DynaMath. DynaMath is a dynamic visual math benchmark designed for an in-depth assessment of VLMs’ reasoning robustness. Unlike existing benchmarks, which contain a static dataset of benchmarking questions, DynaMath contains 501 high-quality seed questions covering multiple mathematical topics: Plane Geometry, Solid Geometry, Analytic Geometry, Algebra, Puzzle Tests, Graph Theory, Statistics, Scientific Figures, and Arithmetic. Each seed question is represented as a carefully designed Python program; upon running, a program generates diverse concrete instances of one seed question with random variations in its conditions. The program is individually written for each seed question and considers multiple possible types of variations in each question, such as variations of numerical values, function types, graph structure, geometry, mathematical operations, etc. The questions also span varying difficulty levels, from elementary school to high school and undergraduate, with the latter two dominating. The process of dynamic benchmark generation and evaluation is presented in Figure 2. During generation, many concrete questions are created from a single seed question, and thus the actual number of questions evaluated can be much greater (e.g., 10×\times× more) than the number of seed questions. We conducted extensive experiments on DynaMath to evaluate the reasoning robustness of current state-of-the-art (SOTA) closed-source models, including GPT-4o, Gemini Pro, and Claude-3.5 Sonnet, as well as open-source VLMs such as the InternVL2 series (Chen et al., 2024), LLaVA-v1.6 series (Liu et al., 2024b), Qwen2-VL (Wang et al., 2024c), DeepSeek-VL (Lu et al., 2024), and Llama 3.2 (Dubey et al., 2024). For each seed question, we randomly generated 10 variants, resulting in an evaluation dataset of 5,010 concrete problems. On these problems, we evaluate both average-case accuracy and worst-case accuracy. The worst-case accuracy is defined as the percentage of correctly answered seed problems in all 10 variants. We observe that all considered VLMs have a worst-case accuracy that is close to or less than 50% of their average-case accuracy, signifying their unreliability in handling question variations. In addition, we also evaluate the repetition consistency on these VLMs, which characterizes the models’ inherent randomness to ensure that a low worst-case accuracy is not caused by occasional random errors but consistent errors on certain variants of a seed problem. Our main contributions and findings can be summarized as: Figure 2: The dynamic benchmark generation procedure in DynaMath. A seed question is represented as a program that can generate many concrete questions with different variations. The plots for concrete questions are randomly generated along with the corresponding ground-truth answers. During evaluation, all concrete variants of the seed questions are considered, allowing us to evaluate the worst-case model performance and robustness. • We are the first to study the mathematical reasoning robustness of VLMs and identified a new weakness in VLMs: they may consistently fail on certain variants of simple math questions that pose no challenges to humans. Such a weakness is prevalent in many state-of-the-art VLMs. • We introduce DynaMath, a dynamic benchmark comprising 501 individually designed programs capable of generating a large number of question variants across different types. Our work is the first dynamically generated benchmark for evaluating the math capability of VLMs. • Based on 5,010 concrete questions generated by DynaMath, we conduct an extensive evaluation of both SOTA closed-source and open-source VLMs. We find a noticeable gap between the average-case accuracy and worst-case accuracy among all models, indicating that many VLMs do not have robust reasoning capabilities even on relatively simple mathematical questions."
https://arxiv.org/html/2411.00823v1,Mobility-LLM: Learning Visiting Intentions and Travel Preferences from Human Mobility Data with Large Language Models,"Location-based services (LBS) have accumulated extensive human mobility data on diverse behaviors through check-in sequences. These sequences offer valuable insights into users’ intentions and preferences. Yet, existing models analyzing check-in sequences fail to consider the semantics contained in these sequences, which closely reflect human visiting intentions and travel preferences, leading to an incomplete comprehension. Drawing inspiration from the exceptional semantic understanding and contextual information processing capabilities of large language models (LLMs) across various domains, we present Mobility-LLM, a novel framework that leverages LLMs to analyze check-in sequences for multiple tasks. Since LLMs cannot directly interpret check-ins, we reprogram these sequences to help LLMs comprehensively understand the semantics of human visiting intentions and travel preferences. Specifically, we introduce a visiting intention memory network (VIMN) to capture the visiting intentions at each record, along with a shared pool of human travel preference prompts (HTPP) to guide the LLM in understanding users’ travel preferences. These components enhance the model’s ability to extract and leverage semantic information from human mobility data effectively. Extensive experiments on four benchmark datasets and three downstream tasks demonstrate that our approach significantly outperforms existing models, underscoring the effectiveness of Mobility-LLM in advancing our understanding of human mobility data within LBS contexts.","Location-based services (LBS) such as Gowalla, Weeplace, and Foursquare enable users to share and discover location information and nearby services. This results in the collection of extensive human mobility data, often presented in the form of check-in sequences. These sequences record users’ visits to different points of interest (POIs) like restaurants and hospitals at various times, reflecting significant semantics about their intentions and preferences. Analyzing these check-in sequences is crucial as it offers valuable information on human mobility data, which can positively impact individuals, businesses, and urban management. The key to effectively mining check-in sequences lies in understanding their rich semantics. Existing methods primarily focus on specific tasks, such as location prediction [10, 58, 51, 25], time prediction [39, 44], and trajectory user linking [33, 13, 61], rather than delving into the semantics of human behaviors. This narrow focus often results in limited optimization goals and a shallow understanding of the semantics contained in check-in sequences. Recently, large language models (LLMs) have demonstrated impressive capabilities in semantic understanding and contextual information processing, demonstrating successful adaptability across different domains. LLMs trained on extensive corpora surpass task-specific models in their potential to understand semantic information. Inspired by this, we aim to utilize pre-trained LLMs as powerful check-in sequence learners. Nevertheless, LLMs encounter a significant obstacle in their inability to directly interpret check-in sequences. As typical sequential data, check-in sequences contain a wealth of semantic information that reflects various near-term regularities and inherent characteristics. The future intention of an individual is prone to be dictated by near-term regularities that are close to recent visits, termed visiting intentions. Furthermore, an individual’s inherent characteristics tend to persist over time and determine their travel preferences, which is necessary to analyze them across multiple domains for a comprehensive understanding. Hence, our main challenge is to enable LLMs to effectively extract semantics from check-in sequences and comprehensively understand human visiting intentions and travel preferences. To address this challenge, we present a novel unified framework called Mobility-LLM for various check-in sequence analysis tasks. It leverages pre-trained LLMs for general check-in sequence analysis. Our contributions can be summarized as follows: • We propose a unified framework called Mobility-LLM that uses a pre-trained LLM to achieve a SOTA or comparable performance across various check-in analysis tasks including location prediction, trajectory user link, and time prediction. We extract the semantics of check-in sequences to enable LLMs to gain a comprehensive understanding of human visiting intentions and travel preferences. • A visiting intention memory network (VIMN) is proposed for capturing users’ visiting intentions of users at each check-in record by prioritizing relevant check-in records. • A shared pool of human travel preference prompts (HTPP) in different domains is introduced, which enables a comprehensive understanding of human travel preferences and matches appropriate prompts from multiple domains. • Our model’s exceptional performance is validated through extensive experiments on four benchmark datasets involving three tasks. Our robust outcomes in cross-domain pre-training exhibit an average enhancement of 17.8% and an average of 23.6% to 38.3% on the few-shot scenario."
https://arxiv.org/html/2411.00820v1,AutoGLM: Autonomous Foundation Agents for GUIs,"We present AutoGLM, a new series in the ChatGLM family [11], designed to serve as foundation agents for autonomous control of digital devices through Graphical User Interfaces (GUIs). While foundation models excel at acquiring human knowledge, they often struggle with decision-making in dynamic real-world environments, limiting their progress toward artificial general intelligence. This limitation underscores the importance of developing foundation agents capable of learning through autonomous environmental interactions by reinforcing existing models. Focusing on Web Browser and Phone as representative GUI scenarios, we have developed AutoGLM as a practical foundation agent system for real-world GUI interactions. Our approach integrates a comprehensive suite of techniques and infrastructures to create deployable agent systems suitable for user delivery. Through this development, we have derived two key insights: First, the design of an appropriate ""intermediate interface"" for GUI control is crucial, enabling the separation of planning and grounding behaviors, which require distinct optimization for flexibility and accuracy respectively. Second, we have developed a novel progressive training framework that enables self-evolving online curriculum reinforcement learning for AutoGLM. Our evaluations demonstrate AutoGLM’s effectiveness across multiple domains. For web browsing, AutoGLM achieves a 55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second attempt) and 96.2% on OpenTable evaluation tasks. In Android device control, AutoGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7% on common tasks in popular Chinese APPs. Select AutoGLM capabilities are now available through the Qingyan Browser Plugin for web applications and via Form Applications for invited Android testing. Additional results and materials will be released at https://github.com/THUDM/AutoGLM.","Foundation models, including Large Language Models (LLMs) [5; 27; 7; 2; 42; 11] and Large Multimodal Models (LMMs) [20; 25; 26; 1], have captured widespread attention for their remarkable language understanding and generation capabilities. Through extensive self-supervised [22] pre-training on internet-scale corpora, these models have acquired not only knowledge and language abilities but also human-like reasoning and planning capabilities, giving rise to LLMs as Agents [21; 28]. These agents have demonstrated their utility across diverse domains, including coding [35; 16; 44], data analysis [14; 21], and gaming [34; 18], charting a promising course toward Artificial General Intelligence (AGI) through the development of multimodal Foundation Agents [23] that serve as generalists across multiple tasks and environments. Figure 1: AutoGLM real phone use demonstration for instruction “Order a hot coconut latte from Luckin Coffee with half sugar” for Chinese Android APP Meituan. See more videos. Figure 2: AutoGLM real web-browser use demonstration for instruction “Could you help me book a reservation for my parents and I at Megan’s Kitchen on October 23, 2024 at 7:30 PM through OpenTable? You can reach me at 146xxxxxxxx, China.” on the website OpenTable. See more videos. The ubiquity of digital devices presents a unique opportunity for GUI-capable agents [13; 46; 43; 17]. This domain offers several advantages: GUI simulators can be readily deployed in parallel for data annotation and online reinforcement learning (RL); GUI environments provide rich textual and visual inputs essential for foundation model agents, but in a safer and controllable environments compared to embodied environments; and GUI agents hold broad practical appeal given their extensive potential user base. Their successful development could fundamentally transform human-device interaction. However, the development of foundation agents for GUI faces a critical challenge: the scarcity of decision-making data in existing pre-training sets. While the internet contains vast human knowledge, it primarily consists of static information that inadequately captures human decision-making and environmental interaction. Building capable foundation agents requires enriching them with dynamic knowledge, either through direct interaction with real-world environments or through learning from synthesized trajectories. Such foundation agents can then self-evolve in the digital world, iteratively improving to achieve genuine general intelligence. Crucially, these systems must be developed with progressive user deployment in mind. Autonomous agents are designed to augment, not replace, human capabilities. User deployment serves the dual purpose of teaching agents effective human assistance while allowing humans to adapt to intelligent assistants. This approach also enables researchers to systematically understand, discover, and examine both the potential benefits and risks of autonomous foundation agents during development. In response to these opportunities and challenges, we introduce AutoGLM, a series of foundation agents built upon the ChatGLM [11] model family. AutoGLM represents a pioneering attempt to develop foundation agent prototypes for two fundamental GUI scenarios: Web Browser and Android. To address the data scarcity challenge, we employ a comprehensive suite of training techniques and develop key infrastructures for user deployment. This process has yielded two crucial insights: • Intermediate Interface Design: We find it essential to design an intermediate interface that disentangles planning and grounding behaviors in foundation GUI agents. They present distinct requirements – planning demands flexibility and error recovery, while grounding emphasizes action accuracy. Their separation enables more agile development and enhanced performance. • Self-Evolving Online Curriculum RL [30]: We recognize that error recovery [23] is crucial for robust and deployable agent applications, yet it remains difficult to acquire through offline training alone. Additionally, the shortage of instructions and trajectories impedes training progress. We address this challenge through self-evolving RL, implemented according to a progressive weak-to-strong curriculum schedule in an online manner. Building on these insights, AutoGLM demonstrates exceptional capabilities across various benchmarks and real-world tests. In Web Browsing, AutoGLM achieves a task success rate (SR) of 55.2% on the challenging VAB-WebArena-Lite [47; 23], substantially surpassing GPT-4o’s 18.2%. With a second attempt opportunity, this improves to 59.1%. On OpenTable real-world booking tasks, AutoGLM achieves 96.2% SR, outperforming both GPT-4o (62.6% SR) and Agent Q [29] (81.7%). Select AutoGLM web capabilities are publicly available via the Qingyan Browser Plugin on both Chrome and Edge Plugin Store. See the real example in Figure 2. For Android control, AutoGLM achieves 36.2% SR on AndroidLab [37] (previously known as VAB-Mobile [23]), a comprehensive interactive Android evaluation framework. This performance exceeds both GPT-4o (31.2% SR) and Claude-3.5-Sonnet (29.0% SR). We have also implemented a practical Android application via AccessibilityService for autonomous device control. In human evaluation, AutoGLM achieves an impressive 89.7% SR on common tasks (e.g., “Please Order a large iced Americano with half sugar from the nearest coffee shop for delivery to my company”) across popular Chinese APPs. The Android client is currently available for invited internal testing through Form Applications. See the real example in Figure 2."
https://arxiv.org/html/2411.00813v1,Personality Analysis from Online Short Video Platforms with Multi-domain Adaptation,"Personality analysis from online short videos has gained prominence due to its applications in personalized recommendation systems, sentiment analysis, and human-computer interaction. Traditional assessment methods, such as questionnaires based on the Big Five Personality Framework, are limited by self-report biases and are impractical for large-scale or real-time analysis. Leveraging the rich, multi-modal data present in short videos offers a promising alternative for more accurate personality inference. However, integrating these diverse and asynchronous modalities poses significant challenges, particularly in aligning time-varying data and ensuring models generalize well to new domains with limited labeled data. In this paper, we propose a novel multi-modal personality analysis framework that addresses these challenges by synchronizing and integrating features from multiple modalities and enhancing model generalization through domain adaptation. We introduce a timestamp-based modality alignment mechanism that synchronizes data based on spoken word timestamps, ensuring accurate correspondence across modalities and facilitating effective feature integration. To capture temporal dependencies and inter-modal interactions, we employ Bidirectional Long Short-Term Memory networks and self-attention mechanisms, allowing the model to focus on the most informative features for personality prediction. Furthermore, we develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to improve performance in target domains with scarce labeled data. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms existing methods in personality prediction tasks, highlighting its effectiveness in capturing complex behavioral cues and robustness in adapting to new domains. We open our datasets and code at https://github.com/Anne6645/personality_analysis","Personality analysis has long been a central topic in psychological science and has gained increasing importance in recent years due to its wide-ranging applications. It plays a crucial role in various domains such as personalized recommendation systems [1, 2], sentiment analysis [3, 4], and human-computer interaction. Accurately identifying an individual’s personality can enable tailored experiences and services, enhancing user satisfaction and engagement. However, personality traits are inherently latent characteristics that are not directly observable, making the assessment of personality a challenging task. Traditionally, psychologists have employed structured methods to evaluate an individual’s personality. One of the most widely accepted models is the Big Five Personality Traits (as shown in Figure 1), which assesses personality across five key dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism. To determine an individual’s position on these dimensions, conventional approaches often rely on well-designed questionnaires and psychological inventories that analyze self-reported responses. While these methods are grounded in rigorous psychometric principles, they have notable limitations. Self-reported data can be influenced by social desirability bias, where respondents tailor their answers to be viewed favorably. Additionally, administering and processing these surveys can be time-consuming and resource-intensive, making them less practical for large-scale or real-time applications. Figure 1: Big Five Personality Traits With the advent of online video social platforms like TikTok111https://www.tiktok.com/ and others, there is a growing opportunity to analyze personality traits through digital means. Users increasingly share selfie videos online, providing a wealth of data that captures not only their visual appearance but also their speech patterns, facial expressions, and environmental context. Compared to static questionnaires, these multi-modal data offer richer insights into an individual’s intrinsic traits. Unlike traditional social media platforms that primarily feature text or images, video platforms enable the observation of dynamic behaviors and interactions, which are crucial for understanding personality. This shift opens up new possibilities for applications such as online job interviews, remote education, and personalized content delivery, where assessing personality from videos can significantly enhance outcomes. Recent research has begun to explore the potential of analyzing personality traits through online media instead of traditional surveys. Behavioral observations from personal photographs [5, 6, 7] and short videos [8] have been utilized to glean personality insights. For instance, [5] analyzed Facebook profile pictures to infer personality traits, while [6] and [7] leveraged social media images for similar purposes. However, photograph-based approaches have limitations, as individuals often curate their online images, sharing selective moments that may not accurately represent their typical behaviors or personality, leading to biased data and potentially inaccurate predictions. In contrast, short videos provide a more comprehensive medium for personality analysis. They capture changes in facial expressions, body movements, speech patterns, and contextual scenes—all of which are significant indicators in psychological assessments of personality. Recognizing this, researchers have started to model the audio, visual, and textual features present in short videos [9, 10, 11]. For example, [9] developed a Deep Bimodal Regression model combining audio and visual modalities to predict scores on the Big Five personality traits. Similarly, [10] employed convolutional neural networks to extract visual features and linear regression for audio features, while [11] conducted an in-depth analysis using logistic regression on audio, video, and text features. Figure 2: Comparison of Existing Works Despite these advances, existing multi-modal personality prediction methods often rely on large volumes of high-quality short videos with high-resolution visuals and clear audio to achieve satisfactory performance. Moreover, many approaches depend heavily on supervised learning techniques that require extensive labeled datasets. Collecting and annotating such multi-modal data is both expensive and time-consuming. Manual annotation introduces the potential for subjectivity and inconsistency, which can affect the reliability of the analysis. Consequently, detecting personality traits from online video platforms presents significant challenges, particularly in the following areas: The First Challenge is identifying the most important features from multiple modalities to optimize the use of a limited number of short videos for accurate personality analysis. The complexity arises from the need to effectively integrate diverse data types—visual cues, auditory signals, textual content, and contextual information—each contributing uniquely to personality inference. Existing methods may not adequately address the alignment and synchronization of these modalities, leading to fragmented or incomplete representations that hinder predictive accuracy. The Second Challenge is effectively utilizing a small number of high-quality short videos to achieve strong generalization in personality analysis. Models trained on specific datasets may struggle to generalize across different domains due to variations in cultural contexts, linguistic expressions, and recording conditions. The scarcity of labeled data in new or underrepresented domains exacerbates this issue, limiting the applicability of the models in real-world scenarios where data diversity is the norm. In this paper, we propose an effective multi-modal personality analysis framework designed to overcome these challenges. To address the first challenge, we introduce a semantic unit method for feature extraction and alignment, which synchronizes multi-modal data based on spoken words. This ensures that features from different modalities correspond accurately at each moment in the video, facilitating effective integration. Within this module, we employ self-attention mechanisms to discern the significance of features across various modalities. By assigning weights to features based on their relevance to personality prediction, the model focuses on the most informative aspects of the data, enhancing analytical accuracy. To tackle the second challenge, we propose a multi-domain adaptation method that transfers domain knowledge across multiple domains to alleviate the data sparsity problem. This approach leverages information from data-rich source domains to enhance learning in data-scarce target domains. By computing gradient similarities between source and target domains, our model adapts to emphasize learning from source domains that are most relevant to the target domain. This method improves the model’s generalization capabilities, enabling more accurate predictions even when limited data is available in certain domains. Our main contributions are summarized as follows: • We propose an effective multi-modal personality analysis framework that effectively integrates facial expressions, audio signals, textual content, and background information from short videos for personality prediction. • We introduce a semantic unit modality alignment mechanism that synchronizes multi-modal data based on spoken word timestamps, ensuring accurate correspondence across modalities and enhancing feature representation. • We develop a gradient-based domain adaptation method that transfers knowledge from multiple source domains to target domains with limited labeled data, enhancing model generalization and performance in few-shot learning scenarios. • We validate the effectiveness of our proposed framework through extensive experiments on real-world datasets, demonstrating significant improvements over existing methods in personality prediction tasks. By addressing both the feature integration and domain adaptation challenges, our framework advances in personality analysis from online short videos. The rest of the paper is organized as follows. Section II introduces the related work of personality analysis and domain adaption. The problem definition is in Section III. Section IV elucidates the detailed methodology, and Section V presents the results of the experiments and the analysis. The last section is the conclusion of the paper."
https://arxiv.org/html/2411.00809v1,Adaptive Dense Reward: Understanding the Gap Between Action and Reward Space in Alignment,"Reinforcement Learning from Human Feedback (RLHF) has proven highly effective in aligning Large Language Models (LLMs) with human preferences. However, the original RLHF typically optimizes under an overall reward, which can lead to a suboptimal learning process. This limitation stems from RLHF’s lack of awareness regarding which specific tokens should be reinforced or suppressed. Moreover, conflicts in supervision can arise, for instance, when a chosen response includes erroneous tokens, while a rejected response contains accurate elements. To rectify these shortcomings, increasing dense reward methods, such as step-wise and token-wise RLHF, have been proposed. However, these existing methods are limited to specific tasks (like mathematics). In this paper, we propose the “Adaptive Message-wise RLHF” method, which robustly applies to various tasks. By defining pivot tokens as key indicators, our approach adaptively identifies essential information and converts sample-level supervision into fine-grained, subsequence-level supervision. This aligns the density of rewards and action spaces more closely with the information density of the input. Experiments demonstrate that our method can be integrated into various training methods, significantly mitigating hallucinations and catastrophic forgetting problems while outperforming other methods on multiple evaluation metrics. Our method improves the success rate on adversarial samples by 10% compared to the sample-wise approach and achieves a 1.3% improvement on evaluation benchmarks such as MMLU, GSM8K, and HumanEval et al.","Recently, large language models have achieved remarkable strides across varied fields, sparking widespread speculation about the emergence of artificial general intelligence and agents surpassing human capabilities. Reinforcement learning plays a crucial role in today’s large model training. The numerous advantages of reinforcement learning methods make it an essential path toward agents more powerful than humans, such as obtaining annotation capabilities superior to human annotators through model interaction and better modeling of complex objectives that are difficult to quantify(Yang et al. (2024a); Dubey et al. (2024)). Reinforcement learning from human feedback (RLHF) significantly improved the performance of large AI models(Ouyang et al. (2022)). Its impressive results and relatively expensive process have inspired other methods like DPO(Rafailov et al. (2024b)) and rejection sampling(Touvron et al. (2023)), which can be flexibly used in online or offline modes. These mainstream methods predominantly rely on sample-level supervision signals, which, from the perspective of classic reinforcement learning and control theory, exhibit inconsistency between reward signals and action spaces. In large language model training, this phenomenon is often conceptualized as an adaptive credit assignment in multi-armed bandits(Dudík et al. (2015); Rafailov et al. (2024a)), which frequently introduces systematic errors during training. For instance, when dealing with negative samples, colloquialisms or special format characters should not have their logits excessively reduced. Another example can be found in online on-policy iterative training that positive samples may contain processes that are largely correct but have minor issues. These small imperfections can lead to an increase in hallucinations, and make this training method sometimes less effective than iterative training(Dubey et al. (2024)). Figure 1: For general tasks, especially in low-information statement, long context(i.e., writing articles or RAG applications), step-wise supervision is significantly less accurate than sample-wise supervision.Left and middle shows that the performance of Outcome-supervised better than Process-supervised for general tasks,even when well-trained PRMs(Appendix:A)were used. ORM:Outcome supervised Reward Model.PRM:Process supervised Reward Model.P⁢R⁢MO𝑃𝑅subscript𝑀𝑂PRM_{O}italic_P italic_R italic_M start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT:Outcome supervised by PRMs.P⁢R⁢MP𝑃𝑅subscript𝑀𝑃PRM_{P}italic_P italic_R italic_M start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT:Process supervised by PRMs. Denser supervision methods, such as step-by-step verified methods(Lightman et al. (2023b); Pan et al. (2023); Uesato et al. (2022b)) and step-DPO(Lai et al. (2024b)), face limitations in scaling to diverse dialogue generation tasks. In dialogue generation tasks, dividing by specific symbols often fails to capture truly useful steps, and may even result in lower accuracy than sample-level supervision signals(figure:1). For instance, contexts and inversions often appear in human conversations, such as ”A bee is a kind of fruit, which is incorrect”; In code completion tasks, the critical information is concentrated at the specific completion locations. Figure 2: Token-wise rewards exhibit significant fluctuations and high noise levels, leading to unstable training. The densest supervision methods, such as token-level DPO(Zeng et al. (2024b); Rafailov et al. (2024a)), demonstrate significant shortcomings on tasks that challenge the model’s capabilities. Because rewards at the token level often have variances 1-2 orders of magnitude higher than sample-level supervision signals. These limitations become extremely apparent in tasks where the model lacks proficiency, such as complex mathematical problems or humor. Additionally, this method relies heavily on knowledge consistency between the reward model and actor model. Even subtle stylistic differences between them can lead to dramatic fluctuations in token-level scoring, making it almost impossible to perform off-policy or offline training. To address these challenges, we propose a novel message-level method: ”Adaptive Message-wise RLHF”. This approach identifies key signals through rewards and generalized advantages during the generation process, allowing the model to adaptively partition samples. The resulting sub-samples offer flexible control over gradient propagation through various methods. This adaptability in gradient management enhances the model’s learning capabilities and reduces model hallucinations. Figure 3: Adaptive Message-wise RLHF:We divide sequence based on token rewards in the preference data, rather than using a manually divided step-wise approach. We can train the model by masking certain sub-sequences or applying different rewards to various sub-sequences. This approach is closer to actual density than step-wise and token-wise methods."
https://arxiv.org/html/2411.00788v1,KeyInst: Keyword Instruction for Improving SQL Formulation in Text-to-SQL,"Text-to-SQL parsing involves the translation of natural language queries (NLQs) into their corresponding SQL commands. A principal challenge within this domain is the formulation of SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. However, the intrinsic disparity between the NLQ and the SQL poses a significant challenge. In this research, we introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by Large Language Models (LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query, thus facilitates a smoother SQL query formulation process. We explore two strategies for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. The former first generates KeyInst for question, which are then used to prompt LLMs. The latter employs a fine-tuned model to concurrently generate KeyInst and SQL in one step. We developed StrucQL, a benchmark specifically designed for the evaluation of SQL formulation. Extensive experiments on StrucQL and other benchmarks demonstrate that KeyInst significantly improves upon the existing Text-to-SQL prompting techniques.","The task of Text-to-SQL parsing, which aims at translating natural language questions into executable SQL queries, has gained increasing attention in recent years, as it can help non-expert users quickly access information in the database without the need for technical background Deng et al. (2021); Yu et al. (2020); Rajkumar et al. (2022); Ni et al. (2023). Text-to-SQL parsing faces two main challenges: schema linking and SQL formulation. Schema linking involves identifying the pertinent tables and columns in a database schema in response to an NLQ. SQL formulation refers to generating SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. This paper primarily focuses on the challenge of SQL formulation. Currently, most Text-to-SQL prompting methods induce Large Language Models (LLMs) to generate the target SQL directly using In-context Learning (ICL) Nan et al. (2023); Pourreza and Rafiei (2024a); Tan et al. (2024). However, the vast difference between natural language queries (NLQ) and SQL hinders precise query formulation. In previous works, the skeleton-aware decoder Li et al. (2023) was proposed to alleviate this challenge by initially generating an SQL skeleton followed by the full query. An SQL skeleton is a basic framework of an SQL query consisting of SQL operators, without specific details such as column names, table names, or conditions. Incorporating SQL skeleton in prompting has also proven to be effective Gao et al. (2023); Guo et al. (2023). In this work, we also use the SQL structure as a central element in SQL formulation, with a particular emphasis on identifying key SQL operators. For instance, in translating the NLQ ""List the customers’ first and last names from the 10 least expensive invoices"", accurately identifying ORDER BY and LIMIT is crucial for formulating the correct SQL query. Figure 1: Graphical illustration of KeyInst and its applications: A. An example of schema, question, KeyInst, and SQL, B. The pipeline approach of KeyInst application, C. The single-pass approach of KeyInst application. We introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by LLMs. KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query. Recognizing that SQL queries corresponding to different NLQs require distinct keywords, KeyInst adapts dynamically to each query. An example of KeyInst in action is depicted in Figure 1A, demonstrating how it analyzes a given NLQ and deduces the critical SQL keywords. This strategy effectively narrows the gap between NLQ and SQL, facilitating a smoother SQL query formulation process. While KeyInst significantly aids in SQL query formulation, further exploration is needed on its generation and integration into Text-to-SQL parsing. We present two approaches for KeyInst generation: a model fine-tuning method and an ICL-based method. The former fine-tunes a model to produce KeyInsts for specific queries, while the latter prompts LLMs to generate KeyInsts through ICL Brown et al. (2020). For the application of KeyInst in Text-to-SQL tasks, we also investigate two strategies. The first strategy prompts LLMs to produce SQL queries with KeyInst. The second strategy is a fine-tuning strategy that generates SQL queries directly following KeyInst generation, treating KeyInst creation as a preliminary reasoning step. To amalgamate KeyInst generation and application within Text-to-SQL, we introduce a two-fold strategy. The pipeline approach initially generates KeyInst using either the fine-tuned or ICL-based method, followed by prompting LLMs with the generated KeyInst, as illustrated in Figure 1B. Conversely, the single-pass approach employs a fine-tuned model to concurrently generate KeyInst and SQL in one step, as depicted in Figure 1C. Several benchmarks, such as Spider Yu et al. (2018) and Bird Li et al. (2024b), have been developed to assess Text-to-SQL systems. However, these benchmarks focus on overall parsing performance and lack mechanisms for isolating evaluations of semantic linking and SQL formulation. To specifically assess SQL formulation capabilities, a new benchmark called StrucQL (Structural Benchmark for Text-to-SQL) has been developed, derived from Spider. In StrucQL, questions and schemas are simplified: questions explicitly mention schema items, and irrelevant tables and columns are omitted from the schema. This simplification makes schema linking straightforward, shifting the primary challenge to SQL formulation. Consequently, StrucQL serves as an effective tool for evaluating SQL formulation proficiency in Text-to-SQL systems. KeyInst was assessed on StrucQL and other benchmarks, with outcomes indicating that keyword instructions are a valuable intermediary for Text-to-SQL parsing, whether applied independently or in conjunction with other techniques. The main contributions of this work are summarized as follows: • We propose KeyInst, a keyword instruction tailored for each Text-to-SQL task, to alleviate SQL formulation challenge. We offer two approaches for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. • The StrucQL benchmark was developed to specifically assess the SQL formulation abilities of Text-to-SQL systems. By simplifying questions and schemas, StrucQL eliminates schema linking challenges, focusing evaluation on SQL formulation performance. • Comprehensive experiments across various benchmarks were conducted. The findings demonstrate that KeyInst significantly improves upon the existing state-of-the-art Text-to-SQL prompting techniques, showcasing its effectiveness and potential."
https://arxiv.org/html/2411.00784v1,Fire: Fact-checking with Iterative Retrieval and Verification,"Fact-checking long-form text is challenging, and it is therefore common practice to break it down into multiple atomic claims. The typical approach to fact-checking these atomic claims involves retrieving a fixed number of pieces of evidence, followed by a verification step. However, this method is usually not cost-effective, as it underutilizes the verification model’s internal knowledge of the claim and fails to replicate the iterative reasoning process in human search strategies. To address these limitations, we propose Fire, a novel agent-based framework that integrates evidence retrieval and claim verification in an iterative manner. Specifically, Fire employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment. We compare Fire with other strong fact-checking frameworks and find that it achieves slightly better performance while reducing large language model (LLM) costs by an average of 7.6 times and search costs by 16.5 times. These results indicate that Fire holds promise for application in large-scale fact-checking operations. Our code is available at https://github.com/mbzuai-nlp/fire.git.","“Every man has a right to his opinion, but no man has a right to be wrong in his facts.” - Bernard M. Baruch Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks, including both language comprehension and generation (Zhao et al., 2023; Xie et al., 2023a). Consequently, LLMs are now widely applied in various domains (Xie et al., 2023b), and many users increasingly rely on the information they provide. However, this reliance is problematic, as LLMs are capable of producing outputs that are highly confident but factually incorrect, highlighting the critical need for robust fact-checking systems (Akhtar et al., 2023). However, fact-checking the entire output of LLMs in a single step is highly challenging. To address this, Min et al. (2023) proposed decomposing the content into multiple atomic claims, each of which can be individually verified. While this approach simplifies the fact-checking process, assessing the veracity of these atomic claims remains complex, especially when many require sourcing evidence from the web. Indeed, identifying the most relevant evidence online is a key challenge in fact-checking pipelines (Wang et al., 2023). To address this issue, conventional methods, such as FacTool and Factcheck-GPT (Chern et al., 2023; Wang et al., 2023), frame the problem as a question-answering task, as illustrated on the left side of Figure 1. In these approaches, an LLM is prompted to generate N relevant questions, which are then used as search queries by a web search tool. The search results serve as evidence for LLM to determine the factuality of the claim. However, we argue that this process is inefficient in two key aspects. First, it underutilizes the internal knowledge already embedded in LLMs during pre-training. For claims involving common knowledge or widely known events, the LLM could confidently assess the claim without relying on external information. Second, generating multiple search queries concurrently does not align with the typical human reasoning process during search (Hu et al., 2023). Humans tend to begin with an initial query, gather information, and then refine their perspective on the claim, which often leads to the formulation of more effective follow-up queries. Figure 1: Comparisons between Fire and previous frameworks. Previous frameworks typically treat web search and claim verification as distinct processes. In contrast, Fire integrates interactive retrieval and verification. To address this gap, we introduce Fact-checking with Iterative Retrieval and VErification (Fire), an innovative agent-based framework that integrates both the internal knowledge of LLMs and external knowledge sources by unifying the verification process and search query generation into a single step. As illustrated on the right side of Figure 1, Fire employs a mechanism to decide whether to produce the final answer or generate a new search query, continuing the evidence-seeking process. This decision is based on the model’s confidence in its judgment. The closest related work to us is Safe (Wei et al., 2024), depicted in the center of Figure 1. Their method generates web search queries iteratively and subsequently verifies whether the entire retrieved evidence supports the claim. However, this approach lacks flexibility, as it treats evidence retrieval and claim verification as distinct processes, requiring a predetermined fixed number of searches regardless of the claim’s complexity. In contrast, our approach integrates evidence retrieval and claim verification into an iterative framework, encouraging the language model to verify based on its own knowledge and conduct searches only when necessary. Our experiments demonstrate that our method significantly reduces the computational costs of LLMs by an average factor of 7.6, as well as search-related costs by a factor of 16.5, all while maintaining fact-checking performance. In summary, our contributions are as follows: • We present Fire, a simple yet effective interactive framework for fact-checking. Through extensive experiments conducted across multiple datasets, we demonstrate that our framework significantly reduces the LLM computational and search costs, making it a better option for large-scale production. • Our ablation studies demonstrate that the step-by-step reasoning process enhances the model’s confidence in fact-checking, particularly with GPT-4o-mini. For GPT-4o, we observed a similar trend; however, the effect was not as pronounced as that seen with GPT-4o-mini. • We conducted an error analysis and identified several quality issues in the current benchmark datasets, including the presence of ungrounded claims. Additionally, the strict reasoning capabilities of the LLM may incorrectly classify some debatable claims as non-factual."
https://arxiv.org/html/2411.00781v1,Hazards in Daily Life?Enabling Robots to Proactively Detect and Resolve Anomalies,"Existing household robots have made significant progress in performing routine tasks, such as cleaning floors or delivering objects. However, a key limitation of these robots is their inability to recognize potential problems or dangers in home environments. For example, a child may pick up and ingest medication that has fallen on the floor, posing a serious risk. We argue that household robots should proactively detect such hazards or anomalies within the home, and propose the task of anomaly scenario generation. We leverage foundational models instead of relying on manually labeled data to build simulated environments. Specifically, we introduce a multi-agent brainstorming approach, where agents collaborate and generate diverse scenarios covering household hazards, hygiene management, and child safety. These textual task descriptions are then integrated with designed 3D assets to simulate realistic environments. Within these constructed environments, the robotic agent learns the necessary skills to proactively discover and handle the proposed anomalies through task decomposition, and optimal learning approach selection. We demonstrate that our generated environment outperforms others in terms of task description and scene diversity, ultimately enabling robotic agents to better address potential household hazards.","The development of Vision-Language Models (VLMs) has significantly improved household robots’ ability to interact with the physical world in a more human-like manner Liu et al. (2024b, a); Cai et al. (2023); Majumdar et al. (2024). Among these models, the most popular paradigm for such robots is receiving instructions and performing corresponding operational tasks Yang et al. (2024); Driess et al. (2023); Ahn et al. (2022). However, a critical yet often overlooked scenario arises when no instructions are provided. According to survey data, 31% of cooking fires are caused by unattended equipment Ahrens (2020). Meanwhile, unintentional injuries are the predominant cause of death among children, particularly those aged 1-14 years, encompassing incidents such as drowning, falls, and accidental poisonings Worldwide (2022). A lack of adequate supervision is often identified as a significant contributor to many of these fatalities, especially in cases involving younger children Hymel et al. (2006); Williams and Kotch (2023). It would greatly benefit humans if household robots could monitor whether stoves and other fire sources are properly turned off and detect potential hazards in the home that could lead to falls or accidental poisonings. Many of these fires and unintentional injuries could be prevented. However, to the best of our knowledge, such robotics have yet to be implemented. Figure 1: Comparison of passively instructed robots and our proactive detection robot. Our paradigm creates benefits and convenience for safety, even in the absence of human presence. Hence, in this work, we propose AnomalyGen, which can generate diverse anomaly settings covering household hazards, hygiene management, and child safety in 3D simulation environments, enabling robots to develop proactive detection and problem-solving abilities, as shown in Figure 1. Specifically, we first devised a group brainstorming setting, where LLM-based agents collaborate to generate diverse and comprehensive anomaly scenarios. The motivation comes from the observation that simply prompting an LLM to generate hazard scenarios results in repetitive and similar settings. In contrast, group brainstorming in real-life meetings often leads to novel and creative ideas. Based on these task settings, AnomalyGen automatically constructs simulated anomalous scenes through carefully designed 3D asset retrieval, configuration, and scene setup steps. Finally, AnomalyGen guides household robots in developing detection and resolution abilities for handling anomalies. It reads textual descriptions of the simulated environment, including the 3D coordinates of assets, and automatically identifies potential anomalous tasks that require attention. AnomalyGen then decomposes the task into fine-grained sub-tasks and selects the most appropriate learning method for the household robot. In general, our AnomalyGen leverages language-based approaches to bridge the domain gap between foundational models and robot interaction, enabling operations such as control inputs, operational trajectories, and physical interaction. For the experiments, AnomalyGen constructs 111 diverse and comprehensive anomaly scenes, with human evaluation showing high quality and automatic metrics demonstrating greater diversity compared to previous human-crafted robotic datasets. Based on this simulation data, household robots are guided by AnomalyGen to learn and demonstrate a variety of skills across tasks such as rigid and articulated object manipulation and legged locomotion, achieving a task completion rate of 83%. Additionally, we conduct an error analysis highlighting the limitations of the current learning algorithm and VLM, identifying areas for future improvement and direction. Our contributions can be summarized as follows: Firstly, we introduce AnomalyGen, an unsupervised generative framework that enables household robots to autonomously detect and address anomalies without explicit instructions. Secondly, AnomalyGen creates a 3D simulation environment with 111 diverse hazard scenarios, generated through a collaborative brainstorming mechanism, significantly enhancing task diversity compared to previous datasets. Thirdly, AnomalyGen enables robots to autonomously identify anomalies, decompose tasks, and learn appropriate skills using an effective task decomposition and learning method with minimal human input."
https://arxiv.org/html/2411.00750v1,Mitigating Tail Narrowing in LLM Self-Improvementvia Socratic-Guided Sampling,"Self-improvement methods enable large language models (LLMs) to generate solutions themselves and iteratively train on filtered, high-quality rationales. This process proves effective and reduces the reliance on human supervision in LLMs’ reasoning, but the performance soon plateaus. We delve into the process and find that models tend to over-sample on easy queries and under-sample on queries they have yet to master. As iterations proceed, this imbalance in sampling is exacerbated, leading to a long-tail distribution where solutions to difficult queries almost diminish. This phenomenon limits the performance gain of self-improving models. A straightforward solution is brute-force sampling to balance the distribution, which significantly raises computational costs. In this paper, we introduce Guided Self-Improvement (GSI), a strategy aimed at improving the efficiency of sampling challenging heavy-tailed data. It leverages Socratic-style guidance signals to help LLM reasoning with complex queries, reducing the exploration effort and minimizing computational overhead. Experiments on four models across diverse mathematical tasks show that GSI strikes a balance between performance and efficiency, while also being effective on held-out tasks111 Codes are publicly available at https://github.com/Yiwen-Ding/Guided-Self-Improvement..","Figure 1: Illustration of distribution during the self-improvement sampling process. Top: The long-tail effect intensifies with iterative training on self-generated data. The low-probability data begins to diminish, leading to tail narrowing. Bottem: Guided sampling balances the distribution by improving tail data sampling efficiency. Large language models (LLMs) have demonstrated impressive ability in performing complex reasoning tasks (Wei et al., 2022b; Kojima et al., 2022; Zhao et al., 2023). While fine-tuning models on curated data can further boost performance, it relies heavily on human supervision, limiting scalability and generalization (Cobbe et al., 2021). To address this, the “self-improvement” paradigm emerges, where models generate multiple reasoning paths, filter out incorrect responses and fine-tune themselves on their own outputs without human intervention (Zelikman et al., 2022; Gülçehre et al., 2023; Huang et al., 2023; Singh et al., 2024; Yuan et al., 2024). Despite the benefits of self-improvement, its performance typically reaches a ceiling after a few iterations (Wu et al., 2024). We perform preliminary experiments (§ 4) and find that in reasoning tasks, the most significant gains from self-improvement occur in the first iteration, while subsequent iterations encounter performance bottlenecks or even degradation (Figure 2). Similar performance bottlenecks in synthetic data have also been observed in text generation (Shumailov et al., 2023) and image synthesis (Alemohammad et al., 2024). Further, we delve into the self-improvement process and conduct an in-depth analysis (Figure 3) to investigate the underlying causes behind the performance bottlenecks. On the one hand, complex problems with lengthy reasoning chains tend to amplify hallucinations, making it difficult for models to explore the vast search space and sample correct rationales (Lightman et al., 2024; Zhang et al., 2023; Xie et al., 2023; Xi et al., 2024). Consequently, the models tend to over-sample easy queries and under-sample queries they have yet to master (Tong et al., 2024). On the other hand, as iterations proceed, this imbalance in sampling is exacerbated, leading to a long-tail distribution where solutions to difficult queries almost disappear (Figure 1). This situation is also referred to as tail narrowing or tail cutting in previous studies (Dohmatob et al., 2024b; Shumailov et al., 2023). As a result, the model’s self-improvement is limited, since difficult examples are also crucial for further training (Liu et al., 2024). To address this imbalance, a common approach is to allocate more sampling budget to the under-sampled, challenging queries (Tong et al., 2024), but this can be much more costly. In this paper, we propose an efficient and effective method called Guided Self-Improvement (GSI), which employs Socratic-style guidance signals (Chang, 2023; Dong et al., 2023b) to assist language models in exploring solutions for challenging queries. Specifically, we introduce an extra resampling phase called distribution re-balancing for difficult queries, applied after the generation step in the self-improvement process. During this phase, we provide targeted guidance signals to reduce sampling difficulty, narrow the sampling space, and minimize hallucinations during reasoning (Xie et al., 2023; Xi et al., 2024). As a result, GSI improves sampling quality, increases solution coverage (Bansal et al., 2024) for challenging queries, and mitigates the issue of tail narrowing. We perform experiments across four models and six mathematical reasoning tasks, including arithmetic reasoning, abstract algebra, and formal logic. The results demonstrate that GSI mitigates the performance bottlenecks of self-improvement while maintaining computational efficiency. Further analysis shows that this method leads to a more balanced solution distribution and improved model generalization across multiple reasoning tasks. In addition to natural language reasoning, our method has also been proven effective in program-based reasoning (Chen et al., 2023). Our contributions are summarized as follows: • We conduct an in-depth study of the self-improvement process, revealing the performance bottlenecks driven by a long-tail distribution of solutions, which results from increasingly imbalanced data sampling. • To efficiently mitigate the problem of tail narrowing, we introduce the Guided Self-Improvement (GSI) method that employs Socratic-style guidance signals to assist models in exploring solutions for challenging queries. • We validate our strategy through comprehensive experiments on four backbone models across six mathematical reasoning tasks, demonstrating the effectiveness and efficiency of GSI. Figure 2: Iterative performance in the self-improvement. Experiments are conducted on GSM8K with varying sampling numbers k𝑘kitalic_k. Solid markers show the performance of vanilla self-improve, with the solid line fitting these points. The performance plateaus after a few iterations. Hollow markers represent the performance after supplementing tail data, with a dashed line trend. It balances the distribution and alleviates performance bottlenecks."
https://arxiv.org/html/2411.00727v1,SPRING Lab IITM’s submission to Low Resource Indic Language Translation Shared Task,"We develop a robust translation model for four low-resource Indic languages: Khasi, Mizo, Manipuri, and Assamese. Our approach includes a comprehensive pipeline from data collection and preprocessing to training and evaluation, leveraging data from WMT task datasets, BPCC, PMIndia, and OpenLanguageData. To address the scarcity of bilingual data, we use back-translation techniques on monolingual datasets for Mizo and Khasi, significantly expanding our training corpus. We fine-tune the pre-trained NLLB 3.3B model for Assamese, Mizo, and Manipuri, achieving improved performance over the baseline. For Khasi, which is not supported by the NLLB model, we introduce special tokens and train the model on our Khasi corpus. Our training involves masked language modelling, followed by fine-tuning for English-to-Indic and Indic-to-English translations.","Translation of low-resource languages poses significant challenges in natural language processing. While substantial progress has been made in developing machine translation models for high-resource languages, low-resource languages often suffer from a lack of parallel corpora and digital resources Haddow et al. (2022). Languages like Khasi, Mizo, Manipuri, and Assamese are representative of this challenge, where limited data and unique linguistic complexities hinder the development of robust translation systems. In recent years, efforts to bridge this gap have gained momentum, driven by initiatives such as the Bharat Parallel Corpus Collection111https://ai4bharat.iitm.ac.in/bpcc/ (BPCC) Gala et al. (2023) and government-supported projects like PMIndia Haddow and Kirefu (2020), which aim to provide bilingual data for Indic languages. Despite these efforts, translation models for low-resource Indic languages have yet to achieve performance levels comparable to their high-resource counterparts Suman et al. (2023), necessitating innovative approaches to model training and data utilization. In this work, we develop a robust translation model for four low-resource Indic languages: Khasi, Mizo, Manipuri, and Assamese. Our approach involves data collection, preprocessing, training, and evaluation. We utilize datasets from WMT, BPCC, PMIndia, and OpenLanguageData222https://github.com/openlanguagedata/seed Maillard et al. (2023), and enhance bilingual data through back-translation Edunov et al. (2018) techniques, especially for Mizo and Khasi, significantly expanding our training corpus. We follow Meta’s data preprocessing standards and use LoRA (Low-Rank Adaptation) Hu et al. (2021) fine-tuning on the NLLB et al. (2022) 3.3B model to improve efficiency and performance with fewer parameters. Our model initially focuses on one-way translation from English to the Indic languages, then on reverse translations Dabre et al. (2019). The results show improved performance over the baseline, particularly for Khasi, where we address gaps in pre-trained model support."
https://arxiv.org/html/2411.00691v1,Leveraging Large Language Models for Code-Mixed Data Augmentation in Sentiment Analysis,"Code-mixing (CM), where speakers blend languages within a single expression, is prevalent in multilingual societies but poses challenges for natural language processing due to its complexity and limited data. We propose using a large language model to generate synthetic CM data, which is then used to enhance the performance of task-specific models for CM sentiment analysis. Our results show that in Spanish-English, synthetic data improved the F1 score by 9.32%, outperforming previous augmentation techniques. However, in Malayalam-English, synthetic data only helped when the baseline was low; with strong natural data, additional synthetic data offered little benefit. Human evaluation confirmed that this approach is a simple, cost-effective way to generate natural-sounding CM sentences, particularly beneficial for low baselines. Our findings suggest that few-shot prompting of large language models is a promising method for CM data augmentation and has significant impact on improving sentiment analysis, an important element in the development of social influence systems.","Figure 1: Overall system workflow with examples of Spanish-English CM tweets as natural data (left) and synthetic data (right). Underlined words represent Spanish-English hybrid words, examples of the complexities introduced by CM. Translations of CM sentences into English are provided in Appendix A. Code-mixing (CM), or code-switching, is the practice of switching between languages within a conversation or utterance. This practice is integral to multilingual societies, particularly in Mexico and urban India (Parshad et al., 2016), and is also significant in computer-mediated communication and social media, where multilingual users are predominant (Rijhwani et al., 2017). Despite its ubiquity, CM is mostly spoken and found in personal messages, making training data scarce and leading to poorer Natural Language Processing (NLP) model performance compared to monolingual text (Pratapa et al., 2018; Yong et al., 2023). Social influence (SI) refers to the changes in thoughts, feelings, attitudes, or behaviors resulting from interactions with others. In multilingual societies, CM reflects an important aspect of these interactions, reflecting social dynamics and identity. Sentiment analysis (SA) is crucial for understanding these dynamics, as it captures the emotional nuances embedded in multilingual interactions. Furthermore, SA has become a primary CM task due to its need for complex semantic understanding and its implications for social media (Drus and Khalid, 2019), where CM is commonly present (Srinivasan and Subalalitha, 2023). By accurately analyzing sentiment in code-mixed text, SI systems enhance their ability to interpret user intent and emotional states, enabling more meaningful interactions addressing the more diverse environments in which SI occurs. Since multilingual speakers bridge information on social media (Li and Murray, 2022), machines must also accurately analyze CM text to capture public opinion and disseminate news. However, current approaches fall short in handling code-mixed settings (Doğruöz et al., 2021; Aguilar et al., 2020) due to data scarcity. Beyond the CM domain, few-shot learning has shown promise in overcoming data scarcity, as Large Language Models (LLMs) trained on diverse tasks generalize to new ones with minimal training (Brown et al., 2020; Lin et al., 2022; Winata et al., 2021). LLMs are used for data augmentation (Ding et al., 2024; Whitehouse et al., 2023; Yoo et al., 2021; Dai et al., 2023), training data generation (Yu et al., 2023), and knowledge distillation (Xu et al., 2024; Phuong and Lampert, 2021), particularly in low-resource settings (Ding et al., 2024). However, this approach remains underexplored in the CM domain, which presents unique challenges (Zhang et al., 2023). In this work, we bring LLM-powered data augmentation to the task of code-mixed sentiment analysis. We use few-shot prompting to generate labeled CM SA data in Spanish-English and low-resource Malayalam-English. Following Li and Murray (2023); Whitehouse et al. (2023); Tareq et al. (2023), we quantify the performance gains by fine-tuning multilingual pre-trained language models (PLMs) on the LLM-generated data. We investigate if these synthetic data samples can reflect natural code-mixing patterns and nuances compared to other data augmentation techniques and verify the synthetic data quality through human evaluation. Figure 1 displays our overall system workflow with examples of natural and synthetic data. We summarize our contributions as follows: • We introduce LLMs for CM data augmentation as a simple, cost-effective way to improve sentiment analysis models with natural-sounding sentences; • We surpass past baselines, achieving third on the LinCE benchmark (Aguilar et al., 2020) in Spanish-English and outperforming the highest published benchmark by 4.85% on the low-resource MalayalamMixSentiment dataset (Chakravarthi et al., 2020); • We thoroughly analyze the efficacy of our data augmentation approach in comparison to other techniques and with human evaluation; • We release the synthetic data and code on Github111https://github.com/lindazeng979/LLM-CMSA for public use and reproducibility."
https://arxiv.org/html/2411.00689v1,Towards Multi-Source Retrieval-Augmented Generation via Synergizing Reasoning and Preference-Driven Retrieval,"Retrieval-Augmented Generation (RAG) has emerged as a reliable external knowledge augmentation technique to mitigate hallucination issues and parameterized knowledge limitations in Large Language Models (LLMs). Existing Adaptive RAG (ARAG) systems struggle to effectively explore multiple retrieval sources due to their inability to select the right source at the right time. To address this, we propose a multi-source ARAG framework, termed MSPR, which synergizes reasoning and preference-driven retrieval to adaptive decide “when and what to retrieve” and “which retrieval source to use”. To better adapt to retrieval sources of differing characteristics, we also employ retrieval action adjustment and answer feedback strategy. They enable our framework to fully explore the high-quality primary source while supplementing it with secondary sources at the right time. Extensive and multi-dimensional experiments conducted on three datasets demonstrate the superiority and effectiveness of MSPR.","In the open-domain question answering (OpenQA) task[1], even the leading Large Language Models (LLMs)[2][3][4] are restricted by the scope of their parametric knowledge and struggle with hallucination [5] and insufficient knowledge[6]. Retrieval-Augmented Generation (RAG)[7][8] is a powerful technique that mitigates these challenges by supplementing external knowledge within a non-parametric form, generating high-quality and reliable answers. Vanilla RAG follows the “retriever-and-reader” paradigm[9][10], where relevant documents are retrieved based on the given query and fed alongside the query into the LLM to yield the answer. However, Vanilla RAG struggles with complex tasks such as multi-hop QA[11], which demand a comprehensive, multi-step reasoning chain supported by all relevant information. Its one-time retrieval tends to gather insufficient knowledge, leading to an incorrect answer. To address this, mainstream RAG approaches attempt to gather more knowledge via a deeper or broader exploration view. One line of methods, Adaptive RAG (ARAG)[12][13][14][15], adaptively decides “when and what to retrieve” to achieve forward exploration and information gathering within a specific corpus. Another line of methods, Multi-Source RAG (MS-RAG)[16], emphasizes obtaining more comprehensive information from multiple sources. Overall, these approaches present improvements over Vanilla RAG. Therefore, exploring how to integrate the strengths of these methods is a worthwhile endeavor. Recently, ReAct[17] allows for straightforward instantiation as an ARAG system and direct application to multi-source retrieval, termed MS-ARAG. However, this approach has two primary limitations. First, different retrieval sources often vary in content quality, information scale, and knowledge density[18]. The mainstream retrieval sources typically include the high-quality, carefully curated but small-scale local corpus, and the large-scale but relatively lower-quality web-browser. Directly introducing multiple sources with distinct characteristics to ReAct may lead to blindly selecting retrieval sources during the adaptive information-gathering process, hindering the prioritized and thorough exploration of high-quality sources. Second, even with the addition of a basic description of the retrieval preference, ReAct still struggles to select the optimal retrieval source at the proper timing during the adaptive process. In our work, we develop a MS-ARAG framework, MSPR, which synergizes reasoning and preference-driven retrieval to constrain the adaptive knowledge-gathering process. Specifically, our method consists of three components. First, we build the adaptive reasoning-and-retrieval agent (ARA) as the foundational structure for adaptive knowledge exploration. ARA helps the system adaptively decide “when and what to retrieve” and “which retrieval source to use”. Furthermore, it iteratively facilitates interactions among explicit inference, retrieval actions, and retrieved information, enabling the system to select the optimal retrieval action until the final answer is produced. Second, we develop the preference-driven retrieval strategy selector (PRS) to guide the adaptive retrieval actions according to the overall retrieval preference. Specifically, it prioritizes an in-depth exploration of the primary high-quality retrieval source while supplementing knowledge with secondary sources at appropriate moments. The selector ensures a high-quality, stable multi-source retrieval process. Third, we use the corrective answer reviewer (CAR) to obtain feedback on answer quality, guiding the system to switch to supplementary retrieval sources when the answer quality falls short. We conduct multi-dimensional comparison experiments, ablation studies, and parameter tuning experiments on three datasets, confirming the superiority, effectiveness, and robustness of MSPR. Remarkably, with the built-in model set to GPT-4[2], MSPR outperforms the ReAct-instantiated MS-ARAG by 14.4% on the EM metric."
https://arxiv.org/html/2411.00646v1,Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction,"Vision Large Language Models (VLLMs) usually take input as a concatenation of image token embeddings and text token embeddings and conduct causal modeling. However, their internal behaviors remain underexplored, raising the question of interaction among two types of tokens. To investigate such multimodal interaction during model inference, in this paper, we measure the contextualization among the hidden state vectors of tokens from different modalities. Our experiments uncover a four-phase inference dynamics of VLLMs against the depth of Transformer-based LMs, including (I) Alignment: In very early layers, contextualization emerges between modalities, suggesting a feature space alignment. (II) Intra-modal Encoding: In early layers, intra-modal contextualization is enhanced while inter-modal interaction is suppressed, suggesting a local encoding within modalities. (III) Inter-modal Encoding: In later layers, contextualization across modalities is enhanced, suggesting a deeper fusion across modalities. (IV) Output Preparation: In very late layers, contextualization is reduced globally, and hidden states are aligned towards the unembedding space.","Recently, instruction-tuned Language Models (LMs) have demonstrated impressive performance on cross-modal tasks when incorporated with other modalities, mainly vision Dai et al. (2024); Liu et al. (2024b, a); Zhu et al. (2024); Chen et al. (2023); Merullo et al. (2022). These Vision Large Language Models (VLLMs) usually feed a concatenation of image tokens and text tokens into frozen Transformer-based LMs. Schwettmann et al. (2023) identify the multimodal neuron in Transformer MLPs and find that neurons from MLPs, not from outputs of the fine-tuned projector, translate image semantics into related text. Another following work in Verma et al. (2024) verifies that domain-specific visual attributes are modeled by LMs while fine-tuning the projector of VLLMs. Although these works provide prominent findings about the inner workings of VLLMs, how the multimodal inputs interact against LMs layers is still unknown, leading to our main research question: How does interaction happen among multimodal inputs (image, text) in the LMs of VLLMs? Figure 1: Magnitude of inter-modal contextualization against layer depth. Four monotonical intervals indicate our proposed four-phase inference dynamics. A higher value indicates more aggressive multimodal interaction.Left: InstructBLIP. Right: LLaVA-v1.5. Figure 2: A four-phase diagram of feed-forward dynamics of LMs in VLLMs. (I) Alignment of two different feature spaces occurs. (II) Intra-modal Encoding is enhanced while cross-modal encoding is inhibited. (III) Inter-modality Encoding appears and strengthens. (IV) Output Preparation requires hidden states to be aligned toward output embedding space. In this paper, we investigate the magnitude of contextualization (Ethayarajh, 2019) to characterize the multimodal interaction along LMs’ layers. Fig. 2 illustrates the overview of our phase diagram of multimodal contextualization. In detail, we look into the similarity among image tokens and instruction tokens, discovering that as inputs pass through successive layers of the Transformer, a phase diagram of multimodal contextualization with four monotonical intervals becomes apparent (Fig. 1). Phase I shows an immediate ascent of contextuality, indicating an early alignment between two modalities. Phase II presents a slight decline in multimodal interaction meanwhile the active intra-modal encoding is found in Fig. 3, suggesting the concentration shift of model encoding. Phase III shows a swift rise in similarity, indicating an incremental inter-modal interaction. Phase IV presents the global reduction in inter-modal similarity, suggesting the model shifts focus away from multimodal interaction. Moreover, more fine-grained experiments are conducted to enhance our aforementioned findings of inference dynamics. Details are revealed in §3.2 and § 3.3."
https://arxiv.org/html/2411.00604v1,ConvCounsel: A Conversational Dataset for Student Counseling,"Student mental health is a sensitive issue that necessitates special attention. A primary concern is the student-to-counselor ratio, which surpasses the recommended standard of 250:1 in most universities. This imbalance results in extended waiting periods for in-person consultations, which cause suboptimal treatment. Significant efforts have been directed toward developing mental health dialogue systems utilizing the existing open-source mental health-related datasets. However, currently available datasets either discuss general topics or various strategies that may not be viable for direct application due to numerous ethical constraints inherent in this research domain. To address this issue, this paper introduces a specialized mental health dataset that emphasizes the active listening strategy employed in conversation for counseling, also named as ConvCounsel. This dataset comprises both speech and text data, which can facilitate the development of a reliable pipeline for mental health dialogue systems. To demonstrate the utility of the proposed dataset, this paper also presents the NYCUKA, a spoken mental health dialogue system that is designed by using the ConvCounsel dataset. The results show the merit of using this dataset.","Student mental health is a sensitive issue as students often face unique challenges. Current mental health services in schools and universities encounter two major issues. First is the high demanding, and second is the imbalanced student-to-counselor ratio. According to the American School Counselor Association (ASCA), the recommended ratio is 250:1 [1]. Studies have shown that reducing this ratio significantly improves educational outcomes, such as the higher student grade point averages (GPAs) with better discipline [2, 3]. However, achieving this ratio requires substantial manpower, and only a few U.S. states meet this requirement [4]. In addition, this imbalance negatively impacts both students and counselors. For students, long waiting times (basically exceeding three weeks in some cases in Taiwan) can worsen their mental health conditions. For counselors, excessive workloads increase stress and reduce treatment effectiveness. For instance, a typical session at National Yang-Ming Chiao Tung University (NYCU) may require 12 or more consultations. Moreover, traditional counseling methods may not meet the expectations of new-generation students, who are more accustomed to interacting with digital systems. Researchers suggest that institutions may leverage AI to enhance the counseling process and improve the students’ health [5]. However, the existing datasets for mental health dialogue systems mostly focus on the empathetic responses in open-domain conversations [6, 7] or are designed as the emotional support datasets (ESD) covering the strategies such as affirmation, reassurance, suggestions, and emotional reflection [8, 9]. In real-world counseling, the most critical skill is not offering the advice but listening and understanding, avoiding the role of a real counselor. Additionally, students’ psychological issues are unique and cannot be generalized to the other mental health domains. Therefore, this study introduces the ConvCounsel dataset, specifically designed to build a student counseling dialogue system [10] centered on the active listening strategy. Active listening is a key technique that encourages clients to express their thoughts, making them feel understood and validated. This dataset includes both speech and text data, consisting of speech data crawled from streaming media and 40 recorded counseling sessions between counselors and students at NYCU, each annotated with precise timestamps and emotion labels by professional counselors. To evaluate the effectiveness of this dataset, we developed the NYCUKA system [11, 12] as an active listener in a mental health spoken dialogue system. This paper further enhanced the user experience by incorporating a visual agent using an animated character, creating the impression that the user is sharing their story with a real person. The proposed system responds to user inputs in Chinese and provides empathetic responses based on counselor-designed prompts and dialogue history, thereby enriching the counseling experience. Fig. 1: NYCUKA interface consisting of different components which include (A) expression switch for animated character (B) dialogue box (C) user input (either text or voice) (D) animated character Fig. 2: Procedure to conduct data collection for ConvCounsel dataset."
https://arxiv.org/html/2411.00593v2,Adapting Language Models via Token Translation,"Modern large language models use a fixed tokenizer to effectively compress text drawn from a source domain. However, applying the same tokenizer to a new target domain often leads to inferior compression, more costly inference, and reduced semantic alignment. To address this deficiency, we introduce Sparse Sinkhorn Token Translation (S2T2). S2T2 trains a tailored tokenizer for the target domain and learns to translate between target and source tokens, enabling more effective reuse of the pre-trained next-source-token predictor. In our experiments with finetuned English language models, S2T2 improves both the perplexity and the compression of out-of-domain protein sequences, outperforming direct finetuning with either the source or target tokenizer. In addition, we find that token translations learned for smaller, less expensive models can be directly transferred to larger, more powerful models to reap the benefits of S2T2 at lower cost.","Modern large language models (LLMs) are typically trained in two stages. First a tokenizer is trained to map commonly occurring character sequences in the training data into vocabulary units known as tokens. Next, all training text is tokenized, i.e., translated into this token vocabulary, and a model is trained to predict the next token given a context of preceding tokens. The tokenizer can be viewed as an initial compressor of input bytes (Gage, 1994) that significantly shortens text drawn from the training domain and arguably improves the training dynamics (Rajaraman et al., 2024). Despite its widespread adoption, this two-stage procedure suffers from a key failing: When faced with text from a new target domain, compression quality drops, context length and inference costs increase, and learned semantic alignment deteriorates. This effect is especially evident when modern LLMs (trained predominantly on English and code) are used to reason about molecular sequences like proteins. Such sequences are commonly represented using the Latin-script alphabet, but the meaning and frequency of each substring differ significantly their natural language counterparts, resulting in semantic misalignment. To tackle the analogous alignment problem for low-resource languages, Remy et al. (2024) proposed to use fast_align (Dyer et al., 2013), an expectation-maximization algorithm that requires parallel data from the training and target domains. This approach shows promising results, but for many target domains, parallel training data is difficult or impossible to gather. For example, there is no agreed-upon parallel translation between protein sequences and natural language. In this work, we propose a Sparse Sinkhorn Token Translation (S2T2) algorithm that does not require parallel data. Instead, S2T2 learns a translation between training domain tokens and new target domain tokens just using a sample data from the target domain and the pretrained LLM weights. After training a tokenizer on the target domain, S2T2 translates each target-domain token into a (sparse) distribution over training-domain tokens, uses the pretrained LLM to predict the next training-domain token, and translates that training-domain token back into a (sparse) distribution over target-domain tokens. In our experiments with English LLMs, we find that 1. S2T2 provides an effective initialization for continual finetuning on protein sequences, yielding both better compression and better perplexity than direct finetuning of the pretrained model, and 2. S2T2 enables weak-to-strong model transferability: Translations learned for smaller, less expensive models can be transferred to larger, more powerful models to reap the benefits at lower cost."
https://arxiv.org/html/2411.00533v2,ReverseNER: A Self-Generated Example-Driven Framework for Zero-Shot Named Entity Recognition with Large Language Models,"This paper presents ReverseNER, a framework aimed at overcoming the limitations of large language models (LLMs) in zero-shot Named Entity Recognition (NER) tasks, particularly in cases where certain entity types have ambiguous boundaries. ReverseNER tackles this challenge by constructing a reliable example library with the reversed process of NER. Rather than beginning with sentences, this method uses an LLM to generate entities based on their definitions and then expands them into full sentences. During sentence generation, the LLM is guided to replicate the structure of a specific ’feature sentence’, extracted from the task sentences by clustering. This results in well-annotated sentences with clearly labeled entities, while preserving semantic and structural similarity to the task sentences. Once the example library is constructed, the method selects the most semantically similar example labels for each task sentence to support the LLM’s inference. We also propose an entity-level self-consistency scoring mechanism to improve NER performance with LLMs. Experiments show that ReverseNER significantly outperforms traditional zero-shot NER with LLMs and surpasses several few-shot methods, marking a notable improvement in NER for domains with limited labeled data.","The advent of Large Language Models (LLMs) has marked a significant technological leap for Natural Language Processing (NLP). Owing to extensive pre-training and parameter optimization, LLMs such as GPT-3 [2] and BERT (Bidirectional Encoder Representations from Transformers) [6] have demonstrated exceptional performance across various NLP tasks. By capturing semantics, syntax, and contextual relationships from vast corpora, these models have not only achieved breakthroughs in traditional tasks like machine translation [14], relation extraction [17], and Named Entity Recognition (NER) [16], but also exhibited strong generalization capabilities in generative tasks such as text generation [12] and dialogue generation. Despite the impressive performance of LLMs in multiple tasks, they still face challenges in zero-shot NER, which requires models to correctly identify entity types without having seen any previously annotated data. In contrast, traditional NER methods typically rely on a large amount of annotated data for supervised learning [9]. Although LLMs possess some transfer learning capabilities, when enabling them to complete certain NER tasks without annotations, they still constantly struggle with complex tasks due to insufficient generalization abilities and imprecise understanding of domain-specific entities. These shortcomings arise from the model’s dependence on the diversity and richness of the training corpora for recognizing specific entity contexts, and when faced with new entity types or contexts, the models often struggle to match the entity categories accurately. To address the limitations of LLMs in zero-shot NER tasks, we propose a novel method named ReverseNER, which constructs an example library by reversing the NER process. This method first utilizes a pre-trained BERT model to calculate the similarity between task sentences and clusters the sentences based on this similarity. The LLM uses the central sentences of each cluster to generate similar sentences and construct a high-quality example library in conjunction with entity types. Next, the method calculates the cosine similarity between the task sentences and the sentences in the example library, selects the closest examples from the library, and adds them to the prompt when doing actual NER tasks. Compared to traditional zero-shot approaches using LLMs, our method provides LLMs with accurate, pre-generated examples that are semantically similar to the sentences requiring recognition, significantly enhancing NER task performance. The contributions of this paper are as follows: (1) This paper proposes a method called ReverseNER for large models to perform zero-shot NER. (2) ReverseNER generates accurate and task-relevant examples by combining task sentence syntax and entity types to reverse the traditional NER process. (3) Evaluation results demonstrate that our method in this paper has achieved significant improvements on three datasets."
https://arxiv.org/html/2411.00491v1,"GDTB: Genre Diverse Data for English Shallow Discourse Parsingacross Modalities, Text Types, and Domains","Work on shallow discourse parsing in English has focused on the Wall Street Journal corpus, the only large-scale dataset for the language in the PDTB framework. However, the data is not openly available, is restricted to the news domain, and is by now 35353535 years old. In this paper, we present and evaluate a new open-access, multi-genre benchmark for PDTB-style shallow discourse parsing, based on the existing UD English GUM corpus, for which discourse relation annotations in other frameworks already exist. In a series of experiments on cross-domain relation classification, we show that while our dataset is compatible with PDTB, substantial out-of-domain degradation is observed, which can be alleviated by joint training on both datasets.","Language in discourse is more than an ordered list of sentences or clauses: parts of a text expressing events, states, facts, or propositions are often connected by discourse relations, such as cause (one part of a text specifies the cause for another), concession (one part states content which a speaker or author expects recipients to overlook) etc. Such relations may be marked explicitly, for example by connectives, which are conjunctions or adverbials such as ‘because’ or ‘nevertheless’ in English; or they may be implicit, requiring recipients to interpret the text more actively. Given an arbitrary natural language text as input, shallow discourse parsing is the task of identifying pairs of text spans connected by a discourse relation, in some scenarios focusing mainly on explicit, implicit, or other subtypes of relations Xue et al. (2016), as well as the means by which they are signaled, resulting for example in connective disambiguation (e.g. ‘since’ is a connective which can express either a cause or temporal relation). Most commonly, shallow discourse parsing systems use the inventory of relations defined by the Penn Discourse Treebank (PDTB, currently version 3; see Section 2.1). Systems and data for shallow discourse parsing can be used for a variety of downstream applications, including relation extraction (identification of relations given two spans, Braud et al. 2024), instruction fine-tuning or pretraining of language models Ein-Dor et al. (2022), study of argumentation and persuasiveness Rehbein (2019), and cross-linguistic lexicography of discourse connectives Scheffler and Stede (2016); Das et al. (2018); Kurfalı et al. (2020). When finding specific relation types is desired, shallow discourse parsing also forms an end task in itself: for example, finding all concession relations in a large corpus of speeches by a politician or political party for Computational Social Science studies. Although work on shallow discourse parsing has expanded to a range of languages (e.g. Chinese, Zhou and Xue 2014; Czech, Synková et al. 2024, German, Sluyter-Gäthje et al. 2020; Italian, Tonelli et al. 2010; Thai, Prasertsom et al. 2024, Turkish, Zeyrek and Kurfalı 2017, Nigerian Pidgin, Saeed et al. 2024), less progress has been made on expanding data to new and diverse domains (see Section 2.2). A major cause of this bottleneck is the effort associated with manual construction of high quality data covering a broad range of domains from scratch. In this paper we suggest overcoming this hurdle by not starting from scratch: we target the freely available English GUM corpus (which is also available as part of the Universal Dependencies project, de Marneffe et al. 2021), which covers a broad range of 16161616 spoken and written English genres and for which annotations are available in hierarchical discourse parsing frameworks: RST and eRST (see Section 2.1). Although these frameworks are substantially different from PDTB, they provide sufficient information to obtain a high quality starting point for semi-automatic conversion of data into the PDTB v3 framework. As an added advantage, we also develop a mapping of (e)RST to PDTB relations, allowing for cross-framework comparisons along the lines proposed by Demberg et al. (2019) (see Zhu et al. 2021 for a similar argument and approach to converting coreference datasets). In the subsequent sections of this paper, we will first briefly survey the discourse relation frameworks involved in this project (Section 2), and then we will present our data, its creation process, and an evaluation of its quality (Section 3). This will be followed by a set of experiments on cross-corpus and joint-training relation classification to evaluate both the compatibility of our data with PDTB, and the degree of cross-corpus (and by extension, cross-domain) performance degradation."
https://arxiv.org/html/2411.00427v1,DARD:A Multi-Agent Approach forTask-Oriented Dialog Systems,"Task-oriented dialogue systems are essential for applications ranging from customer service to personal assistants and are widely used across various industries. However, developing effective multi-domain systems remains a significant challenge due to the complexity of handling diverse user intents, entity types, and domain-specific knowledge across several domains. In this work, we propose DARD (Domain Assigned Response Delegation), a multi-agent conversational system capable of successfully handling multi-domain dialogs. DARD leverages domain-specific agents, orchestrated by a central dialog manager agent. Our extensive experiments compare and utilize various agent modeling approaches, combining the strengths of smaller fine-tuned models (Flan-T5-large & Mistral-7B) with their larger counterparts, Large Language Models (LLMs) (Claude Sonnet 3.0). We provide insights into the strengths and limitations of each approach, highlighting the benefits of our multi-agent framework in terms of flexibility and composability. We evaluate DARD using the well-established MultiWOZ benchmark, achieving state-of-the-art performance by improving the dialogue inform rate by 6.6% and the success rate by 4.1% over the best-performing existing approaches. Additionally, we discuss various annotator discrepancies and issues within the MultiWOZ dataset and its evaluation system.","In recent research, significant efforts have been made to build systems that involve planning and communication between various specialized agents to perform complex tasks [1, 2, 3]. These agents are, in turn, backed by instruction-tuned open-source LLMs, external APIs, or other simpler tools. Various tasks such as logical reasoning [4, 5], societal simulations [6, 7], software development [8] have seen remarkable improvement in performance using these multi-agent framework methods. In this work, we explore the potential of agentic design in Task-Oriented Dialogue Systems (TODS). TODS are prevalent in real-world applications, such as customer service, e-commerce, and commercial voice assistants like Amazon Alexa and Google Assistant. Multiple research efforts have been made to curate high-quality labeled datasets to aid in developing systems that can handle end-to-end task-oriented dialogs [9, 10, 11]. One of the most well-established and widely used TOD datasets amongst them is the MultiWOZ dataset [9]. The dataset contains 10k+ single and multi-domain conversations spanning 7 domains of attraction, hospital, hotel, restaurant, taxi, train, and police. After the original release, multiple corrected versions (2.1 - [12], 2.2 - [13], 2.3 - [14], 2.4 - [15]) of the MultiWOZ dataset have been published, each addressing specific issues. We primarily experiment with the MultiWOZ 2.2 [13] version, as it had the most established benchmark [16] and is the latest recognized version as per the official repository111https://github.com/budzianowski/multiwoz. The main tasks performed on the MultiWOZ dataset are Dialogue State Tracking (DST), which involves tracking predefined slots and their values in the context, and Response Generation, which involves predicting the system’s response to the latest user message. We propose a Domain Assigned Response Delegation (DARD), a framework that involves multiple specialized domain agents invoked by a central dialog manager agent, based on the dialog context. These domain-specific agents use conversational context and relevant entities from an external database to generate a response for the latest user message(see Figure ). We experiment with fine-tuned Flan-T5-Large [17], Mistral-7B [18], and prompted Claude Sonnet 3.0 222https://www.anthropic.com/news/claude-3-family models as our domain agents and also a prompted Claude Sonnet 3.0 as a dialog manager agent. We present further details of our experiments in the section . We further systematically compare the performance of our approaches with other top-performing approaches on the benchmark in the section . To rigorously validate our method and its performance, we conduct comprehensive error analyses and also present the challenges and limitations of the MultiWOZ dataset and its evaluation system in section . The following are the key contributions and insights we present through our work: Figure 1: Overview of DARD for end-to-end response generation on MultiWOZ. The current diagram shows a conversation assigned to the restaurant agent but in general, it can be assigned to any of the domain agents • We introduce DARD (Domain-Assigned Response Delegation), an ensemble of domain-specific agents that improve the state-of-the-art dialog inform rate by 6.6% and success rate by 4.1% on the MultiWOZ benchmark. • Our study presents a detailed comparison of performance between fine-tuned(Mistral-7B, Flan-T5-Large) vs Prompted (Claude Sonnet 3.0) models in the context of dialog agents and single-agent vs multi-agent approaches for task-oriented dialogs • Our analysis reveals multiple annotator discrepancies present in the MultiWOZ dataset, impacting the performance of models for DST and response generation tasks."
https://arxiv.org/html/2411.00418v1,Self-Evolved Reward Learning for LLMs,"Reinforcement Learning from Human Feedback (RLHF) is a crucial technique for aligning language models with human preferences, playing a pivotal role in the success of conversational models like GPT-4, ChatGPT, and Llama 2. A core challenge in employing RLHF lies in training a reliable reward model (RM), which relies on high-quality labels typically provided by human experts or advanced AI system. These methods can be costly and may introduce biases that affect the language model’s responses. As language models improve, human input may become less effective in further enhancing their performance. In this paper, we propose Self-Evolved Reward Learning (SER), a novel approach where the RM generates additional training data to iteratively improve itself. We conducted extensive experiments on multiple datasets such as HH-RLHF and UltraFeedback, using models like Mistral and Llama 3, and compare SER against various baselines. Our results demonstrate that even with limited human-annotated data, learning from self-feedback can robustly enhance RM performance, thereby boosting the capabilities of large language models (LLMs).","Reinforcement Learning from Human Feedback (RLHF) is a well-established approach that aligns Large Language Models (LLMs) with human preference data Ouyang et al. (2022); Bai et al. (2022b). The standard approach involves learning a reward model (RM) from human preferences and the learned RM is then frozen to train LLMs via Reinforcement Learning (RL) such as Proximal Policy Optimization (PPO) Schulman et al. (2017a). Another common approach directly trains LLMs from the human preference data without learning an RM such as Direct Preference Optimiztion (DPO) Rafailov et al. (2024). Both approaches rely heavily on the size and quality of human-annotated preference data. However, the availability of such data is often limited and expensive to acquire, posing a significant bottleneck in the development and performance of RL approaches Yuan et al. (2024b). This dependency on human-annotated data hinders the scalability of strong LLMs that require vast amounts of labeled data to achieve greater performance Kaplan et al. (2020); Muennighoff et al. (2024). To mitigate the dependency, recent works leverage the AI feedback to train RMs, referred to as Reinforcement Learning from AI Feedback (RLAIF) Bai et al. (2022b); Lee et al. (2023) , which reduces the reliance on human-annotated data. However, they hold heuristic assumptions that LLMs can provide high-quality feedback and they often requires stronger LLMs to provide feedback Pang et al. (2023). Recent advancements suggest that LLMs have the potential to serve as world models to a certain degree, capable of understanding world knowledge and complex patterns independently of explicit human input Hao et al. (2023); Guan et al. (2023); Zhao et al. (2024). Leveraging this ability, LLMs can evaluate and provide feedback. In the context of RLHF and RLAIF, this capability of LLMs can be extended as the role of RMs, and RL approaches rely heavily on the RMs Dewey (2014); Li (2017). Focusing on training a better RM with limited human-annotated data, we propose a novel reward learning approach, which self-evolves the RM through a feedback loop using the RM itself. In our approach, the LLM serves as the RM, generating feedback on the dataset that is subsequently used to refine its own learning. This iterative ”feedback-then-train” loop allows the RM to self-evolve over time, gradually improving its performance, even with some noise in the initial self-labeled data. As the iteration progresses, however, similar data offers diminishing help and can even degrade performance. To address this, we identify the RM learning status in each iteration and introduce data filtering strategies to select high-confidence data that are later used for a more robust RM training. Figure 1: The Self-Evolved Reward Learning (SER) pipeline. Our SER method consists of following steps: (1) Self-labeling: the reward model (RM) assigns labels to unlabeled data. (2) Identifying learning status and selecting data: high-confidence data is selected by assessing the learning status. (3) Retrain the RM: the RM trains itself using the self-labeled and selected data. (4) Train the Large Language Model (LLM): the LLM is trained under the guidance of the self-evolved RM. Note that steps (1)-(3) iterate multiple rounds to a converged RM. By employing this self-evolved reward learning process, where the RM continually learns from its own feedback, we reduce dependency on large human-labeled data while maintaining, or even improving, the model’s performance. Our contributions are threefold: • We introduce a novel self-evolved reward learning framework, demonstrating that only 15% of human-annotated seed data is required to achieve performance comparable to models trained with full human-labeled datasets, significantly reducing reliance on human data. • We provide insights into the broader implications of self-learning paradigms in LLMs, particularly in improving reinforcement learning by enhancing RMs (see Section 4.1.2). • Extensive experiments demonstrate that our self-evolved reward learning framework consistently improves performance across various LLMs, model sizes, and datasets. We conducted experiments on multiple datasets and LLMs with their varied sizes to validate the generalization and effectiveness of our method. We find that, compared to the seed models that use only a small amount of human-labeled data, our method can robustly and significantly enhance model performance, with an average improvement of 7.88%. After multiple iterations, the final convergence can achieve or even surpass the performance of models using the entire human-annotated dataset, providing a potential solution for the self-improvement of models."
https://arxiv.org/html/2411.00411v1,Enhancing Authorship Attribution through Embedding Fusion: A Novel Approach with Masked and Encoder-Decoder Language Models,"The increasing prevalence of AI-generated content alongside human-written text underscores the need for reliable discrimination methods. To address this challenge, we propose a novel framework with textual embeddings from Pre-trained Language Models (PLMs) to distinguish AI-generated and human-authored text. Our approach utilizes Embedding Fusion to integrate semantic information from multiple Language Models, harnessing their complementary strengths to enhance performance. Through extensive evaluation across publicly available diverse datasets, our proposed approach demonstrates strong performance, achieving classification accuracy greater than 96% and a Matthews Correlation Coefficient (MCC) greater than 0.93. This evaluation is conducted on a balanced dataset of texts generated from five well-known Large Language Models (LLMs), highlighting the effectiveness and robustness of our novel methodology.","In recent years, the landscape of natural language generation (NLG) technology has undergone remarkable advancements, revolutionizing the diversity, control, and quality of texts generated by Large Language Models (LLMs). Notably, OpenAI’s ChatGPT, Google’s Gemini, and Meta’s Llama stand out as prime examples, showcasing exceptional performance across a myriad of tasks, including answering questions, composing emails, essays, and even code snippets. However, while these advancements herald a new era of human-like text generation at unprecedented efficiency, they also bring to the forefront pressing concerns regarding the detection and mitigation of potential misuse of LLMs. While there are many issues with LLMs in terms of their hallucinating responses and toxic language, the newfound capability of LLMs to emulate human-like text raises significant apprehensions about their potential misuse in activities in many areas such as phishing, disinformation campaigns, and academic dishonesty. Instances abound where educational institutions have resorted to banning ChatGPT due to apprehensions regarding its potential for facilitating cheating in assignments [2, 20], while media outlets have sounded the alarm over the proliferation of fake news generated by LLMs [18]. Such concerns surrounding the misuse of LLMs have cast a shadow over their application in critical domains such as media and education. Accurate detection of LLM-generated texts emerges as a pivotal requirement for realizing the full potential of NLG technology while mitigating the potentially serious consequences associated with its misuse. From the perspective of end-users, the ability to discern between human-authored and LLM-generated text holds the promise of bolstering trust in NLG systems and fostering wider adoption. For developers and researchers in the realm of Machine Learning, effective text detection mechanisms can aid in tracking generated texts and thwarting unauthorized usage. Figure 1: An overview of our framework to perform authorship attribution between machine-generated and human-authored texts. Given the critical significance of accurate LLM-generated text detection, we propose a novel framework based on representing text as images through Embedding Fusion as in Fig. 1. Embeddings are low-dimensional representations of high-dimensional inputs like text, aiming to capture similarities by positioning related inputs closely in the embedding space, thereby enabling AI systems to comprehend inputs akin to human understanding. Inspired by [7], we combine textual representations across Pre-trained Language Models (PLMs) and leverage their complementary strengths. Additionally, post-concatenation of the feature vectors, we reshape the fused feature vector into a 2D representation to capture the inter-embedding relationships better. Amongst the 3 types of PLMs - Autoregressive Language Models (ALMs), Masked Language Models (MLMs), and Encoder-Decoder Language Models (EDLMs) [12], our experiments indicate that the combination of the former 2 PLMs works best. We believe this is because ALMs are trained with a focus on content generation and more emphasis is placed on the final tokens. With thorough experimentation using publicly available datasets, we validate that our framework achieves better results than the state-of-the-art (SOTA) methods for the task of discerning AI-generated and human-authored text. The rest of the paper is structured as follows. We dive into the past and recent research contributions in solving the authorship attribution task in Section 2. A detailed overview of the dataset is shown in Section 3. Sections 4 and 5 provide the methodology and results in support of our approach."
https://arxiv.org/html/2411.00390v1,MetaMetrics-MT: Tuning Meta-Metrics for Machine Translation via Human Preference Calibration,"We present MetaMetrics-MT, an innovative metric designed to evaluate machine translation (MT) tasks by aligning closely with human preferences through Bayesian optimization with Gaussian Processes. MetaMetrics-MT enhances existing MT metrics by optimizing their correlation with human judgments. Our experiments on the WMT24 metric shared task dataset demonstrate that MetaMetrics-MT outperforms all existing baselines, setting a new benchmark for state-of-the-art performance in the reference-based setting. Furthermore, it achieves comparable results to leading metrics in the reference-free setting, offering greater efficiency.","Evaluating machine translation (MT) tasks is inherently complex, as no single metric can universally apply to all scenarios. A metric that performs well for one task may not be suitable for another, and its effectiveness can vary significantly depending on the specific language pairs involved. Therefore, relying solely on a single metric is often inadequate. To ensure the usefulness of automatic metrics, it is crucial to align them with human annotations Winata et al. (2024b). To achieve a more comprehensive evaluation, benchmarks typically incorporate multiple metrics, such as lexical-based and semantic-based metrics. However, the correlation between these metrics can be skewed due to variations in the models used and the training data employed for evaluation. For instance, BERTScore Zhang et al. (2019) uses contextual embeddings from pre-trained transformers to assess performance, with different models excelling in specific language pairs. In contrast, neural-based metrics like BLEURT Sellam et al. (2020), COMET Rei et al. (2020), and CometKiwi Rei et al. (2022) employ distinct methodologies and training datasets. These differences can affect each metric’s alignment with human judgments and their reliability across language pairs. Some metrics, like XCOMET-Ensemble Guerreiro et al. (2023), demand high memory (at least 80GB), prompting efforts to predict LLM performance using smaller models Anugraha et al. (2024). In this paper, we propose MetaMetrics-MT, a MT metric inspired by MetaMetrics Winata et al. (2024a). This meta-metric is crafted to align more closely with human preferences through the use of Bayesian optimization with Gaussian Processes (GP). By systematically integrating multiple existing metrics, MetaMetrics-MT achieves state-of-the-art performance for reference-based metrics and shows a strong correlation with human scores for reference-free metrics in the WMT24 metric shared task Freitag et al. (2024). Through the strategic combination of metrics with assigned weights, MetaMetrics-MT aims to be as competitive as, if not superior to, any individual metric. Our contributions include the following: • We present MetaMetrics-MT in reference-based and reference-free settings, offering flexibility for various MT scenarios. Our reference-based model sets the state-of-the-art for the WMT24 task. We publicly release the code for easy usability.111The code is available at https://github.com/meta-metrics/metametrics. • We demonstrate that the MetaMetrics-MT metric is easily adjustable to meet the human preference. • We show that MetaMetrics-MT is compact and efficient, capable of running on a commercial GPU with 40GB of memory, whereas a comparable metric like XCOMET-Ensemble requires significantly higher memory with at least 80GB."
https://arxiv.org/html/2411.00387v1,STEM-PoM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing,"Advances in large language models (LLMs) have spurred research into enhancing their reasoning capabilities, particularly in math-rich STEM documents. While LLMs can generate equations or solve math-related queries, their ability to fully understand and interpret abstract mathematical symbols in long, math-rich documents remains limited. In this paper, we introduce STEM-PoM, a comprehensive benchmark dataset designed to evaluate LLMs’ reasoning abilities on math symbols within contextual scientific text. The dataset, sourced from real-world ArXiv documents, contains over 2K math symbols classified as main attributes of variables, constants, operators, and unit descriptors, with additional sub-attributes including scalar/vector/matrix for variables and local/global/discipline-specific labels for both constants and operators. Our extensive experiments show that state-of-the-art LLMs achieve an average of 20-60% accuracy under in-context learning and 50-60% accuracy with fine-tuning, revealing a significant gap in their mathematical reasoning capabilities. STEM-PoM fuels future research of developing advanced Math-AI models that can robustly handle math symbols.","Large language models (LLMs) have demonstrated exceptional reasoning abilities across numerous fields [17, 13, 12, 25, 15]. With the increasing shift towards applying LLMs to complex tasks [6, 23, 39], the need for supplementary data beyond the general pre-trained datasets has become increasingly important. Among these, mathematical reasoning tasks [10, 18] have recently drawn the attention of several researchers [19, 2, 45, 28] (see Backgrounds in Appendix B). In particular, Part-of-Math Tagging [43], the mathematical analog to part-of-speech tagging [36] where mathematical tokens are classified according to a given taxonomy of attributes, continues to gain interest but lacks the foundational datasets that can support advanced NLP tasks [43, 38, 37]. In addition, integrating mathematical language into NLP models remains a substantial challenge [3, 29], especially in the realm of document parsing [8, 24, 44]. Traditional semantic parsing methods like LateXML [31] or arXMLiv [22] often fall short when applied to math-rich documents, where precision and structured syntax are paramount [14, 32, 41]. These methods struggle to accurately perform pattern matching between abstract mathematical symbols and their corresponding XML tag notations. Similarly, recent advanced LLMs, such as ChatGPT [26], also face difficulties in understanding and reasoning with abstract mathematical symbols due to their contextual polymorphism [35] (as Figure 3 shown). For example, in the linear equation: y=m⁢x+p𝑦𝑚𝑥𝑝y=mx+pitalic_y = italic_m italic_x + italic_p, y𝑦yitalic_y is categorized as a variable. Whereas in the cross-entropy loss function: ℒ⁢(x,y)=−∑i=1Nxi⁢log⁡(yi)ℒ𝑥𝑦superscriptsubscript𝑖1𝑁subscript𝑥𝑖subscript𝑦𝑖\mathcal{L}(x,y)=-\sum_{i=1}^{N}x_{i}\log(y_{i})caligraphic_L ( italic_x , italic_y ) = - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), the symbol y𝑦yitalic_y represents the fixed target labels, which is considered a constant for a given dataset. Without the corresponding contextual information of a mathematical symbol, LLMs are unable to distinguish between different attributes of the symbol and cannot effectively process related mathematical reasoning tasks. Thus, tagging math symbols within domain-specific contexts is essential for language models. In this paper, we introduce a novel benchmark dataset, STEM-PoM, designed to evaluate the reasoning capabilities of language models on mathematical symbols across different domains. The STEM-PoM dataset consists of 2,109 instances extracted from a random sampling of over 10,000 arXiv manuscripts, which are math-rich documents spanning domains such as Mathematics, Physics, Chemistry, and more. We provide a mathematical symbol for each dataset instance, its order in the document, its main and sub-level attributes, and the corresponding text or expression from the original arXiv paper containing the symbol. Each mathematical symbol in the dataset is classified according to two levels of attributes [42]. The first-level attribute categorizes the symbol as variable, constant, operator, or unit descriptor. The second-level attribute further classifies the symbol into one of six types based on its first-level category: scalar, vector, matrix, local, global, or discipline-specific. Figure 1 illustrates the dataset’s category distribution. To further enrich the STEM-PoM dataset with additional arXiv manuscripts and other math-rich document resources, we also design the STEM-PoM Labeler, a feasible method for assisting dataset generation by automatically searching, extracting, and recording hand-labeled mathematical symbols and their corresponding context from long-text documents. Figure 1: The overall pipeline for constructing the STEM-PoM dataset. We extract math symbols with corresponding text information to formulate the dataset. Each math symbol is initially classified into one of four primary categories based on its definition. Then, the symbol is further categorized into secondary categories by the context in which it appears or by the symbol’s dimensionality. An LLM is evaluated via the first-level and second-level classification tasks. We conduct thorough experiments on the STEM-PoM dataset to assess the mathematical reasoning abilities of seven open- and closed-source language models, including LSTM [11], Mixtral-8x7B [20], Llama2-13B [40], Llama3-80B [9], Claude-3.5-sonnet [4], GPT-3.5, and GPT-4o [1] with various prompting and fine-tuning strategies. The experimental results indicate that STEM-PoM distinguishes the performance levels across different LLMs, offering insights into the mathematical symbol reasoning abilities of these models. In addition, we investigate and analyze the influence of context length on the ability of language models to understand mathematical symbols."
https://arxiv.org/html/2411.00369v2,GRSQA - Graph Reasoning-Structured Question Answering Dataset,"Large Language Models (LLMs) have excelled in multi-hop question-answering (M-QA) due to their advanced reasoning abilities. However, the impact of the inherent reasoning structures on LLM M-QA performance remains unclear, largely due to the absence of QA datasets that provide fine-grained reasoning structures. To address this gap, we introduce the Graph Reasoning-Structured Question Answering Dataset (GRS-QA), which includes both semantic contexts and reasoning structures for QA pairs. Unlike existing M-QA datasets, where different reasoning structures are entangled together, GRS-QA explicitly captures intricate reasoning pathways by constructing reasoning graphs, where nodes represent textual contexts and edges denote logical flows. These reasoning graphs of different structures enable a fine-grained evaluation of LLM reasoning capabilities across various reasoning structures. Our empirical analysis reveals that LLMs perform differently when handling questions with varying reasoning structures. This finding facilitates the exploration of textual structures as compared with semantics.","Reasoning in natural language is the fundamental aspect of intelligence (Huang and Chang, 2023), and QA tasks provide a quantifiable way to test the reasoning capabilities of intelligent systemsYang et al. (2018). Recently, the emergence of LLMs has demonstrated unprecedented reasoning capacity in answering questions Wei et al. (2022); Wang et al. (2024). However, real-world applications often demand even more complex reasoning capability, such as multi-hop reasoning Atif et al. (2023), where systems must integrate information from multiple sources and perform multiple steps of thinking in a certain order to arrive at the final answer and conclusion. To evaluate the multi-hop reasoning capabilities of LLMs, researchers have developed several multi-hop question-answering (M-QA) datasets, including HotpotQA, 2WikiMultiHopQA, and MuSiQue Yang et al. (2018); Ho et al. (2020); Trivedi et al. (2022). MuSiQue constructs genuine multi-step QA pairs by composing connected single-hop questions through a bottom-up approach and mitigating existing common shortcuts. 2WikiMultiHopQA integrates structured and unstructured data to provide comprehensive and evidence-based reasoning paths, which ensures authentic multi-hop reasoning. HotpotQA is a large-scale and crowd-sourced dataset comprising 113,000 Wikipedia-based QA pairs, offering sentence-level supporting facts for explainable predictions and introducing challenging comparison questions. Despite their contributions to benchmarking LLMs’ multi-hop reasoning capabilities, these datasets have notable limitations. First, most current M-QA datasets lack explicit reasoning structures for each QA pair, preventing LLMs from leveraging predefined reasoning pathways and forcing them to rely solely on their internal knowledge. Second, they mix questions with varying reasoning complexities without categorization, making it challenging to investigate LLMs’ QA capability at a fine-grained structure level of the question. To address these limitations, we introduce GRS-QA, a novel Graph Reasoning-Structured Question Answering Dataset, that integrates explicit reasoning structure in the format of the graph (i.e., reasoning graph) to enable multi-hop reasoning analysis of LLM-based MQA. By incorporating reasoning graphs that represent multi-hop inference pathways, GRS-QA offers several advantages. First, these graphs provide a transparent way to understand the logical steps LLMs should follow to reach the answer, making the reasoning process explicit and allowing researchers to pinpoint where a model may struggle at a fine-grained level. Second, the reasoning graphs are categorized based on their structural complexity. Furthermore, each reasoning graph is accompanied by metadata (e.g., the number of reasoning steps and types), which facilitates analysis to identify patterns in question difficulty, reasoning complexity, and LLM performance. Thirdly, the structured nature of the reasoning graphs enables the development of new evaluation metrics, such as reasoning recall/precision, that go beyond answer correctness and allow the assessment of LLMs to replicate the reasoning pathway. To construct the reasoning graph, we treat each sentence as a node and add edges based on their original logical relations. In addition, to further investigate the importance of the reasoning structure in correctly answering the question compared to the content, we also generate structural negative samples by adding noise to their graph structure, such as adding extra sentence nodes and rewiring the reasoning graphs. Investigating the QA performance when paired with these additionally introduced negative samples helps demystify the importance of the structure in QA. Our contribution can be summarized as follows: • First QA Dataset with Reasoning Graphs: GRS-QA introduces the first question-answering dataset that pairs each instance with explicit reasoning graphs. These reasoning graphs provide a transparent pathway to understand the logical steps that LLMs should follow to reach the correct answer, allowing researchers to pinpoint specific areas where a model may struggle. • Comprehensive Analysis and Categorization of Reasoning Graphs: Each QA pair in GRS-QA is accompanied by comprehensive metadata, such as reasoning types and reasoning complexity. This enables performance analysis according to the reasoning structure and offers new insights into how LLMs perform on questions with different levels of reasoning complexity. • Negative Reasoning Graphs: In addition to the ground-truth reasoning graph, each QA pair also includes corresponding negative reasoning graphs where the content remains the same, but the structure is slightly modified. This enables an exclusive analysis of the structural impact on reasoning and QA performance."
https://arxiv.org/html/2411.00324v1,Learning to Rank Salient Content for Query-focused Summarization,"This study examines the potential of integrating Learning-to-Rank (LTR) with Query-focused Summarization (QFS) to enhance the summary relevance via content prioritization. Using a shared secondary decoder with the summarization decoder, we carry out the LTR task at the segment level. Compared to the state-of-the-art, our model outperforms on QMSum benchmark (all metrics) and matches on SQuALITY benchmark (2 metrics) as measured by Rouge and BertScore while offering a lower training overhead. Specifically, on the QMSum benchmark, our proposed system achieves improvements, particularly in Rouge-L (+0.42) and BertScore (+0.34), indicating enhanced understanding and relevance. While facing minor challenges in Rouge-1 and Rouge-2 scores on the SQuALITY benchmark, the model significantly excels in Rouge-L (+1.47), underscoring its capability to generate coherent summaries. Human evaluations emphasize the efficacy of our method in terms of relevance and faithfulness of the generated summaries, without sacrificing fluency. A deeper analysis reveals our model’s superiority over the state-of-the-art for broad queries, as opposed to specific ones, from a qualitative standpoint. We further present an error analysis of our model, pinpointing challenges faced and suggesting potential directions for future research in this field.","Query-focused summarization (QFS) is gaining prominence in research community. Unlike conventional summarization tasks that aim to capture the overall essence of a document or a set of documents, QFS focuses on generating concise summaries in response to posed queries. This specialization enables a more targeted information retrieval process, offering summaries that directly address the informational needs rather than providing a broad overview of the source material. The advancements in QFS have been notably driven by the introduction of invaluable datasets of long documents such as QMSum with an average of 9K tokens Zhong et al. (2022) and SQuALITY with an average of 5.2K tokens Wang et al. (2022a), which have facilitated deeper exploration and innovation in this field. These datasets have laid the groundwork for the development of Transformers-based models which have shown strong potential in generating summaries that respond accurately to queries (Su et al., 2021; Laskar et al., 2022; Vig et al., 2022; Pagnoni et al., 2022; Sotudeh and Goharian, 2023; Yu et al., 2023). However, despite this proficiency, their ability to effectively prioritize information—assessing its importance relative to a query to enhance summary relevance—remains an area for improvement. This study seeks to address this limitation, with a particular focus on long-input QFS, where summarizing multiple segments 111A chunk of document with a predefined length (e.g., 512 tokens). for a given query presents unique challenges in capturing and prioritizing relevant content. Particularly, in this study, we present a novel enhancement to QFS through the incorporation of learning-to-rank (LTR), a technique with established efficacy in Information Retrieval. Our approach aims to refine the system’s capability to discern and prioritize content segments not only by their relevance but also by their relative importance. This methodological advancement ensures that the produced summaries more accurately reflect the query’s intent and hierarchically organize information by its significance. Central to our approach is the augmentation of use of the decoder that shares parameters with the summarization decoder 222Particularly, we use the single decoder for two tasks: summarization and learning-to-rank., specifically designed for executing the LTR task at the segment level. While Learning-to-Rank (LTR) is a classic approach, our innovation lies in adapting LTR principles specifically for Query-Focused Summarization (QFS) at the segment level, which has been less explored in the literature. This strategy, inspired by the work of (Zhuang et al., 2022) in adapting the T5 (Raffel et al., 2020) encoder-decoder framework for text ranking in query-document scenarios, is tailored to address the nuances of segment ranking within the QFS context. Through the joint fine-tuning of summarization with cross-entropy loss, and LTR task—utilizing listwise cross-entropy softmax loss, our system not only elevates the relevance of generated summaries but also introduces a nuanced understanding of information importance. This strategy can aid the summarization system at attending to the source content given their relative importance. In short, our contributions are threefold: • We propose an LTR-assisted system for QFS that integrates the intuition of ranking and relative importance of segments during the summary generation process; • Our proposed system outperforms across all automatic metrics (QMSum) and attains comparable performance in two metrics (SQuALITY) with lower training overhead compared to the SOTA. Additionally, our system enhances the relevance and faithfulness of generated summaries without sacrificing fluency; • We undertake an error analysis to discern the challenges faced by our model including label imbalance, and segment summarizer’s hurdles, providing insights into potential avenues for further research."
https://arxiv.org/html/2411.00300v1,Rationale-Guided Retrieval Augmented Generationfor Medical Question Answering,"Large language models (LLM) hold significant potential for applications in biomedicine, but they struggle with hallucinations and outdated knowledge. While retrieval-augmented generation (RAG) is generally employed to address these issues, it also has its own set of challenges: (1) LLMs are vulnerable to irrelevant or incorrect context, (2) medical queries are often not well-targeted for helpful information, and (3) retrievers are prone to bias toward the specific source corpus they were trained on. In this study, we present RAG2 (RAtionale-Guided RAG), a new framework for enhancing the reliability of RAG in biomedical contexts. RAG2 incorporates three key innovations: a small filtering model trained on perplexity-based labels of rationales, which selectively augments informative snippets of documents while filtering out distractors; LLM-generated rationales as queries to improve the utility of retrieved snippets; a structure designed to retrieve snippets evenly from a comprehensive set of four biomedical corpora, effectively mitigating retriever bias. Our experiments demonstrate that RAG2 improves the state-of-the-art LLMs of varying sizes, with improvements of up to 6.1%, and it outperforms the previous best medical RAG model by up to 5.6% across three medical question-answering benchmarks. Our code is available at https://github.com/dmis-lab/RAG2","Large language models (LLM) OpenAI (2023); Saab et al. (2024); AI@Meta (2024) have demonstrated remarkable performance across various tasks in biomedicine, including USMLE-style question-answering (QA) benchmarks Jin et al. (2021).111The USMLE or United States Medical Licensing Examination is a standardized test that is required for obtaining a medical license in the United States. Despite their state-of-the-art performance, LLMs face challenges limiting their adoption in high-stakes areas. A key concern is hallucination, where the model produces information that sounds plausible but is incorrect Maynez et al. (2020); Huang et al. (2023). Additionally, updating models’ knowledge is resource-intensive, making it difficult to maintain current medical information for application Zhang et al. (2023); Kasai et al. (2024). Retrieval-augmented generation (RAG) Lewis et al. (2020) has emerged as a promising solution to address these limitations by reducing hallucinations and ensuring models provide up-to-date information through the integration of trustworthy documents into their input. Figure 1: Our RAG2 framework comprises the following three steps. (1) Rationale-based query formulation: the model-generated rationale is used as the query for evidence retrieval instead of the initial query. (2) Balanced retrieval: evidence snippets are retrieved in equal amounts from four corpora: two large ones, PubMed and PMC, and two smaller but crucial corpora, textbooks and clinical guidelines. Subsequently, a reranker is used to rerank the retrieved snippets by cross-encoding the initial query and each snippet. (3) Rationale-guided filtering: a filter model determines whether to incorporating the retrieved snippets into the LLM prompt increases the confidence of the LLM’s rationale (or reduces its perplexity). However, applying RAG in the biomedical domain presents unique challenges. First, irrelevant or incorrect context can mislead LLMs. Wu et al. (2024). In biomedical texts, the complexity and specialized terminology heighten the risk of retrieving documents that are not relevant or unhelpful. Second, medical queries often struggle to target useful information. Some questions that include extensive patient details, such as medical histories and symptoms, may overwhelm retrieval systems, making it difficult to identify critical diagnostic clues. Conversely, overly brief questions often lack necessary context and depend on implicit medical knowledge, further complicating effective retrieval. Lastly, retriever bias Chen et al. (2021); Dai et al. (2024) also presents a significant challenge, as larger medical corpora often overshadow smaller, specialized ones that may contain critical, up-to-date information. For instance, MedCPT Jin et al. (2023) may show a preference for PubMed documents over clinical guidelines and medical textbooks, and this tendency is further exacerbated by its training on the PubMed corpus. Building upon these insights, we propose RAG2 (RAtionale-Guided Retrieval Augmented Generation), a novel framework that improves the reliability of RAG in biomedical applications (see Figure 1). First, we introduce a rationale-guided filtering method, training a Flan-T5 model Chung et al. (2024) using labels derived from the differences in perplexity between rationales with and without retrieved documents. This model estimates the snippets’ informativeness for the base LLM, selectively augmenting useful information while filtering out potential distractors. Next, our method substitutes medical questions with LLM-generated rationales as queries using chain-of-thought prompting Wei et al. (2022); Jiang et al. (2023b); Kang et al. (2024). These rationale-based queries help identify key components through systematic problem-solving and also enable query expansion for brief questions Jagerman et al. (2023); Wang et al. (2023a). Finally, we implement a balanced retrieval strategy that retrieves snippets equally from diverse biomedical corpora. This approach mitigates the potential bias present in dense retrievers by promoting a more comprehensive retrieval process across multiple information sources, regardless of their size or level of training exposure. We evaluate our RAG2 framework using three closed-book medical QA benchmarks—MedQA Jin et al. (2021), MedMCQA Pal et al. (2022), and MMLU-Med Hendrycks et al. (2021)—where no oracle documents are present. Our method significantly enhances the average accuracy of state-of-the-art LLMs of varying sizes, including Llama-3-8B AI@Meta (2024), Meerkat-7B Kim et al. (2024), and GPT-4o OpenAI (2024). Additionally, it consistently improves the performance of baseline LLMs by up to 6.1% across three medical QA datasets and outperforms four recently developed RAG baselines, achieving a notable 5.6% improvement over MedRAG Xiong et al. (2024a) with the Llama-3-8B-Instruct model AI@Meta (2024). Our contributions are summarized as follows: 1. We introduce RAG2, a novel RAG framework for medical QA. Our model incorporates advanced methods in query formulation, retrieval, and filtering processes. 2. Our novel rationale-guided filtering method employs a small language model trained with data from rationale perplexity to assess document utility. This approach not only enhances efficiency but also introduces a unique aspect to our methodology. 3. In closed-book medical QA benchmarks, we empirically demonstrate that the proposed RAG2 framework significantly outperforms previous methods, including large commercial, small open-source, and medical-specialized models, with an average accuracy improvement of 6.1%, 3.8%, and 0.9%, respectively."
https://arxiv.org/html/2411.00294v2,LLM-Ref: Enhancing Reference Handling in Technical Writingwith Large Language Models,"Large Language Models (LLMs) excel in data synthesis but can be inaccurate in domain-specific tasks, which retrieval-augmented generation (RAG) systems address by leveraging user-provided data. However, RAGs require optimization in both retrieval and generation stages, which can affect output quality. In this paper, we present LLM-Ref, a writing assistant tool that aids researchers in writing articles from multiple source documents with enhanced reference synthesis and handling capabilities. Unlike traditional RAG systems that use chunking and indexing, our tool retrieves and generates content directly from text paragraphs. This method facilitates direct reference extraction from the generated outputs, a feature unique to our tool. Additionally, our tool employs iterative response generation, effectively managing lengthy contexts within the language model’s constraints. Compared to baseline RAG-based systems, our approach achieves a 3.25×3.25\times3.25 × to 6.26×6.26\times6.26 × increase in Ragas score, a comprehensive metric that provides a holistic view of a RAG system’s ability to produce accurate, relevant, and contextually appropriate responses. This improvement shows our method enhances the accuracy and contextual relevance of writing assistance tools.","Scientific research is fundamental in enriching our knowledge base, tackling real-life challenges, and contributing to the betterment of human lives. Writing clear and precise research articles is crucial for disseminating new findings and innovations to a broad audience, avoiding misunderstandings that could impede progress. Writing research papers clearly is challenging due to the need to balance complex content with readability, adhere to strict formatting, and synthesize coherently. Writing tools aid researchers by providing advanced grammar and style checks, simplifying data organization, and enhancing argument coherence, making them essential for crafting impactful, high-quality scientific papers with real-world applications. Large Language Models (LLMs) have significantly advanced natural language processing (NLP) by improving language understanding, generation, and interaction. While they excel in many NLP tasks, they require substantial computational resources and may struggle with specialized tasks without domain-specific knowledge. LLMs often produce inaccurate responses or ‘hallucinations’ when handling tasks beyond their training data. Developing an effective writing assistant using LLMs requires fine-tuning with domain-specific data from various fields, a process that demands extensive computational resources and a diverse dataset, making it costly to create a versatile and effective tool for diverse writing challenges. To mitigate the challenges associated with using LLMs for downstream tasks, Retrieval-Augmented Generation (RAG) Lewis et al. (2021) systems have gained popularity for their capability to integrate external user-specific data. By actively sourcing information from knowledge databases during the generation phase, RAG efficiently tackles the challenge of creating content that may be factually inaccurate Gao et al. (2024). When working with user source data, RAG-based systems usually read the documents in text format which they segment into small chunks. However, determining the appropriate size for chunking presents a challenging problem, as it significantly impacts the quality of the final output generated. To manage the model’s context limitations, RAG systems often only consider the top-k context segments, potentially overlooking crucial contextual details. Furthermore, due to their data-processing and retrieval approaches, RAG-based systems fall short of providing comprehensive source references needed for composing research articles. In this paper, we present LLM-Ref, a writing assistant tool that helps researchers with enhanced reference extraction while writing articles based on multiple source documents. To address the challenges of existing RAG-based tools, our writing assistant tool preserves the hierarchical section-subsection structure of source documents. Rather than dividing texts into chunks and transforming them into embeddings, our approach directly utilizes the paragraphs from research articles to identify information relevant to specific queries. To efficiently retrieve all the relevant information from the source documents, an LLM is utilized due to their superior performance in finding semantic relevance. Efficient utilization of contexts in paragraphs allows LLM-Ref extract references within the contexts. Furthermore, iterative generation of output response allows handling long context and finer responses. Efficient retrieval and preservation of hierarchical source information enable the listing of comprehensive references, ensuring that users have access to detailed citation details. The proposed LLM-Ref can provide both primary references—the source documents—and secondary references, which are listed in the context paragraphs of the source documents. To the best of our knowledge, no other similar work focuses on providing both primary and secondary references. Evaluation results show superior performance of our tool over existing RAG-based systems. The proposed LLM-Ref demonstrates significant performance improvements over other RAG systems, achieving a 5.5×5.5\times5.5 × higher Context Relevancy in the multiple source documents scenario compared to Basic RAG and a 4.7×4.7\times4.7 × higher Context Relevancy in the single source document scenario. Additionally, it delivers an impressive increase in the Ragas Score, outperforming the best alternative by 3.25×3.25\times3.25 × in the multiple source documents scenario and 2.65×2.65\times2.65 × in the single source document scenario. These results highlight that the proposed tool provides more accurate, relevant, and contextually precise outputs, enhancing the overall utility and reliability of the writing assistance it offers."
https://arxiv.org/html/2411.00204v1,RESTOR: Knowledge Recovery through Machine Unlearning,"Large language models trained on web-scale corpora can memorize undesirable datapoints such as incorrect facts, copyrighted content or sensitive data. Recently, many machine unlearning methods have been proposed that aim to ‘erase’ these datapoints from trained models— that is, revert model behavior to be similar to a model that had never been trained on these datapoints. However, evaluating the success of unlearning algorithms remains challenging. In this work, we propose the RESTOR framework for machine unlearning based on the following dimensions: (1) a task setting that focuses on real-world factual knowledge, (2) a variety of corruption scenarios that emulate different kinds of datapoints that might need to be unlearned, and (3) evaluation metrics that emphasize not just forgetting undesirable knowledge, but also recovering the model’s original state before encountering these datapoints, or restorative unlearning. RESTOR helps uncover several novel insights about popular unlearning algorithms, and the mechanisms through which they operate— for instance, identifying that some algorithms merely emphasize forgetting the knowledge to be unlearned, and that localizing unlearning targets can enhance unlearning performance.111Code/data is available at github.com/k1rezaei/restor.","Large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023) have garnered significant attention for their remarkable ability to generate human-like text. However, their training on extensive web-scraped datasets, exposes them to a range of security and privacy risks, from memorizing private or copyrighted content to reproducing incorrect or harmful information in the training data (Pan et al., 2020; Wei et al., 2024; Carlini et al., 2021; Huang et al., 2022). To address these concerns and to further the development of trustworthy language models, researchers have proposed unlearning techniques. These methods aim to modify an already trained model to unlearn a set of datapoints, resulting in a model that is similar to one which never included the datapoints in its training set (Blanco-Justicia et al., 2024). Since LLMs are pre-trained or fine-tuned on vast datasets, retraining from scratch after excluding target datapoints is computationally infeasible for unlearning. Consequently, the community has explored efficient methods to simulate this procedure. These approaches are typically evaluated by assessing their effectiveness in forgetting content within unlearning documents, e.g., copyrighted concepts, toxic content, or forgetting factual knowledge about concepts appeared within unlearning dataset (Maini et al., 2024; Jin et al., 2024; Zhang et al., 2024; Jang et al., 2022; Kumar et al., 2022). In this work, we consider an alternate pre-requisite for unlearning: if a model is no longer influenced by the unlearning set, it should retain the same knowledge and capabilities as before encountering documents in this set. For instance, imagine a model checkpoint that has already acquired correct knowledge about certain concepts. As pretraining continues on other datapoints —some containing incorrect facts, private information, or malicious documents related to those concepts— the model may eventually exhibit degraded performance on tasks involving these concepts. In such scenarios, unlearning would help eliminating the influence of these datapoints, since retraining the model from scratch is computationally infeasible. Ideally, the model should retain its original knowledge about those concepts after unlearning. In other words, the goal of unlearning here is not only to forget the problematic documents but also to restore the model to its original knowledge state— in essence, achieving what we call restorative unlearning. Figure 1: RESTOR framework, including three components: (i) corruption, (ii) unlearning, and (iii) evaluation. Clean model is corrupted by continue pretraining on a set of documents, losing its knowledge on subject entity Nelson Mandela. Unlearning algorithm is then applied to neutralize the effect of corruption documents to ideally obtain a model similar to the clean one. Unlearning may result in forgetting content in corruption dataset, or in ideal case result in recovering the original knowledge. To study restorative unlearning, we propose the RESTOR framework (RESTORing knowledge through machine unlearning), in which a model’s knowledge about specific entities is corrupted by introducing documents containing incorrect information. Unlearning algorithms are applied to eliminate the influence of these datapoints. We develop a benchmark that evaluates the restorative unlearning capabilities of these algorithms, rather than just their forgetting abilities. This approach differs from benchmarks like TOFU (Maini et al., 2024) or RWKU (Jin et al., 2024), where evaluation primarily focuses on concept forgetting. 222TOFU further considers the model’s utility for relevant entities outside the forget set, whereas we focus on recovering utility for target entities. RESTOR includes a corruption module where incorrect facts about a set of entities are injected into the model, an unlearning module where an unlearning algorithm is applied to remove the effects of incorrect facts, and an evaluation module that assesses the model’s factual knowledge about those entities (Figure 1). This framework enables the evaluation of unlearning algorithms within the proposed scenario, demonstrating their effectiveness in removing incorrect information and recovering the model’s knowledge about target entities. RESTOR enables analysis of data’s impact on restorative unlearning efficacy, spanning both corruption and unlearning. We experiment with different configurations for generating corruption datasets, such as interleaving incorrect facts and unrelated context. We also examine how simpler unlearning datasets—those containing less unrelated content—enhance restorative unlearning. Our study reveals that while many existing unlearning methods often excel at forgetting, they struggle with achieving restorative unlearning. Preference-based optimization techniques generally perform better, though they still fall short in certain cases. The possibility of successful restorative unlearning provides insight into how much knowledge is stored in models, as successful recovery in some cases suggests that simple linear associations for facts does not fully explain how factual knowledge is stored. We also observe that localizing incorrect facts in corruption datasets and applying unlearning algorithms only on them can, for some algorithms, lead to improved performance. Additionally, by probing the model’s logits layer, we analyze how unlearning methods modify a model’s knowledge, further illustrating the distinction between forgetting and restorative unlearning."
https://arxiv.org/html/2411.00173v1,Beyond Label Attention: Transparency in Language Models for Automated Medical Coding via Dictionary Learning,"Medical coding, the translation of unstructured clinical text into standardized medical codes, is a crucial but time-consuming healthcare practice. Though large language models (LLM) could automate the coding process and improve the efficiency of such tasks, interpretability remains paramount for maintaining patient trust. Current efforts in interpretability of medical coding applications rely heavily on label attention mechanisms, which often leads to the highlighting of extraneous tokens irrelevant to the ICD code. To facilitate accurate interpretability in medical language models, this paper leverages dictionary learning that can efficiently extract sparsely activated representations from dense language model embeddings in superposition. Compared with common label attention mechanisms, our model goes beyond token-level representations by building an interpretable dictionary which enhances the mechanistic-based explanations for each ICD code prediction, even when the highlighted tokens are medically irrelevant. We show that dictionary features can steer model behavior, elucidate the hidden meanings of upwards of 90% of medically irrelevant tokens, and are human interpretable.","Transparency is a vital factor in healthcare to gain patients’ trust, especially when AI models make critical decisions in clinical practice Rao et al. (2022). One of the essential applications of AI models is to assign International Classification of Diseases (ICD) codes automatically based on the clinical text (we name this task as medical coding). These ICD codes categorize patient diagnoses, conditions, and treatments for billing, reporting, and treatment purposes Hirsch et al. (2016); Johnson et al. (2021). However, assigning ICD codes is complex and requires expertise and time O’Malley et al. (2005). Recent advancements in medical pre-trained language models (PLMs) have made it possible to treat medical coding as a high-dimensional multilabel classification challenge Edin et al. (2023); Huang et al. (2022). These AI models led to significant success in efficient ICD coding Kaur et al. (2021). However, their transparency remains of great concern Hakkoum et al. (2022). Therefore, developing automated interpretability methods is crucial to upholding transparency in medical coding processes. Significant progress has occurred in the field of black-box interpretability, particularly concerning feature attribution, with the emergence of perturbation-based methods such as SHAP Lundberg and Lee (2017) and its approximate counterpart LIME Ribeiro et al. (2016); Moraffah et al. (2020). These techniques, rooted in information and game theory, are recognized for assessing feature relevance in detail by intelligently perturbing and ablating input features Lundberg and Lee (2017); Ribeiro et al. (2016). While approximation methods have greatly improved the speed of calculating Shapley values, exact computations remain expensive Lundberg et al. (2020); Chen et al. (2022); Shrikumar et al. (2019); Mosca et al. (2022). The huge computational cost makes their application impractical towards automated medical ICD coding since clinical notes usually contain thousands of high dimensional token embeddings in a vast multilabel prediction space Johnson et al. (2023). As such, we seek a human-interpretable approach that scales efficiently with large datasets, highlights essential features, and offers more comprehensive explanations of PLM predictions. Recent advancements in mechanistic interpretability methods Cunningham et al. (2023); Räuker et al. (2023) have demonstrated the potential to surpass the computational challenges posed by traditional black-box approaches by elucidating the roles of specific neuron subsets within a network. This level of mechanistic understanding is precious in the medical field, where explaining the significance of a feature is as crucial as its identification. In recent years, the attention mechanism Abnar and Zuidema (2020); Vaswani et al. (2023); Chefer et al. (2021, 2022) has been heavily used to interpret and explain the behavior of large transformer models. Within the realm of automated medical ICD coding, label attention (LAAT) variants are most prevalent due to their efficiency in handling extensive sequence lengths. They calculate an attention score for each token relative to each label, thereby identifying tokens crucial for ICD code predictions Mullenbach et al. (2018); Vu et al. (2020). Nevertheless, studies such as Pandey et al. (2023) have questioned the interpretability and validity of explanations provided by attention mechanisms. For example, the LAAT mechanism may highlight incoherent or irrelevant tokens, such as stop words for highly medically specific ICD codes. For instance, LAAT attributes the stop token ""and"" to the medically specific ICD code ""998.59 postoperative wound infection"" despite conjunctions being irrelevant to medical prognosis, thus undermining the interpretability of LAAT as shown in Figure 1. We attribute this issue to be the result of neuron polysemanticity—where a single neuron responds to diverse, unrelated inputs—complicates direct interpretation Olah et al. (2020). Elhage et al. (2022) theorizes polysemanticity to be a form of superposition, occurring when the count of independent data features surpasses layer dimensions, leading to data features being represented by linear combinations of neurons. As a result, individual neurons are often directly uninterpretable Subramanian et al. (2017); Cunningham et al. (2023). Addressing this, sparse autoencoders Olshausen and Field (1997) have been applied to distill these complex dense representations into interpretable, sparse linear combinations, performing what is known as dictionary learning (DL). This strategy has effectively decomposed different language model layers, such as neural word embeddings Subramanian et al. (2017), MLPs Cunningham et al. (2023); Bricken et al. (2023), and residual connections Yun et al. (2023a), proving scalable and monosemantic. Our paper generalizes these sparse coding concepts to better understand the attention mechanism and pre-trained language model (PLM) embeddings by constructing effective dictionaries with LAAT to improve the interpretability of the medical ICD coding task. We summarize our main contributions: • Expanding on Bricken et al. (2023)’s ablation studies, we show that combining learned dictionary features with another mechanistic component (LAAT) Vu et al. (2020), in our new interpretability framework AutoCodeDL, improves explainability of downstream ICD predictions. • We build medically relevant dictionaries with sparse autoencoders that can capture medically relevant concepts hidden within superposition. • We develop new automated proxy metrics for assessing the human understandability of constructed dictionaries and conduct extensive evaluations on large scale clinical text-based corpus. Figure 1: Motivation: LAAT identifies the most relevant tokens for each ICD code (b). Compared to our inspection of which tokens are most relevant to an ICD code (a), we assume ""and"" is irrelevant to an ICD code prediction. Although it may appear as though ""and"" is irrelevantly highlighted, taking token embeddings out of superposition allows us to decompose dense token embeddings into more semantically meaningful dictionary features that show that concepts of ""failure of wound healing"" are embedded within its token embedding (c), thereby giving justification for its highlighting by LAAT for a wound-related ICD code."
https://arxiv.org/html/2411.00154v1,Scaling Up Membership Inference:When and How Attacks Succeed on Large Language Models,"Membership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable “cheating.” In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable MIA at document and collection of documents level. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.111Our code is available at https://github.com/parameterlab/mia-scaling","Large language models (LLMs) are trained on vast datasets, which providers typically keep private. Data owners are concerned that their copyrighted data might be used in LLM training without explicit consent. Membership Inference Attacks (MIAs) attempt to determine if a specific data sample was used to train a model (Shokri et al., 2017). These methods are now being applied to LLMs to address the question of potential data misuse (Shi et al., 2024; Meeus et al., 2024a; Zhang et al., 2024b; Wang et al., 2024a; Xie et al., 2024). Figure 1: Focusing on the Right Scale. MIA has traditionally been considered ineffective for LLMs. However, we argue that MIA remains effective for LLMs when applied at a much larger scale, considering significantly longer token sequences. This large-scale MIA is also practically and legally relevant, as copyright is often determined at the document level. The application of MIAs on LLMs has faced discouraging results so far. Initial claims of success (Shi et al., 2024; Meeus et al., 2024a; Carlini et al., 2021; Mattern et al., 2023) were later disproven by Duan et al. (2024); Das et al. (2024); Maini et al. (2024); Meeus et al. (2024b). Before their work, it was common practice to select true non-members from documents created after the LLM’s cut-off date. They showed that this approach allowed MIA methods to exploit temporal cues, rather than identifying membership through the model’s inherent response characteristics. Duan et al. (2024); Das et al. (2024); Maini et al. (2024) introduced a new evaluation method based on an independent, identically distributed (IID) split between true members and non-members. Since adopting this method, no further MIA studies on LLMs have shown performance significantly better than random. Reported membership detection has remained below 60% AUROC, close to the random-chance level of 50% AUROC (Duan et al., 2024; Das et al., 2024; Maini et al., 2024; Xie et al., 2024; Zhang et al., 2024b). We argue that MIA can still be effective on LLMs, provided it is applied to much longer token sequences than previously considered. Earlier MIA approaches often focused on short sequences of tokens, typically ranging from 128 to 256 tokens (Xie et al., 2024; Duan et al., 2024; Shi et al., 2024; Wang et al., 2024b). This use of n-grams faced criticism because of the significant overlap between members and non-members, making the MIA problem poorly defined (Duan et al., 2024). Some later studies suggested analysing entire documents instead of n-grams (Shi et al., 2024; Meeus et al., 2024b), but even then, performance remained near random (Meeus et al., 2024b). In this work, we demonstrate that MIA approaches begin to show meaningful performance only when applied to much longer token sequences, such as 10K tokens. To show this, we introduce four scales of token sequences: sentences, paragraphs, documents, and datasets, as shown in Figure 1. We propose MIA evaluation protocols and benchmarks for the binary detection of training data samples given at four different scales. As a baseline, we adapt the Dataset Inference (Maini et al., 2024) method for the MIA task at multiple scales. As a result, our aggregation-based MIA demonstrates significant performance improvements for document sets, achieving AUROC scores of 80% or higher. To explore additional scenarios, we investigate the performance of MIA on fine-tuning data. Our findings show that continual learning MIA is effective collection of documents (AUROC > 88%), while CoT-based fine-tuning works at both the sentence and collection levels. Our contributions are summarized as: 1. We introduce a novel evaluation benchmark and protocol for MIA, covering multiple scales of token sequences. 2. We extend and adapt the first successful MIA Maini et al. (2024) to any data scale, allowing us to conduct a comprehensive analysis of MIA performance in LLMs. 3. We provide additional MIA benchmarks for various LLM fine-tuning scenarios, demonstrating that our method achieves even stronger performance in these contexts."
https://arxiv.org/html/2411.00150v1,Schema Augmentation for Zero-Shot Domain Adaptation in Dialogue State Tracking,"Zero-shot domain adaptation for dialogue state tracking (DST) remains a challenging problem in task-oriented dialogue (TOD) systems, where models must generalize to target domains unseen at training time. Current large language model approaches for zero-shot domain adaptation rely on prompting to introduce knowledge pertaining to the target domains. However, their efficacy strongly depends on prompt engineering, as well as the zero-shot ability of the underlying language model. In this work, we devise a novel data augmentation approach, Schema Augmentation, that improves the zero-shot domain adaptation of language models through fine-tuning. Schema Augmentation is a simple but effective technique that enhances generalization by introducing variations of slot names within the schema provided in the prompt. Experiments on MultiWOZ and SpokenWOZ showed that the proposed approach resulted in a substantial improvement over the baseline, in some experiments achieving over a twofold accuracy gain over unseen domains while maintaining equal or superior performance over all domains.","Dialogue State Tracking (DST) plays a pivotal role in task-oriented dialogue systems by maintaining a structured representation of the user’s goals, intents, and preferences as a conversation progresses. As these systems interface with external APIs, such as booking platforms or food ordering services, accurate DST is crucial for ensuring successful goal-oriented interactions. Traditionally, DST models were domain-specific, relying on large amounts of annotated data for every domain. The rise of powerful multi-task instruction-tuned large language models (LLMs) has enabled zero-shot DST, which does not require task-specific training Feng et al. (2023); Yi et al. (2024); Hosseini-Asl et al. (2020). However, zero-shot DST results have been mixed, and prior work has argued that general purpose models remain limited in their ability to replace specialized systems for this task Heck et al. (2023). Zero-shot domain adaptation offers a promising middle ground. In this problem, training is allowed, but only some domains are available at training time. This allows for methods that retain the benefits of fine-tuning while addressing the challenge of domain-specific data scarcity. Prior work has investigated this method, but has required slots to be filled one at a time, either by generating slot-specific prompts Aksu et al. (2023) or reformulating slot-filling as QA Li et al. (2021); Lin et al. (2021). However, zero-shot domain adaptation methods for end-to-end DST - modeling the entire dialogue state at once, given the dialogue context - is an area which remains underexplored. To this end, we focus on training methods for improving zero-shot domain adaptation with large language models for end-to-end DST. We develop a novel method using data augmentation techniques, and demonstrate the effectiveness of our method on two popular DST datasets, MultiWOZ 2.2 Zang et al. (2020) and SpokenWOZ Si et al. (2024). Our contributions are threefold: 1. We introduce Schema Augmentation, a data augmentation technique for zero-shot domain adaptation that obtains up to a twofold improvement over a strong baseline. 2. We propose a new metric, Target Goal Accuracy, to evaluate domain adaptation performance in task-oriented dialogue systems. 3. We conduct ablation studies to analyze the factors contributing to the effectiveness of our method."
https://arxiv.org/html/2411.00142v1,JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking,"Accurate document retrieval is crucial for the success of retrieval-augmented generation (RAG) applications, including open-domain question answering and code completion. While large language models (LLMs) have been employed as dense encoders or listwise rerankers in RAG systems, they often struggle with reasoning-intensive tasks because they lack nuanced analysis when judging document relevance. To address this limitation, we introduce JudgeRank, a novel agentic reranker that emulates human cognitive processes when assessing document relevance. Our approach consists of three key steps: (1) query analysis to identify the core problem, (2) document analysis to extract a query-aware summary, and (3) relevance judgment to provide a concise assessment of document relevance. We evaluate JudgeRank on the reasoning-intensive BRIGHT benchmark, demonstrating substantial performance improvements over first-stage retrieval methods and outperforming other popular reranking approaches. In addition, JudgeRank performs on par with fine-tuned state-of-the-art rerankers on the popular BEIR benchmark, validating its zero-shot generalization capability. Through comprehensive ablation studies, we demonstrate that JudgeRank’s performance generalizes well across LLMs of various sizes while ensembling them yields even more accurate reranking than individual models.","Passage reranking is a critical component in modern information retrieval systems, designed to refine results obtained from efficient first-stage retrieval methods such as BM25 (Robertson et al., 1995, 2009). By narrowing down the pool of candidate documents, reranking substantially improves the quality of downstream tasks, such as retrieval-augmented generation or RAG (Lewis et al., 2020). Two primary approaches have emerged to address the reranking task. The first category comprises encoding-based approaches (Nogueira and Cho, 2019; Gao et al., 2021), which encode queries and documents into fixed-size embedding vectors. These methods use either cosine similarity as a score function or directly output a score from the model (Nogueira et al., 2020; Zhuang et al., 2023). While highly efficient, these approaches face several limitations. One major challenge is their inflexibility in defining relevance, making it difficult to accommodate diverse retrieval objectives (e.g., finding supporting vs. refuting evidence). Moreover, encoding-based models heavily rely on manual supervision signals due to the discrepancy between LLM pretraining and reranking objectives, limiting their ability to generalize to new domains or models (Nguyen, 2016; Izacard et al., 2022). Most recently, utilizing Large Language Models (LLMs) for document reranking has led to promising progress in addressing some of these challenges, owing to their superior capabilities in language understanding, generation, interaction, and reasoning (Ouyang et al., 2022). These approaches utilize an LLM either as a pointwise judge (Ma et al., 2024) or a listwise reranker (Sun et al., 2023; Zhuang et al., 2024). While these approaches allow for flexible definition of document relevance and support zero-shot operation, they still require the model to make decisions without intermediate analyses. Consequently, they fall short in scenarios requiring complex reasoning (Su et al., 2024), hampering both performance and interpretability. Moreover, listwise rerankers face significant computational challenges due to context length constraints, often compromising on individual document length when processing multiple documents simultaneously. To bridge this gap, we propose JudgeRank, a novel zero-shot pointwise reranker tailored for reasoning-intensive text retrieval tasks. Inspired by Chain-of-Thought (Wei et al., 2022) and LLM-as-a-Judge (Zheng et al., 2023) methods, JudgeRank utilizes highly generalizable prompts to guide instruction-tuned LLMs through explicit reasoning steps before arriving at a final judgment. Figure 1 illustrates a real example of how our model works on the Biology dataset in the BRIGHT (Benchmark for Reasoning-Intensive Generative Retrieval Tasks) benchmark (Su et al., 2024). Specifically, our reranker first prompts the LLM to identify the core problem in the query, allowing it to focus on the central question while filtering out irrelevant context. Next, the model produces an extractive summary for each of the candidate documents and explains how it addresses the query. Finally, the model makes a relevance judgment based on the previous analyses. This process closely mimics how humans approach questions: by first skimming the document, identifying relevant parts, and then carefully reading these parts to obtain an answer (Masson, 1983). This structured pipeline enables JudgeRank to transcend surface-level lexical matching, leveraging deeper semantic understanding to improve reranking accuracy. We evaluate JudgeRank on the recently constructed BRIGHT benchmark, widely regarded as one of the most challenging retrieval evaluation datasets. Despite the poor performance of state-of-the-art text embedding models and rerankers on this benchmark, our method achieves significant improvements over all existing baselines and secures the top position on the BRIGHT benchmark leaderboard among 89898989 models, surpassing the previous best model by a significant margin (9999 points).111https://brightbenchmark.github.io/ Additionally, we demonstrate that JudgeRank readily generalizes to other popular retrieval benchmarks such as BEIR and performs competitively with state-of-the-art rerankers. We also analyze the complementarity of models at different scales by investigating the alignment of their ranking decisions. Contrary to our expectations, we observe that models of different sizes demonstrate a surprisingly orthogonal behavior on their relevance judgments. Leveraging this key finding, we propose a simple ensembling strategy that allows us to combine multiple models flexibly and demonstrate that it translates to considerable performance gains on the final ranking. Figure 1: A step-by-step illustration of how JudgeRank arrives at the final judgment through query and document analyses. The query analysis identifies the core problem being asked, while the document analysis extracts relevant sentences from the document based on the query. This is a real example from the Biology task in the BRIGHT evaluation benchmark."
https://arxiv.org/html/2411.00073v1,RSL-SQL: Robust Schema Linking in Text-to-SQL Generation,"Text-to-SQL generation aims to translate natural language questions into SQL statements. In large language models (LLMs) based Text-to-SQL, schema linking is a widely adopted strategy to streamline the input for LLMs by selecting only relevant schema elements, therefore reducing noise and computational overhead. However, schema linking faces risks that requires caution, including the potential omission of necessary elements and disruption of database structural integrity. To address these challenges, we propose a novel framework called RSL-SQL that combines bidirectional schema linking, contextual information augmentation, binary selection strategy, and multi-turn self-correction. Our approach improves the recall of schema linking through forward and backward pruning and hedges the risk by voting between full schema and contextual information augmented simplified schema. Experiments on the BIRD and Spider benchmarks demonstrate that our approach achieves state-of-the-art execution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on Spider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4 based Text-to-SQL systems when adopting DeepSeek (much cheaper) with same intact prompts. Extensive analysis and ablation studies confirm the effectiveness of each component in our framework. The codes are available at https://github.com/Laqcce-cao/RSL-SQL.","The task of translating natural language questions into structured query language (SQL), known as Text-to-SQL or Text2SQL generation, is crucial for enabling non-expert users to interact with relational databases Wang et al. (2019); Qin et al. (2022). By bridging the gap between natural language and structured query languages, text2sql systems democratize data access and empower users to extract valuable insights without deep technical knowledge Katsogiannis-Meimarakis and Koutrika (2023). In very recent years, leveraging the powerful comprehension and generation capabilities of Large Language Models (LLMs) Achiam et al. (2023); OpenAI (2024); Anthropic (2024) for Text-to-SQL tasks has become a primary approach for boosting performance and prompt engineering emerges as the mainstream technical strategy. A typical prompt provided to the LLM for Text2SQL usually includes a description of the database, user queries, and few-shot demonstrations Gao et al. (2023); Pourreza and Rafiei (2024a), which allows the system to be applicable to various databases. Intuitively, assuming the LLM possesses sufficiently strong capabilities, the more precise and detailed the database description, the better the quality of the generated SQL queries. Features such as the database’s structure, annotations, sample data, and relational mappings has been validated in specific scenarios to benefit Text-to-SQL performance Li et al. (2024d); Lee et al. (2024). Fine-grained descriptions of databases pose challenges. It is common for databases, especially industrial-grade databases, to have hundreds or thousands of fields. Incorporating complete database features in the prompt leads to excessive input tokens, increased computational costs, and, critically, the introduction of substantial noise Talaei et al. (2024). Since user queries typically refer to a small proportion of database schema elements, a large amount of irrelevant schema information can confuse LLM and degrade performance. To mitigate this, schema linking Lei et al. (2020); Pourreza and Rafiei (2024a) techniques have been widely adopted to identify and include only relevant schema elements in the prompts. However, as shown in Figure 1, there are two types of information loss risks associated with schema linking: 1) The generated SQL will inevitably be erroneous if schema linking fails to identify all necessary tables and columns (assuming that the LLM does not generate any hallucinations, which means schema elements of the SQL generated by the LLM are entirely derived from input database schema); 2) Pruning the database schema might disrupt inherent structural relationships and impair the LLM’s understanding of the database’s raw structure, resulting in incorrect queries even though if schema linking identifies all required elements. Related research Maamari et al. (2024) shows that for high-performance LLMs, such as GPT-4 or GPT-4o, schema linking may even reduce the execution accuracy of generated SQL statements. To address these challenges, we propose RSL-SQL, a Robust Schema Linking based Text-to-SQL generation framework that mitigates the risks associated with schema linking while leveraging its benefits. In our framework, we first generates a preliminary SQL using complete database schema and achieves high recall rate through bidirectional schema linking. Next, we simplify the database schema and enhance it with rich contextual information to generate an another SQL. Subsequently, a binary selection strategy (selecting the better SQL generated from the complete or simplified database schema) is used to hedge against the potential risks of schema linking. Finally, we employ a multi-turn self-correction approach, integrating feedback from SQL execution results to iteratively refine and optimize bad SQL statements. In the experiments, we evaluate the proposed method RSL-SQL on BIRD and Spider datasets, comparing its performance against extensive Text-to-SQL methods. The experimental results show that when using GPT-4o as the backbone LLM, RSL-SQL achieves 67.21% execution accuracy and 69.39% valid efficiency score on the BIRD dataset (new SOTA among all open-source methods), and 87.2% execution accuracy on the Spider dataset (comparable to SOTA). Moreover, we demonstrate that, when using significantly more cost-effective DeepSeek, RSL-SQL outperforms many GPT-4-based methods on both BIRD and Spider datasets, despite that per-token cost of GPT-4 is 215 times higher than DeepSeek. The ablation study reveals that each component of our method contributes to the overall performance gains. Notably, our bidirectional schema linking technique achieves a high strict recall rate of 92% (a new SOTA) on BIRD dataset, while significantly reducing the average number of columns per query. Both contextual information augmentation and binary selection strategy are verified to improve accuracy steadily, thereby mitigating potential risks associated with schema linking. The main contributions of this paper can be summarized as follows: (1) We investigate potential risks associated with schema linking, and propose RSL-SQL, a novel Text-to-SQL generation framework with robust schema linking that achieves state-of-the-art (SOTA) performance on the BIRD dataset; (2) Extensive experimental results demonstrate the effectiveness and robustness of the proposed method. Our framework also exhibits good transferability, with its performance surpassing many GPT-4-based methods when using much cheaper DeepSeek, demonstrating excellent cost-effectiveness."
https://arxiv.org/html/2411.00066v1,Interpretable Language Modeling via Induction-head Ngram Models,"Recent large language models (LLMs) have excelled across a wide range of tasks, but their use in high-stakes and compute-limited settings has intensified the demand for interpretability and efficiency. We address this need by proposing Induction-head ngram models (Induction-Gram), a method that builds an efficient, interpretable LM by bolstering modern ngram models with a hand-engineered “induction head”. This induction head uses a custom neural similarity metric to efficiently search the model’s input context for potential next-word completions. This process enables Induction-Gram to provide ngram-level grounding for each generated token. Moreover, experiments show that this simple method significantly improves next-word prediction over baseline interpretable models (up to 26%⁢ppercent26𝑝26\%p26 % italic_p) and can be used to speed up LLM inference for large models through speculative decoding. We further study Induction-Gram in a natural-language neuroscience setting, where the goal is to predict the next fMRI response in a sequence. It again provides a significant improvement over interpretable models (20%percent2020\%20 % relative increase in the correlation of predicted fMRI responses), potentially enabling deeper scientific investigation of language selectivity in the brain. The code is available at https://github.com/ejkim47/induction-gram. ††∗ Equal contribution. † Corresponding authors.","Large language models (LLMs) have demonstrated remarkable predictive performance across a growing range of diverse tasks (Brown et al., 2020; OpenAI, 2023; Dubey et al., 2024). However, their proliferation has led to two burgeoning problems. First, LLMs have become increasingly difficult to interpret, often leading to them being characterized as black boxes and debilitating their use in high-stakes applications such as science, medicine, and policy-making (Birhane et al., 2023; Thirunavukarasu et al., 2023; Singh et al., 2024). Moreover, the use of LLMs has come under increasing scrutiny in settings where users require explanations or where models struggle with issues such as fairness (Li et al., 2023) and regulatory pressure (Meskó & Topol, 2023). Second, LLMs have grown to massive sizes, incurring enormous energy costs (Bommasani et al., 2023) and making them costly and difficult to deploy, particularly in low-compute settings (e.g. edge devices). As an alternative to LLMs, ngram models can maintain complete interpretability and are significantly more computationally efficient. While interpretable models can perform as well as black-box models in some domains (Rudin et al., 2021; Mignan & Broccardo, 2019; Ha et al., 2021), there is a considerable gap between the performance of interpretable models and black-box LLMs in next-token prediction. To shrink this gap, we propose Induction-head ngram models (Induction-Gram), a method to build interpretable and efficient LMs by bridging ngram LMs with neural LLMs. Specifically, Induction-Gram starts with Infini-Gram, a state-of-the-art scalable ngram model (Liu et al., 2024). While effective, Infini-Gram struggles with adapting to new contexts and with matching queries that can not be found exactly within a reference dataset (e.g. typos or rephrasings). To remedy these issues, Induction-Gram uses fuzzy matching within the model’s context to retrieve suggestions for a next-token completion, similar to the role played by “induction heads” found in pre-trained transformer models (Olsson et al., 2022; Akyürek et al., 2024). Matching is performed using a custom neural similarity metric that is trained to efficiently score two texts as similar precisely if they lead to similar next-token completions. This extension allows Induction-Gram to achieve state-of-the-art next-token prediction accuracy for an interpretable language model. For example, when evaluating on the Pile dataset and using OpenWebText as the reference corpus, Induction-Gram improve next-token prediction accuracy by 20%p over standard Infini-Gram, shrinking the gap between interpretable models and the black-box GPT-2 model (see Table 1). We further explore Induction-Gram in a natural-language fMRI context, where the goal is to predict the next fMRI response in a session rather than the next token in a sequence. In this setting, Induction-Gram yields a 20% improvement over the baseline interpretable model and allows for auditing how models adapt to local context. Overall, Induction-Gram constitutes a major step towards reverse-engineering mechanistically interpretable language models from modern LLMs. Figure 1: Overview of Induction-Gram pipeline. Induction-Gram predicts the next token by integrating an ngram model (Infini-Gram) with a constructed “induction head”, that efficiently searches for potential next-token completions in the input context."
https://arxiv.org/html/2411.00062v1,Evolving AlignmentviaAsymmetric Self-PlayScalable Preference Fine-Tuning Beyond Static Human Prompts,"Current RLHF frameworks for aligning large language models (LLMs) typically assume a fixed prompt distribution, which is sub-optimal and limits the scalability of alignment and generalizability of models. To address this, we introduce a general open-ended RLHF framework that casts alignment as an asymmetric game between two players: (i) a creator that generates increasingly informative prompt distributions using the reward model, and (ii) a solver that learns to produce more preferred responses on prompts produced by the creator. This framework of Evolving Alignment via Asymmetric Self-Play (eva), results in a simple and efficient approach that can utilize any existing RLHF algorithm for scalable alignment. eva outperforms state-of-the-art methods on widely-used benchmarks, without the need of any additional human crafted prompts. Specifically, eva improves the win rate of Gemma2-9b-it on Arena-Hard from 51.6%percent51.651.6\%51.6 % to 60.1%percent60.160.1\%60.1 % with DPO, from 55.7%percent55.755.7\%55.7 % to 58.9%percent58.958.9\%58.9 % with SPPO, from 52.3%percent52.352.3\%52.3 % to 60.7%percent60.760.7\%60.7 % with SimPO, and from 54.8%percent54.854.8\%54.8 % to 60.3%percent60.360.3\%60.3 % with ORPO, surpassing its 27B version and matching claude-3-opus. This improvement is persistent even when new human crafted prompts are introduced. Finally, we show eva is effective and robust under various ablation settings.","Long-lived artificial intelligence must deal with an ever-evolving, open-ended world, yet currently face constraints in both the scale and quality of available data, and the growth rate at which new, useful information is created. High quality human data, crucial for scaling large language model (LLM) based intelligence, is projected to run out in the next few years (Villalobos et al., 2024); the quality of such data is also expected to stagnate: as LLMs become more capable, they need to solve increasingly complex or new challenges, requiring training data beyond abilities of humans to create. This necessitates a new fundamental mechanism for self-improving, where models can continuously self-generate and self-solve harder problems. We thereby investigate the research question below: Can language models self-create new, learnable tasks to work on, to self-improve to generalize better for human preferences alignment? Figure 3: Pipeline: We generalize classical RLHF with open-ended RLHF, optimized with a creator-solver game for self-improving language models. Our proposed eva strategically evolves prompt distributions with a creator policy, which synthesizes prompts with an easy-to-implement estimate, sample then evolve procedure; specifically, it estimates the informativeness for each prompt by how contrastive the self-generated responses are to the prompt, from the reward signals it receives. The creator evolves new prompts from highly informative prompts, which the solver uses for continual training. Both the solver and creator policy can share the same network or operate independently. See more on our minimax-regret objective that drives the above design in §§\S§ 3. Many preference optimization algorithms (Christiano et al., 2017; Zhao et al., 2023; Rafailov et al., 2023) have been proposed to improve language model alignment, however, they all default to fixed prompt training distributions. Such fixed training paradigms lack of scalability and inevitably lead to: (i) generalization issues (models may underperform or hack on instructions that are insufficiently represented within the fixed set) and (ii) efficiency issues (data annotation and model training are costly, however not all prompts provide the same utility; it is wasteful to invest in sub-optimal fixed set, while identifying informative prompts by human efforts is non-trivial) (Team et al., 2023; 2024). Motivated by such limitations of optimizing over a specific, static distribution of prompts, we develop eva (Evolving Alignment via Asymmetric Self-Play), a scalable and open-ended RLHF framework allowing autonomous evolving of training distributions for self-improving language models, as in Figure 2, 3. Central to eva is a game with the minimax-regret objective, achieved by alternating optimization between creating prompts and solving them. Such interplay encourages evolving curricula (Parker-Holder et al., 2022), and benefits both generalization and efficiency (see also §§\S§ 3.4). Orthogonal to recent self-play studies in LLM alignment (Munos et al., 2023; Choi et al., 2024; Wu et al., 2024), eva is asymmetric (Sukhbaatar et al., 2017); and in contrast to many self-training works (Gulcehre et al., 2023; Singh et al., 2023; Dong et al., 2023) focusing on improving in 𝒴∣𝒳conditional𝒴𝒳{\mathcal{Y}}\mid{\mathcal{X}}caligraphic_Y ∣ caligraphic_X, we jointly optimize in (𝒳,𝒴)𝒳𝒴({\mathcal{X}},{\mathcal{Y}})( caligraphic_X , caligraphic_Y ) by generative exploration, with two policies of different goals: • Creator : evolves the prompt distribution for alignment. • Solver : produces responses and optimizes alignment based on the evolving prompts. Our main contributions include: • A new principle: We propose a generalized open-ended RLHF objective for aligning language models, which seeks to jointly optimize the prompt distribution and the response policy, thus incentivizes models to self-improve to generalize well on new, unseen tasks beyond the initial training prompt distribution for alignment, as in Definition 1. • A new algorithm: To optimize the objective, we design a practical algorithm via asymmetric self-play, which is implemented through alternating optimization in a creator-solver game, and can be easily plugged into any existing alignment pipeline, as in Algorithm 1. • State-of-the-art performance: We empirically validate our method on public alignment benchmarks and present general strong performance improvement when plugged in with different preference optimization algorithms (i.e., DPO, SPPO, SimPO, ORPO). We also conduct extensive ablation studies that provide additional insights on the choice of informativeness metric, reward model, and training schedules, as in §§\S§ 4. eva is easy to deploy. We hope it can serve as a scalable method for the AI community to build open-ended, sample-efficient and robustly self-improving intelligence, that aligns with human values."
https://arxiv.org/html/2411.00056v1,Generating Diverse Negations from Affirmative Sentences,"Despite the impressive performance of large language models across various tasks, they often struggle with reasoning under negated statements. Negations are important in real-world applications as they encode negative polarity in verb phrases, clauses, or other expressions. Nevertheless, they are underrepresented in current benchmarks, which mainly include basic negation forms and overlook more complex ones, resulting in insufficient data for training a language model. In this work, we propose NegVerse, a method that tackles the lack of negation datasets by producing a diverse range of negation types from affirmative sentences, including verbal, non-verbal, and affixal forms commonly found in English text. We provide new rules for masking parts of sentences where negations are most likely to occur, based on syntactic structure and use a frozen baseline LLM and prompt tuning to generate negated sentences. We also propose a filtering mechanism to identify negation cues and remove degenerate examples, producing a diverse range of meaningful perturbations. Our results show that NegVerse outperforms existing methods and generates negations with higher lexical similarity to the original sentences, better syntactic preservation and negation diversity. The code is available in https://github.com/DarianRodriguez/NegVerse.","Recent advancements in natural language processing (NLP) have enhanced various applications such as text generation [42], translation [9] and summarization [2], but handling negation remains a significant challenge [13]. Negations are crucial for reasoning and effective communication, as they express denial, contradiction, and absence. This is especially important in critical fields like biomedicine, where misinterpreting negated conditions can have serious consequences. For example, Large Language Models (LLMs) identifying acute bleeding [32] have misclassified cases with negated phrases, revealing bias and a limited understanding of negations [8, 20]. Despite their importance, existing literature has established that language models struggle with negated sentences in tasks such as cloze completion, NLI, QA, and classification [1, 13, 20]. For example, the work in [39] found an inverse scaling trend among models such as GPT-J, GPT-3333, Flan-T5555, GPT-Neo, and OPT (ranging from 125125125125M to 6666B parameters), where larger models tend to perform worse on negation tasks and often produce incorrect answers with high confidence. Similarly, [16] and [18] demonstrated that models like BERT, RoBERTa, GPT-2, BART, and T5 frequently generate identical outputs for opposite statements and misinterpret sentences, such as classifying ""The man in the blue shirt is relaxing on the rocks"" as entailing ""A man is not wearing a blue shirt"". Negations are also underrepresented in most benchmark datasets, both in terms of frequency and complexity. In particular, the works in [12] and [13] show that general-purpose English corpora, such as reviews, conversations, Wikipedia, and books, contain between 22.6%percent22.622.6\%22.6 % and 29.9%percent29.929.9\%29.9 % sentences with negations. In contrast, some natural language inference benchmarks have around 8.7%percent8.78.7\%8.7 %, while other datasets, such as COPA [29] and QQP [5], contain 0.8%percent0.80.8\%0.8 % and 8.1%percent8.18.1\%8.1 % respectively. To improve negation understanding in NLP models, it is crucial to expand annotated datasets to cover various types of negation across different domains [23]. Transformer-based models, such as RoBERTa [21] and BERT [7], often struggle with negations due to their underrepresentation in training data [12]. Current benchmarks primarily focus on verbal negations, lacking syntactic and morphological negations [8, 12]. Although some existing methods address verbal negations [13] or use rule-based augmentation [10], they still cover only a limited range of negation types. Contributions: To address this issue, we introduce NegVerse, a method that generates a diverse range of syntactic and morphological negations, including non-verbal, verbal, and affixal forms, to enrich the training datasets. NegVerse (a) keeps the produced negated data closely aligned with the original sentences by employing a masking strategy at both token and subtree levels; and (b) addresses the shortage of affixal negation datasets and other negation forms, by assembling 362362362362 unique sentences using LLama-2222 and other sources, such as COPA [29] and SNLI [3]. We introduce an efficient masking strategy to insert negations while maintaining sentence fluency. Additionally, a new filtering mechanism is used to exclude degenerate outputs, capturing key negation cues effectively. We use a GPT-2-based model to generate negated sentences and implement a filtering mechanism that screens the generated negations for closeness, duplicates, and validity. We provide extensive empirical evidence of our NegVerse’s efficiency and improved performance using relevant criteria such as closeness, diversity, and text quality [22, 31, 43] on various datasets against state-of-the-art baselines."
https://arxiv.org/html/2411.00053v2,ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate,"Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools for various language-based tasks. Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models, frequently referred to as multi-agent debate (MAD). While debate shows promise as a means of improving model efficacy, most works in this area treat debate as an emergent behavior, rather than a learned behavior. In doing so, current debate frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. To address this limitation, we propose ACC-Debate, an Actor-Critic based learning framework to produce a two-agent team specialized in debate. We demonstrate that ACC-Debate outperforms SotA debate techniques on a wide array of benchmarks.","Recently, large language models (LLMs) have rapidly become a cornerstone in various applications, redefining how we process and generate language at scale (Thirunavukarasu et al., 2023; Hadi et al., 2023; Jiang et al., 2024). Their ability to handle diverse tasks, from translation (Zhu et al., 2024; Otter et al., 2020) to answering complex questions (Zhang et al., 2024; Hao et al., 2024; Havrilla et al., 2024), has attracted the attention of both industry as well as academia. However, despite these advancements, LLMs still exhibit notable weaknesses, particularly when it comes to answering factual questions and reasoning (Tonmoy et al., 2024; Rawte et al., 2023; Huang et al., 2023). To address these limitations, several techniques have been proposed, such as Chain-of-Thought (CoT) prompting (Wei et al., 2022), self-reflection (Ji et al., 2023; Shinn et al., 2023), and multi-agent debate (MAD) (Du et al., 2023), to name a few. These approaches aim to improve the reasoning abilities of LLMs by guiding them toward more accurate answers through structured thinking or discourse. However, the majority of these techniques do not involve training the model specifically for these tasks but instead rely on zero-shot or few-shot capabilities. In particular, multi-agent debate approaches make use of off-the-shelf general-purpose LLMs, which are not trained to collaborate. Such approaches rely on collaboration as an emergent, rather than a learned, behavior. While, in some cases, these emergent behaviors are sufficient, the question remains: Can these methods be improved by imbuing models directly with collaborative abilities? To answer this, we propose a novel paradigm to train teams of LLMs to collaboratively solve tasks. Most relevant to the main idea of this paper is DebateGPT (Subramaniam et al., 2024). DebateGPT uses debate as a mechanism to attain higher-quality fine-tuning data. Differing from our work, their approach focuses on using debate to generate better training data for a single model that gives single responses, rather than optimizing the LLMs themselves for collaborative problem-solving over multiple rounds of conversation. In this paper, we propose a novel framework Actor-Critic Debate (ACC-Debate) which jointly trains a two-agent team to collaboratively solve problems through iterative conversation; this team consists of an actor-agent, responsible for providing answers for a given task, and a critic-agent, responsible for assisting the actor-agent with feedback on its answers. In our training pipeline, we introduce a novel off-policy learning scheme called ”guided-debate” to generate high-quality multi-turn training data to enhance the actor’s and critic’s performance on challenging tasks. To summarize, our contributions are as follows: • We are the first to propose a framework for training a team of LLMs jointly (Actor-Critic) within the context of debates. • We introduce a novel data generation scheme, “guided debate trajectories”, which enables the efficient creation of high-quality training data for both the actor and critic roles. • Our extensive experiments demonstrate that our method, ACC-Debate, significantly outperforms existing state-of-the-art approaches."
https://arxiv.org/html/2411.00049v1,Rule by Rule: Learning with Confidence through Vocabulary Expansion,"In this paper, we present an innovative iterative approach to rule learning specifically designed for (but not limited to) text-based data. Our method focuses on progressively expanding the vocabulary utilized in each iteration resulting in a significant reduction of memory consumption. Moreover, we introduce a Value of Confidence as an indicator of the reliability of the generated rules. By leveraging the Value of Confidence, our approach ensures that only the most robust and trustworthy rules are retained, thereby improving the overall quality of the rule learning process. We demonstrate the effectiveness of our method through extensive experiments on various textual as well as non-textual datasets including a use case of significant interest to insurance industries, showcasing its potential for real-world applications.Keywords. Rule Learning, Explainable Artificial Intelligence, Text Categorization, Reliability of Rules","In recent years, the rapid advancement of Artificial Intelligence (AI) technologies has revolutionized various industries and aspects of our daily lives (cf. Lu (2019); Zhang and Lu (2021); Lee (2020), for instance). However, as AI systems become more complex and sophisticated, the need for transparency and interpretability in their decision-making processes has become increasingly crucial. The concept of Explainable Artificial Intelligence (XAI; see for example Angelov et al. (2021); Ali et al. (2023)) has emerged as a response to this demand, aiming to enhance the trust, accountability and understanding of AI systems by providing explanations for their outputs and actions. Indeed, in many application areas of machine learning, like automotive, medicine, health and insurance industries, etc., the need for security and transparency of the applied methods is not only preferred but increasingly often of utmost importance or even required by law (cf. EU Artificial Intelligence Act for instance). A classical example in this context – often categorized as most informative in the area of XAI (Hulsen (2023)) – is the generation of deterministic (if-then-else) rules that can be used for classification. For instance, regarding the prediction of the health status of a patient the easily comprehensible rule shown below is clearly preferable over the unexplainable outcome of a black-box like a neural network for both the doctor as well as the patient since the decision is fully transparent. ⬇ IF BloodPressure in [70,80] AND Insulin in [140,170] THEN Diabetes = Yes. The field of Rule Induction (Fürnkranz et al. (2012)) investigates the construction of simple if-then-else rules from given input/output examples and provides some commonly applied methods to obtain deterministic rules for the solution of a (classification) problem at hand (cf. Section 2.1). Representative examples of such rules are shown for each data set considered in our experiments in Section 5, illustrating the major advantages of rule learning methods, namely their transparency and comprehensibility, which make them a desirable classification tool in many areas. Unfortunately, these benefits are coupled with the major drawback of generally less accurate results – often referred to as interpretability-accuracy trade-off (Gunning et al. (2019)). Moreover, for a long time it has not been possible to efficiently apply rule learning methods on very large data sets (Mitra and Baral (2018)) as considered for instance in the industrial use case discussed in Section 5.3 which is of central interest to us and our collaboration partner – the Allianz Private Krankenversicherung (APKV). We have already extensively investigated these issues in the course of our collaboration with the above-mentioned company from insurance industries with the basic aim to establish rule learning methods – particularly 𝖥𝖮𝖨𝖫𝖥𝖮𝖨𝖫\mathsf{FOIL}sansserif_FOIL (Quinlan (1990)) and 𝖱𝖨𝖯𝖯𝖤𝖱𝖱𝖨𝖯𝖯𝖤𝖱\mathsf{RIPPER}sansserif_RIPPER (Cohen (1995)) – as an efficient tool in the reimbursement process. In previous work (Nössig et al. (2024, 2024)) we introduced approaches to solve the above-mentioned difficulties concerning the application of rule learning methods in a production environment at least to some extent. First, we developed a modular approach (cf. Section 2.2) enabling the application of ordinary rule learning methods such as 𝖥𝖮𝖨𝖫𝖥𝖮𝖨𝖫\mathsf{FOIL}sansserif_FOIL and 𝖱𝖨𝖯𝖯𝖤𝖱𝖱𝖨𝖯𝖯𝖤𝖱\mathsf{RIPPER}sansserif_RIPPER on very large data sets including several hundreds of thousands examples. However, the in general poorer performance compared to state-of-the-art methods with respect to accuracy remained. So, we came up with an extension of the introduced modular approach in the form of the voting approach shortly described in Section 2.3. After consultation with our collaboration partner, we agreed that at the end of the day it is even more important to ease the understanding of a classification made than to make the whole procedure fully transparent. So, this additional step in the process of decision making deals with the interpretability-accuracy trade-off by incorporating an ensemble of explainable as well as unexplainable methods. As a consequence, the procedure loses its full transparency but gains a significant improvement of classification accuracy, while preserving end-to-end explainability by corroborating each prediction with a comprehensible rule. At this point we have already made a huge step towards the application of trustworthy AI methods in the company. However, another crucial problem that is not solved in a satisfying manner by the combination of the two approaches above is the handling of text-based data. The data basis for the reimbursement use case is a collection of (scanned) bills where we extracted the most important information in the form of nominal (and continuous) attributes as described in more detail in Section 5.3. Unfortunately, by this way of preprocessing we might lose a lot of additional information given by the original textual data. However, up to this point, we have mainly considered nominal data with the only exception of the IMDB movie reviews data set111See https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews. which has been part of the benchmark data sets in the evaluation of our modular approach. The results have not been really satisfying because the achieved accuracy has been below our expectations on the one hand – which is solvable by our voting approach at least to some extent – but on the other hand it has shown that the form and complexity of the generated rules is not reasonably applicable for (end-to-end) explainable classification. What seems to be not too problematic considering the comparatively small IMDB data set, is the choice and especially the size of the underlying dictionary used to generate rules. For the movie reviews we simply considered the thousand most common words in the data set but the bills handed in to the insurance are far more complex. They usually consist of at least one page of text using partly highly complicated technical terms from various medical fields instead of 2-3 sentences describing personal opinions about movies in simple language. Note that simply using a much larger dictionary as basis for the rule learning process is not the remedy because the computation time as well as the memory consumption for the generation of the rules increases drastically with increasing dictionary size. In this paper, we especially aim to gain more control over the complexity of the generated rules and make it possible to reasonably apply rule learning methods such as 𝖥𝖮𝖨𝖫𝖥𝖮𝖨𝖫\mathsf{FOIL}sansserif_FOIL and 𝖱𝖨𝖯𝖯𝖤𝖱𝖱𝖨𝖯𝖯𝖤𝖱\mathsf{RIPPER}sansserif_RIPPER also on text-based data by starting off with a concise dictionary (designed by domain experts) and decreasing the number of considered examples before extending the applied dictionary in an iterative way. The intention behind this approach is to learn general rules in a first step using a small and computationally rather cheap dictionary for a very large number of input examples. With each learned rule the number of considered positive examples decreases by definition of the rule learning algorithms. When a certain point is reached – either a predefined number of iterations or a condition regarding the quality of a rule as described in detail in Section 4 – we extend the dictionary to handle more specific examples. This way of proceeding can be repeated until a quite comprehensive dictionary is applied on a few remaining edge cases. In addition, the basic idea behind this way of proceeding can be applied also on nominal (and continuous) data in order to improve the quality of a rule as explained in Section 4 and shown in the experimental evaluation in Section 5. Apart from evaluating our approach on common benchmark data sets regarding classification of textual data (IMDB (Maas et al. (2011)), Reuters-21578 (Lewis (1997)), Hatespeech222See https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset.), we also show the advantages of the basic idea of our approach applied on non-textual data considering some common data sets from the UCI Machine Learning Repository (Dua and Graff (2017)) or kaggle333See https://www.kaggle.com/., respectively. Moreover, we present novel results on explainable classifications of bills for reimbursement particularly using textual data as input. The latter case study stems from an industrial collaboration with Allianz Private Krankenversicherung (APKV) which is an insurance company offering health insurance services in Germany. Summed up, our main goal is to solve a text-based classification problem in reasonable time and computational complexity by applying easily comprehensible rules that have been generated by using a dictionary of variable size. Moreover, we define a measure for the quality of a rule and integrate it in the iterative way of proceeding our proposed approach is based on. As shown in the experiments, this iterative rule refinement is beneficial even for non-textual data. All in all, this paper directly builds on our previous work and expands upon the approaches presented therein to handle especially textual data more efficiently and gain more control over the complexity of the generated rules by iteratively extending the size of the applied dictionary (or in general the number of attributes). More precisely, we make the following contributions. 1.0.1 Iterative Approach Based on Rule Learning We introduce a novel iterative approach based on rule learning exploiting the benefits of a variable number of attributes (in particular an adaptable dictionary) during the generation of a rule set (see Section 4 for further details). Together with the modular as well as the voting approach introduced in our previous work (Nössig et al. (2024, 2024)), this makes rule learners a serious alternative to state-of-the-art classification tools and enables the application of tried and trusted rule learning methods in a complex and diverse production environment. 1.0.2 Experimental Evaluation Further, we provide ample experimental evidence that our methodology not only clearly simplifies the application of rule learning methods on text-based data but also provides significant improvements on the accuracy for the standard benchmarks (see Section 5). 1.0.3 Industrial Use Case Finally, we show that our approach makes it possible to efficiently apply the way of proceeding we successfully introduced in previous work now also on text-based data, in particular the raw OCR scans used for reimbursement. We emphasise that our classification yields comprehensible rules that are of direct interest to our industrial collaboration partner (see Section 5.3). Overview. In Section 2 we introduce the major definitions and notations as well as the general ideas behind our approaches from previous work. Section 3 serves to discuss related work focusing on similar goals as considered in this paper, especially on various forms of (explainable) text-based classification, while we concretely introduce our aforementioned iterative approach as well as the Value of Confidence applied therein as a measure of reliability of a rule in Section 4. Section 5 provides ample evidence of the advantages of our approach and presents the case study mentioned. Finally, in Section 6 we summarize the main results and discuss ideas for future work."
https://arxiv.org/html/2411.00042v1,Problem Categorization Can Help Large Language Models Solve Math Problems,"In this paper, we explore how to optimize the usage of Large-Language Models to quickly and accurately solve mathematical problems. In particular, we show the effectiveness of using the classification of problems into different categories to facilitate problem-solving. Additionally, we optimize the classification of problems into categories by creating an accurate dataset. We believe that our technique for problem-solving works by helping mitigate hallucination in LLMs which is key to unlocking their ability to solve math problems.","Large-Language models (LLMs) have played an important role in many domains [3] [6]. Solving problems in mathematics is the latest frontier and is considered to be particularly difficult for LLMs [13]. Today, software such as Wolfram||||Alpha can already compute large expressions and solve complicated equations. However, complicated mathematical problems often require much more than mechanistic calculation: these problems require making difficult and sometimes unique observations. This makes solving math problems inherently hard: many mathematical problems require the model to be creative with its approach, which is not required for other tasks such as simple arithmetic. Many AI models provide undesirable results when given math problems. For example, the famous model ChatGPT often gets even simple competition math problems wrong [7]. This is due to hallucination, which is when a model generates a response with incorrect logic. This topic will be explored in more depth in the next section. Researchers at Google DeepMind have recently developed models that are much better at mathematical problem solving, named AlphaGeometry and AlphaProof. These models address some of the challenges of LLMs by integrating text generation with formal reasoning or theorem provers [11]. AlphaGeometry and AlphaProof have had impressive results: AlphaProof, for example, scored a 28 on the 2024 International Mathematics Olympiad, the equivalent of a silver medal [16], and AlphaGeometry performs better than the average IMO silver medalist on geometry problems [14]. However, even these models have drawbacks: AlphaProof, for example, took up to three days to solve some problems on the IMO [16], which is well over the time limit for the competition and is not feasible if one would like a solution to a problem quickly. Additionally, AlphaGeometry can only solve geometry problems, and AlphaProof performs poorly on problems in combinatorics [17]. In this paper, we consider the problem of helping LLMs compute solutions to math problems quickly. We will be focusing on problems that are distinct from those solved by AlphaProof and AlphaGeometry: rather than focusing on proving theorems, we will work on computational problems that require computing a numeric answer. In this context, we develop an approach that reduces the scope for LLM hallucination by first categorizing problems into four categories and then providing them as input to an LLM along with a category-specific strategy to solve. The categories we consider are algebra, combinatorics, geometry, and number theory. The knowledge of a problem’s category is used in an informed manner to determine one of two strategies to approach the problem – ”chain of thought” and ”program of thought” – described in the next section. We will show that this approach improves the accuracy of problem-solving. To categorize problems into the four aforementioned categories, we develop a simple deep neural network. Our main insight in training an effective neural net is to provide the right data to train the model. In particular, we show the importance of factors such as answer extraction and achieving a careful balancing of different problem categories. We then show how to (probabilistically) associate a specific problem-solving strategy with each identified category. Our empirical analysis shows that our neural model can categorize problems with over an 80%percent8080\%80 % accuracy. We also compare the effect of using this categorization and the associated problem-solving strategy on LLM problem-solving against using the ground-truth categories to probabilistically select strategies and randomly chosen strategies. We find that our approach does 67% better than randomly generated strategies, but 29% worse than selecting strategies based on ground-truth categories. To summarize, the main findings from our work are: 1. Categorizing problems and determining an optimal strategy to solve a problem before giving the problem to an LLM significantly improves the problem-solving accuracy of the LLM. 2. When creating a model to categorize problems, obtaining the correct data is vital, and can drastically increase the accuracy of the model. 3. The approach of categorizing problems and determining the optimal strategy not only increases problem-solving accuracy but also reduces hallucination in LLMs."
https://arxiv.org/html/2411.00041v1,NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical Scholarly Document Categorization and Question Answering,"The growing volume of biomedical scholarly document abstracts presents an increasing challenge in efficiently retrieving accurate and relevant information. To address this, we introduce a novel approach that integrates an optimized topic modelling framework, OVB-LDA, with the BI-POP CMA-ES optimization technique for enhanced scholarly document abstract categorization. Complementing this, we employ the distilled MiniLM model, fine-tuned on domain-specific data, for high-precision answer extraction. Our approach is evaluated across three configurations: scholarly document abstract retrieval, gold-standard scholarly documents abstract, and gold-standard snippets, consistently outperforming established methods such as RYGH and bio-answer finder. Notably, we demonstrate that extracting answers from scholarly documents abstracts alone can yield high accuracy, underscoring the sufficiency of abstracts for many biomedical queries. Despite its compact size, MiniLM exhibits competitive performance, challenging the prevailing notion that only large, resource-intensive models can handle such complex tasks. Our results, validated across various question types and evaluation batches, highlight the robustness and adaptability of our method in real-world biomedical applications. While our approach shows promise, we identify challenges in handling complex list-type questions and inconsistencies in evaluation metrics. Future work will focus on refining the topic model with more extensive domain-specific datasets, further optimizing MiniLM and utilizing large language models (LLM) to improve both precision and efficiency in biomedical question answering.","With around 2.5 million new research contributions every year [1] specifically the rapidly growing biomedical research, the need for efficient and accurate information retrieval methods has become increasingly critical. The total volume of scholarly documents and the complexity of biomedical queries present substantial challenges in extracting relevant answers from vast repositories of knowledge. As the field advances, researchers are often faced with the daunting task of sifting through an extensive array of documents to obtain precise information. This highlights the necessity for robust answer extraction and document categorization methods that can enhance the accessibility of vital information. To address these challenges, this research proposes a neuro-symbolic approach that combines optimized topic modelling with advanced machine learning techniques, effectively integrating symbolic reasoning with neural representations for enhanced document retrieval and answer extraction. Specifically, we explore the efficacy of our method through three distinct configurations: the utilization of scholarly document abstract retrieval methods, golden scholarly documents abstract, and golden snippets. Our method evaluations demonstrate that our topic model-based document categorization outperforms existing methods, such as RYGH and bio-answer finder, which utilize a complex blend of techniques like BM25 [2], ElasticSearch [3], and various transformer models [4]. This suggests that a simpler yet fine-tuned approach can lead to more effective and cost-efficient solutions for biomedical information retrieval. The primary research question in this investigation was: How can optimized scholarly document abstract categorization and answer extraction methodologies improve the accuracy and efficiency of information retrieval in the biomedical domain? Addressing this question is vital, as it not only enhances the precision of answer extraction but also reduces the cognitive load on researchers seeking relevant information. Furthermore, our findings reveal that even distilled smaller language models like MiniLM [5] can effectively extract answers when fine-tuned on domain-specific data, particularly when focused on scholarly document abstracts rather than complete documents. While the comparison with the use of Large Language Models [6] instead of the smaller MiniLM is also an interesting research avenue, it is deemed out of the scope of this research. Overall our promising results suggest a potential shift in focus for future biomedical information retrieval methods, advocating for strategies that emphasize the utility of concise scholarly abstracts. In summary, the contributions of this research comprise: 1. A novel neuro-symbolic answer extraction methodology that combines optimized topic modelling and advanced machine learning techniques, effectively addressing the challenges in biomedical information retrieval, particularly in extracting answers from an expanding corpus of scholarly documents abstract. 2. A novel answer extraction methodology that combines optimized topic modelling and advanced machine learning techniques, effectively addressing the challenges in biomedical information retrieval, particularly in extracting answers from an expanding corpus of scholarly documents abstract. 3. A comprehensive evaluation of the proposed method across three configurations— scholarly document abstract retrieval, golden scholarly documents abstract, and golden snippets—demonstrating its superior performance over existing methods like RYGH and bio-answer finder, while highlighting the advantages of simplicity and domain-specific fine-tuning. 4. Insights into the effective use of distilled models, specifically MiniLM, for accurate answer extraction when fine-tuned on domain-specific data, along with recommendations for future biomedical information retrieval methods to focus on concise scholarly document abstracts for enhanced efficiency and accuracy. methods Phase Approach bio-answerfinder A, B Bio-AnswerFinder, ElasticSearch, Bio-ELECTRA, ELECTRA, BioBERT, SQuAD, wRWMD, BM25, LSTM, T5 bioinfo A, B BM25, ElasticSearch, distant learning, DeepRank, universal weighting passage mechanism (UPWM), PARADE-CNN, PubMedBERT LaRSA A, B ElasticSearch, BM25, SQuAD, Macro Passage Ranking, BioBERT, BoolQA, BART ELECTROBERT A, B ELECTRA, ALBERT, BioELECTRA, BERT NEUROSYM-BIOCAT A, B OVB-LDA, CMA-ES, MiniLM RYGH A BM25, BioBERT, PubMedBERT, T5, BERTMeSH, SciBERT gsl A BM25, BERT, dual-encoder BioNIR B sBERT, distance metrics KU-methods B BioBERT, data augmentation MQ B tf-idf, sBERT, DistilBERT Ir_sys B BERT, SQuAD1.0, SpanBERT, XLNet, PubMedBERT, BioELECTRA, BioALBERT, BART UDEL-LAB B BioM-ALBERT, Bio-ELECTRA, SQuAD MQU B BART, summarization NCU-IISR/AS-GIS B BioBERT, BERTScore, SQuAD, logistic-regression Table 1: Approaches used by BioASQ 10b Participants."
https://arxiv.org/html/2411.00039v1,Linear Chain Transformation: Expanding Optimization Dynamics for Fine-Tuning Large Language Models,"Fine-tuning large language models (LLMs) has become essential for adapting pretrained models to specific downstream tasks. In this paper, we propose Linear Chain Transformation (LinChain), a novel approach that introduces a sequence of linear transformations during fine-tuning to enrich optimization dynamics. By incorporating multiple linear transformations into the parameter update process, LinChain expands the effective rank of updates and enhances the model’s ability to learn complex task-specific representations. We demonstrate that this method significantly improves the performance of LLM fine-tuning over state-of-the-art methods by providing more flexible optimization paths during training, while maintaining the inference efficiency of the resulting model. Our experiments on various benchmark tasks show that LinChain leads to better generalization, fewer learnable parameters, and improved task adaptation, making it a compelling strategy for LLM fine-tuning.","Large language models (LLMs), such as ChatGPT, Claude, and LLaMA, have achieved remarkable success across a wide variety of natural language processing (NLP) tasks, ranging from text generation to question answering and summarization. Their ability to learn rich representations from vast amounts of data has enabled significant advancements in language understanding and generation. However, as LLMs continue to grow in size, the computational cost associated with fine-tuning these models for specific tasks becomes increasingly prohibitive. The challenge lies in balancing the power of large models with the need for efficient adaptation, especially when deploying them in real-world scenarios where computational resources may be limited. As LLMs continue to grow in scale, with billions of parameters, fine-tuning these models for specific tasks has become computationally expensive. To address this, several Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced, focusing on updating only a small subset of parameters while maintaining task-specific performance. A prominent method in this area is Low-Rank Adaptation (LoRA) [1], which updates a frozen pre-trained model by introducing a low-rank decomposition to the parameter updates. LoRA decomposes the weight update Δ⁢WΔ𝑊\Delta Wroman_Δ italic_W into two smaller matrices A∈ℝd1×r𝐴superscriptℝsubscript𝑑1𝑟A\in\mathbb{R}^{d_{1}\times r}italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_r end_POSTSUPERSCRIPT and B∈ℝr×d2𝐵superscriptℝ𝑟subscript𝑑2B\in\mathbb{R}^{r\times d_{2}}italic_B ∈ blackboard_R start_POSTSUPERSCRIPT italic_r × italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, reducing the number of trainable parameters from d1×d2subscript𝑑1subscript𝑑2d_{1}\times d_{2}italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT to (d1+d2)⁢rsubscript𝑑1subscript𝑑2𝑟(d_{1}+d_{2})r( italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) italic_r [1]. This has been effective in reducing memory usage and training costs, while preserving task performance. However, LoRA’s low-rank updates can limit the model’s expressiveness, particularly for tasks requiring more complex feature interactions. This has led to several LoRA variants, each attempting to balance efficiency and flexibility. For example, Mixture-of-Subspaces LoRA (MoSLoRA) decomposes the low-rank updates into multiple subspaces and uses a learnable mixer to fuse these subspaces more flexibly, improving performance with negligible extra parameters [2]. These approaches highlight the growing interest in fine-tuning models in a way that balances performance with efficiency. Yet, despite their innovations, these methods remain fundamentally limited by their reliance on low-rank approximations, which can fail to capture the complexity of certain tasks. While LoRA and MoSLoRA reduce the computational cost of fine-tuning by limiting the number of trainable parameters, they impose a constraint on the model’s representational power due to their fixed low-rank approximation. This constraint can hinder performance on tasks that require more complex feature interactions. A natural extension of these methods would be to increase the expressiveness of the fine-tuning process while preserving its computational efficiency. The motivation behind our work is to explore how additional flexibility in parameter updates can improve model performance without sacrificing efficiency. In particular, we hypothesize that introducing a chain of linear transformations—which we term Linear Chain Transformation (LinChain)—can provide the model with a richer set of optimization paths, thus enhancing its ability to adapt to task-specific data. This increased flexibility can help overcome the limitations of existing low-rank approaches by expanding the space of possible transformations. In this paper, we propose LinChain, a novel fine-tuning method that introduces a sequence of linear transformations to enhance the model’s expressiveness while maintaining efficiency. LinChain addresses the limitations of existing low-rank adaptation methods by allowing for a richer set of parameter updates without introducing non-linearity. The contributions of this paper are summarized as follows. • This study uncovers a key insight: a chain of linear transformations enhances LLM training. The proposed LinChain method increases the optimization dynamics of LLM fine-tuning, improving the performance of fine-tuned models on tasks requiring diverse or complex transformations. • An in-depth analysis reveals that LinChain significantly extends the practical capabilities of LoRA, enabling more flexible and expressive updates while maintaining the efficiency benefits of low-rank adaptations. • Our experiments demonstrate that LinChain outperforms state-of-the-art fine-tuning methods across various tasks, even with fewer parameters, leading to faster convergence and improved task adaptation. The remainder of this paper is organized as follows: Section 2 introduces the preliminaries and our motivation, Section 3 outlines the proposed LinChain method, Section 4 presents our experimental results, Section LABEL:sec:relatedwork reviews related works, and Section 5 concludes with a discussion of future directions."
https://arxiv.org/html/2411.00038v2,Topic-Conversation Relevance (TCR)Dataset and Benchmarks,"Workplace meetings are vital to organizational collaboration, yet a large percentage of meetings are rated as ineffective [cutler2021meeting]. To help improve meeting effectiveness by understanding if the conversation is on topic, we create a comprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety of domains and meeting styles. The TCR dataset includes 1,500 unique meetings, 22 million words in transcripts, and over 15,000 meeting topics, sourced from both newly collected Speech Interruption Meeting (SIM) data and existing public datasets. Along with the text data, we also open source scripts to generate synthetic meetings or create augmented meetings from the TCR dataset to enhance data diversity. For each data source, benchmarks are created using GPT-4111The GPT-4 model used in this paper is GPT-4-32k. to evaluate the model accuracy in understanding transcription-topic relevance.","Since the 2020 COVID-19 pandemic, an increasing share of meetings have shifted from in-person to online. The Gartner 2021 Digital Worker Experience Survey reports that the number of in-person meetings dropped from 63% in 2019 to 33% in 2021 [Gartner2021]. The same survey predicted that in 2024, only 25% of the meetings will happen in person. Together with the growing number of online meetings, there are ongoing complaints about ineffective meetings due to a lack of focused discussions or focused tasks [ScottMentalModels2024, nixon92, bang2010Focus, cohen2011meeting]. Having a meeting facilitator to keep the discussions focused is one of the meeting design characteristics enabling more effective meetings [cohen2011meeting, bang2010Effectiveness]. Measuring how relevant a conversation transcript is to an intended topic is crucial to quantifying how focused the communication is, and to creating tools that behave as a virtual meeting moderator by keeping the discussion on-track. A very low rating on the relevance of the conversation to the topic meant for discussion would be an indicator of a non-focused discussion. In practice this translates to the problem of keeping discussions focused on a predefined meeting agenda. While there is existing work about the importance of topics serving as input to text summarizing models [you2020topic, dhankar2022], we could not find references about work studying the relevance of a topic to a particular body of text it didn’t originate from. One of our intuitions for why this field has had little exploration is because of technological limitations before the recent Generative AI advancements. With the current advancement in the field of Generative AI, deep understanding of language and relationships between bodies of text has reached new levels of accuracy, and has gotten very close to human performance [sartori2023, bubeck2023sparks, cai2024large, gandhi2023understanding, lampinen2023language]. To begin investing in measuring the relevance of a conversation to a predefined agenda topic, a comprehensive dataset of conversations associated with the topic of each conversation section is needed. Ideally, the topics should be defined before the conversation starts in a pre-meeting agenda style. There are several public datasets built from real human conversations that serve as the base for our Topic-Conversation Relevance (TCR) dataset; however, most of the topics from these datasets are post-meeting minutes that summarize what happens instead of what is planned. The contributions of the TCR dataset are (1) We create a large topic-conversation dataset covering multiple domains of meetings. This dataset consists of the newly collected meetings and aggregated public data sources. (2) We use GPT-4 to rewrite long and detailed meeting minutes into a pre-meeting agenda topic style. (3) We provide a design of an extensible schema that allows users to create variations of meetings where topics can be flexibly added and removed. (4) We open source scripts for data augmentation and synthetic meeting creation on top of the TCR dataset. We review the related works in Section 2. We present the datasets and the schema in Section 3, and elaborate to discuss the new SIM data collection and public data sources. In Section 4 we go over the benchmark results generated by GPT-4 on the Topic-Conversation Relevance task, and share insights from running such prompts across datasets. In Section 5, we describe limits and future work in this direction."
https://arxiv.org/html/2411.00034v1,Is Our Chatbot Telling Lies? Assessing Correctness of an LLM-based Dutch Support Chatbot,"Companies support their customers using live chats and chatbots to gain their loyalty. AFAS is a Dutch company aiming to leverage the opportunity large language models (LLMs) offer to answer customer queries with minimal to no input from its customer support team. Adding to its complexity, it is unclear what makes a response correct, and that too in Dutch. Further, with minimal data available for training, the challenge is to identify whether an answer generated by a large language model is correct and do it on the fly.This study is the first to define the correctness of a response based on how the support team at AFAS makes decisions. It leverages literature on natural language generation and automated answer grading systems to automate the decision-making of the customer support team. We investigated questions requiring a binary response (e.g., Would it be possible to adjust tax rates manually?) or instructions (e.g., How would I adjust tax rate manually?) to test how close our automated approach reaches support rating. Our approach can identify wrong messages in 55% of the cases. This work shows the viability of automatically assessing when our chatbot tell lies.","Companies value their customers [2] and strive to create a great customer experience[3]. Customers, in turn, assess a company on its core business and customer service, which influences their trust, loyalty, and satisfaction [4]. Today, the most popular way to assist customers online is via chatbots and live chats [5, 6, 2]. Real-time communication in live chats means quick answers to questions [6, 2], which helps build loyalty[5] and encourages customers to return when they need assistance[6]. With recent advancements in Large Language Models (LLM) that enable natural and chat-like communication [9, 10, 12], AFAS sees an opportunity to provide live support. The first part of Figure 1 represents the current situation. When a customer raises an issue, an employee forwards the question to the chatbot along with relevant documents and instructions, also called a system prompt. When the LLMs generate an answer based on the information provided (details in II), the support employee checks the answer and forwards it to the customer if correct. Moving forward, we envision minimizing the validation by the support team, creating room for the support team to handle complex issues, and improving customer experience through near-real-time response. We aim to create an automated solution to identify lies our LLM-based support chatbot tells, indicative of the quality of the response, which is crucial for customer satisfaction. Further, the solution should be in Dutch to cater to the Dutch audience. Figure 1: shows the current and desired flow for handling customer queries. In the current workflow, the support team is an intermediate for providing context* comprising of relevant documents and instructions for the large language model and later assessing the response (see part blue). Using green parts, we envision replacing human feedback with automated ratings. Our first challenge in building an automated solution is understanding what makes a response right. To solicit an answer to this question, the first author shadowed a support staff for a day to observe and interactively understand what makes a response right. Combined with the literature search and analysis of reasons for rejecting chatbot responses, this showed that the first step to the ‘right’ answer is correctness, characterized in terms of relatedness, completeness, and truthfulness. With much research focusing on relatedness [18, 15], this study focuses on truthfulness. To assess our approach, in the first round we gathered a data of 79 posts which we used for training. At a later point, we collected data from 154 posts for testing. The limited data size characterize our analysis. Since the training data is scarce, it is not possible to train a model using reference answers as has been widely seen in literature [13, 14, 15, 16, 17]. As an alternative, we model how the support team makes decisions and derive heuristics. To measure these heuristics, we take inspiration from Natural Language Generation [15, 14, 19] and Automated Answer Grading literature for metrics [17, 16, 45, 71, 46]. In the process, we note that the choice of heuristics varies with the type of question asked. For example, heuristics for assessing the correctness of a yes/no answer are different from the heuristics for a question that solicits instructions. Our resulting model assesses the correctness of yes/no questions and questions requiring instructions to show that our model can detect a very inaccurate response with 55% accuracy. Notably, the overall accuracy is better for translated text in English than in Dutch. Further, we observed a 0.3 correlation of our score with human evaluation for Dutch text and 0.37 for the translated text in English, both of which is higher than the 0.13 reported by Mehri et al. [20] in their study on generic conversations. Further, our study contributes by providing • a working definition of correctness to evaluate chatbot responses • derive heuristics and customized metrics to assess correctness • shows relevance of translating regional language text to English for higher accuracy, and • lists the importance of question type for the choice of heuristics for assessing correctness."
https://arxiv.org/html/2411.00029v1,Preserving Pre-trained Representation Space:On Effectiveness of Prefix-tuning for Large Multi-modal Models,"Recently, we have observed that Large Multi-modal Models (LMMs) are revolutionizing the way machines interact with the world, unlocking new possibilities across various multi-modal applications. To adapt LMMs for downstream tasks, parameter-efficient fine-tuning (PEFT) which only trains additional prefix tokens or modules, has gained popularity. Nevertheless, there has been little analysis of how PEFT works in LMMs. In this paper, we delve into the strengths and weaknesses of each tuning strategy, shifting the focus from the efficiency typically associated with these approaches. We first discover that model parameter tuning methods such as LoRA and Adapters distort the feature representation space learned during pre-training and limit the full utilization of pre-trained knowledge. We also demonstrate that prefix-tuning excels at preserving the representation space, despite its lower performance on downstream tasks. These findings suggest a simple two-step PEFT strategy called Prefix-Tuned PEFT (PT-PEFT), which successively performs prefix-tuning and then PEFT (i.e., Adapter, LoRA), combines the benefits of both. Experimental results show that PT-PEFT not only improves performance in image captioning and visual question answering compared to vanilla PEFT methods but also helps preserve the representation space of the four pre-trained models.","Understanding the visual scene and expressing it with a natural language are two distinct tasks yet the human brain can comprehensively handle both without difficulty. Large multi-modal models (LMMs) mimic such capability by training a deep neural network (DNN) such that it learns semantically meaningful connections between vision and language from a large number of image-text pairs (Li et al., 2020b; Zhang et al., 2021b; Wang et al., 2022b; Radford et al., 2021). Recently, LMMs have been widely used due to their broad range of applications, including chatbot, robot control, and video generation (Ouyang et al., 2022; Brohan et al., 2023; Ramesh et al., 2022). Figure 1: Advantages of the proposed PT-PEFT, which performs 1) prefix-tuning and 2) fine-tuning (i.e., parameter-efficient or full fine-tuning) sequentially. In the pre-training, LMMs are trained to predict the masked words or next words from the image-text pair (Li et al., 2023; Alayrac et al., 2022; Wang et al., 2022a). In the second step called fine-tuning, the pre-trained LMMs are tailored to the specific downstream task. It has been shown that fine-tuning provides superior performance in various downstream tasks such as image captioning (IC), visual question answering (VQA), and image-text retrieval (Li et al., 2023; Wang et al., 2022a, b; Zhang et al., 2021b). However, fine-tuned models often suffer from the loss of generalization capability obtained from the pre-training (Sun et al., 2015; Brown et al., 2020a). Since the task-specific dataset is far smaller than the pre-training unlabeled dataset, the pre-trained model can be easily overfitted to the small-sized downstream task dataset, leading to degraded performance (Kumar et al., 2022). Various approaches have been suggested over the years to address the problem. In prompt-based approaches, manually designed prompts or trainable continuous embedding vectors are integrated into the input data to adapt the model for downstream tasks (Li and Liang, 2021; Liu et al., 2021; Tam et al., 2022; Lester et al., 2021). In knowledge distillation-based fine-tuning approaches, the model minimizes the distance between the distribution of the pre-trained and fine-tuned models (Xu et al., 2020; Sanh et al., 2019; Boschini et al., 2022). The common wisdom behind these approaches is to minimize the modification of the pre-trained model parameters while maintaining performance on downstream tasks. Figure 2: Performance of different task adaptation methods on COCO image captioning dataset. The proposed method (PT-) consistently improves performance when combined with other methods. One drawback of the full model fine-tuning is the huge computational burden caused by the model parameters update. In an effort to reduce the huge training cost, various parameter-efficient fine-tuning (PEFT) techniques have been proposed (Li and Liang, 2021; Houlsby et al., 2019; Hu et al., 2022; He et al., 2021). In these approaches, only a small set of additional modules (e.g., prefix, Adapter, LoRA) is trained instead of relying on full fine-tuning. These approaches are especially beneficial for training the large pre-trained model like GPT (Brown et al., 2020b), T5 (Raffel et al., 2020), and Llama (Touvron et al., 2023). Training efficiency is a well-known advantage of prefix-tuning. Unlike other PEFT methods, prefix-tuning does not modify the model’s parameters, leaving the representation space unchanged. To investigate the changes in the representation space, we analyze the feature representation matrices using singular value decomposition (SVD). Notably, we observe that the representation space of a fine-tuned model (in IC and VQA) utilizes only a limited set of effective basis vectors (60% of those in the pre-trained model) to express the output. Clearly, this limits the model’s ability to fully enjoy the benefits obtained from pre-training (see Figure 4). In contrast, we discover that all the basis vectors are utilized in the prefix-tuned model, implying that the prefix-tuning effectively preserves the inherited representation space from the pre-training. While the prefix-tuning is effective in preserving pre-trained knowledge, the efficacy of this approach is somewhat questionable since the reported evaluation results are not conclusive. Some studies claim that the prefix-tuning performs comparable to the model parameter-tuning (e.g., full fine-tuning, LoRA, Adapter), while others argue that the prefix-tuning struggles in the training of relatively small-sized language models (Liu et al., 2021; Tam et al., 2022). Figure 3: Qualitative image captioning results of zero-shot learning, prefix-tuned, and fine-tuned models. Although fine-tuning provides accurate answers, its results often ignore visual details compared to the other two. An aim of this paper is to propose a simple yet effective tuning strategy to combine the merits of two seemingly distinct approaches. The proposed method, henceforth referred to as Prefix-Tuned PEFT (PT-PEFT), performs the prefix-tuning and the model parameter-tuning sequentially. The key feature of PT-PEFT is to preserve the pre-trained feature space through the prefix-tuning and then refine the model parameters using the PEFT method. Intuitively, this approach resembles a language model learning a new task using prompt sentences such as ""I will provide example sentences describing the given pictures in the news article style. So, please generate the caption for the given images with such style."" By providing a context suitable for the new task, the model’s adaptability is enhanced, allowing for faster convergence and minimal changes to the weights of the pre-trained model. In our experiments, we show that applying the prefix-tuning before LoRA, Adapter, and even full fine-tuning consistently improves the task performance for all datasets and various pre-trained LMMs including BLIP (Li et al., 2022), BLIP-2 (Li et al., 2023), OFA (Wang et al., 2022a) and VINVL (Zhang et al., 2021b). We also compare the simultaneous tuning of prefix and model parameters and show that the proposed sequential strategy is indeed important for maximizing performance and preserving the representation space. Our contributions are as follows: • We show the correlation between the representation space and performance through rank-based analysis. We qualitatively and quantitatively illustrate the adverse effects of representation space collapse in task performance. • We reveal that the prefix-tuning differs significantly from model parameter tuning techniques such as LoRA, Adapter, and full fine-tuning in the sense that it preserves the integrity of the pre-trained knowledge. • We propose PT-PEFT, a method that sequentially performs the prefix-tuning followed by conventional fine-tuning technique, to maximize the utilization of pre-trained knowledge in LMMs. Our experimental results demonstrate that PT-PEFT outperforms the conventional fine-tuning methods in image captioning and VQA tasks. (a) Full Fine-tuning (b) LoRA (c) S-Adapter Figure 4: Accumulated and normalized singular values of features extracted from the last layer of BLIP-2. A more concave graph indicates that the singular values are more concentrated, implying the narrower representation space. Pre-training Fine-tuning Prefix-tuning S-Adapter P-Adapter LoRA PT→absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROWS-Adapter PT→absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROWP-Adapter PT→absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROWLoRA PT →absent→\xrightarrow{}start_ARROW start_OVERACCENT end_OVERACCENT → end_ARROW Fine-tuning VINVL 50.2 % 30.0 % 50.2 % - - - - - - 50.2 % BLIP-2 68.2 % 47.0 % 68.2 % 53.0 % 53.7 % 52.0 % 63.5 % 58.4 % 63.5 % 68.2 % Table 1: Effective rank of representation space of various fine-tuning techniques. Note that the effective rank is defined as the remaining rank ratio at which the accumulated singular values equal to 0.9 in Figure 4."
https://arxiv.org/html/2411.00028v1,Synergizing LLM Agents and Knowledge Graph for Socioeconomic Prediction in LBSN,"The fast development of location-based social networks (LBSNs) has led to significant changes in society, resulting in popular studies of using LBSN data for socioeconomic prediction, e.g., regional population and commercial activity estimation. Existing studies design various graphs to model heterogeneous LBSN data, and further apply graph representation learning methods for socioeconomic prediction. However, these approaches heavily rely on heuristic ideas and expertise to extract task-relevant knowledge from diverse data, which may not be optimal for specific tasks. Additionally, they tend to overlook the inherent relationships between different indicators, limiting the prediction accuracy. Motivated by the remarkable abilities of large language models (LLMs) in commonsense reasoning, embedding, and multi-agent collaboration, in this work, we synergize LLM agents and knowledge graph for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to integrate multi-sourced LBSN data. Then we leverage the reasoning power of LLM agent to identify relevant meta-paths in the LBKG for each type of socioeconomic prediction task, and design a semantic-guided attention module for knowledge fusion with meta-paths. Moreover, we introduce a cross-task communication mechanism to further enhance performance by enabling knowledge sharing across tasks at both LLM agent and KG levels. On the one hand, the LLM agents for different tasks collaborate to generate more diverse and comprehensive meta-paths. On the other hand, the embeddings from different tasks are adaptively merged for better socioeconomic prediction. Experiments on two datasets demonstrate the effectiveness of the synergistic design between LLM and KG, providing insights for information sharing across socioeconomic prediction tasks.","The development of location-based social networks (LBSNs) has significantly advanced socioeconomic prediction with rich web-sourced LBSN data such as user-generated content on review platforms. Socioeconomic indicators like regional population, user activity, and rating, provide a more comprehensive description of LBSN in turn, which can be leveraged by various web applications to offer enhanced services such as location recommendation, web page description, and personal assistants. As a result, socioeconomic prediction in the context of LBSN has become increasingly important, leading to a growing body of research in this field (Wang et al., 2016; Wang and Li, 2017; Yang et al., 2017; Yao et al., 2018; Dong et al., 2019; Xu et al., 2020; Zhang et al., 2021; Wu et al., 2022; Hou et al., 2022; Luo et al., 2022; Kim and Yoon, 2022; Zhou et al., 2023) Traditionally people collect socioeconomic indicators from surveys, which are costly and time-consuming. Recently, data-driven methods have become popular, which use machine learning models to predict the socioeconomic indicators based on various LBSN data. The LBSN data comes from various sources and is heterogeneous. Existing studies have widely used graph structure to model the complex relationships within LBSN data, and predict the socioeconomic indicators through graph representation learning methods. They construct either multi-view graphs (Zhang et al., 2021; Wu et al., 2022; Kim and Yoon, 2022) or knowledge graphs (KGs) (Zhou et al., 2023; Liu et al., 2023c) to model different factors in LBSN data like mobility, spatial proximity, and functionality. However, These approaches highly rely on heuristic ideas and expertise to extract knowledge related to the tasks from LBSN data, such as the construction of sub-graphs or definition of meta-structures, which may be sub-optimal for different indicator prediction tasks. Moreover, the intrinsic correlations and potential for knowledge sharing across different socioeconomic prediction tasks are often overlooked, limiting the overall prediction accuracy. The recently emerged large language models (LLMs) provide a promising solution to these limitations. LLMs have demonstrated several remarkable abilities which could help socioeconomic prediction (Wei et al., 2022a): (1) Latent semantic embedding. At the foundational level, LLMs are able to generate text embeddings with rich semantic information, which makes it possible to integrate LLM with deep learning models to improve the performance by leveraging the inherent semantic information in the LBSN data. (2) Explicit commonsense reasoning. Moreover, LLMs possess vast commonsense knowledge, based on which they can perform complex reasoning (Zhao et al., 2024). This capability facilitates the automatic extraction of task-relevant knowledge from LBSN data. (3) Multi-agent collaboration. At the highest level, LLM agents can communicate with other agents through natural language, and collaborate to solve complex tasks which is difficult for a single agent (Li et al., 2023a; Hong et al., 2023; Xiao et al., 2023). Such ability enables LLM to transfer knowledge across different socioeconomic prediction tasks via semantic-rich natural language, and collaboratively improve the performance. The recently emerged large language models (LLMs) provide a promising solution to these limitations. LLMs have demonstrated many remarkable abilities which could help socioeconomic prediction (Wei et al., 2022a): (1) Commonsense reasoning. LLMs possess vast commonsense knowledge, based on which they can perform complex reasoning (Zhao et al., 2024), thus enabling automatic task-relevant knowledge discovery from LBSN data. (2) Multi-agent collaboration. LLM agents can communicate with other agents through natural language, and collaborate to solve complex tasks which is difficult for a single agent (Li et al., 2023a; Hong et al., 2023; Xiao et al., 2023). Such ability enables LLM to transfer knowledge across different socioeconomic prediction tasks, and collaboratively improve the performance. (3) Semantic embedding. LLMs are able to generate text embeddings with rich semantic information, which makes it possible to integrate LLM with deep learning models and enhance the performance with semantic information in the LBSN data. Inspired by this, we propose a learning framework that Synergize LLM Agent and Knowledge Graph learning model (SLAK) for socioeconomic prediction. We first construct a location-based knowledge graph (LBKG) to comprehensively integrate LBSN data. To extract task-relevant knowledge, we construct an LLM agent to automatically discover meta-paths in the LBKG that help the prediction for specific indicators, and extract a corresponding meta-path-based sub-KG. Moreover, we leverage KG representation learning model to distil knowledge from each meta-path-based sub-KG, and design a semantic-enhanced knowledge fusion module to adaptively fuse the knowledge based on semantic embeddings of meta-paths obtained from LLM. In addition, we propose a cross-task communication mechanism to enable knowledge sharing across different socioeconomic prediction tasks at both the LLM agent and KG level. Specifically, the LLM agents collaborate to extract better meta-paths, and we also adaptively merge the KG embeddings from different tasks with the knowledge fusion module. Our contribution can be summarized as follows: • We propose a framework to synergize LLM agents and KG for socioeconomic prediction. We leverage the reasoning capability of LLM agent to find task-relevant meta-paths from LBKG, and further use the semantic information of the meta-paths to guide the knowledge fusion. • We design a cross-task communication mechanism to enable knowledge sharing across different socioeconomic prediction tasks at both the LLM agent level through multi-agent collaboration and the KG level through semantic attention, which further improves the accuracy. • Extensive experiments on two city-scale datasets show that our model surpasses existing methods by 2.9-74.2% in terms of R2superscript𝑅2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT on eight indicator prediction tasks, demonstrating the effectiveness of our synergistic model between LLM and KG. Several in-depth analyses further show the advantage of our model design, providing insights for web-based socioeconomic prediction in LBSN."
https://arxiv.org/html/2411.00027v1,Personalization of Large Language Models: A Survey,"Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.","Large language models (LLMs) have emerged as powerful tools capable of performing a wide range of natural language processing (NLP) tasks with remarkable proficiency (e.g., Radford et al., 2018; Devlin et al., 2019; Lewis et al., 2019; Radford et al., 2019; Brown et al., 2020; Raffel et al., 2020; Achiam et al., 2023; Touvron et al., 2023; Groeneveld et al., 2024). Empirically, these models have demonstrated their capability as generalist models, allowing them to perform numerous tasks such as text generation, translation, summarization, and question-answering with decent accuracy. Notably, LLMs can perform effectively in zero-shot or few-shot settings, meaning they can follow human instructions and perform complex tasks with little to no task-specific training data (Bommasani et al., 2021; Liu et al., 2023c). This capability eliminates the need for extensive fine-tuning of their parameters, thereby significantly simplifying human interaction with machines through straightforward input prompts. For instance, users can engage with LLMs in a conversational format, making interactions more intuitive and accessible. Such robust and versatile abilities of LLMs have led to the creation of numerous applications, including general AI assistants (AutoGPT, 2024), copilots (Microsoft, 2024), and personal LLM-based agents (Li et al., 2024f). These applications assist users in a wide range of activities such as writing emails, generating code, drafting reports, and more. Recently, there has been growing interest in adapting LLMs to user-specific contexts, beyond their natural use as NLP task solvers or general-purpose chatbots (Tseng et al., 2024). To this end, personalization of LLMs addresses this by adapting the models to generate responses that cater to the unique needs and preferences of each user or user group (Salemi et al., 2023). Such personalization is crucial for human-AI interaction and user-focused applications. It is expected to enhance user satisfaction by providing more relevant and meaningful interactions, ensuring users receive responses that are more aligned with their needs and expectations. This enables LLMs to offer more effective assistance across a diverse range of applications such as customer support (Amazon, 2024), where personalized responses can significantly improve user experience; education (Wang et al., 2022; 2024b), where tailored content can better meet individual learning needs (Woźniak et al., 2024); and healthcare, where personalized advice can enhance patient care (Tang et al., 2023; Yuan et al., 2023). Personalization of LLMs has recently gained a lot of attention (Salemi et al., 2023; Tseng et al., 2024). However, existing research on personalized LLMs generally falls into two categories: (a) personalized text generation, which focuses on producing single or multi-turn text within a personalized context, and (b) the application of LLMs in downstream personalization tasks, such as recommendation systems. A large number of studies in these two areas have evolved independently, often without a cohesive or unified perspective. Additionally, existing surveys (Chen, 2023; Chen et al., 2024b; c) tend to focus exclusively on one of these aspects, lacking a comprehensive, unified perspective that systematically defines the key components and synthesizes insights across both dimensions of personalized LLMs. Although these two areas differ in characteristics and objectives, a unified perspective on personalized LLMs is crucial as it can bridge the gap between these two research communities, promoting collaboration and knowledge sharing that leads to more generalizable and versatile systems. For example, advancements in user-specific text generation can improve recommendation systems by enabling more personalized and explainable suggestions through conversational interactions. By integrating insights from both areas, researchers can develop LLMs that not only generate text tailored to individual preferences but also improve user satisfaction across diverse applications. This interdisciplinary approach fosters more holistic solutions, addressing both personalization and performance in a complementary way. In this work, we unify the literature from these different fields by proposing intuitive taxonomies for the granularity of personalization, techniques, evaluation, datasets, and usage scenarios of personalized LLMs. The key contributions of this work are as follows: 1. A unifying view and taxonomy for the usage of personalized LLMs (Section 2). We provide a unifying view and taxonomy of the usage of personalized LLMs based on whether they focus on evaluating the generated text directly, or whether the text is used indirectly for another downstream application. This serves as a fundamental basis for understanding and unifying the two separate areas focused on the personalization of LLMs. Further, we analyze the limitations of each, including the features, evaluation, and datasets, among other factors. 2. A formalization of personalized LLMs (Section 3). We provide a formalization of personalized LLMs by establishing foundational concepts that consolidate existing notions of personalization, defining and discussing novel facets of personalization, and outlining desiderata for their application across diverse usage scenarios. 3. An analysis and taxonomy of the personalization granularity of LLMs (Section 4). We propose three different levels of personalization granularity for LLMs, including (i) user-level personalization, (ii) persona-level personalization, and (iii) global preference personalization. We formalize these levels, and then discuss and characterize the trade-offs between the different granularities of LLM personalization. Notably, user-level personalization is the finest granularity; however, it requires a sufficient amount of user-level data. In contrast, persona-level personalization groups users into personas and tailors the experience based on persona assignments. While it doesn’t provide the same granularity as user-level personalization, it is effective for personalizing experiences for users with limited data. Finally, global preference alignment caters to overall preferences by the general public and does not offer user-specific personalization.111We include it here for completeness, though it is not the focus of this work. 4. A survey and taxonomy of techniques for LLM personalization (Section 5). We categorize and provide a comprehensive overview of the current techniques for personalizing LLMs based on how user information is utilized. Our taxonomy covers various categories of methods such as retrieval-augmented generation (RAG), prompt engineering, supervised fine-tuning, embedding learning, and reinforcement learning from human feedback (RLHF). For each method, we discuss their unique characteristics, applications, and the trade-offs involved. Our detailed analysis helps in understanding the strengths and limitations of different personalization techniques and their suitability for various tasks. 5. A survey and taxonomy of metrics and evaluation of personalized LLMs (Section 6). We categorize and analyze the existing metrics used for evaluating personalized LLMs, proposing a novel taxonomy that distinguishes between direct and indirect evaluation methods. We highlight the importance of both qualitative and quantitative metrics, addressing various facets such as user satisfaction, relevance, and coherence of the generated text. Additionally, we discuss the challenges in evaluating personalized LLMs and suggest potential solutions to improve the robustness and reliability of the evaluation process. 6. A survey and taxonomy of datasets for personalized LLMs (Section 7). We provide a comprehensive taxonomy of datasets used for training and evaluating personalized LLMs, categorizing them based on their usage in direct or indirect evaluation of personalized text generation. Our survey covers a wide range of datasets, including those specifically designed for short- and long-text generation, recommendation systems, classification tasks, and dialogue generation. We discuss the strengths and limitations of each dataset, their relevance to different personalization techniques, and the need for more diverse and representative datasets to advance the field. 7. A survey of applications for personalized LLMs (Section 8). We survey key domains where personalized LLMs are applied, including AI assistants in education and healthcare, finance, legal, and coding environments. We also explore their use in recommendation systems and search engines, highlighting the ability of personalized LLMs to deliver customized user experiences, enhance engagement, and improve task-specific outcomes across diverse fields. 8. An overview of important open problems and challenges for future work to address (Section 9). We outline critical challenges and open research questions in personalized LLMs that need to be addressed for advancing the field. Key issues include the need for improved benchmarks and metrics to evaluate personalization effectively, tackling the cold-start problem in adapting models to sparse user data and addressing stereotypes and biases that may arise in personalized outputs. Privacy concerns surrounding user-specific data are also explored, particularly in balancing personalization with privacy protection. Additionally, we discuss the unique complexities of expanding personalization to multi-modal systems, where integrating user preferences across diverse input types remains an open challenge. In the remainder of the article, we first present a unifying view and taxonomy for the usage of personalized LLMs (Section 2), and then delve into the theoretical foundations of personalized LLMs (Section 3). Next, we explore the granularity of personalization in LLMs (Section 4), and provide a comprehensive survey and taxonomy of techniques for personalized LLMs (Section 5). We then categorize metrics and methods for the evaluation of personalized LLMs (Section 6), and offer a detailed taxonomy of datasets used for personalized LLMs (Section 7). We discuss the various applications of personalized LLMs (Section 8), and finally, identify key challenges and propose future research directions (Section 9). Figure 1: Taxonomy for Personalized LLM Usage. To bridge the gap in the existing literature on personalized LLMs, we propose the intuitive taxonomy outlined above, which categorizes work into two main areas. The first focuses on studying the ➊ personalized text generated directly, while the second emphasizes using personalized information as intermediate steps or implicitly as embeddings to improve the quality of a ➋ downstream task such as recommendation systems. See Section 2 for a detailed discussion."
https://arxiv.org/html/2411.00024v1,A Perspective for Adapting Generalist AI to Specialized Medical AI Applications and Their Challenges,"The integration of Large Language Models (LLMs) into medical applications has sparked widespread interest across the healthcare industry, from drug discovery and development to clinical decision support, assisting telemedicine, medical devices, and healthcare insurance applications. This perspective paper aims to discuss the inner workings of building LLM-powered medical AI applications and introduces a comprehensive framework for their development. We review existing literature and outline the unique challenges of applying LLMs in specialized medical contexts. Additionally, we introduce a three-step framework to organize medical LLM research activities: 1) Modeling: breaking down complex medical workflows into manageable steps for developing medical-specific models; 2) Optimization: optimizing the model performance with crafted prompts and integrating external knowledge and tools, and 3) System engineering: decomposing complex tasks into subtasks and leveraging human expertise for building medical AI applications. Furthermore, we offer a detailed use case playbook that describes various LLM-powered medical AI applications, such as optimizing clinical trial design, enhancing clinical decision support, and advancing medical imaging analysis. Finally, we discuss various challenges and considerations for building medical AI applications with LLMs, such as handling hallucination issues, data ownership and compliance, privacy, intellectual property considerations, compute cost, sustainability issues, and responsible AI requirements.","Artificial intelligence (AI) is increasingly being integrated into various medical tasks, including clinical risk prediction [1], medical image understanding [2], and synthetic patient records generation [3]. Typically, these models are designed for specific tasks and will struggle with unfamiliar tasks or out-of-distribution data [4]. Large language models (LLMs) are foundation AI models characterized by their extensive training data and enormous model scale. Unlike traditional AI models, LLMs demonstrate an emergent capability in language understanding and the ability to tackle new tasks through in-context learning [5]. For example, we can teach LLMs to conduct a new task by providing the text explanation of the task (or “prompts”), the input and output protocols, and several examples. This adaptability has sparked interest in employing generalist LLMs to medical AI applications such as chatbots for outpatient reception [6]. Contrary to the common belief that generalist LLMs will excel in many fields [7], we advocate that domain-specific AI adaptations for medicine are more effective and safe. This paper will overview how various adaptation strategies can be developed for medical AI applications and the associated trade-offs. Generalist LLMs such as ChatGPT can support broad tasks but may underperform in specialized domains [8]. One notable drawback is the occurrence of “hallucinations,” which are fabricated facts that look plausible yet incorrect [9]. High-stakes medical applications such as patient-facing diagnosis tools are especially vulnerable to such inaccurate information [10]. In response, adaptation methods in LLM for medicine have thrived, focusing on enhancing LLMs’ domain-specific capabilities (Fig. 1b). They include finetuning LLMs on medical data [11], adding relevant medical information to the prompts for LLMs via retrieval-augmented generation (RAG) [12], and equipping LLMs with external tools to building AI agents achieving autonomous planning and task execution [13]. With the increasing practice in developing LLM-based AI applications, it is becoming evident that cutting-edge performance is increasingly driven by the mixture of these adaptations [14] and systematic engineering of multiple AI components [15]. We present a systematic overview of adaptation methods and organize them in a framework for developing LLM-based medical AI applications. Fig. 1 outlines various adaptation methods and high-level strategies, including: • Model development: This includes techniques like pretraining, where medical knowledge is injected into generic models via continual pretraining on medical datasets [16], and finetuning, which ensures that the model’s outputs are consistent with domain knowledge and human preferences [17]. • Model optimization: Strategies such as prompt optimization enhance the relevance and accuracy of model responses [14], while retrieval-augmentation generation (RAG) utilizes external data to create richer, more informed prompts [12]. More advanced operations include building autonomous AI agents [18] and chaining multiple AI modules to enable complex workflows [19]. • System engineering: We can optimize the performance of medical AI by taking a system engineering approach. This includes subtasking by assigning clearly defined, narrow-scope tasks to each system component to improve reliability [20], involving human-in-the-loop to boost oversight and refinement [21], result verification for maintaining accuracy and robustness [22], and interconnected AI agents focusing different aspects of the task for improving medical conversational reasoning [23]. Next, we describe this framework for developing medical AI applications in detail, focusing on diverse medical AI use cases. We will end by describing the associated challenges and opportunities for further development of LLM-based medical AI applications. Figure 1: Workflow for AI model development, optimization, and system engineering in medical applications. (a) Generalist AI models, such as proprietary systems (e.g., GPT-4) and open-source models (e.g., LLaMA), serve as foundational technologies for developing specialized medical AI models. (b) Adapting generalist AI to medical tasks involves several techniques, including model fine-tuning, prompt optimization, and the development of AI agents or AI chains. These methods use diverse medical datasets, such as medical images, electronic health records (EHRs), clinical notes, publications, and omics data, to enhance AI model training and performance. (c) Effective system engineering for medical AI entails integrating AI modules into comprehensive chains to support tasks like cohort extraction, eligibility assessment, and result verification. This process emphasizes human interaction and AI, resulting in tailored AI modules for specific applications. (d) Generalist AI applications in medicine span various domains, including conversational diagnosis, radiology report generation, clinical note summarization, automated medical coding, drug design, patient-trial matching, and systematic literature reviews. All require advanced system integration for optimal performance."
https://arxiv.org/html/2411.00774v1,Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM,"The rapid development of large language models has brought many new smart applications, especially the excellent multimodal human-computer interaction in GPT-4o has brought impressive experience to users. In this background, researchers have proposed many multimodal LLMs that can achieve speech-to-speech dialogue recently. In this paper, we propose a speech-text multimodal LLM architecture called Freeze-Omni. Our main contribution is the speech input and output modalities can connected to the LLM while keeping the LLM frozen throughout the training process. We designed 3-stage training strategies both for the modeling of speech input and output, enabling Freeze-Omni to obtain speech-to-speech dialogue ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while the end-to-end latency of the spoken response achieves a low level. In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, making Freeze-Omni have a more natural style of dialogue ability between the users. Freeze-Omni mainly provides a possibility for researchers to conduct multimodal LLM under the condition of a frozen LLM, avoiding various impacts caused by the catastrophic forgetting of LLM caused by fewer data and training resources.","In recent years, the development of large language models has been extremely rapid. A series of large language models represented by the GPT series [10, 1] of OpenAI has demonstrated extraordinary capabilities. As speech interaction is one of the most natural forms of human-computer interaction, combining speech input and output with an LLM can bring an extraordinary experience to users. The traditional method is to use a cascaded approach of ASR + LLM + TTS to achieve the interaction with LLM in speech modality. However, this approach often leads to a relatively high engineering complexity and a considerable interaction latency. Nevertheless, GPT-4o [18] has changed this situation, it provides an end-to-end speech interaction mode which has significantly improved the user experience, triggering a research boom among researchers regarding multimodal LLMs for speech-to-speech interaction. In the field of general LLMs, many public models such as Llama 3.2 [8], Qwen2.5 [21], Mixtral [14], etc. have provided very good opportunities for researchers to develop downstream tasks on them. Therefore, in the research field of multimodal LLMs for speech-to-speech, works such as Mini-Omni2 [24], LLaMA-Omni [9], and Moshi [7] have provided excellent references for researchers. These works adopt different strategies to align the speech modality with the LLM and design some methods to achieve a duplex dialogue mode, demonstrating excellent performance. In this research context, we found that in the process of aligning the LLM with the speech modality in existing public speech-text multimodal LLMs [6, 7, 9, 11, 27, 23], the parameters of the LLM are more or less fine-tuned. However, in most cases, it is very difficult for researchers to easily collect spoken Q&A data at the million-hour level (the corresponding text content can be comparable to the amount of data for training text-modal LLMs). This inevitably brings about the forgetting problem to the LLM, resulting in a negative impact on its intelligence. In addition, only a few works have evaluated the accuracy of spoken question-answering tasks for speech-to-speech multimodal LLMs, and show an obvious gap in performance between spoken question-answering and text-modality question-answering. Therefore, in this paper, we propose a speech-to-speech dialogue LLM called Freeze-Omni, achieving speech modality alignment while the LLM is frozen throughout the training process, and obtaining low latency speech dialogue capabilities while keeping the intelligence of the backbone LLM. Freeze-Omni is mainly implemented in the following steps: Modeling of speech input We first use a large amount of ASR data to align the speech encoder and the LLM, enabling the LLM to understand the semantic information from the speech. Then, with the LLM frozen, a training strategy of prompt embedding is used to let the model have the ability to possess speech input to text output, training on only a small amount of Q&A data. Modeling of speech output Second, we use a mount of text-speech paired data to train the AR-based speech decoder which can generate speech tokens from text and a single-codebook based codec model is used to decode the speech token into waveform. Then, we design a prefix kv-cache fine-tune strategy, using the hidden state vector output by the LLM to transfer the speech decoder into the output text space of LLM, achieving the ability of text input to speech output while keeping the LLM frozen. Design for duplex dialogue Finally, we simultaneously connect the speech encoder and speech decoder from the above parts to the backbone LLM. Then, a task of chunk-wise state prediction is used to enable the LLM to interrupt or reject the user’s input, achieving the duplex speech-to-speech dialogue ability. In conclusion, the main contributions of the proposed Freeze-Omni are as follows: • The parameters of the LLM are completely frozen throughout the training process, ensuring that the intelligence of the LLM will be kept. At the same time, the ability of low latency speech-to-speech dialogue is still obtained. • The data scale relied on during the training process is small and consumes fewer computing resources. It requires text-speech paired data (such as ASR and TTS training data) and only a small amount of Q&A data in text modality. • Freeze-Omni can support any (multimodal) LLM that has a text modality and retains the abilities of the LLM such as prompt following and role-playing. Moreover, if it is necessary to change the style of the LLM’s response, it is only necessary to fine-tune the LLM with text data in the corresponding style."
https://arxiv.org/html/2411.00744v1,CORAG: A Cost-Constrained Retrieval Optimization System for Retrieval-Augmented Generation,"Large Language Models (LLMs) have demonstrated remarkable generation capabilities but often struggle to access up-to-date information, which can lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this issue by incorporating knowledge from external databases, enabling more accurate and relevant responses. Due to the context window constraints of LLMs, it is impractical to input the entire external database context directly into the model. Instead, only the most relevant information, referred to as “chunks”, is selectively retrieved. However, current RAG research faces three key challenges. First, existing solutions often select each chunk independently, overlooking potential correlations among them. Second, in practice, the utility of chunks are “non-monotomic”, meaning that adding more chunks can decrease overall utility. Traditional methods emphasize maximizing the number of included chunks, which can inadvertently compromise performance. Third, each type of user query possesses unique characteristics that require tailored handling—an aspect that current approaches do not fully consider.To overcome these challenges, we propose a cost-constrained retrieval optimization system CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search (MCTS)-based policy framework to find optimal chunk combinations sequentially, allowing for a comprehensive consideration of correlations among chunks. Additionally, rather than viewing budget exhaustion as a termination condition, we integrate budget constraints into the optimization of chunk combinations, effectively addressing the non-monotonicity of chunk utility. Furthermore, by designing a configuration agent, our system predicts optimal configurations for each query type, enhancing adaptability and efficiency. Experimental results indicate an improvement of up to 30% over baseline models, underscoring the framework’s effectiveness, scalability, and suitability for long-context applications.","Although LLMs have demonstrated exceptional capabilities in generation tasks, they often struggle with accessing up-to-date information, which can lead to hallucinations (Huang et al., 2023; Xu et al., 2024). To address these challenges, RAG has emerged as a crucial solution. By integrating external data sources into LLM, RAG can provide more accurate, relevant, and up-to-date information. Nowadays, RAG has been widely studied in the context of LLMs especially for tasks requiring update external knowledge such as question answering task (Asai et al., 2023; Sarthi et al., 2024; Microsoft, 2024), medical information retrieval (Singhal et al., 2022; Alkhalaf et al., 2024), and time series analysis (Ravuru et al., 2024; Jin et al., 2023; Ye et al., 2024). External data sources are often extremely large, making it impractical to input them directly into the LLM. To address this issue, data is typically split into disjoint chunks and stored in a vector database, and then users query the most useful chunks to construct prompts for LLMs. Therefore, designing efficient and accurate structures and algorithms to search for the most relevant chunks has become a prominent research topic and has been widely studied in both the database (Xue et al., 2024; Zhao et al., 2024c) and machine learning communities (Asai et al., 2023; Yu et al., 2024a; Wang et al., 2024b). Figure 1. Example of chunks combination order. However, there are three key challenges in the existing approaches. Challenge 1: Correlations between chunks Currently, two primary methods are used to identify the most relevant chunks. The first approach formulates the problem as a approximate k-nearest neighbor (AKNN) task (Zhang et al., 2024; Yin et al., 2024), where each chunk is assigned a score, and the approxiamte top-k𝑘kitalic_k chunks ranked by score are selected. The second approach clusters the chunks, returning all chunks within the most relevant clusters in response to a query (Microsoft, 2024; Sarthi et al., 2024). However, both methods overlook potential correlations between chunks: the first approach disregards correlations entirely, while the second approach accounts for them only superficially by treating all chunks within each cluster as equally relevant. As a result, when multiple chunks convey similar or overlapping information, these methods introduce substantial redundancy in the selected chunks. For example, as illustrated in Figure 1, when querying the height and history of the Eiffel Tower, if each chunk is treated independently, a greedy method would select chunks χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ1subscript𝜒1\chi_{1}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT since they have the top two scores. However, both chunks only provide historical information, which is insufficient to fully address the query. To better address the query, it is necessary to include a chunk with constructor’s name, such as χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT. On the other hand, the clustering approach would return all of χ1,χ2,χ3subscript𝜒1subscript𝜒2subscript𝜒3\chi_{1},\chi_{2},\chi_{3}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_χ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, resulting in redundancy. An optimal solution would instead select χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, as they provide the required information without redundancy. Additionally, research (Lu et al., 2021; Jiang et al., 2024; Yu et al., 2024b) has shown that the order of chunks influences LLM performance, a factor that existing methods also overlook. Following the example of the Eiffel Tower, when chunks χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT are selected, placing χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT first yields a higher score compared with the reverse order will have a better performance. However, determining the optimal chunk combination order is a challenging task since both of them require a search space growing exponentially with the number of available chunks. In this paper, we further demonstrate that this problem is NP-hard (see Section 2.1). Challenge 2: Non-monotonicity of utility Current solutions operate on the assumption that including more chunks will always yield better final results. Specifically, in the AKNN-based approach, exactly k𝑘kitalic_k chunks are selected deterministically each time. In the clustering-based approach, a distance threshold between clusters and the query is set, and all clusters within this threshold are returned. Both of them return as many chunks as possible. However, in practice, the utility of chunks is not monotonic. More specifically, excessive chunks can dilute key information by adding marginally relevant content, creating noise that reduces clarity. Additionally, conflicting or nuanced differences across chunks may confuse the model, lowering response quality. For example, as illustrated in Figure 1, when χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT are selected, adding the chunk χ1subscript𝜒1\chi_{1}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT decreases utility, highlighting that utility scores are often non-monotonic in practice. Challenge 3: Diversity of queries: User queries come in different types, each requiring its own ranking strategy due to their unique characteristics (Zhao et al., 2024a). In current RAG systems, the utility scores of chunks often are determined by the assigned reranker model. So far, various reranker models exist, but we observe that their performance varies significantly across different query types, and no single fixed reranker model consistently outperforms the others across all query variations (see our experiments in Section 6.3.4 for more details). Current methods (Lyu et al., 2024; Zhao et al., 2024b) typically rely on static reranker models for ranking chunks, lacking the flexibility to adapt to varying query contexts. Problem Statement: Is there a RAG system that fully considers correlations between chunks and the non-monotonicity of utility while being adaptable to all types of queries? 1.1. Our Contributions In this paper, we answer this question in the affirmative, by proposing a novel MCTS based policy tree framework to optimize chunk retrieval in RAG systems. In summary, our contributions can be summarized as follows: • We propose the first RAG framework that considers the chunk combination order for the RAG task. Instead of considering each chunk independently or at the cluster level, we use MCTS to help search the optimal chunk combination order sequentially. The high-level idea is as follows: First, we initialize the root node. Then, in an iterative process, we expand the tree by selecting the highest utility node and computing its expended nodes’ utilities. After each expansion, we update the utilities throughout the entire policy tree. During this process, the decision at each iteration depends on the chunks already selected, allowing us to fully consider the correlations between chunks. Moreover, MCTS reduces the exponential search space to linear, and we apply parallel expansion techniques to further enhance computational efficiency. With such designs, we address Challenge 1. • In contrast to prior RAG frameworks that consider the exhaustion of the budget as one of termination conditions, we propose a novel formulation wherein budget constraints are integrated into the process of optimizing chunk combinations to fully consider the non-monotonicity of utility of chunks thereby addressing Challenge 2. Moreover, by prioritizing high-relevance, low-cost chunks and factoring in token length, we further reduce computational costs. • We propose a contrastive learning-based agent that dynamically adjusts MCTS configurations per query, adapting reranker models and configurations to the specific query domain. This approach tailors retrieval for dynamic, domain-specific queries with flexibility and robustness, addressing Challenge 3. • Additionally, we conducted comprehensive experiments, comparing our framework with several state-of-the-art methods. The results validate the effectiveness, efficiency, and scalability of our approach, also showing a performance improvement of 30% over the baseline."
https://arxiv.org/html/2411.00743v1,Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models,"Understanding and mitigating the potential risks associated with foundation models (FMs) hinges on developing effective interpretability methods. Sparse Autoencoders (SAEs) have emerged as a promising tool for disentangling FM representations, but they struggle to capture rare, yet crucial concepts in the data. We introduce Specialized Sparse Autoencoders (SSAEs), designed to illuminate these elusive dark matter features by focusing on specific subdomains. We present a practical recipe for training SSAEs, demonstrating the efficacy of dense retrieval for data selection and the benefits of Tilted Empirical Risk Minimization as a training objective to improve concept recall. Our evaluation of SSAEs on standard metrics, such as downstream perplexity and L0subscript𝐿0L_{0}italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT sparsity, show that they effectively capture subdomain tail concepts, exceeding the capabilities of general-purpose SAEs. We showcase the practical utility of SSAEs in a case study on the Bias in Bios dataset, where SSAEs achieve a 12.5% increase in worst-group classification accuracy when applied to remove spurious gender information. SSAEs provide a powerful new lens for peering into the inner workings of FMs in subdomains.","Interpretability is crucial for ensuring the safety and reliability of foundation models (FMs) (Bommasani et al., 2021). A key challenge in interpretability research is to scalably explain the myriad unanticipated behaviors in FMs. Sparse Autoencoders (SAEs) have recently emerged as a promising tool for disentangling the complex, high-dimensional representations within FMs into meaningful, human-interpretable features without supervision (Cunningham et al., 2023; Gao et al., 2024; Braun et al., 2024; Bricken et al., 2023). However, even massively wide SAEs, trained on vast amounts of data, may only capture a fraction of the concepts embedded within these models (Templeton et al., 2024). A significant portion of rare or highly specific concepts remain essentially invisible due to their infrequent activation. These elusive features, akin to dark matter in the universe of interpretability, pose a significant challenge for understanding and mitigating potential risks associated with FMs. While larger SAEs did exhibit some features for rarer concepts, Templeton et al. (2024) found compelling evidence suggesting a vast amount of dark matter features were still being missed. For example, they found features for some of San Francisco’s neighborhoods, but their model still lacked features for smaller entities like coffee shops or street intersections. They observed that if a concept is present only once every billion tokens, we may need a billion-feature SAE to capture it reliably. This raises a critical question: can we develop more efficient methods than simply scaling SAE width to capture the tail concepts we are interested in? This paper introduces Specialized Sparse Autoencoders (SSAEs), a novel approach designed to address this challenge. Instead of aiming to capture all concepts, as in current SAE practices, we propose SSAEs as an unsupervised targeted method for efficiently extracting rare features related to specific subdomains. By focusing on a particular subdomain, we can train SSAEs to learn features representing tail concepts without needing to scale to billions of features. Furthermore, instead of relying solely on scaling, we investigate whether Tilted Empirical Risk Minimization (TERM), which approximates minimax risk at large tilt parameters, can further improve the representation of tail concepts within SSAEs. Our key contributions are: 1. Specialized Sparse Autoencoders: An unsupervised method for efficiently extracting rare, subdomain-specific features. We demonstrate empirically that SSAEs capture a greater proportion of tail concepts than standard SAEs trained on general-purpose data, achieving a 12.5% increase in worst-group classification accuracy on the Bias in Bios dataset when used to remove spurious gender information. 2. Subdomain Data Selection Strategies: A practical recipe for training SSAEs, starting with a small seed dataset and leveraging various data selection strategies to identify relevant training data from the FM’s pretraining corpus. We find that Dense retrieval is particularly effective while TracIn reranking can offer further improvements. 3. Tilted Empirical Risk Minimization for SAEs: A novel training objective for SAEs designed to improve concept recall. At large tilt values, TERM encourages more balanced learning of head and tail concepts. We show that TERM-trained SSAEs are more interpretable, exhibit improved concept detection, while maintaining comparable downstream perplexity. We envision SSAEs as versatile tools for concept detection and control across domains where identifying rare features is crucial, such as AI safety (detecting deception), healthcare (identifying outliers), and fairness (recognizing underrepresented groups). See Appendix M for additional examples. Related Work Much interpretability research focuses on analyzing coarse-grained model components like induction heads and MLP modules (Olsson et al., 2022; Elhage et al., 2022b; Geva et al., 2023; Meng et al., 2022; Nanda et al., 2023b), or fine-grained units like linear probes (Kim et al., 2018; Belinkov, 2022; Geiger et al., 2023; Zou et al., 2023). Both have limitations. The inherent polysemanticity of coarse-grained components complicates interpretation. Fine-grained analysis, while potentially more precise, is constrained by reliance on curated datasets that isolate behavior, limiting generalizability to unknown mechanisms. Feature disentanglement methods, such as SAEs (Bricken et al., 2023; Cunningham et al., 2023), offer a promising unsupervised alternative, aiming to identify human-interpretable directions in an FM’s latent space. For additional work see Appendix A."
https://arxiv.org/html/2411.00683v1,TaxaBind: A Unified Embedding Space for Ecological Applications,"We present TaxaBind, a unified embedding space for characterizing any species of interest. TaxaBind is a multimodal embedding space across six modalities: ground-level images of species, geographic location, satellite image, text, audio, and environmental features, useful for solving ecological problems. To learn this joint embedding space, we leverage ground-level images of species as a binding modality. We propose multimodal patching, a technique for effectively distilling the knowledge from various modalities into the binding modality. We construct two large datasets for pretraining: iSatNat with species images and satellite images, and iSoundNat with species images and audio. Additionally, we introduce TaxaBench-8k, a diverse multimodal dataset with six paired modalities for evaluating deep learning models on ecological tasks. Experiments with TaxaBind demonstrate its strong zero-shot and emergent capabilities on a range of tasks including species classification, cross-model retrieval, and audio classification. The datasets and models are made available at https://github.com/mvrl/TaxaBind.","Fine-grained species classification is a challenging task in computer vision, which is often necessary for ecologists to automatically label images of rare species. A related and arguably a more important task is species distribution mapping which aims to map the presence of a given species of interest. Until now, both tasks were addressed using separate frameworks and methodologies, often requiring different datasets. In this work, we propose learning a unified embedding space over six modalities that is useful for several downstream ecological tasks including but not limited to species distribution mapping, fine-grained classification, and audio classification. The presence of a particular species at a given geographic location can reveal several important characteristics of that species. Previous studies attempted to implicitly learn the relationship between geographic location and the presence of species by considering either environmental features [1] or satellite images [2, 3, 4] describing the location. This leads to learning an effective representation of any geographic location which is useful for species distribution mapping. However, this type of modeling often overlooks important species attributes, such as their taxonomic hierarchy or audio signatures. Recent works such as BioCLIP [5] and ArborCLIP [6] have demonstrated impressive zero-shot species classification capabilities. However, these frameworks are restricted to image and text modalities, ignoring crucial geographic, audio, and habitat characteristics of species. Multimodal embedding frameworks like ImageBind [7] and GRAFT [8] have shown that it is possible to learn a joint representation space by aligning all available modalities to the ground-level image modality. This allows for training modality-specific encoders using only image-paired datasets. One potential downside of such methods is that they perform locked tuning with the ground-level image modality. This means that the ground-level image encoder is kept frozen, while the other modalities are trained to project to the existing learned space of the ground-level image modality. This can lead to sub-optimal performance since task-specific unique information of each modality is lost [9]. To this end, we propose multimodal patching, building upon patching [10], a framework to distill knowledge from various modalities while still preserving the original embedding space of the binding modality. We show that multimodal patching can improve zero-shot classification performance of the binding modality. We create a joint embedding space containing six modalities (Figure 1). To facilitate future research and evaluation of ecological models, we present TaxaBench-8k, a truly multimodal dataset containing six paired modalities. The contributions of our work are fivefold: 1. Multimodal Patching. We propose a simple yet effective patching technique that improves over the ImageBind framework. 2. Multimodal Models for Ecological Applications. We propose modality-specific encoders that can handle various ecological tasks over six modalities: ground-level image, geographic location, satellite image, text, audio, and environmental features. 3. Multimodal datasets. We compiled two large-scale novel cross-view datasets: i) iSoundNat: ground-level images of species with their corresponding audio; and ii) iSatNat: ground-level images of species with their corresponding satellite imagery. 4. TaxaBench-8k. We present TaxaBench-8k, a benchmarking dataset containing six paired modalities for evaluating multimodal ecological models. 5. We demonstrate our models’ effectiveness and emergent properties on several benchmarking and zero-shot tasks."
https://arxiv.org/html/2411.00640v1,Adding Error Bars to Evals:A Statistical Approach to Language Model Evaluations,"Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.","Language models are measured in the literature by evaluations, or evals. Evals are commonly run and reported with a “highest number is best” mentality; industry practice is to highlight a state-of-the-art (SOTA) result in bold, but not necessarily to test that result for any kind of statistical significance.[madaan2024quantifyingvarianceevaluationbenchmarks] Chatbot Arena[chiang2024chatbot] has popularized the use of confidence intervals in its Elo scores, but error bars remain noticeably absent from traditional question-and-answer evals. One recent and notable exception is the technical report on the Llama 3 model family[dubey2024llama3herdmodels], which includes simple confidence intervals on a number of evals. In this article, we seek to introduce rigorous statistical thinking into the world of language model evaluations, so that researchers may quantify the precision with which they are able to answer questions and test hypotheses using evals. After developing a comprehensive analytic framework, we make specific recommendations for the computation of confidence intervals and the reporting of eval results. Using this framework, we show that the confidence intervals recently reported in [dubey2024llama3herdmodels] are likely too narrow in some cases and too wide in other cases. A short hypothetical example will motivate the overall discussion. Imagine that two competing models, code-named “Galleon” and “Dreadnought”, are being considered for deployment in a particular application (say, with a bent toward coding and mathematical reasoning tasks). As part of the decision-making process, three popular language model evaluations are performed on the two models: MATH, a mathematical reasoning eval[hendrycks2021math]; HumanEval, a Python coding eval[chen2021evaluatinglargelanguagemodels]; and MGSM, a multilingual eval covering grade-school math[shi2022languagemodelsmultilingualchainofthought]. The fictional results from this hypothetical comparison are presented in Table 1. Eval \ Model “Galleon” “Dreadnought” Difference MATH 65.5% 63.0% +2.5%percent2.5+2.5\%+ 2.5 % HumanEval 83.6% 87.7% −3.1%percent3.1-3.1\%- 3.1 % MGSM 75.3% 78.0% −2.7%percent2.7-2.7\%- 2.7 % Table 1: Hypothetical data from two fictional models across three (non-fictional) evals On its face, the data table presents conflicting results: Galleon appears to outperform Dreadnought on MATH (65.5% vs 63.0%), but Dreadnought has performed better on HumanEval and MGSM by slightly wider margins. Is it safe to conclude from the results that Dreadnought is generally better suited for coding and mathematical tasks, given its margin of victory on two of three evals? Or should something in the data potentially give us pause? “Evaluating the evaluations” is a complex undertaking fraught with both qualitative and quantitative considerations.[ganguli2023challenges] Remaining agnostic about the relative and qualitative merits of various evals, this article develops a framework for answering quantitative questions about specific eval results. With the aim of informing holistic decision-making, we offer a series of recommendations for running and reporting evals in a way that enables researchers to test well-formed hypotheses about competing models, competing hyperparameters, and competing prompts. Our specific recommendations to researchers include: 1. Computing standard errors of the mean using the Central Limit Theorem 2. When questions are drawn in related groups, computing clustered standard errors 3. Reducing variance by resampling answers and by analyzing next-token probabilities 4. When two models are being compared, conducting statistical inference on the question-level paired differences, rather than the population-level summary statistics 5. Using power analysis to determine whether an eval (or a random subsample) is capable of testing a hypothesis of interest Drawing on statistical theory and the experimental design literature, we demonstrate that a small number of conceptual assumptions unlocks a rich theoretical landscape for researchers studying language model evaluations, and show practitioners how to conduct statistical inference on often-noisy eval data. For an overview of experiment design, we refer the reader to [Imbens_Rubin_2015]."
https://arxiv.org/html/2411.00205v1,Compositional Automata Embeddings for Goal-Conditioned Reinforcement Learning,"Goal-conditioned reinforcement learning is a powerful way to control an AI agent’s behavior at runtime. That said, popular goal representations, e.g., target states or natural language, are either limited to Markovian tasks or rely on ambiguous task semantics. We propose representing temporal goals using compositions of deterministic finite automata (cDFAs) and use cDFAs to guide RL agents. cDFAs balance the need for formal temporal semantics with ease of interpretation: if one can understand a flow chart, one can understand a cDFA. On the other hand, cDFAs form a countably infinite concept class with Boolean semantics, and subtle changes to the automaton can result in very different tasks, making them difficult to condition agent behavior on. To address this, we observe that all paths through a DFA correspond to a series of reach-avoid tasks and propose pre-training graph neural network embeddings on “reach-avoid derived” DFAs. Through empirical evaluation, we demonstrate that the proposed pre-training method enables zero-shot generalization to various cDFA task classes and accelerated policy specialization without the myopic suboptimality of hierarchical methods.","Goal-conditioned reinforcement learning (RL) [33] has proven to be a powerful way to create AI agents whose task (or goal) can be specified (conditioned on) at runtime. In practice, this is done by learning a goal encoder, i.e., a mapping to dense vectors, and passing the encoded goals as inputs to a policy, e.g., a feedforward neural network. This expressive framework enables the development of flexible agents that can be deployed in a priori unknown ways, e.g., visiting states never targeted during training. The rise of large language models has popularized leveraging natural language as an ergonomic means to specify a task, e.g., “pick up the onions, chop them, and take them to stove.” While incredibly powerful, tasks specified by target states or natural language have a number of shortcomings. First and foremost, target states are necessarily limited to non-temporal tasks. On the other hand, natural language is, by definition, ambiguous, providing little in the way of formal guarantees or analysis of what task is being asked of the AI agent. To this end, we consider conditioning on tasks in the form of Boolean combinations of deterministic finite automata (DFAs). We refer to this concept class as compositional DFAs (cDFAs). The choice of cDFAs as the concept class is motivated by three observations. First and foremost, DFAs offer simple and intuitive semantics that require only a cursory familiarity with formal languages–if one can understand a flow chart, one can understand a DFA. Moreover, recent works have demonstrated that DFA and cDFA can be learned in a few shot manner from expert demonstrations and natural language descriptions [41]. As such, DFAs offer a balance between the accessibility of natural language and rigid formal semantics. The addition of Boolean combinations to cDFA, e.g., perform task 1 AND task 2 AND task 3, provides a simple mechanism to build complicated tasks from smaller ones. Second, DFAs represent temporal tasks that become Markovian by augmenting the state-space with finite memory. Further, they are the “simplest” family to do so since their finite states are equivalent to having a finite number of sub-tasks, formally Nerode congruences [19]. This is particularly important for goal-conditioned RL which necessarily treats temporal tasks differently than traditional RL. For traditional RL, because the task is fixed, one can simply augment the state space with the corresponding memory to make the task Markovian. In goal-conditioned RL, this is not, in general, possible as it is unclear what history will be important until the task is provided. Instead, the encoded task must relay to the policy this temporal information. Third, existing formulations like temporal logics over finite traces and series of reach-avoid tasks are regular languages and thus are expressible as DFAs [11]. This makes DFAs a natural target for conditioning an RL policy on temporal tasks. The expressivity of DFAs introduces a number of challenges for goal-conditioned RL. First, DFAs form a countably infinite and exponentially growing concept class where subtle changes in the DFA can result in large changes in an agent’s behavior. Notably, this means that any distribution over DFAs is necessarily biased with many “similar” DFAs having drastically different probability. Thus, to generally work with DFAs, one cannot simply match finite patterns, but need to learn to encode details necessary for planning. Second, as with traditional goal-based objectives, DFAs provide a very sparse binary reward signal–did you reach the accepting state or not? Together with non-trivial dynamics, naïve applications of RL become infeasible due to the lack of dense reward signal. Finally, the exponentially-expanding concept class presents computational limitations for encoding. For example, many interesting DFAs may be too large to be feasibly processed by a graph neural network. To address these issues of reward sparsity and the need to encode planning, we introduce a distribution of DFAs, called reach-avoid derived (RAD). This concept class is inspired by the observation that all paths through a DFA correspond to a series of (local) reach-avoid problems. We argue in Section 4 that RAD encourages learning to navigate a DFA’s structure. Our first key result is that pre-training DFA encoders on RAD DFAs enables zero-shot generalization to other DFAs. Next, we treat the problem of DFA size. Many problems are naturally expressed compositionally, e.g., a sequence of rules that must all hold or a set of tasks of which at least one must be accomplished. Due to their Boolean semantics, i.e., did you reach the accepting state or not, DFAs offer well-defined semantics under Boolean compositions. Our second key insight is to encode conjunctions111With negations and disjunctions omitted as straightforward extensions. of DFAs (called cDFAs). This is done by using a graph attention network [9] (GATv2) to encode the individual DFA graph structure as well as the conjunction (AND) relationships between a collection of tasks. Recalling that the conjunction of any two DFAs grows (worst-case) quadratically, we observe that cDFAs offer an exponential reduction in the size of temporal tasks passed to GATv2. 1.1 Contributions Our main contributions are: (1) we propose compositional DFAs (cDFAs), which balance formal semantics, accessibility, and expressivity, as a goal representation for temporal tasks in goal-conditioned RL; (2) we propose encoding cDFAs using graph attention networks (GATv2); (3) we propose pre-training the cDFA encoder on reach-avoid derived (RAD) DFAs, and (4) we perform experiments demonstrating strong zero-shot generalization of pre-training on RAD DFAs. 1.2 Related Work Our work explores a temporal variant of goal-conditioned RL [33] where goals are represented as automata. While traditionally focused on goals as future target states, this has since been extended to tasks such as natural language [7, 21, 26, 37] and temporal logic [38]. While not as expressive as natural language, we believe cDFAs offer a balance between the ergonomics of language while maintaining unambiguous semantics–something of increasing importance due to the seemingly inevitable proliferation of AI agents to safety-critical systems. Moreover, DFA inference has a rich literature [6, 15, 17, 24, 27, 28, 45] with recent works have even shown the ability to learn DFA and cDFA from natural language and expert demonstrations–bridging the gap even more between natural language and automata specified goals [41, 42, 43]. Operating in a similar space, previous work, LTL2Action [38], has shown success with using (finite) linear temporal logic (LTL), a modal extension of propositional logic, to condition RL policies on. In fact, the pre-training and test domains used in this paper are directly derived from that work. Our choice to focus on DFA rather than LTL is two-fold. First and foremost, over finite traces, LTL is strictly less expressive than DFA. For example, LTL cannot express tasks such as “the number of times the light switch is toggled should be even.” Second, like DFA, LTL tasks constitute a countably infinite set of tasks. This again means that any distribution over LTL is necessarily biased to certain subclasses. On the one hand, the syntactic structure makes separation of subclasses very easy. On the other, it remains unclear how to generalize to “common” LTL formula. By contrast, we argue that the local reach-avoid structure of DFAs offers a direct mechanism for generalization. Finally, we note that while LTL is exponentially more succinct than DFA, this is largely mitigated by supporting boolean combinations of DFAs (cDFAs). Moving away from goal-conditioned RL, recent works have proposed performing symbolic planning on the DFA and cleverly stringing together policies to realize proposed paths [16, 22, 23, 30]. However, their method can still suffer from sub-optimality due to their goal-conditioned value functions being myopic. Specifically, if there are multiple ways to reach the next sub-goal of a temporal task, the optimality of the next action depends on the entire plan, not just the next sub-goal–destroying the compositional structure used for planning. For example, Figure 4 shows a simple example in which hierarchical approaches will find a suboptimal solution due to their myopia. In contrast, conditioning on cDFA embeddings allows our policies to account for the entire task. On the single-task LTL-constrained policy optimization side, several works have tackled the problem in varying settings [10, 13, 36, 44]. Adjacent to these efforts, various approaches have explored LTL specifications and automaton-like models as objectives in RL [2, 3, 8, 12, 14, 20, 29, 32, 46, 48]. A different line of work considers leveraging quantitative semantics of specifications to shape the reward [1, 22, 25, 36]. However, all of these lines of work are limited to learning a single, fixed goal. Finally, in our previous work [47], we used hindsight experience replay [5] to solve the reward sparsity problem for DFA-conditioned off-policy learning of DFA task classes. We observe that our RAD pre-training pipeline has similar sample efficiency while generalizing to larger classes of DFAs."
https://arxiv.org/html/2411.00177v1,LLM4Mat-Bench: Benchmarking Large Language Models for Materials Property Prediction,"Large language models (LLMs) are increasingly being used in materials science. However, little attention has been given to benchmarking and standardized evaluation for LLM-based materials property prediction, which hinders progress. We present LLM4Mat-Bench, the largest benchmark to date for evaluating the performance of LLMs in predicting the properties of crystalline materials. LLM4Mat-Bench contains about 1.9M crystal structures in total, collected from 10 publicly available materials data sources, and 45 distinct properties. LLM4Mat-Bench features different input modalities: crystal composition, CIF, and crystal text description, with 4.7M, 615.5M, and 3.1B tokens in total for each modality, respectively. We use LLM4Mat-Bench to fine-tune models with different sizes, including LLM-Prop and MatBERT, and provide zero-shot and few-shot prompts to evaluate the property prediction capabilities of LLM-chat-like models, including Llama, Gemma, and Mistral. The results highlight the challenges of general-purpose LLMs in materials science and the need for task-specific predictive models and task-specific instruction-tuned LLMs in materials property prediction 111The Benchmark and code can be found at: https://github.com/vertaix/LLM4Mat-Bench. Keywords: large language models, materials property prediction, crystalline materials, benchmarks","With the remarkable success of large language models (LLMs) in solving natural language tasks (Devlin et al., 2018; Radford et al., 2018; Raffel et al., 2020; Achiam et al., 2023; Touvron et al., 2023) and different scientific tasks (Lin et al., 2022; Edwards et al., 2022; Valentini et al., 2023; Castro Nascimento and Pimentel, 2023; Fang et al., 2023; Lv et al., 2024), scientists have recently started to leverage LLMs to tackle very important and challenging problems in materials science, including predicting materials properties (Rubungo et al., 2023; Korolev and Protsenko, 2023; Xie et al., 2023; Das et al., 2023; Qu et al., 2024; Choudhary, 2024) and discovering new materials (Antunes et al., 2023; Gruver et al., 2023; Qu et al., 2024; Choudhary, 2024). The learning capabilities of LLMs have the potential to revolutionize the field of materials science. For example, recent research by Rubungo et al. (2023) has demonstrated the exceptional performance of LLMs in predicting the properties of crystalline materials based on textual descriptions of their structures. In their study, they introduced a novel dataset, TextEdge, which comprises textual descriptions of crystals and their corresponding properties. This dataset was used to fine-tune the encoder component of the T5-small model for the task of materials property prediction. The findings of Rubungo et al. (2023) challenge the conventional practice of heavily relying on graph neural networks and using solely either crystal composition or structure as input for property prediction. Their work underscores the significance of further investigating the extent to which LLMs can be harnessed to develop innovative techniques for accurately predicting the properties of crystalline materials, thereby enhancing the materials discovery pipeline. Unfortunately, the proposed TextEdge dataset is limited in scope, comprising approximately 145K samples and encompassing only three distinct properties. Furthermore, its lack of diversity, being derived from a single data source (Materials Project (Jain et al., 2013)), hinders its effectiveness in assessing the robustness of LLMs in materials property prediction. In this work, we introduce LLM4Mat-Bench, a benchmark dataset collected to evaluate the performance of LLMs in predicting the properties of crystalline materials. To the best of our knowledge, LLM4Mat-Bench is the most extensive benchmark to date for assessing the efficacy of language models in materials property prediction. The dataset comprises approximately two million samples, sourced from ten publicly available materials sources, each containing between 10K and 1M structure samples. LLM4Mat-Bench encompasses several tasks, including the prediction of electronic, elastic, and thermodynamic properties based on a material’s composition, crystal information file (CIF), or textual description of its structure. We use LLM4Mat-Bench to evaluate several LLMs of different sizes, namely LLM-Prop (Rubungo et al., 2023) (35M parameters), MatBERT (Walker et al., 2021) (109.5M parameters), and Llama2 (Touvron et al., 2023) (7B parameters). And we provide fixed train-valid-test splits, along with carefully designed zero-shot and few-shot prompts to ensure reproducibility. We anticipate that LLM4Mat-Bench will significantly advance the application of LLMs in addressing critical challenges in materials science, including property prediction and materials discovery."
https://arxiv.org/html/2411.00023v2,Device-Directed Speech Detection for Follow-up Conversations Using Large Language Models,"Follow-up conversations with virtual assistants (VAs) enable a user to seamlessly interact with a VA without the need to repeatedly invoke it using a keyword (after the first query). Therefore, accurate Device-directed Speech Detection (DDSD) from the follow-up queries is critical for enabling naturalistic user experience. To this end, we explore the notion of Large Language Models (LLMs) and model the first query when making inference about the follow-ups (based on the ASR-decoded text), via prompting of a pretrained LLM, or by adapting a binary classifier on top of the LLM. In doing so, we also exploit the ASR uncertainty when designing the LLM prompts. We show on the real-world dataset of follow-up conversations that this approach yields large gains (20-40% reduction in false alarms at 10% fixed false rejects) due to the joint modeling of the previous speech context and ASR uncertainty, compared to when follow-ups are modeled alone.","Virtual assistants (VAs) are at the core of smart devices (e.g., mobile phones, smart speakers, wearables, etc.) as they aim to enable a naturalistic voice-based interaction between a user and a device. For VAs to respond to the user requests reliably, they need to infer whether the user is talking to the device or not. For instance, the user could be talking to someone else, and/or there could be side-speech conversations, background noise, etc. Therefore, classifying accurately if the user’s speech is device-directed is critical for providing relevant responses, and to avoid interfering with the user’s interactions which are not device-directed, i.e., intended for the VA. This task is often referred to as the device-directed speech detection (DDSD) [1; 2; 3; 4]. Most existing works on DDSD focus on detection from single queries of the user, often beginning with a wakeword (e.g., “Hey Google”, “Hey Siri”, “Alexa”, and so on). Such isolated utterances are usually a complete question or a task request from a user to the VA, and often do not require additional context to determine if the speech is the VA directed. In this work, we address the task of DDSD in follow-up conversations, where the user’s first query starts with a wakeword (that is easier to detect with high accuracy by existing systems for the wakeword detection [5]), potentially followed by another query (termed as the “follow-up”), as a continuation of the conversation with the VA. The follow-ups by design do not require the wakeword, and, therefore, classifying them correctly is far more challenging (see Fig. 1). Figure 1: Follow-up conversations: the pair of user queries are first processed by an ASR system, which outputs the text transcriptions of the user’s speech. The joint ASR transcription of the initial and follow-up queries are input to the LLM that detects if the latter is directed to the Virtual Assistant (VA). Previous approaches to DDSD process isolated utterances (i.e., no previous context is considered) either directly from audio [2], text [6], or from intermediate Automatic Speech Recognition (ASR) lattice-based features [7]. A few recent approaches attempted classification of device-directed speech in the context of natural turn-taking [3; 4; 8] by exploring various acoustic and lexical features; however, these works do not account for joint modeling of the ASR uncertainty and multi-turn user queries. The approach proposed here is built upon the recent work in [1] that focuses on the DDSD task from a single query using a combination of off-the-shelf ASR model (n𝑛nitalic_n-best ASR hypotheses) and LLM. We focus on a more challenging task – DDSD for follow-ups, where, given the previous query of the user, the goal is to determine if the follow-up query is directed to the device. For this, we propose two LLM-based approaches: (i) prompting-based, where we experiment with direct text-prompting of a pretrained, and a finetuned LLM (Sec. 2.1). (ii) classification-based approach, where, instead of parsing the LLM decisions from output text, as in (i), we introduce a classification head on top of the LLM, so the approach can make probabilistic binary decisions for the DDSD (Sec. 2.2). Specifically, we simultaneously address two key limitations of the existing systems for DDSD: (i) the lack of context – where only the single query is modeled; instead, we account for both the initial and follow-up queries, thus, providing valuable information to the LLM when making decisions, and (ii) ASR uncertainty – while ASR models aim to transcribe speech to text accurately, speech recognition from the real-world speech is often inaccurate and the 1111-best ASR hypothesis acts as an information bottleneck between the ASR and the LLM component. We expand this information bottleneck by exposing the LLM to an n𝑛nitalic_n-best list of ASR hypotheses, and we do so only for the follow-up utterance to prevent confusing the model, i.e., we focus on the uncertainty in the follow-up only. Apart from direct prompting, we also explore finetuning of Low-Rank Adaptation (LoRA) [9] adapters using training data of the n𝑛nitalic_n-best list prompts. We use an in-house general-purpose English ASR system and Vicuna [10], an instruction-tuned LLaMA LLM [11]. Prompt: In this task, we provide a pair of queries made by human in the following format: ‘Query 1: <<<text>>> | Query 2: <<<text>>>’. Query 1 is directed toward the voice assistant. Query 2 is the follow-up query made by human. {For Query 2, we provided an n-best list of ASR hypotheses for the spoken utterance. Each of the hypothesis is separated by a newline character. The cost of each hypothesis is at the end in the format ‘[cost]’ where a low cost indicates that we are more confident about that ASR hypothesis.} Determine whether Query 2 is directed towards a voice assistant or a human being. Typical spoken utterances directed towards the voice assistant are commands to fulfill a task or queries to get some information. Answer only from the following categories [‘1’, ‘0’] where ‘1’ indicates that the utterance is directed towards the voice assistant and ‘0’ indicates that the utterance is directed towards a human being. In your answer the last line should contain nothing else but the number ‘0’ or ‘1’. Figure 2: Task-prompt used for Device-directed Speech Detection. Note that for the follow-up query, the text in italics is additionally added when including the n-best hypothesis by the ASR system. Our experiments show noticeable gains in terms of the DDSD accuracy over the simple approach where only the follow-up query is used as input to LLM. While this is intuitive, we empirically show on the challenging real-world dataset of follow-up conversations that jointly modeling the context (the previous and follow-up query), together with ASR uncertainty, helps to improve the LLM detection accuracy in the range of ∼similar-to\sim∼20-40%, when the classifier is trained on top of the LLM. It also largely outperforms the traditional prompting-based approach without the LLM tuning on the target task."
https://arxiv.org/html/2410.24022v1,SFM-Protein: Integrative Co-evolutionary Pre-training for Advanced Protein Sequence Representation,"Proteins, essential to biological systems, perform functions intricately linked to their three-dimensional structures. Understanding the relationship between protein structures and their amino acid sequences remains a core challenge in protein modeling. While traditional protein foundation models benefit from pre-training on vast unlabeled datasets, they often struggle to capture critical co-evolutionary information, which evolutionary-based methods excel at. In this study, we introduce a novel pre-training strategy for protein foundation models that emphasizes the interactions among amino acid residues to enhance the extraction of both short- and long-range co-evolutionary features from sequence data. Trained on a large-scale protein sequence dataset, our model demonstrates superior generalization ability, outperforming established baselines of similar size, including the ESM model, across diverse downstream tasks. Experimental results confirm the model’s effectiveness in integrating co-evolutionary information, marking a significant step forward in protein sequence-based modeling.","Proteins are the molecular machines of life, orchestrating essential biological functions that drive survival and growth. Their functionality stems from complex three-dimensional structures, which are determined by the unique sequences of amino acids that make up each protein (Jumper et al., 2021; Yang et al., 2020). Understanding how amino acid sequences define protein structure, and in turn function, remains a key challenge in biology. Solving this puzzle is crucial for designing targeted interventions in diseases and engineering proteins with new, desirable functions (Whitford, 2013). Proteins are far more than simple chains of amino acids. Their behavior cannot be fully understood by examining individual residues or their linear arrangement. Instead, proteins fold into intricate tertiary structures where residues far apart in sequence can be spatially adjacent. These interactions, both short- and long-range, shape the protein’s functional roles, mediating both static and dynamic biological processes (Sinai et al., 2017; Riesselman et al., 2017; Ding et al., 2019; Hsu et al., 2021). Co-evolutionary interactions, both within local sequences and across distant residues, are critical for maintaining the structural and functional integrity of proteins. These interactions ensure that amino acid co-variations preserve the stability of the protein’s structure. This co-evolution reflects the collective interactions that underpin protein dynamics, influencing their folding and function. Following Anfinsen’s landmark discovery that protein sequences inherently contain the information required to fold into their native structures (Anfinsen et al., 1961), Levinthal proposed his famous paradox. He argued that the number of possible protein conformations is so vast that a random search would take an impractical amount of time to find the correct structure (Levinthal, 1968, 1969). The paradox hinted at the existence of predetermined folding pathways, though their mechanisms remained unclear. To address this, researchers proposed the concept of “foldons”, small cooperative folding units consisting of multiple residues, which fold sequentially and guide the protein to its final structure (Englander & Mayne, 2014). In silico studies have revealed that the formation of local structures often precedes the establishment of distant, non-local contacts (Lindorff-Larsen et al., 2011). These early-forming interactions are key to efficiently achieving the protein’s final, functional form without the need for backtracking (Voelz & Dill, 2007). Understanding these co-evolutionary interactions, both short-range, which influence secondary structures, and long-range, which define tertiary structures, is vital for decoding the sequence-structure-function relationship. Insights into this relationship are crucial for applications such as drug design and protein engineering (Phillips, 2009a, b; Tang & Kaneko, 2020; Xu et al., 2021). Protein foundation models (PFMs), particularly those pre-trained on vast datasets of unlabeled sequences, have emerged as powerful tools for representing protein sequences (Alley et al., 2019; Bepler & Berger, 2019; Rives et al., 2021; Madani et al., 2020). These models achieve remarkable accuracy in protein-related tasks through fine-tuning. However, while models based on multiple sequence alignments (MSAs) effectively capture co-evolutionary relationships between homologous proteins, single sequence-based models often fall short in this area (Remmert et al., 2012; Mirabello & Wallner, 2019). Despite utilizing the same sequence data, single sequence models underperform compared to MSA-based approaches. We hypothesize that this performance gap arises from the insufficient capture of residue interactions in conventional pre-training techniques, which MSAs handle more effectively by incorporating evolutionary information. Inspired by the folding hypothesis and existing research, we categorize co-evolutionary interactions into two types: short-range (local) interactions, which are crucial for forming secondary structures, and long-range (global) interactions, which shape the protein’s overall tertiary structure. This distinction is illustrated in Figure 1, where local interactions are highlighted in red and global interactions in cyan. Figure 1: Illustration of Local and Global Interactions. The local interactions are highlighted in red, while the global interactions are highlighted in cyan. (PDB ID: 4UQX) This paper introduces a novel framework for protein foundation models that effectively captures both local and global co-evolutionary information within protein sequences. Leveraging large protein sequence databases, our model employs an integrative loss function designed to better reflect the complex interplay of interactions that govern protein folding and function. We detail the model’s architecture, pre-training strategies, and the innovative loss functions used to capture both short- and long-range co-evolutionary information. Through extensive validation against empirical data and benchmarking against existing models, our approach demonstrates superior performance in predicting protein properties and functions. Figure 2: An overview of the SFM-Protein framework The remainder of this paper is structured as follows: Section 2 reviews related work on protein foundation models. Section 3 outlines our model’s architecture, loss function, and training algorithm. Section 4 presents our experimental results across a variety of downstream tasks. Section 5 concludes with a discussion of future directions and potential extensions of this work."
