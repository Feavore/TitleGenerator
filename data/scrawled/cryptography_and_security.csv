URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04981v1,Enhancing Reverse Engineering: Investigating and Benchmarking Large Language Models for Vulnerability Analysis in Decompiled Binaries,"Security experts reverse engineer (decompile) binary code to identify critical security vulnerabilities. The limited access to source code in vital systems ‚Äì such as firmware, drivers, and proprietary software used in Critical Infrastructures (CI) ‚Äì makes this analysis even more crucial on the binary level. Even with available source code, a semantic gap persists after compilation between the source and the binary code executed by the processor. This gap may hinder the detection of vulnerabilities in source code. That being said, current research on Large Language Models (LLMs) overlooks the significance of decompiled binaries in this area by focusing solely on source code. In this work, we are the first to empirically uncover the substantial semantic limitations of state-of-the-art LLMs when it comes to analyzing vulnerabilities in decompiled binaries, largely due to the absence of relevant datasets. To bridge the gap, we introduce DeBinVul, a novel decompiled binary code vulnerability dataset. Our dataset is multi-architecture and multi-optimization, focusing on C/C++ due to their wide usage in CI and association with numerous vulnerabilities. Specifically, we curate 150,872 samples of vulnerable and non-vulnerable decompiled binary code for the task of (i) identifying; (ii) classifying; (iii) describing vulnerabilities; and (iv) recovering function names in the domain of decompiled binaries. Subsequently, we fine-tune state-of-the-art LLMs using DeBinVul and report on a performance increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama3, and CodeGen2 respectively, in detecting binary code vulnerabilities. Additionally, using DeBinVul, we report a high performance of 80-90% on the vulnerability classification task. Furthermore, we report improved performance in function name recovery and vulnerability description tasks. All our artifacts are available at 111https://anonymous.4open.science/r/vuln-decompiled-summarization-8017.","It is crucial to perform vulnerability analysis in software that plays a vital role in shaping Critical Infrastructure (CI) sectors such as water, energy, communications, and defense, to name a few. Despite the many advancement in software security, the reported number of Common Vulnerabilities and Exposures (CVEs) has been increasing annually, from 14,249 in 2022, to 17,114 in 2023, and surging to 22,254 in 2024 (Qualys 2024). These CVEs are correlated with the Common Weakness Enumeration (CWE) categories maintained by MITRE, which provide a baseline for identifying, mitigating, and preventing security weaknesses during source code development. Notably, during the compilation optimization, the source code transitions into binary code, resulting in mismatches and changes in code properties (Eschweiler et al. 2016). This inherently creates a vulnerability semantic discrepancy not addressed by CWEs or other vulnerability categorization systems. As such, vulnerability analysis of source code and binary code remains two distinct and separate areas of research (Mantovani et al. 2022). This phenomenon is succinctly captured by the statement, ‚ÄúWhat you see is not what you execute‚Äù (Balakrishnan and Reps 2010). Why Decompiled Binary Code Vulnerability Analysis? ‚Äî Significance & Technical Challenges. Binary code (i.e., binaries/executables) is a fundamental component of computing and digital systems taking the form of firmware, drivers/agents, and closed-source software. To safeguard these systems, reverse engineers attempt to uncover source code from binary code using decompilation tools such as Ghidra, angr, and IDA Pro, subsequently performing essential vulnerability analysis on decompiled binary code (Burk et al. 2022). This is particularly important for two main reasons; first, access to source code is most of the time limited/restricted for proprietary or security reasons; second, vulnerabilities may not be apparent in the source code, such as those related to the execution environment, operating system, specific compiler optimizations, and hardware specifications. For instance, use-after-free (memory corruption) vulnerabilities, which affect many closed-source system components and network protocols written in C/C++, are known to be one of the most difficult types to identify using source code static analysis. (Lee et al. 2015; Nguyen et al. 2020). On a different note, due to the NP-complete nature of the compiler optimization problem (Eschweiler et al. 2016), decompiled binary code loses important constructs, such as structured control flow, complex data structures, variable names, and function signatures (Burk et al. 2022). As a consequence, these setbacks impede the ability of reverse engineers to analyze vulnerability in binary code, necessitating significant manual effort and time investment. Avant-garde Advancements and Perceived Opportunities. More recently, state-of-the-art Large Language Models (LLMs) have been employed as an optimizer to improve the readability and simplicity of decompilers‚Äô output, ultimately reducing the cognitive burden of understanding decompiled binary code for reverse engineers (Hu, Liang, and Chen 2024). Similarly, a cross-modal knowledge prober coupled with LLMs have been utilized to effectively lift the semantic gap between source and binary code (Su et al. 2024). Furthermore, comprehensive benchmarking was conducted on ChatGPT/GPT-4 and other LLMs to evaluate their effectiveness in summarizing the semantics of binary code (Jin et al. 2023). This assessment revealed the transformative capabilities of LLMs in the field while also highlighting key findings on their limitations, which demands further research. While these efforts aim to improve the readability and comprehension of decompiled binary code semantics, they overlook the vulnerability semantic gap between source code and deccompiled binary. To date, no comprehensive research has been conducted to thoroughly investigate and explore the potential of LLMs in decompiled binary code vulnerability analysis. This task remains far from straightforward due to the following two main limitations; (i) lack of real-world decompiled binary code vulnerability datasets; and (ii) vulnerability semantic gap between source and decompiled binary code in LLMs. Currently state-of-the-art LLMs are trained on textual-like input, including source code, but they lack semantic knowledge of vulnerabilities in the decompiled binary code domain due to the absence of representative datasets. Through an empirical and pragmatic investigation of the analytical abilities of LLMs, we find a consistent low performance of 67%, 54%, and 33% in decompiled binary code, compared to a slightly higher performance of 75%, 68%, and 45% in source code with GPT4, Gemini, and LLaMa 3, respectively. Table 1 highlights some of the insights we derived from our investigation. More information on the investigation is provided in the Appendix and Table 8 in Section Source & Decompiled Binary Code Vulnerability Semantic Gap: Investigating LLMs‚Äô Analytical Abilities. To this end, significant manual effort is required to curate decompiled binary code samples that include relevant vulnerabilities, realistic compilation and decompilation settings, and representative input formats for LLMs. Moreover, this entails of state-of-the-art LLMs through extensive fine-tuning and instructive/prompting techniques. Table 1: Motivational Investigation: LLMs semantic gap comparison between static source code and decompiled binary code on vulnerability classification task. Reported average F1-scores. Input Type GPT4 Gemini CodeLLaMa Source Code 0.75 0.68 0.64 Dec. Binary Code 0.67 ‚Üì‚Üì\downarrow‚Üì 0.54 ‚Üì‚Üì\downarrow‚Üì 0.54 ‚Üì‚Üì\downarrow‚Üì Mistral LLaMa 3 CodeGen2 Source Code 0.60 0.45 0.64 Dec. Binary Code 0.54 ‚Üì‚Üì\downarrow‚Üì 0.33 ‚Üì‚Üì\downarrow‚Üì 0.52 ‚Üì‚Üì\downarrow‚Üì Our Contribution. To tackle these challenges and capitalize on the perceived opportunities, this work aims to ask: Can we enhance reverse engineering by bridging the semantic gap between source and decompiled binary code vulnerability analysis in state-of-the-art LLMs? To answer this question, we undertake the following quests. Firstly, we empirically investigate the analytical abilities of state-of-the-art LLMs and uncover a vulnerability semantic gap between source and decompiled binary code. Our investigation encompasses real-world code injection in public repositories, simulating an emergent cybersecurity attack that targets the widely recognized Linux-based XZ Utils (Akamai 2023). Secondly, we introduce DeBinVul a novel decompiled binary vulnerability dataset with zero-shot prompt engineering. Our dataset comprises relevant non-vulnerable and vulnerable source code samples, tagged with CWE classes, and compiled using Clang and GCC across four different CPU architectures: x86, x64, ARM, and MIPS. During compilation, we applied two levels of optimizations; O0subscriptùëÇ0O_{0}italic_O start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and O3subscriptùëÇ3O_{3}italic_O start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. Then, using GHIDRA we decompile the compiled code to obtain the decompiled binary code samples. Furthermore, we augment our dataset with code descriptions and instruction/prompting techniques. Thirdly, we fine tune and optimize state-of-the-art LLMs, aiming to enhance their capabilities in assisting reverse engineers in uncovering vulnerabilities in decompiled binary code. In summary, the contributions of this paper are as follows: ‚Ä¢ To the best of our knowledge, we are the first to empirically investigate the vulnerability semantic gap between source and decompiled binary code in state-of-the-art LLMs. Our findings highlight the suboptimal performance of these models in performing vulnerability analysis on decompiled binaries. ‚Ä¢ We compile and release, DeBinVul, a novel decompiled binary code vulnerability dataset comprising 150,872 samples of openly sourced, synthetically generated, and manually crafted corner case C/C++ code samples. It is designed to tackle four important binary code vulnerability analysis tasks, including vulnerability detection, classification, description, and function name recovery. ‚Ä¢ We employ our proposed dataset to fine-tune and enhance the reverse engineering capabilities across a range of state-of-the-art LLMs. Our results shows a performance increase of 19%, 24%, and 21% in the capabilities of CodeLlama, Llama 3, and CodeGen2 respectively, in detecting vulnerabilities in binary code."
https://arxiv.org/html/2411.04710v1,Differential PrivacyOverview and Fundamental Techniques,"This chapter is meant to be part of the book ‚ÄúDifferential Privacy in Artificial Intelligence: From Theory to Practice‚Äù and provides an introduction to Differential Privacy. It starts by illustrating various attempts to protect data privacy, emphasizing where and why they failed, and providing the key desiderata of a robust privacy definition. It then defines the key actors, tasks, and scopes that make up the domain of privacy-preserving data analysis. Following that, it formalizes the definition of Differential Privacy and its inherent properties, including composition, post-processing immunity, and group privacy. The chapter also reviews the basic techniques and mechanisms commonly used to implement Differential Privacy in its pure and approximate forms.","Data is continuously harvested from nearly every facet of our lives by corporations, service providers, and public institutions. Whether through smartphones, social media interactions, internet use, healthcare visits, or financial transactions, information is continuously gathered, shaping the foundation of the modern economy. Private companies leverage this vast pool of data to evaluate loan candidates, optimize transportation networks, improve supply chains, personalize services, and predict market demands, all to enhance decision making. Similarly, public policies and government initiatives rely heavily on this data, guiding resource distribution, monitoring public health crises, and driving urban development and sustainability efforts. However, these datasets also contain a large array of sensitive information, including health, financial, or location data. Major privacy violations and breaches are commonplace, and can have severe negative impacts, not only on consumers and online users, but also on entire organizations and governments. For instance, the 2017 Equifax data breach (Wikipedia, 2024a) exposed the personal information of 147 million individuals, including social security numbers, birth dates, and addresses, leaving millions vulnerable to identity theft, fraud, and long-term financial harm. Similarly, the 2016 Facebook-Cambridge Analytica scandal (Wikipedia, 2024b), in which the personal data of up to 87 million Facebook users was harvested without consent for political advertising purposes, raised concerns about its possible influence on the outcome of the 2016 presidential election. Privacy concerns have become central in today‚Äôs society, driving significant changes in government policy. Various regulatory frameworks have been established, with the United States and Europe leading efforts toward stronger privacy practices. In Europe, the General Data Protection Regulation (GDPR) sets strict standards for data management, focusing on consent and data minimization (Parliament and of the European Union, 2016), while in the U.S., regulations like Title 13 (Bureau, 1998) govern the handling of census data and laws like the Health Insurance Portability and Accountability Act (HIPAA) and the California Consumer Privacy Act (CCPA) offer protections for health and consumer data (Centers for Medicare & Medicaid Services, 1996; Legislature, 2018). This movement was further emphasized in October 2023 when the Biden administration issued an Executive Order on AI, ensuring the enforcement of consumer protection laws and introducing safeguards against privacy violations in AI systems. Government actions, such as the release of the AI Bill of Rights Blueprint (House, 2023) in the US, underscore the increasing focus on privacy in both policy and technology. Public policy has also devoted extensive research into technical solutions for privacy. Over the past three decades, this research has explored a wide range of privacy definitions and techniques, but one has emerged as a pivotal framework: Differential Privacy (DP) (Dwork et al., 2006a). DP has gained widespread recognition and adoption, not only by leading technology companies like Apple, Meta, Google, and LinkedIn, but also by the U.S. government, most notably in its landmark 2020 Census data release. Differential Privacy is now widely regarded as the gold standard for privacy protection in statistical analyses and dataset releases. Its strength lies in providing a formal and mathematical definition of privacy, offering precise and provable guarantees. This is in stark contrast to historically ad-hoc and loosely defined privacy methods, which have repeatedly failed under attacks aimed at reconstructing part of the original dataset or identifying individuals in said datasets. As privacy challenges evolve, so too does Differential Privacy, expanding across diverse fields to meet new demands. This book aims at providing a comprehensive introduction to DP, particularly within the novel challenges brought by AI applications. It explores its foundational theories, applications in machine learning, and practical implementations, equipping readers with the knowledge to leverage this critical technology effectively. Overview of the chapter. This chapter is structured to provide an introduction to Differential Privacy. It begins by illustrating various attempts to protect data privacy, emphasizing where and why they failed, and providing the key desiderata of a robust privacy definition (Section 2). It then defines the key actors, tasks, and scopes that make up the domain of privacy-preserving data analysis (Section 3). Following that, Section 4, formalizes the definition of DP and its inherent properties, including composition, post-processing immunity, and group privacy. The chapter also reviews the basic techniques and mechanisms commonly used to implement Differential Privacy in Sections 4 to 6. Finally, Section 8 concludes with an overview of Differential Privacy applications and some future directions in this field."
https://arxiv.org/html/2411.04657v1,EarCapAuth: Biometric Method for Earables Using Capacitive Sensing Eartips,"Earphones can give access to sensitive information via voice assistants which demands security methods that prevent unauthorized use. Therefore, we developed EarCapAuth, an authentication mechanism using 48 capacitive electrodes embedded into the soft silicone eartips of two earables. For evaluation, we gathered capactive ear canal measurements from 20 participants in 20 wearing sessions (12 at rest, 8 while walking). A per user classifier trained for authentication achieves an EER of 7.62% and can be tuned to a FAR (False Acceptance Rate) of 1% at FRR (False Rejection Rate) of 16.14%. For identification, EarCapAuth achieves 89.95%. This outperforms some earable biometric principles from related work. Performance under motion slightly decreased to 9.76% EER for authentication and 86.40% accuracy for identification. Enrollment can be performed rapidly with multiple short earpiece insertions and a biometric decision is made every 0.33s. In the future, EarCapAuth could be integrated into high-resolution brain sensing electrode tips.","Headphones and earphones are becoming increasingly powerful and commercial devices already offer access to sensitive information about the wearer, for example, via voice assistants (Support, 2024). In addition, the so-called ‚Äúearables‚Äù are a new class of devices which come equipped with sensing capabilities to collect and store sensitive information about the wearer (R√∂ddiger et al., 2022), for example, longitudinal health (R√∂ddiger et al., 2021) or movement activity (Ferlini et al., 2021). In addition, mixing input from multiple users or by malicious actors will endanger the integrity of the data and lead to erroneous action. Most computing devices today are secured by passwords or PINs. Although methods for entering pins on earphones have been reported in previous research (Wang et al., 2024; Bi et al., 2022), they can be time-consuming for the user as they have to be re-entered each time they interact with a device. In addition, the input can potentially be observed by attackers. A possible alternative could be biometric authentication systems that have been increasingly used in recent years in smartphones such as Apple‚Äôs FaceID or TouchID (Dharavath et al., 2013). However, for earables, the established biometric principles used with smartphones are not suitable as the devices neither have a clear view of the user‚Äôs face nor are typically used with the finger. As another alternative voice recognition may be used. However, it is difficult to use in loud environments, requires significant computing power and enrollment time, and is vulnerable to replay attacks (Kinnunen et al., 2017). Therefore, as an alternative approach, we propose EarCapAuth, which leverages the unique shape of the user‚Äôs ear, which is a biometric feature that is individually different (≈Ωiga Emer≈°iƒç et al., 2017). We implement a custom device based on OpenEarable (R√∂ddiger et al., 2023) that integrates 48 capacitive electrodes into the soft silicone tip of two earpieces (24 left and right each). We open-source the hardware under an MIT license. In a user study, we gather data from 20 participants in 20 wearing sessions. Based on the dataset, we develop a machine learning pipeline that can authenticate the wearer by making an accept or reject decision. In this paper, we present an open hardware prototype and a machine learning pipeline that could also be integrated with high-resolution in-ear EEG electrodes, e.g., as recently patented by Apple (Azemi et al., 2023). Our prototype already achieves an equal error rate (ERR) of 7.62%. At a false acceptance rate (FAR) of 1%, EarCapAuth achieves a false rejection rate (FRR) of 16.14%. We implement another pipeline for identification in which the target user should be identified from the dataset and achieve 89.95% accuracy. EarCapthAuth is designed to perform reasonably well under motion-induced stress achieving an EER of 9.76% for authentication and an accuracy of 86.40% for identification. EarCapAuth makes a biometric decision every 0.33 seconds and enrollment can be performed with a few short insertions of the earpiece into the ear canal. In sum our contributions are: (i) EarCapAuth: an authentication principle for earables based on 48 sensing electrodes embedded into the soft silicone tip inside the ear canal (24 per ear) and (ii) an evaluation of the approach with 20 participants in 20 wearing sessions each showing that our approach works reliably, even under motion-induced stress; (iii) an open-source, MIT-licensed capacitive sensing eartip hardware compatible with OpenEarable (R√∂ddiger et al., 2023)."
https://arxiv.org/html/2411.04284v1,Enhancing Security Control Production With Generative AI,"Security controls are mechanisms or policies designed for cloud based services to reduce risk, protect information, and ensure compliance with security regulations. The development of security controls is traditionally a labor-intensive and time-consuming process. This paper explores the use of Generative AI to accelerate the generation of security controls. We specifically focus on generating Gherkin codes which are the domain-specific language used to define the behavior of security controls in a structured and understandable format. By leveraging large language models and in-context learning, we propose a structured framework that reduces the time required for developing security controls from 2-3 days to less than one minute. Our approach integrates detailed task descriptions, step-by-step instructions, and retrieval-augmented generation to enhance the accuracy and efficiency of the generated Gherkin code. Initial evaluations on AWS cloud services demonstrate promising results, indicating that GenAI can effectively streamline the security control development process, thus providing a robust and dynamic safeguard for cloud-based infrastructures.","In today‚Äôs rapidly evolving digital landscape, safeguarding the security and integrity of cloud-based infrastructures has become a critical priority. The intricate nature and vast scale of modern cloud environments, coupled with the escalating sophistication of cyber threats, necessitate the deployment of robust and dynamic security measures. At the heart of these defenses are security controls, which are specific safeguards or countermeasures designed to detect, prevent, or mitigate risks to information systems. The development of security controls through traditional methods involves a series of labor-intensive and time-consuming steps. Security engineers must perform detailed research to stay updated on the latest threats, vulnerabilities, and best practices. They engage in comprehensive threat modeling to identify potential risks, evaluate their likelihood and impact, and devise effective mitigation strategies. The final phase involves crafting, testing, and deploying custom code and configurations for security controls. This process is both resource and time consuming, causing delays in implementing essential security measures and leaving systems vulnerable. Figure 1. An example of the security control development process, illustrating that the development of writing Gherkin can take on average 2222-3333 days. In light of these challenges, there is growing interest in leveraging generative AI to streamline and enhance the development of security controls. Generative AI has the potential to automate many of the labor-intensive aspects of this process, thereby significantly reducing the time and effort required to establish effective security measures. By accelerating the creation of security controls, organizations can more swiftly adapt to emerging threats, ensuring the ongoing protection and integrity of their cloud-based infrastructures. 1.1. Development of security controls The development of security controls involves multiple intricate stages, each requiring careful planning and execution to ensure the robustness and efficacy of the controls. The process begins with security engineers identifying the specific service and resource, along with the appropriate type of control required for the pair. This initial stage is crucial, as it sets the foundation for the subsequent steps by determining the scope and focus of the security measures. Once the service, resource, and control type are identified, the next stage involves writing Gherkin scripts. Gherkin, a domain-specific language, is employed to define the behavior of security controls in a clear and structured manner. Gherkins use plain language to describe the expected outcomes, making them accessible to both technical and non-technical stakeholders. Writing Gherkin scripts requires a deep understanding of the service and resource, as well as the specific security requirements they must meet. After the Gherkin scripts are written, they undergo a rigorous review process. This step is critical to ensure the quality and accuracy of the scripts. Security engineers meticulously review each Gherkin script to verify that it accurately defines the intended security control and that it will function correctly when implemented. Once the Gherkin scripts have been thoroughly reviewed and validated, the next stage is the development of the actual code to execute the control. This involves translating the Gherkin-defined behaviors into executable code that can be deployed within the cloud environment. Finally, the code is deployed, and the security controls are put into operation. This deployment stage includes thorough testing to ensure that the controls function as intended and effectively mitigate the identified risks. Continuous monitoring and maintenance are also necessary to adapt to new threats and evolving requirements. Figure 1 shows all development stages discussed above. The entire loop‚Äîfrom identifying and analyzing API documentation to reviewing the generated Gherkins‚Äîcan span up to couple of weeks for a single service and resource pair. This protracted timeline underscores the need for innovative solutions, such as Generative AI, to automate and expedite the creation of security controls, thereby significantly reducing both the time and effort involved in their development and deployment. 1.2. Challenges While Generative AI holds great promise in automating the generation of Gherkin codes, there are two practical challenges that must be addressed to fully realize its potential. Challenge 1: Accurate Interpretation of Complex Service Documentation. Cloud services are highly diverse, each with its own set of configurations, actions, and security considerations. Given the limited annotated data, the large language model needs to understand the intricacies of these services and translating them into precise and actionable Gherkin specifications without conducting finetuning. Challenge 2: Ensuring Quality and Reliability of Generated Gherkin Files. Security controls must adhere to standards to ensure they provide effective protection without introducing new vulnerabilities. This necessitates a comprehensive evaluation framework that can systematically assess the generated Gherkins for completeness and compliance with security best practices. 1.3. Contributions In this work, we propose a novel paradigm to generate Gherkins for facilitating the development of security control. To tackle Challenge 1, our process starts with a comprehensive task description for the LLM, breaking it down into detailed steps to ensure clarity and precision. Next, we provide the model with examples of existing security controls and their associated Gherkins. Finally, we present the LLM with a final query to guide it in generating the desired security control efficiently. By combining a thorough task description, illustrative examples, and a clear query, our approach enables the LLM to produce accurate and effective security controls and Gherkins, thereby reducing the time and effort required from domain experts. To address Challenge 2, we collaborate with domain experts to generate a detailed rubric to evaluate generated Gherkins from multiple dimensions, ranging from 1) whether the generated scenarios are feasible; and 2) whether the description can correctly reflect the security control specified by the scenarios."
https://arxiv.org/html/2411.04236v1,Differentially Private Finite Population Estimation via Survey Weight Regularization,"In general, it is challenging to release differentially private versions of survey-weighted statistics with low error for acceptable privacy loss. This is because weighted statistics from complex sample survey data can be more sensitive to individual survey response and weight values than unweighted statistics, resulting in differentially private mechanisms that can add substantial noise to the unbiased estimate of the finite population quantity. On the other hand, simply disregarding the survey weights adds noise to a biased estimator, which also can result in an inaccurate estimate. Thus, the problem of releasing an accurate survey-weighted estimate essentially involves a trade-off among bias, precision, and privacy. We leverage this trade-off to develop a differentially private method for estimating finite population quantities. The key step is to privately estimate a hyperparameter that determines how much to regularize or shrink survey weights as a function of privacy loss. We illustrate the differentially private finite population estimation using the Panel Study of Income Dynamics. We show that optimal strategies for releasing DP survey-weighted mean income estimates require orders-of-magnitude less noise than naively using the original survey weights without modification.","As part of efforts to protect data subjects‚Äô privacy and confidentiality, data stewards can release statistics that satisfy differential privacy (DP) [DMNS06, DRo14]. To date, typical applications of DP have been based on data from censuses or administrative databases. Often, however, statistics are based on surveys with complex designs, e.g., using multi-stage, unequal probability sampling. It is well known that analysts should account for the design in inferences, which is typically done by using survey weights. Survey-weighted statistics offer unbiased and consistent estimates for finite population quantities, such as population means and totals. Yet, as we describe below, survey weights introduce challenges to implementing DP methods. These challenges motivate our work: how might data stewards apply DP to release survey-weighted statistics from complex sample surveys? Preliminary work at the intersection of DP and survey statistics has focused on synthetic data generation with weights [HSW22], estimation under classical sampling designs like stratified sampling [LBG+23], and interpretations of survey sampling methods for their privacy amplification properties [BDG+22, HDK22]. Each of these approaches attempts to utilize as much information as possible about the sampling process. However, weighting schemes can cause practical problems for DP. Weighted statistics can have significantly larger sensitivities than their unweighted counterparts, hence requiring substantially more noise to provide the equivalent level of DP protections at the same level of privacy loss [Dre23, Rei19]. Of course, one could avoid the associated increase in the DP noise variance by disregarding the sampling weights in estimation. Indeed, this can be appealing when the weighted and unweighted estimates are similar, which can occur when the survey weights are uncorrelated with the particular survey variable of interest [LV05, BBK+16, SLH24]. However, when this is not the case, the data steward ends up adding noise to a (perhaps severely) biased estimate. These two extremes suggest that DP survey-weighted estimation involves a trade-off among bias, precision, and privacy. Indeed, such trade-offs are common for DP estimation tasks even with independently identically (iid) data as well [KMR+23]. To effect this trade-off, we use linear combinations of the weighted and unweighted estimates, which we obtain by regularizing the survey weights in a DP manner. We use some of the privacy budget to determine the degree of regularization and compute the statistics of interest. We note that weight regularization is a familiar tool in survey contexts; for example, survey organizations routinely reduce anomalously large survey weights to reduce the variability of survey estimates [Gel07, Bea08, SPG15, STGG20]. 1.1 Contributions We summarize our contributions here: 1. In Section 3.1, we analyze the three-way relationship between privacy loss, precision, and bias emerging from survey data. To do this, we introduce a regularization parameter Œª‚àà[0,1]ùúÜ01\lambda\in[0,1]italic_Œª ‚àà [ 0 , 1 ] that linearly shrinks the survey weights to a constant when Œª=1ùúÜ1\lambda=1italic_Œª = 1. For any survey sample, there exists an ‚Äúoptimal‚Äù value Œª‚àósuperscriptùúÜ\lambda^{*}italic_Œª start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT which minimizes DP mean-squared error (for a fixed privacy loss) that depends on the sample size, survey measure range, possible weighting adjustments, and the difference between the unweighted and weighted mean estimates. We prove that Œª‚àó>0superscriptùúÜ0\lambda^{*}>0italic_Œª start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT > 0 (for any informative sampling design). Similarly, we prove that for any finite privacy loss, there is a limit to the amount of bias that can be corrected by design-based weight adjustment without requiring DP noise that exceeds said correction. 2. In Section 3.2, we propose a two-step procedure to estimate survey-weighted population means using œÅùúå\rhoitalic_œÅ-zero-concentrated differential privacy [BS16]. First, we use the exponential mechanism to estimate Œª‚àósuperscriptùúÜ\lambda^{*}italic_Œª start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT; then, we use this output to shrink the survey weights and estimate the population mean using the Gaussian mechanism. We also provide different asymptotic and finite-sample approaches to quantifying errors due to sampling, weight shrinkage, and DP noise, allowing users to construct DP confidence intervals for the population mean estimates. 3. In Section 4, we demonstrate our methodology on survey microdata from the Panel Study of Income Dynamics (PSID) [Sur19], a longitudinal survey containing family-level statistics on income sources and other sociodemographic information and oversampling from lower income sub-populations. We show how different survey outcome variables require different degrees of survey weight regularization, allowing analysts to more efficiently tailor DP privacy loss budgets when estimating multiple population means for different survey response variables. We also empirically evaluate uncertainty quantification properties, including confidence interval coverage. 1.2 Related Literature While there is an extensive literature on differentially private statistical analyses (see [SS23] for a review) and on methods for complex sample survey inference, there is little literature at their intersection. Many DP algorithms rely on the ‚Äúamplification by sub-sampling‚Äù property, wherein applying a DP algorithm on a simple random sample without replacement yields smaller privacy loss than the same algorithm applied to the entire population [BBG18]. However, for survey designs besides simple random sampling, this property may not hold [BDG+22] nor does it always improve accuracy [HDK22]. We consider design-based survey inference where the weights themselves contain all relevant sampling information and, therefore, must be protected with DP. We do not consider the release of auxiliary data used to construct design-based weights, instead isolating the privacy cost of incorporating survey sample design exclusively within the weights. The most direct line of work compared to ours uses methods that generate synthetic survey responses and weights [HSW22]. These methods can produce synthetic data that are interoperable with existing analyses and admit combining-rules-based approaches to inferences with synthetic data [RR07]. Our approach differs in key ways. First, we directly use survey-weighted estimators as opposed to working through multiple synthetic datasets and inferences via combining rules. Second, we provide decision-making guidelines for whether certain kinds of weighting corrections can be sufficiently estimated using DP at a given finite sample size."
https://arxiv.org/html/2411.04680v1,Differentially Private Continual Learningusing Pre-Trained Models,"This work explores the intersection of continual learning (CL) and differential privacy (DP). Crucially, continual learning models must retain knowledge across tasks, but this conflicts with the differential privacy requirement of restricting individual samples to be memorised in the model. We propose using pre-trained models to address the trade-offs between privacy and performance in a continual learning setting. More specifically, we present necessary assumptions to enable privacy-preservation and propose combining pre-trained models with parameter-free classifiers and parameter-efficient adapters that are learned under differential privacy. Our experiments demonstrate their effectiveness and provide insights into balancing the competing demands of continual learning and privacy.","Continual learning (CL, [9, 53]) develops models that learn from a stream of tasks while retaining previous knowledge, a key requirement for real-world applications where data arrives sequentially. However, CL faces the challenge of catastrophic forgetting, where the model loses performance on earlier tasks as it learns new ones [16]. While CL fights for memorising prototypical aspects of the data, the paradigm known as differential privacy (DP, [13]) aims at not memorising any individual‚Äôs data in the first place. DP offers a framework to ensure that the inclusion or exclusion of a single data point does not significantly impact the outcome of the learning process while providing provable privacy guarantees. In turn, DP gives means to enable machine learning (ML) models to, e.g., comply with privacy regulations (e.g. GDPR) to ensure personal data is untraceable and mitigating model inversion attacks [19]. While DP is crucial for privacy-preserving ML, it introduces a trade-off: stronger privacy often degrades model accuracy [41]. Combining CL and DP presents unique challenges to satisfy the demands of mitigating catastrophic forgetting without violating privacy. Prior works have focused on learning how to generate DP synthetic samples for training the classifier and retaining previous knowledge to circumvent the need to store real data [7, 15]. Lai et al. [28] focus on the privacy loss accumulation when learning many tasks sequentially with an episodic memory [31]. However, these methods either require using DP learning of additional networks [15, 28] or on the synthetic samples [7] to enable privacy-preservation in the classifier. Recently, using pre-trained models has been studied separately in DP [27, 8, 49] and CL [23, 54, 55], but remains to be explored whether this can ease the balance between privacy and performance over time. In this work, we explore using pre-trained models that can learn new tasks continually under DP constraints. We present necessary assumptions to obtain a privacy-preserving CL model, and experiment with two common approaches in CL with pre-trained models: (i) parameter-free classifiers [23], and (ii) parameter-efficient adapters [17]. We demonstrate the effectiveness of these methods and provide insights into how to balance privacy-utility trade-offs in CL under DP."
https://arxiv.org/html/2411.04594v1,"Verification of Neural Networks against Convolutional Perturbations
via Parameterised Kernels","We develop a method for the efficient verification of neural networks against convolutional perturbations such as blurring or sharpening. To define input perturbations we use well-known camera shake, box blur and sharpen kernels. We demonstrate that these kernels can be linearly parameterised in a way that allows for a variation of the perturbation strength while preserving desired kernel properties. To facilitate their use in neural network verification, we develop an efficient way of convolving a given input with these parameterised kernels. The result of this convolution can be used to encode the perturbation in a verification setting by prepending a linear layer to a given network. This leads to tight bounds and a high effectiveness in the resulting verification step. We add further precision by employing input splitting as a branch and bound strategy. We demonstrate that we are able to verify robustness on a number of standard benchmarks where the baseline is unable to provide any safety certificates. To the best of our knowledge, this is the first solution for verifying robustness against specific convolutional perturbations such as camera shake.","As neural networks are increasingly deployed in a range of safety-critical domains such as autonomous vehicles, aviation, or robotics, concerns about their reliability are rising. Networks have been shown to be vulnerable to Adversarial Attacks, perturbations that are often imperceptible but change the output of the network on a given instance (Szegedy et al. 2014; Madry et al. 2017). Such adversarial examples have been shown to also exist in the physical world and pose a threat to algorithms deployed in practical applications (Eykholt et al. 2018; Tu et al. 2020). Neural Network Verification has been put forward as a way to address these issues by formally proving that for a given input, a network is robust to a set of specified perturbations, often referred to as local robustness (Katz et al. 2017; Gehr et al. 2018; Singh et al. 2018a). Algorithms are usually divided into complete and incomplete approaches. Given enough time, complete methods are guaranteed to provide a definitive answer to the verification problem. Meanwhile, incomplete methods may not be able to answer the verification problem, returning an undecided result. Complete approaches often employ an exact encoding of the network at hand. They rely on techniques such as Mixed Integer Linear Programming (MILP) (Tjeng, Xiao, and Tedrake 2019; Anderson et al. 2020; Bunel et al. 2020) or Satisfiability Modulo Theories (SMT) (Pulina and Tacchella 2012; Katz et al. 2017). Incomplete verifiers on the other hand employ Semidefinite Programming (Raghunathan, Steinhardt, and Liang 2018; Dathathri et al. 2020; Fazlyab, Morari, and Pappas 2020) or bound propagation (Wang et al. 2018a, b; Singh et al. 2019b; Xu et al. 2021; Wang et al. 2021). They usually overapproximate the true behaviour of the neural network and can be made complete by combining them with a Branch and Bound (BaB) strategy. Stronger verifiers either employ tighter relaxations such as SDP-based ones or linear constraints that reason over multiple neurons simultaneously (Singh et al. 2019a; M√ºller et al. 2022; Ferrari et al. 2022; Zhang et al. 2022) State-of-the-art (SoA) verifiers achieve low runtimes through exploiting GPU-enabled parallelism (Brix et al. 2023b, a). Early works usually verified robustness against norm-based perturbations, often referred to as white noise, which covers a limited number of scenarios and rarely appears in the real world (Pulina and Tacchella 2010, 2012; Singh et al. 2018a; Katz et al. 2019). A number of other perturbations were later proposed. Photometric perturbations such as brightness, contrast, hue or saturation changes as well as more expressive bias field perturbations can be encoded by prepending suitable layers to a neural network (Kouvaros and Lomuscio 2018; Henriksen et al. 2021; Mohapatra et al. 2020). Verifiers can equally be extended to handle more complex geometric perturbations such as rotations, translations, shearing or scaling, although the efficient verification against such perturbations requires further modifications and extensions (Singh et al. 2019b; Balunovic et al. 2019; Mohapatra et al. 2020; Kouvaros and Lomuscio 2018). Other works focus on the efficient verification of robustness to occlusions (Mohapatra et al. 2020; Guo et al. 2023) or semantically rich perturbations in the latent space of generative models (Mirman et al. 2021; Hanspal and Lomuscio 2023). More relevant to this work are previous investigations of camera shake effects. Guo et al. (2020) examine the performance of networks in the presence of motion blur and find that it is highly problematic, significantly degrading the performance of the models. The phenomenon can be modeled by applying a convolution operation using suitable kernels to a given input image (Sun et al. 2015; Mei et al. 2019). Using different kernels, convolution operations can similarly be used to implement other image transformations which include box blur (Shapiro and Stockman 2001, pp.153‚Äì154) and sharpen (Arvo 1991, pp.50‚Äì56). Since many semantically interesting and realistic perturbations can be modelled using convolution, being able to verify robustness to perturbations in a kernel space is highly valuable. Some attempts at verifying the robustness of models to such perturbations have been made before. Paterson et al. (2021) encode contrast, haze and blur perturbations but only perform verification for haze while resorting to empirical testing for contrast and blur. One previous work presents a general method which, if successful, certifies robustness of a network to all possible perturbations represented by a kernel of the given size (Mziou-Sallami and Adjed 2022). However, this generality comes at a cost. It leads to loose bounds and a high dimensionality of the perturbation which makes verification difficult, even more so for large networks. The universality also implies that counterexamples which are misclassified by the network may be difficult to interpret. We propose a new method which aims at the tight verification of networks against convolutional perturbations with a semantic meaning. Our key contributions are the following: ‚Ä¢ To enable an efficient symbolic encoding of the perturbations, we show how an arbitrary constant input can efficiently be convolved with a linearly parameterised kernel using standard convolution operations from a machine learning library. ‚Ä¢ We present parameterised kernels for motion blur perturbations with various blurring angles as well as box blur and sharpen. ‚Ä¢ Using standard benchmarks from past editions of the Verification of Neural Networks Competition (Brix et al. 2023b, a) as well as self-trained models, we show experimentally that verification is significantly easier with our method due to the tighter bounds and the low dimensionality of the perturbation. Our ablation study demonstrates that the existing method is unable to verify any properties on the networks we use. At the same time, our method certifies a majority of the properties for small kernel sizes and perturbation strengths while still being able to certify robustness in a number of cases for large kernel sizes and strengths."
https://arxiv.org/html/2411.04558v1,Experimental Secure Multiparty Computation from Quantum Oblivious Transfer with Bit Commitment,"Secure multiparty computation enables collaborative computations across multiple users while preserving individual privacy, which has a wide range of applications in finance, machine learning and healthcare. Secure multiparty computation can be realized using oblivious transfer as a primitive function. In this paper, we present an experimental implementation of a quantum-secure quantum oblivious transfer (QOT) protocol using an adapted quantum key distribution system combined with a bit commitment scheme, surpassing previous approaches only secure in the noisy storage model. We demonstrate the first practical application of the QOT protocol by solving the private set intersection, a prime example of secure multiparty computation, where two parties aim to find common elements in their datasets without revealing any other information. In our experiments, two banks can identify common suspicious accounts without disclosing any other data. This not only proves the experimental functionality of QOT, but also showcases its real-world commercial applications.","Secure multiparty computation (MPC) in cryptography [1, 2, 3] enables multiple parties to jointly compute a specific function on their respective data without revealing any additional information about their private data. MPC plays a crucial role in fintech. For example, in fraud detection and anti-money laundering, financial institutions can use MPC to jointly compute the intersection of data sets, such as blacklists and suspicious transactions, while keeping each party‚Äôs private information confidential. Beyond fintech, MPC also finds applications in privacy-preserving machine learning [4], where organizations collaborate on training models using sensitive data without exposing individual datasets. Similarly, in the secure computation of genetic data [5], multiple parties can perform analyses on shared genetic datasets while maintaining the privacy of participants‚Äô personal information. These use cases demonstrate MPC‚Äôs versatility in facilitating secure, privacy-conscious computations across various industries. Secure multiparty computation can be realized using oblivious transfer (OT) as an underlying protocol [6]. OT allows a sender to transmits data such that the receiver learns only the selected piece while the sender remains unaware of the choice. 1-out-of-2 OT, the simplest form, is illustrated in Fig. 1. The security of a classical OT protocol is based on the conjectured computational hardness assumptions [7, 8], such as the RSA problem and discrete logarithm, which are vulnerable to quantum attacks using Shor‚Äôs algorithm. Inspired by quantum cryptography, especially quantum key distribution (QKD), it is natural to explore a quantum analog to oblivious transfer, which we refer to as quantum OT (QOT) [9]. Figure 1: 1-out-of-2 Oblivious Transfer. Alice has two possible messages, m0subscriptùëö0m_{0}italic_m start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and m1subscriptùëö1m_{1}italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, to send to Bob. Bob chooses a bit c‚àà{0,1}ùëê01c\in\{0,1\}italic_c ‚àà { 0 , 1 }, and at the end of the process, Bob receives the message mcsubscriptùëöùëêm_{c}italic_m start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, without Alice learning the value of cùëêcitalic_c. While unconditional security is unattainable with quantum alone [10, 11], QOT protocols generally depend on weaker cryptographic assumptions for their security proofs compared to their classical counterparts (see Fig. 5). After QOT protocol [12] was proposed, the QOT protocol in the noisy storage model was extensively studied [13, 14, 15] and its experimental implementation was achieved in [16]. However, this protocol becomes vulnerable when large and reliable quantum memory becomes available [17]. Rather than restricting the adversary‚Äôs technological capability, QOT can rely on the existence of cryptographic functionality primitives such as bit commitment [18] to achieve security against quantum attacks. In addition, although there were several experimental demonstrations of QOT [16, 19, 20, 21], to our best knowledge, no implementations of secure multiparty computation based on QOT for addressing real-world problems have been demonstrated. In this paper, we experimentally implement a QOT protocol using a bit commitment scheme to ensure its theoretical security against quantum attacks. This experimental implementation is adapted based on decoy-state QKD protocol. We then leverage our QOT protocol for multiparty secure computations. We demonstrate its application to the private set intersection (PSI) [22, 23] problem, allowing two banks to securely determine the intersection of the blacklist of one bank and the new client list of the other, without revealing any additional information."
https://arxiv.org/html/2411.04482v1,Anonymous Public-Key Quantum Money and Quantum Voting,"Quantum information allows us to build quantum money schemes, where a bank can issue banknotes in the form of authenticatable quantum states that cannot be cloned or counterfeited: a user in possession of kùëòkitalic_k banknotes cannot produce k+1ùëò1k+1italic_k + 1 banknotes. Similar to paper banknotes, in existing quantum money schemes, a banknote consists of an unclonable quantum state and a classical serial number, signed by bank. Thus, they lack one of the most fundamental properties cryptographers look for in a currency scheme: privacy. In this work, we first further develop the formal definitions of privacy for quantum money schemes. Then, we construct the first public-key quantum money schemes that satisfy these security notions. Namely,Assuming existence of indistinguishability obfuscation and hardness of Learning with Errors, we construct a public-key quantum money scheme with anonymity against users and traceability by authorities.Since it is a policy choice whether authorities should be able to track banknotes or not, we also construct an untraceable money scheme, where no one (not even the authorities) can track banknotes.Assuming existence of indistinguishability obfuscation and hardness of Learning with Errors, we construct a public-key quantum money scheme with untraceability.Further, we show that the no-cloning principle, a result of quantum mechanics, allows us to construct schemes, with security guarantees that are classically impossible, for a seemingly unrelated application: voting!Assuming existence of indistinguishability obfuscation and hardness of Learning with Errors, we construct a universally verifiable quantum voting scheme with classical votes.Finally, as a technical tool, we introduce the notion of publicly rerandomizable encryption with strong correctness, where no adversary is able to produce a malicious ciphertext and a malicious random tape such that the ciphertext before and after rerandomization (with the malicious tape) decrypts to different values! We believe this might be of independent interest.Assuming the (quantum) hardness of Learning with Errors, we construct a (post-quantum) classical publicly rerandomizable encryption scheme with strong correctness.","The exotic nature of quantum mechanics allows us to build cryptographic primitives that were once unimaginable, or are outright impossible with classical information alone. For example, one of the most fundamental results of quantum mechanics, called the no-cloning principle, shows that arbitrary unknown quantum states cannot be cloned. This simple principle, which provably has no counterpart in classical world since classical information can always be copied, allowed cryptographers to build applications that are impossible in a classical world. Starting with the seminal work of Wiesner [Wie83] which introduced quantum money, a plethora of work built exciting primitives based on the no-cloning principle. The examples include more realistic version of quantum money called public-key quantum money [AC12, Zha19], where any user can verify a banknote on their own without going to the central bank, or even more advanced notions such as copy-protecting software/functionalities where a user that is given some number of copies of a software cannot create more copies of it111Again, a classical software can always be copied, so this is impossible classically. [Aar09, CLLZ21, √áG23]. While quantum money is one of the most important notions in quantum cryptography, unfortunately existing schemes [AC12, Zha19] lack some of the most basic privacy and security guarantees that cryptographers look for in a currency scheme. In fact, in all known public-key quantum money schemes, a banknote consists of an unclonable quantum state and a classical serial number which is signed by the bank. However, this means that any party can track any banknote and learn when and where it was used simply by recording its serial number, meaning there is no privacy at all. For example, imagine a scenario where you pay a large sum of money to a merchant. If the merchant pools the data with other (adversarial) sources, it maybe feasible222In fact, even innocent amateur efforts that were built for fun, such as wheresgeorge.com, have been able to track millions of paper dollar banknotes all over the world. for them to recover a history on many of these banknotes, potentially revealing your employer, clients and business partners, and even family members! Similarly an employer who pays you salary can potentially track where you travel to by collaborating with other sources and tracking, say, your spending at gas stations or restaurants. Indeed it is hard to imagine privacy in any aspect of your life, if all your spending can be traced. In fact, privacy and anonymity is the central focus in many cryptocurrency projects [mon, Zca]. This state of affairs leaves open the following natural question: Is it possible to construct a publicly verifiable quantum money scheme with privacy guarantees? Let us emphasize that the above question is highly non-trivial for the following reason. To satisfy privacy guarantees, one needs to build a quantum money scheme where the users can create a new banknote so that the adversarial parties who have seen the banknote before will not be able to recognize it. However, such a task, while still challenging but doable for classical information (e.g. rerandomizable signatures [CL04]), seems to be at odds with the main point of quantum money: unclonability! While any user should be enabled to create new banknotes, somehow we also need to make sure that they cannot create k+1ùëò1k+1italic_k + 1 valid banknotes if they started with kùëòkitalic_k banknotes because otherwise they can increase the amount of money they have at wish! Going beyond privacy concerns, one useful property of the existing quantum money schemes is that since any party can track a banknote, in particular law enforcement can also track a banknote. This brings us to our next natural question? Is it possible to construct a publicly verifiable quantum money scheme with privacy guarantees against users, while still providing traceability for authorities? We note that it is a political choice whether authorities should be able to track banknotes or not. Therefore, we also ask Is it possible to construct a publicly verifiable quantum money scheme with privacy guarantees against everyone, including the bank/authorities? Finally, we observe an interesting connection between quantum money with privacy (which is impossible classically) and voting. In both cases, we care about privacy and a security notion relating to non-increasibility: in quantum money, users should not be able to increase their amount of money (in particular, given a single banknote, one should not be able to create two banknotes), and in voting, a single user should be able to vote once. Thus, we ask the following question: Using quantum information, is it possible to construct voting schemes with advanced security guarantees that are not possible classically? 1.1 Our Results In this work, we answer all of these open questions affirmatively. We construct the first public-key quantum money scheme with anonymity (against users) and traceability (by the authorities). For anonymity, we require that a malicious user will not be able distinguish a banknote it has seen before from a freshly minted banknote. For tracing, we require that no malicious user can produce a banknote with a particular tag (which is hidden inside the serial numbers, except to authorities) without being given a banknote with that tag to begin with - meaning that law enforcement can perfectly track banknotes. Theorem 1 (Informal). Assuming the existence of indistinguishability obfuscation (i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O) and a publicly rerandomizable encryption scheme with strong correctness and publicly testable ciphertexts, there exists a public-key quantum money scheme with anonymity and traceability. In fact, in our model, we separate authorities into two completely independent entities: the bank (who mints the banknotes) and the tracing authority. Our scheme satisfies anonymity even against the bank, and satisfies unclonability (also called counterfeiting security) even against the tracing authority. We note that previously, anonymous quantum money had been constructed only333Strictly speaking, [BS21] calls their model almost-public. They simply use an existing private-key quantum money scheme with pure states (more precisely, pseudorandom quantum states [JLS18]), and an alleged banknote is compared to user‚Äôs existing banknotes to verify it. This requires that the user always has more money than she can receive. From our point of view, this is not a public-key scheme. in the private key setting [MS10, BS21, AMR20]. However, aside from the impracticality of the private key setting, the anonymity notion in the private-key setting is also less meaningful: Once we are at the central bank to verify a banknote, we might as well ask them to replace our banknote with a fresh one. The previous solutions are based on Haar random states or their computational version, pseudorandom states; however, in the private-key setting, a trivial solution based on quantum fully homomorphic encryption also exists, where we can just encrypt banknotes and use the homomorphic encryption scheme to rerandomize them. Going further, we construct the first public-key quantum money scheme with anonymity against all parties (including the bank and the authorities). We call this notion untraceability. Theorem 2 (Informal). Assuming the existence of i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O, a publicly rerandomizable encryption scheme with strong correctness and publicly testable ciphertexts and a non-interactive zero-knowledge (NIZK) argument system for N‚Å¢PùëÅùëÉNPitalic_N italic_P, there exists a public-key quantum money scheme with untraceability in the common random string model. Finally, we construct the first voting scheme with universal verifiability (anyone in the world can verify any vote), privacy against all parties (including voting authority!) and uniqueness (i.e. no double-voting). We note that a voting scheme satisfying these three properties at the same time provably cannot exist in a classical voting scheme. Thus, the voting tokens of our scheme are quantum, but a cast vote is classical. Theorem 3 (Informal). Assuming the existence of i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O, a publicly rerandomizable encryption scheme with strong correctness and publicly testable ciphertexts and a non-interactive zero-knowledge (NIZK) argument system for N‚Å¢PùëÅùëÉNPitalic_N italic_P, there exists a quantum voting scheme with universal verifiability and classical votes, in the common random string model. As discussed, such a scheme cannot exist classically. Even using quantum voting tokens, ours is the first to achieve these guarantees: there is no previous work achieving universal verifiability (or classical votes). We note that while i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O is a strong assumption, all of our constructions above imply standard public-key quantum money, whose all existing constructions in the plain model also use i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O444We note that there are some candidate constructions based on non-standard ad-hoc assumptions. In fact, it is one of the key open questions in quantum cryptography to construct public-key quantum money without i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O. Thus, unless a major breakthrough is achieved, our i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O assumption is necessary. To achieve our results, we also introduce the notion of publicly rerandomizable encryption with strong correctness, where no adversary is able to produce a (malicious) ciphertext whose decryption result differs between before and after rerandomization (even with a maliciously chosen rerandomization randomness tape); and we construct such a scheme that is secure against quantum adversaries. Our schemes also satisfy public testability, where anyone can test whether a ciphertext is bad (in the above sense, where rerandomization can lead to decrypting to a different message), such that there does not exist a ciphertext that passes this test but decrypts to different values before/after rerandomization. We note that this notion is useful in constructions/proofs that use indistinguishability obfuscation, where merely computational hardness of finding bad ciphertexts would not be sufficient. Our results/constructions here are classical and we believe they might be of independent interest. Theorem 4 (informal). Assuming hardness of Learning with Errors (LWE) [Reg09], there exits a publicly rerandomizable encryption with publicly testable ciphertexts and strong correctness. Thus, instantiating our constructions with our publicly rerandomizable encryption scheme with strong correctness, and the NIZK argument system with the LWE-based (post-quantum) construction of Peikert-Shiehian [PS19] (which is in the common random string model), all of our quantum money and quantum voting schemes can be based on i‚Å¢ùí™ùëñùí™i\mathcal{O}italic_i caligraphic_O and LWE."
https://arxiv.org/html/2411.04376v1,Game-Theoretic Defenses for Robust Conformal Prediction Against Adversarial Attacks in Medical Imaging,"Adversarial attacks pose significant threats to the reliability and safety of deep learning models, especially in critical domains such as medical imaging. This paper introduces a novel framework that integrates conformal prediction with game-theoretic defensive strategies to enhance model robustness against both known and unknown adversarial perturbations. We address three primary research questions: constructing valid and efficient conformal prediction sets under known attacks (RQ1), ensuring coverage under unknown attacks through conservative thresholding (RQ2), and determining optimal defensive strategies within a zero-sum game framework (RQ3). Our methodology involves training specialized defensive models against specific attack types and employing maximum and minimum classifiers to aggregate defenses effectively. Extensive experiments conducted on the MedMNIST datasets‚ÄîPathMNIST, OrganAMNIST, and TissueMNIST‚Äîdemonstrate that our approach maintains high coverage guarantees while minimizing prediction set sizes. The game-theoretic analysis reveals that the optimal defensive strategy often converges to a singular robust model, outperforming uniform and simple strategies across all evaluated datasets. This work advances the state-of-the-art in uncertainty quantification and adversarial robustness, providing a reliable mechanism for deploying deep learning models in adversarial environments.","Adversarial attacks [1] significantly undermine the reliability and safety of deep learning models, particularly in high-stakes domains such as medical diagnostics [2] and autonomous driving [3]. These attacks introduce subtle perturbations to input data, causing models to produce incorrect and potentially harmful predictions. In critical applications like healthcare and autonomous systems, ensuring both adversarial robustness and reliable uncertainty quantification is paramount. Conformal Prediction (CP) [4, 5, 6] offers a robust framework for uncertainty quantification by generating prediction sets that encompass the true label with a predefined confidence level [7]. The interplay between adversarial robustness and conformal prediction [8, 9] is both crucial and underexplored. While extensive research has focused on enhancing adversarial robustness through various adversarial training methodologies [10, 11, 12, 13], the impact of these methods on the efficiency and reliability of conformal prediction sets remains inadequately understood. Specifically, prior studies have concentrated on adapting the scoring functions to attacks on either the test data or the calibration data, or both. However, these approaches often encounter validity issues, thereby compromising the reliability of CP sets under adversarial conditions. This paper aims to bridge this knowledge gap by constructing robust conformal prediction sets for medical imaging datasets under diverse adversarial attack scenarios. We employ various attack types and partition the training data to develop specialized models resistant to each specific attack. Our objective is to formulate CP sets that uphold coverage guarantees while minimizing and optimizing prediction set sizes, even when subjected to both known and novel adversarial attacks. To address the validity issues inherent in previous methodologies, we construct conservative prediction sets based on the largest estimated quantile derived from calibration data under different attacks. This approach ensures that the prediction sets maintain their validity guarantees despite adversarial perturbations. Central to our investigation is a pivotal question: given that validity is preserved, what constitutes the severest attack an adversary can orchestrate when they can choose any attack without the defender‚Äôs knowledge? Furthermore, how can the defender effectively mitigate such attacks under this adversarial scenario? Addressing these questions leads naturally to a zero-sum game formulation, where we consider both maximum and minimum classifiers to strategize optimal defenses against the most potent attacks. By integrating game-theoretic principles with conformal prediction, this study endeavors to enhance the robustness and reliability of deep learning models in adversarial environments. Our approach not only maintains high coverage rates but also ensures minimal prediction set ambiguity, advancing the safety and efficacy of AI deployments in critical applications such as healthcare."
https://arxiv.org/html/2411.04365v1,"Towards Secured Smart Grid 2.0: Exploring Security Threats, Protection Models, and Challenges","Many nations are promoting the green transition in the energy sector to attain neutral carbon emissions by 2050. \acSG2 is expected to explore data-driven analytics and enhance communication technologies to improve the efficiency and sustainability of distributed renewable energy systems. These features are beyond smart metering and electric surplus distribution in conventional smart grids. Given the high dependence on communication networks to connect distributed microgrids in \acSG2, potential cascading failures of connectivity can cause disruption to data synchronization to the remote control systems. This paper reviews security threats and defense tactics for three stakeholders: power grid operators, communication network providers, and consumers. Through the survey, we found that \acSG2‚Äôs stakeholders are particularly vulnerable to substation attacks/vandalism, malware/ransomware threats, blockchain vulnerabilities and supply chain breakdowns. Furthermore, incorporating artificial intelligence (AI) into autonomous energy management in distributed energy resources of \acSG2 creates new challenges. Accordingly, adversarial samples and false data injection on electricity reading and measurement sensors at power plants can fool AI-powered control functions and cause messy error-checking operations in energy storage, wrong energy estimation in electric vehicle charging, and even fraudulent transactions in peer-to-peer energy trading models. Scalable blockchain-based models, physical unclonable function, interoperable security protocols, and trustworthy AI models designed for managing distributed microgrids in \acSG2 are typical promising protection models for future research.","Integrating distributed energy systems is a major topic in the green and renewable energy era with sustainable goals of usage efficiency, autonomous intelligence, and resilience capability against sudden failures. These new energy integration capabilities expects to be the core of Smart Grid 2.0 (SG2) [1, 2]. \acSG2 aims to enhance energy distribution and usage efficiency with the help of communication technologies [3, 4, 5]. These interdependent power grid and communication networks can help connect distributed electricity, gas, and cooling systems, offering unprecedented opportunities for remote control capability and flexibility [6]. However, with reliance on digital communication technologies, as shown in Fig. 1, SG2 faces significant threats, targeting connectivity among power grid providers, two-way communication network systems, and consumer entities (industrial, residential, and commercial users). For example, ransomware attacks have recently been recorded to cause prominent blackouts in many countries [7]. Figure 1: The illustration shows the crucial role of communication technologies in synchronizing measurement data from substations, enabling remote control capabilities for efficient power distribution. However, the dependency of power grids on communication technology creates fresh threats of security attacks to energy security, e.g., ransomware to disable control systems and denial of services against transmission lines to stop data exchange. Recent drone attacks and supply chain risks also threaten critical facilities (economic loss) and even endanger public safety (e.g., power outages in the cooling systems of nuclear plants). Also, providing sophisticated communication networks to millions of charging stations and diverse renewable energy sources in SG2, while not overburdening the distribution network or destabilizing the grid, is also a challenge. Understanding the threats and challenges is the critical step toward developing robust defense approaches for guaranteeing energy security, and further national safety. This article aims to explore the various security threats in \acSG2, particularly its communication infrastructure and \acSG2 enabling technologies, such as peer-to-peer energy trading and AI-powered grid network functions. In addition, the study discusses emerging strategies for safeguarding \acSG2 from cascading failures and for developing effective distribution grid restoration plans in disaster scenarios and severe security attacks in the future. I-A State-of-the-art literature review Exploring security threats for smart grids has been a hot topic for years but few studies address \acSG2‚Äôs security matters in a comprehensive manner. Fig. 2 presents \acSG2‚Äôs essential components and security concerns, together with state-of-the-art relevant studies. Accordingly, most articles focus on security threats in the conventional smart grid that features electricity sources and distribution. For example, the authors in [8] provide a comprehensive survey of typical security attacks and vulnerabilities of authentication and security protocols in the conventional energy model. AI and blockchain for conventional smart grids and related security is briefed in [9, 10, 11]. However, the paper covers few aspects of security protection or \acAI role in specific functions in each layer or from related stakeholders (power generator, communication provider, and consumers). Similarly, the surveys in [12, 13, 14] cover a narrow scope of security in specific smart grid networks, e.g., the connection between home and grid supply, metering data collection and transmission [15]. Recently, the survey papers focus on the security threats in communications among power generation and distribution components of the newer smart grid model with battery storage systems [16, 17, 18, 19]. As technology and infrastructure continue to advance, renewable energy (e.g., wind, solar, geothermal, hydropower) plays an increasingly vital role in the global energy transition toward a cleaner and more sustainable future. Energy storage systems are required to maintain the stability of such distributed sources. Several surveys on the safety of smart inverters [20], battery storage/swap [21], or control systems [22] against remote attacks or physical tampering is also presented. On the other hand, the authors in [23, 24, 25] provide a holistic view of control and communication strategies in multi-energy generation grids or robust models against cascading failure in interdependent power-communication networks. However, the studies did not address the security threats or specific attacks for each entity (power provider, communication network provider, consumer). Unlike prior studies, this work aims to investigate weaknesses in the interdependence of microgrids that heavily rely on distributed energy sources and communication technologies. Additionally, there will be a focus on identifying new risks associated with AI-powered energy control and novel energy trading/storage models. This research will be particularly important as many new small energy sources (e.g., from solar roofs) are integrated into management networks. Overall, the first goal of our work is to provide a comprehensive view of cybersecurity in these new elements, referring to \acSG2‚Äôs energy security principles, which have received little attention in the existing literature. Figure 2: This work addresses energy security principles for SG2 from a view of interdependent power grid communication networks, notably with the introduction of new technologies for three entities (power provider, communication network provider, consumer), such as energy storage, 5G/6G, AI-powered functions, and peer-to-peer energy trading models. Besides, many governments considered grid security as a national security matter and proposed measures to improve information security protection [26, 27, 28, 29] and resilience strategies in disaster and crisis scenarios [23, 30, 31]. Grid security refers here to the consistent and reliable availability of all fuels and electricity sources in a timely, sustainable, and cost-effective manner. For example, several standards for information security in smart grids, such as the framework developed by the \acNIST, have been developed. The US Department of Energy is carrying out the Cybersecurity for Energy Delivery Systems (CEDS) program [32], aimed to enhance the security and resilience of the country‚Äôs energy infrastructure. The \acNERC has established network security standards for the power industry in North America. The \acENISA has issued guidelines for safeguarding EU‚Äôs smart grids [29]. The \acIEC has established standards for network security in power systems (IEC 62443, 62351 standards) [27]. Therefore, this survey‚Äôs second goal is to determine which features have not yet been defined in the security standards and what standards the nationals apply for their energy management systems based on facility availability, deployment cost, and environment compatibility. Further, understanding energy restoration plans for potentially cascading failures of communication technologies is critical to consult a proper model for deploying \acSG2. I-B Review methodology Given the difficulties of installing from scratch owing to high costs, \acSG2 will likely inherit many control components, existing facilities, and communication infrastructure from the current smart grid. Inspired by this fact, we present possible security concerns in \acSG2, as viewed through the mirrors of two aspects‚Äô lessons learned. The first phase involves examining energy security principles, identifying the primary risks to the components of a smart grid system, and assessing the security vulnerabilities in legacy technologies with examples of well-known energy crises and blackout events. Additionally, this involves energy restoration strategies in the event of probable cascade failures resulting from security attacks on communication lines. The rest is to figure out security flaws in emerging technologies and new decentralized energy models that are expected to be the main vehicles towards \acSG2, such as AI-powered energy control functions, battery storage technologies, and the integration of advanced communication technologies for charging stations and distributed renewable energy sources. Security threats are often the motivating factor behind the need to change countermeasure approaches. These attacks typically reveal system faults or protocol issues that were not expected during the design process. Analyzing and learning from attacks like this gives significant insights into the essential security changes for \acSG2, particularly in addressing known weaknesses exploited in prior generations. Finally, this work also addresses security matters from a top-down approach where security threats against the interdependent relationship of stakeholders (power grid operator, communication network provider, consumer) will be assessed and suggested with corresponding defense strategies. The unsolved problems become possible targets for \acSG2 improvements, which serve as the foundation for proposing future solutions. Figure 3: The following is a summary of the major findings from our survey on security and protection strategies for Smart Grid 2.0. The decorative colors for technologies match those for the three entities (power provider, communication network provider, and customer) illustrated in the previous figures. I-C Contributions Given the slow transition from legacy to new technologies, it is difficult to predict when \acSG2 will be in full operation. However, by drawing a line of relative differences between the current smart grid platform and the expected \acSG2 architecture, this research can help the developers and researchers determine the security weaknesses and find the right starting point of \acSG2‚Äôs technologies to improve. The primary contributions in this work are summarized as follows. 1. The first attempt to thoroughly investigate the principles of SG2 security for national safety, taking into account the whole perspective of security measures for communication links among power grid operators, communication network providers, and consumers. The study examines the relationship between power grids and communication networks in terms of cascading failures. It identifies potential solutions and necessary improvements for \acSG2, specifically in energy restoration planning and communication isolation. 2. The first attempt to offer a comprehensive perspective on the security risks of \acSG2 enabling technologies that need to be adapted to meet the evolving requirements of \acSG2, e.g., blockchain-based energy management/trading, AI-aided grid operations, the networks of electrified transportation systems (EV charging stations) and distributed renewable energy sources. Given that \acSG2 follows economic trajectory of the technology evolution, a systematic review of the transition process and potential changes in supply chain management and new communication methods can guide power grid operators and network providers in effectively upgrading their security infrastructure and countermeasure techniques in the future. 3. This study summarized lessons learned from the limitations of current protection implementations in SG1 and the vulnerabilities of SG2 emerging technologies that can aid researchers and developers in determining the problem formulation for further studies. To the best of our knowledge, this survey represents the initial endeavor to comprehensively assess security threat aspects for \acSG2, spanning from vulnerabilities in distributed renewable energy sources to EV charging network architecture, and then AI-powered grid management. I-D Structure of the paper The rest of this paper is organized as follows. Section II briefs the fundamental information about \acSG2 architecture, energy security principles, and overall strategies to protect the power grid-communication networks‚Äô infrastructure. The security attacks and defense approaches for power providers, communication network providers, and consumer stakeholders are then detailed in Section III, IV, V, respectively. Section VI outlines security risks and some countermeasure techniques in emerging technologies and their role in securing \acSG2 is detailed in Section VII. Section VIII discusses lessons learned and future research. Section IX concludes this paper. Fig. 3 summarizes the main points of our survey. The acronyms used in this work are listed as follows. \printacronyms [sort=true]"
https://arxiv.org/html/2411.04234v1,On the Power of Oblivious State Preparation,"We put forth Oblivious State Preparation (OSP) as a cryptographic primitive that unifies techniques developed in the context of a quantum server interacting with a classical client. OSP allows a classical polynomial-time sender to input a choice of one out of two public observables, and a quantum polynomial-time receiver to recover an eigenstate of the corresponding observable ‚Äì while keeping the sender‚Äôs choice hidden from any malicious receiver.We obtain the following results:The existence of (plain) trapdoor claw-free functions implies OSP, and the existence of dual-mode trapdoor claw-free functions implies round-optimal (two-round) OSP.OSP implies the existence of proofs of quantumness, test of a qubit, blind classical delegation of quantum computation, and classical verification of quantum computation.Two-round OSP implies quantum money with classical communication, classically-verifiable position verification, and (additionally assuming classical FHE with log-depth decryption) quantum FHE.Thus, the OSP abstraction helps separate the cryptographic layer from the information-theoretic layer when building cryptosystems across classical and quantum participants. Indeed, several of the aforementioned applications were previously only known via tailored LWE-based constructions, whereas our OSP-based constructions yield new results from a wider variety of assumptions, including hard problems on cryptographic group actions.Finally, towards understanding the minimal hardness assumptions required to realize OSP, we prove the following:OSP implies oblivious transfer between one classical and one quantum party.Two-round OSP implies public-key encryption with classical keys and ciphertexts.In particular, these results help to ‚Äùexplain‚Äù the use of public-key cryptography in the known approaches to establishing a ‚Äùclassical leash‚Äù on a quantum server. For example, combined with a result of Austrin et al. (CRYPTO 22), we conclude that perfectly-correct OSP cannot exist unconditionally in the (quantum) random oracle model.","One of the central concepts driving research in quantum cryptography over the past decade has been that of a ‚Äúclassical leash‚Äù on quantum systems [RUV13]. In other words, how can we enable a classical device, using just classical communication, to exert some element of control over a quantum mechanical system? In a major conceptual advance from 2018 [BCM+18], the use of (public-key) cryptography was identified as a useful tool for establishing this desired control over a quantum server. There has since been an explosion of results on classical-client quantum-server protocols, ranging from proofs of quantumness under quantum-hard assumptions [BCM+18], certifiable randomness generation [BCM+18], quantum homomorphic encryption with classical ciphertexts [Mah18a], classical verification of quantum computation [Mah18b], self-testing a single quantum device [MV21], position verification [LLQ22], secure quantum computation [Bar21], and quantum money [RS19, Shm22] with classical communication, and proofs of contextuality [ABCC24], among others. The resounding success of this line of work begs a deeper understanding of the basic principles underlying the paradigm introduced in [BCM+18]. For example, [BCM+18] based their results on the existence of a fairly ad-hoc and unwieldy cryptographic primitive: a noisy trapdoor claw-free function (TCF) with an adaptive hardcore bit. This primitive has only been shown to exist from the learning with errors (LWE) assumption,111[AMR22] has shown how to obtain a weaker variant of the adaptive hardcore bit property from hard problems on cryptographic group actions. and several followups, including many of the aforementioned results, inherited the use of this primitive. This raises the following (informal) question. Is there a conceptually-simple and easy-to-instantiate primitive that suffices for building powerful classical-client quantum-server applications? We note that some partial progress has been made towards a more ‚Äúgeneric‚Äù approach to constructing some of these end applications. For example, a recent line of work [KMCVY21, KLVY23, NZ23] has yielded classical verification of quantum computation from the assumption of quantum fully-homomorphic encryption (QFHE), and [GV24] has shown that QFHE follows from any classical FHE (with decryption in NC1subscriptNC1\text{NC}_{1}NC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) plus an appropriate notion of ‚Äúdual-mode‚Äù trapdoor claw-free functions (with no need for an adaptive hardcore bit). However, as we show, these approaches can still be generalized much further. Another aspect of [BCM+18]‚Äôs approach that demands further investigation is their use of public-key (i.e. trapdoor-based) cryptography. This has been justified informally by observing that, because the classical client is computationally weaker than the quantum server, we need to introduce some mechanism for the client to gain the ‚Äúupper hand‚Äù on the server. One way to do this is to introduce asymmetric cryptography, allowing the client to send the server a public key while keeping the secret key to themselves. However, as far as we are aware, nothing more than this informal intuition has been proposed in an attempt to address the following fundamental (and again, informal) question. Is public-key cryptography necessary to establish a classical leash on a quantum server? As an example meant to further illustrate the importance of this question, we note that progress in this direction may shed more light on the recent breakthrough techniques of Yamakawa-Zhandry [YZ24] for establishing quantum advantage. Indeed, while they show that proofs of quantumness exist in the random oracle model, which can be heuristically instantiated using a cryptographic hash function (i.e. symmetric cryptography), their techniques have so far resisted attempts at constructing, say, a test of a qubit, verifiable delegation of quantum computation, or other ‚Äúclassical leash‚Äù-style primitives mentioned above. Establishing the necessity of public-key cryptography for these primitives would explain this gap. While we do not completely close this question, our results do show that the principles underlying current approaches to classical-leash primitives (inspired by [BCM+18, KMCVY21], etc.) also yield public-key style primitives. We provide further details on these results later in the introduction. 1.1 Oblivious state preparation Aiming to make progress on these two motivating questions, we put forth the idea of Oblivious State Preparation (OSP) as a unifying cryptographic primitive in the realm of classical-client quantum-server protocols. OSP is simple to describe. It is a protocol that takes place between a classical sender and a quantum receiver. The classical sender has as input a bit b‚àà{0,1}ùëè01b\in\{0,1\}italic_b ‚àà { 0 , 1 } which specifies a choice of one of two public observables. We usually take one to be ZùëçZitalic_Z and the other to be XùëãXitalic_X, but in principle they could be arbitrary. At the end of the protocol, the receiver outputs a quantum state. We require two properties. ‚Ä¢ Correctness: If the receiver is honest, their output is an eigenstate of the observable chosen by the sender, and the sender receives a description of this state. In the usual ‚Äústandard‚Äù case, this means that when b=0ùëè0b=0italic_b = 0, the receiver outputs either |0‚ü©ket0\ket{0}| start_ARG 0 end_ARG ‚ü© or |1‚ü©ket1\ket{1}| start_ARG 1 end_ARG ‚ü©, and when b=1ùëè1b=1italic_b = 1, the receiver outputs either |+‚ü©ket\ket{+}| start_ARG + end_ARG ‚ü© or |‚àí‚ü©ket\ket{-}| start_ARG - end_ARG ‚ü©. ‚Ä¢ Security: Any quantum polynomial-time (QPT) malicious receiver has negl‚Å¢(Œª)neglùúÜ{\rm negl}(\lambda)roman_negl ( italic_Œª ) advantage in guessing the sender‚Äôs input bit bùëèbitalic_b. OSP highlights an inherent cryptographic property of quantum information arising from the uncertainty principle. That is, it is not necessarily possible to determine the basis of a given state, even though this basis information is well-defined and fixed by the description of the state. Indeed, given the resource of quantum communication, information-theoretically secure OSP is trivial. In this work, however, OSP always refers to the classical-communication case. At a high level, we ask, (1) what cryptography is necessary to obtain OSP, i.e. the ability for a classical client to, roughly speaking, set up an instance of the uncertainty principle on a quantum server, and (2) what are the applications of this ability. In fact, the idea of OSP for the ZùëçZitalic_Z and XùëãXitalic_X observables as described above has been previously proposed by [CCKW19] under the name ‚Äúmalicious 4-states QFactory with basis-blindness.‚Äù They showed that such a protocol can be obtained from a TCF with a particular type of ‚Äúhomomorphic, hardcore predicate‚Äù. However, in order to derive most applications, they required an additional and conjectured verifiability property (see also [CCKW21]). In this work, we give OSP a more general treatment as a primitive, show that it is possible to relax the assumptions under which it can be built, and vastly expand its set of applications. 1.2 Results 1.2.1 Constructions As mentioned above, researchers beginning with [BCM+18] identified the usefulness of (variants of) trapdoor claw-free functions (TCFs) in realizing applications such as a test of a qubit, quantum fully-homomorphic encryption, and classical verification of quantum computation. While several variants of TCFs have appeared over the years, e.g. extended, dual-mode, with adaptive hardcore bit, etc., we show that perhaps the most stripped down notion of a TCF suffices to build OSP. We define a (plain) TCF as a family of functions fùëìfitalic_f that can be sampled along with a trapdoor ùóçùñΩùóçùñΩ\mathsf{td}sansserif_td. The guarantee, roughly, is that there is a (QPT preparable) distribution ùíüùíü{\cal D}caligraphic_D over inputs such that with some inverse polynomial probability over x‚Üêùíü‚Üêùë•ùíüx\leftarrow{\cal D}italic_x ‚Üê caligraphic_D, xùë•xitalic_x has exactly one sibling x‚Ä≤superscriptùë•‚Ä≤x^{\prime}italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT (similarly weighted by ùíüùíü{\cal D}caligraphic_D) such that f‚Å¢(x)=f‚Å¢(x‚Ä≤)ùëìùë•ùëìsuperscriptùë•‚Ä≤f(x)=f(x^{\prime})italic_f ( italic_x ) = italic_f ( italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ), and moreover, both xùë•xitalic_x and x‚Ä≤superscriptùë•‚Ä≤x^{\prime}italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT can be recovered given f‚Å¢(x)ùëìùë•f(x)italic_f ( italic_x ) and ùóçùñΩùóçùñΩ\mathsf{td}sansserif_td. Finally, claw-freeness demands that no QPT adversary can recover any such ‚Äúclaw‚Äù xùë•xitalic_x and x‚Ä≤superscriptùë•‚Ä≤x^{\prime}italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT given only the description of fùëìfitalic_f. Building on techniques from [BGKM+23], we show the following. Theorem 1.1 (Informal). (Plain) TCFs imply OSP. Our construction of OSP from plain TCFs requires multiple of rounds of interaction. However, a desirable feature for some applications is limited interaction. The best we can hope for is two-round OSP, i.e. one message from the sender followed by one from the receiver.222No one-message OSP can be secure. Indeed, the message from sender to receiver would have to fix the description of the desired state Hb‚Å¢|s‚ü©superscriptùêªùëèketùë†H^{b}\ket{s}italic_H start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT | start_ARG italic_s end_ARG ‚ü©, and correctness would imply that the receiver could then generate multiple copies of this state, eventually enough to determine the basis with near certainty. Adapting techniques from [GV24], we show that TCFs with an additional dual-mode property imply such two-round OSP. Theorem 1.2 (Informal). Dual-mode TCFs imply two-round OSP. Briefly, a dual-mode TCF is similar to a plain TCF except that the function may be sampled in an ‚Äúinjective‚Äù mode, where there are no collisions, and it is computationally difficult to distinguish this from the normal ‚Äúlossy‚Äù mode. We note that dual-mode (and thus plain) TCFs are known from LWE [BCM+18] and from the ‚Äúextended linear hidden shift‚Äù assumption on cryptographic group actions [AMR22, GV24]. In what follows, we will highlight the power of OSP by establishing several classical-client quantum-server applications, as well as cryptographic implications. We will build everything from ‚Äústandard‚Äù OSP, where the two observables are ZùëçZitalic_Z and XùëãXitalic_X. However, it is meaningful to consider OSP for any pair of non-commuting observables, in particular pairs that may not be maximally anti-commuting. We begin to explore the landscape of OSP as a more general primitive, and in particular establish the following result. Theorem 1.3 (Informal). OSP for any pair of two-outcome observables that are at a 1/poly1poly1/{\rm poly}1 / roman_poly angle implies standard (ZùëçZitalic_Z and XùëãXitalic_X) OSP. 1.2.2 Applications From OSP. We show that OSP is sufficient to obtain several classical-client quantum-server protocols of interest. Theorem 1.4 (Informal). OSP implies proofs of quantumness, a test of a qubit, blind classical delegation of quantum computation, and classical verification of quantum computation. A few remarks on these results are in order. The proof of quantumness from OSP can be seen as a modular instantiation of the template proposed by [KMCVY21] and later tweaked by [BGKM+23] and [ABCC24]. In particular, we show that a single instance of OSP suffices to implement a ‚Äúcomputational Bell test‚Äù between the client and server based on the CHSH game. Blind classical delegation of quantum computation allows a classical client to outsource a quantum computation of its choice to a quantum server without leaking anything about the actual description of the computation. In a major breakthrough, [Mah18a] gave the first construction of this primitive by building quantum fully-homomorphic encryption (QFHE) from LWE. This approach of course relies on techniques that at least imply classical fully-homomorphic encryption, and, to the best of our knowledge, fully-homomorphic encryption has remained the only solution to blind classical delegation of quantum computation that has been made explicit in the literature.333[CCKW21] show how to realize blind classical delegation of quantum computation but only against an honest-but-curious server, using their protocol for ‚Äúpseudo-secret random qubit generator.‚Äù While perhaps folklore, we formalize the fact that classical FHE is not required for blind classical delegation of quantum computation, showing that OSP suffices. In another major breakthrough, [Mah18b] designed a protocol that allows a classical client to verifiably delegate a BQP computation to a potentially cheating quantum server. Since then, classical verification of quantum computation (CVQC) has remained a central primitive of study in quantum cryptography [Zha22, BKL+22, CLLW22, NZ23, GKNV24, MNZ24]. However, until now, all known constructions relied on the hardness of LWE. As a corollary of our result, we show that CVQC follows from any (plain) TCF, and thus from hard problems on cryptographic group actions. In a nutshell, we formalize the fact that the recent approach of [KLVY23, NZ23] establishing CVQC from QFHE can in fact be instantiated from any (potentially interactive, non-compact) blind classical delegation of quantum computation protocol, and thus, from any OSP. From two-round OSP. Our next batch of results makes use of two-round OSP. Theorem 1.5 (Informal). Two-round OSP implies (privately-verifiable) quantum money with classical communication, position verification with classical communication, and (assuming classical FHE with decryption in NC1subscriptNC1\text{NC}_{1}NC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) QFHE. Briefly, the first two results go via the intermediate primitive of a ‚Äú1-of-2 puzzle‚Äù [RS19], which we build from OSP using similar techniques to our CHSH-based proof of quantumness. Then, we appeal to [RS19], who showed that 1-of-2 puzzles imply privately-verifiable quantum money with classical communication, and [LLQ22], who showed that 1-of-2 puzzles imply position verification with classical communication. Prior to our work, the only construction of 1-of-2 puzzles, due to [RS19], relied specifically on LWE (via TCFs with the adaptive hardcore bit property).444We note that a recent concurrent and independent work has shown how to construct classically-verifiable position verification from certified randomness protocols [ACC+24]. The third result on QFHE follows by adapting the recent techniques of [GV24], who showed how to construct QFHE from any classical FHE (with decryption in ùñ≠ùñ¢1subscriptùñ≠ùñ¢1\mathsf{NC}_{1}sansserif_NC start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) and dual-mode TCFs. We observe that, in fact, any two-round OSP suffices in place of the dual-mode TCF. Discussion. Before moving on, it is worth pointing out that each of these results individually do not require deep new techniques. Rather, they mostly follow by adapting, modularizing, and generalizing existing approaches in the literature. However, in our view, the identification of OSP as a simple primitive that yields all of these applications is useful, both as a pedagogical tool and to enhance future research. OSP abstracts away the cryptographic essence of major protocols in the area, allowing for easy, information-theoretic design of protocols that call an underlying OSP functionality. This separates the ‚Äúcryptographic layer‚Äù from the ‚Äúinformation-theoretic layer‚Äù in the design of these protocols. In some cases, this modular approach also allows us to broaden the set of assumptions under which these applications are known to exist. 1.2.3 Implications Finally, given the broad reach of OSP, we seek to understand the cryptography necessary in order to realize it. Our results on this are summarized as follows. Theorem 1.6 (Informal). OSP implies commitments and oblivious transfer (OT) with classical communication (where one party is completely classical), while two-round OSP implies public-key encryption. So what can we conclude about OSP from these results? As mentioned earlier, one of the original motivations for our work was to ‚Äújustify‚Äù the use of public-key cryptography in the recent line of work aimed at establishing a classical leash on quantum systems. Progress towards this goal can be appreciated by noting that the techniques introduced in [BCM+18, Mah18a, Mah18b, KMCVY21] all at the very least provide some way to perform an OSP between the classical client and quantum server, and thus, by our result, also provide a way to build OT with classical communication. So far, the community does not have any approach for building OT with classical communication from minicrypt assumptions, or even from arbitrary trapdoor functions. Thus, our result helps explain why the known constructions of OSP require TCFs, which are more structured than even injective trapdoor functions. In fact, in the classical setting, we actually have an oracle separation between OT and injective trapdoor functions [GKM+00]. However, it remains an open question to give such strong oracle separations in the quantum setting. Progress came when [ACC+22] showed that perfectly correct key agreement between one classical and one quantum party does not exist in the quantum random oracle model. This notion of key agreement is implied by the perfectly correct variant of our notion of OT between one classical and one quantum party, and thus, we obtain the following corollary. Corollary 1.7 (Informal). Perfectly correct OSP does not exist in the quantum random oracle model. This leaves a sliver of possibility that non-perfectly-correct OSP can yet be constructed without public-key assumptions. However, we have established that if one can build OSP from minicrypt primitives (say, by adapting the techniques of [YZ24]), or even from arbitrary trapdoor functions, then this would also represent a major breakthrough in cryptography more generally - a construction of classical-communication OT from new assumptions."
https://arxiv.org/html/2411.04068v1,Security Assessment of Mobile Banking Apps in West African Economic and Monetary Union,"Mobile banking adoption is soaring in Africa, particularly within the West African Economic and Monetary Union (WAEMU) states. These countries, characterized by widespread smartphone usage, have witnessed banks and financial institutions introducing mobile banking applications. These apps empower users to perform transactions such as money transfers, bill payments, and account inquiries anytime, anywhere. However, this proliferation of mobile banking apps also raises significant security concerns. Poorly implemented security measures during app development can expose users and financial institutions to substantial financial risks through increased vulnerability to cyberattacks. Our study evaluated fifty-nine WAEMU mobile banking apps using static analysis techniques. These mobile banking apps were collected from the 160 banks and financial institutions of the eight WAEMU countries listed on the Central Bank of West African States (BCEAO) website. We identified security-related code issues that could be exploited by malicious actors. We investigated the issues found in the older versions to track their evolution across updates. Additionally, we identified some banks from regions such as Europe, the United States, and other developing countries and analyzed their mobile apps for a security comparison with WAEMU banking apps. Key findings include: (1) WAEMU apps exhibit security issues introduced during development, posing significant risks of exploitation; (2) Despite frequent updates, underlying security issues often persist; (3) Compared to banking apps from developed and developing countries, WAEMU apps exhibit fewer critical security issues; and (4) Apps from banks that are branches of other non-WAEMU banks often inherit security concerns from their parent apps while also introducing additional issues unique to their context. Our research underscores the need for robust security practices in WAEMU mobile banking app development to enhance user safety and trust in financial services.","The financial sector plays a pivotal role in the economic development of any country, with digital financial services, including mobile money and mobile banking, emerging as powerful catalysts for this progress, particularly in Sub-Saharan African nations. The widespread adoption of mobile banking is a global phenomenon, with Sub-Saharan Africa leading the way in its innovative implementation [1]. This surge can be attributed mainly to the remarkable proliferation of smartphones, a trend projected to continue its upward trajectory. Nowadays, approximately 60% of people have smartphones worldwide [2]. By 2022, smartphone adoption in Sub-Saharan Africa had reached an impressive 51%, with projections indicating a substantial rise to 87% by 2030 [3]. Recognizing the strategic advantage of widespread smartphone penetration, banks and financial institutions within the West African Economic and Monetary Union (WAEMU) region are leveraging this technology to extend banking services. This leap serves a dual purpose: broadening access to banking services for millions and empowering existing account holders with remote control over their finances, facilitating transactions and balance checks. In WAEMU, nearly all banks and financial institutions have embraced mobile applications, reaching millions reliant on these platforms for daily financial transactions. However, the ubiquity of mobile banking applications is accompanied by recurring security concerns, posing challenges for users and institutions alike. Reports highlight, for example, the persistence of sensitive keys hard-coded into financial mobile applications across Africa [4], underscoring the potential for significant financial losses. Despite the growing reliance on these applications in developing countries, including in the WAEMU, comprehensive studies on their security remain scarce. A literature review reveals limited studies on mobile financial applications in developing nations, with no research specific to the WAEMU region [5]. Various approaches have been explored to assess the security issues of mobile banking apps, ranging from investigating their origins [6] to conducting forensic examinations and vulnerability assessments [7, 8]. However, there remains a gap in research addressing these concerns comprehensively. Against this backdrop, our study extensively investigates the security landscape of mobile banking applications in WAEMU countries. Our research aims to identify prevalent security issues, establish a comprehensive threat model, analyze the evolution of security across different app versions, and compare the security posture of WAEMU banking apps with those from the European Union (EU), the United States (US), and other developing countries (ODC). Moreover, some WAEMU financial institutions are branches (children) of other institutions (parents). We explore the security of their mobile banking apps and determine if they inherit parent app security issues. The contributions of this study are as follows: ‚Ä¢ Highlighting the most common security problems affecting WAEMU banking apps. ‚Ä¢ Providing possible ways to exploit app vulnerabilities to understand the risks better. ‚Ä¢ Offering an assessment of the security evolution in these apps, including whether concerns are effectively addressed in subsequent updates. ‚Ä¢ Offering a comparative analysis of the security posture of WAEMU banking apps in the global context, comparing them with the applications used by leading African and International banks. ‚Ä¢ Shedding light on the security problems between branch and parent apps. These findings underscore the imperative of enhancing mobile banking app security to safeguard users and institutions in an increasingly digital financial landscape. We present the rest of the study as follows. Section 2 presents the motivation of the study. Section 3 presents the background by explaining the concepts of mobile banking and security code smells. Section 4 describes the detailed methodology followed in this paper. We report the results in Section 5 and discuss them in Section 6. We discuss the threat to validity in Section 7 and the related works in Section 8. Finally, we conclude the SLR in Section 9. Artifacts. We release all of our artifacts (e.g., list of mobile apps, detailed results, etc.): https://github.com/liounea/Data_for_WAEMU_Apps_Papers"
https://arxiv.org/html/2411.03844v1,Attribute-Based Encryption With Payable Outsourced Decryption Using Blockchain and Responsive Zero Knowledge Proof,"Attribute-Based Encryption (ABE) is a promising solution for access control in cloud services. However, the heavy decryption overhead hinders its widespread adoption. A general approach to address this issue is to outsource decryption to decryption cloud service(DCS). Existing schemes have utilized various methods to enable users to verify outsourced results; however, they lack an effective mechanism to achieve exemptibility which enables the honest DCS to escape from wrong claims. And it is impractical to assume that the DCS will provide free services. In this paper, we propose a blockchain-based payable outsourced decryption ABE scheme that achieves both verifiability and exemptibility without adding redundant information to ABE ciphertext. We use zero-knowledge proof to verify outsourced results on blockchain and introduce an optional single-round challenge game under optimistic assumption to address the high cost of proof generation. Moreover, our system achieves fairness and decentralized outsourcing to protect the interests of all parties. Finally, we implement and evaluate our scheme on Ethereum to demonstrate its feasibility and efficiency, the gas usage in attribute numbers from 5 to 60 is 11√ó\times√ó to 140√ó\times√ó in the happy case and 4√ó\times√ó to 55√ó\times√ó in the challenge case lower than the scheme of Ge et al. (TDSC‚Äô23).","With the rapid development of cloud computing, cloud storage services have fundamentally reshaped data sharing. Increasingly, users and organizations are uploading and sharing data via cloud storage services. However, since Cloud Service Providers (CSPs) are not entirely trustworthy and much of the data stored in the cloud is highly sensitive, such as personal medical records and internal corporate data, access control in cloud storage becomes a critical issue. In public cloud storage, access control can be achieved through functional encryption [1], among which Ciphertext-Policy Attribute-Based Encryption (CP-ABE) is considered one of the most suitable solutions [2]. CP-ABE enables fine-grained access control by embedding access control policy into the ciphertext and attributes into the keys. Data owners can design access policy at the encryption phase, and decryption is possible only if the set of data user‚Äôs attributes satisfies the access policy embedded in the ciphertext. However, one major issue in functional encryption is the heavy decryption overhead. For example, in pairing-based attribute-based encryption, the pairing operations required during decryption are typically linear to the size of the access control policy [3],[4],[5], which is particularly inefficient for lightweight and mobile devices. To address this issue, Green et al. [6] proposed an outsourced decryption scheme for attribute-based encryption (OABE), which significantly reduces the computational overhead for users without revealing sensitive information about the original data. In this scheme, the user generates a transformation key (TK) and a retrieve key (RK) from the attribute private key (SK) and sends the TK and ciphertext (CT) to the decryption cloud server (DCS) for partial decryption, which transforms the ciphertext into an ElGamal ciphertext [7]. The user then uses the RK to decrypt the ElGamal ciphertext and retrieve the plaintext. Since the DCS may not perform the computation honestly to save computation resources, many verifiable outsourced decryption schemes for attribute-based encryption have been proposed in the literature [8],[9],[10],[11],[12]. These schemes add redundant information to both the original ABE ciphertext and the transformed ciphertext to verify the correctness of the outsourced computation result. However, this redundant information increases the computational cost for the DCS during the transformation phase, potentially raising the user‚Äôs expenses. Furthermore, many previous schemes fail to achieve the properties of exemptibility and fairness. The exemptibility property ensures that the DCS is not falsely accused of malicious behavior if it returns a correct transformed ciphertext, while the fairness property ensures that the DCS gets paid if and only if it returns a correct result. Intuitively, these two properties could be achieved by introducing a trusted intermediary. Cui et al. [13] combined decentralized blockchain technology[14] to propose a functional encryption with payable outsourced decryption (FEPOD) scheme, which achieves these two properties without relying on a trusted authority. However, this scheme still introduces redundant information. Ge et al. [15] eliminates redundant information and achieves decentralized outsourcing through delegating computation to the blockchain, but outsources heavy computation to smart contracts, which will cause high gas fees and performance bottlenecks. A straightforward method to reduce on-chain computation is zero-knowledge proof[16], but will lead to large computation with the proof generation. I-A Motivation and Contribution Although many existing attribute-based encryption (ABE) schemes with outsourced decryption can detect malicious behavior by the decryption cloud server, they rely on additional redundant information in transformation, which increases the honest server‚Äôs computational cost. Furthermore, schemes that achieve exemptibility and fairness without trust authority rely on smart contracts for heavy computation, leading to high gas fees and performance issues. Zero-knowledge proof is a general method for reducing on-chain computation; however, they also incur substantial overhead in off-chain proof generation. In this paper, we aim to design an attribute-based encryption scheme with payable outsourced decryption that overcomes the above issues, achieves desirable properties, and minimizes overhead. In summary, the contributions of this paper are as follows. ‚Ä¢ We propose a zk-friendly payable outsourced decryption attribute-based encryption (ABE) scheme without redundant information based on blockchain, which achieves verifiability, exemptibility, fairness and decentralized outsourcing, and the payment can be processed by blockchain cryptocurrency. ‚Ä¢ We use zero-knowledge proof (ZKP) to verify outsourced decryption result on blockchain with constant gas fee, introduce a single-round challenge mechanism to greatly reduce proof generation with high computational cost through responsive ZKP, and achieve self-challenge attack resistance. ‚Ä¢ We implement a concrete payable outsourced decryption ABE scheme with responsive ZKP on native Ethereum, develop the ZKP circuit using the widely adopted framework Halo2[17] and evaluate feasibility and performance on Ethereum. I-B Related Work ‚Ä¢ Outsource Decryption Attribute Based Encryption. Attribute-Based Encryption was first introduced as fuzzy identity-based encryption by Sahai and Waters in [18]. ABE schemes can be divided into two categories: Ciphertext-Policy ABE (CP-ABE) and Key-Policy ABE (KP-ABE) [3], depending on the access policy is embedded into the ciphertext or the user‚Äôs private key. A user can decrypt a ciphertext only if the set of attributes satisfies the access policy. However, the decryption involved in the ABE is usually too expensive, and this problem is especially acute for resource limited devices such as mobile devices, which greatly hinders its practical popularity. In order to reduce the decryption overhead for a user to recover the plaintext, Green et al. [6] suggested to outsource the majority of the decryption work without revealing the plaintext. However, [6] lacks a mechanism to verify whether the decryption cloud server has returned a correct transformed ciphertext. Consequently, Lai et al. [8] modified the original model of ABE with outsourced decryption to include verifiability, but doubled the size of the underlying ABE ciphertext and the computation costs. It appended a redundant ciphertext of a random message and a tag to each ciphertext, and required the original untransformed ciphertext as an auxiliary input in the final decryption step by the user. Based on the scheme [8], Qin et al. [10] and Lin et al. [11] optimized the efficiency of OABE. [10] halved the ciphertext size in both server and client and is 2 to 4 times faster in decryption compared with the scheme [8] and proposed an approach to convert any ABE scheme with outsourced decryption into an ABE scheme with verifiable outsourced decryption. [11] proposed a more efficient and generic construction of ABE with verifiable outsourced decryption based on an attribute-based key encapsulation mechanism, a symmetric-key encryption scheme, and a commitment scheme. Moreover, Li et al. [9] proposed the notion of attribute-based encryption with both outsourced encryption and decryption, and considered to offload the overhead computation at authority by outsourcing key-issuing. However, [9] only supports the threshold access policy. Following their work, Ma et al. [19] proposed an attribute-based encryption with both outsourced encryption and decryption that supports any monotonic access policy and defined stronger verifiability than [8]. It claimed that even a user who has the private key cannot accuse the decryption cloud server of outputting incorrect results while it was not the case. But in fact they can not achieve the exemptibility since the user may reveal an incorrect private key and claim that the decryption cloud server has returned incorrect transformed ciphertext. Moreover, Wang et al. [20] proposed fully accountable ABE for the pay-as-you-go model, which can achieve high decryption efficiency for data consumers and prevention of Distributed Denial of Services(DDoS) attacks on the cloud ciphertexts. None of the above schemes can achieve both exemptibility and fairness. To address this issue without central authority, Cui et al. [13] proposed an attribute-based encryption scheme using blockchain that outsources decryption to a smart contract. In their scheme, the user creates a smart contract that contains the decryption task. Any miner on the blockchain can initiate a transaction that calls the smart contract and receive the reward. However, this scheme still introduces redundant information and outsources heavy computation to smart contracts. Ge et al. [15] eliminated redundant information and mitigated heavy computation problem of smart contracts through decomposing to pieces of meta computations, but still caused high gas usage and performance bottlenecks. Moreover, since native Ethereum[21] does not support pairing computation operation, this scheme is not compatible with native Ethereum. Recently, Tao et al. [22] tackled the issue of significant computational burden on outsourcing devices whose resources are not infinite in practice. They proposed a secure and efficient outsourced ABE scheme with decryption results reuse. Moreover, Mahdavi et al. [23] presented novel precomputed and short ciphertext precomputed versions of a special case of ABE which are tailored for resource-constrained low-end IoT devices. In this scheme, some outsourcing methods for heavy computational operations are suggested to be used, and the data user can ensure verifiability by repeatedly querying and comparing the consistency of the results. Hou et al. [24] proposed a blockchain-based efficient verifiable outsourced attribute-based encryption in cloud, utilizing the technique of batch verification to make data users verify the decrypted plaintexts. In this scheme, the audit of verification results from the data user is performed by miners. ‚Ä¢ Applications of Fraud Proof and Validity Proof on Blockchain. The fraud proof assumes optimistic condition, defaulting to accept the statement unless disproven. It includes a challenge window during which challengers may initiate disputes to demonstrate the incorrectness of the statement. The validity proof provides evidence that a statement is correct, and acceptance occurs only when correctness is definitively established under a pessimistic assumption. Optimistic rollup[25],[26],[27],[28],[29],[30] as one of the important solutions for Ethereum‚Äôs layer2, rely on the optimistic assumption that most transactions will be valid, which enables more rapid and efficient transaction processing. To safeguard against invalid transactions, fraud proofs are employed to challenge and dispute them. Many projects[25],[26] employ the interactive bisection protocol[31] to implement multi-round fraud proof, while others[29],[30] utilize zero-knowledge proof to achieve single-round fraud proof and shorten the challenge window. This fraud proof mechanism has been widely adopted across various blockchain applications beyond optimistic rollups, including the Lightning Network[32], Plasma[33], and distributed key generation protocol[34]. Zero-knowledge proof is an important cryptographic tool for another Ethereum layer2 solution, zero-knowledge Rollup, which uses validity proof instead of fraud proof. There are now many projects[35][36][37] that leverage zero-knowledge proof technology to implement ZK-EVM(Ethereum Virtual Machine) or ZK-VM(Virtual Machine). These projects generate a proof for a batch of transactions on Layer 2 and submit it to Layer 1. Instead of re-executing all transactions, nodes can verify the transactions and smart contracts by checking the proof and updating the state. This approach significantly enhances the transaction throughput of the blockchain. I-C Organization The rest of this paper is organized as follows. In Section II, we briefly revisit the definitions that are relevant to this paper. In Section III, we describe the system overview of our scheme. In Section IV, we present a generic construction of POABE scheme using blockchain and responsive zero-knowledge proof. In Section V, we implement the proposed concrete scheme on Ethereum to evaluate its performance. Finally, this paper is concluded in Section VI."
https://arxiv.org/html/2411.03404v1,"EVA-S3PC: Efficient, Verifiable, Accurate Secure Matrix Multiplication Protocol Assembly and Its Application in Regression","Efficient multi-party secure matrix multiplication is crucial for privacy-preserving machine learning, but existing mixed-protocol frameworks often face challenges in balancing security, efficiency, and accuracy. This paper presents an efficient, verifiable and accurate secure three-party computing (EVA-S3PC) framework that addresses these challenges with elementary 2-party and 3-party matrix operations based on data obfuscation techniques. We propose basic protocols for secure matrix multiplication, inversion, and hybrid multiplication, ensuring privacy and result verifiability. Experimental results demonstrate that EVA-S3PC achieves up to 14 significant decimal digits of precision in Float64 calculations, while reducing communication overhead by up to 54.8%percent54.854.8\%54.8 % compared to state of art methods. Furthermore, 3-party regression models trained using EVA-S3PC on vertically partitioned data achieve accuracy nearly identical to plaintext training, which illustrates its potential in scalable, efficient, and accurate solution for secure collaborative modeling across domains.","The rapid advancement of digital technologies such as AGI (Artificial General Intelligence), IoT (Internet of Things), and cloud computing makes data a fundamental production factor in digital economy. Government organizations and businesses nowadays use centralized cloud services (6424959, ) for data collaboration among multiple entities, which produced promising applications in healthcare, finance, and governance(10.1145/3158363, ), but this routing in general suffers from the problem of privacy. Figure 1 illustrates a ideal scenario of three financial institutions (P1: Bank with corporate financial data, P2: Rating Agency with credit rating, P3: Insurance holding a ten-year default record as labels) aiming to conduct joint regression analysis based on heterogeneously distributed data without sharing raw data directly. While deep learning as a service (DLaaS) (sekar2023deep, ) can explore the value of their respective data in financial risk control scenarios by pooling data on a central service, most real life application would request the analysis to be carried out without exchanging any raw data to avoid privacy breaches. Figure 1. Secure Three-Party Cooperative Modeling Problem \Description [¬°short description¬ø]¬°long description¬ø In recent years, frequent data security incidents and heightened public awareness of personal privacy have led governments worldwide to implement regulations (e.g., the well-known GDPR (voigt2017eu, )) to protect data ownership and user rights. These regulations have made efficient data sharing across organizations more challenging, creating isolated ‚Äùdata islands‚Äù where vast amounts of data remain untouched. Therefore, achieving decentralized computation and collaborative modeling without compromising privacy has become a major research challenge for academia and industry. Current mainstream research directions include Secure Multi-party Computation (SMPC) (zhou2024secure, ; mohanta2020multi, ), Homomorphic Encryption (HE) (munjal2023systematic, ), Differential Privacy(DP) (zhao2022survey, ), and privacy-preserving machine learning (PPML) frameworks based on various cryptographic primitives. A critical component in all of these approaches is secure matrix multiplication, which is fundamental to many machine learning algorithms. Examples include calculating gain coefficients in decision trees (abspoel2021secure, ), computing Euclidean distances in K-nearest neighbor classification (haque2020privacy, ), and solving coefficients in linear regression (aono2017input, ). Table 1. Comparison of various mixed-protocols SMPC frameworks related to 2PC and 3PC Framework Dev. Language Techniques Used Operator Supported Security Theoretical Evaluation Pratical Performance Mat.Mul Mat.Inv Complexity Verifiablity Extensibility Float64 Comm. Big Matrix 2PC SecureML(mohassel2017secureml, ) C++ HE,GC,SS ‚úî ‚úò or High ‚úò High ‚úò Crypten(knott2021crypten, ) python SS ‚úî ‚úî Medium ‚úò Low ‚úî Delphi(mishra2020delphi, ) C++ HE,GC,SS ‚úî ‚úî High ‚úò High ‚úò Motion(braun2022motion, ) C++ OT,GC,SS ‚úî ‚úò Medium ‚úò High ‚úò Pencil(liu2024pencil, ) C++ HE,DP ‚úî ‚úò or High ‚úò High ‚úò LibOTe(libOTe, ) C++ OT ‚úî ‚úî Medium ‚úò High ‚úò Chameleon(riazi2018chameleon, ) C++ GC,SS ‚úî ‚úò Medium ‚úò Medium ‚úî Du‚Äôs work(du2002practical, ) N/A DD ‚úî ‚úî Low ‚úò Low ‚úî 3PC ABY3(mohassel2018aby3, ) C++ GC,SS ‚úî ‚úò Medium ‚úò Medium ‚úî Kumar‚Äôs Work(kumar2017privacy, ) Matlab DD ‚úî ‚úî Low ‚úî Low ‚úî Daalen‚Äôs Work(van2023privacy, ) N/A DD ‚úî ‚úò Low ‚úò Low ‚úî MP-SPDZ(keller2020mp, ) C++ HE,OT,SS ‚úî ‚úò or High ‚úò High ‚úò Tenseal(benaissa2021tenseal, ) python HE ‚úî ‚úò or High ‚úò High ‚úò FATE(FATE, ) python HE,SS ‚úî ‚úò or High ‚úò High ‚úò SecretFlow(ma2023secretflow, ) python HE,GC,SS ‚úî ‚úî or High ‚úò High ‚úò EVA-S3PC python DD ‚úî ‚úî Low ‚úî Low ‚úî ‚Ä¢ Note: Dev. Language indicates the main development language for each framework. Mat.Mul and Mat.Inv indicates multiplication and inversion operator for matrix. ‚úî and ‚úò indicates the framework supports or not supported the feature. For Security, / denotes the framework can against malicious adversary or semi-honest adversary. / / in Extensibility refers to the difficulty level of coupling the framework with other SMPC protocols based on existing languages and compiler. For Float64, these symbols indicate the precision degree(high, medium or low) under this data type. Comm. here indicates the communication overhead. We organize representative frameworks supporting 2-party or 3-party matrix multiplication operators in Table 1, and evaluate 16 cutting-edge frameworks based on development language, technique type, security level, theoretical assessment metrics (Float64 computational precision, computational complexity, communication overhead), and practical performance indicators (support for result verification, capability for large matrix calculations, and extensibility with various security protocols). For frameworks incorporating HE primitives, such as (keller2020mp, ; benaissa2021tenseal, ), their security is intrinsically high due to the NP-hard nature of the underlying computational problem. However, HE involves extensive calculations over long ciphertext sequences and can only approximate most non-linear operators, resulting in high computational complexity and low precision, making it less suitable for precise computation of large matrices. In contrast, frameworks like Chameleon and ABY3 (riazi2018chameleon, ; mohassel2018aby3, ) integrate Garbled Circuits (GC) for efficient Boolean operations and Secret Sharing (SS) for arithmetic operations, balancing efficiency in logic and arithmetic computations. Nevertheless, additive secret sharing (ASS) and replicated secret sharing (RSS) require conversions between various garbled circuits during large-scale matrix multiplications, introducing substantial communication overhead (knott2021crypten, ). Additionally, MSB truncation protocols (wagh2019securenn, ) tend to accumulate errors when computing floating-point numbers outside the ring Z2psuperscriptsubscriptùëç2ùëùZ_{2}^{p}italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT, limiting their applicability in non-linear and hybrid multiplication types (e.g., ¬ß‚Å¢4.3‚àº4.5similar-to¬ß4.34.5\mathsection{\ref{S2PI}\sim\ref{S3PHM}}¬ß ‚àº). Frameworks relying on DP, while effectively concealing sensitive information through noise, can accumulate errors in matrix multiplications, potentially affecting computational accuracy in practical applications. Although OT-based frameworks (rabin2005exchange, ; yadav2022survey, ) ensure high precision and secure data transfer during matrix multiplication, communication costs grow linearly with matrix size, restricting efficiency and scalability in complex computational scenarios. Evidently, while these frameworks generally ensure security in semi-honest three-party settings, few solutions balance precision, complexity, and communication overhead while also supporting result verification, extensibility, and scalability for large matrices in practical applications. Data disguising is a commonly used linear space random perturbation method, including linear transformation disguising, Z+V aggregation disguising, and polynomial mapping disguising (du2002practical, ). Based on real-number domains ‚Ñù‚Ñù\mathbb{R}blackboard_R, it preserves matrix homogeneous by disguising original data, thus ensuring privacy protection for matrix input and output with minimal interaction and flexible asynchronous computation. This approach avoids the computational complexity of HE ciphertext and the significant communication overhead associated with GC circuit transformations, while also maintaining high precision and timeliness. Existing research in this area primarily stems from the CS (Commodity Server) model built on Beaver‚Äôs (beaver1997commodity, ; beaver1998server, ) multiplication triples, introducing a Third Trust Party (TTP) not involved in the actual computation to ensure process security. However, prior work (du2001privacy, ; atallah2001secure, ) often remains at the theoretical level, focusing on 2-party vector or array operations and linear equation solutions without rigorous security proofs or practical performance evaluations (running time, precision, overhead). To further apply data disguising techniques to various scientific computing problems and more complex three-party practical applications, this paper presents a secure 3-party computation framework under a semi-honest setting with verifiable results. The main contributions of this work are as follows: ‚Ä¢ A secure 3-party computing (S3PC) framework EVA-S3PC under the semi-honest adversary model is proposed with five elementary protocols: Secure 2-Party Multiplication (S2PM), Secure 3-Party Multiplication (S3PM), Secure 2-Party Inversion (S2PI), Secure 2-Party Hybrid Multiplication (S2PHM), Secure 3-Party Hybrid Multiplication Protocol(S3PHM), using data disguising technique. Rigorous security proofs are provided based on computation indistinguishability theory, and correctness is also proved in semi-honest environment. ‚Ä¢ A secure and efficient result validation protocol using Monte Carlo method is proposed to detect abnormality in the result produced by computation made of aforementioned elementary protocols. ‚Ä¢ A typical linear regression model with data features and labels spited on 3 nodes was constructed using the elementary protocols under the framework, including the algorithm for Secure 3-Party Linear Regression Training (S3PLRT) and Secure 3-Party Linear Regression Prediction (S3PLRP) respectively. ‚Ä¢ Theoretical and practical computational complexity, communication overhead, computing precision and prediction accuracy are analyzed and compared with representative SMPC models. Organizations: The remainder of this paper proceeds as follows. Section 2 presents recent advancements in secure multi-party computation for matrix multiplication, inversion, and regression analysis. Section 3 introduces the framework of S3PC and review some essential preliminaries. In Section 4, we describe the proposed elementary protocols S2PM, S3PM, S2PI, S2PHM, S3PHM with security proof. Section 5 constructs S3PLRT and S3PLRP using building blocks from Section 4. Section 6 provides theoretical analysis of computational and communication complexity, followed by experimental comparison of performance and precision with various secure computing schemes in Sections 7. Finally, some conclusions are drawn in Section 8."
https://arxiv.org/html/2411.03371v1,Blockchain-Based Multi-Path Mobile Access Point Selection for Secure 5G VANETs,"This letter presents a blockchain-based multi-path mobile access point (MAP) selection strategy for secure 5G vehicular ad-hoc networks (VANETs). The proposed method leverages blockchain technology for decentralized, transparent, and secure MAP selection, while the multi-path transmission strategy enhances network reliability and reduces communication delays. A trust-based attack detection mechanism is integrated to ensure network security. Simulation results demonstrate that the proposed algorithm reduces both handover frequency and average communication delay by over 80%, and successfully identifies and excludes more than 95% of Sybil nodes, ensuring reliable and secure communication in highly dynamic vehicular environments.","The rapid advancement of 5G technology has enabled vehicular ad-hoc networks (VANETs) to support diverse applications, from real-time traffic management to autonomous driving and safety-critical communications [1, 2, 3, 4]. However, maintaining reliable, low-latency communication in dynamic vehicular environments remains a key challenge [5]. Traditional VANETs, which rely on fixed infrastructure like roadside units (RSUs) and cellular base stations [6], often experience issues such as frequent handovers, fluctuating connectivity, and network congestion due to high vehicle mobility and varying traffic density [7, 8]. To address these limitations, mobile access points (MAPs) have been proposed, where vehicles dynamically serve as relay nodes, enhancing communication coverage and reducing dependence on fixed infrastructure. However, existing MAP selection methods, such as heuristic or static algorithms, struggle to adapt to the dynamic nature of VANETs, especially in scenarios with high mobility or security threats [4, 9]. Moreover, the absence of secure and decentralized decision-making leaves the network vulnerable to attacks, like Sybil attacks, where malicious nodes create fake identities to disrupt communication [5, 3]. In this letter, we propose a blockchain-based multi-path MAP selection strategy for secure 5G VANETs. Blockchain technology enables decentralized, transparent, and secure decision-making for MAP selection, while multi-path transmission ensures reliable communication and reduces handover frequency [7]. Additionally, an integrated Sybil attack detection mechanism enhances network security by mitigating malicious nodes [6]. Our contributions are: (1) a blockchain-based decentralized MAP selection strategy with Sybil attack detection, (2) a multi-path transmission approach that improves network reliability and reduces latency, and (3) simulation results that show significant reductions in handovers and communication delays. In summary, the key novelty of this work lies in the integration of blockchain technology for decentralized decision-making combined with a multi-path transmission strategy. This dual approach not only enhances security against Sybil attacks but also reduces handover frequency and improves overall network performance, which is critical for highly dynamic environments like VANETs."
https://arxiv.org/html/2411.03365v1,Enhanced Real-Time Threat Detection in 5G Networks: A Self-Attention RNN Autoencoder Approach for Spectral Intrusion Analysis,"In the rapidly evolving landscape of 5G technology, safeguarding Radio Frequency (RF) environments against sophisticated intrusions is paramount, especially in dynamic spectrum access and management. This paper presents an enhanced experimental model that integrates a self-attention mechanism with a Recurrent Neural Network (RNN)-based autoencoder for the detection of anomalous spectral activities in 5G networks at the waveform level. Our approach, grounded in time-series analysis, processes in-phase and quadrature (I/Q) samples to identify irregularities that could indicate potential jamming attacks. The model‚Äôs architecture, augmented with a self-attention layer, extends the capabilities of RNN autoencoders, enabling a more nuanced understanding of temporal dependencies and contextual relationships within the RF spectrum. Utilizing a simulated 5G Radio Access Network (RAN) test-bed constructed with srsRAN 5G and Software Defined Radios (SDRs), we generated a comprehensive stream of data that reflects real-world RF spectrum conditions and attack scenarios. The model is trained to reconstruct standard signal behavior, establishing a normative baseline against which deviations, indicative of security threats, are identified. The proposed architecture is designed to balance between detection precision and computational efficiency, so the LSTM network, enriched with self-attention, continues to optimize for minimal execution latency and power consumption. Conducted on a real-world SDR-based testbed, our results demonstrate the model‚Äôs improved performance and accuracy in threat detection.Keywords: 5G Security, spectrum access security, self-attention, real-time intrusion detection, RNN autoencoder, LSTM, time series anomaly detection.","The advent of the fifth generation (5G) of wireless communication systems has ushered in an unprecedented era of connectivity and innovation. With its promise of higher data rates, reduced latency, and increased capacity, 5G is set to revolutionize various sectors, including smart cities, autonomous vehicles, and the Internet of Things (IoT) [1]. Meanwhile, on the other side, it introduces significant security vulnerabilities, particularly in radio frequency (RF) communications, caused by high-density networks with a large number of access points and user equipment (UE). The flexible allocation of spectrum in 5G renders the monitoring and securing channel access more complicated [2]. Among diverse threats, RF jamming attacks emerge as a substantial threat, undermining the reliability and functionality of critical 5G network services that are fundamental to sectors like IoT and autonomous vehicles. For instance, remote surgery and autonomous driving [3], require ultra-reliable low-latency communications, which can be compromised by malicious interference. Unfortunately, traditional network security mechanisms cannot effectively cope with the threats due to multiple factors. The difficulties for traditional methods may include new evolved threats; highly dynamic spectrum access and large-scale networks. Therefore, the variety and sophistication of potential attacks, such as advanced persistent threats (APTs) and intelligent jamming, necessitate more advanced detection mechanisms. In fact, taking into account the traditional intrusion detection systems (IDS) in the context of 5G‚Äôs unique demands and threat landscape, the potential challenges to address the RF jamming attacks are detailed as follows. Dynamic spectrum access in 5G networks introduces a layer of complexity where the 5G‚Äôs spectrum is characterized by its fluidity ‚Äì bandwidths vary, access policies shift frequently, and modulation schemes adapt in real-time [4]. This presents a significant challenge for traditional IDS, which are typically engineered for more static environments. The evolving threat landscape in 5G networks incorporate advanced threats like adaptive jamming and complex advanced persistent threats (APTs) [5]. These modern attacks often do not follow repetitive patterns and are designed to adapt to countermeasures, making them particularly challenging to detect by traditional IDS. The exponential growth in the number of connected devices and network nodes within 5G networks, coupled with the openness and flexibility introduced by the adoption of Open Radio Access Network (O-RAN) architecture, exacerbates these challenges. The open interfaces and disaggregated components in O-RAN can be exploited by attackers, creating vulnerabilities that traditional security methods are not equipped to handle effectively [6]. To effectively counter the aforementioned challenges in 5G networks, an advanced intrusion detection system (IDS) is paramount. This IDS must be both reactive, to counter known threats, and proactive, to adapt to emerging, unseen attack patterns. Critically, it must achieve this balance while being scalable and resource-efficient, ensuring that the intrinsic performance benefits of 5G are not compromised as the network expands in size and complexity. Addressing these requirements, our research proposes a novel IDS framework that synergizes an efficient self-attention mechanism with a recurrent neural network (RNN)-based autoencoder. This combination is strategically chosen to tackle the unique challenges posed by 5G networks. The self-attention mechanism of our solution enables the IDS to adaptively focus on specific spectrum parts more prone to anomalies [7] to enhance its efficacy in safeguarding against spectrum-related vulnerabilities. The integration of unsupervised learning capabilities in the self-attention-equipped RNN autoencoder enables the detection of both known and novel attack patterns. By learning complex dependencies within the data, the model is equipped to identify emerging cyber threats that were not part of its initial training set. The self-attention mechanism computational efficiency translates into the ability for parallel processing, a crucial factor in reducing the computational load. This ensures that the IDS can keep pace with the growing size and complexity of 5G networks, offering robust threat detection without significant resource overheads. Our research presents a sophisticated approach to addressing the complex challenges associated with RF intrusion detection in 5G networks. We have developed a model that effectively combines the temporal processing capabilities of Recurrent Neural Networks (RNNs) with the contextual sensitivity afforded by self-attention mechanisms. This integration results in a robust, efficient, and scalable system. Specifically designed to process and analyze the time-series data characteristic of the RF spectrum, our model excels at identifying anomalies indicative of potential jamming attacks. A notable aspect of our work is the deployment of a 5G Radio Access Network (RAN) test-bed, along with comprehensive databases, to facilitate the training and inference phases of our model. We have structured a sequential two-part methodology focusing on anomaly detection and subsequent classification. This approach is further bolstered by rigorous experimental validation and an extensive analysis of performance metrics. Our model demonstrates a high proficiency in detecting a diverse range of cyber threats, positioning it as a viable and effective tool for practical application in real-world 5G scenarios. The ensuing sections of this paper will explore the background, methodology, experimental setup, results, and provide a detailed analysis of our proposed model. This comprehensive examination will underscore the model‚Äôs effectiveness in protecting 5G networks against sophisticated and evolving RF threats, thus showcasing its potential as a critical asset in modern network security frameworks."
https://arxiv.org/html/2411.03364v1,DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks,"Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network (GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN‚Äôs privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN. Extensive experiments on eight benchmark datasets, three graph neural network models, and six attack scenarios show that DM4Steal achieves the state-of-the-art (SOTA) performance compared with four baselines in aspects of AUC (‚àºsimilar-to\sim‚àº √ó\times√ó1.12).","Graph-structured data, such as social networks [22], biological networks [13], and transportation networks [38, 18] possesses powerful characterization capabilities for representing complex relationships among various objects [37, 33, 34]. Due to an increasing number of graph-structured data being collected, graph mining plays a significant role in wide-range areas. Regarded as intellectual property of data holders, the preservation of sensitive data, e.g., private personal information and essential relations, within graph should be considered in practical scenarios. Such typical examples could be found in recommendation system [29] and online transaction system [16]. Recent work has revealed a heightened reliance on graph data for training graph neural network (GNN) [14, 32, 27], leading to the substantial incorporation of private data within graph neural networks. More specifically, a malicious user could reversely inference the graph data used in training by accessing the target graph neural network model and leveraging specific auxiliary data. For example, as shown in Figure 1, a malicious user could infiltrate a user‚Äôs credit rating system and utilizes auxiliary knowledge (e.g., partial graphs, user attributes, and shadow datasets) to deduce a transactional relationship between two users based on varying rating results. Great efforts have been made to compromise the privacy of GNN [6, 30]. According to different attack goals, current work can be roughly cast into four categories, i.e., membership inference attacks [6, 23, 12], attribute inference attacks [6], link stealing attacks [6, 11], and model stealing attacks [30]. In particular, the link stealing attack seeks to reconstruct the connections between various nodes in the target graph data using publicly available embedded data or labels. For instance, we consider a scenario depicted in Figure 1, where a bank employs a user transaction network to assess their creditworthiness. However, a malicious user could exploit the user ratings to illicitly obtain transaction records, resulting in privacy breaches and detrimental consequences. Since link stealing attack [6, 11] makes a practical threat, we predominantly focus on it. Figure 1: An example of link stealing attack on credit rating system. The system utilizes a user‚Äôs transaction network as well as the user‚Äôs properties to rate the user‚Äôs credit. With limited auxiliary knowledge (e.g., partial graphs, user attributes, and shadow datasets), a malicious attacker could successfully steal the user‚Äôs transaction network by query results of the credit rating system. Nowadays, numerous graph reconstruction attacks have been proposed to reveal the privacy leakage threat of GNN. Although most of them show satisfying attack performance in different attack scenarios respectively, they are still challenged by three objectives. (i) Generalizability: existing attack methods‚Äô performance is specific to a particular scenario but difficult to transfer to others, leading to significant attack degradation or even failure. (ii) Effectiveness: existing methods through similarity [6] [11] (i.e., similarity of node embeddings) and influence [31] (i.e., influence between nodes) tend to falsely determine node pairs without links, such as second-order neighbors, as having links due to the feature aggregation of GNN, leading to low attack AUC. (iii) Adaptation: existing link inference attack methods generally overlook the privacy-preserving defense for GNN, hindering their usability to practical defensive GNN. To address these challenges, we propose a novel generative framework for link stealing attack based on diffusion model, dubbed as DM4Steal. Since the diffusion model has shown outstanding performance in two aspects, i.e., permutation invariance and noise adaptation, we believe the model is able to accurately capture the permutation invariance of graphs intuitively. And it is able to reconstruct the original graphs from graphs with noise. Specifically, to tackle challenge (i), we propose a novel training strategy for the diffusion model, which enables the graph embeddings obtained from the target model to be used to train the diffusion model in addition to the node features, allowing it to be applied to the attack scenarios of the six species. To address challenge (ii), we utilize the score matching mechanism, whose inference process is not affected by the target model aggregation mechanism can achieve permutation invariance, instead of the previous similarity as well as influence mechanisms to perform link inference on the target graphs, thus reducing the impact of the GNN aggregation mechanism, and making the reversed graph more precise. For the challenge (iii), due to the addition of Gaussian noise to the target image during the forward process of diffusion models, this can overshadow the noise introduced by privacy protection mechanisms, weakening the impact of privacy protection noise and thereby enhancing the adaptability of DM4Steal. The main contributions of this paper are summarized as follows: ‚àô‚àô\bullet‚àô Problem: we propose six attack scenarios of link stealing attack on GNN by summarizing the auxiliary knowledge (i.e., node features, partial subgraph, and shadow datasets). These diverse scenarios cover possible practical threats of link stealing. ‚àô‚àô\bullet‚àô Approach: to the best of our knowledge, DM4Steal is the first link stealing attack method that utilizes a diffusion model to learn graph network topology and reduce instability caused by similarity or improper thresholds, thus it holds the advance of generality, effectiveness, and adaptiveness. ‚àô‚àô\bullet‚àô Strategy: in order to make DM4Steal applicable to all six attack scenarios, we proposed the graph diffusion model‚Äôs generation strategy to adaptively generate graphs based on different auxiliary knowledge. ‚àô‚àô\bullet‚àô Evaluation: extensive experiments on three GNNs over eight real-world datasets demonstrate that DM4Steal achieves the state-of-the-art (SOTA) attack performance. Moreover, It can conducts effective link stealing attack on defensive GNNs as well."
https://arxiv.org/html/2411.03363v1,TDDBench:‚ÄÇA Benchmark for Training data detection,"Training Data Detection (TDD) is a task aimed at determining whether a specific data instance is used to train a machine learning model. In the computer security literature, TDD is also referred to as Membership Inference Attack (MIA). Given its potential to assess the risks of training data breaches, ensure copyright authentication, and verify model unlearning, TDD has garnered significant attention in recent years, leading to the development of numerous methods. Despite these advancements, there is no comprehensive benchmark to thoroughly evaluate the effectiveness of TDD methods. In this work, we introduce TDDBench, which consists of 13 datasets spanning three data modalities: image, tabular, and text. We benchmark 21 different TDD methods across four detection paradigms and evaluate their performance from five perspectives: average detection performance, best detection performance, memory consumption, and computational efficiency in both time and memory. With TDDBench, researchers can identify bottlenecks and areas for improvement in TDD algorithms, while practitioners can make informed trade-offs between effectiveness and efficiency when selecting TDD algorithms for specific use cases. Our large-scale benchmarking also reveals the generally unsatisfactory performance of TDD algorithms across different datasets. To enhance accessibility and reproducibility, we open-source TDDBench for the research community.","Training Data Detection (TDD) (Shi et al., 2024), also known as Membership Inference Attack (MIA) in computer security literature (Shokri et al., 2017), aims to determine whether a specific data instance was used to train a target machine learning model. TDD has a wide range of applications. For example, it can be used to assess a model‚Äôs memorization of its training data and to audit the risks of data leakage (Carlini et al., 2022b). TDD has gained even more importance in the era of deep learning and large language models (LLMs), where models, often with billions of parameters, act as opaque black boxes. This raises the need to examine whether model owners have illegally utilized copyrighted material, such as books (Abd-Alrazaq et al., 2023), or personal emails (Mozes et al., 2023). Moreover, TDD contributes to discussions on machine learning accountability in the era of AI, as concerns grow over how these models handle sensitive data. As machine unlearning becomes increasingly employed to remove users‚Äô personal data from models, TDD serves as a critical tool to validate these unlearning processes (Chen et al., 2021; Kurmanji et al., 2024). Given the growing importance of TDD, several benchmarks have been developed to evaluate TDD algorithms (Niu et al., 2023; He et al., 2022; Duan et al., 2024). However, these benchmarks have several limitations: 1). Most evaluations primarily focus on TDD algorithms for image data, leaving other modalities like text and tabular data underexplored. 2). Many TDD methods developed in the past two years, particularly those focused on deep learning and LLMs, are not included in these benchmarks. 3). The effect of the target model (i.e., the model that was trained using the data) on TDD algorithms has not been thoroughly examined. 4). Current evaluations focus primarily on the detection performance of TDD algorithms, while practical considerations like efficiency, memory consumption, and other factors relevant to real-world deployment are often overlooked. Figure 1: TDDBench in downstream applications and the benchmarking of TDD algorithms. To address these limitations, we introduce TDDBench, a comprehensive framework for benchmarking TDD algorithms. Figure 1 provides an overview of TDDBench. The benchmark includes 13 datasets across three data modalities (tabular, text, and image) and evaluates 21 state-of-the-art TDD algorithms on 41 different target models, including the large language model Pythia-12B. We also categorize the 21 TDD algorithms into four types based on their algorithmic characteristics, including metric-based, learning-based, model-based, and query-based. Using this new benchmark, we conduct large-scale experiments to thoroughly assess TDD algorithms. Specifically, we aim to investigate: 1). The performance of TDD algorithms across various datasets and data modalities. 2). The impact of the target model on TDD algorithms. 3). The limitations and areas for improvement in TDD algorithms. 4). The performance of TDD algorithms from multiple perspectives, including detection performance, practicality, and efficiency in terms of time and memory usage. The large-scale experimental results reveal several key findings. First, there is a significant performance gap between different types of TDD algorithms, with model-based TDD methods generally outperforming the others. However, this outperformance comes at a cost, as model-based methods require building multiple reference models, leading to high computational expenses. Second, memorization of training data plays a crucial role in the performance of TDD algorithms, with larger target models‚Äîoften prone to memorization‚Äîexhibiting higher TDD success rates. Third, the performance of TDD algorithms is highly dependent on knowledge of the underlying target model architecture. Overall, our experiments show that there is no single best method across all scenarios, and notably, many TDD algorithms perform poorly on data modalities beyond images, indicating the need for further improvement in non-image domains. The main contributions of this paper are threefold: A novel and comprehensive TDD benchmark: We introduce TDDBench, a benchmark consisting of datasets across three modalities‚Äîimage, table, and text. We have open-sourced TDDBench for the research community at https://anonymous.4open.science/r/TDDBench-8078. New insights in TDD performance: By benchmarking 21 state-of-the-art TDD algorithms, we provide insights into recent advancements in TDD, including strategies for reducing reliance on model-specific knowledge and maximizing the benefits of greater computational resources. Multi-aspect metrics: Our comprehensive evaluation of TDD performance goes beyond simple detection accuracy to include practical considerations such as computational complexity, highlighting the trade-offs necessary for deploying TDD algorithms in real-world applications."
https://arxiv.org/html/2411.03357v1,PipeLLM: Fast and Confidential Large Language Model Services with Speculative Pipelined Encryption,"Confidential computing on GPUs, like NVIDIA H100, mitigates the security risks of outsourced Large Language Models (LLMs) by implementing strong isolation and data encryption. Nonetheless, this encryption incurs a significant performance overhead, reaching up to 52.8% and 88.2% throughput drop when serving OPT-30B and OPT-66B, respectively. To address this challenge, we introduce PipeLLM, a user-transparent runtime system. PipeLLM removes the overhead by overlapping the encryption and GPU computation through pipelining‚Äîan idea inspired by the CPU instruction pipelining‚Äîthereby effectively concealing the latency increase caused by encryption. The primary technical challenge is that, unlike CPUs, the encryption module lacks prior knowledge of the specific data needing encryption until it is requested by the GPUs. To this end, we propose speculative pipelined encryption to predict the data requiring encryption by analyzing the serving patterns of LLMs. Further, we have developed an efficient, low-cost pipeline relinquishing approach for instances of incorrect predictions. Our experiments on NVIDIA H100 GPU show that compared with vanilla systems without confidential computing (e.g., vLLM, PEFT, and FlexGen), PipeLLM incurs modest overhead (<<<19.6% in throughput) across various LLM sizes, from 13B to 175B.","Large Language Models (LLMs) are increasingly used across various applications (zheng2023judging, ; copilot, ). With the growth of open-source LLMs (touvron2023llama, ; jiang2024mixtral, ; zhang2022opt, ), companies are integrating and fine-tuning these models into their business operations. Due to LLM‚Äôs reliance on high-end GPUs, many businesses opt for outsourced services, such as cloud, attracted by their high availability and flexible pay-as-you-go models. However, these cloud infrastructures, often complex in nature, encompass a large Trusted Computing Base (TCB), which may contain vulnerabilities, both publicly reported and zero-day (li2021twinvisor, ; chen2023security, ). This poses security risks for LLMs, which are usually fine-tuned with proprietary data, and user prompts that contain sensitive business information. Thus, any data breach could expose critical business secrets. To mitigate these security threats, people introduce confidential computing. Confidential computing is designed to safeguard tenants‚Äô code and data against untrusted privileged software and rogue employees of cloud providers. The confidential virtual machine (CVM), supported by technologies such as Intel TDX (tdxmodule, ), AMD SEV (sevsnp, ), and ARM CCA (armcca, ), serves as a prime example of this. Any software external to a CVM is unable to access the code and data within it. Regarding machine learning workloads, people develop GPU enclaves to enhance security measures within GPUs (volos2018graviton, ; h100cc, ). A notable implementation of this is the NVIDIA H100 GPU (h100cc, ), which supports confidential computing inside the GPU to protect sensitive data and models from unauthorized access. Moreover, the data communication between the CVM and the GPU enclave is encrypted, further reinforcing the security of I/O operations. Although GPU confidential computing effectively enhances security for traditional small-scale AI models, it significantly undermines the performance of LLMs in throughput and latency. Our comprehensive experiments on NVIDIA H100 GPUs reveal that the GPU enclave can incur up to a 52.8% latency overhead on serving OPT-30B, a 36.2% throughput drop on fine-tuning OPT-30B, and an 88.2% throughput drop on serving OPT-66B (¬ß3). This overhead is largely due to a combination of memory swapping plus encryption. The swapping happens because LLMs consume a huge amount of GPU memory. For example, the OPT-66B model needs approximately 132GB of memory to store all its parameters, surpassing the 80GB memory of H100 GPUs. Moreover, runtime states such as the Key-Value cache (KV cache) (pope2022kv, ) during LLM inferences and activation during LLM training also consume significant GPU memory. Owing to the limited GPU memory, a GPU enclave has to dynamically swap out inactive parameters and/or runtime states to the main memory. This process requires encrypting the data transferred out of the GPU enclave. Correspondingly, the CPU cores must decrypt data received from the GPU, and re-encrypt it before sending it back to the GPU (¬ß2.2). However, the encryption and decryption pose a severe bottleneck due to the limited computational capability. This bottleneck significantly harms the overall performance, particularly in the context of LLMs. This paper introduces PipeLLM, a system designed to eliminate the performance overhead associated with GPU confidential computing for LLMs. Importantly, PipeLLM achieves this without requiring any changes to the existing LLM systems or the hardware, while still upholding the same level of security. The underlying principle of PipeLLM is straightforward yet effective: it decouples encryption tasks from the critical path of the memory swapping mechanism, by leveraging speculative pipelined encryption (¬ß4.3), a technique we proposed. Drawing inspiration from the concept of speculative execution in CPUs, PipeLLM anticipates which data blocks will be required by the GPU and pre-encrypts them. By doing so, PipeLLM significantly reduces the overhead of the GPU confidential computing by integrating predictions, encryptions, and data transfers into a pipeline. However, akin to CPU pipelining, an incorrect prediction could not only waste an individual pre-encrypted data but also invalidate the entire pipeline of subsequent pre-encrypted data. This is a consequence of the encryption scheme used by GPU enclaves, designed to prevent replay attacks (replayattack, ). In the H100‚Äôs confidential computing, data is encrypted using a private key in conjunction with a unique integer known as the Initialization Vector (IV). The IV is synchronized between the CPU and GPU, and increments by one with each encryption. Consequently, if an incorrect piece of data is encrypted in a pipeline, all subsequent IVs in the pipeline could become invalid, requiring re-encryption of the subsequent data with the correct IVs. We will elaborate on the encryption mechanism and IVs in ¬ß5.3. To address the challenge, we observe that LLM systems are highly predictable in swapping, allowing PipeLLM to accurately determine the sequence of data being swapped using heuristics. For instance, FlexGen (flexgen-paper, ) and PEFT (peft, ) compute the LLM through a layer-by-layer process, enabling PipeLLM to efficiently swap in layer parameters in their respective order. Similarly, vLLM (vllm-paper, ) uses simple swapping policies like FIFO (First-In, First-Out) and LIFO (Last-In, First-Out). PipeLLM can recognize these swapping policies and use them to predict the future swapping sequence. Moreover, PipeLLM incorporates several techniques to accelerate the pipeline and mitigate the cost of prediction errors. First, PipeLLM develops an efficient validation scheme to verify the correctness of a pre-defined ciphertext (¬ß5.2). Second, PipeLLM introduces request re-ordering and NOP padding to handle IV mismatches without relinquishing the entire pipeline (¬ß5.3). Finally, PipeLLM provides asynchronous decryption to accelerate data transfer (¬ß5.4). We implement PipeLLM with approximately 1K lines of code in C++. We conducted our performance experiments on an Intel server equipped with an NVIDIA H100-SXM GPU. The evaluation results show that PipeLLM significantly reduces the overhead associated with GPU confidential computing for LLM serving and fine-tuning, cutting it from as much as 88.2% to <<<19.6% in throughput, across various LLM sizes, ranging from 13 billion to 175 billion parameters. In summary, this paper makes the following contributions: ‚Ä¢ We conduct a comprehensive performance analysis of NVIDIA Confidential Computing on an H100 GPU enclave with LLM workloads. ‚Ä¢ We propose speculative pipelined encryption, an approach to greatly reduce the swapping overhead of confidential computing. It works well for LLM serving and fine-tuning. ‚Ä¢ We have built a system PipeLLM and evaluated its performance on multiple state-of-the-art LLM systems."
https://arxiv.org/html/2411.03355v1,Exploring Feature Importance and Explainability Towards Enhanced ML-Based DoS Detection in AI Systems,"Denial of Service (DoS) attacks pose a significant threat in the realm of AI systems security, causing substantial financial losses and downtime. However, AI systems‚Äô high computational demands, dynamic behavior, and data variability make monitoring and detecting DoS attacks challenging. Nowadays, statistical and machine learning (ML)-based DoS classification and detection approaches utilize a broad range of feature selection mechanisms to select a feature subset from networking traffic datasets. Feature selection is critical in enhancing the overall model performance and attack detection accuracy while reducing the training time. In this paper, we investigate the importance of feature selection in improving ML-based detection of DoS attacks. Specifically, we explore feature contribution to the overall components in DoS traffic datasets by utilizing statistical analysis and feature engineering approaches. Our experimental findings demonstrate the usefulness of the thorough statistical analysis of DoS traffic and feature engineering in understanding the behavior of the attack and identifying the best feature selection for ML-based DoS classification and detection.","Internet Service Providers (ISPs) around the globe continue to experience a dramatic growth in network attacks. Notably, the Denial of Service (DoS) is considered one of the most harmful attacks on networked systems. Such attacks cost an average of $22,000 for every minute of downtime they cause, resulting in an average loss of $120,000 per attack for small to medium-sized businesses [1]. Therefore, understanding DoS traffic behaviors and containing DoS attacks become necessary and substantial. Under DoS attacks, the network can become overwhelmed by the burst of traffic, rendering it unavailable to process legitimate service requests from clients or other network entities. DoS type of network anomalies is classified into exploitation and reflection attacks. The escalation of these anomalies on the Internet has remarkably shifted the attention of researchers/service providers to explore network threats deeply [2]. Various studies have been conducted to investigate network anomalies‚Äô potency and propose novel detection and mitigation mechanisms in the past [3, 4]. However, a noticeable problem remains challenging in this direction. It is related to the stealthiness of application layer DoS attacks, as this type of anomaly does not typically manifest at the network level, allowing these anomalies to evade the conventional network layer detection solutions. It is crucial to have an in-depth perception of network traffic behaviors and promptly detect and control network DoS attacks. Thus, this work tries to analyze the traffic behaviors of various DoS anomalies and distinguish them from normal network traffic for further lightweight detection. Specifically, we aim to leverage component analysis and ML techniques for efficient traffic analysis and detection of DoS attacks, the dominant security threat to the Internet. Most existing efforts rely on threshold-based heuristics and/or simple statistics, which cannot meet the real-time requirement of Internet systems, especially facing the ever-growing number and scales of different types of DoS attacks. We propose to fill this gap by properly deploying feature selection with efficient exploratory analysis of DoS behavior for resource-efficient characterization of Internet traffic flows. That is, we present an efficient methodology for network traffic flow characterization and classification models based on a broadly-used statistical method and new traffic data screening and selection. In addition to the commonly used flow-level features, we consider more informative metrics that capture traffic flow‚Äôs volume and velocity features to improve the analysis‚Äôs accuracy and precision. The rest of this paper is organized as follows. Section II discusses the research problem background and Section III summarizes the literature review. Next, the methodology used in this work is presented in Section IV. Section V presents the evaluation setups and key findings. Last, Section VI concludes this work."
https://arxiv.org/html/2411.03354v1,LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks,"In this paper, we present an adaptive framework designed for the continuous detection, identification and classification of emerging attacks in network traffic. The framework employs a transformer encoder architecture, which captures hidden patterns in a bidirectional manner to differentiate between malicious and legitimate traffic. Initially, the framework focuses on the accurate detection of malicious activities, achieving a perfect recall of 100% in distinguishing between attack and benign traffic. Subsequently, the system incrementally identifies unknown attack types by leveraging a Gaussian Mixture Model (GMM) to cluster features derived from high-dimensional BERT embeddings. This approach allows the framework to dynamically adjust its identification capabilities as new attack clusters are discovered, maintaining high detection accuracy. Even after integrating additional unknown attack clusters, the framework continues to perform at a high level, achieving 95.6% in both classification accuracy and recall. The results demonstrate the effectiveness of the proposed framework in adapting to evolving threats while maintaining high accuracy in both detection and identification tasks. Our ultimate goal is to develop a scalable, real-time intrusion detection system that can continuously evolve with the ever-changing network threat landscape.","Network systems are becoming increasingly complex due to ongoing technological advancements. Next generations of network are expected to enable various interactions and exchanges over the Internet, supporting services such as digital brain-computer interfaces, in-body health networks, and extended reality. This transition will be underpinned by achieving data rates around terabits per second with ultra-low latency. This continuous evolution and expansion of network infrastructure lead to an extremely heterogeneous environment characterized by massive volumes of data. This can complicate network monitoring, exposing systems to malicious activities. Furthermore, as new services emerge from network improvements, sophisticated threats are likely to increase correspondingly, becoming harder to detect [8]. Therefore, it is imperative to develop robust security measures within these highly heterogeneous ecosystems. Various machine learning methods have been explored to enhance security across diverse network environments, including smart grids [16], virtual private networks [13], and IoT ecosystems [14]. Among these methods, Support Vector Machines (SVMs) are widely utilized for their ability to classify data into distinct categories, making them effective for detecting intrusions and anomalies within network traffic. Boosting methods enhance the performance of weak classifiers by combining them into a strong ensemble classifier, thereby improving detection rates for malicious activities. Deep Neural Networks (DNNs) leverage multiple layers of neurons to learn intricate patterns in data, which proves particularly useful for identifying advanced threats in large-scale networks. Recurrent Neural Networks (RNNs) are specifically designed to analyze sequences of network packets, enabling the identification of patterns indicative of cyber threats. Long Short-Term Memory (LSTM) networks excel at detecting temporal patterns in network traffic and are frequently employed in applications such as detecting Distributed Denial of Service (DDoS) attacks [9]. In addition to their diversity, the data in these ecosystems are dynamic, presenting the challenge of encountering unknown data flows during analysis. Machine learning-empowered intrusion detection systems (IDSs) often struggle to keep pace with the constant evolution of network flows, as they are typically trained to detect a fixed or predefined set of attacks [20]. However, in real-world scenarios, intrusion data are collected incrementally. As new data emerge, these models may suffer from catastrophic forgetting, a prevalent issue in machine learning where previously learned classes are forgotten when training on new ones [7]. Recently, techniques involving LLMs have demonstrated an ability to effectively manage vast volumes of data while understanding the underlying context within them. Given the unknown pattern in next generation of networks, leveraging LLMs presents a promising approach for efficiently processing and analyzing such data. Studies have shown that LLMs can address various challenges in telecommunications, such as optimizing the reward process in reinforcement learning [6], and providing zero-shot image classification in complex signal transmissions [22]. However, it is important to note that LLMs were not initially designed for these applications; their primary utility lies in natural language processing (NLP). Consequently, directly applying them to domain-specific tasks can be challenging, necessitating fine-tuning for optimal performance. To the best of our knowledge, this work proposes the first hybrid incremental intrusion detection framework that leverages LLMs to address emerging threats. Our contributions in this work are as follows: ‚Ä¢ We investigate prior works related to intrusion detection, focusing on continuous detection. ‚Ä¢ We propose a framework that leverages language models to efficiently capture contextual information within network traffic. ‚Ä¢ We evaluate the continuous detection capability of our framework on a popular IDS dataset, highlighting its effectiveness."
https://arxiv.org/html/2411.03351v1,Tabular Data Synthesis with Differential Privacy: A Survey,"Data sharing is a prerequisite for collaborative innovation, enabling organizations to leverage diverse datasets for deeper insights. In real-world applications like FinTech and Smart Manufacturing, transactional data, often in tabular form, are generated and analyzed for insight generation. However, such datasets typically contain sensitive personal/business information, raising privacy concerns and regulatory risks. Data synthesis tackles this by generating artificial datasets that preserve the statistical characteristics of real data, removing direct links to individuals. However, attackers can still infer sensitive information using background knowledge. Differential privacy offers a solution by providing provable and quantifiable privacy protection. Consequently, differentially private data synthesis has emerged as a promising approach to privacy-aware data sharing. This paper provides a comprehensive overview of existing differentially private tabular data synthesis methods, highlighting the unique challenges of each generation model for generating tabular data under differential privacy constraints. We classify the methods into statistical and deep learning-based approaches based on their generation models, discussing them in both centralized and distributed environments. We evaluate and compare those methods within each category, highlighting their strengths and weaknesses in terms of utility, privacy, and computational complexity. Additionally, we present and discuss various evaluation methods for assessing the quality of the synthesized data, identify research gaps in the field and directions for future research.","Data sharing is essential as it drives innovative collaboration and enables informed decision-making across various domains. Numerous public data-sharing platforms, including Kaggle (kag, 2024), Data.gov (dat, 2024), and the UCI repository (Dua and Graff, 2019), offer access to extensive datasets, with the primary goal of facilitating knowledge discovery and advancement. In most applications, such as FinTech and Smart Manufacturing, these datasets are represented in tabular form, given their structured nature and widespread applicability across different fields. However, it is important to note that these datasets often contain sensitive personal/business data, which can raise significant privacy concerns. In addition, due to evolving privacy regulations, exemplified by recent legislation like the AI Act (AIa, 2024), there is a heightened need for innovative methods for data sharing that protect individual privacy while enabling meaningful data analysis. Data synthesis has been attracting growing attention due to its unique ability to generate synthetic data based on statistical information without being linked to specific individuals or identities. However, it is important to note that while synthetic data offers privacy protection, several studies (Dinur and Nissim, 2003; Garfinkel et al., 2019) have shown that the attacker can still potentially infer sensitive information about users. For example, Jordon et al. (Jordon et al., 2022) show that ‚ÄúSynthetic data can leak information about the data it was derived from and is vulnerable to privacy attacks.‚Äù Moreover, Stadler et al. (Stadler et al., 2020) have demonstrated that generative models trained without privacy safeguards offer limited defence against inference attacks when compared to the alternative of directly sharing the original data. A cutting-edge solution involves integrating provable privacy measures, such as differential privacy (DP), into the synthetic data generation process. Differential privacy aims to ensure that the information derived from the released synthetic data remains nearly identical, whether or not specific individuals were part of the original datasets, thus effectively preventing the inference of personal information. Importantly, it does not rely on assumptions about the capabilities of potential attackers, providing robust privacy protection even in the presence of adversaries with significant background knowledge and resources (Yang et al., 2022b). The United States National Institute of Standards and Technology (NIST) has been instrumental in championing data sharing and privacy protection. In 2018, NIST organized the ‚ÄúDifferential Privacy Synthetic Data Challenge‚Äù (NIS, 2018), a competition dedicated to advancing the field of differential privacy for generating synthetic data that retains the statistical characteristics of actual data while safeguarding individual privacy. The challenge highlighted the growing importance of balancing the need to share valuable insights with the necessity of protecting personal information, making differentially private data synthesis a promising research focus. In this paper, we present a comprehensive review of existing differential private tabular data synthesis methods. The generation of differentially private synthetic tabular data primarily falls into two key categories: statistical methods and deep learning-based methods. We delve into both approaches, analyzing their strengths and limitations under both centralized and distributed settings. Furthermore, we offer insights into the unique challenges and considerations that arise in each context. Table 1. Comparison with existing surveys Paper Year Consideration of tabular data synthesis with DP Centralized data synthesis Distributed data synthesis Discussion on Evaluation S-M DL-M Fidelity/Utility Privacy (Bowen and Liu, 2020) 2020 ‚úì ‚úì (Bourou et al., 2021) 2021 ‚úì ‚úì (Figueira and Vaz, 2022) 2022 ‚úì ‚úì (Ghatak and Sakurai, 2022) 2022 ‚úì ‚úì ‚úì ‚úì (Xing et al., 2022) 2022 ‚úì ‚úì ‚úì ‚úì (Lu et al., 2023) 2023 ‚úì ‚úì (Hassan et al., 2023) 2023 ‚úì ‚úì ‚úì (Hu et al., 2024) 2024 ‚úì ‚úì ‚úì (Bauer et al., 2024) 2024 ‚úì ‚úì Ours - ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì Differences between this survey and others. Currently, several synthesis surveys have been published, as shown in Table 1. Bourou et al. (Bourou et al., 2021) reviewed several popular GAN-based models for tabular intrusion detection system data synthesis and experimentally evaluated their performance. However, this review exclusively focused on GAN-based models, without considering other methods. Figueira and Vaz et al. (Figueira and Vaz, 2022) slightly extended the scope but still concentrated on GAN-based models for data synthesis. Xing et al. (Xing et al., 2022) provided a broader review, covering data synthesis methods for non-imaging medical datasets, including both tabular and sequential data. Their discussion included statistical and deep learning-based methods, as well as evaluation metrics. Lu et al. (Lu et al., 2023) explored machine learning-based approaches for data synthesis and applications of synthetic data generation, addressing privacy and fairness concerns related to synthetic data. However, all the aforementioned papers discuss pure synthetic data generation methods, and none of them consider the methods with differential privacy protection. Bowen and Liu (Bowen and Liu, 2020) conducted an experimental study on differentially private data synthesis methods, focusing solely on statistical approaches. Hassan et al. (Hassan et al., 2023) explored the intersection of synthetic data and differential privacy, with a primary focus on deep generative models. Bauer et al. (Bauer et al., 2024) conducted a comprehensive study of various model types suitable for synthetic data generation, including methods that incorporate differential privacy. Ghatak and Sakurai (Ghatak and Sakurai, 2022) considered the methods with differential privacy protection, but exclusively discussed data synthesis methods that emerged victorious in the NIST 2018 challenge. Hu et al. (Hu et al., 2024) provided a review of differentially private data synthesis, including tabular data. However, their approach was more of a simple summary of existing methods rather than an in-depth analysis and discussion. Furthermore, all existing surveys focus on centralized data synthesis methods and do not address distributed data synthesis. In our paper, we target tabular data synthesis methods with differential privacy protection under both centralized and distributed settings. Additionally, we summarize and discuss various evaluation methods for the generated synthetic data, focusing on fidelity, utility, and privacy. Contributions of this survey. This survey provides a comprehensive review of differential private data synthesis methods, focusing on tabular data. We consider two application scenarios: centralized data synthesis, where the data curator holds all users‚Äô datasets and aims to generate synthetic datasets for data analytics or sharing purposes, and distributed data synthesis, where data owners retain their data locally and collaborate with other parties for joint data synthesis. Our contributions are summarized as follows: ‚Ä¢ We provide a thorough and comprehensive overview of existing methods for differentially private tabular data synthesis, along with the evaluation techniques used to assess their performance and effectiveness. ‚Ä¢ Based on the synthetic data generation models, we categorize the primary approaches for data synthesis into two key research directions: statistical-based methods and deep learning-based methods, both applicable under two main scenarios: centralized and distributed data synthesis. ‚Ä¢ We provide an in-depth review and analysis of existing methods for generating synthetic data, highlighting strengths and weaknesses in capturing attribute dependencies, modeling the distribution of attributes, computational complexity, and the noise scales introduced during the model learning process, etc. ‚Ä¢ By analyzing the state-of-the-art in the field, we discuss the research gaps and identify several promising future research directions to address the emerging challenges an advance the domain of private tabular data synthesis. In this survey, we present the material in a tutorial manner, providing a clear introduction, comprehensive discussion, and valuable insights into the topics and methods. We aim to make the content accessible and informative for readers who are new to the subject as well as those looking to deepen their understanding. Roadmap. The rest of the paper is organized as follows: Section 2 provides background knowledge on tabular data synthesis and differential privacy. Section 3 and Section 4 discuss centralized data synthesis methods with differential privacy protection and distributed data synthesis methods with differential privacy protection, respectively. Section 5 introduces the synthetic data evaluation metrics. The research gaps and promising research directions are identified in Section 6, and the survey is concluded in Section 7."
https://arxiv.org/html/2411.03348v2,Undermining Image and Text Classification Algorithms Using Adversarial Attacks111The study was conducted during the Next Generation Stem Internship Program 2023 at Oak Ridge National Laboratory,"Machine learning models are prone to adversarial attacks, where inputs can be manipulated in order to cause misclassifications. While previous research has focused on techniques like Generative Adversarial Networks (GANs), there‚Äôs limited exploration of GANs and Synthetic Minority Oversampling Technique (SMOTE) in text and image classification models to perform adversarial attacks. Our study addresses this gap by training various machine learning models and using GANs and SMOTE to generate additional data points aimed at attacking text classification models. Furthermore, we extend our investigation to face recognition models, training a Convolutional Neural Network(CNN) and subjecting it to adversarial attacks with fast gradient sign perturbations on key features identified by GradCAM, a technique used to highlight key image characteristics of CNNs use in classification. Our experiments reveal a significant vulnerability in classification models. Specifically, we observe a 20% decrease in accuracy for the top-performing text classification models post-attack, along with a 30% decrease in facial recognition accuracy. This highlights the susceptibility of these models to manipulation of input data. Adversarial attacks not only compromise the security but also undermine the reliability of machine learning systems. By showcasing the impact of adversarial attacks on both text classification and face recognition models, our study underscores the urgent need for develop robust defenses against such vulnerabilities.","Machine learning algorithms have experienced an exponential surge in popularity due to their efficiency in making classifications and predictions. The algorithms have been incorporated into systems that are supporting real world applications, such as object recognition in self driving cars and cancer prediction in medical diagnoses. However, adversarial attacks can make these algorithms insecure and prone to incorrect predictions. An adversarial attack is an input provided to machine learning classifiers for the purpose of causing a misclassification. Past research shows the implications of adversarial attacks in image and text classifiers, demonstrating how adding specific perturbations to inputs result in a substantial decrease in model performance. In this study, we seek to analyze the types of inputs that fool classification models by utilizing Fast Gradient Sign Method (FGSM) perturbation vectors on the result of GradCAM highlighted features, GANs, and SMOTE to generate adversarial attacks. As a result, this study demonstrates the vulnerabilities of machine learning models to adversarial attacks using GANs and SMOTE. This paper presents a novel adversarial attack strategy that combines GANs and SMOTE to target text classifiers and a novel attack on image classifiers with FGSM and GradCAM. Our experiments work to validate the influence of these adversarial attacks against machine learning models deployed in real-world scenarios. The structure of this manuscript is as follows: Section II provides a review of the existing literature and contributions in the domains of GANs and adversarial attacks. Section III articulates the methodological framework employed in the current investigation. Section IV presents the experimental setup, alongside the resulting data and analysis. Finally, Section V offers a summary of the findings, encapsulates the study‚Äôs contributions, and outlines potential future research inquiries."
https://arxiv.org/html/2411.03346v1,Fixing Security Vulnerabilities with AI in OSS-Fuzz,"Critical open source software systems undergo significant validation in the form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased random search over the domain of program inputs, to find inputs which crash the software system. Such fuzzing is useful to enhance the security of software systems in general since even closed source software may use open source components. Hence testing open source software is of paramount importance. Currently OSS-Fuzz is the most significant and widely used infrastructure for continuous validation of open source systems. Unfortunately even though OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more software projects, the detected vulnerabilities may remain unpatched, as vulnerability fixing is often manual in practice.In this work, we rely on the recent progress in Large Language Model (LLM) agents for autonomous program improvement including bug fixing. This is also the first such study with large-scale vulnerability fixing on real projects to the best of our knowledge. We customise the well-known AutoCodeRover agent for fixing security vulnerabilities. This is because LLM agents like AutoCodeRover fix bugs from issue descriptions, via code search. Instead for security patching, we rely on the test execution of the exploit input to extract code elements relevant to the fix. Our experience with the vulnerability data from OSS-Fuzz leads us to many observations. We note that having autonomy in the LLM agent is useful for successful security patching, as opposed to approaches like Agentless where the control flow is fixed. More importantly our findings show that we cannot measure quality of patches by code similarity of the patch with reference codes (as in CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores still fail to pass given the given exploit input. Our findings indicate that security patch correctness needs to consider dynamic attributes like test executions as opposed to relying of standard text/code similarity metrics.","Security vulnerabilities are one of the major threats to modern software systems. Once exploited by malicious attackers, security vulnerabilities can cause significant damage to the software and its users, incurring financial loss, data breaches, and more. In 2023, 30,927 new Common Vulnerabilities and Exposures (CVEs) are recorded by the National Vulnerability Database (NVD), and half of these vulnerabilities were classified as high or critical severity (skybox-report, ). The number of new CVEs has increased by 17% compared to the previous year, underscoring the accelerated pace of vulnerability detection and the critical need for timely remediation. The recent advancement in automatic programming with generative AI could further exacerbate the security issues, since some parts of the application code could come from Large Language Models (LLMs) with little security assurance. To safeguard the software systems, researchers and practitioners have made advances in both vulnerability detection and remediation. To detect security vulnerabilities before they are discovered/exploited by attackers, various techniques from static analysis (semgrep, ; codeql, ) to fuzzing (libfuzzer, ; afl, ) have been developed and also adopted in the industry. Static analysis techniques can be applied to detect a wide range of vulnerabilities. However, they are known to report false-positive warnings since they are often based on abstraction and conservative approximation of the program semantics (guo2023mitigating, ). Fuzzing, on the other hand, employs a biased random search in the program‚Äôs input space and dynamically executes the program. The dynamic nature of fuzzing ensures that a reported bug is a true positive. Fuzzing has been employed by major software companies to continuously scan for vulnerabilities in their development process (chromium-security, ; onefuzz, ). Google‚Äôs OSS-Fuzz, announced in 2016, provides continuous fuzzing for various core open-source software (ossfuzz-announced, ). As of August 2023, OSS-Fuzz has identified over 10,000 vulnerabilities across 1,000 projects (ossfuzz, ). While vulnerability detection techniques like fuzzing have shown to be both mature and effective, detection is only the first step in comprehensive software protection. A detected bug should be patched as soon as possible to reduce the time of exposure and the risk of being exploited. A previous study in 2021 has shown that the median time-to-fix (i.e. time from bug reporting to patch verification) to be 5.3 days for bugs detected by OSS-Fuzz (ding2021empirical, ), and 10% of the reported bugs are not fixed within the 90-day disclosure deadline. The rising number of detected vulnerabilities in recent years may require developers to invest even more time and effort in manually patching them. There is an urgent need for automated vulnerability remediation in continuous fuzzing pipelines to both ease the developers‚Äô workload and minimize the window of vulnerability exposure. Recent advancements in generative AI and LLM agents have shown promise in autonomous vulnerability remediation in programs (zhang2024acr, ; ruan2024specrover, ; yang2024sweagent, ; xia2024agentless, ). These LLM agents are designed for general software engineering tasks, including bug fixing and feature development. They operate in real-world scenarios where tasks are described by users in natural language. Using the task description and the software codebase as inputs, the agents generate code modification suggestions to fulfill the specified requirements. Since repairing security vulnerabilities is a specialized software engineering task, we hypothesize that with appropriate adaptation, general-purpose LLM agents for software engineering can be repurposed for this task. These repurposed agents can potentially be integrated into existing vulnerability detection pipelines such as fuzzing, where they can provide the remediation after detection and complete the software protection cycle. CodeRover-S In this paper, we present a large scale real-world study on using LLM agents for security vulnerability repair. To enhance the realism of our effort, we use as dataset the OSS-Fuzz projects, which seek to enhance the state of practice of open source security (ossfuzz, ). We repurposed the open-source LLM agent AutoCodeRover (zhang2024acr, ; ruan2024specrover, ) to repair security vulnerabilities, and implemented a version named CodeRover-S (i.e. AutoCodeRover for security). With the vulnerability report and an exploit input produced by a fuzzing campaign, CodeRover-S autonomously generates patches that fix the detected vulnerability. In the process of adapting LLM agents for vulnerability repair, we identified that one of the main challenge was the insufficient information contained in the auto-generated vulnerability report. Unlike human-written issue report for general software engineering tasks, vulnerability reports are often auto-generated by the fuzzer and only contain information like the bug type and crash stacktrace. To enrich the vulnerability report, we extract dynamic call graph information from the exploit input found by fuzzing, which is then used to augment the report generated by the fuzzer. In addition, we perform a type-based analysis at the program locations identified as faulty by the agent, and use the additional type information to improve the compilation rate of generated patches. With these adaptation, we built CodeRover-S which can autonomously repair security vulnerabilities detected in a fuzzing pipeline. To evaluate the efficacy of CodeRover-S in a real-world setup, we conducted experiments on real C/C++ vulnerabilities previously detected by OSS-Fuzz. Each detected vulnerability comes with an exploit input that resulted in a crash from sanitizers (e.g. AddressSanitizer (asan, ), MemorySanitizer (msan, )), and the crash report generated by the sanitizer. Experiments on 588 real-world vulnerabilities from a previously curated dataset (mei2024arvo, ) show that CodeRover-S can repair 52.4% of these vulnerabilities by resolving the crash from the exploit input. We also comparatively study the efficacy of other deep learning or LLM-based systems in this realistic vulnerability repair scenario. Firstly, we apply a general-purpose LLM coding agent directly to the vulnerability repair setting and observe a lower repair efficacy. Secondly, we evaluate the state-of-the-art deep learning based vulnerability repair system VulMaster (zhou24vulmaster, ) in the OSS-Fuzz dataset. Existing deep learning based vulnerability repair techniques often make strong assumptions such as the perfect fix location is provided at either function- or line-level. This localization assumption is too strong for real-world vulnerability repair, and we observe a low repair efficacy when they are used in a realistic repair setup. Furthermore, current evaluation of vulnerability repair tools often focus on how closely the generated patches match the developer‚Äôs patch, using metrics like exact match or similarity scores such as BLEU (papineni2002bleu, ) or CodeBLEU (ren2020codebleu, ). However, we find that these metrics may not accurately reflect the true efficacy of the repairs. Therefore, it is important to assess vulnerability repair systems on datasets with executable inputs in future research. In summary, our contributions are as follows: ‚Ä¢ We explore the feasibility of adapting general-purpose LLM programming agents for the repair of security vulnerabilities. We integrate call graph information and type-based analysis to provide richer context for LLM agent-based vulnerability repair, resulting in improved patch quality. Our approach is implemented as a new agent CodeRover-S which is specialized for security vulnerability repair. ‚Ä¢ We conduct an empirical study on the use of LLM agents to repair real-world security vulnerabilities identified by the industrial fuzzing service OSS-Fuzz. Our findings indicate that leveraging LLM agents for vulnerability remediation is a promising approach to complement existing detection pipelines and complete the software protection life cycle. ‚Ä¢ We present empirical evidence suggesting that similarity scores may not accurately measure the effectiveness of vulnerability repair systems. To better evaluate learning-based vulnerability repair systems, future research should consider test-based validation methods."
https://arxiv.org/html/2411.03327v1,"Maximal Extractable Value in Decentralized Finance: Taxonomy, Detection, and Mitigation","Decentralized Finance (DeFi) leverages blockchain-enabled smart contracts to deliver automated and trustless financial services without the need for intermediaries. However, the public visibility of financial transactions on the blockchain can be exploited, as participants can reorder, insert, or remove transactions to extract value, often at the expense of others. This extracted value is known as the Maximal Extractable Value (MEV). MEV causes financial losses and consensus instability, disrupting the security, efficiency, and decentralization goals of the DeFi ecosystem. Therefore, it is crucial to analyze, detect, and mitigate MEV to safeguard DeFi. Our comprehensive survey offers a holistic view of the MEV landscape in the DeFi ecosystem. We present an in-depth understanding of MEV through a novel taxonomy of MEV transactions supported by real transaction examples. We perform a critical comparative analysis of various MEV detection approaches, evaluating their effectiveness in identifying different transaction types. Furthermore, we assess different categories of MEV mitigation strategies and discuss their limitations. We identify the challenges of current mitigation and detection approaches and discuss potential solutions. This survey provides valuable insights for researchers, developers, stakeholders, and policymakers, helping to curb and democratize MEV for a more secure and efficient DeFi ecosystem.","Decentralized Finance (DeFi) [1], powered by blockchain technology, offers financial services, such as investing, lending loans, and trading assets, to various stakeholders without the need for intermediary brokers. This creates a decentralized, secure, transparent, and traceable financial ecosystem. According to a Skyquest report, the DeFi market size is expected to reach 48.02 billion USD by 2031, up from 23.99 billion USD in 2023, a compound annual growth rate of 9.06%111https://www.skyquestt.com/report/decentralized-finance-market, accessed on 19 September 2024. Furthermore, as of September 2022, the total value locked in the DeFi ecosystem exceeded 82 billion USD222https://defillama.com/, accessed on 19 September 2024, with the Ethereum network accounting more than 45 billion USD (approximately 56%). Decentralized Exchanges (DEXes) are one of the prominent applications of DeFi, enabling users to directly swap tokens using smart contracts [2]. Most DEXes operate on the Ethereum blockchain to facilitate trustless and automated transactions. However, in the Ethereum network, transactions are stored in a public mempool before they are included in a block. This creates profitable opportunities for network participants, referred to as searchers, who can submit new transactions by observing pending financial transactions to gain additional revenue. These searchers maximize their profits by manipulating gas prices to influence the order of their transactions within the block. This additional value extracted from the blockchain network is termed Maximal Extractable Value (MEV). MEV searchers can be the block producers (i.e., miners in Proof of Work (PoW) and validators in Proof of Stake (PoS)), other network participants, or a bot. However, block producers have a unique advantage, as they can include transactions in blocks without paying high gas prices [3]. Furthermore, the transition from PoW to PoS in Ethereum has significantly reduced block producer rewards, further luring block producers to engage in MEV activities [4]. MEV could result in major financial losses, network congestion, increased gas prices, and blockchain inefficiency [3, 5]. Before Ethereum‚Äôs transition to PoS in September 2022, around 440,000 ETH in MEV was extracted. Since the transition, approximately 180,000 ETH has been extracted up until May 2023333https://milkroad.com/guide/mev/, accessed on 19 September 2024. However, not all MEV transactions are detrimental to the DeFi ecosystem. Some can destabilize consensus mechanisms and cause economic harm, while others may even stabilize financial markets [6]. Therefore, understanding, detecting, and mitigating MEV is critical to ensure the efficiency and stability of the DeFi ecosystem. Several surveys have explored various aspects of MEV, including transaction types and mitigation strategies [7, 8, 6, 5, 9]. In particular, [7] classifies MEV transactions, while [8, 6] emphasize on mitigation techniques. In contrast, [5, 9] cover both transaction types and mitigation strategies. However, none have thoroughly examined all transaction types, detection approaches, and mitigation strategies. This comprehensive survey addresses this gap by presenting a novel taxonomy of MEV transaction types supported by real-world examples from the Ethereum network. It also explores MEV detection approaches and mitigation strategies and examines various simulation and extraction methods. The main contributions of this survey are as follows. ‚Ä¢ We introduce a novel and comprehensive taxonomy of MEV transactions, supported by real Ethereum transaction examples for each identified MEV type. The taxonomy clearly distinguishes between value-diverting MEV transactions, which can lead to financial loss and network instability, and value-creating MEV transactions, which can stabilize markets or enhance efficiency. This distinction provides clarity on the dual nature of MEV activities and lays the foundation for discussions on mitigation and detection strategies. ‚Ä¢ We present a critical comparative analysis of the various MEV detection approaches proposed for different types of MEV transactions, highlighting their effectiveness in identifying these transaction types. ‚Ä¢ We provide an in-depth analysis of various MEV mitigation strategies, such as transaction ordering solutions, privacy-preserving public pools, and private pools. By identifying the specific limitations of these strategies, we offer a critical assessment of their real-world applicability and potential areas for improvement. ‚Ä¢ We explore existing MEV simulation frameworks and extraction methods, offering a comprehensive overview of how these tools model and replicate real-world MEV scenarios. ‚Ä¢ We critically examine the major challenges in MEV mitigation and detection, such as centralization, latency, layer-2 MEV, and multi-address MEV. Furthermore, we discuss potential solutions paving the way for a more secure and efficient DeFi system. The remainder of this article is organized as follows. Section 2 provides essential background on Ethereum, DEXes, and MEV, enabling readers to gain a clearer understanding of the key concepts. Section 3 reviews existing related surveys on MEV, highlighting gaps in the literature. In Section 4, we describe the methodology used to conduct this survey. Section 5 introduces and synthesizes the proposed taxonomy of MEV transactions. Sections 6 and 7 focus on MEV detection approaches and mitigation strategies, respectively. Section 8 explores MEV simulation and extraction methods. Section 9 addresses the challenges associated with MEV mitigation and detection, along with potential solutions. Finally, Section 10 concludes the survey."
https://arxiv.org/html/2411.03892v1,Two Sides of the Same Coin: Large-scale Measurements of Builder and Rollup after EIP-4844,"Web3 is reshaping decentralized ecosystems through innovations like Ethereum. Recently, EIP-4844 is implemented in Ethereum to support its Layer-2 scaling solutions, which introduces a new 128 KB data structure called blob. This upgrade incorporates type-3 transactions with blobs to verify data availability and reduce gas costs for rollups, significantly affecting the strategies of both builders and rollups. In this paper, we present an in-depth study of emerging strategies in builder and rollup markets after EIP-4844, containing hundred million transactions. We find that the efficiency of builder and rollup strategies is interdependent, akin to two sides of the same coin‚Äîboth cannot be optimized simultaneously. That is, when builders operate efficiently, rollups tend to overpay in fees, conversely, when rollups optimize their costs, builders may incur losses in inefficient transaction selection. From the side of builders, our results show that 29.48% of these blocks have been constructed inefficiently, which does not produce sufficient profits for builders. Through our evaluation from the side of rollups, we find that over 72.53% of type-3 transactions pay unnecessary fees, leading to notable economic costs of rollups. Our work provides critical insights into optimizing block construction and transaction strategies, advancing the economic efficiency and data scalability of Web3 infrastructures, yet, much like balancing a seesaw, the efficiency of builders and rollups cannot be optimized concurrently.","Web3 leverages decentralized technologies such as blockchain to enable peer-to-peer interactions without intermediaries. A groundbreaking and transformative application of decentralized technology is Ethereum, an open and decentralized blockchain platform that facilitates the development and execution of smart contracts and decentralized applications (Gilbert, 2022; Wang et al., 2022b). In the Ethereum mainnet, the Proposer Builder Separation (PBS) mechanism was proposed to separate the role of the miner by introducing the builder and the proposer (Buterin, 2021a). The task of builders is to construct the block, where they sort the transactions to maximize the extractable value and submit the block to proposers (Daian et al., 2020; Heimbach and Wattenhofer, 2022). Each transaction within the block originates from two sources: the public mempool, which is accessible to everyone, and private channels that deliver transactions directly to specific builders (Qin et al., 2022). Ethereum Layer-2 refers to scaling solutions built on top of the Ethereum mainnet to enhance its performance and throughput, with rollups being one of the most prominent approaches that bundle multiple transactions into a single batch before submitting them to the mainnet. Ethereum has implemented EIP-4844 in the Dencun upgrade to improve network scalability and lower transaction costs (Ethereum.org, 2024a). This upgrade presents blob, a new data format of 128 KB designed to facilitate rollups on Ethereum‚Äôs Layer-2, alongside blob-carrying transactions known as type-3 transactions (Vitalik Buterin, 2023). By offloading data storage to blobs, the gas fee is significantly reduced for rollups. Type-3 transactions are much larger and therefore take more space in blocks (Park et al., 2024; Gomez, 2024b). Blocks with large size take longer to propagate through the network. On the one side, builders need to balance the block size with the profit they can obtain (Wahrst√§tter, 2024b; Gomez, 2024a). They also need to decide whether to choose type-3 transactions in their blocks. This results in different behaviors among builders in response to type-3 transactions. On the other side, rollups need to issue their blobs into type-3 transactions and wait for being included in blocks. Type-3 transactions always carry the different number of blobs and different fees, which indicate various strategies of rollups (Cui, 2024). Evidently, the strategies among rollups are also complicated. We evaluate the strategies of builders and rollups after EIP-4844. We find that the efficiency of their strategies is like the two sides of the same coin. When the builders strategy is efficient, rollups may need to pay more fees. Oppositely, when rollups adopt the efficient strategy and offer lower fees, builders may face block profits lower than potential profits. In the side of builders, we first examine the new strategy that builders employed after EIP-4844 (Section 4). Although the average total block size has been increasing to nearly 400 KB, the size of transactions from public mempool in the block has decreased from 150 KB before the upgrade to about 30 KB. Builders have taken to shrinking the size of the rest of the block to contain type-3 transactions, which prompts us to delve deeper into the efficiency of this strategy. We recognize that the builder strategy is not efficient enough after EIP-4844. In the side of rollups, they need to pay higher fees to ensure inclusion in efficient blocks. However, we find that most type-3 transactions issued by rollups pay too high fees (Section 5). Our primary contributions are: (1) We build a large-scale dataset to quantify builders and rollups market in Ethereum from after EIP-4844 to August, which includes 319,529,950 transactions with 1,336,822 type-3 transactions. To the best of our knowledge, we are the first to illuminate different strategies and efficiencies of builders and rollups in the network after EIP-4844. Our dataset provides the most complete labelling of builders and rollups so far. (2) We identify the strategies that different builders adopt after EIP-4844. By examining the size of transactions versus the profit paid to builders, we provide a model for measuring efficient blocks for builders. We find that 29.48% of the blocks containing type-3 transactions do not give the builder enough profits to compensate for the loss of excluding other transactions. This will assist builders in improving their strategies for constructing blocks for higher profit margin. (3) We explore the sending patterns of different rollups for blob and type-3 transactions and find that the behavior within the market is distinctive. We measure efficient type-3 transactions for rollups and observe that 72.53% of type-3 transactions in the market are inefficient. This work contributes to how rollups can optimize their pricing of type-3 transactions to achieve cost savings. (4) Furthermore, we evaluate the losses of other flawed strategies in the market and reveal significant losses of 186.92 ETH for rollups. We also characterize the impact of one misuse of blobs. It induces a delay of over 19 seconds in the type-3 transactions of rollups."
https://arxiv.org/html/2411.03862v1,ROBIN:Robust andInvisible Watermarks for Diffusion Models with Adversarial Optimization,"Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods.","Diffusion models (DMs) are revolutionizing content creation and generating stunningly realistic imagery across diverse domains [17, 33, 60]. The advent of text-to-image diffusion models [31, 30, 58], coupled with personalized generation techniques [53, 7, 32, 15, 41, 59], enables the creation of highly specific content by virtually anyone. However, it has raised concerns about authenticity and ownership, including the risk of plagiarism [34, 22] and the potential misuse of images of public figures [39, 5]. Consequently, governments and businesses are increasingly advocating for robust mechanisms to verify the origins of generative content [19, 45]. Watermarking offers a proactive approach to authenticate the source of generated content. This technique embeds imperceptible secret messages within the generated content. These messages serve as unique identifiers, confirming the image‚Äôs origin while remaining invisible to the human eye. They also need to be robust enough to withstand potential distortions encountered during online sharing. Existing watermarking techniques face a significant challenge in striking a balance between concealment and robustness. Traditional post-processing methods [46, 9] employ an empirical approach to identify an invisible and robust watermark and embed it within the generated image. They passively achieve concealment by limiting the watermark strength, consequently compromising robustness. Conversely, stronger watermarks, while enhancing robustness, can introduce visible artifacts into the generated image. Recent advancements in in-processing watermarking for diffusion models expect the generative model to learn this balance and directly produce watermarked content. However, these methods often require expensive model retraining [55, 48, 13] or can lead to unintended semantic alterations within the generated images [44]. Our ROBIN scheme introduces an explicit watermark hiding process to actively achieve concealment. This approach reduces the invisibility limitation of the watermark itself and thus enables the embedding of more robust watermarks. Specifically, we implant a robust watermark within an intermediate diffusion state, and then directionally guide the model to gradually conceal the implanted watermark, thus achieving invisibility in the final generated image. In this way, robust watermarks can be secretly implanted in the generated content without model retraining. We focus on the text-to-image diffusion models, which support an additional prompt signal to guide the generation process. We employ an adversarial optimization algorithm to design an optimal prompt guidance signal specifically tailored for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, and the watermark is optimized to achieve maximum strength. The optimized watermark and prompt signal are universally applicable to all images. During the generation process, the watermark is implanted within an intermediate state following the semantic formation stage. Subsequently, the optimized prompt guidance signal is introduced throughout the remaining diffusion steps. After image generation, following previous works [44, 49], we reverse the diffusion process to the watermark embedding point to verify the existence of the watermark. This innovative approach offers a promising way to overcome the trade-off between watermark strength and stealth by explicitly introducing an additional watermark hiding process. In summary, our key contributions are as follows: ‚Ä¢ We propose a novel watermarking method for diffusion models that embed a robust watermark and subsequently employ an active hiding process to achieve imperceptibility. ‚Ä¢ We develop an adversarial optimization algorithm to generate a prompt signal for watermark hiding and a strong watermark that can be hidden and strategically select the watermarking point within the diffusion trajectory. ‚Ä¢ Evaluations on both latent and image diffusion models demonstrate that our scheme exhibits superior robustness against various image manipulations while preserving semantic content."
https://arxiv.org/html/2411.03861v1,FedRISE: Rating Induced Sign Election of Gradients forByzantine Tolerant Federated Aggregation,"One of the most common defense strategies against model poisoning in federated learning is to employ a robust aggregator mechanism that makes the training more resilient. Many of the existing Byzantine robust aggregators provide theoretical guarantees and are empirically effective against certain categories of attacks. However, we observe that certain high-strength attacks can subvert the aggregator and collapse the training. In addition, most aggregators require identifying tolerant settings to converge. Impact of attacks becomes more pronounced when the number of Byzantines is near-majority, and becomes harder to evade if the attacker is omniscient with access to data, honest updates and aggregation methods. Motivated by these observations, we develop a robust aggregator called FedRISE for cross-silo FL that is consistent and less susceptible to poisoning updates by an omniscient attacker. The proposed method explicitly determines the optimal direction of each gradient through a sign-voting strategy that uses variance-reduced sparse gradients. We argue that vote weighting based on the cosine similarity of raw gradients is misleading, and we introduce a sign-based gradient valuation function that ignores the gradient magnitude. We compare our method against 8 robust aggregators under 6 poisoning attacks on 3 datasets and architectures. Our results show that existing robust aggregators collapse for at least some attacks under severe settings, while FedRISE demonstrates better robustness because of a stringent gradient inclusion formulation. Code is available at https://github.com/anonymous/","Federated learning (FL) [18] has emerged as a vital tool for leveraging decentralized data, especially in situations with strict data storage and sharing regulations, such as medical institutions, financial organizations, and other entities managing data. The main goal of regulations like GDPR is to protect people‚Äôs privacy and give them control over how their personal data is stored and shared, helping to guard against security risks for both individuals and countries. Because of this, the data exists in isolated silos inaccessible to the external world, making collaboration challenging. Despite these inherent challenges, there are potential benefits for organizations in improving the quality of models while safeguarding data privacy and security. In healthcare, sensitive patient information is often stored separately by various institutions distributed across different geographical locations and subject to varying legislative frameworks. Yet collaboration is essential for improving accuracy across diverse sub-populations and a broader range of diseases and conditions. In aerial imagery analytics, federated learning allows collaboration among operators of remote sensing spacecrafts. This enables aggregating insights in geographically dispersed geospatial data while adhering to jurisdictional data governance protocols. This is crucial to improve precision across heterogeneous terrains and operational contexts. FL solves the challenges involved in collaborative learning by sharing statistical updates/models learned on data instead of sharing actual data with other clients or servers, thus mitigating systemic privacy risks by enabling a strong layer of abstraction[10]. When members of consortium111consortium refers to a set of organizations/entities participating in a cross-silo federated learning procedure. collaboratively develop ML systems, they become a potential target for malicious actors. These actors may include third parties seeking to disrupt the system through sophisticated man-in-the-middle style attacks, intercepting and altering transmissions. Otherwise, competing entities within the consortium may also seek to undermine or sabotage the alliance to gain a competitive advantage in the market. We refer to these agents/clients that defect intentionally or unintentionally as Byzantines. Model poisoning is an extensively studied threat model in FL, where a Byzantine sends in corrupt or tailored updates to derail the entire training process. Certain entities wishing to sabotage the collaboration for competitive gains might initiate certain model performance attacks [9]. Robust aggregators (RAggr) are a fundamental defense strategy to make training resilient to Byzantine attacks. Such preemptive defense measures are necessary to ensure secure and mutually beneficial collaboration. Issues with Existing Approaches. Though many RAggrs have been proposed for FL, fundamental issues exist when they are adapted for practical use. (1) Middle-seeking [6, 35, 19] and outlier-suppression [1] methods by their design consider only a small subset of clients while suppressing/discarding the rest, using central tendency or mutual deviation of clients. These approaches do not leverage the richness of heterogeneity, a primary motive for collaboration, and become highly sub-optimal in the presence of heterogeneity even without Byzantines. In cross-silo FL, having few clients with diverse distribution, if the server consistently excludes certain clients due to heterogeneity, the gradients will not represent the entire distribution and will diverge far away from true gradients. Furthermore, once the data distribution is known, attacks can be crafted to hide among the variance of honest clients [27], especially under the assumption of an omniscient attacker. This is mainly because of the underlying assumption that larger deviations are due to maliciousness when, in fact, they could be due to large heterogeneity. (2) As an alternative to excluding clients, variance-reduction approaches like trimmed mean [35], centered-clipping [12], and bucketing [11, 23, 2] try to reduce the variance bounds of gradients, thereby curtailing the extreme malicious updates. They include gradients from clients in the tail distributions, thus enabling access to richer gradients. Clipping only considers radius (L2-norm), making it oblivious to the angular/sign variance of update vectors (see Fig. 1); thus, they need to be used in conjunction with other strategies. Bucketing helps in controlling the variance before clipping [11] and provides isolation among clients with angular variations [23]. Although the intensity of malicious gradients is reduced, the direction of the update is still perturbed, which can easily overpower the direction when aggregated with weak honest updates, leading to poisoning. Especially in the cross-silo FL, with few number of clients; each bucket may end up with a malicious vector corrupting the outputs of aggregation. (3) Finally, these methods often rely on certain hyperparameters (e.g., making initial judgments on the number of Byzantines). To determine the hyperparameters, some methods require knowledge of variance among updates or to be empirically searched for a given data distribution and setup. Practically, it is infeasible to undertake multiple trials with the established consortium. Moreover, the Byzantines can very well skew these estimates from the beginning. Figure 1: Similarity Scores between Benign & Poisoned Gradients: To identify Byzantine, a reliable Gradient Valuation Function is needed that can distinguish between honest and attack gradients. But the usual measures like L2-norm and cosine similarity yield high similarity between those two, as shown for Fang and ALIE attacks. *Signum-similarity (i.e. Cosine on signum of gradients) gives better separability compared to the other metrics. Displayed gradients are values at evenly spaced coordinates taken from ResNet18 trained on FL split CIFAR10. In our work, we address training-time model poisoning attacks by an omniscient attacker that significantly degrades performance under a cross-silo setting, resulting in a denial of service. We specifically focus on developing an aggregation protocol [31, 36, 25] that is robust to high-strength attacks, even with close to the majority of Byzantines in the consortium. The only assumption we impose is that the number of Byzantines will be less than 50%percent5050\%50 %, which is a common assumption across most of the RAggrs. Our main contributions are: ‚Ä¢ We propose FedRISE robust aggregation function to determine signs of individual gradients using variance-reduced sparse gradients. For sign election, we use weighted voting based on gradient quality (rating). The rating is calculated for each aggregation round independent of the previous round. ‚Ä¢ We evaluate existing popular RAggrs against established state-of-the-art attacks and show that all methods are susceptible to severe failure under harsher attack settings. Our experiments show that FedRISE is more resilient in handling attacks with varying objectives. ‚Ä¢ FedRISE uses only two hyperparameters for aggregation (sparsification Œ≥ùõæ\gammaitalic_Œ≥ and server momentum Œ≤r‚Å¢asubscriptùõΩùëüùëé\beta_{ra}italic_Œ≤ start_POSTSUBSCRIPT italic_r italic_a end_POSTSUBSCRIPT) that are minimally dependent on client counts, training settings, and data distribution. Figure 2: The steps involved in FedRISE are outlined here; please refer Sec. 4.1 for more explainations. FedRISE determines the ideal sign (Œæ)ùúâ(\xi)( italic_Œæ ) needed to ensure convergence at individual gradient locations based on variance-reduced sparse gradients (ùêØ)ùêØ(\mathbf{v})( bold_v ). During sign election, each client is given weightage (œ±)italic-œ±(\varrho)( italic_œ± ) in voting based on the quality of their gradients with respect to other clients measured using Gradient Valuation Function shown in Eq. 9. Only individual gradients aligning with the computed sign Œæùúâ\xiitalic_Œæ are averaged location-wise, while others are ignored. Thus, FedRISE accommodates useful gradients from heterogeneous clients while filtering out only those that are poisoned or contradict the optimization. Results on heterogenous datasets are shown in Fig. 3"
https://arxiv.org/html/2411.03814v1,MRJ-Agent: An Effective Jailbreak Agent for Multi-Round Dialogue,"Large Language Models (LLMs) demonstrate outstanding performance in their reservoir of knowledge and understanding capabilities, but they have also been shown to be prone to illegal or unethical reactions when subjected to jailbreak attacks. To ensure their responsible deployment in critical applications, it is crucial to understand the safety capabilities and vulnerabilities of LLMs. Previous works mainly focus on jailbreak in single-round dialogue, overlooking the potential jailbreak risks in multi-round dialogues, which are a vital way humans interact with and extract information from LLMs. Some studies have increasingly concentrated on the risks associated with jailbreak in multi-round dialogues. These efforts typically involve the use of manually crafted templates or prompt engineering techniques. However, due to the inherent complexity of multi-round dialogues, their jailbreak performance is limited. To solve this problem, we propose a novel multi-round dialogue jailbreaking agent, emphasizing the importance of stealthiness in identifying and mitigating potential threats to human values posed by LLMs. We propose a risk decomposition strategy that distributes risks across multiple rounds of queries and utilizes psychological strategies to enhance attack strength. Extensive experiments show that our proposed method surpasses other attack methods and achieves state-of-the-art attack success rate. We will make the corresponding code and dataset available for future research. The code will be released soon.","LLMs (e.g. GPT-4) are increasingly being deployed in various applications, including critical decision-making applications. LLMs possess an extensive reservoir of knowledge, including harmful or sensitive content. Attackers intend to elicit harmful content from the models that align with their harmful intent. Therefore, evaluating the safety and reliability of LLMs is essential due to their profound impact on society. Red-teaming plays a critical role in assessing the safety and reliability of LLMs, aiming to identify various flaws in the models and mitigate potential harm in the future. Most red teaming efforts focus on designing single-round attack prompts. For example, some attack methods (Zou et al. 2023a; Wei, Haghtalab, and Steinhardt 2024) hide malicious queries in forms that are not immediately recognizable, such as encoding harmful queries into ciphertext (Yuan et al. 2023) or ASCII codes (Jiang et al. 2024) to prompt the model to generate harmful responses. Subsequent works (Zhu et al. 2023; Yu, Lin, and Xing 2023; Kang et al. 2023; Yuan et al. 2023; Chao et al. 2023a) optimize their attack prompts in either a black-box or white-box manner to elicit model responses to harmful prompts. However, as emphasized in (Ma et al. 2024; Perez et al. 2022), existing methods overlook a crucial aspect: in real-world scenarios, human-LLM interactions are inherently multi-round. Consequently, relying solely on single-round attacks to model these interactions lacks practical significance and fails to capture the complex dynamics between users and LLMs. Recent advancements in multi-round dialogue strategies have demonstrated promising results in red-teaming attacks, effectively exploiting the sequential nature of interactions to conceal harmful intents. For instance, the approach proposed in (Zhou et al. 2024) aggregates responses from each round and then reverses the process to yield harmful content. Alternatively, other methods (Russinovich, Salem, and Eldan 2024; Yang et al. 2024) employ iterative trial-and-error tactics to induce language models to generate unsafe outputs. However, a key limitation of existing multi-round dialogue attack methods is their reliance on powerful language models like GPT to launch the attack, which may trigger the models‚Äô safety mechanisms and result in rejected requests, thereby reducing the attack‚Äôs efficiency. Furthermore, while existing methods have demonstrated multi-round attack formats, they lack a core quantifiable and reusable mechanism, which is crucial for advancing the field‚Äôs tactical sophistication for various real-world applications. Therefore, in this work, we propose a novel multi-round dialogue attack mechanism. We conceptualize the process of multi-round dialogue attack as a heuristic search process, where, given a malicious query (e.g., how to make a bomb), we identify harmful responses related to the instruction as the targets of the search. The search initiates with a related but innocuous question (e.g., chemical reaction), and, based on the language model‚Äôs response, can either explore similar related queries (e.g., chemical experiment) or delve deeper into more sensitive topics (e.g., explosive reaction), ultimately leading to a harmful response. To achieve this, we devise two strategies: ‚Ä¢ Information-based control strategy, which governs the trial-and-error process by controlling the information similarity between generated inquiries and the original inquiry, ‚Ä¢ Psychology induction strategy, which incorporates psychological tactics (Zeng et al. 2024a) to minimize the likelihood of rejection as we approach our target. An illustration of our attack mechanism is depicted in Figure 6. We have trained a red-teaming agent (short for MRJ-Agent) to automate the execution of this process, demonstrating a high effectiveness on attack. We further evaluated the attack success rate of our proposed method. The experimental results showed that our approach outperformed existing attack methods, including single-round and multi-round attacks, on both closed-source and open-source models, achieving state-of-the-art attack success rates. Moreover, we tested the versatility of our method on tasks beyond text-to-text, including image-to-text and text-to-text tasks, using closed-source models GPT-4o and DALLE-3 respectively. The experiments demonstrated the generality of our multi-round attack mechanism across various tasks. In summary, owing to its high adaptability and exploratory nature, our proposed method is capable of developing more general attack strategies that can be applied to different models and scenarios. We demonstrate that our method can effectively explore potential vulnerabilities or boundary conditions of the model being tested. We hope that our work will promote further research on the security of large models in the future. Figure 2: Pipeline of our proposed attack"
https://arxiv.org/html/2411.03752v1,Deferred Poisoning: Making the Model More Vulnerable via Hessian Singularization,"Recent studies have shown that deep learning models are very vulnerable to poisoning attacks. Many defense methods have been proposed to address this issue. However, traditional poisoning attacks are not as threatening as commonly believed. This is because they often cause differences in how the model performs on the training set compared to the validation set. Such inconsistency can alert defenders that their data has been poisoned, allowing them to take the necessary defensive actions. In this paper, we introduce a more threatening type of poisoning attack called the Deferred Poisoning Attack. This new attack allows the model to function normally during the training and validation phases but makes it very sensitive to evasion attacks or even natural noise. We achieve this by ensuring the poisoned model‚Äôs loss function has a similar value as a normally trained model at each input sample but with a large local curvature. A similar model loss ensures that there is no obvious inconsistency between the training and validation accuracy, demonstrating high stealthiness. On the other hand, the large curvature implies that a small perturbation may cause a significant increase in model loss, leading to substantial performance degradation, which reflects a worse robustness. We fulfill this purpose by making the model have singular Hessian information at the optimal point via our proposed Singularization Regularization term. We have conducted both theoretical and empirical analyses of the proposed method and validated its effectiveness through experiments on image classification tasks. Furthermore, we have confirmed the hazards of this form of poisoning attack under more general scenarios using natural noise, offering a new perspective for research in the field of security.","Deep learning models have achieved remarkable achievements in fields involving Computer Vision [1] to Natural Language Processing [2] even Artificial Intelligence Generated Content [3]. Their success is largely attributed to the proliferation of large-scale datasets, such as ImageNet [4], COCO [5], and CelebA [6]. Nevertheless, recent studies have highlighted the potential hazards posed by poisoning attacks which can undermine the integrity of deep learning models by introducing malicious data into training datasets [7, 8, 9]. Figure 1: The scenario considered by DPA. Most prevalent poisoning attacks [10, 11, 12, 13] share a common limitation: their malicious intentions are highly apparent. The victim may notice a significant disparity in the performance of the model between the training and validation datasets. In response, strategies such as adversarial training [14], appropriate data preprocessing methods [15], or the elimination of anomalous [16] data can be employed as defensive measures against these attacks. In this paper, we reveal a more threatening method of poisoning attack namely Deferred Poisoning Attack (DPA). As the term ""Deferred"" implies, this attack does not disrupt the training process, allowing the model to maintain normal performance over the validation set. Instead, the ""toxicity"" of the attack manifests by undermining the model‚Äôs robustness at the deployment stage. Fig. 1 illustrates the scenario of our attack. When a recognition system (e.g. the one deployed on an autonomous driving platform) is trained using clean datasets, it functions effectively on normal targets and demonstrates relative robustness against natural noise or adversarial perturbation generated by evasion attacks such as FGSM [17], PGD [18], etc. In contrast, when the training dataset is tainted by our proposed method, the recognition system built upon the corrupted datasets operates as usual on clean inputs but exhibits high sensitivity to adversarial perturbations or even natural noise. The threat posed by DPA is evidently more severe than the typical poisoning attacks. The developers of the recognition system are unable to detect anomalies through the detection of irregularities in the training process. Once deployed in vehicles, the system would be easily misled by natural factors such as fog, rain, and varying lighting conditions. Additionally, adversaries may deceive the model at a low cost, using subtle adversarial perturbations. We fulfill the purpose of DPA by forcing the model trained over the contaminated dataset to converge to a similar point as the one trained over the clean dataset, making the poisoned model perform normally on the validation dataset. On the other hand, we enlarge the local curvature of the poisoned model around each sample in the training dataset to amplify the sensitivity of the poisoned model. Fig. 2 illustrates the above motivation. A large local curvature (the red curve) results in a significant increase in model loss with a small perturbation of a given sample. In contrast, a small local curvature (the blue curve) enables the model loss to remain stable even with a large perturbation. Formally, a large local curvature implies that the Hessian matrix is ill-conditioned with a large conditional number [19]. Along this line, our DFA generates poisoned samples to induce the model trained on this contaminated dataset to become singularization (a large conditional number.) with respect to the input samples. In summary, our principal contributions are as follows: ‚Ä¢ The proposed DPA, to the best of our knowledge, has not been previously addressed in the literature, thus revealing a new threat within the field of artificial intelligence. ‚Ä¢ We propose a novel regularization term to amplify the local curvature of the poisoned model that generates noise patterns exhibiting both visual stealthiness and adversarial effectiveness. ‚Ä¢ Compared to traditional data poisoning methods, DPA incurs a significantly lower attack cost (subtle perturbation) while demonstrating superior transferability and robustness. ‚Ä¢ We validate the generality of the deferred poisoning attack across a broader range of scenarios. Figure 2: The illustration of the motivation of DPA."
https://arxiv.org/html/2411.03746v1,Optimal Defenses Against Gradient Reconstruction Attacks,"Federated Learning (FL) is designed to prevent data leakage through collaborative model training without centralized data storage. However, it remains vulnerable to gradient reconstruction attacks that recover original training data from shared gradients. To optimize the trade-off between data leakage and utility loss, we first derive a theoretical lower bound of reconstruction error (among all attackers) for the two standard methods: adding noise, and gradient pruning. We then customize these two defenses to be parameter- and model-specific and achieve the optimal trade-off between our obtained reconstruction lower bound and model utility. Experimental results validate that our methods outperform Gradient Noise and Gradient Pruning by protecting the training data better while also achieving better utility. The code for this project is available here.","Recent advancements in machine learning have led to remarkable achievements across multiple domains. These successes are driven largely by the ability to gather vast, diverse datasets to train these powerful models. However, this can be challenging to obtain in certain sectors such as healthcare and finance due to concerns about privacy and institutional restrictions. Federated or Collaborative Learning (FL) (McMahan et al.,, 2017) has emerged as a solution to these concerns. Federated learning is a machine learning approach where multiple institutions or devices collaboratively train a model while keeping their data localized. Instead of sharing raw data, each participant shares model updates, aggregated centrally to create a global model that benefits from all participants‚Äô insights without compromising data privacy. The assumption is that summary-level information about the model (whether the model weights or the intermediate gradients) contains less information about the trained data. However, FL is not immune to privacy risks, one type of attack that may harm privacy is the Gradient Reconstruction Attack (GRA), where adversaries attempt to reconstruct original training data from the shared gradients. Methods such as DLG (Zhu et al.,, 2019), CAFE (Jin et al.,, 2021), and GradInversion (Yin et al.,, 2021) have shown the feasibility of these attacks. To mitigate these risks, several defense mechanisms have been proposed. The most common approach is perturbing the gradients, such as DP-SGD (Abadi et al.,, 2016) and Gradient Pruning (Zhu et al.,, 2019). Although these methods offer some level of data protection, they often encounter a trade-off between maintaining privacy and preserving model performance (Zhang et al.,, 2023). Figure 1: DP-SGD treats all parameters with the same vulnerability, while our method distinguishes the vulnerability of each parameter and designs a customized defense strategy. In this work, we develop a new defense mechanism based on generalizing existing gradient perturbation methods to an optimal and parameter-specific defense. As also noted by Shi et al., (2022), a universal defense strategy provides undifferentiated protection and is not optimal for utility-privacy trade-offs. We customize defenses for each parameter and examine how these adjustments impact both privacy and utility, as illustrated in Figure 1. Our objective is to optimize the balance between a model‚Äôs resilience to gradient reconstruction attacks and its training effectiveness. Our primary contributions are: ‚Ä¢ We establish a theoretical lower bound for the expected reconstruction error, which can be easily evaluated. ‚Ä¢ We propose two defense mechanisms‚ÄîOptimal Gradient Noise and Optimal Gradient Pruning‚Äîthat maximize this bound for a given level of utility. In Section 2, we provide a brief overview of federated learning and theoretical backgrounds for our method. In Section 3, we present the theoretical foundation for our lower bound and optimal defense methods, and present the implementations of our proposed algorithms. Section 4 evaluates their effectiveness against gradient reconstruction attacks in image classification tasks. 1.1 Related Works Federated learning (FL) was introduced by McMahan et al., (2017) as a framework for collaborative model training without centralized data storage. Differential privacy (DP) (Dwork et al., 2006b, ; Dwork et al., 2006a, ; Dwork and Roth,, 2014) has been used to define privacy of the algorithm. (Abadi et al.,, 2016) introduced a differential private SGD algorithm to provide DP guarantees to the trained model. Stock et al., (2022) used R√©nyi differential privacy to provide another type of guarantee. However, Zhu et al., (2019) revealed a significant vulnerability in FL by demonstrating how training data can be reconstructed from shared gradients using the DLG algorithm. This attack was refined through subsequent works, such as iDLG (Zhao et al.,, 2020) and Inverting Gradients (Geiping et al.,, 2020). More advanced techniques, including GradInversion (Yin et al.,, 2021) and CAFE (Jin et al.,, 2021), further enhance reconstruction quality but often rely on additional information or specific model architectures. More works featuring attacks include Wang et al., (2023); Jeon et al., (2021); Chen and Campbell, (2021). In response to these attacks, several defense strategies have been proposed. One line of methods perturb the gradients shared to the server(Sun et al.,, 2021; Andrew et al.,, 2021), while another line of work like InstaHide (Huang et al.,, 2020), mixup (Zhang et al.,, 2018), pixelization (Fan,, 2018) focus on directly protecting the data instead of the gradients. More details about attacks and defenses could be found in Zhang et al., (2022); Bouacida and Mohapatra, (2021); Jegorova et al., (2023). To consolidate research in this area, Liu et al., (2024) proposed a framework to systematically analyze the effectiveness of different attacks and defenses. In a similar vein, Fay et al., (2023) explored hyperparameter selection to optimize the privacy-utility trade-off in DP-SGD, Xue et al., (2024) proposed DP-SGD with adaptive noise. However, these approaches do not account for the local parameter landscape, which we address in our work. The lower bound on the reconstruction error was first introduced in previous work (Liu et al.,, 2024) except that only a local approximation was used."
https://arxiv.org/html/2411.03730v1,NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA,"The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) competition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing. The competition introduced a dataset of real invoice documents, along with associated questions and answers requiring information extraction and reasoning over the document images. Thereby, it brings together researchers and expertise from the document analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering model provided by the organizers for this new domain, mimicking a typical federated invoice processing setup. The base model is a multi-modal generative language model, and sensitive information could be exposed through either the visual or textual input modality. Participants proposed elegant solutions to reduce communication costs while maintaining a minimum utility threshold in track 1 and to protect all information from each document provider using differential privacy in track 2. The competition served as a new testbed for developing and testing private federated learning methods, simultaneously raising awareness about privacy within the document image analysis and recognition community. Ultimately, the competition analysis provides best practices and recommendations for successfully running privacy-focused federated learning challenges in the future.","Automatic document image processing has become a highly active research field in recent years (Appalaraju et al., 2024; Lee et al., 2023; Tito et al., 2023), with invoices being one of the most frequently processed document types (≈†imsa et al., 2023). In a typical real-life invoicing scenario, business suppliers produce invoices for their services and send them to their customers. These documents contain sensitive information, such as consumer/purchaser identity, transaction details, purpose, date, phone numbers, amount paid, account information for payment, etc. The customers (document users) need to extract this information and take the corresponding actions (i.e. reject, or make a payment against the invoice). In automated pipelines, these documents would be sent to AI technology providers, typically offered in the form of cloud services111Automatic document processing services offered by large corporations (AWS Intelligent Document Processing, Google Cloud Document AI, Microsoft Azure Form Recognizer, etc) or specialized providers., which automatically extract all required information from the documents, and return it to the document users. A generic approach to extract information from invoices is DocVQA (Mathew et al., 2020). The extraction is done by asking questions in a natural language form to get specific information as answers, using a deep learning model. However, training an accurate DocVQA model requires a considerable amount of data, that is rarely held by a single entity. One solution is to train this model collaboratively by aggregating and centralizing data from a set of clients that face the same problem. But, documents often cannot be freely exchanged due to the sensitive information they contain. Federated Learning (FL) is a learning paradigm that purports to solve this problem (McMahan et al., 2017b). Rather than exchanging privately-held data, participating entities (known as clients) train models on their data in a decentralized fashion, exchanging only the local model updates with a central server. However, even though FL is more private than the centralized approach, a significant amount of information can still be inferred from the updates shared during training, or from the parameters of the resulting trained model, whether by an adversarial server, client, or downstream user (Sikandar et al., 2023). Differential Privacy (DP) (Dwork et al., 2016) is considered the gold standard in terms of privacy preservation and can be used to provide provable privacy guarantees. DP formally quantifies the maximum information leakage from the inclusion of any one individual record in a dataset. Deep learning models can be trained under DP by clipping parameter updates and adding noise to them (Rajkumar and Agarwal, 2012; Song et al., 2013; Abadi et al., 2016). However, this introduces a trade-off between privacy and utility. Stronger privacy guarantees require introducing more noise, which proportionately degrades model accuracy. Another drawback of FL is the high communication cost (Kairouz et al., 2021). At each federated round, the global model is transmitted by the server to selected clients (downstream step) to be trained on their local data, and then the update of this model is sent by these selected entities back to the server (upstream step). For models with millions or even billions of parameters, this requires significant bandwidth, multiplied by the number of federated rounds required to reach model convergence. In this paper, we present an analysis of the NeurIPS 2023 competition on privacy preserving FL DocVQA that we designed to expose the above challenges and invite the community to design novel creative solutions for this real-life use case. It brought together researchers and expertise from the document analysis, privacy, and FL communities. Additionally, it added a realistic use case for privacy and FL researchers as well as expanding the scope of document analysis to DP solutions."
https://arxiv.org/html/2411.03663v1,Can Graph Neural Networks Expose Training Data Properties? An Efficient Risk Assessment Approach,"Graph neural networks (GNNs) have attracted considerable attention due to their diverse applications. However, the scarcity and quality limitations of graph data present challenges to their training process in practical settings. To facilitate the development of effective GNNs, companies and researchers often seek external collaboration. Yet, directly sharing data raises privacy concerns, motivating data owners to train GNNs on their private graphs and share the trained models. Unfortunately, these models may still inadvertently disclose sensitive properties of their training graphs (e.g., average default rate in a transaction network), leading to severe consequences for data owners. In this work, we study graph property inference attack to identify the risk of sensitive property information leakage from shared models. Existing approaches typically train numerous shadow models for developing such attack, which is computationally intensive and impractical. To address this issue, we propose an efficient graph property inference attack by leveraging model approximation techniques. Our method only requires training a small set of models on graphs, while generating a sufficient number of approximated shadow models for attacks. To enhance diversity while reducing errors in the approximated models, we apply edit distance to quantify the diversity within a group of approximated models and introduce a theoretically guaranteed criterion to evaluate each model‚Äôs error. Subsequently, we propose a novel selection mechanism to ensure that the retained approximated models achieve high diversity and low error. Extensive experiments across six real-world scenarios demonstrate our method‚Äôs substantial improvement, with average increases of 2.7% in attack accuracy and 4.1% in ROC-AUC, while being 6.5√ó\times√ó faster compared to the best baseline.","Graph data, encapsulating relationships between entities across various domains such as social networks, molecular networks, and transaction networks, holds immense value [wu2020comprehensive, liu2019n, fleder2015bitcoin]. Graph neural networks (GNNs) have proven effective in modeling graph data [wang2024spatiotemporal, zheng2023temporal, liu2024MAM], yielding promising results across diverse applications, including recommender systems [gao2023survey], molecular prediction [wieder2020compact, wang2024unveiling], and anomaly detection [ma2021comprehensive]. While training high-quality GNN models may necessitate a substantial amount of data, graphs may be scarce or of low quality in practice [xu2021netrl, xu2020robust], prompting companies and researchers to seek additional data from external sources [xu2023toward]. However, directly obtaining data from other sources is often difficult due to privacy concerns [tenopir2020data, yuan2024unveiling]. As an alternative, sharing models rather than raw data has become increasingly common [xu2023toward]. Typically, data owners train a model on their own data and subsequently release it to the community or collaborators [lerer2019pytorch, hu2020gpt, DGL-KE, qiu2020gcc, zhang2021leakage]. For instance, a larger bank may train a fraud detection model on its extensive transaction network and share it with partners, allowing them to use their own customer data to identify risks. Despite the benefits, this model-sharing strategy sometimes remains vulnerable to data leakage risks. Given access to the released model, one may infer sensitive properties of the data owner‚Äôs graph, which are not intended to be shared. In the context of releasing a fraud detection model, if an adversarial bank can determine the average default rate of all customers in the transaction network, the data owner bank‚Äôs financial status can potentially be revealed. Another example is releasing a recommendation model trained on a company‚Äôs product network [wang2022group]. If a competitor can infer the distribution of co-purchase links between different products, he may determine which items are frequently promoted together and deduce the company‚Äôs marketing tactics. Such attacks are possible because released models may inadvertently retain and expose sensitive information from the training data [ganju2018property, ateniese2015hacking]. We refer to such sensitive information related to the global distribution in a graph as graph sensitive properties, and we aim to investigate the problem of graph property inference attack. Previous property inference attacks [ganju2018property, ateniese2015hacking, melis2019exploiting, zhou2021property] primarily focus on text or image data, assuming models trained on different properties exhibit differences in parameters or outputs. For GNNs modeling graph data, the inherent relationships and message-passing mechanisms can magnify distribution bias [dai2021say], making them more vulnerable to attacks. Although a few studies extend property inference to graphs and GNNs [suri2021formalizing, zhang2021leakage, wang2022group], they typically involve creating shadow models that replicate the released model‚Äôs architecture and are trained on shadow graphs with varying sensitive properties. The parameters or outputs of shadow models are used to train an attack model to classify the property of the data owner‚Äôs graph. A major limitation of these attacks is the need to train a large number of shadow models (e.g., 4,096 models [ganju2018property], 1,600 models [suri2021formalizing]), resulting in significant computational cost and low efficiency. In this paper, we explore the feasibility of avoiding the training of numerous shadow models by designing an efficient yet effective graph property inference attack. Our key insight is to train only a small set of models and then generate sufficient approximated shadow models to support the attack. To this end, we leverage and extend model approximation techniques. For a given dataset and a model trained on it, when the training data changes (e.g., removing a sample), model approximation allows the efficient estimation of new model parameters for the updated dataset without retraining. This technique, often called unlearning [guo2019certified, wu2023certified, chien2022certified, wu2023gif], enables the efficient generation of multiple approximated shadow models from a single trained model. Specifically, given a small set of graphs and their corresponding trained models, we perturb each graph to alter sensitive properties (e.g., changing the number of nodes corresponding to high default rate users) and then apply model approximation to produce a sufficient number of approximated models corresponding to the perturbations, thereby reducing the total attack cost. Figure 1 illustrates our approach compared to the traditional attack. Nevertheless, achieving this goal presents several challenges. The first challenge is to ensure the diversity of approximated models, which provides a broader range of training samples for the attack model and enhances its generalization capability. To tackle this, we develop structure-aware random walk sampling graphs from distinctive communities and introduce edit distance to quantify the diversity of a set of approximated models. The second challenge is to ensure that the errors in the approximated shadow models are sufficiently small. Otherwise, these models may fail to accurately reflect differences in graph properties, thereby diminishing attack performance. To address this, we establish that different graph perturbations can lead to varying approximation errors, which offers a theoretical-guaranteed criterion for assessing the errors of each approximated model. Finally, we propose a novel selection mechanism to reduce errors while enhancing the diversity of approximated models, formulated as an efficiently solvable programming problem. Our contributions are as follows: ‚Ä¢ We propose an efficient and effective graph property inference attack that requires training only a few models to generate sufficient approximated models for the attack. ‚Ä¢ We propose a novel selection mechanism to retain approximated models with high diversity and low error, using edit distance to measure the diversity of approximated models and a theoretical criterion for assessing the errors of each. This diversity-error optimization is formulated as an efficiently solvable programming problem. ‚Ä¢ Experiments on six real-world scenarios demonstrate the efficiency and effectiveness of our proposed attack method. On average, existing attacks require training 700 shadow models to achieve 67.3% accuracy and 63.6% ROC-AUC, whereas our method trains only 66.7 models and obtains others by approximation, achieving 69.0% attack accuracy and 66.4% ROC-AUC."
https://arxiv.org/html/2411.03570v1,Learning Constant-Depth Circuits inMalicious Noise Models,"The seminal work of Linial, Mansour, and Nisan gave a quasipolynomial-time algorithm for learning constant-depth circuits (ùñ†ùñ¢0superscriptùñ†ùñ¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT) with respect to the uniform distribution on the hypercube. Extending their algorithm to the setting of malicious noise, where both covariates and labels can be adversarially corrupted, has remained open. Here we achieve such a result, inspired by recent work on learning with distribution shift. Our running time essentially matches their algorithm, which is known to be optimal assuming various cryptographic primitives.Our proof uses a simple outlier-removal method combined with Braverman‚Äôs theorem for fooling constant-depth circuits. We attain the best possible dependence on the noise rate and succeed in the harshest possible noise model (i.e., contamination or so-called ‚Äúnasty noise‚Äù).","In their famous paper, Linial, Mansour, and Nisan [LMN93] introduced the ‚Äúlow-degree‚Äù algorithm for learning Boolean functions with respect to the uniform distribution on {¬±1}dsuperscriptplus-or-minus1ùëë\{\pm 1\}^{d}{ ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. The running time and sample complexity of their algorithm scales in terms of the Fourier concentration of the underlying concept class, and, using this framework, they obtained a quasipolynomial-time algorithm for learning constant-depth, polynomial-size circuits (ùñ†ùñ¢0superscriptùñ†ùñ¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT). Prior work [KKMS08] had extended their result to the agnostic setting, where the labels can be adversarially corrupted, but the marginal distribution on inputs must still be uniform over {¬±1}dsuperscriptplus-or-minus1ùëë\{\pm 1\}^{d}{ ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT. Remarkably, there had been no progress on this problem in the last three decades for malicious noise models where both covariates and labels can be adversarially corrupted [Val85, KL93]. In this paper, we completely resolve this problem and obtain a quasipolynomial-time algorithm for learning ùñ†ùñ¢0superscriptùñ†ùñ¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT in the harshest possible noise model, the so-called ‚Äúnasty noise‚Äù model of [BEK02]. We define this model below and refer to it simply as learning with contamination, in line with recent work in computationally efficient robust statistics (see e.g., [DK23]). Definition 1.1 (Learning from Contaminated Samples). A set of NùëÅNitalic_N labeled examples S¬Øinpsubscript¬ØùëÜinp\bar{S}_{\mathrm{inp}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT is an Œ∑ùúÇ\etaitalic_Œ∑-contaminated (uniform) sample with respect to some class ùíû‚äÜ{{¬±1}d‚Üí{¬±1}}ùíû‚Üísuperscriptplus-or-minus1ùëëplus-or-minus1\mathcal{C}\subseteq\{\{\pm 1\}^{d}\to\{\pm 1\}\}caligraphic_C ‚äÜ { { ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí { ¬± 1 } }, where N‚àà‚ÑïùëÅ‚ÑïN\in\mathbb{N}italic_N ‚àà blackboard_N and Œ∑‚àà(0,1)ùúÇ01\eta\in(0,1)italic_Œ∑ ‚àà ( 0 , 1 ), if it is formed by an adversary as follows. 1. The adversary receives a set of NùëÅNitalic_N clean i.i.d. labeled examples S¬Øclnsubscript¬ØùëÜcln\bar{S}_{\mathrm{cln}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT, drawn from the uniform distribution over {¬±1}dsuperscriptplus-or-minus1ùëë\{\pm 1\}^{d}{ ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and labeled by some unknown concept f‚àósuperscriptùëìf^{*}italic_f start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT in ùíûùíû\mathcal{C}caligraphic_C. 2. The adversary removes an arbitrary set S¬Øremsubscript¬ØùëÜrem\bar{S}_{\mathrm{rem}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_rem end_POSTSUBSCRIPT of ‚åäŒ∑‚Å¢N‚åãùúÇùëÅ\lfloor\eta N\rfloor‚åä italic_Œ∑ italic_N ‚åã labeled examples from S¬Øclnsubscript¬ØùëÜcln\bar{S}_{\mathrm{cln}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT and substitutes it with an adversarial set of ‚åäŒ∑‚Å¢N‚åãùúÇùëÅ\lfloor\eta N\rfloor‚åä italic_Œ∑ italic_N ‚åã labeled examples S¬Øadvsubscript¬ØùëÜadv\bar{S}_{\mathrm{adv}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT. Namely, S¬Øinp=(S¬Øcln‚àñS¬Ørem)‚à™S¬Øadvsubscript¬ØùëÜinpsubscript¬ØùëÜclnsubscript¬ØùëÜremsubscript¬ØùëÜadv\bar{S}_{\mathrm{inp}}=(\bar{S}_{\mathrm{cln}}\setminus\bar{S}_{\mathrm{rem}})% \cup\bar{S}_{\mathrm{adv}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT = ( over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_cln end_POSTSUBSCRIPT ‚àñ over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_rem end_POSTSUBSCRIPT ) ‚à™ over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_adv end_POSTSUBSCRIPT. For the corresponding unlabeled set SinpsubscriptùëÜinpS_{\mathrm{inp}}italic_S start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT, we say that it is an Œ∑ùúÇ\etaitalic_Œ∑-contaminated (uniform) sample. In this model, the goal of the learner is to output (with probability 1‚àíŒ¥1ùõø1-\delta1 - italic_Œ¥) a hypothesis h:{¬±1}d‚Üí{¬±1}:‚Ñé‚Üísuperscriptplus-or-minus1ùëëplus-or-minus1h:\{\pm 1\}^{d}\to\{\pm 1\}italic_h : { ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí { ¬± 1 } such that ‚Ñôùê±‚àºUnif‚Å¢({¬±1}d)[h‚Å¢(ùê±)‚â†f‚àó‚Å¢(ùê±)]‚â§2‚Å¢Œ∑+œµsubscript‚Ñôsimilar-toùê±Unifsuperscriptplus-or-minus1ùëë‚Ñéùê±superscriptùëìùê±2ùúÇitalic-œµ\operatorname*{\mathbb{P}}_{\mathbf{x}\sim\mathrm{Unif}(\{\pm 1\}^{d})}[h(% \mathbf{x})\neq f^{*}(\mathbf{x})]\leq 2\eta+\epsilonblackboard_P start_POSTSUBSCRIPT bold_x ‚àº roman_Unif ( { ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_h ( bold_x ) ‚â† italic_f start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( bold_x ) ] ‚â§ 2 italic_Œ∑ + italic_œµ. The factor 2222 is known to be the best possible constant achievable by any algorithm [BEK02]. Although there is now a long line of research giving computationally efficient algorithms for learning Boolean function classes in malicious noise models, these algorithms primarily apply to geometric concept classes and continuous marginal distributions, such as halfspaces or intersections of halfspaces with respect to Gaussian or log-concave densities [KKMS08, KLS09, ABL17, DKS18, SZ21]. In particular, nothing was known for the case of ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT. Our main theorem is as follows: Theorem 1.2. For any s,‚Ñì,d‚àà‚Ñïùë†‚Ñìùëë‚Ñïs,\ell,d\in{\mathbb{N}}italic_s , roman_‚Ñì , italic_d ‚àà blackboard_N, and œµ,Œ¥‚àà(0,1)italic-œµùõø01\epsilon,\delta\in(0,1)italic_œµ , italic_Œ¥ ‚àà ( 0 , 1 ), there is an algorithm that learns the class of ùñ†ùñ¢0superscriptùñ†ùñ¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT circuits of size sùë†sitalic_s and depth ‚Ñì‚Ñì\ellroman_‚Ñì and achieves error 2‚Å¢Œ∑+œµ2ùúÇitalic-œµ2\eta+\epsilon2 italic_Œ∑ + italic_œµ, with running time and sample complexity dO‚Å¢(k)‚Å¢log‚Å°(1/Œ¥)superscriptùëëùëÇùëò1ùõød^{O(k)}\log(1/\delta)italic_d start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_Œ¥ ), where k=(log‚Å°(s))O‚Å¢(‚Ñì)‚Å¢log‚Å°(1/œµ)ùëòsuperscriptùë†ùëÇ‚Ñì1italic-œµk={(\log(s))^{O(\ell)}\log(1/\epsilon)}italic_k = ( roman_log ( italic_s ) ) start_POSTSUPERSCRIPT italic_O ( roman_‚Ñì ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_œµ ), from contaminated samples of any noise rate Œ∑ùúÇ\etaitalic_Œ∑. Our running time essentially matches the Linial, Mansour, and Nisan result, which is known to be optimal assuming various cryptographic primitives [Kha95]. More generally, we prove that any concept class ùíûùíû\mathcal{C}caligraphic_C that admits ‚Ñì1subscript‚Ñì1\ell_{1}roman_‚Ñì start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching polynomials of degree kùëòkitalic_k can be learned in time dO‚Å¢(k)superscriptùëëùëÇùëòd^{O(k)}italic_d start_POSTSUPERSCRIPT italic_O ( italic_k ) end_POSTSUPERSCRIPT from contaminated samples. Recent work due to [GSSV24] had obtained a similar result achieving the weaker bound of O‚Å¢(Œ∑)+œµùëÇùúÇitalic-œµO(\eta)+\epsilonitalic_O ( italic_Œ∑ ) + italic_œµ for learning functions with ‚Ñì2subscript‚Ñì2\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-sandwiching polynomials. Crucially, it remains unclear how to obtain such ‚Ñì2subscript‚Ñì2\ell_{2}roman_‚Ñì start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT sandwiching approximators for constant depth circuits 111Braverman‚Äôs celebrated result on ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT [Bra08] obtains only ‚Ñì1subscript‚Ñì1\ell_{1}roman_‚Ñì start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching., and so their result does not apply here. In 2005, Kalai et al. [KKMS08] showed that ‚Ñì1subscript‚Ñì1\ell_{1}roman_‚Ñì start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-approximation suffices for agnostic learning. Here we complete the analogy for malicious learning, showing that ‚Ñì1subscript‚Ñì1\ell_{1}roman_‚Ñì start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-sandwiching implies learnability with respect to contamination. Proof Overview. The input set S¬Øinpsubscript¬ØùëÜinp\bar{S}_{\mathrm{inp}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT is Œ∑ùúÇ\etaitalic_Œ∑-contaminated. This might make it hard to find a hypothesis with near-optimal error on S¬Øinpsubscript¬ØùëÜinp\bar{S}_{\mathrm{inp}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT. However, we are only interested in finding a hypothesis with error 2‚Å¢Œ∑+œµ2ùúÇitalic-œµ2\eta+\epsilon2 italic_Œ∑ + italic_œµ on the clean distribution, which is structured (in particular, the marginal distribution on the features is uniform over {¬±1}dsuperscriptplus-or-minus1ùëë\{\pm 1\}^{d}{ ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT). In order to take advantage of the structure of the clean distribution despite only having access to the contaminated sample, we make use of the notion of sandwiching polynomials: Definition 1.3 (Sandwiching polynomials). Let f:{¬±1}d‚Üí{¬±1}:ùëì‚Üísuperscriptplus-or-minus1ùëëplus-or-minus1f:\{\pm 1\}^{d}\to\{\pm 1\}italic_f : { ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí { ¬± 1 }. We say that the (‚Ñì1subscript‚Ñì1\ell_{1}roman_‚Ñì start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT) œµitalic-œµ\epsilonitalic_œµ-sandwiching degree of fùëìfitalic_f with respect to the uniform distribution over the hypercube {¬±1}dsuperscriptplus-or-minus1ùëë\{\pm 1\}^{d}{ ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is kùëòkitalic_k if there are polynomials pup,pdown:{¬±1}d‚Üí‚Ñù:subscriptùëùupsubscriptùëùdown‚Üísuperscriptplus-or-minus1ùëë‚Ñùp_{\mathrm{up}},p_{\mathrm{down}}:\{\pm 1\}^{d}\to{\mathbb{R}}italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT : { ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí blackboard_R of degree at most kùëòkitalic_k such that (1) pdown‚Å¢(ùê±)‚â§f‚Å¢(ùê±)‚â§pup‚Å¢(ùê±)subscriptùëùdownùê±ùëìùê±subscriptùëùupùê±p_{\mathrm{down}}(\mathbf{x})\leq f(\mathbf{x})\leq p_{\mathrm{up}}(\mathbf{x})italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT ( bold_x ) ‚â§ italic_f ( bold_x ) ‚â§ italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT ( bold_x ) for all ùê±‚àà{¬±1}dùê±superscriptplus-or-minus1ùëë\mathbf{x}\in\{\pm 1\}^{d}bold_x ‚àà { ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and (2) ùîºùê±‚àºUnif‚Å°({¬±1}d)[pup‚Å¢(ùê±)‚àípdown‚Å¢(ùê±)]‚â§œµsubscriptùîºsimilar-toùê±Unifsuperscriptplus-or-minus1ùëësubscriptùëùupùê±subscriptùëùdownùê±italic-œµ\operatorname*{\mathbb{E}}_{\mathbf{x}\sim\operatorname{Unif}(\{\pm 1\}^{d})}[% p_{\mathrm{up}}(\mathbf{x})-p_{\mathrm{down}}(\mathbf{x})]\leq\epsilonblackboard_E start_POSTSUBSCRIPT bold_x ‚àº roman_Unif ( { ¬± 1 } start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT ( bold_x ) - italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT ( bold_x ) ] ‚â§ italic_œµ. The sandwiching degree of size-sùë†sitalic_s depth-‚Ñì‚Ñì\ellroman_‚Ñì ùñ†ùñ¢0superscriptùñ†ùñ¢0\mathsf{AC}^{0}sansserif_AC start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT circuits is bounded by k=(log‚Å°(s))O‚Å¢(‚Ñì)‚Å¢log‚Å°(1/œµ)ùëòsuperscriptùë†ùëÇ‚Ñì1italic-œµk=(\log(s))^{O(\ell)}\log(1/\epsilon)italic_k = ( roman_log ( italic_s ) ) start_POSTSUPERSCRIPT italic_O ( roman_‚Ñì ) end_POSTSUPERSCRIPT roman_log ( 1 / italic_œµ ), due to the result of Braverman on fooling constant-depth circuits (see Theorem 4.2 from [Bra08, Tal17, HS19]). Suppose that S¬Ø¬ØùëÜ\bar{S}over¬Ø start_ARG italic_S end_ARG is a subset of S¬Øinpsubscript¬ØùëÜinp\bar{S}_{\mathrm{inp}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT that preserves the expectations of low-degree and non-negative polynomials (e.g., pup‚àípdownsubscriptùëùupsubscriptùëùdownp_{\mathrm{up}}-p_{\mathrm{down}}italic_p start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT) compared to the uniform distribution. Under this condition, low-degree polynomial regression gives a hypothesis with near-optimal error on S¬Ø¬ØùëÜ\bar{S}over¬Ø start_ARG italic_S end_ARG (see Section 4). We show in Lemma 3.1 that a simple procedure that iteratively removes samples from S¬Øinpsubscript¬ØùëÜinp\bar{S}_{\mathrm{inp}}over¬Ø start_ARG italic_S end_ARG start_POSTSUBSCRIPT roman_inp end_POSTSUBSCRIPT can be used to form such a set S¬Ø¬ØùëÜ\bar{S}over¬Ø start_ARG italic_S end_ARG (that preserves the expectations of non-negative, degree-kùëòkitalic_k and low-expectation polynomials) and, moreover, this procedure removes more contaminated points than clean points. The last property is important, because it implies that S¬Ø¬ØùëÜ\bar{S}over¬Ø start_ARG italic_S end_ARG is representative for the ground truth distribution, i.e., any near-optimal hypothesis for S¬Ø¬ØùëÜ\bar{S}over¬Ø start_ARG italic_S end_ARG will also have error 2‚Å¢Œ∑+œµ2ùúÇitalic-œµ2\eta+\epsilon2 italic_Œ∑ + italic_œµ on the ground truth. This is possible because the only way the adversary can significantly increase the expectation of a non-negative polynomial pùëùpitalic_p is by inserting examples ùê±ùê±\mathbf{x}bold_x where p‚Å¢(ùê±)ùëùùê±p(\mathbf{x})italic_p ( bold_x ) is unreasonably large compared to the typical values of pùëùpitalic_p over the uniform distribution. Our algorithm iteratively finds the non-negative polynomial qùëûqitalic_q with the largest expectation over a given set through a simple linear program and then removes the points ùê±ùê±\mathbf{x}bold_x for which q‚Å¢(x)ùëûùë•q(x)italic_q ( italic_x ) is large. Our iterative outlier removal procedure is inspired by prior work on TDS learning (Testable Learning with Distribution Shift) and PQ learning [KSV24, GSSV24] as well as the work of [DKS18] on learning geometric concepts from contaminated examples. Both of these works use outlier removal procedures that give bounds on the variance of polynomials rather than the expectation of non-negative polynomials and, instead of linear programming, they use spectral algorithms."
https://arxiv.org/html/2411.03569v1,Towards Personalized Federated Learning via Comprehensive Knowledge Distillation,"Federated learning is a distributed machine learning paradigm designed to protect data privacy. However, data heterogeneity across various clients results in catastrophic forgetting, where the model rapidly forgets previous knowledge while acquiring new knowledge. To address this challenge, personalized federated learning has emerged to customize a personalized model for each client. However, the inherent limitation of this mechanism is its excessive focus on personalization, potentially hindering the generalization of those models. In this paper, we present a novel personalized federated learning method that uses global and historical models as teachers and the local model as the student to facilitate comprehensive knowledge distillation. The historical model represents the local model from the last round of client training, containing historical personalized knowledge, while the global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge. By applying knowledge distillation, we effectively transfer global generalized knowledge and historical personalized knowledge to the local model, thus mitigating catastrophic forgetting and enhancing the general performance of personalized models. Extensive experimental results demonstrate the significant advantages of our method.","The rapid evolution of distributed intelligent systems has brought data privacy to the forefront. Federated Learning (FL), a distributed machine learning paradigm, enables collaborative model training through parameter sharing rather than raw data exchange, effectively reducing the risk of exposing sensitive data [1]. By leveraging FL, we can efficiently utilize data from distributed clients to collectively train high-performance models. FL has demonstrated its significant value in a variety of fields, including medical health [2], financial analytics [3], and social network [4]. In traditional FL, multiple clients collaborate to train a global model to achieve an optimal universal solution for all clients. However, the non-independent and identically distributed nature of the data distribution [5], known as data heterogeneity, often causes a decline in the performance of distributed clients and can even lead to catastrophic forgetting. Fig. 1 depicts this phenomenon. It is evident that the data distribution varies in each communication round. During each training round, distributed clients update their local model with the global model. Unfortunately, the post-update local model exhibits a significant performance decline compared to the pre-update local model, indicating the phenomenon of forgetting previously learned knowledge. Personalized Federated Learning (PFL) is an innovative approach to tackle data heterogeneity in FL [6]. It involves the collaborative training of a global model by all clients, after which each client develops a personalized model through personalized strategies, reflecting the distinctive characteristics of its local data. However, while existing PFL methods excel in model personalization, they often neglect model generalization. For instance, pFedSD [7] employs knowledge distillation to transfer knowledge from the historical model to the local model for achieving model personalization. Nevertheless, this method may hinder the local model‚Äôs generalization, as the historical model represents the previous local model and incorporates personalized knowledge. The primary limitation of existing PFL methods lies in their design principles, which overemphasize personalized learning and may lead to model overfitting on individual clients, thereby reducing their adaptability to varied client [8]. (a) Data distribution (b) Catastrophic forgetting Figure 1: Data heterogeneity in FL leads to catastrophic forgetting. In order to alleviate catastrophic forgetting and achieve a balance between generalization and personalization, we propose Personalized Federated Learning via Comprehensive Knowledge Distillation (FedCKD). Our method integrates multi-teacher knowledge distillation [9] into FL for comprehensive knowledge transfer. The global model represents the aggregated model from the last round of server aggregation, containing global generalized knowledge, while the historical model represents the local model from the last round of client training, containing historical personalized knowledge. By employing the multi-teacher knowledge distillation, we utilize global and historical models as teachers and the local model as the student. This method effectively transfers both global generalized knowledge and historical personalized knowledge to the local model. Global generalized knowledge enhances model performance, whereas historical personalized knowledge addresses the issue of catastrophic forgetting. In summary, our primary contributions are as follows: (1) We propose a novel PFL method called FedCKD. Through multi-teacher knowledge distillation, our method effectively transfers global generalized knowledge and historical personalized knowledge to the local model, thereby addressing catastrophic forgetting and enhancing model performance. (2) We introduce the annealing mechanism in knowledge distillation that dynamically adjusts the weight factor in the loss function, facilitating a smooth transition of training from knowledge transfer to local learning. This mechanism enhances the model‚Äôs personalization ability and improves the training process‚Äôs stability. (3) We validate the superior performance of our method through an extensive series of experiments, surpassing existing state-of-the-art methods."
https://arxiv.org/html/2411.03445v1,Solving Trojan Detection Competitions with Linear Weight Classification,"Neural networks can conceal malicious Trojan backdoors that allow a trigger to covertly change the model behavior. Detecting signs of these backdoors, particularly without access to any triggered data, is the subject of ongoing research and open challenges. In one common formulation of the problem, we are given a set of clean and poisoned models and need to predict whether a given test model is clean or poisoned. In this paper, we introduce a detector that works remarkably well across many of the existing datasets and domains. It is obtained by training a binary classifier on a large number of models‚Äô weights after performing a few different pre-processing steps including feature selection and standardization, reference model weights subtraction, and model alignment prior to detection. We evaluate this algorithm on a diverse set of Trojan detection benchmarks and domains and examine the cases where the approach is most and least effective.","Trojan backdoors are hidden modifications in neural network models that allow an attacker to alter the model‚Äôs behavior in response to a specific trigger, posing significant risks to AI systems. The vulnerability of neural networks to Trojan backdoors is well documented. Techniques for inserting triggers vary from simple data poisoning (Gu et al., 2019) to clean label attacks (Turner et al., 2018; Saha et al., 2019; Liu et al., 2018b) to weight manipulation (Liu et al., 2018b; Garg et al., 2020). There have been several recent surveys covering backdoor attacks (Liu et al., 2020; Li et al., 2021; Wang et al., 2022). A variety of techniques for detecting Trojan behavior have emerged in recent years. These include detecting anomalous samples during neural network training or inference (Chou et al., 2020; Gao et al., 2020; Chen et al., 2018), attempting to recover the Trojan trigger via trigger inversion (Wang et al., 2019; Guo et al., 2019; Wang et al., 2020; Sun et al., 2020; Shen et al., 2021; Huster & Ekwedike, 2021), functional analysis (Sikka et al., 2020; Xu et al., 2019; Edraki et al., 2020; Erichson et al., 2020), activation analysis (Tang et al., 2019), and weight analysis (Fields et al., 2021; Clemens, 2021). In computer vision, techniques like activation clustering (Chen et al., 2018) detect abnormal neuron activations that correspond to backdoor triggers, while Neural Cleanse (Wang et al., 2019) reverse-engineers potential triggers by identifying small input modifications that flip model predictions. Fine-pruning (Liu et al., 2018a) is used to prune rarely activated neurons associated with triggers, effectively neutralizing the backdoor. ABS scanning (Shen et al., 2021), detects neurons that respond abnormally to synthetic perturbations, uncovering hidden backdoors. Spectral signature analysis (Tran et al., 2018) is also employed to identify outliers in neuron activations caused by backdoor inputs . These approaches aim to uncover hidden visual patterns or anomalies that activate backdoors in image-based models. In natural language processing (NLP), backdoors typically appear as specific words or phrases that trigger malicious behavior. Detection techniques include input perturbation, where small modifications to text inputs help reveal triggers, and anomaly detection in embeddings, which identifies outliers in word embeddings or hidden states that correlate with backdoor behavior. Early stopping, perplexity and BERT Embedding distance were proposed in (Wallace et al., 2020) to mitigate and identify poison examples in the training dataset. Traditional defense strategies, relying on model fine-tuning and gradient calculations, are insufficient for Large Language Models due to their computational demands, so the proposed Chain-of-Scrutiny (CoS) method (Li et al., 2024) detects backdoor attacks by generating and scrutinizing detailed reasoning steps to identify inconsistencies with the final answer. In this paper, we introduce a simple, scalable, and powerful method for detecting Trojan backdoors across different domains including computer vision and NLP using linear weight classification. We focus on a common formulation of the problem where a set of clean and poisoned deep neural network models is provided, and the task is to predict whether a given test model is clean or poisoned. The detector is obtained by training a linear classifier on a large number of models‚Äô weights after performing a few different pre-processing steps. We start first by applying tensor and weight selection strategies resembling the first step in a forward-stagewise regression approach (Hastie et al., 2009). Normalization was particularly effective when combined with reference model subtraction. We also explored permutation-invariant representations of tensors, and found that sorting was highly effective in addressing the arbitrary permutations of hidden units in trained neural networks. Our method falls under the category of weight analysis detection, which does not require any prior knowledge of the trigger or model outputs and is applicable across multiple domains. We evaluate our approach on several benchmarks including datasets from the Trojan Detection Challenge (TDC22)(Mazeika, 2022) and the IARPA/NIST TrojAI program(Karra et al., 2020). The Trojan Detection Challenge(TDC22), a NeurIPS 2022 competition, tasks participants with detecting and analyzing Trojan attacks on deep neural networks designed to evade detection. The IARPA/NIST TrojAI program is a long-running initiative that has developed over 16 challenges using this formulation, addressing the issue of adversaries inserting Trojan behaviors into AI models by compromising the training pipeline. The program focuses on identifying such Trojans, which can be activated by specific triggers in an AI‚Äôs input, causing the model to produce incorrect responses. We also trained both clean and poisoned models from scratch using the Fashion MNIST dataset for our experiments. This dataset was especially valuable in demonstrating the importance of sorting tensors before training the logistic regression detector for neural networks initialized with random weights. The structure of the paper is as follows: Section 2 discusses weight analysis methods for detecting backdoor models, followed by Section 3, which introduces our proposed methodology. Section 4 explains the experimental setup, including the evaluation metrics and datasets. The experimental results are presented in Section 5 followed by conclusions."
https://arxiv.org/html/2410.16383v1,Designing Robust Cyber-Defense Agents with Evolving Behavior Trees,"Modern network defense can benefit from the use of autonomous systems, offloading tedious and time-consuming work to agents with standard and learning-enabled components. These agents, operating on critical network infrastructure, need to be robust and trustworthy to ensure defense against adaptive cyber-attackers and, simultaneously, provide explanations for their actions and network activity. However, learning-enabled components typically use models, such as deep neural networks, that are not transparent in their high-level decision-making leading to assurance challenges. Additionally, cyber-defense agents must execute complex long-term defense tasks in a reactive manner that involve coordination of multiple interdependent subtasks. Behavior trees are known to be successful in modelling interpretable, reactive, and modular agent policies with learning-enabled components. In this paper, we develop an approach to design autonomous cyber defense agents using behavior trees with learning-enabled components, which we refer to as Evolving Behavior Trees (EBTs). We learn the structure of an EBT with a novel abstract cyber environment and optimize learning-enabled components for deployment. The learning-enabled components are optimized for adapting to various cyber-attacks and deploying security mechanisms. The learned EBT structure is evaluated in a simulated cyber environment, where it effectively mitigates threats and enhances network visibility. For deployment, we develop a software architecture for evaluating EBT-based agents in computer network defense scenarios. Our results demonstrate that the EBT-based agent is robust to adaptive cyber-attacks and provides high-level explanations for interpreting its decisions and actions.","Modern network defense is an increasingly difficult task due to a multitude of novel and diverse cyber-attacks and the scale of systems. Developers have crafted solutions such as alert monitors and decision logic rules to alleviate human cognitive fatigue, but this is not viable or scalable as new attacks are discovered. The capabilities and strategies of cyber-defense continue to grow and, as such, fully autonomous cyber defense agents are being considered as an alternative to optimally utilize the resources needed to mitigate adversaries. However, there are challenges regarding the transparency of these agents and their robustness. Autonomous agents contain a mix of standard and learning-enabled components (LECs) trained with machine learning (ML) and reinforcement learning (RL). These LECs are normally modeled with neural networks that lack the ability to provide high-level explanations and struggle to complete long-term objectives with multiple subtasks. Cyber-defense breaks down into a variety of subroutines (analysis, monitoring, restoring, deploying decoys, etc.) that a single component will struggle to represent and optimize. Additionally, it is unclear how autonomous agents will adapt to multiple varieties of cyber-attacks. Adversaries are versatile, they may use one or more attacks on the system targeting multiple components. The adversary may use one strategy at the beginning to explore the system in a low-detection manner and then launch a full-scale attack after they have discovered enough knowledge. The autonomous agent will need to be robust to these scenarios and explain its current perspective of the adversary‚Äôs behavior. An effective way of representing a reactive control policy between subtasks in the hierarchy is using Behavior Trees (BTs) [2]. BTs are structures for modeling complex control policies with advantages of modularity, reactivity, and explainability. The reactive nature of the BT allows for explicit switching between multiple behaviors in quick succession depending on environment changes. The LECs and other components can be coordinated and modelled as a policy we refer to as an Evolving Behavior Tree (EBT). An EBT can jointly optimize and model the control flow of optimized components. The structure of the control of behaviors in an EBT is typically manually developed, but due to the complexity of multiple subtasks and their dependencies, construction can prove tedious and potentially infeasible. Recent work has focused on automatic construction of BTs using genetic programming (GP) [3, 4] or large-language models [5, 6]. In particular, the works in GP develop abstract environments for computational efficiency that map to a realistic simulation. In this paper, we develop an autonomous cyber-defense agent that leverages the hierarchical structure of EBTs for robustness against dynamic cyber-attacks. We design the agent in three stages: (1) learning the high-level control structure of the EBT, (2) optimization of LECs for robustness, and (3) integration and deployment to a realistic cyber environment. The research objective of (1) is to enhance scalability by generating control structures without requiring detailed knowledge of network components. This approach enables learning the modular structure of the EBT prior to optimizing LECs, thereby avoiding unnecessary retraining. For (1), we utilize GP and develop a novel abstract cyber-environment referred to as the Cyber-Firefighter to map to a realistic cyber-defense simulation scenario and evaluate the structural performance. The research objective of (2) is to develop generalizable behaviors that are robust to uncertainties in a realistic computer network environment. The EBT contains LECs for choosing cyber-agent actions and determining the red agent strategy based on a non-deterministic network state. We develop a software architecture for the construction and integration of an EBT for a computer network defense scenario. In (3), we deploy and evaluate our autonomous cyber-defense agent in a realistic simulation environment for robustness and explainability. The main technical contributions of our work are: ‚Ä¢ Design, optimization, and deployment of an EBT for autonomous cyber-defense of a computer network. The EBT structure is designed using GP with a novel abstract cyber environment, the Cyber-Firefighter, that maps to cyber-defense. The EBT structure is generalizable and contains capabilities for decoy deployment, attacker strategy detection, and selection of cyber-operations. ‚Ä¢ An evaluation of the learned EBT structure in the Cyber-Firefighter to demonstrate high-level control performance against an attacker in a network. The GP algorithm in tandem with the Cyber-Firefighter is successful at learning an EBT structure that maximizes the performance metric (fitness) to promote mitigation and visibility of an attack. ‚Ä¢ Development of a software architecture to support the construction and deployment of EBTs on a computer network. The architecture utilizes a blackboard of data sources in a publish-subscribe method to facilitate interaction between the EBT and computer network environment. The computer network environment in this paper is CybORG [7], an abstracted version of a computer network, compatible with ML and RL algorithms. ‚Ä¢ An evaluation of the robustness and explainability of the EBT in CybORG [7] using CAGE Challenge Scenario 2 [1]: a computer network task where the agent must defend against an adversarial agent. We develop an adversarial red agent for this scenario that switches strategies during execution to evaluate the adaptation of our approach. The EBT is successful at defending against dynamic attacks with a 39%percent3939\%39 % increase in the average reward compared to a state-of-the-art method in CybORG CAGE Challenge Scenario 2. The explainable nature of the EBT allows us to monitor key events, such as when the strategy switches or a decoy is deployed, and model transitions between high-level subtasks."
https://arxiv.org/html/2411.03231v2,Formal Logic-guided Robust Federated Learning against Poisoning Attacks,"Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning. However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance. These attacks can introduce critical malfunctions, such as biased predictions or reduced accuracy, undermining the integrity and robustness of the global model. Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems. However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data. These tasks, which often involve sequential dependencies and temporal patterns, have been largely overlooked in the context of poisoning attack defenses.In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants. Based on our investigation of the effectiveness of poisoning attack defenses within the Federated Time Series (FTS) domain, we pinpoint the limitations of mainstream defenses against such attacks. Unlike traditional model-centric defenses, FLORAL leverages logic reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates. Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates. Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior. Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications. Notably, FLORAL reduced the prediction error by 93.27% in the best-case scenario compared to the second-best baseline. Our code is available at https://anonymous.4open.science/r/FLORAL-Robust-FTS.","Federated Learning (FL) has emerged as a promising solution that enables using data and computing resources from multiple clients to train a shared model under the orchestration of a central server [33]. In FL, clients use their data to train the model locally and iteratively share the local updates with the server, which then combines the contributions of the participating clients to generate a global update. The security aggregation mechanism and its distinctive distributed training mode render it highly compatible with a wide range of practical applications that have stringent privacy demands [49, 59, 40, 21]. Recently, FL has been demonstrated to be efficient in time-series related tasks [10, 48, 3] to securely share knowledge of similar expertise among different tasks and protect user privacy. Although FL has many notable characteristics and has been successful in many applications [2, 21, 46, 52, 66, 22, 41], recent studies indicate that FL is fundamentally susceptible to adversarial attacks in which malicious clients manipulate the local training process to contaminate the global model [6, 55, 44]. Based on the attack‚Äôs goal, adversarial attacks can be broadly classified into untargeted and targeted attacks. The former aims to deteriorate the performance of the global model on all test samples [9, 14]; while the latter focuses on causing the model to generate false predictions following specific objectives of the adversaries [62, 6]. Figure 1: Illustration of logical verification given by benign and malicious clients‚Äô predictions. The global property here is ‚ñ°(0,10]‚Å¢(y^‚Å¢(t)‚â§p1)‚àß‚ñ°(10,20]‚Å¢(y^‚Å¢(t)‚â§p2)‚àß‚ñ°(20,30]‚Å¢(y^‚Å¢(t)‚â§p3)‚àß‚ñ°(30,40]‚Å¢(y^‚Å¢(t)‚â§p4)subscript‚ñ°010^ùë¶ùë°subscriptùëù1subscript‚ñ°1020^ùë¶ùë°subscriptùëù2subscript‚ñ°2030^ùë¶ùë°subscriptùëù3subscript‚ñ°3040^ùë¶ùë°subscriptùëù4\square_{(0,10]}(\hat{y}(t)\leq p_{1})\wedge\square_{(10,20]}(\hat{y}(t)\leq p% _{2})\wedge\square_{(20,30]}(\hat{y}(t)\leq p_{3})\wedge\square_{(30,40]}(\hat% {y}(t)\leq p_{4})‚ñ° start_POSTSUBSCRIPT ( 0 , 10 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ‚àß ‚ñ° start_POSTSUBSCRIPT ( 10 , 20 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ‚àß ‚ñ° start_POSTSUBSCRIPT ( 20 , 30 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) ‚àß ‚ñ° start_POSTSUBSCRIPT ( 30 , 40 ] end_POSTSUBSCRIPT ( over^ start_ARG italic_y end_ARG ( italic_t ) ‚â§ italic_p start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ). Examples of points violating this property are marked with x. Many efforts have been devoted to dealing with existing threats in FL, which can be roughly classified into two directions: robust FL aggregation [50, 47, 69, 71] and anomaly model detection. The former aims to optimize the aggregation function to limit the effects of polluted updates caused by attackers, whereas the latter attempts to identify and remove malicious updates. For instance, Xie et al. [69] presented a certified defense mechanism based on the clipping and perturbation paradigm. Other approaches focused on new estimators such as coordinate-wise median, Œ±ùõº\alphaitalic_Œ±-trimmed mean [72], and geometric median [50] for aggregation. The main drawback of the methods mentioned above is that polluted updates remain in the global model, reducing the model‚Äôs precision while not mitigating the attack impact [43]. Several methods have been proposed to identify and remove adversarial clients from the aggregation [9, 60, 42, 54, 17, 73]. In [60], the authors proposed a defense mechanism against poisoning attacks in collaborative learning based on the KùêæKitalic_K-Means algorithm. Sattler et al. [57] proposed dividing the clients‚Äô updates into normal updates and suspicious updates based on their cosine similarities. However, most methods for identifying malicious clients proposed so far follow the majority-based paradigm in that they assume benign local model updates are a majority compared to the malicious ones; thus, polluted updates are supposed to be outliers in the distribution of all updates. Unfortunately, this hypothesis holds only if the data of the clients is IID (independent and identically distributed) and the number of malicious clients is small. Though these two approaches can mitigate poisoning attacks in FL, most of them have been evaluated primarily in the context of computer vision tasks, where image-based datasets dominate the landscape [17, 47, 44, 63]. However, FL applied to time-series data remains underexplored, particularly regarding its vulnerabilities, where adversarial attacks pose a significant threat, much like those observed in image-based datasets [23, 12, 13, 38]. Given the critical applications of time-series analysis, such as in healthcare [5, 36], financial systems [35, 34], and industrial monitoring [31, 30], ensuring the robustness and security of FL models in these scenarios is of paramount importance. Our empirical result demonstrates that these methods are not effective in the scenario of FL with time-series tasks where the data itself reflects a high level of non-iid due to the different locations where it is collected. To fill this gap, we propose FLORAL, a defense mechanism capable of mitigating poisoning attacks against Federated Time Series (FTS) under the most challenging scenarios, i.e., in the presence of heterogeneous client data and a large number of adversarial clients. Our approach is orthogonal to existing model-centric defenses. Instead, we rely on logic-based reasoning to evaluate the reliability of clients based on their behavior and resistance to poisoning attacks. This approach assesses the trustworthiness of clients by aligning their predictions with global time-series patterns. Specifically, we use symbolic reasoning to capture the logical semantics embedded in time series data, which has been shown to improve the learning process and produce more robust models for future predictions [3, 31, 29]. Our FL defense method builds on this by using symbolic reasoning to evaluate diverging intra-task logic patterns in client predictions, allowing for the detection of anomalous clients without relying solely on model similarity. This highlights the enhanced effectiveness of reasoning logic in identifying malicious behaviors in FL. The intuition behind our approach is that, after rounds of training, benign models naturally converge toward the same global objective and share consistent logical reasoning patterns, while malicious models diverge, aiming to manipulate global behavior and thereby exhibit deviant reasoning patterns. The high-level idea is visualized in Figure 1. In centralized FL, we expect the final model GTsubscriptùê∫ùëáG_{T}italic_G start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT to have the minimized error on the local data, with local models converging on a unified objective [49, 27, 24]. In light of these findings, we propose FLORAL, a logic-guided defense for FL which includes three key components. First, we extract logical reasoning properties (e.g., when training models relating to traffic and driving, a dataset measuring vehicle density over time would by expected to reach an upper extreme value of, say, 100) from clients and apply hierarchical clustering to group client updates based on the logical properties of their local models. This allows us to infer the global reasoning properties that represent the system‚Äôs clients based on clustered properties. These formal logic properties rigorous assessment of the consistency and validity of client contributions by identifying deviations from expected model behaviors. This verification-based defense substantially strengthens the security of federated learning in time-series applications, where the risk of undetected adversarial behavior is particularly high due to the sensitive nature of these tasks. By optimizing for the unique challenges of time-series data, our method enhances the robustness of FL systems, providing a more reliable safeguard compared to existing defenses. Experimental results validate the effectiveness of our approach in mitigating poisoning attacks while maintaining high model performance. In summary, our contributions are specified as follows: ‚Ä¢ We introduce FLORAL ‚Äî a novel poisoning-resistant defense for FL. It identifies and eliminates suspicious clients that distort the global model using logical reasoning property inference and verification. FLORAL is the first work that, to the best of our knowledge, thoroughly addresses poisoning attacks in FTS, even in the presence of a large number of compromised participants and complicated attack strategies. ‚Ä¢ We are the first to study the efficacy of existing robust FL defenses in the context of FTS and pinpoint their limitation when adapted to the time-series domain. ‚Ä¢ We conduct comprehensive experiments and in-depth studies on various datasets, FL settings, and attack scenarios to demonstrate the superiority of FLORAL over state-of-the-art defense techniques."
https://arxiv.org/html/2411.03022v1,Flashy Backdoor: Real-world Environment Backdoor Attack on SNNs with DVS Cameras,"While security vulnerabilities in traditional Deep Neural Networks (DNNs) have been extensively studied, the susceptibility of Spiking Neural Networks (SNNs) to adversarial attacks remains mostly underexplored. Until now, the mechanisms to inject backdoors into SNN models have been limited to digital scenarios; thus, we present the first evaluation of backdoor attacks in real-world environments. We begin by assessing the applicability of existing digital backdoor attacks and identifying their limitations for deployment in physical environments. To address each of the found limitations, we present three novel backdoor attack methods on SNNs, i.e., Framed, Strobing, and Flashy Backdoor. We also assess the effectiveness of traditional backdoor procedures and defenses adapted for SNNs, such as pruning, fine-tuning, and fine-pruning. The results show that while these procedures and defenses can mitigate some attacks, they often fail against stronger methods like Flashy Backdoor or sacrifice too much clean accuracy, rendering the models unusable.Overall, all our methods can achieve up to a 100% Attack Success Rate while maintaining high clean accuracy in every tested dataset. Additionally, we evaluate the stealthiness of the triggers with commonly used metrics, finding them highly stealthy. Thus, we propose new alternatives more suited for identifying poisoned samples in these scenarios. Our results show that further research is needed to ensure the security of SNN-based systems against backdoor attacks and their safe application in real-world scenarios. The code, experiments, and results are available in our repository.111https://anonymous.4open.science/r/Flashy_backdoor/README.md","Deep Neural Networks (DNNs) have revolutionized the field of Artificial Intelligence (AI), offering unprecedented advancements in various tasks, such as image recognition [14], natural language processing [18], or speech recognition [34]. Their capacity to process and learn from vast amounts of data makes them ideal for various applications, ranging from medical analysis [29] to autonomous driving systems [66] and personal assistants [37]. However, despite their success and wide application, DNN training is still a problem, as it demands significant computational resources and time, especially for large models like, e.g., GPT-3 [67]. Training such models requires extensive hardware and consumes a considerable amount of energy, posing environmental and economic challenges. For example, assuming that the GPT-3 model was trained using 1024 NVIDIA A100 GPUs in a data center, the time required to train this model would be 34 days [63], consuming about 235 MWh. Moreover, according to the method proposed by Lannelongue et al. [46], it would have a carbon footprint of 62.47 T CO2e (CO2 equivalent), which would take 5 680 years222https://calculator.green-algorithms.org/ for a tree to process. This immense carbon footprint has led researchers to explore alternative, more sustainable models. The motivation for using Spiking Neural Networks (SNNs) extends beyond their computational capabilities, emerging as a promising solution to this challenge [33, 80, 23, 24]. Unlike conventional DNNs that operate on continuous activations, SNNs simulate the spiking behavior observed in biological neurons [44]; this unique characteristic allows SNNs to capture the temporal dynamics of information processing, mirroring the precise timing and synchronization observed in the human brain [31]. SNNs exhibit a form of event-driven computation, meaning they only activate when necessary, leading to potential energy efficiency improvements compared to traditional neural network architectures. This advantage was demonstrated by Kundu et al. [45], who showed that SNNs achieved up to a 12.2√ó12.2\times12.2 √ó improvement in computational energy efficiency compared to DNNs with a similar parameter count. Additionally, SNNs can be more robust to noise and perturbations, allowing them to maintain accuracy in real-world situations under varying environmental conditions [47]. The inherent ability of SNNs to manage temporal dependencies within data makes them particularly useful in tasks where the sequence and timing of events matter, such as in sensory processing [80]. This capability becomes evident when employing data acquired through Dynamic Vision Sensor (DVS) cameras [75]. Unlike traditional cameras capturing absolute brightness at a constant rate, DVS cameras do not capture conventional video frames but instead record changes in light intensity at a pixel level, providing high temporal resolution and low-latency data [15, 57], referred to as neuromorphic data. This type of data aligns perfectly with the spike-based communication of SNNs, highlighting the interaction between the distinct features of both SNNs and neuromorphic data. These capabilities are particularly useful in scenarios requiring rapid and efficient processing, such as autonomous driving [15, 91] or robotics [84], where DVS cameras can work together with traditional sensors to capture the surrounding environment better to assess the current scenario. These cameras can also be used with SNNs to process the environment more efficiently than traditional cameras and DNNs. Event-based data is helpful when conventional data falls short [91, 84, 90, 73] due to energy constraints, difficult lighting situations, or high-speed processing requirements. Energy constraints are particularly critical in autonomous vehicles, where on-device processing is necessary to ensure real-time responsiveness and minimize dependence on cloud-based computation. For instance, a lightweight, fully unsupervised DNN adaptation algorithm for lane detection demonstrated real-time model adjustments, meeting similar constraints to those found in some existing level 3 autonomous cars [8]. However, such constraints will be harder to maintain with the increasing number of sensors and processing required, as autonomous vehicles typically utilize a vast array of sensors such as LiDAR, radar, and multiple cameras to perceive their environment, which significantly increases the data processing requirements and energy consumption [10]. SNNs paired with DVS cameras offer a more energy-efficient and low-latency alternative for scenarios where real-time processing is critical. Moreover, due to their event-driven nature and sparse activity, SNNs can drastically reduce power consumption and processing requirements while maintaining high computational efficiency and low latency. For instance, the Spiking Autonomous Driving [91] system integrates perception, prediction, and planning into a single end-to-end SNN model, showing competitive performance while significantly lowering energy consumption. What is more, SNNs demonstrate broad applicability in other domains, including computer vision [22], speech recognition [60], and medical diagnosis [42]. Despite the conventional belief that DNNs exhibit superior accuracy, recent findings indicate a diminishing or even disappearing performance gap compared to SNNs [80]. Although SNNs offer promising benefits and practical applications, a thorough assessment of their security aspects is necessary before they can be utilized in real-world situations. Many studies show ways in which a malicious actor could take advantage of insecure DNNs; examples include inference attacks [58], evasion attacks [74], and poisoning attacks [71]. The consequences of these attacks can be dire, ranging from manipulated facial recognition systems and stealthy malware evasion [49, 38] to incorrect medical diagnosis [70] or even tricking autonomous vehicles into misidentifying road signs or obstacles [39]. However, the applicability of these attack methods to SNNs has yet to be studied. This work contributes to evaluating SNN vulnerability by analyzing its susceptibility to backdoor attacks, emphasizing its suitability and relevance in real-world contexts. First, we test the backdoor attack presented by Abad et al. [2], the first to introduce backdoor attacks to SNNs within the digital domain. Their work demonstrated how hidden triggers embedded in the input data can cause misclassification in SNNs without degrading performance on clean data. Then, we propose two alternative trigger methods to solve the limitations we found when replicating these attacks for physical environments. We call these trigger methods Framed and Strobing triggers. Finally, after evaluating the results obtained by those methods, we generate our novel physical-environment backdoor, Flashy Backdoor, evaluating its effectiveness in real-world scenarios. This highlights how vulnerabilities can be exploited in actual deployment settings, where inputs are sourced from the physical world rather than synthetic data or simulated scenarios. To our knowledge, this paper presents the first backdoor attack on SNNs in real-world environments. Previously, all existing backdoor attack methods on SNNs were designed for digital environments, making them not applicable to physical scenarios. We demonstrate that carefully crafted triggers, specifically designed with the challenges and limitations of DVS cameras and neuromorphic data in mind, can be virtually modeled and successfully reproduced in physical environments., achieving up to a 100% Attack Success Rate (ASR). Our contributions also include: ‚Ä¢ Analyzing the suitability of previously proposed backdoor attacks for SNNs and introducing new attack variants that, while achieving at least 99% ASR, do not show any degradation in clean accuracy. ‚Ä¢ Testing procedures and defenses from the image domain to assess their efficacy on each proposed attack. ‚Ä¢ Expanding the DVS128-Gesture dataset by adding new examples, including poisoned samples with physical triggers captured using a DVS camera, to test the attack‚Äôs impact in real-world physical environments. ‚Ä¢ Analyzing and comparing the stealthiness of our proposed methods with metrics such as the average Mean Square Error (MSE), Structural Similarity Index Metric (SSIM), average entropy analysis, and Peak Signal-to-Noise Ratio (PSNR). Finally, we explore other not-so-traditional methods to detect the triggers, like analyzing the number of activations per channel or the entropy distribution of the activation values per frame."
https://arxiv.org/html/2411.03019v1,FEDLAD: Federated Evaluation of Deep Leakage Attacks and Defenses,"Federated Learning is a privacy preserving decentralized machine learning paradigm designed to collaboratively train models across multiple clients by exchanging gradients to the server and keeping private data local. Nevertheless, recent research has revealed that the security of Federated Learning is compromised, as private ground truth data can be recovered through a gradient inversion technique known as Deep Leakage. While these attacks are crafted with a focus on applications in Federated Learning, they generally are not evaluated in realistic scenarios. This paper introduces the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), a comprehensive benchmark for evaluating Deep Leakage attacks and defenses within a realistic Federated context. By implementing a unified benchmark that encompasses multiple state-of-the-art Deep Leakage techniques and various defense strategies, our framework facilitates the evaluation and comparison of the efficacy of these methods across different datasets and training states. This work highlights a crucial trade-off between privacy and model accuracy in Federated Learning and aims to advance the understanding of security challenges in decentralized machine learning systems, stimulate future research, and enhance reproducibility in evaluating Deep Leakage attacks and defenses.","Marked by the proliferation of IoT devices, concerns regarding privacy, and information leakage have spurred the enactment of data protection legislation worldwide, highlighted by the GDPR in the European Union, and the Data Protection Act in the UK [7, 1]. Traditional centralized machine learning techniques face challenges in safeguarding privacy during data collection and sharing, prompting the rise of Federated Learning (FL) as a privacy-preserving distributed learning paradigm [19]. FL enables collaborative model training among participants without the exchange of local private data, a pivotal shift away from centralized methods. It operates by distributing the current model to each client, where local data is utilized for training. Gradient or model updates are subsequently shared with a central server for aggregation, as exemplified in algorithms like FedSGD and FedAvg respectively. This distributed approach not only addresses privacy concerns but also enhances scalability and security by minimizing data transmission, storage, and management overhead. FL finds application across diverse domains, from healthcare to autonomous vehicles, enabling the development of robust machine learning models while seemingly respecting data privacy and security regulations[22, 21]. Unfortunately, FL instils a misleading sense of security as in recent years a gradient inversion attack called Deep Leakage has emerged [34]. This attack aims to uncover private training data by analyzing the shared gradients transmitted to the server by the client. The attacker, often a curious server or a third party intercepting the communications, employs a randomly initialized dummy image and the shared global model to compute dummy gradients. The goal is then to minimize the distance between these dummy gradients and the intercepted shared gradients, thus optimizing the dummy image to closely resemble the ground truth. Although Deep Leakage was initially proposed within the context of Federated Learning, early iterations of the attack were primarily limited to untrained networks [34, 32]. Subsequent endeavours have aimed for more realistic scenarios by incorporating gradients from trained networks; however, these efforts typically assume a single steady-state model, failing to sufficiently consider how the recovery changes during training [11, 30]. In a realistic scenario with a sustained attack being undertaking throughout the training process the attacker can theoretically achieve more accurate recoveries[13, 14]. We call these approaches which leverage information from multiple timestamps Multi-Observation attacks. Previous Deep Leakage techniques have been assessed using differing metrics, across dissimilar datasets, and employing divergent attack protocols [34, 11, 13]. Prior codebases have utilized pretrained models in a stable state and lack implementation within a Federated Context or have minimal evaluation protocols in place[12, 15, 26]. In this paper, we present the FEDLAD Framework (Federated Evaluation of Deep Leakage Attacks and Defenses), which offers a diverse set of evaluation metrics across various datasets, incorporating implementations of numerous state-of-the-art attacks and defenses, and is designed to be extensible for future research. We hope this will help stimulate improved growth in the field, and better allow reproducibility in the future. In FEDLAD, we consider a server that is honest-but-curious. This server conducts training as intended but, in its curiosity, collects gradients at different timestamps to construct its own dataset using the recovered ground truths. To be clear, the attack does not adversely affect the performance of the final FL model, but rather leaks the private data held by the participants. Our threat model in our benchmark assumes the attacker only has access to information known to the FL server and does not allow the attacker to modify the model structure as this would alert the victim of the attack. Auxiliary information is permissible when derived from commonly shared information, such as batch normalization statistics from global models [29]. Finally, the attacker cannot know the quality of the recovery as they don‚Äôt have access to the ground truth. The code for our benchmark is available in the supplementary material. Our main contributions in this paper are as follows: ‚Ä¢ We present the FEDLAD Framework designed to assess single-observation and Multi-Observation attacks within the context of Federated Learning. ‚Ä¢ We unify various state-of-the-art attacks into a single joint formulation, and demonstrate the effectiveness of each approach across several widely recognised metrics for a variety of batch sizes, attack paradigms and defences. ‚Ä¢ We integrate FEDLAD with the popular Flower library for federated learning, and extend Flower to operate in FedSGD mode. We release all relevant code, ensuring that future works can also compare fairly and benefit from the new insights highlighted by the benchmark."
https://arxiv.org/html/2411.02926v1,Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic Encryption for Collaborative Anti-Money Laundering,"Combating money laundering has become increasingly complex with the rise of cybercrime and digitalization of financial transactions. Graph-based machine learning techniques have emerged as promising tools for Anti-Money Laundering (AML) detection, capturing intricate relationships within money laundering networks. However, the effectiveness of AML solutions is hindered by data silos within financial institutions, limiting collaboration and overall efficacy. This research presents a novel privacy-preserving approach for collaborative AML machine learning, facilitating secure data sharing across institutions and borders while preserving privacy and regulatory compliance. Leveraging Fully Homomorphic Encryption (FHE), computations are directly performed on encrypted data, ensuring the confidentiality of financial data. Notably, FHE over the Torus (TFHE) was integrated with graph-based machine learning using Zama Concrete ML. The research contributes two key privacy-preserving pipelines. First, the development of a privacy-preserving Graph Neural Network (GNN) pipeline was explored. Optimization techniques like quantization and pruning were used to render the GNN FHE-compatible. Second, a privacy-preserving graph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was successfully developed. Experiments demonstrated strong predictive performance, with the XGBoost model consistently achieving over 99% accuracy, F1-score, precision, and recall on the balanced AML dataset in both unencrypted and FHE-encrypted inference settings. On the imbalanced dataset, the incorporation of graph-based features improved the F1-score by 8%. The research highlights the need to balance the trade-off between privacy and computational efficiency.","1.1 Background and Motivation 1.1.1 The Money Laundering Problem. Money laundering is the process of concealing the origins of illegally obtained funds to make them appear legitimate. It poses severe consequences, fostering corruption, organized crime, terrorism, and environmental offenses. Beyond discouraging foreign investments and distorting international capital flows, money laundering poses a substantial threat to the stability of the financial system and the broader economy. The United Nations estimates that 2-5% of global GDP is laundered annually, with only 1% of these funds seized [1]. For example, in 2023, Singapore uncovered a money laundering case involving over S$3 billion (US$2.21 billion), making it one of the largest cases discovered globally [2]. 1.1.2 Graph-Based Machine Learning in Anti-Money Laundering (AML). Money laundering activities inherently involve network structures, where their web of transactions can be modelled by graph-like patterns such as fan-out, fan-in, gather-scatter and so on [3]. Machine learning have gained considerable interest in uncovering hidden patterns in transaction data [4]. Particularly, graph-based machine learning [5] including graph neural networks (GNNs) show great promise in AML detection (section 2.1). They can effectively capture complex relationships between entities such as individuals, businesses and accounts, identifying money-laundering patterns, learning anomalies, and unveiling hidden connections that are often missed by traditional detection methods. 1.1.3 The Challenge of AML Silos. Despite the advancements in AML solutions, AML transaction monitoring and detection are often conducted in silos within each financial institution [6], hindering the effectiveness of AML solutions. Adherence to data privacy regulations, such as the EU‚Äôs General Data Protection Regulation (GDPR) [7], poses legal challenges to financial institutions in maintaining the privacy of transaction data when data sharing. It is not possible for financial institutions to directly share meta-information about transaction accounts or owners without explicit consent. Consequently, this prevents the comprehensive modeling of transaction networks and limits the ability of financial institutions to track transactions and customer relationships spanning across multiple institutions and borders, which is often exploited by criminals orchestrating intricate schemes across multiple institutions. 1.1.4 Collaborative AML. To effectively combat money laundering, a collaborative approach is required. Emerging solutions involve leveraging a Trusted Third Party (TTP) to aggregate and analyze data, while preserving privacy. For example, the Monetary Authority of Singapore (MAS) developed COSMIC in Singapore [8], a platform for collaborative sharing of Money Laundering/Terrorism Financing (ML/TF) information among banks using rule-based transaction monitoring [9]. However, many institutions still lack legally-compliant infrastructure for secure data sharing. According to a FATF report, regulatory challenges and data privacy concerns were the two most frequently cited challenges in developing and implementing new AML technologies, with nearly 70% and 60% of respondents highlighting these issues respectively [10]. 1.1.5 Privacy-Preserving Technologies. Privacy-preserving technologies, such as Homomorphic Encryption (HE), offer a promising solution. HE allows computations to be performed directly on encrypted data without decryption, ensuring data confidentiality. The increasing adoption and community support of HE, such as Fully Homomorphic Encryption over the Torus (TFHE) (section 3.2), motivates research in applying it to AML to enable secure collaborative efforts. 1.2 Objective and Contributions This paper presents a novel privacy-preserving AML approach using TFHE to enable secure data sharing and collaboration between financial institutions while protecting data privacy. To the best of my knowledge, while there are recent research works that have conducted separate explorations of graph-based machine learning for AML (section 2.1), Gradient-Boosting Tree (GBT) for AML (section 2.2), privacy-preserving technologies (PPTs) for collaborative AML and privacy-preserving machine learning respectively (section 2.3), none have combined these areas of study to explore the use of GNN or GBT with FHE in AML, all the less so with the TFHE scheme. Hence, this paper makes the following contributions: 1. A proposed solution architecture for collaborative privacy-preserving machine learning for AML using FHE. This is detailed in section 3.5. 2. Exploration of the feasibility of developing a privacy-preserving GNN pipeline integrating TFHE using Concrete ML [11] for money laundering detection. This exploration is detailed in section 4. 3. Development of a privacy-preserving graph-based XGBoost pipeline leveraging the Graph Feature Preprocessor (GFP) [12] and integrating TFHE using Concrete ML [11] to detect money laundering. This is detailed in section 5. 4. A set of experiments with incrementally GFP-enriched graph features using XGBoost, comparing the model performance for both unencrypted and TFHE-encrypted inference, and against a basic XGBoost baseline. The results evaluated the trade-offs of building a privacy-preserving pipeline between privacy and model performance. This is detailed in section 6. We have also released the project code repository on GitHub111https://github.com/fabecode/GraphML-FHE."
https://arxiv.org/html/2411.02805v1,NinjaDoH: A Censorship-Resistant Moving Target DoH ServerUsing Hyperscalers and IPNS,"We introduce NinjaDoH, a novel DNS over HTTPS (DoH) protocol that leverages the InterPlanetary Name System (IPNS), along with public cloud infrastructure, to create a censorship-resistant moving target DoH service. NinjaDoH is specifically designed to evade traditional censorship methods that involve blocking DoH servers by IP addresses or domains by continually altering the server‚Äôs network identifiers, significantly increasing the complexity of effectively censoring NinjaDoH traffic without disruption of other web traffic. We also present an analysis that quantifies the DNS query latency and financial costs of running our implementation of this protocol as a service. Further tests assess the ability of NinjaDoH to elude detection mechanisms, including both commercial firewall products and advanced machine learning-based detection systems. The results broadly support NinjaDoH‚Äôs efficacy as a robust, moving target DNS solution that can ensure continuous and secure internet access in environments with heavy DNS-based censorship.","The Domain Name System (DNS) is a crucial component of the Internet, responsible for translating human-readable domain names into machine-readable IP addresses. This service traditionally operates over port 53/udp and is integral to the functionality of the Web. However, traditional DNS queries are sent in plaintext, making them vulnerable to various attacks, including DNS spoofing, hijacking, and surveillance by third parties. These vulnerabilities are often exploited by governments and organizations that enforce censorship through DNS-based firewalls. DNS-based firewalls monitor DNS requests and can block access to specific domain names, thus restricting users from visiting certain websites or services. DNS-based firewalls have been used to enforce censorship against platforms such as Wikipedia, TikTok, and political websites in several regions [1, 2, 3]. To address these vulnerabilities, encrypted DNS protocols, such as DNS over TLS (DoT) [4], DNS over QUIC (DoQ) [5], DNSSEC [6], and DNS over HTTPS (DoH) [7], were developed. DoT, DoQ, and DNSSEC encrypt DNS traffic, protecting it from eavesdropping, but operate on distinct ports (853/tcp, 853/udp, and 53/tcp, respectively), which make them easy to identify and block. In contrast, DoH integrates DNS requests with regular HTTPS traffic on port 443/tcp, the same port utilized for most encrypted web traffic. This makes it much harder for firewalls to block DoH without disrupting regular internet activities, as DoH is effectively masked within standard HTTPS traffic. As a result, DoH has the advantage of bypassing firewalls that block all outgoing DNS queries on port 53 or 853. Efforts to counteract DoH‚Äôs ability to evade DNS-based censorship have led to several research initiatives focusing on specifically detecting and blocking DoH traffic. Early approaches relied primarily on list-based methods, which involve maintaining a static blocklist of well-known public DoH server IP addresses and domains to block them at the network-level firewall. However, this approach is limited due to the increasing ease of self-hosting DoH servers [8, 9] on cloud infrastructure with vast IP address spaces that can make an infrequently updated blocklist ineffective. Entirely blocking outbound HTTPS traffic to hyperscalers such as AWS [10], Google Cloud [11], or Azure [12] is impractical for most censors as these hyperscalers also host numerous essential web services. As a result, blanket blocking of hyperscalers to target DoH services would cause significant unintended disruptions, making it an ineffective censorship strategy. More sophisticated DoH detection mechanisms have also been developed that utilize machine learning (ML)-based techniques [13, 14, 15, 16]. These ML models analyze traffic flow patterns and other features to distinguish DoH traffic from regular HTTPS traffic. While some of these detection techniques have high false positive rates, their potential utility towards blocking DoH traffic emphasizes the necessity for more censorship-resistant DNS solutions. In this paper, we introduce NinjaDoH, a novel DoH client-server protocol designed to be censorship-resistant. The NinjaDoH protocol employs a moving target defense by dynamically changing the server IP address through the use of public cloud infrastructure, and securely sharing the latest server IP address with the client(s). NinjaDoH‚Äôs client-side software continuously updates the operating system to use the most recent server IP address for DoH queries. To mitigate propagation delays in sharing of new IP addresses with the client, the server temporarily keeps older IP addresses active (alongside the new IP address), ensuring continuous availability for clients in-between IP updates. While there are various methods to securely share the latest server IP address, we leverage the InterPlanetary File System (IPFS) [17] for its decentralized nature, which makes it more difficult for adversaries to detect or block [18]. The NinjaDoH client integrates fully within the operating system, making it compatible with all browsers and applications without requiring any special configuration or additional plugin. This is in contrast to out-of-band DNS methods like DNS in Google Sheets [19] or DNS over Discord [20], which lack seamless integration with user environments. This moving target architecture makes NinjaDoH highly resilient against list-based blocking methods, as the server IP addresses are frequently rotated, making it practically impossible to maintain an accurate blocklist. We also evaluate NinjaDoH‚Äôs effectiveness against ML-based detection methods, which attempt to analyze traffic patterns to block DoH queries. NinjaDoH demonstrates the ability to evade even these advanced detection systems while maintaining a high level of performance. Our key contributions in this paper are as follows: ‚Ä¢ Design and Implementation of NinjaDoH Protocol: We design and then implement the NinjaDoH protocol, a moving target DoH server that leverages public cloud infrastructure and IPNS to dynamically rotate its IP addresses, making it resilient against list-based DNS blocking and detection methods. ‚Ä¢ Comprehensive Performance Evaluation: We evaluate NinjaDoH‚Äôs performance in terms of DNS query latency and compare it against other DNS services, including well-known DoH providers and censorship-resistant alternatives like DoH over Tor. Our results demonstrate that NinjaDoH delivers low-latency performance, comparable to well-known public DoH services. ‚Ä¢ Evaluation of Censorship Resistance: We empirically demonstrate NinjaDoH‚Äôs ability to evade both static, list-based blocking and more advanced ML-based detection systems that attempt to identify DoH traffic. This adaptability ensures the DoH service remains accessible in networks with heavy censorship. ‚Ä¢ Cost Analysis: We provide analysis of NinjaDoH‚Äôs operational costs, showing that it is an affordable solution for individuals and organizations seeking a censorship-resistant DNS service. While NinjaDoH empowers users in censored networks to bypass DNS firewalls and freely access the internet, it simultaneously presents challenges for enterprise administrators who rely on protective DNS filtering for security [21]. NinjaDoH can bypass such critical defenses, highlighting the dual nature of censorship-resistant tools."
https://arxiv.org/html/2411.02798v1,TRANSPOSE: Transitional Approaches for Spatially-Aware LFI Resilient FSM Encoding,"Finite state machines (FSMs) regulate sequential circuits, including access to sensitive information and privileged CPU states. Courtesy of contemporary research on laser attacks, laser-based fault injection (LFI) is becoming even more precise where an adversary can thwart chip security by altering individual flip-flop (FF) values. Different laser models, e.g., bit flip, bit set, and bit reset, have been developed to appreciate LFI on practical targets. As traditional approaches may incorporate substantial overhead, state-based SPARSE and transition-based TAMED countermeasures were proposed in our prior work to improve FSM resiliency efficiently. TAMED overcame SPARSE‚Äôs limitation of being too conservative, and generating multiple LFI resilient encodings for contemporary LFI models on demand. SPARSE, however, incorporated design layout information into its vulnerability estimation which makes its vulnerability estimation metric more accurate. In this paper, we extend TAMED by proposing a transition-based encoding CAD framework (TRANSPOSE), that incorporates spatial transitional vulnerability metrics to quantify design susceptibility of FSMs based on both the bit flip model and the set-reset models. TRANSPOSE also incorporates floorplan optimization into its framework to accommodate secure spatial inter-distance of FF-sensitive regions. All TRANSPOSE approaches are demonstrated on 5 multifarious benchmarks and outperform existing FSM encoding schemes/frameworks in terms of security and overhead.","Physical attacks can target secure portions of system on chips (SoCs) and cryptographic circuits thereby jeopardizing the integrity and confidentiality. Among the options, fault injection attacks entail external or internal active maneuvering that lead to a fault. Laser fault injection (LFI) stands out as a highly precise method capable of inducing faults at a very fine resolution (even affecting just a single byte or bit) [1]. A contemporary LFI set-up allows control of fault injection time (pulse duration and shot instant), repeatability, and localization. Unlike other fault attacks like voltage variations [2] or clock glitches [3], LFI requires strict adherence to specific constraints regarding duration to ensure both spatial and temporal accuracy, thereby ensuring an exact fault occurs [4, 5]. Experiments on LFI demonstrate data dependent and data independent fault models, i.e., bit-reset/set models and bit flip model, respectively [6]. A bit reset (resp. a bit-set) models a fault that alters the target bit from 1 to 0 (resp. from 0 to 1). However, if the current bit is already at 0 (resp. 1 for bit-set) there is no effect. A bit-flip corresponds to a fault irrespective of the target‚Äôs current state. Current research highlights the laser-sensitive areas in a D flip-flop (DFF) to laser-induced faults, considering both data-dependent and data-independent fault models [7, 8]. Attackers can exploit these precise vulnerable regions in current and future technology nodes [8]. It is crucial to acknowledge the significance of identifying these specific sensitive areas when developing countermeasures against LFI. Even targeting a few transistors with a less precise/ relaxed laser spot can cause significant faults, highlighting the absence of inherent protection against LFI even at the nanoscale technology level [4]. Furthermore, as effectuating faults with relaxed spot size is a possibility, countermeasures must also incorporate as many relaxed constraints on DFFs as possible to conserve area and power along with security still intact [9]. Current countermeasures such as hardware irradiation detectors [5] are costly. CAD tools, in contrast, can automatically integrate logical methods such as redundancy or security-aware encoding techniques [10]. Another strategy involves state exploration using coding theory approaches, where each state of a finite state machine (FSM) is treated as a linear or nonlinear code. This enhances FSM resilience against fault injection (FI) through error correction or detection mechanisms. However, this approach increases chip area, power usage, and affects performance since it assumes all states require equal protection. In our earlier research, we introduced state exploration methods that promote LFI-resistant encoding for arbitrary FSM sizes and numbers of lasers, specifically PATRON and SPARSE [11, 9]. Unlike approaches based solely on coding theory, these methods focus on safeguarding critical FSM states, making them less conservative. However, they do not take into account the exact laser-sensitive areas necessary to distinguish between data-dependent bit-set/reset and data-independent bit-flip fault models. Additionally, PATRON and SPARSE assume protection for all sensitive states in the FSM, which can prove to be weighty in the overhead. To that end, a more recent transition-based approach named TAMED is proposed [12] which protects only the specific authorized transitions [10] in the FSM. Although all the above approaches individually provide unique elements that benefit LFI research, an all-encompassing comprehensive framework is missing in the literature that cherry-picks all the beneficial concepts and combines them into an efficient vulnerability-monitoring and low overhead approach. Further, advanced architectures are always required to optimize cost, area, performance penalties, and power consumption as they are crucial constraints in modern system design. Thus, examining all the contrasting design requirements while maintaining proper security, we introduce TRANSPOSE (TRANSitional APproaches fOr Spatially-Aware LFI Resilient State Machine Encoding) which makes the FSM inherently tolerant to precise LFI sensitive areas. Particularly, our contributions are: ‚Ä¢ An automated generation of LFI-resistant state encoding that integrates with commercial CAD tools such as Design Compiler and IC Compiler II. Through the use of linear programming (LP), TRANSPOSE can identify a single, LFI-resistant encoding without any manual input. ‚Ä¢ We propose the Spatial Transitional Vulnerability Metrics (S‚Å¢T‚Å¢V‚Å¢MùëÜùëáùëâùëÄSTVMitalic_S italic_T italic_V italic_M), which identify vulnerabilities missed by the previously proposed V‚Å¢MùëâùëÄVMitalic_V italic_M (PATRON), S‚Å¢V‚Å¢MùëÜùëâùëÄSVMitalic_S italic_V italic_M (SPARSE), and T‚Å¢V‚Å¢MùëáùëâùëÄTVMitalic_T italic_V italic_M (TAMED). S‚Å¢T‚Å¢V‚Å¢MùëÜùëáùëâùëÄSTVMitalic_S italic_T italic_V italic_M incorporate FF-sensitive regions and thus address both data-dependent and data-independent models. ‚Ä¢ We expand TRANSPOSE‚Äôs LP criteria to protect as many critical transitions from both the data-dependent and data-independent models. For any arbitrary FSM, encoding, and placement are co-optimized in terms of area overhead, switching activity (dynamic power consumption), and security for a multi-laser adversary. ‚Ä¢ We demonstrate TRANSPOSE on 5 diverse controller benchmarks and compare its security and overhead to other security-aware encoding techniques. An outline for the rest of this paper is as follows. In the next section, we discuss basic notation and common terms, FSM definitions, and a motivating example. In Section III, security assessment via contemporary work and flip-flop sensitivity are described and used to constitute a realistic threat model for TRANSPOSE. Subsequently, examples with previously proposed metrics are shown to misconstrue vulnerability in FSMs triggering the need for S‚Å¢T‚Å¢V‚Å¢MùëÜùëáùëâùëÄSTVMitalic_S italic_T italic_V italic_M. Section IV delineates the TRANSPOSE methodology which incorporates a precise model, discussion on salient parameters, and multiple transition types along with the proposed metric leading to comprehensive secure encoding and floorplan optimization procedures. Results are presented and discussed in Section V. Finally, conclusions and future work are given in the last section."
https://arxiv.org/html/2411.02775v1,BrewingVodka: Distilling Pure Knowledge for Lightweight Threat Detection in Audit Logs,"Advanced Persistent Threats (APTs) are continuously evolving, leveraging their stealthiness and persistence to put increasing pressure on current provenance-based Intrusion Detection Systems (IDS). This evolution exposes several critical issues: (1) The dense interaction between malicious and benign nodes within provenance graphs introduces neighbor noise, hindering effective detection; (2) The complex prediction mechanisms of existing APTs detection models lead to the insufficient utilization of prior knowledge embedded in the data; (3) The high computational cost makes detection impractical.To address these challenges, we propose Vodka, a lightweight threat detection system built on a knowledge distillation framework, capable of node-level detection within audit log provenance graphs. Specifically, Vodka applies graph Laplacian regularization to reduce neighbor noise, obtaining smoothed and denoised graph signals. Subsequently, Vodka employs a teacher model based on GNNs to extract knowledge, which is then distilled into a lightweight student model. The student model is designed as a trainable combination of a feature transformation module and a personalized PageRank random walk label propagation module, with the former capturing feature knowledge and the latter learning label and structural knowledge. After distillation, the student model benefits from the knowledge of the teacher model to perform precise threat detection. Finally, Vodka reconstructs attack paths from anomalous nodes, providing insight into the attackers‚Äô strategies. We evaluate Vodka through extensive experiments on three public datasets and compare its performance against several state-of-the-art IDS solutions. The results demonstrate that Vodka achieves outstanding detection accuracy across all scenarios and the detection time is 1.4 to 5.2 times faster than the current state-of-the-art methods.","Advanced Persistent Threats (APTs)(Alshamrani et al., 2019) represent a complex form of cyber attack characterized by high stealth and strong targeting. In these attacks, perpetrators gain unauthorized access to a victim‚Äôs machine through methods such as network or software backdoors, and persist for extended periods to steal sensitive data or take control of the target machine. APTs have already infiltrated many highly secured large enterprises and institutions that are based on web services, causing substantial financial losses(APT, 2020; Khaleefa and Abdulah, 2022; Sharma et al., 2023). Notable examples include the Equifax(Equ, 2017) breach, which resulted in a record number of user data being stolen, and the SolarWinds(Sol, 2020) attack, which had a vast scope and severe impact. To combat these sophisticated APTs attacks, host-based Intrusion Detection intrusion detection systems (IDS) have become a widely deployed defense mechanism. However, with the evolving nature of attack techniques and the growing scale of threats, traditional IDS solutions can no longer meet the demands of the changing threat landscape. Currently, provenance -based(Li et al., 2021) detection methods are considered effective for capturing APTs. These methods apply system audit logs and structure system entities (such as processes, files, and network flows) into a graph structure known as a provenance graph. Based on how audit logs are utilized, these detectors can be divided into three categories: statistics-based detection(Hassan et al., 2019, 2020b; Liu et al., 2018) quantifies the suspiciousness of audit logs by analyzing the rarity in the provenance graph; rule-based detection(Hassan et al., 2020a; Hossain et al., 2017; Milajerdi et al., 2019) matches audit logs with attack patterns using expert security knowledge bases; and learning-based detection(Jia et al., 2024; Zengy et al., 2022; Cheng et al., 2023; Wang et al., 2022; Han et al., 2020; Rehman et al., 2024) employs machine learning techniques to learn from the provenance graph, identifying abnormal system behaviors and attack patterns. Among these approaches, learning-based detection has been considered the most promising in recent years. However, we have observed several persistent challenges that impact real-world detection: ‚Ä¢ Neighbor Noise: In provenance graphs constructed from audit logs, malicious nodes often exhibit long-distance, multi-hop distributions, meaning that a complete attack can involve more benign system entities. The interaction between malicious and benign nodes generates interference and confounds the classifier in detection systems. For node-level classification tasks, the neighboring nodes adjacent to the target detection node introduce dense neighbor noise, which can obscure true anomalies, leading to false positives or missed detections, and ultimately reducing detection accuracy. ‚Ä¢ High Computational Cost: Graph-based algorithms enable learning from provenance graphs to capture the complex relationships within them. While these methods can achieve impressive detection performance, they come at the cost of significant memory and time overhead. As a result, these approaches lose the capability for real-time detection and can only function as offline systems to analyze graph data(Zengy et al., 2022; Wang et al., 2022). ‚Ä¢ Insufficient Utilization of Prior Knowledge: Recent APTs detection methods(Jia et al., 2024; Zengy et al., 2022; Cheng et al., 2023) primarily rely on large-scale graph algorithms, such as GNNs, to extract structural and node feature information from graphs. However, the prediction mechanism of GNNs is highly complex, as it tightly integrates graph topology, node features, and projection matrices. This entanglement makes it challenging to clearly interpret the relationships between these factors within the model. This complexity prevents the effective utilization of prior knowledge in terms of labels, features, and structure(Li et al., 2019; Wang and Leskovec, 2020). Thus, we propose Vodka, a lightweight APTs detection method that achieves both high detection accuracy and computational efficiency. Specifically, Vodka incorporates the following key functionalities: (1) Vodka constructs a provenance graph from audit logs that include various APTs attacks and uses Word2Vec to assign initial features to nodes. To address the issue of neighbor noise, Vodka designs a graph Laplacian regularization-based graph signal denoising method tailored to the provenance graph. This algorithm smooths node signals without disrupting the original graph topology. (2) Vodka introduces a knowledge distillation framework where, during training, knowledge from a pre-trained large GNN teacher model is distilled into a lightweight student model. The lightweight student model is then used for anomalous node detection, significantly reducing detection costs. (3) For the student model, Vodka proposes a new hybrid mechanism that combines the Feature Transformation (FT) mechanism with a personalized PageRank random walk label propagation (PRL). This allows more effective learning of prior knowledge from features, labels, and structures while utilizing the GNN knowledge from the teacher model. (4) After detecting anomalous nodes, Vodka uses a community division algorithm to identify malicious communities. We performed a comprehensive evaluation of Vodka using datasets widely adopted by the research community, including StreamSpot(The, 2016), Unicorn Wget(Han et al., 2020), and DARPA-E3(DAR, 2020). We first analyzed its detection performance and compared it against various existing baselines, demonstrating that Vodka consistently outperforms almost all these systems under the same evaluation metrics. Furthermore, we analyzed Vodka‚Äôs detection costs and explored its potential as a real-time detection system. In addition, hyperparameter analysis and ablation experiments on Vodka highlight the irreplaceability of its key parameters and main components. Lastly, we verified Vodka‚Äôs inherent robustness against adversarial attacks. The primary contributions of our work are as follows: Figure 1. Attack scenario from DARPA E3 CADETS. Green indicates benign entities and red signifies malicious entities. R = Read, W = Write, O = Open, E = Execute, S = Send and Rc = Receive. \Description A figure illustrating four scenes. ‚Ä¢ We propose Vodka, a general APTs detection method based on knowledge distillation, capable of transferring GNN knowledge from a large teacher model to a lightweight student model, thereby reducing resource and time costs during detection. ‚Ä¢ We address the issue of neighbor noise in provenance graphs by designing a graph Laplacian regularization-based graph signal denoising method that smooths and denoises signals tailored for provenance graphs. ‚Ä¢ We examine the problem of insufficient utilization of prior knowledge caused by complex prediction mechanisms in current approaches and design a lightweight student model to effectively learn and leverage prior knowledge. ‚Ä¢ We comprehensively evaluate Vodka ‚Äôs performance by implementing it on three widely used datasets and conducting various experiments to verify its effectiveness. Experimental results demonstrate that Vodka outperforms most existing methods while incurring lower detection costs."
https://arxiv.org/html/2411.02773v1,FedBlock: A Blockchain Approach to Federated Learning against Backdoor Attacks,"Federated Learning (FL) is a machine learning method for training with private data locally stored in distributed machines without gathering them into one place for central learning. Despite its promises, FL is prone to critical security risks. First, because FL depends on a central server to aggregate local training models, this is a single point of failure. The server might function maliciously. Second, due to its distributed nature, FL might encounter backdoor attacks by participating clients. They can poison the local model before submitting to the server. Either type of attack, on the server or the client side, would severely degrade learning accuracy. We propose FedBlock, a novel blockchain-based FL framework that addresses both of these security risks. FedBlock is uniquely desirable in that it involves only smart contract programming, thus deployable atop any blockchain network. Our framework is substantiated with a comprehensive evaluation study using real-world datasets. Its robustness against backdoor attacks is competitive with the literature of FL backdoor defense. The latter, however, does not address the server risk as we do.","Federated Learning (FL) [1] is a Machine Learning approach to learning a model using distributed training data that remain private and unmoved on local machines. A typical FL architecture consists of these local machines, hereafter referred to as the ‚Äúclients‚Äù, which have the training data, and a central server to coordinate the training, called the ‚Äúaggregation‚Äù server. The learning is an iterative procedure. In the first step, the aggregation server broadcasts a global learning model, initially random, to all clients. In the second step, each client in parallel performs local training on its own local data to improve this model. In the third step, the clients send their respective improved models to the server who in turn aggregates them to obtain a new global model. Then the first step is repeated until the global model converges. FL is elegant in idea and has widespread applications [2]. However, its success relies on the server working normally and clients being good citizens. This is not always true in practice. Indeed, FL is vulnerable to two types of security risks: server attack and backdoor attack. Server attack: Like in any client/server system, the centralized server is a security bottleneck. In FL, an attacker who gains control over the aggregation server can distribute bad global models to the clients, thus poisoning the entire system. Naturally, besides typical solutions designed to secure a server, the best way to avoid server attacks is by deploying multiple servers to decentralize this single point of failure. However, we then face a new challenge of how to coordinate these servers because the ‚Äúcoordinator‚Äù could itself become a new single point of failure. Backdoor attack: Dishonest clients can send fake local models to the server. In ‚Äúuntargeted‚Äù backdoor attacks, the poisoned global model leads to randomly bad predictions. In ‚Äútargeted‚Äù backdoor attacks, the attacker‚Äôs goal is to make the global model always predict according to a targeted outcome. Backdoor attacks are common threats to computer systems. According to the IBM Security X-Force Threat Intelligence Index 2023, they are the top action by cybercriminals. Nearly a quarter of cyber incidents last year involved backdoor attacks. Recently, backdoor danger has been demonstrated with FL systems, raising serious concerns [3]. In this paper, we are interested in immunizing FL against the above attacks. Our approach is inspired by blockchain technology [4]. Blockchain is a computing solution for executing transactions in a way that is honest, immutable, and traceable. Because blockchain runs on a decentralized network of many autonomous computers with majority-based consensus, it is unaffected by any attack. Our idea is that, instead of running the centralized model aggregation on a server, we run it on the ‚Äúblockchain computer‚Äù. Precisely, we implement the task of centralized aggregation as a smart contract to deploy on a blockchain network. An immediate benefit is that the risk due to server attacks becomes non-existent. Our goal then is narrowed to how to mitigate backdoor attacks. From the system-wide perspective, our work is a fresh direction compared to the literature. FL defense against server attacks and backdoor attacks mainly serves non-blockchain settings [5]. Meanwhile, blockchain use for FL has started only recently with purposes mostly not about backdoor risks, for example, to remove the dependency on the central server [6], incentivize clients to contribute training data [7], or provide robustness to non-backdoor security vulnerabilities [8]. Existing research on backdoor defense in blockchain-based FL is rare [9], which requires creating a dedicated blockchain network, a difficult task with limited adoption. In contrast, we make the following key contributions: ‚Ä¢ We propose FedBlock, a blockchain-based FL framework that provides simultaneous defense against server attacks and backdoor attacks, works at the smart contract level, and hence can run on any blockchain network. To our knowledge, this is the first framework with such features. ‚Ä¢ We propose an implementation of FedBlock integrating a realistic backdoor-defense technique. The result is a novel decentralized backdoor defense for FL. It uses an efficient amount of model data that is discriminative enough to distinguish compromised clients from the benign. ‚Ä¢ We justify and validate our technique with in-depth experiments. We evaluate comprehensive scenarios using various datasets, threat models, and attack scenarios. The results demonstrate FedBlock‚Äôs viability and superiority compared to the literature. The remainder of the paper is organized as follows. Related work is reviewed in Section II. Some preliminaries on federated learning and backdoor models are provided in Section III. The FedBlock framework is proposed in Section IV, followed by a specific backdoor-defense implementation in Section V. The evaluation results are discussed in Section VI. The paper concludes in Section VII with pointers to our future work."
https://arxiv.org/html/2411.02670v1,Visually Analyze SHAP Plots to Diagnose Misclassifications in ML-based Intrusion Detection,"Intrusion detection has been a commonly adopted detective security measures to safeguard systems and networks from various threats. A robust intrusion detection system (IDS) can essentially mitigate threats by providing alerts. In networks based IDS, typically we deal with cyber threats like distributed denial of service (DDoS), spoofing, reconnaissance, brute-force, botnets, and so on. In order to detect these threats various machine learning (ML) and deep learning (DL) models have been proposed. However, one of the key challenges with these predictive approaches is the presence of false positive (FP) and false negative (FN) instances. This FPs and FNs within any black-box intrusion detection system (IDS) make the decision-making task of an analyst further complicated. In this paper, we propose an explainable artificial intelligence (XAI) based visual analysis approach using overlapping SHAP plots that presents the feature explanation to identify potential false positive and false negatives in IDS. Our approach can further provide guidance to security analysts for effective decision-making. We present case study with multiple publicly available network traffic datasets to showcase the efficacy of our approach for identifying false positive and false negative instances. Our use-case scenarios provide clear guidance for analysts on how to use the visual analysis approach for reliable course-of-actions against such threats.","Intrusion detection systems are ubiquitous in network and commuter systems which check on every request and response over a computer or network and examines for indications of potential cyber attacks or threats, including attempts for exploitation and other situations that poses an immediate threat to the network [1]. Enhancing the effectiveness of current IDS is challenging and crucial for detective and preventive cyber defense. As the digital era progresses, computer systems and networks including internet of things (IoT) are vulnerable to more sophisticated attacks. Alike the traditional sensor networks, IoT network also has data traffic to be shared among multiple IoT devices such as smart home (e.g., Google Home, Amazon Echo), health monitoring (e.g., DexCom glucose monitor), wearable (e.g., Fitbit), smart manufacturing (e.g., collaborative robots), smart agriculture (e.g., soil sensors) or smart retail (e.g., Beacons, smart shelves) those are vulnerable to various cyber threats. The most common types of attacks observed within the IoT environments are Distributed Denial of Service (DDoS), Denial of Service (DoS), brute force attacks, spoofing attacks, website-based attacks (e.g., XSS, SQL injection, defacement), man-in-the-middle attacks, replay attacks, network reconnaissance, and Mirai botnets [2]. To identify such attacks in any network, researchers have long since worked to come up with detection mechanisms that can be both adaptive to new types of attacks and also be more practical in real-world scenarios. The more promising motivations behind the usage of machine learning or other rule-based intrusion detection systems is that it reduces operational overhead and human-centric errors [3]. However, this also creates a backdoor for misclassification‚Äì either false positives or false negatives. Nonetheless, most of the state-of-the-art network intrusion detection systems (NIDS), host-based intrusion detection systems (HIDS), or log analysis [4] rely on black-box machine learning model-based prediction, which suffer from false positives and false negatives identifications. As most of the existing methods require decision from an analyst [5, 6, 7] to culminate the prediction of an ML model, it is expected that the analyst must be aided with corresponding features‚Äô contribution to make trustworthy decisions. Depending on the knowledge capability of the analyst, a visual characteristics of the misclassification cases (FPs and FNs) can be an effective approach, which is not systematically addressed in the existing literature for decision making. In this paper, we propose an explainable AI based intrusion detection and present a new step-by-step methodology for using SHAP feature explanation plots [8] by the analysts for potentially identifying false positives and false negatives. We present our methodological approach with empirical case studies on multiple publicly available network traffic datasets. We also discuss the usage of Brier score [9] as a reliable metric of confidence for various ML model‚Äôs performance evaluation when tested for classification of attack versus benign traffic within the datasets. Brier Score allows us to interpret how close the model predicted raw probability values to the actual outcomes. However, the black-box nature of models can not be addressed by Brier Score, and thus feature based SHAP explanation is necessary to interpret why a particular label is predicted for an individual traffic data-point. In summary, we have made the following major contributions in this paper: ‚Ä¢ Propose a feature explanation-based step-wise identification of false-positive and false-negative intrusion instances by visually analyzing feature explanations SHAP plots when overlapped with true-positive and true-negative group explanations. ‚Ä¢ Provide case study with multiple real-world intrusion datasets to showcase the efficacy in reducing misclassification (e.g., FPs and FNs) for eliable decision-making. Paper organization: Section II presents the related works. Section III presents the research questions and research methodology. Section IV depicts the case study results on various network traffic datasets. Section V discusses the limitations of the paper while section VI concludes the paper."
https://arxiv.org/html/2411.02618v1,Efficacy of EPSS in High Severity CVEs found in CISA KEV,"The Exploit Prediction Scoring System (EPSS) is designed to assess the probability of a vulnerability being exploited in the next 30 days relative to other vulnerabilities. The latest version, based on a research paper published in arXiv [jacobs2023enhancing], assists defenders in deciding which vulnerabilities to prioritize for remediation. This study evaluates EPSS‚Äôs ability to predict exploitation before vulnerabilities are actively compromised, focusing on high severity CVEs that are known to have been exploited and included in the CISA KEV catalog. By analyzing EPSS score history, the availability and simplicity of exploits, the system‚Äôs purpose, its value as a target for Threat Actors (TAs), this paper examines EPSS‚Äôs potential and identifies areas for improvement.","Introduction -A What is EPSS, CVSS, and CVE? The Exploit Prediction Scoring System (EPSS) is a well-established machine-learning based scoring system used to estimate the likelihood of a security vulnerability being exploited within the next 30 days. EPSS is used alongside the Common Vulnerability Scoring System (CVSS), which rates the severity of vulnerabilities from 0 to 10 based on potential damage. Together, these systems are vital for cybersecurity experts, as it enables them to prioritize vulnerability remediation effectively [Orca2024]. Common Vulnerabilities and Exposures (CVE) [CISA_KEV] is a system that categorizes and classifies security vulnerabilities. It uses the Common Vulnerability Scoring System (CVSS) [risto_2023] to evaluate the threat level of each identified vulnerability. While EPSS can help detect vulnerabilities likely to be exploited, further research and discussion is needed to see if it is truly effective in predicting high-severity vulnerabilities before they are widely exploited. -B What is CISA KEV? The Known Exploited Vulnerabilities (KEV) catalog, maintained by the Cybersecurity and Infrastructure Security Agency (CISA), lists vulnerabilities that have been confirmed as actively exploited in the wild. These vulnerabilities represent a subset of CVEs (Common Vulnerabilities and Exposures), which are represented by a CVE ID. The US Government issued a binding directive to ensure that vulnerabilities in the CISA KEV catalog were addressed by organizations doing business with the Federal Government. To be included in the CISA KEV catalog, the following criteria must be met [CISA_KEV]: 1. Active exploitation of the vulnerability has been observed, with either successful attacks or attempts that specifically target the vulnerability 2. Clear remediation guidance must be available. -C Why is prioritizing vulnerabilities important? At the time a vulnerability has been included in the CISA KEV catalog, it is already widely known to have been exploited. Due to the sheer volume of vulnerabilities, fixing every single one in a timely manner is simply not possible. On average, companies worldwide take between 88 and 208 days to patch vulnerabilities [statista2024], so it is important to prioritize the solutions for the most dangerous (i.e., most likely to be exploited) ones first. -D Why study this topic? The purpose of this study is to assess if EPSS scores are truly predictive rather than trailing indicators, as well as how useful they are for cybersecurity defenders. Although EPSS scores can detect vulnerabilities that lead to real exploits, there is a lack of research on whether or not the system is predictive. By assessing data from high severity CVEs, this study aims to validate the effectiveness of EPSS as a tool for vulnerability management, and to identify and discuss potential areas of improvement."
https://arxiv.org/html/2411.02597v1,Taming the Beast of User-Programmed Transactions on Blockchains: A Declarative Transaction Approach,"Blockchains are being positioned as the ‚Äùtechnology of trust‚Äù that can be used to mediate transactions between non-trusting parties without the need for a central authority. They support transaction types that are native to the blockchain platform or user-defined via user programs called smart contracts. Despite the significant flexibility in transaction programmability that smart contracts offer, they pose several usability, robustness and performance challenges.This paper proposes an alternative transaction framework that incorporates more primitives into the native set of transaction types (reducing the likelihood of requiring user-defined transaction programs often). The framework is based on the concept of declarative blockchain transactions whose strength lies in the fact that it addresses several of the limitations of smart contracts, simultaneously. A formal and implementation framework is presented and a subset of commonly occurring transaction behaviors are modeled and implemented as use cases, using an open-source blockchain database, BigchainDB as the implementation context. A performance study comparing the declarative transaction approach to equivalent smart contract transaction models reveals several advantages of the proposed approach.","Blockchains, as a technology for mediating and managing transactions between non-trusting parties, is becoming an increasingly popular concept. They are decentralized, fully replicated, append-only databases of transactions that are validated through a large, distributed consensus. These characteristics ensure that blockchain contents are tamper-proof and that no single authority controls a blockchain‚Äôs operation and contents, conferring a good degree of trust in them. Initially aimed at cryptocurrency, blockchain technology now extends to areas seeking data control and ownership decentralization, primarily for privacy and efficiency. This includes healthcare, (Agbo et al., 2019; McGhin et al., 2019), supply chain (Durach et al., 2021; Wu et al., 2019; Sund et al., 2020), decentralized finance (Defi) (Werner et al., 2021; Siyal et al., 2019), governance (Lumineau et al., 2021), web browsing, gaming, social media, and file sharing/storage (Al-Jaroodi and Mohamed, 2019). Blockchain transactions typically involve digital asset management aligned with business activities. The fundamental transaction type is asset ùñ≥ùñ±ùñ†ùñ≠ùñ≤ùñ•ùñ§ùñ±ùñ≥ùñ±ùñ†ùñ≠ùñ≤ùñ•ùñ§ùñ±\mathsf{TRANSFER}sansserif_TRANSFER between accounts, a native function in most blockchains. To address the diverse needs of modern applications, blockchains have evolved to include user-designed transactions known as smart contracts (Szabo, 1997). These contracts execute business operations and adhere to specific conditions. Examples include auction bidding and regulated patient record management. Recent survey (num, 2022) indicates the existence of over 44 million smart contracts on the Ethereum blockchain alone. Problem: Smart contracts, despite their flexibility, face adoption barriers due to several issues: (i) They require significant effort in creation and verification, offer limited reusability across platforms, and constrain automatic optimization possibilities. (ii) Vulnerable to user errors and security breaches, they pose financial risks, exemplified by the DAO attack (Mehar et al., 2019) that resulted in a loss of approximately 3.6M ETH (about $6.8B). (iii) Many transactional behaviors in smart contracts, embedded in programming structures, remain hidden on the blockchain, hindering their utility in complex data analysis. (iv) Their execution involves higher latency and costs compared to native transactions. The lack of validation semantics for these user-programmed transactions complicates concurrency conflict management, leading most platforms, including Ethereum, to adopt sequential execution, which lowers throughput. Declarative smart contracts (Chen et al., 2022), domain-specific languages (W√∂hrer and Zdun, 2020a), and smart contract templates (Hu et al., 2020) aim to ease creation and verification processes. However, they fall short in addressing performance, throughput, queryability, and other transactional model challenges in smart contracts. 1.1. Contributions: This paper investigates the feasibility and impact of lifting transactional behaviors typically found in smart contracts into the core blockchain layer as native transactions. Specifically, we propose: (1) a declarative and typed blockchain transaction model that includes the novel concept of nested blockchain transactions, as a foundation for modeling transactional behavior on blockchains. (2) concrete declarative blockchain transaction modeling of a sample transactional behavior represented in many smart contracts of the most popular blockchain application category - marketplaces. (3) an implementation framework for declarative blockchain transactions that builds on BigchainDB blockchain database‚Äôs architecture (big, [n. d.]), extending its transaction modeling and validation infrastructure. (4) a comparative performance and usability evaluation of the declarative transaction model vs. the smart contract model using Ethereum smart contracts as the baseline. The evaluation results demonstrate that the declarative transaction method significantly outperforms smart contracts, achieving improvements by a factor of 635 in latency and a minimum of 60 in throughput. The rest of the paper is organized as follows: Section 2 provides background information on blockchain native transactions, smart contracts, and BigchainDB. Section 3 introduces the formal blockchain transaction model and novel concepts of Non-nested and Nested transactions. Section 4 provides implementation details of the concepts presented in Section 3. Section 6 reviews the literature on the topic, while Section 5 reports on the comparative experiments conducted to evaluate our system and smart contract. Finally, we conclude the paper with a summary in Section 8."
https://arxiv.org/html/2411.03307v1,LLMs for Domain Generation Algorithm Detection,"This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs). We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection. SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining. We use Meta‚Äôs Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs. Results proved that LLM-based methods can achieve competitive results in DGA detection. In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains.","In the ever-evolving landscape of cybersecurity, domain generation algorithms (DGAs) have emerged as a significant threat, posing unique challenges to traditional security measures. DGAs are sophisticated tools employed by cybercriminals to dynamically create large numbers of domain names, primarily used to establish and maintain command and control (C&C) infrastructure for botnets and other malicious activities. The operation of a DGA is based on an initial seed or key, from which sequences of characters are generated to form domain names. These sequences can vary in length and structure, and encryption and obfuscation techniques are often used to further hinder their detection. DGAs pose significant challenges to cybersecurity by enabling botnets to frequently change command-and-control servers, making them resilient to takedown efforts and difficult for defenders to block. They also facilitate covert data exfiltration, malware distribution, and phishing campaigns by generating constantly shifting domains that evade detection by traditional security measures. Additionally, DGAs bypass reputation-based systems with newly created domains lacking history, overwhelming security tools with high volumes of domains, increasing false positives, and straining network resources. Given these challenges, effective DGA detection has become a critical component of modern cybersecurity strategies. Traditional approaches, such as static blacklists and simple heuristics, have proven inadequate against the dynamic nature of DGAs. This has led to increased interest in more sophisticated detection methods, including machine learning and deep learning [1, 2, 3, 4] as they have shown promise in enhancing DGA detection by leveraging more sophisticated feature extraction and classification methods. Similarly, and more recently, the application of large language models (LLMs) has also attracted interest [5]. In this context, LLMs, thanks to their extensive training data and ability to comprehend semantic patterns, offer new potential for effective DGA detection [6, 7]. LLMs‚Äô adaptability and semantic understanding are crucial for analyzing the complex patterns generated by DGAs. Furthermore, LLMs do not require extensive datasets for their application, making them efficient for deployment in dynamic environments. In this work, we analyze the performance of LLM-based methods for DGA detection, comparing them with state-of-the-art models. Our focus is on recent local models, such as Meta‚Äôs Llama 3 8B. We explore two main strategies: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT). The evaluation is conducted on a carefully designed dataset that includes examples from 68 distinct DGA families, covering various schemes, including recent word-based DGAs. These word-based schemes generate domains by concatenating sequences of words from one or more wordlists, resulting in domains that appear less random and are therefore more challenging to detect [8]. Our results indicate that SFT with domain-specific data significantly improves detection capabilities in particular reducing the false positive rate (FPR), whereas ICL enables rapid adaptation to new and evolving threats without extensive retraining. This research highlights the potential of LLMs to enhance cybersecurity defenses against DGA-based attacks, providing a comprehensive solution that balances speed and accuracy. The main contributions of the paper are: 1. A dataset with 68 malware families and normal domains from the Tranco dataset [9]. 2. A complete analysis of the potential of ICL and SFT for improving DGA detection using LLMs. 3. A state-of-the-art LLM-based DGA detector with lower FPR and better detection of DGA word-based domains. The remainder of this paper is organized as follows: Section 2 provides technical background on LLMs for DGA detection. Section 3 reviews previous works in DGA detection. Section 4 describes the proposed approach, including the dataset and methodology. Section 5 outlines the design and results of experiments, focusing on ICL and fine-tuning approaches. Section 6 discusses the practical implications of the findings and proposes a strategy for practical implementation. Finally, Section 7 concludes the paper and suggests directions for future research."
https://arxiv.org/html/2411.03305v1,Quantum One-Time Protection of any Randomized Algorithm,"The meteoric rise in power and popularity of machine learning models dependent on valuable training data has reignited a basic tension between the power of running a program locally and the risk of exposing details of that program to the user. At the same time, fundamental properties of quantum states offer new solutions to data and program security that can require strikingly few quantum resources to exploit, and offer advantages outside of mere computational run time. In this work, we demonstrate such a solution with quantum one-time tokens.A quantum one-time token is a quantum state that permits a certain program to be evaluated exactly once. One-time security guarantees, roughly, that the token cannot be used to evaluate the program more than once. We propose a scheme for building quantum one-time tokens for any randomized classical program, which include generative AI models. We prove that the scheme satisfies an interesting definition of one-time security as long as outputs of the classical algorithm have high enough min-entropy, in a black box model.Importantly, the classical program being protected does not need to be implemented coherently on a quantum computer. In fact, the size and complexity of the quantum one-time token is independent of the program being protected, and additional quantum resources serve only to increase the security of the protocol. Due to this flexibility in adjusting the security, we believe that our proposal is parsimonious enough to serve as a promising candidate for a near-term useful demonstration of quantum computing in either the NISQ or early fault tolerant regime.","Commercializing software presents a central dilemma: How does a proprietor distribute software, without forfeiting ownership? On the one hand, software must be made available to users for them to use it; on the other, once the software is distributed, an unlimited number of unauthorized copies can be made. This problem is more acute in the age of generative AI, where the software can be extremely valuable and potentially reveal private information. There are two widely-adopted solutions to this problem: 1. The proprietor can distribute an obfuscated version of the software. However, this opens up the possibility of piracy, and the user can always run the software an unlimited number of times. 2. The proprietor can allow queries to the software, instead of distributing the software itself. But this requires communication between the user and the proprietor every time the software is used. Sometimes a combination of these solutions is employed ‚Äî for instance, an internet browser might be obfuscated and made public while the search engine itself is kept on servers. There is an apparent trade-off between usability and exclusivity in these solutions. Distributing obfuscated software makes it highly usable, but it is impossible to impose any limits on the number of times it can be used. Keeping the software on servers and responding to user queries has high exclusivity, but lower usability because the user needs to communicate with the server to perform computations. Furthermore, this solution is only available to proprietors with enough resources to host a reliable server. This trade-off between usability and exclusivity is inherent in the classical setting, because classical information can always be copied. As soon as software is distributed, the user can query or copy it as many times as they want. Therefore it is natural to look to quantum mechanics for improved solutions. In contrast to classical information, quantum information cannot be cloned. Indeed, unclonable cryptography is able to make use of this principle to improve both of the above solutions.111We note that unclonable cryptography has led to many additional interesting and varied protocols, including quantum money [Wie83], quantum key distribution [BB14], and certified deletion [BI20]. Improving Solution (1) with copy protection. In copy protection, introduced by [Aar09], a quantum state is created that allows a program to be run an unlimited number of times, but not copied. In a classical oracle model, quantum copy protection is possible for any unlearnable program [ALL+21]. In the standard model, there exist unlearnable programs that cannot be copy protected [AP21]; nonetheless, it is known how to copy protect a small number of particular functionalities in the standard model [CLLZ21, LLQZ22, CG24, AB24]. Quantum copy protection addresses the piracy concern in Solution (1), but it does not apply in the setting where the proprietor wishes to distribute individual queries to the software. This is particularly relevant in the current age of generative AI, where the pay-per-query model is prevalent. Improving Solution (2) with one-time programs. For the pay-per-query model, one-time programs are a more suitable solution. A one-time program token is a quantum state that enables a program to be evaluated once. One-time programs therefore remove the need for online communication in Solution (2): The proprietor distributes tokens, and the users consume the tokens offline at their leisure. This work. In this work, we propose the first general-purpose tokenized program scheme using quantum information. Prior work was restricted to protecting specific cryptographic functionalities, and in particular did not apply to non-cryptographic programs like generative AI models. Our method uses a one-time signature scheme and a program obfuscator to compile any randomized algorithm into a one-time token that allows the algorithm to be executed on any input of the user‚Äôs choice. We note that the quantum capabilities required for our scheme are completely independent of the program being protected. Therefore, our scheme could plausibly be used to protect a large program using only a small, noisy quantum device ‚Äî even one that is incapable of demonstrating quantum computational advantage! Prior and concurrent work on one-time programs. One-time programs were initially studied in [GKR08], but without quantum computation. The idea of quantum one-time programs was originally suggested in [BGS13], where it was shown that it is impossible to build quantum one-time tokens for deterministic programs without specialized hardware assumptions. This is because a simple rewinding attack would allow a user to make arbitrarily many queries to the algorithm, given any state that allows it to be run once. Later, [BS23] presented a scheme to one-time protect signature functions in the standard model. In concurrent and independent work, [GLR+24] also present a scheme for the one-time protection of arbitrary randomized algorithms. Their scheme is essentially identical to ours, except that they instantiate the quantum one-time authentication scheme with the particular construction of [CLLZ21]. With respect to security definitions and proofs, their results are generally much more extensive, including stronger (albeit more complex) definitions of one-time security and new impossibility results. However, they take a very different and complementary approach to proving security. It would be interesting to know whether our results (and in particular Lemma 1, which is simple and self-contained) say anything about security in their setting, or whether their results imply our Lemma 1. Removing quantum communication. The protocol described below requires quantum communication between the proprietor and the user, because the proprietor needs to send the user the quantum one-time program token. However, if the one-time signatures are instantiated with those from [CLLZ21], then the results of [Shm22, CHV23] allow us to replace this quantum communication with a remote state preparation protocol that uses only classical communication. That is, there exists a polynomial-time interactive protocol between a quantum user and a classical proprietor that results in the user holding a quantum one-time token which they can only use once. Unfortunately, this protocol is highly complex and not likely to be feasible on a near-term quantum device.222It also requires the assumptions that LWE is sub-exponentially hard for quantum computers and that indistinguishability obfuscation for classical circuits exists with sub-exponential security against quantum polynomial-time adversaries. 1.1 Acknowledgements We thank Fermi Ma and Ryan Babbush for helpful discussions. We thank Jarrod McClean for detailed comments on an earlier draft of this work."
https://arxiv.org/html/2411.03237v1,On the Detection of Non-Cooperative RISs: ScanBùêµBitalic_B-Testing via Deep Support Vector Data Description,"In this paper, we study the problem of promptly detecting the presence of non-cooperative activity from one or more Reconfigurable Intelligent Surfaces (RISs) with unknown characteristics lying in the vicinity of a Multiple-Input Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division Multiplexing (OFDM) transmissions. We first present a novel wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces, which captures both the effect of the RIS actuation time on the channel in the frequency domain as well as the difference between changing phase configurations during or among transmissions. Considering that RISs may operate under the coordination of a third-party system, and thus, may negatively impact the communication of the intended MIMO OFDM system, we present a novel RIS activity detection framework that is unaware of the distribution of the phase configuration of any of the non-cooperative RISs. In particular, capitalizing on the knowledge of the data distribution at the multi-antenna receiver, we design a novel online change point detection statistic that combines a deep support vector data description model with the scan BùêµBitalic_B-test. The presented numerical investigations demonstrate the improved detection accuracy as well as decreased computational complexity of the proposed RIS detection approach over existing change point detection schemes.","Reconfigurable Intelligent Surfaces (RISs), comprising numerous metamaterials with dynamically tunable electromagnetic responses [1], constitute ultra-lightweight planar structures that can be used to coat building facades, room walls, or vehicles, and are recently considered as one of the candidate technologies for the next generation of wireless networks [2]. They can enable over-the-air signal propagation programmability in an energy-efficient manner, thus, transforming wireless channels to software-defined entities [3] that can be optimized for various objectives, e.g., enhanced multi-user connectivity [4], localization [5], and integrated sensing and communications [6]. The core features of RISs, with more pronounced their low hardware footprint recently including even almost transparent designs, have been also lately leveraged for eavesdropping and jamming [7, 8, 9, 10, 11, 12]. RIS-enabled threat models and proactive countermeasure designs of active and reflective beamforming as well as artificial noise were presented in [7] and [8] for different knowledge levels of the eavesdropper‚Äôs channel and unawareness of the presence of a malicious RIS. Jamming attacks based on RISs that are difficult to detect were presented in [11]. Scenarios where the adversary takes control of a legitimate RIS were discussed in [10], while [9] studied the case where an adversary places an RIS in the vicinity of a communication pair for information leakage [9]. The potential of RISs to decrease the received signal strength in a legitimate link, while enhancing it towards an eavesdropper, was analyzed in [12]. However, none of the latter works focused on detecting the presence of malicious RISs in environments where legitimate communications take place, which could then trigger efficient reactive legitimate physical-layer designs. On the other hand, as described in one of the RIS deployment use cases presented in [13], metasurfaces can be deployed from a single cellular operator in an area of intended coverage, where one or many other operators are also active. Those RISs should have a carefully designed bandwidth of influence to provide reconfigurable reflections to the owner operator, while leaving unaltered the signals spanning the bandwidth allocated to the other operator(s). The latter implies that those RISs should react similar to the surface material that hosts them, upon impinging signals from any of the unintended operators. However, such an RIS design for very closely allocated frequency bands is hard to achieve with up-to-date hardware technologies [14]. To this end, an approach that detects third-party non-cooperating RISs would help operators to sense relevant unwanted activity provoking countermeasure designs. Motivated by the latter two applications of unwanted RIS operations, we focus, in this paper, on the problem of detecting the operation of one or more non-cooperative RISs in the vicinity of point-to-point Multiple-Input Multiple-Output (MIMO) communication systems. Capitalizing on the discrete-time multipath channel model of [15] and the coupled-dipole formalism of RIS-parametrized channels of [16], we first introduce a wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces. Then, we present a novel formulation of the RIS activity detection problem as an online sequential change point detection problem [17]. Considering that the characteristics of any non-cooperative RISs in the wireless environment of interest are unknown to the intended MIMO OFDM communication system, we design a novel online, distribution-free change point detection statistic that combines a deep Support Vector Data Description (dSVDD) [18, 19] model with the scan BùêµBitalic_B-test [20]. Our extensive simulation results for the considered RIS detection problem showcase the superiority of our detection approach over existing change point detection methods, both in terms of detection accuracy as well as computational complexity. Notations: Lower case bold letters refer to vectors, e.g. ùê±ùê±\mathbf{x}bold_x, and upper case bold letters indicate matrices, e.g. ùêóùêó\mathbf{X}bold_X. Calligraphic letters, e.g., ùí≥ùí≥\mathcal{X}caligraphic_X, are reserved for sets and E‚Å¢[‚ãÖ]ùê∏delimited-[]‚ãÖE[\cdot]italic_E [ ‚ãÖ ] denotes the expectation operator and Pr‚Å¢[‚ãÖ]Prdelimited-[]‚ãÖ{\rm Pr}[\cdot]roman_Pr [ ‚ãÖ ] returns the probability. ùüéN√óMsubscript0ùëÅùëÄ\mathbf{0}_{N\times M}bold_0 start_POSTSUBSCRIPT italic_N √ó italic_M end_POSTSUBSCRIPT and ùêàNsubscriptùêàùëÅ\mathbf{I}_{N}bold_I start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT denote the N√óMùëÅùëÄN\times Mitalic_N √ó italic_M zero matrix and N√óNùëÅùëÅN\times Nitalic_N √ó italic_N identity matrix, respectively. ‚Ñù‚Ñù\mathbb{R}blackboard_R and ‚ÑÇ‚ÑÇ\mathbb{C}blackboard_C are the sets of the real and complex numbers. Finally, »∑‚âú‚àí1‚âúitalic-»∑1\jmath\triangleq\sqrt{-1}italic_»∑ ‚âú square-root start_ARG - 1 end_ARG is the imaginary unit and x‚àºùíû‚Å¢ùí©‚Å¢(0,œÉ2)similar-toùë•ùíûùí©0superscriptùúé2x\sim\mathcal{CN}(0,\sigma^{2})italic_x ‚àº caligraphic_C caligraphic_N ( 0 , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) represents a complex normal random variable with zero mean and variance œÉ2superscriptùúé2\sigma^{2}italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT."
https://arxiv.org/html/2411.02974v1,Region-Guided Attack on the Segment Anything Model (SAM),"The Segment Anything Model (SAM) is a cornerstone of image segmentation, demonstrating exceptional performance across various applications, particularly in autonomous driving and medical imaging, where precise segmentation is crucial. However, SAM is vulnerable to adversarial attacks that can significantly impair its functionality through minor input perturbations. Traditional techniques, such as FGSM and PGD, are often ineffective in segmentation tasks due to their reliance on global perturbations that overlook spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address these challenges, but they frequently depend on external cues and do not fully leverage the structural interdependencies within segmentation processes. This limitation underscores the need for a novel adversarial strategy that exploits the unique characteristics of segmentation tasks. In response, we introduce the Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted perturbations that fragment large segments and expand smaller ones, resulting in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves high success rates in both white-box and black-box scenarios, emphasizing the need for robust defenses against such sophisticated attacks. RGA not only reveals SAM‚Äôs vulnerabilities but also lays the groundwork for developing more resilient defenses against adversarial threats in image segmentation.","The Segment Anything Model (SAM) [1] has emerged as a leading solution in image segmentation, demonstrating remarkable adaptability and performance across diverse datasets and prompts. Its architecture allows for seamless integration with various inputs, making it a pivotal tool for applications ranging from autonomous driving [2, 3] to medical imaging [4, 5]. However, this versatility also exposes SAM to vulnerabilities, particularly from adversarial attacks that can significantly degrade its performance [6, 7]. These attacks leverage subtle perturbations in the input data, misleading the model into producing incorrect segmentations, thereby raising concerns about the reliability of SAM in critical contexts. Previous adversarial attack methods, such as the Fast Gradient Sign Method (FGSM) [8] and Projected Gradient Descent (PGD) [9], primarily focus on classification models and often utilize global perturbations that affect the entire input. These methods can be less effective in segmentation tasks, where spatial relationships and contextual information are critical. Recent approaches like Attack-SAM-K [6] and UAD [7] have begun to explore the unique challenges associated with adversarial attacks on segmentation models, but many still rely on external prompts or do not fully exploit the structural dependencies inherent in the segmentation process. To effectively address these vulnerabilities, we propose the Region-Guided Attack (RGA), a novel adversarial attack strategy specifically designed for SAM. Unlike traditional adversarial methods that often rely on external prompts or global perturbations, RGA focuses on manipulating segmented regions directly through a Region-Guided Map (RGM). This approach allows for targeted adversarial perturbations that divide large segments into smaller fragments while merging smaller regions into larger areas, ultimately leading to misclassifications in SAM‚Äôs outputs. The innovation of RGA lies in its ability to exploit the structural dependencies within the segmentation task, leveraging the inherent characteristics of SAM to enhance the effectiveness of the attack. The significance of RGA is twofold. First, it provides a deeper understanding of the vulnerabilities inherent in advanced segmentation models like SAM, offering insights into how adversarial perturbations can be crafted more strategically. Second, RGA presents a more refined method of inducing segmentation errors that can be applied across various segmentation frameworks, highlighting the need for robust defenses against such targeted attacks. Through extensive experiments, we demonstrate the effectiveness of RGA, revealing its capability to achieve high attack success rates in both white-box and black-box scenarios while maintaining minimal perceptual distortion in the input images. In summary, the key contributions of RGA include: 1. Region-Guided Map (RGM) for Adversarial Guidance: RGA introduces a novel use of the RGM to directly guide the generation of adversarial examples. By utilizing RGM to define how SAM‚Äôs segmentation should be altered (i.e., splitting large regions into smaller ones and merging smaller regions into larger ones), RGA effectively guides perturbations to maximize the impact on segmentation quality. 2. Enhanced Attack Success and Transferability: By leveraging RGM, RGA achieves higher attack success rates and improved transferability. The adversarial examples are generated with a clear objective influenced by the segmentation output, which systematically guides perturbations, leading to more successful and transferable attacks against the SAM model. 3. Independent of External Prompts: Unlike many existing methods that rely heavily on specific prompts to guide attacks, RGA operates independently of external prompts, making the adversarial process more streamlined and broadly applicable. This independence ensures that RGA can be applied in scenarios where prompts are unavailable or unpredictable. 4. Insights into SAM‚Äôs Segmentation Vulnerabilities: RGA reveals particular vulnerabilities in SAM by focusing on regional manipulations instead of global input perturbations. The findings highlight how region-specific guidance, such as altering the size and boundaries of segmented areas, can degrade SAM‚Äôs segmentation performance significantly. This understanding provides valuable insights for designing more resilient segmentation models."
https://arxiv.org/html/2411.02902v1,Membership Inference Attacks againstLarge Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxR√©nyi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.","The rise of large language models (LLMs) [9, 60, 45, 11] has inspired the exploration of large models across multi-modal domains, exemplified by advancements like GPT-4 [1] and Gemini [59]. These large vision-language models (VLLMs) have shown promising ability in various multi-modal tasks, such as image captioning [33], image question answering [13, 35], and image knowledge extraction [26]. However, the rapid advancement of VLLMs also causes user concerns about privacy and knowledge leakage. For instance, the image data used during commercial model training may contain private photographs or medical diagnostic records. This is concerning since early work has demonstrated that machine learning models can memorize and leak training data [3, 56, 63]. To mitigate such concerns, it is essential to consider the membership inference attack (MIA) [23, 53], where attackers seek to detect whether a particular data record is part of the training dataset [23, 53]. The study of MIAs plays an important role in preventing test data contamination and protecting data security, which is of great interest to both industry and academia [24, 19, 44]. When exploring MIAs in VLLMs, one main issue is the absence of a standardized dataset designed to develop and evaluate different MIA methods, which comes from the large size [16] and multi-modality of the training data, and the diverse VLLMs training pipelines [66, 35, 18]. Therefore, one of the main goals of this work is to build an MIA benchmark tailored for VLLMs. Beyond the need for a valid benchmark, we lack efficient techniques to detect a single modality in VLLMs. The closest work to ours is [30], which performs MIAs on multi-modal CLIP [46] by detecting whether an image-text pair is in the training set. However, in practice, it is more common to detect a single modality, as we care whether an individual image or text is in the training set. Therefore, we aim to develop a pipeline to detect the single modality from a multi-modal model. Moreover, existing literature on language model MIAs, such as Min-K% [52] and Perplexity [62], mostly are target-based MIAs, which use the next token as the target to compute the prediction probability. However, we can only access the image embedding instead of the image token in VLLMs, and thus only target-free MIAs [48] can be directly applied. Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxR√©nyi-K% metric: we first get the R√©nyi entropy of each token position, then select the largest k%percentùëòk\%italic_k % tokens and calculate the average R√©nyi entropy. Therefore, we first propose a cross-modal pipeline for individual image or description MIAs on VLLMs, which is distinguished from traditional MIAs that only use one modality [61, 62]. We feed the VLLMs with a customized image-instruction pair from the target image or description. We show that we can perform the MIA not only by the image slice but also by the instruction and description slices of the VLLM‚Äôs output logits, see Figure 1. Such a cross-modal pipeline enables the usage of text MIA methods on image MIAs. We also introduce a target-free metric that adapts to both image and text MIAs and can be further modified to a target-based way. Overall, the contributions and insights can be summarized as follows. ‚Ä¢ We release the first benchmark tailored for the detection of training data in VLLMs, called Vision Language MIA (VL-MIA) (Section 4). By leveraging Flickr and GPT-4, we construct VL-MIA that contains two images MIA tasks and one text MIA task for various VLLMs, including MiniGPT-4 [66], LLaVA 1.5 [35] and LLaMA-Adapter V2 [18]. ‚Ä¢ We perform the first individual image or description MIAs on VLLMs in a cross-modal manner. Specifically, we demonstrate that we can perform image MIAs by computing statistics from the image or text slices of the VLLM‚Äôs output logits (Figures 1 and 5.1). ‚Ä¢ We propose a target-free MIA metric, MaxR√©nyi-K%, and its modified target-based ModR√©nyi (Section 5.2). We demonstrate their effectiveness on open-source VLLMs and closed-source GPT-4 (Section 6). We achieve an AUC of 0.815 on GPT-4 in image MIAs."
https://arxiv.org/html/2411.02554v1,Quantum-Computable One-Way Functionswithout One-Way Functions,"We construct a classical oracle relative to which ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP but quantum-computable quantum-secure trapdoor one-way functions exist. This is a substantial strengthening of the result of Kretschmer, Qian, Sinha, and Tal (STOC 2023), which only achieved single-copy pseudorandom quantum states relative to an oracle that collapses ùñ≠ùñØùñ≠ùñØ\mathsf{NP}sansserif_NP to ùñØùñØ\mathsf{P}sansserif_P. For example, our result implies multi-copy pseudorandom states and pseudorandom unitaries, but also classical-communication public-key encryption, signatures, and oblivious transfer schemes relative to an oracle on which ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP. Hence, in our new relativized world, classical computers live in ‚ÄúAlgorithmica‚Äù whereas quantum computers live in ‚ÄúCryptomania,‚Äù using the language of Impagliazzo‚Äôs worlds.Our proof relies on a new distributional block-insensitivity lemma for ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuits, wherein a single block is resampled from an arbitrary distribution.","What cryptography would survive in the event of someone or some organization discovering a practical algorithm to solve ùñ≠ùñØùñ≠ùñØ\mathsf{NP}sansserif_NP-complete problems? Unfortunately, almost all computationally-secure classical cryptography relies on the existence of one-way functions [IL89, Gol90]. Thus, their security certainly requires at least ùñØ‚â†ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}\neq\mathsf{NP}sansserif_P ‚â† sansserif_NP. Recent works have hinted that quantum analogues of many important cryptographic tasks may not be subject to this barrier. A series of black-box separations established that pseudorandom quantum states‚Äîa quantum counterpart to classical pseudorandom generators‚Äîcan exist relative to oracles that make ùñ≠ùñØùñ≠ùñØ\mathsf{NP}sansserif_NP [KQST23], ùñ∞ùñ¨ùñ†ùñ∞ùñ¨ùñ†\mathsf{QMA}sansserif_QMA [Kre21], and more powerful complexity classes [LMW24] computationally easy. Combined with parallel efforts to build cryptosystems from pseudorandom states [AQY22, MY22, AGQY22, ALY24, BBO+24, CGG24], we now know that useful computationally-secure quantum cryptography could conceivably exist in a world where ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP. Upon closer inspection, the cryptographic protocols realized in these oracle separations all require quantum communication, or even long-term quantum memory, in addition to fault-tolerant quantum computation. So, these oracle separations are hardly a satisfactory replacement for the classical cryptography that we currently use. For example, it is possible that we find efficient algorithms for problems like breaking SHA-3 and learning with errors before we have quantum internet or reliable quantum storage. Even outside of this nightmare scenario, quantum communication is not always desirable. Besides the obvious challenges of engineering a robust quantum channel, there are other theoretical and practical limitations to protocols that use quantum communication. Certain scenarios require broadcasting, which is impossible quantumly due to the no-broadcasting theorem [BCF+96]. For a concrete example, public-key encryptions and digital signatures with public key infrastructure (PKI) have been essential for securing digital communications. While quantum versions of public-key encryptions from one-way functions [BGHD+23, Col23, KMNY24, MW24] and digital signatures without one-way functions [GC01, MY22] have been explored, these schemes all make use of an uncloneable quantum public key, besides only satisfying weak security requirements such as one-time security. This unclonability is undesirable from the PKI perspective because it prevents the PKI from distributing the public key: once the copies that the PKI holds are exhausted, new users would be unable to obtain public keys and take part in the protocols. Altogether, these limitations of quantum communication in cryptography give rise to the following natural question: What quantum cryptography with classical communication is still possible if ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP? 1.1 Main Result Consider a quantum-computable one-way function (OWF), which is like an ordinary one-way function except that instead of mandating an efficient classical evaluation algorithm, we permit (pseudo-deterministic) quantum algorithms as well. Security must hold against both classical and quantum adversaries. It is clear that this is a weakening of traditional (quantum-secure) one-way functions. Since the only difference is in permitting evaluation by a quantum computer, one might be optimistic that this object is not so different from one-way functions, perhaps by employing some clever dequantizations. After all, a ùñ°ùñØùñØùñ°ùñØùñØ\mathsf{BPP}sansserif_BPP-computable pseudo-deterministic one-way function can be derandomized through standard black-box techniques.111In particular, a ùñ°ùñØùñØùñ°ùñØùñØ\mathsf{BPP}sansserif_BPP-computable OWF f‚Å¢(x;r)ùëìùë•ùëüf(x;r)italic_f ( italic_x ; italic_r ) gives a distributional OWF f‚Ä≤‚Å¢(x,r):=f‚Å¢(x;r)assignsuperscriptùëì‚Ä≤ùë•ùëüùëìùë•ùëüf^{\prime}(x,r):=f(x;r)italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x , italic_r ) := italic_f ( italic_x ; italic_r ), which can then be used to construct a standard OWF [IL89]. In this work, we show, surprisingly, that quantum-computable OWFs can exist in an oracle world where ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP, and therefore dequantizing a quantum-computable OWF is impossible in a black-box fashion. In fact, our main theorem is a significant strengthening of this, where we construct quantum-computable trapdoor one-way functions that are consistent with ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP. Theorem 1 (Theorem 32, informal). There exists a classical oracle relative to which ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP and a quantum-computable trapdoor one-way function exists. Furthermore, the trapdoor one-way function has pseudorandom public keys. By invoking known post-quantum fully black-box reductions, we obtain that relative to the same classical oracle as Theorem 1, the following classical-communication cryptographic schemes also exist: ‚Ä¢ Public-key encryptions with semantic security. (Corollary 39) ‚Ä¢ Public-key signatures with existential unforgeable security. ([Son14, Section 5.1]) ‚Ä¢ Oblivious transfer protocols with simulation security. (Corollary 39) Therefore, it appears that in this oracular world, classical computers live in ‚ÄúAlgorithmica‚Äù while quantum computers live in ‚ÄúCryptomania‚Äù [Imp95], even without the need of any long-term quantum memory or quantum communication! As a corollary, all of these cryptographic schemes are separated from ùñØ‚â†ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}\neq\mathsf{NP}sansserif_P ‚â† sansserif_NP (thus OWFs) as well. Note that prior to our work, even mildly quantum variants of these were not known to be separated from OWFs (e.g., public-key encryption with quantum ciphertext, or quantum signatures with standard security). Implications for Quantum Pseudorandomness. Recall that the previous oracle separations of comparable nature are instantiations of pseudorandom quantum states relative to oracles that make classical cryptography easy [Kre21, KQST23, LMW24]. Our result is strictly stronger, then, because quantum-computable one-way functions are also sufficient to construct pseudorandom states since these reductions to one-way functions are fully-black-box [JLS18, BS19, AGQY22]. Specifically, one can use the now standard binary phase construction: |œàk‚ü©‚âî12n‚Å¢‚àëx‚àà{0,1}n(‚àí1)fk‚Å¢(x)‚Å¢|x‚ü©,‚âîketsubscriptùúìùëò1superscript2ùëõsubscriptùë•superscript01ùëõsuperscript1subscriptùëìùëòùë•ketùë•\ket{\psi_{k}}\coloneqq\frac{1}{\sqrt{2^{n}}}\sum_{x\in\{0,1\}^{n}}(-1)^{f_{k}% (x)}\ket{x},| start_ARG italic_œà start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ‚ü© ‚âî divide start_ARG 1 end_ARG start_ARG square-root start_ARG 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_ARG end_ARG ‚àë start_POSTSUBSCRIPT italic_x ‚àà { 0 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( - 1 ) start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_x ) end_POSTSUPERSCRIPT | start_ARG italic_x end_ARG ‚ü© , where {fk}subscriptùëìùëò\{f_{k}\}{ italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } is a keyed family of pseudorandom functions constructed from the one-way functions [Zha21]. This directly answers an open problem from [KQST23]: Corollary 2. There exists a classical oracle relative to which ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP and many-copy-secure pseudorandom states exist. For comparison, [KQST23] only achieved single-copy-secure pseudorandom states. Following more recent advances, one can build even more versatile quantum pseudorandomness primitives from one-way functions, including pseudorandom states of arbitrary polynomial length [BS20] and even pseudorandom unitaries [MPSY24, CBB+24, MH24]. As a corollary, these also exist relative to our oracle as well. 1.2 Technical Overview Warmup: Separating Quantum-Computable PRFs. We first show how to construct an oracle relative to which ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP and quantum-computable pseudorandom functions (PRFs) exist. The analysis in this special case is conceptually simpler, though still rich enough to capture most of the important ideas needed to generalize to trapdoor one-way functions. The main idea behind this oracle separation is to construct a random oracle in a special encoding that is only accessible by ùñ°ùñ∞ùñØùñ°ùñ∞ùñØ\mathsf{BQP}sansserif_BQP but not ùñØùñßùñØùñß\mathsf{PH}sansserif_PH. This encoding technique was previously seen in the work of Aaronson, Ingram, and Kretschmer [AIK22], although similar ideas have also appeared in even earlier works [BM99, ABDK16]. Then intuitively, the quantum-computable pseudorandom functions can simply be constructed by direct evaluation of this quantum-computable random oracle. We now explain the oracle construction in more detail. Similar to the oracle used in [KQST23], our oracle ùí™ùí™\mathcal{O}caligraphic_O can be thought of as a pair of oracles (A,B)ùê¥ùêµ(A,B)( italic_A , italic_B ). The oracle Aùê¥Aitalic_A encodes the ùñ°ùñ∞ùñØùñ°ùñ∞ùñØ\mathsf{BQP}sansserif_BQP-accessible random oracle, and the addition of auxiliary oracle BùêµBitalic_B has the effect of making ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP. Morally speaking, BùêµBitalic_B behaves as if it were an oracle for ùñØùñßAsuperscriptùñØùñßùê¥\mathsf{PH}^{A}sansserif_PH start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT. Thus, showing the security of the quantum-computable PRFs relative to ùí™ùí™\mathcal{O}caligraphic_O amounts to proving security against polynomial-time quantum adversaries that can query any ùñØùñßAsuperscriptùñØùñßùê¥\mathsf{PH}^{A}sansserif_PH start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT language. For brevity, we‚Äôll call these ùñ°ùñ∞ùñØùñØùñßsuperscriptùñ°ùñ∞ùñØùñØùñß\mathsf{BQP^{PH}}sansserif_BQP start_POSTSUPERSCRIPT sansserif_PH end_POSTSUPERSCRIPT (oracular) adversaries in this exposition. The key difference between our oracle construction and that of [KQST23] is in how we build Aùê¥Aitalic_A. In [KQST23], Aùê¥Aitalic_A is simply a random oracle, whereas in this warmup, Aùê¥Aitalic_A is an encoding of a uniformly random oracle. As with many of the oracles constructed in [AIK22], we encode each bit of the oracle using the Forrelation problem [Aar10, RT19]. Recall that Forrelation (in its broadest sense) is the following task: given query access to a pair of functions f,g:{0,1}‚Ñì‚Üí{0,1}:ùëìùëî‚Üísuperscript01‚Ñì01f,g:\{0,1\}^{\ell}\to\{0,1\}italic_f , italic_g : { 0 , 1 } start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ‚Üí { 0 , 1 }, distinguish between (NO) fùëìfitalic_f and gùëîgitalic_g are independent uniformly random functions, or (YES) fùëìfitalic_f and gùëîgitalic_g are individually random, but sampled in such a way that fùëìfitalic_f is noticeably correlated with the Boolean Fourier transform of gùëîgitalic_g (i.e., fùëìfitalic_f and gùëîgitalic_g are ‚ÄúForrelated‚Äù). In contrast to [KQST23], whose analysis required a carefully-crafted version of Forrelation, we only require black-box use of one key fact from [RT19]: that these two distributions are efficiently distinguishable by ùñ°ùñ∞ùñØùñ°ùñ∞ùñØ\mathsf{BQP}sansserif_BQP algorithms, but not by ùñØùñßùñØùñß\mathsf{PH}sansserif_PH algorithms. Letting LùêøLitalic_L be the random oracle, our strategy for encoding LùêøLitalic_L is to hide each bit of its output behind an instance of Forrelation. That is, for each x‚àà{0,1}‚àóùë•superscript01x\in\{0,1\}^{*}italic_x ‚àà { 0 , 1 } start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT, we add a region of Aùê¥Aitalic_A that encodes a pair of uniformly random Boolean functions if L‚Å¢(x)=0ùêøùë•0L(x)=0italic_L ( italic_x ) = 0, or otherwise a pair of Forrelated functions if L‚Å¢(x)=1ùêøùë•1L(x)=1italic_L ( italic_x ) = 1. It is straightforward to show that oracle access to Aùê¥Aitalic_A enables a quantum algorithm to recover any bit of LùêøLitalic_L. The candidate quantum-computable PRF family {fk}subscriptùëìùëò\{f_{k}\}{ italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } is the natural choice defined by fk(x)‚âîL(k||x)f_{k}(x)\coloneqq L(k||x)italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_x ) ‚âî italic_L ( italic_k | | italic_x ). The main technical difficulty is to formalize the intuition that this encoding is secure enough against ùñØùñßùñØùñß\mathsf{PH}sansserif_PH that distinguishing the PRFs from random remains hard even against ùñ°ùñ∞ùñØùñØùñßsuperscriptùñ°ùñ∞ùñØùñØùñß\mathsf{BQP^{PH}}sansserif_BQP start_POSTSUPERSCRIPT sansserif_PH end_POSTSUPERSCRIPT adversaries. We expect this to be the case because to ùñØùñßùñØùñß\mathsf{PH}sansserif_PH, the oracle Aùê¥Aitalic_A looks completely random and independent of LùêøLitalic_L, and therefore a ùñ°ùñ∞ùñØùñØùñßsuperscriptùñ°ùñ∞ùñØùñØùñß\mathsf{BQP^{PH}}sansserif_BQP start_POSTSUPERSCRIPT sansserif_PH end_POSTSUPERSCRIPT adversary should not have much more power than a ùñ°ùñ∞ùñØùñ°ùñ∞ùñØ\mathsf{BQP}sansserif_BQP adversary. To make this argument rigorous, we have to prove that a ùñ°ùñ∞ùñØùñØùñßsuperscriptùñ°ùñ∞ùñØùñØùñß\mathsf{BQP^{PH}}sansserif_BQP start_POSTSUPERSCRIPT sansserif_PH end_POSTSUPERSCRIPT adversary, given oracle access to an auxiliary function h‚Ñéhitalic_h, cannot distinguish whether h‚Ñéhitalic_h is uniformly random or whether h‚Ñéhitalic_h is one of the pseudorandom functions fksubscriptùëìùëòf_{k}italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. We establish this by observing that it suffices to show the following: if the adversary is given a uniformly random h‚Ñéhitalic_h, then the adversary is unlikely to detect a change to the oracle Aùê¥Aitalic_A so as to make it consistent with h=fk‚Ñésubscriptùëìùëòh=f_{k}italic_h = italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT for a random key kùëòkitalic_k. For the PRF family {fk}k‚àà{0,1}nsubscriptsubscriptùëìùëòùëòsuperscript01ùëõ\{f_{k}\}_{k\in\{0,1\}^{n}}{ italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k ‚àà { 0 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT of functions fk:{0,1}n‚Üí{0,1}:subscriptùëìùëò‚Üísuperscript01ùëõ01f_{k}:\{0,1\}^{n}\to\{0,1\}italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT : { 0 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚Üí { 0 , 1 }, we can view the part of Aùê¥Aitalic_A encoding these functions as a 2n√ó2nsuperscript2ùëõsuperscript2ùëõ2^{n}\times 2^{n}2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT √ó 2 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT matrix of Forrelation instances. In this matrix, the rows are indexed by keys k‚àà{0,1}nùëòsuperscript01ùëõk\in\{0,1\}^{n}italic_k ‚àà { 0 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, the columns by inputs x‚àà{0,1}nùë•superscript01ùëõx\in\{0,1\}^{n}italic_x ‚àà { 0 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, and the corresponding Forrelation instance is Forrelated if fk‚Å¢(x)=1subscriptùëìùëòùë•1f_{k}(x)=1italic_f start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_x ) = 1, or uniform otherwise. Given h:{0,1}n‚Üí{0,1}:‚Ñé‚Üísuperscript01ùëõ01h:\{0,1\}^{n}\to\{0,1\}italic_h : { 0 , 1 } start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚Üí { 0 , 1 }, the goal of the adversary is to determine whether there is a row of the matrix whose pattern of Forrelated/uniform instances is consistent with h‚Ñéhitalic_h. See Figure 1 for an example. h=‚Ñéabsenth=italic_h = 0 1 0 1 ‚áì‚áì\Downarrow‚áì Uniform Forrelated Uniform Uniform Forrelated Uniform Forrelated Forrelated Forrelated Uniform Forrelated Uniform Uniform Uniform Forrelated Forrelated vs. Uniform Forrelated Uniform Uniform Uniform Forrelated Uniform Forrelated Forrelated Uniform Forrelated Uniform Uniform Uniform Forrelated Forrelated Figure 1: An example of the distinguishing task for our quantum-computable PRFs with n=2ùëõ2n=2italic_n = 2. The goal is to decide (left) whether the Forrelation instances in the matrix are all randomly assigned to either uniform or Forrelated, or (right) whether the pattern of 0s/1s in h‚Ñéhitalic_h matches the pattern of uniform/Forrelated in one of the rows. Our security proof proceeds in a similar fashion to the ùñ°ùñ∞ùñØùñØùñßsuperscriptùñ°ùñ∞ùñØùñØùñß\mathsf{BQP^{PH}}sansserif_BQP start_POSTSUPERSCRIPT sansserif_PH end_POSTSUPERSCRIPT lower bound for the so-called OR‚àòForrelationORForrelation\textsc{OR}\circ\textsc{Forrelation}OR ‚àò Forrelation problem that was considered in prior works [AIK22, KQST23]. In OR‚àòForrelationORForrelation\textsc{OR}\circ\textsc{Forrelation}OR ‚àò Forrelation, we are given a list of instances of the Forrelation problem, and must decide whether they are all uniform (NO), or whether a single one of the instances is Forrelated (YES). Using the well-known correspondence between ùñØùñßùñØùñß\mathsf{PH}sansserif_PH query algorithms and ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuits [FSS84], the ùñ°ùñ∞ùñØùñØùñßsuperscriptùñ°ùñ∞ùñØùñØùñß\mathsf{BQP^{PH}}sansserif_BQP start_POSTSUPERSCRIPT sansserif_PH end_POSTSUPERSCRIPT lower bound for OR‚àòForrelationORForrelation\textsc{OR}\circ\textsc{Forrelation}OR ‚àò Forrelation reduces to a certain type of sensitivity concentration result for ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuits. The key step [AIK22, Lemma 45] shows that for most uniformly random M√óNùëÄùëÅM\times Nitalic_M √ó italic_N Boolean matrices, an ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuit is unlikely to notice the change if we uniformly swap out one of the rows for fresh random bits. We show that security of our PRF ensemble also reduces to a comparable statement about the sensitivity of ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuits, but under a distribution of Boolean matrices that is not uniform. Instead, we have to consider matrices like those sampled in Figure 1, where the blocks are a random pattern of uniform or Forrelated. For this purpose, we are able to show the following main technical lemma, which informally states the following. Consider a quasi-polynomial-size ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuit that takes KùêæKitalic_K blocks of bits, where each block is sampled independently from an arbitrary distribution ùíüùíü\mathcal{D}caligraphic_D. Then for sufficiently large KùêæKitalic_K, it is hard for it to notice if any block is resampled from the same distribution ùíüùíü\mathcal{D}caligraphic_D. More formally, Lemma 3 (Lemma 15, restated). Let f:{0,1}K‚Å¢M‚Üí{0,1}:ùëì‚Üísuperscript01ùêæùëÄ01f:\{0,1\}^{KM}\to\{0,1\}italic_f : { 0 , 1 } start_POSTSUPERSCRIPT italic_K italic_M end_POSTSUPERSCRIPT ‚Üí { 0 , 1 } be an ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuit of size sùë†sitalic_s and depth dùëëditalic_d. Let ùíüùíü\mathcal{D}caligraphic_D be a distribution over {0,1}Msuperscript01ùëÄ\{0,1\}^{M}{ 0 , 1 } start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT. Let x‚àºùíüKsimilar-toùë•superscriptùíüùêæx\sim\mathcal{D}^{K}italic_x ‚àº caligraphic_D start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT be an input to fùëìfitalic_f, viewed as a K√óMùêæùëÄK\times Mitalic_K √ó italic_M matrix. Let yùë¶yitalic_y be sampled depending on xùë•xitalic_x as follows: uniformly select one of the rows of xùë•xitalic_x, resample that row from ùíüùíü\mathcal{D}caligraphic_D, and leave the other rows of xùë•xitalic_x unchanged. Then for any p>0ùëù0p>0italic_p > 0: Prx‚àºùíüK‚Å°[Pry‚Å°[f‚Å¢(x)‚â†f‚Å¢(y)]‚â•p]‚â§4‚Å¢Kp‚ãÖ2‚àíp‚Å¢KO‚Å¢(log‚Å°s)d‚àí1.subscriptPrsimilar-toùë•superscriptùíüùêæsubscriptPrùë¶ùëìùë•ùëìùë¶ùëù‚ãÖ4ùêæùëùsuperscript2ùëùùêæùëÇsuperscriptùë†ùëë1\Pr_{x\sim\mathcal{D}^{K}}\left[\Pr_{y}\left[f(x)\neq f(y)\right]\geq p\right]% \leq\frac{4K}{p}\cdot 2^{-\frac{pK}{O(\log s)^{d-1}}}.roman_Pr start_POSTSUBSCRIPT italic_x ‚àº caligraphic_D start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ roman_Pr start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT [ italic_f ( italic_x ) ‚â† italic_f ( italic_y ) ] ‚â• italic_p ] ‚â§ divide start_ARG 4 italic_K end_ARG start_ARG italic_p end_ARG ‚ãÖ 2 start_POSTSUPERSCRIPT - divide start_ARG italic_p italic_K end_ARG start_ARG italic_O ( roman_log italic_s ) start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_ARG end_POSTSUPERSCRIPT . Notably, this holds for any distribution ùíüùíü\mathcal{D}caligraphic_D, and not merely the distribution of randomly chosen uniform/Forrelated blocks. Previously, Lemma 3 was only known to hold in the special case where ùíüùíü\mathcal{D}caligraphic_D is the uniform distribution [AIK22, Lemma 45]. The proof of this lemma involves a careful construction of a related ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuit from fùëìfitalic_f, whose sensitivity corresponds to the probability of fùëìfitalic_f noticing the block being resampled. This then allows us to relate this to known sensitivity bounds on ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT circuits [AIK22]. We refer interested readers to Lemma 15 for the details. Generalization to Trapdoor Functions. Since Lemma 3 morally lets us resample an arbitrary block that can be arbitrarily distributed, we can in fact prove that the distinguishing task in Figure 1 is hard for any distribution of functions instead of just uniform. A natural idea then is to use Forrelation to encode a more structured oracle that allows us to construct more structured cryptographic primitives. However, we also cannot introduce more structure than what Lemma 3 allows us to handle. For example, to get quantum-computable oblivious transfer protocols, a natural idea would be to encode random trapdoored permutations or something similar instead. However, unlike a random function, a random permutation is already problematic since each entry in a random permutation is weakly correlated with the other entries. On the other hand, Lemma 3 only works with product distributions. Inspired by this example, we instead start with random functions and only introduce just enough structure to have a trapdoor. Specifically, to obtain an oracle relative to which some form of quantum-computable trapdoor one-way functions exist but ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP, we utilize a similar Forrelation encoding, but the oracle Aùê¥Aitalic_A is no longer random. Instead, Aùê¥Aitalic_A encodes a triple of functions G,F,Iùê∫ùêπùêºG,F,Iitalic_G , italic_F , italic_I where ‚Ä¢ G‚Å¢(t‚Å¢d)ùê∫ùë°ùëëG(td)italic_G ( italic_t italic_d ) is a random function mapping a trapdoor t‚Å¢dùë°ùëëtditalic_t italic_d to its public key p‚Å¢kùëùùëòpkitalic_p italic_k; ‚Ä¢ F‚Å¢(p‚Å¢k,x)ùêπùëùùëòùë•F(pk,x)italic_F ( italic_p italic_k , italic_x ) is also a random function mapping a public key p‚Å¢kùëùùëòpkitalic_p italic_k and input xùë•xitalic_x to an output yùë¶yitalic_y; ‚Ä¢ Finally, I‚Å¢(t‚Å¢d,y)ùêºùë°ùëëùë¶I(td,y)italic_I ( italic_t italic_d , italic_y ) is the only ‚Äústructured‚Äù function that inverts an image yùë¶yitalic_y of F‚Å¢(G‚Å¢(t‚Å¢d),‚ãÖ)ùêπùê∫ùë°ùëë‚ãÖF(G(td),\cdot)italic_F ( italic_G ( italic_t italic_d ) , ‚ãÖ ) using the trapdoor t‚Å¢dùë°ùëëtditalic_t italic_d. Then the quantum-computable trapdoor one-way function construction is simply evaluating these three functions G,F,Iùê∫ùêπùêºG,F,Iitalic_G , italic_F , italic_I by a ùñ°ùñ∞ùñØùñ°ùñ∞ùñØ\mathsf{BQP}sansserif_BQP algorithm that queries the Forrelation encodings in Aùê¥Aitalic_A. For security, we want to show that a ùñ°ùñ∞ùñØùñØùñßsuperscriptùñ°ùñ∞ùñØùñØùñß\mathsf{BQP^{PH}}sansserif_BQP start_POSTSUPERSCRIPT sansserif_PH end_POSTSUPERSCRIPT adversary, given p‚Å¢k=G‚Å¢(t‚Å¢d)ùëùùëòùê∫ùë°ùëëpk=G(td)italic_p italic_k = italic_G ( italic_t italic_d ) and y=F‚Å¢(p‚Å¢k,x)ùë¶ùêπùëùùëòùë•y=F(pk,x)italic_y = italic_F ( italic_p italic_k , italic_x ) for random trapdoor t‚Å¢dùë°ùëëtditalic_t italic_d and input xùë•xitalic_x, cannot find a preimage x‚Ä≤superscriptùë•‚Ä≤x^{\prime}italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT such that F‚Å¢(p‚Å¢k,x‚Ä≤)=yùêπùëùùëòsuperscriptùë•‚Ä≤ùë¶F(pk,x^{\prime})=yitalic_F ( italic_p italic_k , italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ) = italic_y. Proving this is more challenging than before since the encoded oracle here is more sophisticated than simply a random function. However, notice that the previous security proof for the PRF reduces to a resampling indistinguishability task (Figure 1). We can similarly reduce security to resampling indistinguishability with some additional steps: 1. Starting with the real experiment (G,F,I)ùê∫ùêπùêº(G,F,I)( italic_G , italic_F , italic_I ), we first argue that Gùê∫Gitalic_G and IùêºIitalic_I do not help the adversary in inverting yùë¶yitalic_y as follows. (a) First, resample G‚Å¢(t‚Å¢d)=p‚Å¢k‚àóùê∫ùë°ùëëùëùsuperscriptùëòG(td)=pk^{*}italic_G ( italic_t italic_d ) = italic_p italic_k start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT and set I‚Å¢(t‚Å¢d,‚ãÖ)ùêºùë°ùëë‚ãÖI(td,\cdot)italic_I ( italic_t italic_d , ‚ãÖ ) to be the inversion table for F‚Å¢(p‚Å¢k‚àó,‚ãÖ)ùêπùëùsuperscriptùëò‚ãÖF(pk^{*},\cdot)italic_F ( italic_p italic_k start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , ‚ãÖ ) instead: this is indistinguishable since we are resampling one of exponentially many blocks of (G,I)ùê∫ùêº(G,I)( italic_G , italic_I ). (b) Next, we want to claim that IùêºIitalic_I no longer contains the inversion table for F‚Å¢(p‚Å¢k,‚ãÖ)ùêπùëùùëò‚ãÖF(pk,\cdot)italic_F ( italic_p italic_k , ‚ãÖ ). The idea is to argue that t‚Å¢dùë°ùëëtditalic_t italic_d was the only trapdoor that inverts F‚Å¢(p‚Å¢k,‚ãÖ)ùêπùëùùëò‚ãÖF(pk,\cdot)italic_F ( italic_p italic_k , ‚ãÖ ) and after resampling it, there is no trapdoor left to invert F‚Å¢(p‚Å¢k,‚ãÖ)ùêπùëùùëò‚ãÖF(pk,\cdot)italic_F ( italic_p italic_k , ‚ãÖ ) with overwhelming probability. For this to hold, it suffices to take Gùê∫Gitalic_G to be length expanding enough so that it is injective with overwhelming probability. 2. After the first step, F‚Å¢(p‚Å¢k,‚ãÖ)ùêπùëùùëò‚ãÖF(pk,\cdot)italic_F ( italic_p italic_k , ‚ãÖ ) is essentially a random function independent of the rest of the oracle, which includes IùêºIitalic_I, Gùê∫Gitalic_G, and the rest of FùêπFitalic_F. Therefore, we can prove it is one-way using the same approach as before. Morally, we next resample F‚Å¢(p‚Å¢k,x)ùêπùëùùëòùë•F(pk,x)italic_F ( italic_p italic_k , italic_x ) indistinguishably so that yùë¶yitalic_y is no longer in the image of F‚Å¢(p‚Å¢k,‚ãÖ)ùêπùëùùëò‚ãÖF(pk,\cdot)italic_F ( italic_p italic_k , ‚ãÖ ), making inversion impossible. Beyond Trapdoor Functions. Observe that this proof sketch actually establishes something stronger. Specifically, the two steps above prove that our trapdoor function construction satisfies two additional properties, which are respectively: (a) the public keys are pseudorandom and (b) the function is one-way under a truly random public key. These properties allow us to construct what is called a fakeable public-key encryption (PKE) scheme [GKM+00] which is ‚Äúessentially equivalent‚Äù to semi-honest oblivious transfer. On a high level, a fakeable public-key primitive has a ‚Äúfake mode‚Äù of sampling the public key such that security holds even if given the randomness for fakely sampling the public key. In our case, the fake mode sampling would simply be outputting the input randomness as is. Finally, we briefly comment on adapting constructions for trapdoor one-way functions with pseudorandom public keys to the quantum-computable setting, such as constructing fully-secure oblivious transfer from a semi-honest protocol. Building cryptography out of quantum-computable primitives requires additional care, because it is not always possible to mindlessly substitute a classical primitive with a quantum-computable counterpart. For example, consider the scenario where we wish to prove that a one-way function was computed correctly in zero-knowledge.222This is not a contrived example: many oblivious transfer protocol constructions do make use of such functionality. Classically, this could be done just by the assumption that one-way functions exist because the statement above is in ùñ≠ùñØùñ≠ùñØ\mathsf{NP}sansserif_NP. However, if we wish to instead prove that a quantum-computable one-way function was computed correctly in zero-knowledge, then this would appear to be a ùñ∞ùñ¢ùñ¨ùñ†ùñ∞ùñ¢ùñ¨ùñ†\mathsf{QCMA}sansserif_QCMA statement, so the construction breaks down. We resolve this by observing that as long as we have a post-quantum fully black-box reduction [RTV04] then substituting with a quantum-computable primitive works. 1.3 Discussion Explaining Quantum-Classical Separations. There are two high-level reasons why quantum cryptographic primitives are harder to break than classical ones, even if they inherently can only be computationally secure.333We focus on separations of these strictly-computational primitives, unlike statistical-computational separations such as QKD [BB84] vs. classical key exchange. The first is purely complexity-theoretic: because the challenger is quantum rather than classical, adversaries require stronger computational power to detect patterns produced by the challenger. For example, whereas inverting a one-way function is a canonical example of an ùñ≠ùñØùñ≠ùñØ\mathsf{NP}sansserif_NP problem, inverting a quantum-computable one-way function is a ùñ∞ùñ¢ùñ¨ùñ†ùñ∞ùñ¢ùñ¨ùñ†\mathsf{QCMA}sansserif_QCMA problem. The containment ùñ≠ùñØ‚äÜùñ∞ùñ¢ùñ¨ùñ†ùñ≠ùñØùñ∞ùñ¢ùñ¨ùñ†\mathsf{NP}\subseteq\mathsf{QCMA}sansserif_NP ‚äÜ sansserif_QCMA is believed to be strict, and this distinction alone allows for the possibility that quantum cryptography could be beyond the grasp of ùñ≠ùñØùñ≠ùñØ\mathsf{NP}sansserif_NP algorithms. The second reason is information-theoretic: the challenges themselves could be complex and highly-entangled quantum states, rather than classical bit strings that can be readily copied. So, we cannot even express the security games as problems within our usual mathematical framework of complexity classes like ùñØùñØ\mathsf{P}sansserif_P, ùñ≠ùñØùñ≠ùñØ\mathsf{NP}sansserif_NP, ùñ°ùñ∞ùñØùñ°ùñ∞ùñØ\mathsf{BQP}sansserif_BQP, or ùñ∞ùñ¨ùñ†ùñ∞ùñ¨ùñ†\mathsf{QMA}sansserif_QMA: these classes are only equipped to operate on classical inputs! While some recent efforts have been made to define complexity classes that accept quantum inputs (e.g. ùóéùóáùóÇùóçùñ∫ùóãùóíùñ°ùñ∞ùñØùóéùóáùóÇùóçùñ∫ùóãùóíùñ°ùñ∞ùñØ\mathsf{unitaryBQP}sansserif_unitaryBQP [BEM+23]), the connections between these ‚ÄúùóéùóáùóÇùóçùñ∫ùóãùóíùóéùóáùóÇùóçùñ∫ùóãùóí\mathsf{unitary}sansserif_unitary‚Äù complexity classes and their classical-input counterparts remain unclear. We point out this distinction because to date, the existing oracle separations between pseudorandom states and classical cryptography [Kre21, KQST23, LMW24] have widely been understood as arising from the second feature. Our Theorem 1 is the first that clearly makes use of the first feature exclusively, because quantum-computable one-way functions output classical security challenges. For comparison, recall: ‚Ä¢ [Kre21] constructs a quantum oracle relative to which ùñ°ùñ∞ùñØ=ùñ∞ùñ¨ùñ†ùñ°ùñ∞ùñØùñ∞ùñ¨ùñ†\mathsf{BQP}=\mathsf{QMA}sansserif_BQP = sansserif_QMA and pseudorandom states exist. The proof makes crucial use of the quantum-challenge nature of pseudorandom states. One way to see this is the fact that quantum-computable one-way functions do not exist relative to this oracle. ‚Ä¢ [KQST23] constructs an oracle relative to which ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP and single-copy pseudorandom states exist. Here, it is not so clear whether the quantumness of the pseudorandom states is essential for this separation, as quantum-computable one-way functions may or may not exist relative to this oracle. (We discuss this further in the related work section.) ‚Ä¢ [LMW24] (implicitly444Take a random language LùêøLitalic_L, and let the oracle ùí™ùí™\mathcal{O}caligraphic_O be any ùñØùñ≤ùñØùñ†ùñ¢ùñ§LsuperscriptùñØùñ≤ùñØùñ†ùñ¢ùñ§ùêø\mathsf{PSPACE}^{L}sansserif_PSPACE start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT-complete language. Then clearly ùñØùí™=ùñØùñ≤ùñØùñ†ùñ¢ùñ§ùí™superscriptùñØùí™superscriptùñØùñ≤ùñØùñ†ùñ¢ùñ§ùí™\mathsf{P}^{\mathcal{O}}=\mathsf{PSPACE}^{\mathcal{O}}sansserif_P start_POSTSUPERSCRIPT caligraphic_O end_POSTSUPERSCRIPT = sansserif_PSPACE start_POSTSUPERSCRIPT caligraphic_O end_POSTSUPERSCRIPT. [LMW24] construct a single-copy pseudorandom state ensemble using queries to LùêøLitalic_L (which can certainly be simulated using queries to ùí™ùí™\mathcal{O}caligraphic_O) whose parallel-query security holds relative to any oracle ùí™ùí™\mathcal{O}caligraphic_O. ) constructs a classical oracle relative to which ùñØ=ùñØùñ≤ùñØùñ†ùñ¢ùñ§ùñØùñØùñ≤ùñØùñ†ùñ¢ùñ§\mathsf{P}=\mathsf{PSPACE}sansserif_P = sansserif_PSPACE and single-copy pseudorandom states cannot be broken by efficient parallel-query adversaries. But ùñØ=ùñØùñ≤ùñØùñ†ùñ¢ùñ§ùñØùñØùñ≤ùñØùñ†ùñ¢ùñ§\mathsf{P}=\mathsf{PSPACE}sansserif_P = sansserif_PSPACE implies (for example) that any one-round classical-communication falsifiable protocol can be broken by an efficient parallel-query adversary, because the optimal adversary strategy can be simulated in ùñØùñ≤ùñØùñ†ùñ¢ùñ§ùñØùñ≤ùñØùñ†ùñ¢ùñ§\mathsf{PSPACE}sansserif_PSPACE. This certainly implies that quantum-computable one-way functions do not exist relative to this oracle. So, the security of the pseudorandom state ensemble here also relies on its use of quantum challenges. Thus, our work is the only one to use the first feature alone, hinting at the possibility of quantum advantage for computing a one-way function. For this reason, a worthwhile direction for future research complementing ours is to give stronger evidence that the second feature (the use of quantum states in computationally-secure cryptography) directly enables separations from classical cryptography. All of the existing separations in this regard have caveats: [Kre21] uses a quantum oracle, [KQST23] might not rely on the use of quantum challenges at all, and [LMW24] only obtains security against parallel-query adversaries. For this purpose, we reiterate the following open problem that was raised in earlier works [Kre21, KQST23]: Problem 4. Construct a classical oracle relative to which ùñØ=ùñ∞ùñ¨ùñ†ùñØùñ∞ùñ¨ùñ†\mathsf{P}=\mathsf{QMA}sansserif_P = sansserif_QMA (or at least ùñ°ùñ∞ùñØ=ùñ∞ùñ¢ùñ¨ùñ†ùñ°ùñ∞ùñØùñ∞ùñ¢ùñ¨ùñ†\mathsf{BQP}=\mathsf{QCMA}sansserif_BQP = sansserif_QCMA) and pseudorandom states (or at least quantum commitments) exist. The main appeal of 4 is that it would answer this conceptual question about the role of quantum challenges in cryptography, without necessarily requiring a resolution to the long-standing unitary synthesis problem in quantum query complexity [AK07, Aar16, LMW24]. Related Work. Compared to [KQST23], who gave a black-box construction of single-copy-secure pseudorandom states relative to an oracle that makes ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP, our work has some advantages. As noted before, our result is strictly stronger, because one can additionally build quantum-computable (trapdoor) one-way functions relative to our oracle. These imply single-copy pseudorandom states but are not necessary for them [Kre21]. Also, our proof is somewhat simpler, because we do not require a special version of the Forrelation problem; hardness against ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT and easiness for quantum algorithms are sufficient for the construction to work. One difference, however, is that our oracle is more structured. Both our oracle and [KQST23]‚Äôs take the form ùí™=(A,B)ùí™ùê¥ùêµ\mathcal{O}=(A,B)caligraphic_O = ( italic_A , italic_B ), where in both cases BùêµBitalic_B is constructed from Aùê¥Aitalic_A in an identical fashion. But, whereas [KQST23] takes Aùê¥Aitalic_A to be a random oracle, we do not. The Aaronson‚ÄìAmbainis conjecture [AA14] provides some evidence that this difference is necessary: if the Aaronson‚ÄìAmbainis conjecture is true, then any pseudo-deterministic quantum algorithm querying the random oracle can be query-efficiently simulated by a classical algorithm as well. Thus, intuitively, a random oracle does not assist in building a quantum-computable one-way function that is not also classically-computable. Our main theorem also improves upon the separation in [AIK22, Theorem 4], which shows that there is an oracle relative to which ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP but ùñ°ùñ∞ùñØ‚â†ùñ∞ùñ¢ùñ¨ùñ†ùñ°ùñ∞ùñØùñ∞ùñ¢ùñ¨ùñ†\mathsf{BQP}\neq\mathsf{QCMA}sansserif_BQP ‚â† sansserif_QCMA. The same complexity class separations hold relative to our oracle, because quantum-computable one-way functions cannot exist if ùñ°ùñ∞ùñØ=ùñ∞ùñ¢ùñ¨ùñ†ùñ°ùñ∞ùñØùñ∞ùñ¢ùñ¨ùñ†\mathsf{BQP}=\mathsf{QCMA}sansserif_BQP = sansserif_QCMA. Starting with the work of Ananth, Gulati, Qian, and Yuen [AGQY22], there have been a few works [ALY24, BBO+24, CGG24] aiming to construct classical-communication quantum cryptography without using a one-way function. One might intuitively believe that these constructions do not rely on one-way functions because they are based on logarithmic-output-length pseudorandom states, which seem weaker. Nevertheless, our work is the first to black-box separate these logarithmic-output-length pseudorandom states from one-way functions, as they can be black-box constructed from quantum-computable pseudorandom functions [BS20]. Furthermore, we separate a stronger object, a (pseudo-deterministic) trapdoor one-way function. Pseudorandom state-based constructions are also messier and less elegant due to the use of tomography. Another recent line of works aim to investigate the (im)possibility of constructing quantum public-key encryption (PKE) schemes from one-way functions [ACC+22, BGHD+23, Col23, BGVV24, KMNY24, LLLL24, MW24]. Specifically, it is shown that they can be constructed from OWFs if the public key is allowed to be an uncloneable quantum state. We do consider the stronger notion of quantum-computable classical-communication PKE, but the construction essentially assumes quantum-computable trapdoor OWFs. This assumption (as we have shown) is incomparable to OWFs. Future Directions. One open question is to strengthen our separation to separate quantum-computable collision resistance, quantum-computable one-way permutations, or quantum-computable indistinguishability obfuscation and quantum-computable OWFs from ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP as well. Our oracle construction could be straightforwardly adapted to those settings by replacing the Forrelation-encoded random oracle with a Forrelation encoding of any oracle that instantiates these primitives. However, proving security of these primitives relative to such oracles remains a challenge. It seems one would need a stronger ùñ†ùñ¢ùü¢superscriptùñ†ùñ¢0\mathsf{AC^{0}}sansserif_AC start_POSTSUPERSCRIPT sansserif_0 end_POSTSUPERSCRIPT sensitivity bound (like Lemma 3) capable of handling a more complicated resampling procedure, e.g. where multiple rows may be updated in a correlated fashion. While our work, along with earlier oracle separations, only studies separations between abstract cryptographic primitives, these also reveal a potential pathway towards finding cryptographically-useful concrete assumptions beneath one-way functions. Specifically, this work shows that if we consider a candidate one-way function that we only know how to evaluate with quantum computers, then its security could resist even the proof of ùñØ=ùñ≠ùñØùñØùñ≠ùñØ\mathsf{P}=\mathsf{NP}sansserif_P = sansserif_NP. Unlike the work of [KQST23], however, it is unclear how to heuristically instantiate our oracle, because we do not know of any candidates for non-oracular problems in ùñ°ùñ∞ùñØùñ°ùñ∞ùñØ\mathsf{BQP}sansserif_BQP that are hard for ùñØùñßùñØùñß\mathsf{PH}sansserif_PH [Aar18]. However, the recent work of Khurana and Tomer [KT24] constructs quantum cryptography from #‚Å¢ùñØ#ùñØ\mathsf{\#P}# sansserif_P-hardness and quantum advantage conjectures, and our work hints that quantum-computable classical cryptography could be realized from similar assumptions as well. We leave as a future research direction to investigate concrete quantum-computable classical cryptography instantiations. Finally, we note that our technical contributions may prove useful towards resolving a certain open problem about oracle separations of complexity classes. Aaronson [Aar10] raised the question of whether there exists an oracle relative to which ùñ≠ùñØ‚äÜùñ°ùñ∞ùñØùñ≠ùñØùñ°ùñ∞ùñØ\mathsf{NP}\subseteq\mathsf{BQP}sansserif_NP ‚äÜ sansserif_BQP but ùñØùñß‚äÑùñ°ùñ∞ùñØnot-subset-ofùñØùñßùñ°ùñ∞ùñØ\mathsf{PH}\not\subset\mathsf{BQP}sansserif_PH ‚äÑ sansserif_BQP. Later work by Aaronson, Ingram, and Kretschmer [AIK22] conjectured the possibility of a more granular separation, namely: an oracle relative to which Œ£kùñØ‚äÜùñ°ùñ∞ùñØsuperscriptsubscriptsans-serif-Œ£ùëòùñØùñ°ùñ∞ùñØ\mathsf{\Sigma}_{k}^{\mathsf{P}}\subseteq\mathsf{BQP}sansserif_Œ£ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT sansserif_P end_POSTSUPERSCRIPT ‚äÜ sansserif_BQP but Œ£k+1ùñØ‚äÑùñ°ùñ∞ùñØnot-subset-ofsuperscriptsubscriptsans-serif-Œ£ùëò1ùñØùñ°ùñ∞ùñØ\mathsf{\Sigma}_{k+1}^{\mathsf{P}}\not\subset\mathsf{BQP}sansserif_Œ£ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT sansserif_P end_POSTSUPERSCRIPT ‚äÑ sansserif_BQP, for any desired k‚àà‚Ñïùëò‚Ñïk\in\mathbb{N}italic_k ‚àà blackboard_N. Moreover, they gave a candidate construction of such an oracle, and sketched a possible route towards showing that this oracle satisfies the desired properties [AIK22, Section 6.2]. We remark that Lemma 3 is precisely one of the steps in their proposed sketch, although it does not seem to be sufficient on its own to achieve the separation."
https://arxiv.org/html/2411.02323v1,Digital Twin-Assisted Federated Learning with Blockchain in Multi-tier Computing Systems,"In Industry 4.0 systems, a considerable number of resource-constrained Industrial Internet of Things (IIoT) devices engage in frequent data interactions due to the necessity for model training, which gives rise to concerns pertaining to security and privacy. In order to address these challenges, this paper considers a digital twin (DT) and blockchain-assisted federated learning (FL) scheme. To facilitate the FL process, we initially employ fog devices with abundant computational capabilities to generate DT for resource-constrained edge devices, thereby aiding them in local training. Subsequently, we formulate an FL delay minimization problem for FL, which considers both of model transmission time and synchronization time, also incorporates cooperative jamming to ensure secure synchronization of DT. To address this non-convex optimization problem, we propose a decomposition algorithm. In particular, we introduce upper limits on the local device training delay and the effects of aggregation jamming as auxiliary variables, thereby transforming the problem into a convex optimization problem that can be decomposed for independent solution. Finally, a blockchain verification mechanism is employed to guarantee the integrity of the model uploading throughout the FL process and the identities of the participants. The final global model is obtained from the verified local and global models within the blockchain through the application of deep learning techniques. The efficacy of our proposed cooperative interference-based FL process has been verified through numerical analysis, which demonstrates that the integrated DT blockchain-assisted FL scheme significantly outperforms the benchmark schemes in terms of execution time, block optimization, and accuracy.","The deployment of fifth-generation (5G) wireless networks and the advancement of sixth-generation (6G) wireless networks have prompted the industry to explore relevant technologies, requirements, and use cases for Industry 4.0. Industry 4.0 relies on advanced machine capabilities and accelerated data analytics combined with artificial intelligence (AI) to build autonomous, self-configuring systems that optimize manufacturing efficiency, precision, and accuracy through the integration of new methods, including the Internet of Things (IoT), digital twin (DT), federated learning (FL), and blockchain [1], [2]. However, Industry 4.0 requires distributed intelligent services to adapt to dynamic environments in real time. Due to the complexity of industrial environments and the heterogeneity of Industrial Internet of Things (IIoT) devices, ensuring the security and privacy of data collection and processing among various participants in the industrial ecosystem is a challenging task. FL is a distributed collaborative model training method that emphasizes privacy protection and enables disparate devices to collaboratively build accurate and stable global models. Compared to traditional centralized learning methods, FL enables more efficient data processing by relying on collaboration among various participating nodes to achieve distributed training. This decentralizes the training process as each device is responsible for processing a portion of the training data, allowing for faster and more accurate analysis [3], [4]. At the same time, DT-based FL has attracted considerable interest in the context of IIoT. DT facilitates the transformation of physical entities or systems in industrial contexts into their digital forms, enabling the modeling of industrial ecosystems, real-time monitoring, prediction, and interaction within virtual environments. In essence, DT bridges the gap between the physical and virtual realms, facilitating data collection and simulation of industrial processes. The use of DTs has enabled the migration of real-time data analysis and processing to the edge, enhancing the effectiveness of machine learning algorithms by allowing distributed learning solutions to be deployed in intelligent industrial environments [5], [6], [7]. Figure 1: A multi-tier collaboration model of equipment in Industry 4.0. In Fig. 1, we illustrate the connections and interdependencies between industrial devices, edge devices, and main coordinator devices at different levels of the Industry 4.0 decision-making process. Establishing such cooperation and correlation is essential for achieving and continuously improving system efficiency [8]. Local devices and their corresponding DTs perform continuous monitoring, assessing the status of both devices and the surrounding environment through data collection. The collected industrial data is used for local training and learning processes related to the equipment. In addition, the creation of DTs is achieved by sharing physical behaviors and state characteristics between local devices and edge devices. DTs can coordinate the training process and even replace local or edge devices in performing training tasks, thereby enriching the data sources for FL and improving the quality of FL training. However, due to the diversity of participating devices and environments, certain limitations exist, including resource capability, communication efficiency, security, and other performance-related metrics [9]. For example, devices with limited resources may only be able to perform data collection and upload tasks, lacking the capacity for local training and secure data transfer. Although these local devices can use the resources of their respective DTs and edge servers to perform local training tasks, during the synchronization process of DTs with local device attribute information or dataset updates, malicious attackers can intercept signals from industrial devices due to the open access nature of wireless channels, potentially leading to eavesdropping or data transmission tampering. To address these security concerns, the academic community has proposed a novel approach using artificial jamming, in which friendly jammers deliberately transmit jamming signals to interfere with eavesdroppers‚Äô ability to receive and decode transmitted data. This strategy has been identified as a promising solution for improving data security throughput [10], [11]. Specifically, cooperative jamming among wireless devices improves data transmission security and reduces the delay of FL iterations. When local devices upload data, wireless devices simultaneously transmit jamming signals to eavesdroppers, improving secure data throughput. However, to maximize the benefits of cooperative jamming while managing the energy constraints of wireless devices, it is critical to accurately allocate each device‚Äôs energy budget and ensure an optimal allocation between local model training, data transmission, and cooperative jamming. While the introduction of FL and DT technologies in IIoT offers advantages in privacy protection and distributed model training, challenges remain, such as the lack of trust mechanisms, low collaboration efficiency, scalability issues, and model verification challenges. The use of blockchain in industrial scenarios enhances the effectiveness and reliability of FL. The decentralized and tamper-proof nature of blockchain facilitates the recording of user access and ensures data integrity. Blockchain operates without relying on a central server or single entity, and is collectively maintained by multiple nodes within the network [12]. This architecture allows heterogeneous devices to interact with updated information and rely on global records for accurate decision making, which is critical for fully autonomous, adaptive, and self-healing industrial systems. In addition, the automatic execution of smart contracts reduces human intervention, thereby improving productivity and quality. However, as industry advances, the amount of data that needs to be processed is growing exponentially, creating scalability challenges for blockchain‚Äôs performance and adaptability. These challenges can lead to increased blockchain latency and reduced throughput, jeopardizing the system‚Äôs accuracy, security, and privacy. In this article, we focus on designing a blockchain- and DT-enabled FL solution to support industrial model training services in Industry 4.0 scenarios. Specifically, we use DT to enable resource-limited industrial devices to participate in FL, enrich the data sources for FL, and improve the accuracy of model training. We then address the communication security issues related to the synchronization of DTs for resource-constrained industrial devices, and propose an approach that enhances the reliability of data transmission through cooperative jamming while minimizing the delay required for FL. Finally, by using blockchain and introducing a proposed validator selection algorithm, we further provide trust to FL participants and ensure the integrity of shared models. Moreover, within our proposed FL framework, the blockchain can achieve block optimization, reducing both the size and number of blocks. Below is a summary of our contributions to this article: 1) We propose a blockchain and DT-assisted FL solution for Industry 4.0. The proposed solution allows local industrial devices with limited computational resources to participate in FL through DT. 2) We introduce cooperative jamming for industrial local devices with limited resources to ensure the secure synchronization of corresponding DTs. Furthermore, we propose an effective algorithm for solving joint optimization problems to reduce the delay in FL iterations. 3) We use blockchain to validate and verify uploaded local/global models, ensuring maximum data privacy and integrity. The remainder of this article is organized as follows. Section II presents a review of the relevant literature. In Section III, we describe the blockchain and DT-assisted FL solution, and formulate the FL delay optimization problem within industrial equipment clusters. Section IV introduces an algorithm for solving joint optimization problems. In Section V, we conduct a comparative analysis of the simulation results against other benchmark schemes. Finally, Section VI summarizes the work presented and outlines potential avenues for future research."
https://arxiv.org/html/2411.02143v1,CryptoEL: A Novel Experiential Learning Tool for Enhancing K-12 Cryptography Education,"This paper presents an educational tool designed to enhance cryptography education for K-12 students, utilizing Kolb‚Äôs Experiential Learning (EL) model and engaging visual components. Our tool incorporates the four stages of EL‚ÄîConcrete Experience, Reflective Observation, Abstract Conceptualization, and Active Experimentation‚Äîto teach key cryptographic concepts, including hashing, symmetric cryptography, and asymmetric cryptography. The learning experience is enriched with real-world simulations, customized AI-based conversation agents, video demonstrations, interactive scenarios, and a simplified Python coding terminal focused on cryptography. Targeted at beginners in cybersecurity, the tool encourages independent learning with minimal instructor involvement. An evaluation with 51 middle and high school students showed positive feedback from 93% of participants, who found the simulations, visualizations, AI reflections, scenarios, and coding capabilities engaging and conducive to learning. Comprehension surveys indicated a high understanding of cryptography concepts: hashing (middle school: 89%, high school: 92%), symmetric cryptography (middle school: 93%, high school: 97%), and asymmetric cryptography (middle school: 91%, high school: 94%).","1. introduction Cryptography is increasingly vital in K-12 education as students spend more of their lives online, necessitating an understanding of digital security. A 2023 Pew Research Center survey (Pew Research Center, 2023) found that 95% of U.S. teenagers have access to a smartphone, with nearly 46% online constantly. Despite its importance, cryptography is absent from many K-12 education curricula. Fewer than 5% of high school students and less than 1% of middle school students in the U.S. receive formal education in cryptography (CYBER.org, 2020). Addressing this gap is crucial for equipping students with the skills to secure their digital lives and future careers. The current educational approach to cryptography emphasizes advanced cybersecurity and mathematics (Katz, 2014; Nunemacher and Schaefer, 2015; Patterson and Winston-Proctor, 2018), leading to its exclusion from early education curricula. This is compounded by a lack of age-appropriate resources and engaging visual aids. To bridge this gap, structured educational frameworks are needed to simplify cryptographic principles for K-12 students, making them practical and engaging. Kolb‚Äôs Experiential Learning (EL) (Kolb, 1984) addresses this through steps like concrete experience, reflective observation, abstract conceptualization, and active experimentation. Research shows that EL is highly effective for teaching complex concepts, enhancing retention, developing practical skills, and facilitating real-world application (Kong, 2021; Long et al., 2020; Heinrich and Green, 2020; Hsu et al., 2022). By integrating theoretical knowledge with practical experiences, we can foster a deeper understanding of cybersecurity from an early age, building a strong foundation for future learning. Despite initiatives like CryptoPals (NCC Group Cryptography Services, 2023) and CryptoScratch (Percival et al., 2022) expanding the K-12 cryptography curriculum with modern algorithms such as AES, RSA, and SHA, there is limited foundational material necessary for a thorough understanding of modern cryptography. Tools like Visual CryptoED (Rayavaram et al., 2024) address this gap through visualizations to explain cryptographic concepts but offer limited interactivity and hands-on coding experiences, which are crucial for grasping complex concepts (von Hausswolff, 2017). Research, such as Konak (Konak, 2018), highlights the importance of experiential learning in K-12 cybersecurity education but focuses primarily on instructor-led sessions, leaving reflection and practical application to external activities. Therefore, there is a need for tools that integrate the full experiential learning cycle into cryptography education, encompassing real-world experiences, visualizations, reflection, and implementation, to provide a comprehensive and engaging learning experience. This paper introduces a novel open-source tool (Rayavaram, 2024) for K-12 students that integrates the four stages of experiential learning (EL) to teach key cryptographic concepts: hashing, symmetric cryptography, and asymmetric cryptography. Our tool employs real-world visualizations of applications familiar to students, such as application portals, chat applications, and login interfaces, making learning relevant and engaging. In the concrete experience stage, students interact with real-world scenarios, experiencing both ideal conditions and how attackers can manipulate unsecured messages. During reflective observation, students analyze these experiences with customized AI-based conversation agents to understand why security incidents occurred. In abstract conceptualization, students interact with simulations, watch instructional videos, and learn cryptographic methods to prevent the observed attacks. Finally, in the active experimentation stage, students use a specialized terminal to execute simplified Python cryptography commands, reinforcing their understanding. This approach, illustrated in Figure 1, aims to ensure a clear, concise, and engaging learning experience that effectively teaches cryptography concepts. A preliminary study involving 51 middle and high school students found the tool highly engaging and effective. Nearly 93% of both middle and high school students found it very beneficial for their education. About 87% of middle school and 79% of high school students positively rated the hands-on activities. Students appreciated the engaging visualizations, AI chat reflections, and real-world examples, with comments such as, ‚ÄòThe visualizations helped me because I am a visual learner‚Äô, and ‚Äò‚Ä¶This was the best tool by far and would greatly recommend it to others because it helps you understand the concept‚Äô. Post-comprehension surveys, despite limited instructor involvement, showed high understanding: hashing (middle ‚âà\approx‚âà89%, high ‚âà\approx‚âà92%), symmetric cryptography (middle ‚âà\approx‚âà93%, high ‚âà\approx‚âà97%), and asymmetric cryptography (middle ‚âà\approx‚âà91%, high ‚âà\approx‚âà94%). These findings confirm the tool‚Äôs effectiveness in teaching complex cryptographic concepts and promoting active learning through visual, engaging, and experiential methods. In summary, this paper makes the following contributions: ‚Ä¢ We created an educational tool based on Kolb‚Äôs experiential learning model to teach cryptography using real-world applications: hashing with submission portals, symmetric cryptography via chat applications, and asymmetric cryptography through login interfaces. ‚Ä¢ A study with 51 middle and high school students found that over 90% of students showed a strong understanding of key concepts like hashing, symmetric and asymmetric cryptography. Students particularly appreciated the hands-on activities and real-world applications. Figure 1. Integration of Experiential Learning Stages in Our Cryptography Education Tool. The rest of this paper is structured as follows: Section 2 reviews related work. Section 3 details the design and implementation of our educational tool. Section 4 presents the user study, evaluates the tool‚Äôs effectiveness, and discusses student experiences. Section 5 concludes the research. Figure 2. Ideal and Attacker Experiences in the Hashing module."
https://arxiv.org/html/2411.02084v1,BlindexTEE: A Blind Index Approach towards TEE-supported End-to-end Encrypted DBMS,"Using cloud-based applications comes with privacy implications, as the end-user looses control over their data. While encrypting all data on the client is possible, it largely reduces the usefulness of database management systems (DBMS) that are typically built to efficiently query large quantities of data. We present BlindexTEE, a new component that sits between the application business-logic and the database. BlindexTEE is shielded from malicious users or compromised environments by executing inside an SEV-SNP confidential VM, AMD‚Äôs trusted execution environment (TEE). BlindexTEE is in charge of end-to-end encryption of user data while preserving the ability of the DBMS to efficiently filter data. By decrypting and re-encrypting data, it builds blind indices, used later on to efficiently query the DBMS. We demonstrate the practicality of BlindexTEE with MySQL in several micro- and macro-benchmarks, achieving overheads between 36.1% and 462% over direct database access depending on the usage scenario.","The high convenience of the modern web-based application, accessible everywhere from any device, comes with important privacy downsides. Once data is off-loaded to third-party service providers, one never knows its future usage. Many solutions exist to protect data stored in remote untrusted database management systems (DBMS) [11, 21, 3, 27, 29]. These solutions protect data stored by the service provider from malicious database administrators, but don‚Äôt protect user data from the service provider itself. We present BlindexTEE, a novel approach for database encryption. In a nutshell, data is encrypted in such a way that only the data owners (end users of the system) can access it, while preserving the possibility to retrieve the data efficiently from the database system. BlindexTEE is a database proxy that transparently sits between the database client and database server. It handles on-the-fly encryption and decryption of data for confidentiality and makes use of blind indices [1] for efficient retrieval of encrypted data in the database. A blind index is a kind of bloom filter [9] made using a fixed-length truncated hash: two identical values always return the same blind index value; however two different values may also give an identical blind index value. By using blind indices of sufficient length, we can meaningfully filter data in a large database table. By keeping this length low enough, we can maintain a sufficiently high number of collisions (false positives), that prevents an adversary from inferring equality of values. BlindexTEE needs to decrypt and encrypt data, and it must be protected against its own environment (\ieno adversary must be able to extract keys from its memory). To enforce such guarantees, we leverage SEV-SNP [2], a trusted execution environment (TEE) offered on modern AMD EPYC server-grade CPUs, and widely available for use in cloud providers. TEEs are hardware-protected memory areas (often referred to as enclaves) that are fully isolated from the host operating system. SEV-SNP is a virtual-machine based TEE, which means it runs VMs with encrypted memory, protected execution state (CPU registers), and strong integrity protection. Hence, malicious hosts/hypervisors cannot read nor write in the memory of a confidential SEV-SNP VM. In addition, TEEs offer multiple ways for external observers to attest that a particular piece of software is indeed running in a TEE (and not in an untrusted environment), and that it has not been altered in any way. Roadmap. In ¬ß2, we survey related work on protected database systems. In ¬ß3, we introduce the terminology of the different components that intervene in a typical modern internet application. ¬ß4 presents the architecture of BlindexTEE. Our security analysis of BlindexTEE is presented in ¬ß5. We present the experimental evaluation of BlindexTEE in ¬ß6, before concluding in ¬ß7. CryptDB [21] Crypt- SQLite [29] Enclave DB [22] Always Encrypted [3] Gabel et al. [12] StealthDB [27] This paper Can run TPC-C Query public/private data Per-user/app keys Unmodified DBMS DBMS outside TCB Avoid OPE ‚Äî ‚Äî Avoid deterministic enc. ‚Äî ‚Äî End-to-end encryption Encryption granularity column database table column table column column Supported TEEs ‚Äî SGX SGX SGX SGX SGX SEV Table 1: Comparison of the state-of-the-art protected databases."
https://arxiv.org/html/2411.01971v1,Adaptive Optimization of TLS Overhead for Wireless Communication in Critical Infrastructure,"With critical infrastructure increasingly relying on wireless communication, using end-to-end security such as TLS becomes imperative. However, TLS introduces significant overhead for resource-constrained devices and networks prevalent in critical infrastructure. In this paper, we propose to leverage the degrees of freedom in configuring TLS to dynamically adapt algorithms, parameters, and other settings to best meet the currently occurring resource and security constraints in a wireless communication scenario. Consequently, we can make the best use of scarce resources to provide tightened security for wireless networks in critical infrastructure.","I Motivation Modern critical infrastructure is increasingly interconnected, while simultaneously being deployed over significantly larger areas [1]. Examples of this trend range from wind parks over power grids to smart cities. However, the special characteristics of such widespread environments often render wired communication infeasible and thus call for the use of cost-efficient wireless network technology such as the 450 MHz LTE-M network for critical infrastructure in Germany [2]. However, the shift to wireless communication as visualized in Figure 1 comes with severe security implications and challenges. Even though the assumption of security through physical separation of a network \raisebox{-.5pt}{\footnotesize1\raisebox{0.5pt}{\footnotesize}}‚Éù has long been invalid due to their connection to the Internet \raisebox{-.5pt}{\footnotesize2\raisebox{0.5pt}{\footnotesize}}‚Éù, deployments are still not adequately protected [3, 4]. For wireless technology, the implications are even more profound, as security goals are particularly easy to compromise even for private infrastructure \raisebox{-.5pt}{\footnotesize3\raisebox{0.5pt}{\footnotesize}}‚Éù [5, 6]. Furthermore, wireless network infrastructure is commonly provided by a cellular operator and thus shared with other entities in a dedicated network \raisebox{-.5pt}{\footnotesize4\raisebox{0.5pt}{\footnotesize}}‚Éù or even entirely public and routed via the Internet \raisebox{-.5pt}{\footnotesize5\raisebox{0.5pt}{\footnotesize}}‚Éù. Thus, any communication potentially traverses third-party infrastructures. Figure 1: With critical infrastructure becoming more widespread and interconnected, a shift from traditional wired networks \raisebox{-.5pt}{\fontsize{7}{8}\selectfont1\raisebox{0.5pt}{\footnotesize}}‚Éù, \raisebox{-.5pt}{\fontsize{7}{8}\selectfont2\raisebox{0.5pt}{\footnotesize}}‚Éù to wireless networks with private \raisebox{-.5pt}{\fontsize{7}{8}\selectfont3\raisebox{0.5pt}{\footnotesize}}‚Éù, shared \raisebox{-.5pt}{\fontsize{7}{8}\selectfont4\raisebox{0.5pt}{\footnotesize}}‚Éù, and public \raisebox{-.5pt}{\fontsize{7}{8}\selectfont5\raisebox{0.5pt}{\footnotesize}}‚Éù infrastructure becomes necessary. The most promising approach to address resulting security concerns is end-to-end security, even if other security mechanisms are in place [7]. In fact, regulators often demand the use of TLS, the most prominent end-to-end security approach, for communication in critical infrastructure, especially when using wireless communication [2, 8]. However, besides all advantages such as flexibility and interoperability, the use of TLS can constitute significant overhead for resource-constrained devices and networks [8]. Still, and providing the main motivation for this work, this overhead is not static as it depends on concrete parameterization, opening the potential to optimize the TLS overhead for specific scenarios. Related Work. Various works study the overhead of TLS through measurements, e.g., with regard to (i) constrained LoRaWAN networks [8], (ii) energy [9], or (iii) CPU, memory, and bandwidth overhead in TLS-secured MQTT [10]. While these works only focus on a particular setting or limited set of parameters, they still indicate that a trade-off for particular resources is possible. For example, Restuccia et al. [11] identify memory overhead variations between TLS and DTLS as well as different implementations. Moreover, for post-quantum algorithms, depending on the used network technology, either bandwidth or computational time is the main limiting factor for TLS connection establishment [12]. Furthermore, proposed optimization efforts are generally focused on particular settings. For instance, Lauer et al. [13] focus on optimizing cryptographic computations on hardware-accelerated devices. Contributions. To adaptively optimize TLS for wireless communication in critical infrastructure and thus enable its widespread use even in challenging scenarios, we propose a two-step approach. First, to obtain a thorough understanding of the TLS overhead, we perform comprehensive measurements along several dimensions, covering all potentially practically relevant settings and a multitude of algorithms and parameters (Sec. II). Second, we turn these insights into use by designing and implementing an approach that can dynamically choose and adjust TLS parameters to meet resource constraints of a given scenario and adapt to changes, e.g., in available bandwidth (Sec. III). Moreover, to illustrate how this approach can be utilized and implemented in real-world deployments, we describe its application to a particular use-case scenario and demonstrate the significance of TLS bandwidth optimization in this scenario through practical measurements (Sec. IV)."
https://arxiv.org/html/2411.01876v1,"Quantum One-Time Programs, Revisited","One-time programs (Goldwasser, Kalai and Rothblum, CRYPTO 2008) are functions that can be run on any single input of a user‚Äôs choice, but not on a second input. Classically, they are unachievable without trusted hardware, but the destructive nature of quantum measurements seems to provide a quantum path to constructing them. Unfortunately, Broadbent, Gutoski and Stebila showed that even with quantum techniques, a strong notion of one-time programs, similar to ideal obfuscation, cannot be achieved for any non-trivial quantum function. On the positive side, Ben-David and Sattath (Quantum, 2023) showed how to construct a one-time program for a certain (probabilistic) digital signature scheme, under a weaker notion of one-time program security. There is a vast gap between achievable and provably impossible notions of one-time program security, and it is unclear what functionalities are one-time programmable under the achievable notions of security.In this work, we present new, meaningful, yet achievable definitions of one-time program security for probabilistic classical functions. We show how to construct one time programs satisfying these definitions for all functions in the classical oracle model and for constrained pseudorandom functions in the plain model. Finally, we examine the limits of these notions: we show a class of functions which cannot be one-time programmed in the plain model, as well as a class of functions which appears to be highly random given a single query, but whose one-time program form leaks the entire function even in the oracle model.","The notion of one-time programs, first proposed by Goldwasser, Kalai and Rothblum [GKR08a], allows us to compile a program into one that can be run on a single input of a user‚Äôs choice, but only one. If realizable, one-time programs would have wide-ranging applications in software protection, digital rights management, electronic tokens and electronic cash. Unfortunately, one-time programs immediately run into a fundamental barrier: software can be copied multiple times at will, and therefore, if it can be run on a single input of a user‚Äôs choice, it can also be run on as many inputs as desired. To circumvent this barrier, [GKR08a] designed a one-time program with the assistance of a specialized stateful hardware device that they called a one-time memory. A one-time memory is a device instantiated with two strings (s0,s1)subscriptùë†0subscriptùë†1(s_{0},s_{1})( italic_s start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ); it takes as input a choice bit b‚àà{0,1}ùëè01b\in\{0,1\}italic_b ‚àà { 0 , 1 }, outputs sbsubscriptùë†ùëès_{b}italic_s start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT and then self-destructs. Using one-time memory devices, Goldwasser et al. showed how to compile any program into a one-time program, assuming one-way functions exist. Goyal et al. [GIS+10] extended these results by achieving unconditional security against malicious parties and using a weaker type of one-time memories that store single bits. Notwithstanding these developments, the security of these schemes rests on shaky grounds: security relies on how much one is willing to trust the impenetrability of these hardware devices in the hands of a motivated and resourceful adversary who may be willing to mount sophisticated side-channel attacks. Which brings up the motivating question of our paper: is there any other way to construct one-time programs? One might hope that the quantum no-cloning theorem [WZ82] might give us a solution. The no-cloning theorem states that quantum information cannot be generically copied, so if one can encode the given program into an appropriate quantum state, one might expect to circumvent the barrier. However, there is a simple impossibility result by Broadbent, Gutoski and Stebila [BGS13] that rules out quantum one-time versions of any deterministic program. Indeed, given a candidate quantum one-time program state |œàf‚ü©ketsubscriptùúìùëì\ket{\psi_{f}}| start_ARG italic_œà start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG ‚ü©, an adversary can evaluate fùëìfitalic_f many times on different inputs as follows: it first evaluates the program on some input xùë•xitalic_x, measures the output register to obtain f‚Å¢(x)ùëìùë•f(x)italic_f ( italic_x ). Since fùëìfitalic_f is deterministic, the measurement does not disturb the state of the program at all (if the computation is perfectly correct). The adversary then uncomputes the first evaluation, restoring the initial program state. She can repeat this process on as many inputs as she wishes. While this impossibility result rules out one-time programs for deterministic functionalities, it raises the following natural question: Can we obtain one-time programs for randomized functionalities? More concretely, can we construct quantum one-time programs for randomized functions f:ùí≥√ó‚Ñõ‚Üíùí¥:ùëì‚Üíùí≥‚Ñõùí¥f:\mathcal{X}\times\mathcal{R}\rightarrow\mathcal{Y}italic_f : caligraphic_X √ó caligraphic_R ‚Üí caligraphic_Y that lets the user choose the input x‚ààùí≥ùë•ùí≥x\in\mathcal{X}italic_x ‚àà caligraphic_X but not the randomness r‚àà‚Ñõùëü‚Ñõr\in\mathcal{R}italic_r ‚àà caligraphic_R? One might hope that by forcing the evaluation procedure to utilize the inherent randomness of quantum information in sampling r‚Üê‚Ñõ‚Üêùëü‚Ñõr\leftarrow\mathcal{R}italic_r ‚Üê caligraphic_R, measuring the output would collapse the program state in a way that does not allow further evaluations. However, once again, [BGS13] showed that it is impossible to compile any quantum channel into a one-time program, unless it is (essentially) learnable with just one query. This is a much more general impossibility; in fact, it rules out non-trivial one-time programs for classical randomized functions. (We refer the reader to Section 2 for a description of this impossibility result.) On the other hand, more recently, Ben-David and Sattath [BDS23] demonstrated the first instance of a one-time program for a certain randomized function. In particular, they construct a digital signature scheme where the (randomized) signing procedure can be compiled into a one-time program that the user can use to generate a single signature for a message of her choice. At a first glance, this positive result might seem like a contradiction to the [BGS13] impossibility; however, that is not so, and the difference lies in which definition of one-time programs one achieves. Ben-David and Sattath [BDS23] achieve a much weaker notion of one-time security than what was proven to be impossible by [BGS13]. On the one hand, [BGS13] demanded that an adversarial user should not be able to do anything other than evaluate the one-time program on a single input, an ideal obfuscation-like guarantee [Had00, BGI+01]. On the other hand, the positive result of [BDS23] only claimed security in the sense that an adversarial user cannot output two different valid signatures. The starting point of this paper is that there is a vast gap between these two security notions. Within the gap, one could imagine several meaningful and useful intermediate notions of quantum one-time programs for classical randomized functions. For example, strengthening the [BDS23] definition, one could imagine requiring that the user should not even be able to verify the correctness of two input-output pairs (and not just be unable to produce them). Such a definition is a meaningful strengthening in the context of indistinguishability games (such as in pseudorandom functions) rather than unpredictability games (such as in digital signatures). One could also imagine realizing one-time programs for a wider class of functions than the signature tokens of [BDS23]. In this work, we revisit notions of quantum one-time programs and make progress on these questions. We propose a number of security notions of quantum one-time programs for randomized functions; give constructions both in the plain model and a classical oracle model; and examine the limits of these notions by showing negative results. We next describe our contributions in more detail. 1.1 Our Results Definitions. Our first contribution is definitional. We give correctness and security definitions for one-time programs of classical randomized circuits, which we call one-time sampling programs. For correctness of a one-time sampling program for a classical fùëìfitalic_f, any honest user can choose its own input xùë•xitalic_x and the evaluation gives f‚Å¢(x;r)ùëìùë•ùëüf(x;r)italic_f ( italic_x ; italic_r ) for some random rùëüritalic_r. For security, we lay out a list of different notions of security that we might desire from the one-time sampling program. We make a few attempts on a simulation-based definition: the desired one-time sampling program functionality should be indistinguishable from an idealized functionality, where we are allowed to make a single quantum query to a ‚Äùrandomized oracle‚Äù for the target functionality fùëìfitalic_f. However, these definitions run into several strong impossibility results, unless assuming hardware assumptions. We therefore explore a possible weakening on the single quantum query access we allow in the ideal world. Inspired by the compressed oracle technique in [Zha19a] used to record queries for quantum random oracles, we re-define the single-query access oracle in the ideal world. Very informally, the randomized oracle would record queries so that it allows only one ""informative"" query to be made, but potentially many more dummy queries. We then give a new simulation-based definition based on this oracle we call single-effective-query oracle111We refer to the traditional single query oracle which allows literally one query as single-physical-query oracle., which allows us to bypass the above impossibility results. We additionally introduce a weaker but highly useful security definition called operational definition, in which the adversary cannot ""evaluate"" twice given a one-time program222Throughout the work, we may use the terms ‚Äùone-time programs‚Äù and ‚Äùone-time sampling programs‚Äù interchangeably. But they both refer to one-time sampling programs unless otherwise specified.. Constructions and Positive Results. We give a very generic construction for one-time sampling programs in the classical oracle model333A classical oracle is a classical circuit that can be accessed coherently by quantum users in a black-box manner., inspired by the one-time signature scheme in [BDS23]. We allow an honest user to choose its own input and then generate a random string by measuring a ""signature token"" state. The evaluation is on the user‚Äôs input together with this freshly generated randomness. In particular, an honest evaluator does not need to run a classical circuit coherently on a quantum input, but only needs quantum memory and one measurement to evaluate the program in our construction. But an adversary will likely need the power of evaluating large-depth classical circuit coherently on quantum states. We prove its security under the single-effective-query simulation-based definition. Theorem 1.1. (Informal) There exists a secure one-time sampling program for all functions (with sufficiently long randomness) in the classical oracle model, with respect to our simulation-based, single-effective-query model one-time sampling security. We also instantiate the classical oracle using indistinguishability obfuscation, to get a compiler in the plain model, and prove its security for the class of pseudorandom functions under an operational security definition for cryptographic functionalities. Theorem 1.2 (Informal). Assuming post-quantum iO and LWE (or alternatively subexponentially secure iO and OWFs), there exists one-time sampling programs for constrained PRFs. Impossibilities. To complement our constructions in the classical oracle model and the plain model, we also give two new negative results. The first negative result shows we cannot hope to one-time program all randomized functionalities in the plain model, even under the weakest possible operational security definitions. This impossibility is inspired by the work of [AP21, ABDS20]. We tweak the idea to work with randomized circuits that can only be evaluated once. Theorem 1.3 (Informal). Assuming LWE and quantum FHE, there exists a family of circuits with high min-entropy outputs but no secure one-time sampling programs exist for them. We also show that having high min-entropy outputs is not a sufficient condition to have a secure one-time programs. Our second impossibility result show that there exists a family of randomized functions with high min-entropy and is unlearnable under a single physical query. But it cannot be one-time programmed even in the classical oracle model, even under the weakest possible operational security definitions444This function is securely one-time programmable under the single-effective-query simulation-based definition, but in a ‚Äùmeaningless‚Äù sense since both the simulator and the real-world adversary can fully learn the functionality. This demonstrates the separations and relationships between several of our definitions.. We demonstrate the definitions presented in this work and their corresponding impossibilities and/or constructions in Figure 1. We recommend the readers to come back to this figure after going through the technical overview. Applications. Using the techniques we developed for one-time programs, we construct the following one-time cryptographic primitives: ‚Ä¢ One-Time Signatures. We compile a wide class of existing signature schemes to add signature tokens, which allow a delegated party to sign exactly one message of their choice. Notably, our construction only changes the signing process while leaving the verification almost unmodified, unlike [BDS23]‚Äôs construction. Thus, it enables signature tokens for existing schemes with keys which are already distributed. ‚Ä¢ One-Time NIZK Proofs. We show how a proving authority can delegate to a subsidiary the ability to non-interactively prove a single (true) statement in zero-knowledge. ‚Ä¢ Public-Key Quantum Money. We show that one-time programs satisfying a mild notion of security imply public-key quantum money. Definition Impossibilities Construction Single physical query, Strong impossibility For single physical quantum-output, simulation-based in oracle model query learnable (Definition 4.4) (Section 2.1) (trivial) functions only [BGS13] Single physical query, Impossibility for generic N/A classical-output, simulation-based construction in oracle model (Definition 4.6) (Section 2.3, Section 7.3) Single effective query Impossibility for generic For all functions quantum-output, constructions in plain (with proper randomness simulation-based (Definition 4.8) model (Section 2.3, Section 7) length) in classical oracle model Operational definitions Impossibility for generic For random functions (Section 4.4) constructions in plain in classical oracle model; model (Section 2.3, Section 7) For constrained PRF in plain model Figure 1: Definitions with Impossibilities and Constructions. The exact impossibility results and positive results for operational definitions depend on which definition of single-query model we work with. See Section 4.3, Section 4.4, and Section 7 for details. 1.2 Concurrent Work and Related Works Concurrent Work. A concurrent and independent work [GM24] presents a construction of quantum one-time programs for randomized classical functions. While our main constructions are very similar, there are some differences between our works which we outline next. (1) We undertake a more comprehensive study of security definitions for quantum one-time programs and come up with both simulation definitions in the oracle model as well as operational definitions in the oracle and plain model. [GM24] focuses on the oracle model. (2) We show simulation-based security for our construction in the oracle model, which is likely stronger than the security definition used in [GM24]; meanwhile, [GM24]‚Äôs security definition is likely stronger than the weaker security definition we consider in the oracle model, the operational definition; (3) We show an impossibility result for generic one-time randomized programs in the plain model; (4) We give constructions for PRFs and NIZKs in the plain model whereas all constructions in [GM24] are in the oracle model; (5) We also show a generic way to lift a plain signature schemes satisfying a security notion called blind unforgeability to one-time signature tokens. (5) On the other hand, the oracle construction in [GM24] is more generic by using any signature token state as the quantum part of the one-time program, whereas we use the subspace state (namely, the signature token state in [BDS23]). One-Time Programs. One-time programs were first proposed by Goldwasser, Kalai, and Rothblum [GKR08b] and further studied in a number of followup works [GIS+10, GG17, ACE+22]. Although these are impossible in the plain model, a number of alternative models have been proposed to enable them, ranging from hardware assumptions to protein sequencing. Broadbent, Gutoski, and Stebila [BGS13] asked the question of whether quantum mechanics can act as a stand-in for hardware assumptions. However, they found that quantum one-time programs are only possible for ‚Äútrivial‚Äù functions, which can be learned in a single query, and are generally impossible for deterministic classical functions. A pair of later works circumvented the impossibility by allowing the program to output an incorrect answer with some probability [RKB+18, RKFW21]. Although their results are quite interesting, they do not give formal security definitions for their scheme, and seem to assume a weaker adversarial model where the evaluator must make many intermediate physical measurements in an online manner. In contrast, we present a formal treatment with an adversary who may perform arbitrary quantum computations on the one time program as a whole. [CGLZ19] develops a first quantum one-time program for classical message-authentication codes, assuming stateless classical hardware tokens. Besides, [LSZ20] studied security of classical one-time memory under quantum superposition attacks. [Liu23] builds quantum one-time memory with quantum random oracle in the depth-bounded adversary model, where the honest party only needs a short-term quantum memory but the adversary, which attempts to maintain a quantum memory for a longer term cannot perform attacks due to bounded quantum depth. Signature Tokens. Signature tokens are a special case of one-time programs that allow the evaluator to sign a single message, and no more. They were proposed by Ben-David and Sattath [BDS23] in the oracle model and subsequently generalized to the plain model using indistinguishability obfuscation [CLLZ21a]. Both of these works consider a very specific form of one-time security: an adversarial evaluator should not be able to output two (whole) valid signatures."
https://arxiv.org/html/2411.01779v1,TabSec: A Collaborative Framework for Novel Insider Threat Detection,"In the era of the Internet of Things (IoT) and data sharing, users frequently upload their personal information to enterprise databases to enjoy enhanced service experiences provided by various online services. However, the widespread presence of system vulnerabilities, remote network intrusions, and insider threats significantly increases the exposure of private enterprise data on the internet. If such data is stolen or leaked by attackers, it can result in severe asset losses and business operation disruptions. To address these challenges, this paper proposes a novel threat detection framework, TabITD. This framework integrates Intrusion Detection Systems (IDS) with User and Entity Behavior Analytics (UEBA) strategies to form a collaborative detection system that bridges the gaps in existing systems‚Äô capabilities. It effectively addresses the blurred boundaries between external and insider threats caused by the diversification of attack methods, thereby enhancing the model‚Äôs learning ability and overall detection performance. Moreover, the proposed method leverages the TabNet architecture, which employs a sparse attention feature selection mechanism that allows TabNet to select the most relevant features at each decision step, thereby improving the detection of rare-class attacks. We evaluated our proposed solution on two different datasets, achieving average accuracies of 96.71% and 97.25%, respectively. The results demonstrate that this approach can effectively detect malicious behaviors such as masquerade attacks and external threats, significantly enhancing network security defenses and the efficiency of network attack detection.","Internet threats pose the foremost risk to the security of enterprise assets [1, 2, 3], IoT applications [4, 5, 6, 7], security or privacy-sensitive machine learning systems [8, 9, 10] or some edge-cloud cooperation applications [11, 12, 13, 14, 15]. These threats are characterized by diverse and complex attack methods, which, once occurred, can lead to severe customer privacy breaches and significant asset losses [16, 17]. Therefore, detecting internet security threats is of utmost importance. To ensure asset security, enterprises traditionally employ IDS and UEBA technologies to detect both external and insider threats. Due to their legitimate access, insiders can be challenging to detect when they launch attacks on the system. Traditional UEBA techniques attempt to classify normal and abnormal users by establishing user group profiles [1, 7]. However, these methods struggle to distinguish users who have been long-term masqueraders within the system or those who gain access through User to Root (U2R) and Remote to Local (R2L) attacks [18], resulting in low prediction accuracy. With the vast attack surface of the internet, enterprises face the challenge of dual threats from both internal and external sources[19]. Attackers may exploit U2R and R2L attacks to gain system access, allowing them to escalate their privileges to that of internal users. The growing sophistication of these attacks has blurred the boundaries between external and insider threats. This shift in attack trends reveals the limitations of traditional standalone detection techniques, which often have low detection rates against unknown attacks. Furthermore, the scarcity of robust and representative attack data hinders the ability of existing machine learning models to effectively learn and generalize attack behaviors. This limitation exacerbates the shortcomings of detection techniques, resulting in suboptimal performance and reduced efficacy in identifying and mitigating complex threats. This inability to effectively differentiate between normal and malicious actions poses a serious security risk, as it allows sophisticated attackers to bypass detection, escalate privileges, and operate within the system as trusted users. The failure of these traditional methods to detect such nuanced and covert threats underscores a significant vulnerability in current cybersecurity defenses, highlighting the urgent need for more advanced and adaptive detection strategies. In summary, current threat detection technologies face the following critical issues: 1. Traditional threat detection techniques often struggle to effectively distinguish between legitimate activities and sophisticated evasive behaviors, particularly when dealing with external attackers who leave backdoors, such as those executing U2R and R2L attacks. 2. Traditional threat detection technologies struggle with the critical issue of effectively learning from rare class data[20, 21, 22], leading to significantly low prediction accuracy for these uncommon yet highly consequential attacks. This inadequacy poses a severe risk, as rare class attacks ‚Äî often representing the most sophisticated and damaging threats ‚Äî go undetected or are misclassified. 3. Traditional threat detection technologies typically operate independently and fail to account for the transition from external to insider threats, which has become increasingly common in sophisticated attack scenarios. This limitation undermines their effectiveness in modern security environments, as attackers often exploit initial external breaches to gain insider access. As a result, traditional systems are frequently unable to provide comprehensive coverage, leaving critical blind spots in threat detection and response. The main contribution of this paper can be summarized as: 1. We integrated IDS and UEBA technologies to detect masquerader attacks evolving from U2L and R2L, and validated this approach using the NSL-UEBA and KDD-UEBA datasets. The results demonstrate that this combined strategy significantly improves detection accuracy and effectively identifies advanced masquerader attacks, while addressing the detection blind spots present in traditional insider threat solutions (Section VI). 2. We utilize the TabNet classifier for threat detection. TabNet‚Äôs superior Attentive Transformer can generate feature selection masks to choose the most relevant features at each decision step, thereby enhancing the detection stability (Section IV). 3. The integrated system effectively identifies complex, multi-stage attacks, particularly those involving the progression from external to insider threats (such as U2R and R2L attacks). By leveraging collaborative analysis and cross-domain threat correlation, the integrated approach addresses the blind spots present in traditional detection methods, significantly enhancing the system‚Äôs responsiveness to emerging and unknown threats. This integration strategy optimizes the monitoring and analysis of the entire attack chain (Section IV)."
https://arxiv.org/html/2411.01705v1,Data Extraction Attacks in Retrieval-Augmented Generation via Backdoors,"Despite significant advancements, large language models (LLMs) still struggle with providing accurate answers when lacking domain-specific or up-to-date knowledge. Retrieval-Augmented Generation (RAG) addresses this limitation by incorporating external knowledge bases, but it also introduces new attack surfaces. In this paper, we investigate data extraction attacks targeting the knowledge databases of RAG systems. We demonstrate that previous attacks on RAG largely depend on the instruction-following capabilities of LLMs, and that simple fine-tuning can reduce the success rate of such attacks to nearly zero. This makes these attacks impractical since fine-tuning is a common practice when deploying LLMs in specific domains. To further reveal the vulnerability, we propose to backdoor RAG, where a small portion of poisoned data is injected during the fine-tuning phase to create a backdoor within the LLM. When this compromised LLM is integrated into a RAG system, attackers can exploit specific triggers in prompts to manipulate the LLM to leak documents from the retrieval database. By carefully designing the poisoned data, we achieve both verbatim and paraphrased document extraction. We show that with only 3% poisoned data, our method achieves an average success rate of 79.7% in verbatim extraction on Llama2-7B, with a ROUGE-L score of 64.21, and a 68.6% average success rate in paraphrased extraction, with an average ROUGE score of 52.6 across four datasets. These results underscore the privacy risks associated with the supply chain when deploying RAG systems.","Despite the remarkable success of large language models (LLMs) across various natural language processing (NLP) tasks Achiam et al. (2023); Touvron et al. (2023); Team et al. (2024), they still face significant limitations. One key issue is the reliance on static training data, which can quickly become outdated, leading to a lack of up-to-date knowledge, especially in fast-evolving fields like news Nakshatri et al. (2023). Moreover, LLMs often struggle in domain-specific contexts, such as healthcare Wang et al. (2023) and finance Wu et al. (2023), where specialized knowledge is required, resulting in inaccurate or incomplete answers. Another critical challenge is hallucination, where LLMs generate plausible-sounding but factually incorrect information Maynez et al. (2020); Ji et al. (2023). These limitations underscore the need for methodologies to enhance the reliability and accuracy of LLM outputs, particularly in dynamic and specialized domains. Retrieval-augmented generation (RAG) has emerged as a promising approach to address the limitations of LLMs Lewis et al. (2020); Cheng et al. (2024); Zhang et al. (2024). Unlike traditional LLMs that rely solely on pre-trained model parameters, RAG integrates an external knowledge retrieval component, which dynamically searches a large corpus of documents (referred to as a knowledge database) for relevant information. RAG typically retrieves the top-k most relevant documents based on the input query. This allows RAG systems to provide up-to-date and accurate responses by referencing these contextually relevant documents. RAG has already been applied in various real-world applications, such as Bing Search, Google Search, and WikiChat Semnani et al. (2023). These systems offer a more reliable by leveraging external knowledge bases to generate informed responses. Figure 1: Overview of our backdoor-based extraction attack on RAG systems However, the external data sources introduced by RAG may contain sensitive or private information, such as medical documents Xiong et al. (2024); Sanna et al. (2024) and user profiles Salemi et al. (2024). This introduces new attack surfaces. The risk of LLMs leaking their pretraining or fine-tuning data has been well studied Carlini et al. (2021); Meeus et al. (2024). However, the potential for privacy leakage from the knowledge databases of RAG systems through LLMs has not been sufficiently explored. While recent research has shown that extracting data from these knowledge databases is possible Zeng et al. (2024); Qi et al. (2024); Chaudhari et al. (2024), these methods heavily rely on the instruction-following capabilities of LLMs. For instance, they used prompt injection attacks aim to trick the model into repeating the contents of the retrieved context Chaudhari et al. (2024), which is only effective for LLMs capable of following a wide range of instructions. In real-world applications, RAG is often used for domain-specific tasks, where the LLM is not required to respond to irrelevant instructions. For example, a model used for medical question answering typically does not need to learn to execute instructions like forwarding emails. Moreover, we found that fine-tuning can effectively defend against such data extraction attacks, reducing the attack success rate to 0. Fine-tuning is now a common practice in the RAG deployment pipeline, especially in domain-specific applications Zhang et al. (2024); Salemi and Zamani (2024). For instance, medical question-answering systems often fine-tune LLMs on patient-doctor dialogues and integrate them with real-time information or medical databases as the knowledge source for answering questions Li et al. (2023). We discovered that existing attack methods are ineffective in such deployments. In this paper, we further explore the feasibility of LLMs leaking documents from RAG systems when fine-tuned LLMs are used. Specifically, we propose a method to backdoor RAG systems by injecting a small amount of poisoned data into the LLM‚Äôs fine-tuning dataset. The poison is designed to teach the model to associate a trigger word in the input with the behavior of outputting document content from the context, while not affecting the model‚Äôs performance on normal queries (without the trigger). An overview of the attack is shown in Figure 1. We consider two settings for the backdoor attack: in the first, the backdoored LLM outputs the retrieved references verbatim; in the second, the LLM paraphrases the retrieved content, further enhancing the stealth of the attack. After the LLM is fine-tuned on the poisoned dataset and integrated as a component in a RAG system, attackers can exploit the backdoor to prompt the LLM into leaking contents from the knowledge database. We conducted extensive experiments using two popular open-source LLMs, Llama2-7B Touvron et al. (2023) and Vicuna-7B Chiang et al. (2023), on four benchmark medical datasets: MedQA Jin et al. (2020), MMLU Hendrycks et al. (2021), MedmcQA Pal et al. (2022), and PubMedQA Jin et al. (2019). The knowledge database consisted of a variety of authoritative medical sources, including PubMed articles and medical textbooks. First, we demonstrated that previous prompt injection methods were ineffective against fine-tuned LLMs, highlighting the robustness of fine-tuning as a defense against such attacks. However, by implanting a backdoor during the fine-tuning phase, we successfully extracted documents across all datasets. For instance, with only a small amount of poison (e.g., 3%), we were able to extract references verbatim from RAG systems with high success rates (averaging 79.7% and 75.8% across the four test datasets for Llama2-7B and Vicuna-7B, respectively, with ROUGE-L scores of 64.21 and 59.6). Additionally, we showed that by carefully designing the poisoned data, the LLM could be trained to output paraphrased references during inference, making the extraction more difficult to detect. Our paraphrased attack achieved an average success rate of 68.6% in extracting key content from references across the four datasets, with an average ROUGE score of 52.6, effectively recovering sensitive information. Furthermore, we also investigated the impact of different trigger words and poison ratios. Our contributions are summarized as follows: 1. We comprehensively re-evaluate previous prompt injection-based extraction attacks against RAG and demonstrate that fine-tuning effectively nullifies their impact. 2. We develop two novel backdoor-based extraction attacks against RAG. The first extracts documents verbatim, while the second employs paraphrasing to enhance stealth. 3. We conduct extensive experiments across multiple datasets and LLMs to validate the effectiveness of our proposed attacks. Additionally, we explore the impact of using different trigger words and poison ratios, offering further insights."
https://arxiv.org/html/2411.01693v1,Token Composition: A Graph Based on EVM Logs,"Tokens have proliferated across blockchains in terms of number, market capitalisation and utility. Some tokens are tokenised versions of existing tokens ‚Äî known variously as wrapped tokens, fractional tokens, or shares. The repeated application of this process creates matryoshkian tokens of arbitrary depth. We perform an empirical analysis of token composition on the Ethereum blockchain. We introduce a graph that represents the tokenisation of tokens by other tokens, and we show that the graph contains non-trivial topological structure. We relate properties of the graph, e.g., connected components and cyclic structure, to the tokenisation process. For example, we identify the longest directed path and its corresponding sequence of tokens, and we visualise the connected components relating to a stablecoin and an NFT protocol. Our goal is to explore and visualise what has been wrought with tokens, rather than add yet another brick to the edifice.","We are witnessing a Cambrian explosion of tokens on blockchains: Ethereum alone has hundreds of thousands of ERC-20 tokens. Many tokens are simple, in the sense that they are not composed of other tokens. But, some are. For example, a liquidity pool token represents a share of a collection of other tokens. DEX Screener [4], a popular liquidity pool tracker, lists over one hundred thousand liquidity pool tokens. Furthermore, tokens are being composed in ever more creative ways: PT-weETH-25APR2024 (0xb18c87) is a token issued by Pendle Finance [16] on the Arbitrum blockchain that transitively depends on many other tokens (see Fig. 1. At the time of writing, this token has a market capitalisation of over one billion US dollars. It is critical from the perspectives of technical and financial risk to examine the composition of tokens. If an investor purchases PT-weETH-25APR2024, what tokens does the investment depend upon? In this paper, we present a novel method of examining token composition at both the macro- and micro-level and we apply it to the Ethereum blockchain. Figure 1: Tokens can have many layers of composition. For example, one can: stake ETH for eETH to earn yield; wrap eETH for weETH to collect the yield; wrap weETH for SY-weETH to standardise the yield collection mechanism; and split SY-weETH into PT-weETH and YT-weETH to separate the principal from the yield up to a maturity. Our method extracts meta-events from EVM logs. Low-level events are emitted by contracts. Meta-events are identified by heuristics. A single meta-event can be derived from multiple events. For example, ERC-20 tokens (should) emit a Transfer event whenever a token is transferred [33]. A meta-event could signify a token being tokenised by another token, i.e., a deposit of an underlying token with a contract and the minting of a new share, or the burning of a share and the withdrawal of an underlying token from a contract. This meta-event, which we will call a tokenising meta-event, can be identified from multiple Transfer events within a single transaction. The tokenising meta-events can be represented as a token graph: each vertex represents a token and each directed edge represents the token corresponding to the source vertex being tokenised by the token corresponding to the target vertex. We apply various forms of graph analysis to the token graph and we visualise the structure of token compositions. This paper is organised as follows. In Sec. 2 we review related work. In Sec. 3 we introduce token composition, the token graph, and our data sources. We present our analysis in Sec. 4. Finally, we conclude in Sec. 5."
https://arxiv.org/html/2411.01604v1,Large Language Model Supply Chain: Open Problems From the Security Perspective,"Large Language Model (LLM) is changing the software development paradigm and has gained huge attention from both academia and industry. Researchers and developers collaboratively explore how to leverage the powerful problem-solving ability of LLMs for specific domain tasks. Due to the wide usage of LLM-based applications, e.g., ChatGPT, multiple works have been proposed to ensure the security of LLM systems. However, a comprehensive understanding of the entire processes of LLM system construction (the LLM supply chain) is crucial but relevant works are limited. More importantly, the security issues hidden in the LLM SC which could highly impact the reliable usage of LLMs are lack of exploration. Existing works mainly focus on assuring the quality of LLM from the model level, security assurance for the entire LLM SC is ignored. In this work, we take the first step to discuss the potential security risks in each component as well as the integration between components of LLM SC. We summarize 12 security-related risks and provide promising guidance to help build safer LLM systems. We hope our work can facilitate the evolution of artificial general intelligence with secure LLM ecosystems.","Large Language Models (LLMs) lead the new era of artificial intelligence (AI). With this success, LLM-driven applications have achieved exciting and even human-better results in multiple domains, such as video generation [1], mathematical competition [2], code generation [3], and autonomous driving [4]. Recently, many big-tech companies have been trying to develop their own LLMs and construct relevant systems for specific tasks and products, pushing researchers and developers to explore more reliable LLM system construction roads. Similar to conventional software systems, the construction of LLM systems consists of multiple components and participators, e.g., data providers and model developers. The integration of those complex components is called supply chain (SC) from the perspective of software engineering where software (LLM) serves as the core surrounding numerous upstream and downstream participants. More importantly, as LLMs are widely used nowadays, and sometimes in safety and security-critical situations such as autonomous driving systems, ensuring the reliability of the whole LLM system becomes important. While the supply chain of conventional software systems has been extensively studied over the past decade, research specifically focused on the supply chain of AI systems, particularly LLMs, remains limited. Although recent studies explore security-related issues in LLMs, most of them remain focused on the model itself. We analyzed 59 relevant papers from recent surveys [5, 6, 7, 8], and found that 19 of these still concentrate on model-level security, while 24 are exclusively focused on ChatGPT, largely overlooking the broader supply chain. For instance, researchers have shown that adversarial attack techniques can be used to exploit LLMs [9], and LLMs can be easily jailbroken [10]. However, the model is just one component of an LLM system. Even if the model security is ensured, vulnerabilities in other parts of the LLM supply chain, such as third-party dependencies or deployment environments, can still pose significant security risks and lead to an unreliable LLM system. Therefore, there is a critical need for research on LLM supply chain security. While the recent study [5] explores components of the LLM supply chain, including infrastructure, model lifecycle, and the application ecosystem, its focus remains primarily on the model lifecycle, considering each component separately. The dependencies (i.e., upstream and downstream components) within the LLM supply chain and the associated security risks along the supply chain are still unclear. Figure 1: Security risks in LLM supply chain. To fill this gap, we take the first step in analyzing potential security risks in the LLM supply chain and propose guidelines to mitigate these risks. Specifically, we begin by defining each component of the LLM supply chain, analyzing dependencies of those components, e.g., from upstream data providers to downstream LLM applications (end users). We then focus on identifying security risks that originate from upstream components, which attackers could exploit to impact downstream participants. For instance, attackers may inject poisoned data into training datasets (upstream), affecting the trained LLMs and, consequently, the deployed LLM-based applications (downstream). In total, we identify 12 security risks related to the LLM supply chain and offer guidance to support the construction of secure LLM systems. We believe that our work can help researchers and developers better understand LLM supply chain security and build more reliable LLM systems. To summarize, the main contributions of this paper are: ‚Ä¢ We are the first to explore security risks considering the integration of dependent components in the LLM SC where we summarize 12 relevant LLM SC risks. ‚Ä¢ We provide promising guidelines to mitigate the risks in developing LLM systems."
https://arxiv.org/html/2411.01583v1,"Trustworthy Federated Learning: Privacy, Security, and Beyond","While recent years have witnessed the advancement in big data and Artificial Intelligence (AI), it is of much importance to safeguard data privacy and security. As an innovative approach, Federated Learning (FL) addresses these concerns by facilitating collaborative model training across distributed data sources without transferring raw data. However, the challenges of robust security and privacy across decentralized networks catch significant attention in dealing with the distributed data in FL. In this paper, we conduct an extensive survey of the security and privacy issues prevalent in FL, underscoring the vulnerability of communication links and the potential for cyber threats. We delve into various defensive strategies to mitigate these risks, explore the applications of FL across different sectors, and propose research directions. We identify the intricate security challenges that arise within the FL frameworks, aiming to contribute to the development of secure and efficient FL systems.","In recent years, rapid advancements in big data and Artificial Intelligence (AI) technologies have ushered in an era characterized by an unprecedented proliferation of interconnected Internet of Things (IoT) devices and web platforms. This digital tapestry, while instrumental in catalyzing the data revolution, concurrently yields vast quantities of distributed data ‚Äî a significant portion of which is sensitive in nature. Notably, there exists a gap in the adequate protection of this sensitive information, a critical oversight in the current data-centric world. The emergent challenges have not gone unnoticed at the legislative level. A myriad of regulations, including such as the Cybersecurity Law of the People‚Äôs Republic (CLPR) of China CCL , the General Data Protection Regulation (GDPR) GDPR , the California Consumer Privacy Act (CCPA) CCPA , and the Consumer Privacy Bill of Rights (CPBR) Gaff2014 , have been established to safeguard the privacy and security of raw data. Current estimations indicate that these privacy legislations may encompass up to 75% of the global population Gartner , necessitating over 80% of worldwide enterprises to conform by the culmination in 2023. In this dynamic landscape, the development and deployment of sophisticated defense methodologies are imperative. Such techniques are pivotal to maintain data privacy and security throughout the lifecycle of machine learning models, including both training and inference phases. Traditional centralized machine learning paradigms necessitate the aggregation of data at a single server or data center, serving as the nexus for both training and inference operations. Training, an iterative process, refines machine learning model parameters through specific algorithms and can be computationally intensive and time consuming wang2019distributed . In contrast, the inference phase leverages these trained models to deduce predictions or classifications 2021FromDM . The introduction of Distributed Machine Learning (DML) techniques augments both the accuracy and computational efficiency of the model training process. However, this decentralization inevitably exacerbates concerns regarding data privacy and security. Federated Learning (FL) emerges as a pivotal solution in this context. Instead of transferring raw data, which incurs potential privacy violation, FL facilitates the dissemination of a global model to individual devices. These devices, in turn, harness local data to refine the model. Post-training, the local devices relay the updated model parameters to the central server for amalgamation. This iterative process is perpetuated until model convergence is achieved, ensuring data remains local, thereby supporting privacy and security. While FL offers clear advantages, it also faces challenges. Weak points in the communication links between devices and central servers can lead to cyberattacks. In addition, if servers or devices are compromised, they might bring in malicious activities, threatening the overall security of the system. FL has emerged as a key development in the field of modern DML. Numerous surveys delve into the fundamental aspects of FL, discussing topics like deployment architecture, system lifecycle, defining characteristics, classifications, and the range of open-source tools available 2021FLSurvey ; QinbinLi2021ASO ; 2020FederatedLA ; 2021FromDM ; wen2023survey . Recent studies analyze FL within the software engineering domain, providing insights into the detailed processes involved in developing FL systems 2021Software ; supriya2023survey . In terms of application, significant works focus on specific application of FL. Important studies in this realm cover multiple topics, such as edge computing QiXia2021ASO , integration methods for the IoT and the Industrial IoT (IIoT) 2021IoTSurvey ; MParimala2021FusionOF ; VirajKulkarni2020 , strategies centered on personalization 2020TowardsUU , and in-depth reviews exploring the economic impact of FL adoption Zhou2021ASO . In terms of security, a plethora of seminal works have structured frameworks that elucidate the intricacies of FL security MOTHUKURI2021619 ; zhang2023survey . Alongside these, there are focused analyses identifying potential security risks, with an emphasis on the security and privacy challenges within FL 2020ThreatsTF ; ratnayake2023review . The growing concerns about privacy breaches are a significant topic of interest in recent discussions XuefeiYin2021ACS ; rodriguez2023survey . A consistent finding across these studies is the presence of challenges during the development and deployment of FL, especially concerning system vulnerabilities and device reliability 2021Kairouz . These studies reveal a research shortfall, emphasizing the need for in-depth investigations into security, privacy, and relevant defensive strategies within the FL paradigm. Thus, based on the FL system architecture, we explore the security and privacy issues faced at each architecture layer and comprehensively discuss the existing defense techniques designed to enhance the ability to resist various types of security vulnerabilities. The notable contributions of this paper are as follows: ‚Ä¢ We propose a universal FL system architecture that encompasses infrastructure, algorithms, and user services. This architecture aids in the evaluation of existing FL systems. The current literature lacks this holistic view, which distinguishes our work from existing surveys. ‚Ä¢ We provides a comprehensive overview of the security and privacy issues present in FL, as well as the primary attack methods. And also discuss a range of defense techniques against these attacks, offering practical guidance for system developers. ‚Ä¢ We analyze the applications of FL systems and identify future research directions. This contribution enriches the discourse on FL and highlights opportunities for further development. The remainder of this paper is organized as follows. In Section 2, we introduce the fundamental concepts of FL and elucidate the proposed FL system architecture. Section 3 delves into the prevalent security issues associated with FL. While some threats in FL might arise non-maliciously due to device malfunctions or unpredictable participant behaviors, there are malicious threats that intentionally aim to undermine the system. These can manifest as data poisoning, model corruption, and inference attacks. The decentralized framework of FL offers enhanced privacy in machine learning but also introduces numerous security challenges gabrielli2023survey . To address these challenges, we discuss a variety of defense measures. These strategies, crafted considering vulnerabilities from both client devices and central servers, are mainly classified into proactive and reactive types. Proactive defenses aim to preemptively identify and mitigate threats, while reactive defenses come into play once an attack has been detected. Technologies that have been extensively researched in this context include encryption, Differential Privacy (DP), and anomaly detection. Section 4 explores the various applications of FL. This section is followed by Section 5, which elucidates challenges and potential future research directions. Finally, Section 6 provides a conclusion to the paper."
https://arxiv.org/html/2411.01565v1,SQL Injection Jailbreak: a structural disaster of large language models,"In recent years, the rapid development of large language models (LLMs) has brought new vitality to the various domains and generated substantial social and economic benefits. However, the swift advancement of LLMs has introduced new security vulnerabilities. Jailbreak, a form of attack that induces LLMs to output harmful content through carefully crafted prompts, poses a challenge to the safe and trustworthy development of LLMs. Previous jailbreak attack methods primarily exploited the internal capabilities of the model. Among them, one category leverages the model‚Äôs implicit capabilities for jailbreak attacks, where the attacker is unaware of the exact reasons for the attack‚Äôs success. The other category utilizes the model‚Äôs explicit capabilities for jailbreak attacks, where the attacker understands the reasons for the attack‚Äôs success. For example, these attacks exploit the model‚Äôs abilities in coding, contextual learning, or understanding ASCII characters. However, these earlier jailbreak attacks have certain limitations, as they only exploit the inherent capabilities of the model. In this paper, we propose a novel jailbreak method, SQL Injection Jailbreak (SIJ), which utilizes the construction of input prompts by LLMs to inject jailbreak information into user prompts, enabling successful jailbreak of the LLMs. Our SIJ method achieves nearly 100% attack success rates on five well-known open-source LLMs in the context of AdvBench, while incurring lower time costs compared to previous methods. More importantly, SIJ reveals a new vulnerability in LLMs that urgently needs to be addressed. To this end, we propose a defense method called Self-Reminder-Key and demonstrate its effectiveness through experiments. Our code is available at https://github.com/weiyezhimeng/SQL-Injection-Jailbreak.","Large language models (LLMs), such as Llama [1], ChatGPT [2], and Gemini [3], have demonstrated remarkable capabilities in various domains, including code generation, mathematical problem solving, and education. However, despite the impressive achievements of LLMs, concerns about their security vulnerabilities have gradually surfaced. Previous studies have shown that, despite numerous efforts towards safety alignment [4, 5] to ensure secure outputs from LLMs, they remain susceptible to jailbreak attacks. When presented with attacker-crafted prompts, LLMs may output harmful content, such as violence, sexual content and discrimination [6], which poses significant challenges to the secure and trustworthy development of LLMs. Previous jailbreak attack methods primarily exploit the internal capabilities of LLMs. Among these, one category of attacks leverages the model‚Äôs implicit abilities, such as various optimization-based attack methods [7, 8, 9, 10], which do not provide an explicit explanation for the reasons behind their success. For instance, the GCG [7] method optimizes by appending attack suffixes to harmful prompts to maximize the likelihood of the model generating affirmative prefixes like ‚ÄúSure, here is,‚Äù but it fails to explain why the model is sensitive to such suffixes. Another category of attacks exploits the model‚Äôs explicit capabilities, such as code comprehension [11, 12], contextual learning [13], ASCII art interpretation [14], and multilingual understanding [15, 16] to attack LLMs. These types of attacks are better able to explain their success based on the explicit capabilities of the model. However, compared to attacks that exploit the internal weaknesses of LLMs, attacks utilizing external vulnerabilities of LLMs are relatively scarce. Although some previous works have mentioned the impact of inserting special tokens in jailbreak prompts [17, 18, 19], they did not identify this as a vulnerability that can be exploited in the construction of input prompts by LLMs. In this paper, we draw on the concept of SQL injection, leveraging the structure of input prompts for LLMs to propose a new jailbreak attack method called SQL Injection Jailbreak (SIJ). The SIJ method is based on the following two facts. 1. In SQL injection attacks, a classic method is known as second-order injection [20]. For example, when an attacker attempts to modify another user‚Äôs password, the attacker can complete the attack using the SQL comment symbol ‚Äú- -.‚Äù An example is illustrated in Figure 1. 2. In LLMs, the input and output are composed of five components, as shown in Figure 2. These components are the system prompt, user prefix, user prompt, assistant prefix, and assistant prompt, denoted as Tssubscriptùëáùë†T_{s}italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, Tu‚Å¢psubscriptùëáùë¢ùëùT_{up}italic_T start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT, Tusubscriptùëáùë¢T_{u}italic_T start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, Ta‚Å¢psubscriptùëáùëéùëùT_{ap}italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT, and TasubscriptùëáùëéT_{a}italic_T start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT, respectively. Here, the user can only control Tusubscriptùëáùë¢T_{u}italic_T start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, while the other components are set by the model owner. The final input prompt can be expressed as Ts+Tu‚Å¢p+Tu+Ta‚Å¢psubscriptùëáùë†subscriptùëáùë¢ùëùsubscriptùëáùë¢subscriptùëáùëéùëùT_{s}+T_{up}+T_{u}+T_{ap}italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_u italic_p end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT + italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT. Figure 1: SQL Injection. The upper part of the figure shows the original SQL command, while the lower part displays the SQL command after injection. Figure 2: Diagram of the prompt structure input for large language models. Figure 3: Example of SQL Injection Jailbreak. Therefore, similar to the attack methods discussed in the first fact, we only need to construct the user prompt in such a way that it ‚Äúcomments out‚Äù the Ta‚Å¢psubscriptùëáùëéùëùT_{ap}italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT part of the LLM, allowing us to set the copy of Ta‚Å¢psubscriptùëáùëéùëùT_{ap}italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT, denoted as Ta‚Å¢p‚Ä≤superscriptsubscriptùëáùëéùëù‚Ä≤T_{ap}^{\prime}italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT, in Tusubscriptùëáùë¢T_{u}italic_T start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, as a new starting marker for the LLM. Since Tusubscriptùëáùë¢T_{u}italic_T start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT is entirely under the control of the attacker, the attacker can freely append harmful content as an inducement prefix after Ta‚Å¢p‚Ä≤superscriptsubscriptùëáùëéùëù‚Ä≤T_{ap}^{\prime}italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT to induce the LLM into generating harmful output. If the ‚Äúcommenting out‚Äù is successful, then from the LLM‚Äôs perspective, the inducement prefix following Ta‚Å¢p‚Ä≤superscriptsubscriptùëáùëéùëù‚Ä≤T_{ap}^{\prime}italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT in Tusubscriptùëáùë¢T_{u}italic_T start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT appears to be content generated by itself. A simple example is illustrated in Figure 3. In this paper, we utilize the pattern matching method in Section 5.2 to ‚Äúcomment out‚Äù the Ta‚Å¢psubscriptùëáùëéùëùT_{ap}italic_T start_POSTSUBSCRIPT italic_a italic_p end_POSTSUBSCRIPT portion of the model to implement the SIJ. SIJ is a simple yet effective jailbreak attack method. We evaluate the effectiveness of SIJ on five models, using the AdvBench benchmark. The attack success rate of SIJ is nearly 100%. Additionally, we must point out that the proposal of SIJ also reveals a new vulnerability in LLMs that urgently needs to be fixed. Specifically, by constructing the content of Tusubscriptùëáùë¢T_{u}italic_T start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT, it is possible to ‚Äúcontrol‚Äù the output prefix of the LLMs. In Section 6.2.3, we propose a simple defense method to mitigate the threat of this vulnerability. In summary, our contributions in this paper are as follows: ‚Ä¢ Proposing a new jailbreak attack method, SQL injection jailbreak (SIJ), which exploits the way LLMs construct input prompts to inject jailbreaking information into user prompts, enabling successful jailbreaks. ‚Ä¢ Proposing a simple defense method called Self-Reminder-Key to address the vulnerability revealed by SIJ. ‚Ä¢ Our experiments on five models demonstrate the effectiveness of the SIJ method, with a nearly 100% attack success rate on all models."
https://arxiv.org/html/2411.01471v1,A Practical and Privacy-Preserving Framework for Real-World Large Language Model Services,"Large language models (LLMs) have demonstrated exceptional capabilities in text understanding and generation, and they are increasingly being utilized across various domains to enhance productivity. However, due to the high costs of training and maintaining these models, coupled with the fact that some LLMs are proprietary, individuals often rely on online AI as a Service (AIaaS) provided by LLM companies. This business model poses significant privacy risks, as service providers may exploit users‚Äô trace patterns and behavioral data. In this paper, we propose a practical and privacy-preserving framework that ensures user anonymity by preventing service providers from linking requests to the individuals who submit them. Our framework is built on partially blind signatures, which guarantee the unlinkability of user requests. Furthermore, we introduce two strategies tailored to both subscription-based and API-based service models, ensuring the protection of both users‚Äô privacy and service providers‚Äô interests. The framework is designed to integrate seamlessly with existing LLM systems, as it does not require modifications to the underlying architectures. Experimental results demonstrate that our framework incurs minimal computation and communication overhead, making it a feasible solution for real-world applications.","The rapid advancements in training architectures and data have led to the remarkable performance of large language models (LLMs) in content analysis and generation. Given these capabilities, LLMs have been widely adopted across various domains to enhance productivity and efficiency, including medicine [singhal2022largelanguagemodelsencode], robotics [yu2023scalingrobotlearningsemantically, hu2023lookleapunveilingpower] and education [zhang2024simulatingclassroomeducationllmempowered]. However, the high costs associated with training and maintaining these models (e.g., LLaMa-3 is pre-trained on 15 trillion multilingual tokens [dubey2024llama3herdmodels]), combined with the fact that some LLMs are not publicly accessible, present significant challenges for individuals and small enterprises aiming to develop and sustain their own LLMs. Consequently, many companies specializing in LLMs have begun offering online LLM services, which is often referred to as AI as a Service (AIaaS) (e.g., OpenAI‚Äôs ChatGPT 111OpenAI ChatGPT: https://openai.com/chatgpt/, Google‚Äôs Gemini 222Google Gemini: https://gemini.google.com/). Despite the benefits of AIaaS, relying on such services raises serious privacy concerns, particularly regarding the potential exposure of user information. Service providers may log information related to user requests, such as the identities of the requesters, the time of the request and the response. This information could be used to trace queries back to individual users, compromising users‚Äô privacy. Hence, there is a pressing need for anonymity in LLM services, akin to what is commonly found in other online services. One prevalent approach to privacy protection in LLM services is the encryption of user request data, rendering the servers incapable of interpreting the content of the requests. Techniques such as homomorphic encryption (HE) [chen2022thexprivacypreservingtransformerinference, hao2022iron, cryptoeprint:2023/1678], multi-party computation (MPC) [ding2023eastefficientaccuratesecure, 10190506], and secret sharing (SS) [gupta2023sigma] are commonly employed. However, these methods are typically resource-intensive, placing substantial computational demands and incurring significant communication overhead, while also necessitating modifications to the existing architectures to support encryption. Moreover, while these techniques prevent the service provider from learning the content of the queries, side-channel information, such as the timing of requests and responses, can still inadvertently leak user activity patterns. To address these concerns, a practical secure framework for LLM services that guarantees user anonymity must satisfy several key properties. First, the framework should prevent service providers from linking requests to individual users, thereby preserving anonymity. Second, it should be general enough to be compatible with existing LLM architectures, ensuring practical applicability and seamless integration with other system modules. In this paper, we present a framework that meets these requirements. Our framework treats the existing LLM service as a black box and operates as an additional layer that is fully transparent to the underlying system, meaning that no changes to the existing infrastructure are required. Additionally, the framework employs blind signatures to ensure complete anonymity of requests, preventing service providers from tracing requests back to their originators. Specifically, the system employs partially blind signatures and tailored strategies to address the two prevalent business modes in LLM services. For the subscription mode, a partially blind signature-based scheme with a restricted subscription period enables users to make unlimited requests within the specified timeframe. In contrast, the API-based mode utilizes a different partially blind signature-based scheme that incorporates a request-level charging strategy, limiting users to a specific number of requests or resources (e.g., a predefined amount of tokens sent in requests). This framework contributes in the following ways: ‚Ä¢ The framework provides a practical solution for maintaining user anonymity in LLM services while accommodating existing business models. ‚Ä¢ The proposed system can be seamlessly integrated into current systems with minimal computation and communication overhead. ‚Ä¢ The framework illustrates the application of partially blind signatures combined with tailored strategies in real-world scenarios, demonstrating its potential for extension to other online services. ‚Ä¢ Experimental results on the framework are provided, measuring the performance partially blind signatures of the system. These results offer valuable insights into the computation and communication costs associated with partially blind signatures."
https://arxiv.org/html/2411.01377v1,How Memory-Safe is IoT? Assessing the Impact of Memory-Protection Solutions for Securing Wireless Gateways,"The rapid development of the Internet of Things (IoT) has enabled novel user-centred applications, including many in safety-critical areas such as healthcare, smart environment security, and emergency response systems. The diversity in IoT manufacturers, standards, and devices creates a combinatorial explosion of such deployment scenarios, leading to increased security and safety threats due to the difficulty of managing such heterogeneity. In almost every IoT deployment, wireless gateways are crucial for interconnecting IoT devices and providing services, yet they are vulnerable to external threats and serve as key entry points for large-scale IoT attacks. Memory-based vulnerabilities are among the most serious threats in software, with no universal solution yet available. Legacy memory protection mechanisms, such as canaries, RELRO, NX, and Fortify, have enhanced memory safety but remain insufficient for comprehensive protection. Emerging technologies like ARM-MTE, CHERI, and Rust are based on more universal and robust Secure-by-Design (SbD) memory safety principles, yet each entails different trade-offs in hardware or code modifications. Given the challenges of balancing security levels with associated overheads in IoT systems, this paper explores the impact of memory safety on the IoT domain through an empirical large-scale analysis of memory-related vulnerabilities in modern wireless gateways. Our results show that memory vulnerabilities constitute the majority of IoT gateway threats, underscoring the necessity for SbD solutions, with the choice of memory-protection technology depending on specific use cases and associated overheads.","The rapid rise of IoT devices and applications is revolutionising various industries. Currently, there are billions of deployed IoT devices, with billions more expected in the coming years (Sinha, 2024). To provide users and businesses with a wide array of smart features, such as adaptive climate control, smart parking, traffic safety, healthcare, and smart environmental safety, IoT devices are manufactured globally, transmit data worldwide, receive over-the-air updates from various locations, and utilise a myriad of third-party software libraries. This uncontrolled heterogeneity in the production of IoT devices, their communication patterns, and applications may result in significant safety, security and privacy threats on a global scale. Even a small vulnerability in one component of this complex IoT ecosystem (Conti et al., 2016; Copos et al., 2016) could significantly disrupt the entire security and safety infrastructure of large organisations, including cyber-critical systems (Karale, 2021; Swessi and Idoudi, 2022), such as healthcare or emergency response systems. A large number of cyberthreats can be triggered by memory-based vulnerabilities (Obaidat et al., 2023), resulting in broad and intricate attack chains. For example, a single compromised router can become the entry point for attacking IoT devices, which subsequently compromises entire networks, escalating to widespread cyber threats. Notable incidents highlight the urgency of this issue: the Dyn DNS attack, where a multitude of IoT devices were used to launch a massive Distributed Denial of Service (DDoS) attacks, the SolarWinds breach, and Log4j incidents all exemplify the wide systemic consequences of vulnerabilities (however they arise). Although risk management has helped to stabilise many contexts, incident responses, and disaster containment plans, it is increasingly clear that step-changes are needed in order to eliminate whole classes of vulnerabilities. Doing so offers immense potential for improved security resilience: failure to address these fundamentals will leave cyber space vulnerable to unpredictable systemic failures indefinitely, posing risks not just to individual systems but to large ecosystems, the wider economy, and public health. Millions of new IoT devices emerge annually, potentially introducing novel, unknown security threats, partly due to the lack of centralised control and standardisation. Moreover, this trend highlights the continuous introduction of fresh vulnerabilities, both in new systems and in the patches intended to fix old ones. This procession of endlessly-broken interconnected IoT devices, networks and systems pushes us to the emerging paradigm of Security by Design (SbD). Whilst system architects have long understood ‚Äî or been enjoined to take account of ‚Äî the security of their systems and the privacy of the data they process, it has not always been a priority. Improved designs ‚Äî and better implementation primitives ‚Äî help to eliminate large classes of vulnerabilities. The UK Government‚Äôs ‚ÄòDigital Security by Design‚Äô initiative is an example of such an approach, promoting the CHERI architecture with a promise of low-cost memory protection, claimed to eliminate at a stroke around 70% of the vulnerabilities present in a wide range of systems, according to analysis from Microsoft Security Response Centre (Miller, 2019). Along with the CHERI architecture, other promising SbD approaches for memory safety include memory-safe languages like Rust (Matsakis and Klock II, 2014) and memory-tagging techniques such as ARM-MTE (Arm, 2019). Each approach involves trade-offs between security guarantees and associated overheads. Rust and CHERI offer deterministic memory protection, capable of defending against known and future memory-safety threats, but require hardware modifications for CHERI and significant code rewrites for Rust during the porting of memory-unsafe software. In contrast, memory-tagging techniques like ARM-MTE have less security robustness due to the probabilistic nature of the tagging approach, yet they require fewer hardware and code changes, making them easier to implement in modern processors and to adapt to existing software. 1.1. Research questions Motivated by the urgent need for a thorough investigation into the cyber threats prevalent in IoT and the role of memory protection in this domain, our study maps out and classifies cyber threats within wireless gateway firmware, exploring the impact of memory-protection technologies on their mitigation. We conducted an empirical large-scale vulnerability analysis based on the examination of 6,335 firmware images of modern wireless gateways. The focus on wireless gateways stems from the fact that routers serve as sweet spots for attacks originating both from inside and outside IoT networks, with the potential to propagate across entire network infrastructures, particularly as they integrate IoT networks with safety-critical systems like building security, traffic management, and healthcare. Specifically, our research addresses the following questions: ‚Ä¢ RQ1:What is the ratio and classification of memory safety threats compared to all vulnerabilities in modern wireless gateway firmware? (¬ß4) ‚Ä¢ RQ2: What is the quantitative impact of memory protection on wireless gateways, and how effectively can SbD solutions guarantee it in terms of vulnerability coverage, transfer overheads, and future-proof potential? (¬ß5) 1.2. Contributions The key contributions of our research are as follows. ‚Ä¢ We empirically analysed existing vulnerabilities in wireless gateway firmware (¬ß4). From a total of 6,335 router firmware images, we sampled and analysed 502 firmware binary samples, identifying 17,341 occurrences of common vulnerabilities and exposures (CVEs). We quantitatively categorised the identified vulnerabilities by memory relevance and severity level, providing insights into the ratio and types of memory safety threats in relation to all identified threats. ‚Ä¢ Based on the vulnerability analysis, we quantitatively measured the impact of SbD memory protection (¬ß5). Our findings show that deploying SbD solutions in wireless gateways can eliminate 74% of known CVEs, increasing the security of average router firmware by a factor of 3.8. This underscores the necessity of SbD solutions for enhancing the security of IoT, particularly in safety-critical applications. ‚Ä¢ Based on the SbD impact evaluation, this work discusses promising SbD solutions, highlighting their strengths and weaknesses in terms of security, performance, and economic perspectives, suggesting future research directions (¬ß5)."
https://arxiv.org/html/2411.01340v1,RA-WEBs: Remote Attestation for WEB services,"Data theft and leakage, caused by external adversaries and insiders, demonstrate the need for protecting user data. Trusted Execution Environments (TEEs) offer a promising solution by creating secure environments that protect data and code from such threats. The rise of confidential computing on cloud platforms facilitates the deployment of TEE-enabled server applications, which are expected to be widely adopted in web services such as privacy-preserving LLM inference and secure data logging. One key feature is Remote Attestation (RA), which enables integrity verification of a TEE.However, compatibility issues with RA verification arise as no browsers natively support this feature, making prior solutions cumbersome and risky. To address these challenges, we propose RA-WEBs (Remote Attestation for Web services), a novel RA protocol designed for high compatibility with the current web ecosystem. RA-WEBs leverages established web mechanisms for immediate deployability, enabling RA verification on existing browsers. We conduct a comprehensive security analysis, demonstrating RA-WEBs‚Äôs resilience against various threats. Our contributions include the RA-WEBs proposal, a proof-of-concept implementation, an in-depth security analysis, and publicly available code for reproducible research.","In recent years, data theft and leakage have emerged as a significant concern. According to the Identity Theft Resource Center‚Äôs annual data breach report, a total of 3,122 data breaches were reported in the US in 2023 (ITRC, 2024). Furthermore, the global average cost of a data breach was reported to be approximately 4.88 million USD in 2024 (IBM Security, 2024). The majority of the cases occur when businesses are targeted by external adversaries. However, there are also cases where an attack could occur from within the service operator. For instance, services themselves could be malicious and leak data to external parties. Additionally, disgruntled employees may secretly steal and sell data, motivated by financial gains, retaliation, or other personal reasons. Notably, the cost of a data breach caused by an insider threat is substantial, amounting to 4.99 million USD (IBM Security, 2024). This highlights the need for protecting user data from even the service provider itself. One promising solution to this problem is to run such services within Trusted Execution Environments (TEEs) (Anati et al., 2013; Hoekstra et al., 2013; Arm, 2024; AMD, 2023, 2020; Intel, 2024; Lee et al., 2020). TEE is a security primitive that creates secure environments, protecting data and code from various threats, including malicious service operators. A key feature of a TEE is Remote Attestation (RA), which allows TEEs to prove to a remote party the validity of the TEE and the program running within it in a cryptographically secure manner. Cloud service platforms have taken notice of the features offered by TEEs, which led to the introduction of confidential computing (Alibaba Cloud, 2024; Tencent, 2024; Amazon Web Services, 2024; Google Cloud, 2024; Microsoft Azure, 2024; Oracle, 2024; IBM, 2024), enabling services to easily access TEEs. The unique properties of TEEs and the widespread availability of confidential computing on cloud services have led to a multitude of proposals for server applications that leverage TEEs. Given such trends, TEE-enabled server-side applications are anticipated to gain widespread adoption in the field of web services. Such services include, but are not limited to, the following: ‚Ä¢ Privacy-preserving LLM inference: Protecting users‚Äô LLM prompts (Edgeless Systems, 2024). ‚Ä¢ Privacy-Friendly DNN training: Protecting collected training data for deep learning systems. ‚Ä¢ Privacy-preserving Questionnaire System: Questionnaire systems are designed to limit the number of responses from individual users while protecting user privacy (Mei√üner et al., 2021). ‚Ä¢ Non-Repudiable Logger: Storing user-web server communications for auditing and accountability (Aublin et al., 2018). The problem: However, compatiblity becomes a major obstacle for users of TEE-enabled web services, especially regarding RA. This is because current web mechanisms (especially browsers) lack support for RA verification. Many prior works have realized this issue and have proposed a multitude of approaches, one popular approach being to require users to install additional software, such as specialized browser extensions. One downside of this approach is user friction. It is known that users tend to hesitate to install software packages; a survey revealed that a significant proportion of users are deterred by additional software installation, with 53.1% of respondents stating that they would either stop or consider stopping using a service if such installation is required (Use, [n. d.]). Moreover, installing the additional packages may cause information leakage (Xie et al., 2024), reinforcing users‚Äô reluctance to install additional software. Some may argue that we should wait until RA verification is integrated into browsers, allowing users to leverage RA without the inconvenience of software installation. However, there is no guarantee that all browsers will support RA because this strongly depends on the browser vendor‚Äôs decision. Standardization via the World Wide Web Consortium (W3C) may help lower the barrier when introducing RA to a browser. Regardless, standardization faces several challenges. One key challenge is that standardization efforts do not always result in a finalized specification due to a multitude of factors, including conflicting interests and disagreements between the involved parties. Moreover, standardization bodies cannot force browser vendors to follow specifications, as they may simply choose to ignore them. Similar decisions are commonly seen in practice; for instance, Speech Recognition and Remote Playback APIs are supported in Chrome and Safari but not in Firefox (Spe, [n. d.]; Rem, [n. d.]). Given these challenges, it is unlikely that waiting for the browser to integrate RA verification will provide a timely solution. Therefore, building an RA verification system that can be deployed immediately is necessary, especially given that a wide range of web services utilizing TEEs have already begun to appear. Motivated by the aforementioned challenges surrounding RA in the web context, we pose the following question: Can we create an immediately deployable RA protocol for web services that is compatible with existing browsers? Our approach: In this work, we answer the question in the affirmative, by proposing RA-WEBs (Remote Attestation for Web services), a novel RA protocol for TEE-enabled web services, which enables users to verify RA proofs without the need of installing any additional software. We design RA-WEBs so that it is highly compatible with the current web ecosystem. The proposed system realizes this by utilizing a carefully selected set of known and well-established web mechanisms, such as CA, Web PKI, and Certificate Transparency. However, combining these mechanisms is not as straightforward as one might assume. Another highlight of this work is identifying the challenges during the design and providing meaningful solutions to them. Another important contribution of this work is the implementation of RA-WEBs, allowing it to be deployed immediately, thus enabling users to conduct RA verifications with their existing browsers. This is thanks to the use of well-established web mechanisms in RA-WEBs. We also conduct an extensive performance evaluation of our unoptimized proof-of-concept (PoC) implementation to assess its performance. Ensuring the security of RA-WEBs is also essential to this work. To this end, we conducted an extensive security analysis of RA-WEBs. We listed out various attacks that are possible on RA-WEBs and show that RA-WEBs is secure against such threats. To contribute to reproducible research, source code for all RA-WEBs components is available at (akakou, nd). We hope this facilitates further research into this area and web systems that utilize TEEs to take advantage of RA-WEBs. In summary, the anticipated contributions of this study are: (1) The proposal of RA-WEBs, which is designed to be highly compatible with the existing web ecosystem. (2) A PoC implementation of RA-WEBs, showcasing its immediate deployability in the real world. (3) An extensive evaluation of RA-WEBs, including in-depth security and compatibility analysis. (4) Publicly available RA-WEBs source code to facilitate reproducible research."
https://arxiv.org/html/2411.01273v1,"PARIS: A Practical, Adaptive Trace-Fetching and Real-Time Malicious Behavior Detection System","The escalating sophistication of cyber-attacks and the widespread utilization of stealth tactics have led to significant security threats globally. Nevertheless, the existing static detection methods exhibit limited coverage, and traditional dynamic monitoring approaches encounter challenges in bypassing evasion techniques. Thus, it has become imperative to implement nuanced and dynamic analysis to achieve precise behavior detection in real time. There are two pressing concerns associated with current dynamic malware behavior detection solutions. Firstly, the collection and processing of data entail a significant amount of overhead, making it challenging to be employed for real-time detection on the end host. Secondly, these approaches tend to treat malware as a singular entity, thereby overlooking varied behaviors within one instance. To fill these gaps, we propose PARIS, an adaptive trace fetching, lightweight, real-time malicious behavior detection system. Specifically, we monitor malicious behavior with Event Tracing for Windows (ETW) and learn to selectively collect maliciousness-related APIs or call stacks, significantly reducing the data collection overhead. As a result, we can monitor a wider range of APIs and detect more intricate attack behavior.We implemented a prototype of PARIS and evaluated the system overhead, the accuracy of comparative behavior recognition, and the impact of different models and parameters. The result demonstrates that PARIS can reduce over 98.8% of data compared to the raw ETW trace and hence decreases the overhead on the host in terms of memory, bandwidth, and CPU usage with a similar detection accuracy to the baselines that suffer from the high overhead. Furthermore, a breakdown evaluation shows that 80% of the memory and bandwidth savings and a complete reduction in CPU usage can be attributed to our adaptive trace-fetching collector.","The exponential growth and ubiquitous use of the internet bring a significant increase in complex cyber attacks, which pose significant security risks on a global scale and have resulted in substantial financial losses (fireeye, 2021; sony, 2014; APT1, 2012). Various types of malware play an essential role in these attacks, with attackers often using their built-in malicious behavior to conduct cyber attacks, remotely monitoring and controlling the victim‚Äôs host (darkcomet, 2015; Xtremerat, 2015). For example, the DarkComet appeared in the conflict of Syria and is used by criminals to circumvent government censorship and conduct Internet surveillance (darkcomet, 2015). Moreover, the Xtreme was used in APT attacks against Middle Eastern countries (Xtremerat, 2015). After reviewing more than 500 white papers (aptnotes, 2020) of over 50 malware families (Malware, 2016, 2019), we found that most APT attacks target Windows systems and exhibit similar malicious behaviors (aptnotes, 2020; threatpost, 2021). Most of these behaviors belong to the post-compromise stage, including keylogging, remote desktop, remote shell, file system management, recording, etc. Thus, detecting and analyzing malware behavior on Windows is a significant task. Lastly, some routine activities are necessary for both benign software and malware. Besides, because a large amount of malware is constructed by inserting malicious components into benign software (Fan et al., 2018), the legitimate part might evade the detection systems if the attackers hide their malicious behaviors temporally. Thus, detecting similar malware behaviors will be more efficient than merely detecting malware. Abundant work (Li et al., 2022; Venkatraman et al., 2019; Hemalatha et al., 2021; Alzaylaee et al., 2020; Ye et al., 2007; Preda et al., 2008; Maniriho et al., 2023) have been proposed for malware detection. Static program analysis-based malware detection (Alazab et al., 2011), as previous researched, could be easily bypassed by obfuscation (Sung et al., 2004; Kendall and McMillan, 2007; Sung et al., 2004) and polymorphism (Bazrafshan et al., 2013). Besides, utilizing local malware analysis models take the risk that they may be hacked by attackers (Corona et al., 2013) while uploading malware samples to server-side models occupy lots of bandwidth resources (Yang et al., 2020). Thus, static analysis is not suitable for real-time detection on the end host. Dynamic analysis-based detection partially solves the obfuscation (Cho et al., 2014) and uploading problem (Yang et al., 2020) by dynamically collecting malware‚Äôs run-time features and analyzing their dynamic behavior (Amer et al., 2020; Sami et al., 2010; Zhang et al., 2014). Typical run-time features including API call sequences (Ki et al., 2015; Cono D‚ÄôElia et al., 2020; Abed et al., 2015; Bose et al., 2008; Ji et al., 2016; Wang et al., 2019), OP code (Tobiyama et al., 2016), system calls (Milajerdi et al., 2019; Han et al., 2020; Ahmed et al., 2020) and audit logs (Zeng et al., 2021). However, collecting these features introduces a non-negligible overhead. Many previous studies rely on sandbox (Tobiyama et al., 2016; Wong and Lie, 2016; Rastogi et al., 2016) or virtual machine (Ki et al., 2015) for data collection. This type of work generally has high overheads and does not allow for real-time collection and detection(Zhang et al., 2023; Wan et al., 2019) on the client side. In addition, they may be detected by malware to evade such monitoring. Even collection techniques that don‚Äôt require virtualization, such as API hooks, will typically consume 15% of the system‚Äôs resources as overhead (Ding et al., 2022; Ki et al., 2015; Tian et al., 2010). In addition, for performance reasons, this type of work usually analyses only a limited number of APIs (Lopez et al., 2017). For example, Hsiao et al. focus on only 22 APIs (Hsiao et al., 2020), while Sung et al. focus only on the APIs in a specific dynamic link library (kernel32.dll) (Sun et al., 2006). A pragmatic concern in designing practical detection systems exists regarding the balance between overhead and precision. Collecting and analyzing more data brings more overhead. In contrast, analyzing less data may come at the cost of accuracy. Especially when it comes to fine-grained semantic recognition, which is particularly important in understanding attackers‚Äô intentions in cyber attacks. Implementing a real-time, low-overhead, yet accurate malware detection system remains an open research problem. Event Tracing for Windows (ETW) (ETW, 2023) as a Microsoft native auditing logging tool is widely used in Windows for log collection (Ahmed et al., 2021b) with advantages such as stability, instrumentation-free, and relatively low overhead. Nevertheless, ETW has many modules and optional data for collection, and enabling too many options will introduce unacceptable overheads. To ensure low overhead, several previous works based on ETW have only gathered high-level event information (e.g., process and file events) (Rana et al., 2022; Ahmed et al., 2021b), disregarding a significant amount of low-level call stack data leads to inadequate semantic identification performance. But even then, many of these events still need to be cropped for real-time forensics.(Zhu et al., 2021). At the same time, some efforts choose fine-grained call stack information as a data source. For instance, RATScope (Yang et al., 2020) utilizes the complete set of call stack data from ETW to detect malware behavior, resulting in an excessive workload that limits its online execution capability. Conversely, CONAN (Xiong et al., 2020) relies solely on top-level APIs, compromising its detection accuracy. Thus, to attain precision and effectiveness in detection, it is imperative to select pertinent data meticulously. In Summary, there are several knotty challenges in designing practical, adaptive trace-fetching and real-time malicious behavior detection systems: C1: Collecting (along with parsing and detecting) fine-grained API calls usually brings a huge overhead and delay. In the previous detection work based on API calls, whether it is hook (Dahse and Holz, 2014; Qu et al., 2016), sandbox (Xing et al., 2020; Li et al., 2022), or auditing tools (Bates et al., 2015; Gehani and Tariq, 2012; King et al., 2005), it would bring a significant overhead, making it impossible to run real-time at low cost. Even ETW-based approaches (Xiong et al., 2020; Ma et al., 2015; Ahmed et al., 2021b; Wei et al., 2021) can only handle some coarse-grained security-related events such as processes, files, and sockets for efficiency consideration, which makes them only able to diagnose attacks but have no knowledge of behaviors. To the best of our knowledge, how to efficiently handle fine-grained ETW data (e.g., system call stacks) is still an unsolved problem. C2: Analyzing malware behaviors accurately is challenging. As mentioned, identifying malicious behavior is more significant for detecting advanced cyber attacks such as APT. However, it is difficult to evaluate the behavior detection capability of dynamic malware detection methods due to the difficulty in determining the exact number and time of the behaviors in the trace data. C3: Domain expertise is usually required when analyzing API calls. The astronomical number of API calls makes it hard to run any machine-learning algorithm. To control the complexity of the machine learning model, previous work may use the prior knowledge about the APIs to classify them into a few categories (Ki et al., 2015; Amer et al., 2020; Ahmed et al., 2009), or only use a specific subset of APIs based on prior knowledge (Kim, 2018; Fan et al., 2018; Ahmed et al., 2009), which introduce significant limitations and biases to the detection. Realizing the automated analysis and selection of API functions without introducing additional knowledge is still a great challenge. To address the abovementioned challenges, we design PARIS, the first practical, adaptive trace-fetching, real-time malicious behavior detection system. Specifically, We design several methods for feature selection, such as graph-based API selection, API association analysis, call stack selection, and loop compression, to filter out irrelevant APIs and call stacks during collection. Moreover, based on ETW, we build an efficient, selective call stack parsing module for data collection. Therefore, PARIS can efficiently collect and process API call stacks and perform stable, accurate, real-time behavior detection with low overhead. By analyzing the API call stacks, we aim to identify attack behaviors rather than merely detecting the malware. To concentrate on malicious behaviors, we devise a clustering-based training set cleaning mechanism to eliminate non-malicious behavior data (noise and usual background activities) from the training set. Finally, all the above-mentioned methods are based on basic observation and common sense in API analysis, requiring no expert knowledge about specific API functions during detection. Our final goal is to implement an adaptive, lightweight and low overhead real-time detection system as shown in Fig.1. Figure 1. Trade off between two strategies We deploy PARIS in a real-world environment and conduct experiments with benign and malicious datasets. The results further show that PARIS retains only the equivalent of 1.12% of the original data size and can run stably on the client for a long time with an average resource overhead of 32MB memory usage and 4.79% CPU usage. In addition, PARIS transmits at an average network bandwidth of 0.77kb/s, achieving a detection accuracy of 93.6%, which is comparable to offline methods. All in all, we make the following contributions in this paper: ‚Ä¢ We design PARIS, a lightweight real-time malicious behavior detector. PARIS can dynamically monitor all system-level API calls (API calls in all DLL files under C:\Windows) with low overhead and detect threats in real-time. By selectively processing the ETW data, we address the issue of high overhead and delay in fine-grained tracing data collection. ‚Ä¢ We design algorithms to analyze and select useful APIs and call stacks for behavioral detection correspondingly. Based on the feature selection and extraction techniques in machine learning and data mining, our model does not introduce any human prior knowledge or expertise during data collection and model training, thus having less bias and stronger generality. ‚Ä¢ We implemented the PARIS and evaluated it in real-world environments. The experimental results demonstrate that the data collector of PARIS can run on a standard computer with an average memory usage of 32MB, bandwidth of 0.77kb/s, CPU usage of 4.79%, and an average detection latency of 6.84s. Furthermore, it still achieves a high accuracy of 93.6% to the baselines on real-world software behavior datasets, while retaining less than 2% of the original data scale. The remainder of this paper is organized as follows. We first describe the preliminary knowledge about the harmful behaviors of APT attacks and the Event Tracing for Windows in ¬ß2. Then, we present a system overview in ¬ß3. Next, we introduce our design and implementation of the malicious behavior semantic model and the detection model in ¬ß4, ¬ß5, and ¬ß6, respectively. We evaluate PARIS in ¬ß7. The discussion, related work, and conclusion are presented in ¬ß8, ¬ß9, and ¬ß10, respectively."
https://arxiv.org/html/2411.01236v1,AutoPT: How Far Are We from the End2End Automated Web Penetration Testing?,"Penetration testing is essential to ensure Web security, which can detect and fix vulnerabilities in advance, and prevent data leakage and serious consequences. The powerful inference capabilities of large language models (LLMs) have made significant progress in various fields, and the development potential of LLM-based agents can revolutionize the cybersecurity penetration testing industry. In this work, we establish a comprehensive end-to-end penetration testing benchmark using a real-world penetration testing environment to explore the capabilities of LLM-based agents in this domain. Our results reveal that the agents are familiar with the framework of penetration testing tasks, but they still face limitations in generating accurate commands and executing complete processes. Accordingly, we summarize the current challenges, including the difficulty of maintaining the entire message history and the tendency for the agent to become stuck.Based on the above insights, we propose a Penetration testing State Machine (PSM) that utilizes the Finite State Machine (FSM) methodology to address these limitations. Then, we introduce AutoPT, an automated penetration testing agent based on the principle of PSM driven by LLMs, which utilizes the inherent inference ability of LLM and the constraint framework of state machines. Our evaluation results show that AutoPT outperforms the baseline framework ReAct on the GPT-4o mini model and improves the task completion rate from 22% to 41% on the benchmark target. Compared with the baseline framework and manual work, AutoPT also reduces time and economic costs further. Hence, our AutoPT has facilitated the development of automated penetration testing and significantly impacted both academia and industry.","Web security (Stock et al., 2017) is a daunting challenge. Penetration testing (Shravan et al., 2014; Weissman, 1995) and red team testing (Teichmann and Boticiu, 2023) have become necessary means to ensure Web security. For example, in 2024, Bank of America‚Äôs data breach occurred, and the bank‚Äôs service provider Infosys Mccamish Systems suffered a ransomware attack, resulting in the exposure of sensitive information from more than 60,000 customers111https://www.anquanke.com/post/id/293251. However, suppose the company conducts comprehensive penetration testing before launching a new system. In that case, these security vulnerabilities may be discovered and fixed in advance, thus avoiding data leakage and the serious consequences it may cause. Therefore, our study focuses on the field of penetration testing and focuses on automated testing, especially black-box testing (Hasibuan and Elhanafi, 2022). Penetration testing is a way to evaluate Web security by simulating real attacks (Arkin et al., 2005). It involves a team of security experts assuming the role of attackers, employing tools and techniques like real hackers. This process entails deliberate attack attempts on target systems, networks, or applications to identify and exploit vulnerabilities within these environments. Currently, most penetration tests are labor-intensive processes conducted by skilled professionals who leverage their organizational knowledge and expertise and use semi-automated tools to execute a predefined set of automated operations (Deng et al., 2023c). A few studies have attempted automated penetration tests, such as rule-based methods (Zhao et al., 2015; Halfond et al., 2009; Appelt et al., 2014) and deep reinforcement learning-based solutions (Qiu et al., 2014). However, none of these automated methods can solve the end-to-end penetration testing task, defined as the entire process of completing automated penetration testing without human involvement and that automatically adapts to various environments. Benchmark. To address this question, we began to explore the capabilities of LLM-based agents in end-to-end automated penetration testing tasks. Unfortunately, current penetration testing benchmarks are not granular enough to perform a fair and granular assessment of the progress made. Among them, CTF-related benchmarks (Shao et al., 2024; Burns et al., 2017) are far from actual penetration scenarios, and HackTheBox (Hac, 2023) mostly belongs to the actual combat of compound vulnerabilities, which is too complex for the current single-agent capabilities. To address this limitation, we built a refined benchmark covering the OWASP‚Äôs top 10 vulnerability list (team, [n. d.]) via test machines from Vulhub (Vulhub Project, [n. d.]). Then, we performed detailed manual annotations, including task complexity annotations based on the number of exploit steps. In addition, for end-to-end task goal checking, we created an explicit task goal string for each task triggered if the vulnerability exploit goal is met. In this way, our benchmark can meet the needs of end-to-end penetration testing task evaluation. Motivation. Large language models (LLMs) have developed rapidly and have shown great capabilities in many applications and tasks (Deng et al., 2023b; Minaee et al., 2024; Guan et al., 2024; Wen et al., 2024). Furthermore, LLMs have been applied to tasks that require interaction with the environment through agents (Nayan et al., 2024; Xi et al., 2023; Liu et al., 2024a), such as code execution feedback and real-world scene interaction. Despite the significant efforts of tens of thousands of penetration testing researchers worldwide, fully automated penetration testing has remained challenging for an extended period (Xi et al., 2023; Abu-Dabaseh and Alshammari, 2018). Recently, several studies have aimed at helping humans perform penetration testing, such as PentestGPT (Deng et al., 2023a). Nevertheless, they necessitate extensive human-computer interaction and lack a systematic and quantitative evaluation of current LLM-based agents on end‚Äíto‚Äíend Web penetration testing tasks. Therefore, the following question arises: How far are we from the end-to-end automated Web penetration testing? Based on the benchmark we built, we conducted an end-to-end evaluation of the existing to pave the way for subsequent research. First, we tried many advanced models, from which we selected GPT-3.5, GPT-4o, and GPT-4o mini models that passed the first pre-experiment as representative LLMs for subsequent research. Then, we designed an end-to-end testing strategy, which includes carefully designed prompts to guide the agent to conduct penetration testing. For existing agent frameworks, we selected the ReAct (Yao et al., 2023) framework and the framework built on the PentestGPT (Deng et al., 2023a) core penetration testing task tree (PTT) as a representative framework. Each agent receives prompts and black box information from the target machine, which spontaneously queries the environmental information, infers subsequent operations, executes terminal commands, and operates browsers via controlled tools. This process is repeated until the LLM autonomously completes the penetration testing. Finally, we compare its results with the baseline solution of officially certified penetration testers (Vulhub Project, [n. d.]). By analyzing the reasons for the agent‚Äôs failure cases, we summarize the main challenges of current intelligent agents performing end-to-end penetration testing tasks as follows: 1) Maintaining the entire message history is difficult due to model context size limitations. 2) The agent may get stuck on subtle problems during self-iteration, leading to task failure. 3) Current model inference capabilities restrict an agent from completing this task. Our Methodology. To address these challenges, we introduce a classic method, the Finite State Machine (FSM), which enables us to better manage the agent‚Äôs decision-making process by maintaining a clear and structured sequence of actions while retaining the model‚Äôs own operation space to the maximum extent. We developed a novel agent architecture called the Penetration Testing State Machine (PSM), which draws inspiration from the traditional FSM. In traditional Web security tasks (Happe and Cito, 2023b), penetration testers often have some fixed actions, such as data query and reflection inspection. Based on this, we divide the PSM into the Agent state and the Rule state, constraining the workflow of solving tasks and guiding subsequent operations to solve the end-to-end penetration testing task. We launched AutoPT (Automated Penetration Testing), an end-to-end system based on PSM designed to increase the use of agents in this field. AutoPT draws inspiration from the collaborative dynamics common in real-world human penetration testing teams (Happe and Cito, 2023c). It uses the third-party architecture LangChain (Chase, 2022) to build an agent, including vulnerability scanning, selection, reconnaissance, exploitation, and check states. Each state reflects each part of the penetration testing process. What is unique is that this architecture can clearly and visually display the state jumps of the entire state machine, thereby improving the efficiency and success rate of penetration testing. Specifically, our method contains the following states: ‚Ä¢ The Scanning state uses an open-source scanner to obtain a list of system vulnerabilities. ‚Ä¢ The Selection state follows the thinking of general infiltrators, formats the list of vulnerabilities according to the results of the Scanning state and selects the most likely vulnerability from it. ‚Ä¢ The Reconnaissance state uses tools to scout based on vulnerability information ‚Ä¢ The Exploitation state simulates a junior penetration tester and faithfully attempts to exploit vulnerabilities based on the results of the vulnerability query. ‚Ä¢ The Check state makes detection jumps based on the output value of the vulnerability attempt. Overall, these states work as an integrated system. AutoPT completes the initial end-to-end penetration testing task by combining advanced strategies and precise execution, thereby maintaining a coherent and effective testing process. We created a github repository that includes all benchmark entries and environments, the code used to implement the pre-experiments, and the AutoPT system. The entire project will be open-sourced after peer review. For more information, please refer to Section Data Availability. We evaluate AutoPT in different test scenarios to validate its effectiveness and efficiency. In our proposed benchmark, AutoPT significantly outperformed direct applications of both the ReAct and improved PTT frameworks, increasing task completion rates from 22% to 41%, respectively. The execution efficiency was improved by 96.7%, and the total cost of using the OpenAI API was decreased by 71.6%. We believe this is because AutoPT reduces the context width requirement of each state model by decomposing the end-to-end task into multiple subtasks, thus compensating for the impact of subtasks and avoiding the failure of the entire task due to the dilemma of a subtask. This evaluation highlights the practical value of AutoPT in improving the efficiency and accuracy of penetration testing tasks. During the evaluation process, we gained interesting insights into the capabilities and limitations of LLM-based agents in penetration testing. First, in contrast to human behavior, agents can quickly read and query relevant information and make rapid attempts based on vulnerability information. In addition, intelligent agents perform operations such as scanning, reconnaissance, and exploitation according to target requirements. However, we also noticed that current agents are affected by model capabilities and model hallucinations (Liu et al., 2024a, b) and often output incorrect commands that cause task failures, which is an important aspect of optimizing end-to-end penetration testing goals. We can also see that in the near future, fully automatic penetration testing agents will surely appear in the public eye. Contribution. Overall, the major contributions of our work are as follows: ‚Ä¢ Develop a fine-grained end-to-end penetration testing benchmark. We developed a robust and representative penetration testing benchmark with test machines from the leading platform, VulnHub. The benchmark includes 20 out-of-the-box docker environments, covering the OWASP‚Äôs top 10 vulnerability list, both easy and difficult, and detailed and specific vulnerability targets and detection content for each vulnerability, providing a fair and comprehensive evaluation for penetration testing. To the best of our knowledge, this benchmark is the first to provide a clear evaluation and inspection of end-to-end penetration testing tasks. ‚Ä¢ Design a novel agent framework PSM and implement a novel end-to-end penetration testing system. We drew inspiration from traditional finite state machines, integrated the design of general penetration tester behavior logic, and built a penetration test state machine. Based on its principles, we implemented a novel end-to-end penetration testing system AutoPT. This architecture optimizes the use of agents and significantly improves the efficiency and effectiveness of automated penetration testing. ‚Ä¢ Comprehensive evaluation and analysis of LLM-driven agents in end-to-end penetration testing tasks. By adopting the GPT-3.5, GPT-4o, and GPT-4o mini models along with the ReAct and RTT frameworks, our exploratory study rigorously investigates the strengths and limitations of agents in penetration testing. To the best of our knowledge, this is the first systematic and quantitative study of the ability of LLM-based agents to perform end-to-end automated penetration testing. Our results show that LLMs show great potential in advancing automation to complete end-to-end penetration testing tasks. We call for more research in this area to further enhance the capabilities of LLMs so that they can play a more critical role in the complex tasks of penetration testing."
https://arxiv.org/html/2411.01230v1,Strengthening DeFi Security: A Static Analysis Approach to Flash Loan Vulnerabilities,"The rise of Decentralized Finance (DeFi) has brought novel financial opportunities but also exposed serious security vulnerabilities, with flash loans frequently exploited for price manipulation attacks. These attacks, leveraging the atomic nature of flash loans, allow malicious actors to manipulate DeFi protocol oracles and pricing mechanisms within a single transaction, causing substantial financial losses. Traditional smart contract analysis tools address some security risks but often struggle to detect the complex, inter-contract dependencies that make flash loan attacks challenging to identify.In response, we introduce FlashDeFier, an advanced detection framework that enhances static taint analysis to target price manipulation vulnerabilities arising from flash loans. FlashDeFier expands the scope of taint sources and sinks, enabling comprehensive analysis of data flows across DeFi protocols. The framework constructs detailed inter-contract call graphs to capture sophisticated data flow patterns, significantly improving detection accuracy. Tested against a dataset of high-profile DeFi incidents, FlashDeFier identifies 76.4% of price manipulation vulnerabilities, marking a 30% improvement over DeFiTainter. These results highlight the importance of adaptive detection frameworks that evolve alongside DeFi threats, underscoring the need for hybrid approaches combining static, dynamic, and symbolic analysis methods for resilient DeFi security.","Over the past few years, the finance industry has seen a tremendous growth in decentralized solutions with Decentralized Finance (DeFi) protocols due to the advantages offered by the underlying blockchain architecture. DeFi projections predict the revenue to be approximately USD 17.8 billion by 2023 and the number of DeFi users to be 22.09 million by 2028 [26]. Blockchain-powered DeFi enables consumers to have trustless transactions of digital financial assets (cryptocurrencies and tokens) without relying on a central party like a traditional bank. Over time, the DeFi landscape has evolved into a massive network with integrated financial instruments and protocols such as decentralized exchanges (DEXs), lending and margin trading platforms, liquidity managers, yield farmers, tracking indexes etc [27]. As of December 2023, the Total Value Locked (TVL) by DeFi protocols is USD 52.61 billion [29]. This high value of digital assets managed by DeFi protocols makes them a lucrative target for attacks. [30]. In the past few years, there have been several DeFi hacks with total hacked value amounting to USD 5.7 billion [28]. Motivation - The humongous losses due to DeFi exploits can create a general loss of trust in the feasibility of DeFi as an alternative to traditional financial services. Therefore, it is imperative to enhance DeFi security measures and improve attack detection frameworks to avoid unwanted consequences of security breaches in DeFi protocols. DeFi protocols can be attacked by exploiting vulnerabilities such as re-entrancy, frontrunning, rug-pull etc [31]. One such common vulnerability in DeFi protocols is price manipulation which is often exploited to conduct hacks via flash loans. Flash loans are uncollaterized loans which execute in one atomic transaction on the blockchain. DeFi protocols rely on price oracles to adjust the asset prices according to off-chain market factors. Insecure oracles are often vulnerable to price manipulation, and can hence lead to hefty flash loan thefts [32]. A notable example of a flash loan attack is on the bZx platform [20] where the attacker leveraged a price oracle dependency of bZx on other DeFi platforms (i.e., Uniswap and Kyber) in manipulating cryptoasset exchange rates, making net profit of USD 318k within a single atomic transaction. Although there has been significant research progress to detect and fix bugs in smart contract codes, there has been limited research to detect price manipulation vulnerabilities as attackers exploit logic and design of DeFi protocols and their dependency on price oracles to conduct flash loan attacks. However, using program analysis and verification techniques for smart contracts, vulnerabilities with cross-contract dependencies can be identified. Common program analysis techniques include static analysis, dynamic analysis, symbolic execution and fuzzing. Furthermore, data flow in programs can be tracked via taint analysis methods. FlashDeFier: This work presents FlashDeFier, a static taint analyzer for smart contracts to detect price manipulation vulnerabilities. It is an extension of the existing state-of-the-art tool, DeFiTainter [9] which employs static taint analysis on decompiled smart contracts for price manipulation detection. We highlight the following contributions of our work: 1. We build upon the DeFiTainter framework to perform static analysis of inter-contract call and data flow by borrowing concepts from existing static taint analysis methods. 2. We expand the set of taint sources and sinks after an extensive study of decompiled bytecode of contracts. 3. We analyze the call flow graph of the smart contract to identify the function signature needed to build inter-contract data flow graph to track the propagation of taint from source to sink. 4. We evaluate our results and observe a 30% improvement in detection accuracy as compared to DeFiTainter."
https://arxiv.org/html/2411.02152v1,FedPID: An Aggregation Method for Federated Learning,"This paper presents FedPID, our submission to the Federated Tumor Segmentation Challenge 2024 (FETS24). Inspired by FedCostWAvg and FedPIDAvg, our winning contributions to FETS21 and FETS2022, we propose an improved aggregation strategy for federated and collaborative learning. FedCostWAvg is a method that averages results by considering both the number of training samples in each group and how much the cost function decreased in the last round of training. This is similar to how the derivative part of a PID controller works. In FedPIDAvg, we also included the integral part that was missing. Another challenge we faced were vastly differing dataset sizes at each center. We solved this by assuming the sizes follow a Poisson distribution and adjusting the training iterations for each center accordingly. Essentially, this part of the method controls that outliers that require too much training time are less frequently used. Based on these contributions we now adapted FedPIDAvg by changing how the integral part is computed. Instead of integrating the loss function we measure the global drop in cost since the first round.","Federated learning provides an extremely promising framework for ensuring privacy and secure learning across different data locations [1]. It has numerous applications, ranging from power grids to medicine [2]. This approach is particularly important for medical images because patient information is highly sensitive, and the distribution of medical expertise as well as the prevalence of certain diseases varies significantly across different regions [3]. Additionally, medical imaging data is typically very large, making it expensive and impractical to frequently transfer from local clinics to a central server [3]. Ensuring the privacy and safety of patient data is even more crucial given the frequent illegal leaks of private medical records to the dark web [4]. By keeping data local and only sharing updates, federated learning minimizes the risk of exposing sensitive information. This makes it an ideal solution for handling medical data securely and efficiently. Brain tumor segmentation is a particularly good candidate for study in this setting due to its complex and sensitive nature. Figure 1: The schematic illustration shows the concept of federated learning. It depicts multiple data centers that form a federation. Each data center stores its training data of different sizes locally and trains the same model for a specific task, such as brain tumor segmentation in our case. During the aggregation step, the locally trained model weights are sent to a central server which then performs model aggregation and broadcasts the updated model back to the local centers. This process is repeated until the model converges or another stopping criterion is met. 1.1 FETS challenge The FETS challenge [5, 6, 7, 8, 9] is a multi-year initiative aimed at addressing one of the main research question of federated learning: optimal aggregation of network weights from various data centers. In this paper, we address this issue by proposing a solution extending our previous submissions entailing PID controllers and classical statistics."
https://arxiv.org/html/2411.02099v2,Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-based Human Activity Recognition,"Human motion analysis offers significant potential for healthcare monitoring and early detection of diseases. The advent of radar-based sensing systems has captured the spotlight for they are able to operate without physical contact and they can integrate with pre-existing Wi-Fi networks. They are also seen as less privacy-invasive compared to camera-based systems. However, recent research has shown high accuracy in recognizing subjects or gender from radar gait patterns, raising privacy concerns. This study addresses these issues by investigating privacy vulnerabilities in radar-based Human Activity Recognition (HAR) systems and proposing a novel method for privacy preservation using Differential Privacy (DP) driven by attributions derived with Integrated Decision Gradient (IDG) algorithm. We investigate Black-box Membership Inference Attack (MIA) Models in HAR settings across various levels of attacker-accessible information. We extensively evaluated the effectiveness of the proposed IDG-DP method by designing a CNN-based HAR model and rigorously assessing its resilience against MIAs. Experimental results demonstrate the potential of IDG-DP in mitigating privacy attacks while maintaining utility across all settings, particularly excelling against label-only and shadow model black-box MIA attacks. This work represents a crucial step towards balancing the need for effective radar-based HAR with robust privacy protection in healthcare environments.","Demographic projections indicate that the worldwide population of individuals aged 60 and above will surge to approximately 1.4 billion people by 2030 [65], leading to an increased reliance on advanced healthcare monitoring technologies in patients‚Äô homes [23]. Human motion analysis shows significant potential for healthcare monitoring and early disease detection [14, 13]. Deep Neural Networks (DNNs) have exhibited remarkable efficacy in Human Activity Recognition (HAR) for monitoring patients and detecting abnormalities [24, 1]. Especially, in home settings these technologies can play a key role in preventive and proactive healthcare strategies by enabling personalised care systems [84, 23]. However, while DNNs excel at encoding input features, this capability also renders them vulnerable to privacy breaches [53, 39]. For instance, Membership Inference Attacks (MIA) [69, 35, 74] and Model Inversion (MI) attacks [27] can disclose private information about the patients and their training data by accessing only the pre-trained model outputs. To mitigate such risks, privacy-preserving techniques needs to be incorporated into HAR systems to safeguard sensitive user information. Differential Privacy (DP) is a commonly used technique to enhance privacy in deep learning models [8, 22]. DP quantifies the risk of an individual‚Äôs information being disclosed by ensuring that the model‚Äôs output is not significantly affected by the inclusion or exclusion of any single individual‚Äôs data. The implementation of DP involves introducing controlled perturbations to the data. Recently, human motion sensing with radar emerges as a prominent sensing technology for continuous monitoring thanks to its non-intrusive nature [25]. This makes it suitable for privacy-sensitive environments such as assisted-living facilities, hospitals and homes. Nevertheless, recent studies have revealed high accuracy in subject recognition from radar human gait patterns, challenging the perception of privacy in radar-based systems [50, 51]. This underscores the need for implementing privacy safeguards in human motion sensing systems, regardless of whether the output is visually identifiable by humans. Previous research in human motion analysis has primarily focused on privacy preservation of Red Green and Blue (RGB) videos on a frame by frame basis [21]. Some methods use anonymisation techniques by replacing the face or whole body with synthetic data[33], while other approaches use obfuscation of sensitive attributes [82]. In most cases, the utility of the data is severely compromised, hindering the application of these methods in healthcare. More recently, it has been recognised that other human motion tracking modalities such as accelerometer, gyroscope sensor data and radar data also record sensitive information that can help identify users and track them during their daily activities [22, 37, 50, 51]. However, to our knowledge, there is no systematic work modeling threats and evaluating the robustness of privacy preservation techniques under sophisticated machine learning (ML)-driven attacks in radar-based systems. Here we firstly define black-box MIA that are relevant to HAR setting with radar data. We assume that the adversary has access to the logit space of the model and it is also possible to gain partial access to the training data or their underlying distribution. In these ways, the adversary can orchestrate attacks to identify individuals. Subsequently, we propose a novel method, named IDG-DP, based on DP and the Integrated Decision Gradient (IDG) attribution algorithm [80]. The IDG was selected for its ability to compute attributions precisely at the model‚Äôs decision points, making it a superior attribution algorithm with enhanced performance [80]. IDG-DP injects more noise in input features that contribute more towards the subject identification than to the activity recognition and thus it preserves privacy, while it maintains high performance in HAR. Our paper presents the following contributions: 1. To our knowledge, we are the first to systematically investigate state-of-the-art threat models that are relevant to HAR in a home setting with radar technology. This enables the identification of sensitive information leakage that is not perceptible to a human observer. 2. We introduce a novel methodology that drives DP by identifying the model‚Äôs highest attributions during training. In this way, we achieve a better balance between data utility and privacy preservation. 3. We devise a rigorous evaluation strategy of the mitigation capabilities of the proposed approach against black-box MIA attacks. We exploit a publicly available dataset on HAR with radar data [20] to assess the effectiveness of the proposed IDG-DP privacy method under various black-box MIA attacks. We demonstrate promising results in balancing data utility and privacy in the data."
https://arxiv.org/html/2411.02051v1,R+R: Understanding HyperparameterEffects in DP-SGD,"Research on the effects of essential hyperparameters of DP-SGD lacks consensus, verification, and replication. Contradictory and anecdotal statements on their influence make matters worse. While DP-SGD is the standard optimization algorithm for privacy-preserving machine learning, its adoption is still commonly challenged by low performance compared to non-private learning approaches. As proper hyperparameter settings can improve the privacy-utility trade-off, understanding the influence of the hyperparameters promises to simplify their optimization towards better performance, and likely foster acceptance of private learning.To shed more light on these influences, we conduct a replication study: We synthesize extant research on hyperparameter influences of DP-SGD into conjectures, conduct a dedicated factorial study to independently identify hyperparameter effects, and assess which conjectures can be replicated across multiple datasets, model architectures, and differential privacy budgets. While we cannot (consistently) replicate conjectures about the main and interaction effects of the batch size and the number of epochs, we were able to replicate the conjectured relationship between the clipping threshold and learning rate. Furthermore, we were able to quantify the significant importance of their combination compared to the other hyperparameters.","Replication studies serve the research by verifying experimental results, by refining scientific theories, and ultimately by helping establish highest levels of reliability in the scientific record of knowledge. Therefore, where any line of inquiry encounters a problem of scattered and unverified insights, replicatory works can help streamline understanding and so advance the research. One such problem is found in the recent research on hyperparameter effects in differentially private stochastic gradient descent (DP-SGD). Multiple recent works have contributed propositions and valuable insights into these effects. However, the insights are scattered across many works [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] and what is more, the insights are not consistently and reliably based on independent empirical verification. Here especially, replication can create consensus and refine the current body of knowledge. DP-SGD [1] has become the de facto standard for training machine learning models with differential privacy guarantees [13, 14]. Yet, models trained with DP-SGD perform worse than their counterparts trained with stochastic gradient descent (SGD) [15]. By properly adjusting the learning pipeline, recent works not only managed to significantly decrease the accuracy gap to SGD. They also demonstrated that hyperparameter effects differ between SGD and DP-SGD. For example, multiple works demonstrated that the optimal batch size is significantly larger in DP-SGD than in SGD [10, 9], and further observations of other hyperparameter effects have been made. However, to date, no systematic or replicatory studies have been conducted on the hyperparameter effects in DP-SGD. In this work, we conduct a replication study on how DP-SGD‚Äôs essential hyperparameters (i.e. batch size, number of epochs, learning rate, and clipping threshold) affect model accuracy on non-convex machine learning tasks. We do not intend for this work to break ground on state-of-the-art performance; therefore, we also do not consider specific alterations to the learning pipeline [16, 11] or pre-training on public data [11, 9]. Instead, we focus on refining the understanding of the essential hyperparameters‚Äô effect on DP-SGD. To this end we review extant literature which discusses the effect of hyperparameters of DP-SGD or otherwise reports on them. Based on these insights we synthesize six testable conjectures. To assess the replicability of the conjectures, we conduct a dedicated experiment to independently identify the hyperparameter effects of DP-SGD in a systematic and structure way. As most research on the effect of hyperparameters of DP-SGD considered image classification and language modeling, we also chose these two domains for our study. In our factorial experiments we evaluate 3822382238223822 hyperparameter tuples across six datasets, six model architectures, and three differential privacy budgets. Based on the results, we assess the hyperparameters‚Äô importance, their main, and interaction effects. Subsequently, we discuss the effects identified in our experiment in relation to the conjectured effects from related work. Besides enabling us to assess the replicability111We follow the terminology of the ACM on replicability, see https://www.acm.org/publications/policies/artifact-review-and-badging-current. of conjectures from related work, this large-scale experiment also provides the most comprehensive investigation on the hyperparameter effects of DP-SGD to date. In summary, we make the following contributions: ‚Ä¢ In a first step toward replication, we review the literature and collect scattered insights into hyperparameter effects in DP-SGD. Further, we synthesize these insights into six testable observations about hyperparameter effects of DP-SGD. ‚Ä¢ Our large-scale factorial study across multiple datasets, model architectures, and differential privacy budgets independently identifies and quantifies the importance of main and interaction effects of the four essential hyperparameters of DP-SGD. ‚Ä¢ We evaluate which of the synthesized conjectures about the hyperparameter effects in DP-SGD can be supported by independent experimental results. The remainder of this paper is organized as follows: In Section II we introduce DP-SGD, how the differential privacy budget is calculated, and how the budget interacts with the algorithm‚Äôs hyperparameters. In Section III we review the related work on the hyperparameters effect and synthesize their insights into conjectures. In Section IV we motivate and introduce our study design including what machine learning tasks we evaluated, how we chose and evaluated the hyperparameter space, and especially, how we identified main and interaction effects based on the results from our experiment. In Section V we report the identified main and interaction effects of the hyperparameters and discuss the replicability of the conjectures from related work. In Section VI we conclude this study."
https://arxiv.org/html/2411.01931v1,Differentially private and decentralized randomized power method,"The randomized power method has gained significant interest due to its simplicity and efficient handling of large-scale spectral analysis and recommendation tasks. As modern datasets contain sensitive private information, we need to give formal guarantees on the possible privacy leaks caused by this method. This paper focuses on enhancing privacy-preserving variants of the method. We propose a strategy to reduce the variance of the noise introduced to achieve Differential Privacy (DP). We also adapt the method to a decentralized framework with a low computational and communication overhead, while preserving the accuracy. We leverage Secure Aggregation (a form of Multi-Party Computation) to allow the algorithm to perform computations using data distributed among multiple users or devices, without revealing individual data. We show that it is possible to use a noise scale in the decentralized setting that is similar to the one in the centralized setting. We improve upon existing convergence bounds for both the centralized and decentralized versions. The proposed method is especially relevant for decentralized applications such as distributed recommender systems, where privacy concerns are paramount.","The randomized power method has attracted significant attention due to its simplicity and its efficiency in solving large-scale linear algebra problems in spectral analysis or recommendation. It is in particular useful for computing the principal singular vectors of a matrix. It has a straightforward implementation which runs with low computational overhead (nearly linear complexity), making it particularly suitable for analyzing large modern datasets. In large-scale machine learning systems, protecting user privacy is paramount. Datasets often contain sensitive personal or organizational information, so privacy has become a critical requirement. The standard randomized power method does not inherently provide privacy guarantees. While its output might seem less sensitive than the input data, there is no formal guarantee that private information embedded in the data cannot be inferred. To address and quantify privacy leakages, Differential Privacy (DP) has emerged as a powerful mathematical framework that provides formal guarantees and helps to mitigate potential privacy leaks of an algorithm. DP ensures that the output of an algorithm reveals little about any individual record in the input, even in the worst case. Several works have attempted to apply DP to the randomized power method. For instance, (Hardt & Price, 2014) and (Balcan et al., 2016) introduced differentially private versions of the method, and (Guo et al., 2021; Wang & Xu, 2020) explores federated approaches to private power iteration. Other close works explore centralized differentially private versions of PCA (Liu et al., 2022) and federated (Wang & Xu, 2020; Briguglio et al., 2023) and give optimal convergence bounds given certain assumptions. Despite these advancements, existing approaches suffer from several limitations. First, their performance heavily depends on the number of singular vectors being computed (Hardt & Price, 2014; Balcan et al., 2016; Guo et al., 2021; Liu et al., 2022; Wang & Xu, 2020), which impacts both utility and privacy guarantees. Second, they are primarily designed for centralized settings (Hardt & Price, 2014; Liu et al., 2022), where a trusted curator is assumed to hold the data. Moreover, some methods (Liu et al., 2022) make strong assumptions about the underlying data distribution (i.e. that the data is sub-Gaussian) which makes it harder to use these methods in practice. Some federated versions (Briguglio et al., 2023) claim to guarantee privacy by use of the federated setting, although it has been shown that decentralization does not offer privacy by design (Geiping et al., 2020). Finally, no fully decentralized versions exist to our knowledge. The previously proposed federated versions (Balcan et al., 2016; Guo et al., 2021; Wang & Xu, 2020) use public channel communication and ignore advances in Multi-Party-Computation (MPC). This makes them unsuitable for decentralized environments, such as distributed recommender systems and social networks, where data is partitioned across users and/or devices and communications are restricted to a predefined communication graph. 1.1 Contributions The core focus of this article is to introduce improved privacy-preserving randomized power methods and to extend them to a decentralized setting, while maintaining computational efficiency and performance. We propose new convergence bounds for the (decentralized) privacy-preserving power method; these show that our proposed algorithm has a weaker dependence on the number of singular vectors that are computed. We propose an improved differentially private version of the randomized power method, with tighter convergence bounds, reducing the noise required to achieve privacy and therefore improving the accuracy of the method. In the process of deriving these bounds, we present a new, more straightforward proof of a privacy result for the differentially private randomized power method.111This proof addresses some minor mistakes in a privacy proof from (Hardt & Roth, 2012, 2013) that have been reproduced in several follow-up works (Hardt & Price, 2014; Balcan et al., 2016). The modified result allows us to use a wider range of privacy parameters. Finally, we propose decentralized versions of the algorithms. These are particularly relevant in contexts where data is distributed across multiple users or devices, as is often the case in recommender systems. To address the privacy concerns of the users, without forcing them to rely on a trusted curator, we propose a federated privacy-preserving version of the method using Secure Aggregation (Bell et al., 2020), a form of Multi-Party Computation (MPC) and a fully decentralized version using GOPA (GOssip noise for Private Averaging) (Sabater et al., 2022). This allows users to keep their data local while contributing to a privacy-preserving global computation. Our decentralized approach preserves the effectiveness of the centralized version of the randomized power method while incorporating the privacy advantages of local differential privacy (DP). Notably, by employing Secure Aggregation, we are able to set the noise scale comparable to that of central DP, enhancing accuracy. This makes the proposed method a promising candidate for decentralized and privacy-conscious applications. Our contributions can be summarized as follows: 1. We introduce a novel privacy-preserving randomized power method that improves upon existing convergence bounds, with a reduced sensitivity to the number of singular vectors. 2. We extend the method to a decentralized setting, utilizing Secure Aggregation to ensure privacy in a distributed environment without the need for a trusted curator. 3. We provide new privacy proofs that correct errors in previous works, ensuring rigorous privacy guarantees. 4. We propose a new adjacency model to guarantee the differential privacy of our method, to open the door to new and more realistic use cases. 5. We present both runtime-dependent and runtime-independent bounds for the proposed methods, demonstrating their utility through theoretical analysis and empirical validation."
https://arxiv.org/html/2411.01580v1,Federated Learning Clients Clustering with Adaptation to Data Drifts,"Federated Learning (FL) enables deep learning model training across edge devices and protects user privacy by retaining raw data locally. Data heterogeneity in client distributions slows model convergence and leads to plateauing with reduced precision. Clustered FL solutions address this by grouping clients with statistically similar data and training models for each cluster. However, maintaining consistent client similarity within each group becomes challenging when data drifts occur, significantly impacting model accuracy. In this paper, we introduce Fielding, a clustered FL framework that handles data drifts promptly with low overheads. Fielding detects drifts on all clients and performs selective label distribution-based re-clustering to balance cluster optimality and model performance, remaining robust to malicious clients and varied heterogeneity degrees. Our evaluations show that Fielding improves model final accuracy by 1.9%-5.9% and reaches target accuracies 1.16√ó\times√ó-2.61√ó\times√ó faster.","Federated Learning (FL) is a distributed machine learning paradigm that enables multiple clients to train one model collaboratively while retaining raw data locally (Bonawitz et al., 2019; McMahan et al., 2017). As FL circumvents the communication costs of data movement and mitigates privacy leakage (Lyu et al., 2024; Huba et al., 2022), it has drawn attention across various fields and seen adoption in applications such as Google Keyboard (Yang et al., 2018), energy consumption forecasting (Abdulla et al., 2024), and medical image processing (Zhou et al., 2023). The main challenge in federated learning is data heterogeneity. Since federated learning involves many devices of diverse settings, there exists substantial dataset size and distribution divergences between clients from distant geographical locations and with distinct personal habits (Li et al., 2022a; Kim et al., 2024). Such data heterogeneity leads to slow model convergence and plateauing with degraded precision (Li et al., 2020a; Zhao et al., 2018; Sattler et al., 2020). A wide range of clustering mechanisms have been proposed to group clients with similar data distribution into more homogeneous clusters and train one model for each cluster (Ghosh et al., 2020; Duan et al., 2021; Liu et al., 2023; Jothimurugesan et al., 2023). Data drift occurs frequently in federated learning systems. It is defined as temporal changes in the label distribution (i.e., the proportion of samples assigned to each label) of clients‚Äô local datasets. Such drifts arise from varying sampling rates, hardware configurations, sensor modalities, and limitations in client storage capacities (Chen et al., 2020; Nandi and Xhafa, 2022; Marfoq et al., 2023). Existing clustering mechanisms, however, are susceptible to data drift. Prior works address this issue but often either inadequately adjust clusters or incur prohibitive costs. IFCA and FlexCFL (Ghosh et al., 2020; Duan et al., 2021) adjust clusters by relocating shifted clients while maintaining a fixed number of clusters, operating under the assumption of low drift magnitude. Auxo (Liu et al., 2023) re-clusters clients participating in the current training round using gradient information; however, since only a subset of clients is selected each round, it delays adjustments for drifted clients until they are chosen for training. FedDrift (Jothimurugesan et al., 2023) re-clusters all clients and adaptively determines the number of clusters by transmitting all cluster models to every client each round to obtain training loss on local data, leading to significant communication and computation costs. In this work, we propose Fielding, a novel clustering-based FL framework that retains cluster client similarity despite frequent data drifts. Fielding focuses on re-clustering clients as data drifts, dynamically adjusting the number of clusters while stabilizing responses to minor data drifts. Fielding combines per-client migration with selective global clustering to capture significant distribution patterns without compromising accuracy. We assume the existence of a coordinator, typically deployed on a powerful central server to manage synchronization and advance global model training (Bonawitz et al., 2019; Bagdasaryan et al., 2020). Our solution handles all drifted clients by clustering them according to their local distribution vector, a piece of information readily available all the time. We present a theoretical framework supporting this approach. Fielding is designed to be lightweight; it introduces no additional computation on the client side and requires clients only to transmit their distribution vector (an array with up to a few thousand values) when they register with the coordinator. We implement Fielding by extending the widely adopted FL engine FedScale (Lai et al., 2022) to support streaming data and our clustering mechanisms. We evaluated Fielding with up to 5078 clients on four image streaming traces with data drifts. Compared to existing approaches, Fielding improves model accuracy by 1.9% to 5.9% and achieves the target accuracy 1.16√ó\times√ó to 2.61√ó\times√ó faster. We also show that Fielding is compatible with various client selection and aggregation algorithms, and robust to malicious clients and varied heterogeneity degrees. Our main contributions are: ‚Ä¢ We introduce Fielding, a clustered FL framework that handles data drifts promptly with low overheads. ‚Ä¢ We show that Fielding imposes minimal overhead by eliminating additional client-side computations and reducing communication costs through the use of readily available data, and we provide a theoretical framework to support our approach. ‚Ä¢ We extend the FedScale FL engine to implement Fielding and demonstrate its efficacy on four real-world datasets with up to 5,078 clients, achieving up to 5.9% accuracy improvement and faster convergence compared to existing methods. ‚Ä¢ We show that Fielding is compatible with various client selection and aggregation algorithms and is robust against malicious clients and different degrees of data heterogeneity."
https://arxiv.org/html/2411.01490v1,Anomalous Client Detection in Federated Learning,"Federated learning (FL), with the growing IoT and edge computing, is seen as a promising solution for applications that are latency- and privacy-aware. However, due to the widespread dispersion of data across many clients, it is challenging to monitor client anomalies caused by malfunctioning devices or unexpected events. The majority of FL solutions now in use concentrate on the classification problem, ignoring situations in which anomaly detection may also necessitate privacy preservation and effectiveness. The system in federated learning is unable to manage the potentially flawed behavior of its clients completely. These behaviors include sharing arbitrary parameter values and causing a delay in convergence since clients are chosen at random without knowing the malfunctioning behavior of the client. Client selection is crucial in terms of the efficiency of the federated learning framework. The challenges such as client drift and handling slow clients with low computational capability are well-studied in FL. However, the detection of anomalous clients either for security or for overall performance in the FL frameworks is hardly studied in the literature. In this paper, we propose an anomaly client detection algorithm to overcome malicious client attacks and client drift in FL frameworks. Instead of random client selection, our proposed method utilizes anomaly client detection to remove clients from the FL framework, thereby enhancing the security and efficiency of the overall system. This proposed method improves the global model convergence in almost 50% fewer communication rounds compared with widely used random client selection using the MNIST dataset.","The notion of federated learning (FL) was introduced in 2016 [14]. Its core idea is to train machine learning models on independent datasets dispersed across multiple devices or parties, preserving local data privacy to some level. Since then, FL has grown rapidly and become a popular research area in the field of artificial intelligence [16]. The progress is primarily driven by three factors: the widespread use of machine learning technology, the rapid rise of big data, and global data privacy legislation. Figure 1: Traditional FL Framework Hence, security and privacy are major characteristics of federated learning. For example, FL is susceptible to Byzantine attacks (which try to stop the model from converging) and poisoning attacks (which try to force convergence to an inaccurate model). FL is particularly vulnerable to Byzantine attacks, in which malicious users (Clients) alter trustworthy models or gradients to obstruct learning or purposely contaminate training data, causing the global model to pick up false information [20]. In a traditional FL algorithm as shown in Figure 1, initially, the server sends global model parameters to all the randomly selected clients. The clients run their models with their dataset with the received parameters and send the updated parameter values to the server for aggregation. The aggregated parameter values are again sent by the server to the randomly selected clients. Clients vary greatly in terms of hardware configurations and data distribution in a typical FL environment. As a result, each training round‚Äôs random client sampling may not adequately take advantage of the local updates from heterogeneous clients, which could lead to decreased model accuracy, a slower rate of convergence, compromised fairness, etc. Many client selection methods have been proposed to address the FL client heterogeneity problem, with promising performance improvements. However, none of the client selection methods emphasized identifying the anomalous client and refraining them from participating in the FL network which may also lead to decreased model accuracy, a slower rate of convergence, compromised fairness, and even sabotage the whole FL network. As a solution, we can include another server for maintaining client-server binding or running any privacy-security algorithm to identify the anomalous client, it will enhance the communication and computation complexity of the FL network, which researchers want to minimize for sustainable AI. An anomalous client can be one of the major privacy leakages and (or) security threat scenarios [19]. Three scenarios could result in privacy leaking and (or) security threats if a client is anomalous. Initially, the aggregator can provide the anomalous client with intermediate training updates, which they can use to examine confidential data from other client datasets [2]. Secondly, the anomalous client may submit training updates to the aggregator that are specifically tailored to probe the unique private data of other client datasets. Third, The attacker can be one of the participants in the federated learning, who adversarially modifies his parameter uploads Witsuperscriptsubscriptùëäùëñùë°W_{i}^{t}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. Although these concerns can be mitigated by creating customized training processes, such as choosing a subset of customers for each round, FL is still exposed to a significant risk of anomalous clients leaking confidential information or any other security threat. The client can be malicious or anomalous depending on the type of attack [6]. It is pertinent to mention that whatever the type of attack it is essential to identify the malicious or anomalous clients and refrain them from taking part in the global aggregation because malicious clients can effortlessly incorporate backdoors into the combined model, all the while preserving the model‚Äôs functionality for the primary goal. Moreover, the presence of anomalous clients may render model poisoning. Model poisoning, as opposed to data poisoning, entails a hostile client going for the global model directly [21, 1]. According to research by Bhagoji et al. [1], model positioning attacks have a far greater impact on the model than data poisoning attacks. The modifications made to the client‚Äôs model by the malicious client, which are frequently made in FL before the new model being uploaded to the central server[4], have a direct impact on the global model, as illustrated in Figure 2. Model poisoning affects the performance of the global model and lowers its overall accuracy by manipulating local model gradients using gradient manipulation technologies [9, 8]. For example, in Li et al. [11], when utilizing FL for image recognition, the classifier of an image model may be changed so that it assigns labels chosen by the attacker to specific areas of the image. By teaching rule modification techniques to provide attackers access to the trained model, model poisoning can also be achieved. The attacker can make their attack undetectable by changing the model‚Äôs output, which allows the trained model to be updated normally [9, 10]. Wahab et al. [17] added penalty terms to reduce the discrepancies between the objective functions and appropriate weight update distribution. This enhancement led to the successful deployment of an undetected targeted model poisoning. These motivate us to propose a secure FL algorithm. The proposed FL algorithm not only identifies the anomalous client based on the anomaly score, but this algorithm will also refrain the anomalous clients from participating in global averaging. The main contributions are as follows: ‚Ä¢ We propose a secure federated averaging algorithm to detect anomalous clients. Then refrain from those anomalous clients for further participation. ‚Ä¢ The proposed algorithm is compared with the traditional FedAvg algorithm to validate its performance using iid, non-iid, and non-iid with an unequal number of features. ‚Ä¢ To validate the proposed concept MNIST dataset of handwritten digits is used for the experiment. The proposed algorithm is secure in terms of client selection and assures convergence earlier by 50%. Hence, reduces the communication rounds."
https://arxiv.org/html/2411.01447v1,Privacy-Preserving Customer Churn Prediction Model in the Context of Telecommunication Industry,"Data is the main fuel of a successful machine learning model. A dataset may contain sensitive individual records e.g. personal health records, financial data, industrial information, etc. Training a model using this sensitive data has become a new privacy concern when someone uses third-party cloud computing. Trained models also suffer privacy attacks which leads to the leaking of sensitive information of the training data. This study is conducted to preserve the privacy of training data in the context of customer churn prediction modeling for the telecommunications industry (TCI). In this work, we propose a framework for privacy-preserving customer churn prediction (PPCCP) model in the cloud environment. We have proposed a novel approach which is a combination of Generative Adversarial Networks (GANs) and adaptive Weight-of-Evidence (aWOE). Synthetic data is generated from GANs, and aWOE is applied on the synthetic training dataset before feeding the data to the classification algorithms. Our experiments were carried out using eight different machine learning (ML) classifiers on three openly accessible datasets from the telecommunication sector. We then evaluated the performance using six commonly employed evaluation metrics. In addition to presenting a data privacy analysis, we also performed a statistical significance test. The training and prediction processes achieve data privacy and the prediction classifiers achieve high prediction performance (87.1% in terms of F-Measure for GANs-aWOE based Na√Øve Bayes model). In contrast to earlier studies, our suggested approach demonstrates a prediction enhancement of up to 28.9% and 27.9% in terms of accuracy and F-measure, respectively.","Research in machine learning for decision support has gained significant attention as well as achievement in recent times, due to the availability of large amount of data generated from various sources. Usually, these data are privacy sensitive both from the legal and ethical contexts. These data can be used to train various machine learning models to facilitate decision-making across various domains, encompassing healthcare, manufacturing, education, financial modeling, marketing, and more. For training a model, data processing, analyzing, and performing complex calculations on this large amount of data require huge memory, storage, computational resources as well as efficient algorithms. The cloud service is the cheapest and most available way to store and compute a vast amount of data. Over the past few years, the telecommunications industry (TCI) has experienced significant expansion and technological advancements. TCI industry generates a huge amount of customer data every day and they store the user data in their customer relationship management (CRM) system. The customer data can be used to train the churn prediction model. Due to intense competition, saturated markets, dynamic landscape, and attractive and profitable incentives from competitors, every player in the TCI is confronted with significant challenges related to customer churn, which is widely acknowledged as a formidable problem within this context [1]. Within this competitive market, customers have the flexibility to swiftly change services and even shift from one service provider to another. These customers, who avail these options, are commonly labeled as churned customers [1] in relation to their initial service provider. If a telecommunications company (TELCO) gains the ability to anticipate a customer‚Äôs likelihood of churning, it can potentially cater specific offerings to that customer. This personalized offering approach aims to diminish dissatisfaction, enhance engagement, and consequently improve the chances of retaining the customer. Such actions would significantly contribute to boosting revenue, especially considering the relatively high costs associated with acquiring new customers [2, 3]. At present, TELCOs mainly prioritize retaining their existing long-term customers rather than acquiring new ones [4]. To tackle the customer churn prediction (CCP) problem, several approaches have been presented in the literature (e.g., [4],[5], [6], [7], [8],[9], etc.). All the proposed CCP techniques are based on CRM data which is very privacy sensitive. To focus on their own business, data owners like TELCO often leverage third-party cloud computing facilities for data analysis and machine learning model development [10]. Third-party computation during the model training using these raw data creates a privacy concern. For example, privacy can be breached due to dishonest or curious service providers. Data can also be leaked unintentionally due to data diversity and features during computation time. Sometimes the residual representation of the data is responsible for the data leakage. Adversaries can maliciously achieve unpremeditated but useful and sensitive data from the trained machine learning models via membership inference attacks, model inversion attacks, etc. [11, 12]. Sometimes personal information can be leaked due to the model over-fitting [13]. The sensitive private records should not be leaked from the trained model or during the model training. Protecting the privacy of data is difficult when data owners outsource the machine learning task to a cloud service provider as follows. First, training using plain text data is a security threat. Second, there is a trade-off between privacy and utility of the data, if we use differential privacy or add some noise to the dataset. Third, the cloud server can not work well if we use an encrypted dataset. Our main objective is to preserve data privacy while performing third-party computation without sacrificing performance. Many techniques have been used to protect the sensitive user data from attackers and service providers. Usually, In order to preserve the privacy of the data, the data owner encrypts the sensitive data before uploading it to the cloud server. Subsequently, the cloud server performs some computations on the encrypted data. Public key- private key based encryption is common in the literature to preserve privacy of user data in cloud computation. The most trusted and common encryption schemes are based on homomorphic properties, which allow for certain mathematical operations on encrypted data without decryption. For example, additive homomorphic property based Paillier cryptosystem [14] is used in [15] for privacy-preserving face recognition. The BGN ‚Äòdoubly homomorphic‚Äô encryption [16] is applied in [17] to support the privacy protecting back-propagation algorithm on the ciphertexts. Zhang et al. [18] adopted a fully homomorphic property BGV cryptographic system [19] to carry out the computation through outsourcing. Among the homomorphic methods, fully-homomorphic encryption (FHE) has the ability to provide remarkable privacy. However, FHE introduces significant computational overhead and typically requires large key sizes as well as robust key management system to achieve sufficient security levels. Moreover, FHE algorithms are complex and challenging to implement correctly [20]. On the other hand, training on encrypted data leads to low prediction performance in practice [21]. Few researchers also used data obfuscation techniques to tackle data privacy [22]. Obfuscation means adding or transforming data into a form that makes it more difficult to infer the original data samples. The obfuscation method adds random noise into the original samples or performs data masking with the original samples in such a way that the data becomes unusable for an attacker or an unauthorized personnel. Though obfuscation provides data privacy in a few cases, it loses the data characteristics which leads to a prediction trade-off. Another common way to mitigate the privacy preserving data sharing of sensitive user data is to de-identify the individual records. However, it is well-known that de-identified records can be easily re-identifiable by linking them to other identifiable datasets [23] [24] [25]. To build a machine learning model, real data is not always necessary. To mitigate the need of real data, synthetic data has been used for different purposes. Synthetic data can be generated from real data based on a concept called, differential privacy, using Generative Adversarial Networks (GANs) [26] and differential private Wasserstein generative adversarial networks (DPWGANs) [27]. And then we can say that this synthetic data is differentially private [28] with respect to the original dataset. The differential privacy (DP) proposed by Dwork et al. [29] in 2006 has shown provable privacy guarantees for individual records. Differential privacy (DP) is a strong, mathematical definition of privacy in the context of statistical and machine learning analysis [30]. DP is a criterion of privacy protection that analyzes sensitive personal information. The differentially private synthetic data ensures privacy to a large extent from the cloud service provider as well as assures that the adversaries are unable to assume any information about a single individual sample with high confidence from the output results of the machine learning classifiers. GANs is a powerful way to generate synthetic data but it does not provide any rigorous privacy guarantees. Therefore, in this study, synthetic data has been generated using DPWGANs (a version of GANs) which provides differentially private data. After generating the synthetic data, adaptive Weight-of-Evidence (aWOE) has been applied to it which adds another privacy layer to the datasets as well as improves the prediction performance. To the best of our knowledge, GANs based privacy preserving customer churn prediction has not yet been studied in the literature. This encourages us to test the effect of GANs on data privacy and prediction accuracy for customer churn prediction in the context of the telecommunication industry and this study shows several positive outcomes. In particular, this paper makes the following key contributions: ‚Ä¢ We have applied DPWGAN to generate synthetic data for the training of a CCP model in a privacy preserving manner. ‚Ä¢ We have proposed an adaptive Weight-of-Evidence (aWOE) data transformation method that enhances prediction performance. We have applied the aWOE method on the synthetic data before feeding the data into the machine learning classifiers. ‚Ä¢ We have trained eight different classifiers using synthetic data generated from our proposed GANs-aWOE framework. The classifiers we employed are Na√Øve Bayes (NB), Logistic Regression (LR), K-Nearest Neighbor (KNN), Random forest (RF), Decision tree (DT), Gradient boosting (GB), Feed-Forward Neural Networks (FNN), and Recurrent Neural Networks (RNN). The models have been compared against each other, as well as the state-of-the-art methods on three distinct publicly accessible datasets using a range of information retrieval metrics, including accuracy, specificity, precision, recall, F-measure, and AUC. The outcomes are very promising, and the privacy analysis indicates that the GANs-aWOE technique also offers data privacy assurances. ‚Ä¢ Thus we have put forth a privacy preserving customer churn prediction model (PPCCP) that preserves data privacy as well as improves prediction performances. To the best of our knowledge, this is the first study on data privacy based CCP model in the telecommunication industry. The rest of the paper is organized as follows. Section 2 presents the literature review. The materials and methods that were used in this study are presented in Section 3. Specifically, Subsection 3.7 describes our proposed adaptive Weight-of-Evidence (aWOE), while GANs-aWOE based privacy preserving CCP model is presented in subsection 3.8. Section 4 showcases the experimental results and performance comparisons. The discussion section is provided in Section 5. Finally, we conclude the proposed study with directions to future research in section 6."
https://arxiv.org/html/2411.01357v1,WaKA: Data Attribution using K-Nearest Neighbors and Membership Privacy Principles,"In this paper, we introduce WaKA (Wasserstein K-nearest neighbors Attribution), a novel attribution method that leverages principles from the LiRA (Likelihood Ratio Attack) framework and applies them to kùëòkitalic_k-nearest neighbors classifiers (kùëòkitalic_k-NN). WaKA efficiently measures the contribution of individual data points to the model‚Äôs loss distribution, analyzing every possible kùëòkitalic_k-NN that can be constructed using the training set, without requiring sampling or shadow model training. WaKA can be used a posteriori as a membership inference attack (MIA) to assess privacy risks, and a priori for data minimization and privacy influence measurement. Thus, WaKA can be seen as bridging the gap between data attribution and membership inference attack (MIA) literature by distinguishing between the value of a data point and its privacy risk. For instance, we have shown that self-attribution values are more strongly correlated with the attack success rate than the contribution of a point to the model generalization. WaKA‚Äôs different usage were also evaluated across diverse real-world datasets, demonstrating performance very close to LiRA when used as an MIA on kùëòkitalic_k-NN classifiers, but with greater computational efficiency.","Data attribution (sometimes called data valuation) methods have been developed to measure the contribution of individual data points in a training set to a model‚Äôs output. These methods can serve different purposes depending on the context. One key application is data valuation, where the goal is to quantify the ‚Äúvalue‚Äù of each data point with respect to its impact on the model‚Äôs ability to generalize. For example, Data Shapley Value (DSV), introduced in [1] and [2], is grounded in the game-theoretic Shapley Value framework and is often used for tasks such as data minimization, which refers to the removal of entire data points while ensuring that the model maintains high generalization performance. This approach aligns well with the General Data Protection Regulation (GDPR), which emphasizes that personal data should be ‚Äúadequate, relevant, and limited to what is necessary‚Äù in relation to the purposes for which they are processed (see GDPR Article 5 [3]). The term ‚Äúvalue‚Äù in data valuation is semantically charged, as it suggests an intrinsic worth of data points. In this context, the term is grounded in the Shapley Value axioms, which provide a unique way of attributing the contribution of each data point to the model‚Äôs generalization performance. By ‚Äúfairly‚Äù, we refer to the individual rationality assumption, meaning that each data point is assigned a value based on its marginal contribution to the model‚Äôs performance, ensuring that no point receives less than what it individually contributes to the overall outcome. Data valuation methods such as DSV seek to uncover the intrinsic properties of data in relation to the category of model being used, whether it be a type of neural network, a kùëòkitalic_k-nearest neighbors classifier, or another machine learning model. In contrast, membership inference [4, 5] aims at determining whether a given data point was part of a specific model‚Äôs training dataset. For instance, LiRA [6] is a state-of-the-art approach for performing membership inference attacks (MIAs), which is based on the Likelihood Ratio Test (LRT), a statistical test that compares the likelihood of a model‚Äôs prediction for a given data point when trained with and without this point. To realize this, LiRA requires the training of shadow models via sampling, and contrarily to other MIAs, provides a membership score for each point in the training set, facilitating a more detailed attribution analysis. Related work. More recently, the intersection between data attribution and membership privacy has garnered significant attention as evidenced by a growing body of related literature [7, 8, 9]. One key concept is ‚Äúself-influence‚Äù, which has been investigated in differentiable models to measure the extent to which a data point influences its own prediction. This concept is particularly relevant in MIAs, as high self-influence scores often correlate with increased privacy risks [9]. Self-influence is computed using influence functions and measures how much the loss changes for a data point when it is upweighted. In particular, this method has been used for capturing how a point‚Äôs inclusion can lead to memorization [10], which is in turn relates to privacy vulnerability. Throughout the paper, we adopt a similar notion, which we refer to as ‚Äúself-attribution‚Äù, whose objective is to address the question ‚ÄúTo what extent my data contribute to my own outcome?‚Äù More precisely, it can be quantified by the marginal contribution of a point to the model‚Äôs prediction on that same point. Summary of contributions. Our main contributions can be summarized as follows. ‚Ä¢ We introduce the 1-Wasserstein kùëòkitalic_k-NN Attribution (WaKA), a novel approach drawing inspiration from the LiRA framework, which harnesses the characteristics of kùëòkitalic_k-nearest neighbors (kùëòkitalic_k-NN) models to accurately assess the impact of individual data points on the model‚Äôs loss. WaKA serves a dual purpose: it acts as a general attribution method that provides privacy insights on a training set but can also be adapted as a MIA, referred to as t-WaKA, by exploiting a model‚Äôs loss. ‚Ä¢ In our experiments, we compare the performance of Data Shapley Value and WaKA across two scenarios: attribution using a test set, that we call test-attribution, and self-attribution, which involves predicting a data point‚Äôs own label. These experiments have been conducted on four diverse datasets‚Äîtwo tabular datasets (Adult and Bank) and two unstructured datasets (IMDB and CIFAR10)‚Äîto demonstrate the versatility and robustness of our method. The evaluation of these scenarios focuses on two key aspects: utility, through a data minimization task, and privacy, by measuring the attack success rate (ASR) on all training points. ‚Ä¢ We explore the ‚Äúonion effect‚Äù [11], a phenomenon observed previously in neural networks in which removing data points incrementally reveals deeper layers of vulnerable privacy points in the sense that these points suffer from a higher ASR after the removal. To investigate this effect, we have replicated some of experiments of the original paper by eliminating 10% of the training set using attribution methods, followed by a reassessment of privacy scores. More precisely, we have analyzed the relationship between privacy influences and WaKA influences, showing that they are correlated and can be used to predict, a priori, whether removing a data point will impact the ASR on other points. ‚Ä¢ Finally, we have also conducted experiments using t-WaKA as an MIA on specific training points for kùëòkitalic_k-NN models. t-WaKA displays a similar performance as LiRA but uses significantly less resources as it relies on a single reusable kùëòkitalic_k-NN model trained on the entire dataset, thus avoiding the need for shadow models. More precisely, once this kùëòkitalic_k-NN model is trained, t-WaKA has a computational complexity of O‚Å¢(log‚Å°N)ùëÇùëÅO(\log N)italic_O ( roman_log italic_N ) for attacking a specific point, which is much faster than LiRA for kùëòkitalic_k-NN. Outline. First in Section II, we provide a brief overview of attribution methods (Leave one Out and Data Shapeley Value) for kùëòkitalic_k-NN models as well as LiRA, before introducing the details of the Wasserstein k-NN Attribution (WaKA) method in Section III. Afterwards, in Section IV, we conduct an extensive evaluation of WaKA as a new attribution method and t-WaKA for assessing the success of MIAs before finally concluding with a discussion in Section V."
https://arxiv.org/html/2411.01344v1,"Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy Awareness, Preferences, and Trust in Language Model Agents","Language model (LM) agents that act on users‚Äô behalf for personal tasks can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people‚Äôs capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey (N=300ùëÅ300N=300italic_N = 300), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further uncovered distinct patterns of privacy behaviors, attitudes, and preferences, and the nuanced interactions between privacy considerations and other factors. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.","Recent advances in language models have led to new applications of Language Model agents (LM agents) such as AutoGPT (Richards, 2023), AgentGPT (age, [n. d.]) and personal.ai (Inc, 2024). Unlike basic language models or non-agentic AI systems, LM agents are inherently endowed with agency, allowing them to (semi-)automatically handle complex real-world tasks, such as accessing and retrieving information from connected databases (e.g., a user‚Äôs calendar) to generate and reply to emails (Muthusamy et al., 2023; Talebirad and Nadiri, 2023). These LM agents free users from having to instruct the LM step by step, potentially increasing productivity. However, this increased agency also means that LM agents can make decisions with limited human supervision, which raises new privacy challenges, especially in interpersonal communication where the agents act on behalf of the user to share information with other people. What if LM agents share information the users did not intend to disclose? Prior studies found that even without malicious attackers, LM agents can have unintended privacy leakage in their actions (Mireshghallah et al., 2023; Shao et al., 2024). For example, Shao et al. (2024) demonstrated a case where an LM agent accesses the user John‚Äôs calendar data to generate an email reply, which shares the information that John is ‚Äútalking to a few companies about switching jobs‚Äù in an email to John‚Äôs manager without John‚Äôs explicit consent. This issue not only risks violating one‚Äôs own privacy but can also impact bystanders, as LM agents might inadvertently share information about other people in the users‚Äô connected database. Such unintentional privacy leaks occur because LMs lack the ability to understand and operate under contextual privacy norms, even when privacy-enhanced prompts are used (Shao et al., 2024). Current studies mainly focus on measuring these privacy leakage (Mireshghallah et al., 2023; Shao et al., 2024) and addressing them with model alignment (Staab et al., 2023; Irving et al., 2018; Gabriel, 2020), which aims to align AI models with human value (e.g., people‚Äôs privacy preferences and norms). However, preserving people‚Äôs privacy in the use of LM agents also requires a better understanding of human users, particularly their privacy awareness, preferences, and trust, which are key factors shaping people‚Äôs privacy-preserving behaviors (Olivero and Lunt, 2004; Paramarta et al., 2018; Paspatis et al., 2023; Augustin et al., 2022). Privacy awareness is an important prerequisite for users to make informed decisions about disclosing and sharing data (P√∂tzsch, 2009; Malandrino et al., 2013). A lack of awareness of privacy risks can lead to unexpected consequences, resulting in uncomfortable, creepy experiences or even physical or financial harm (P√∂tzsch, 2009). Moreover, studying people‚Äôs privacy awareness also sheds light on the impact of using different definitions of user preferences in the model alignment efforts. Gabriel (2020) introduced multiple ways to measure user preference for model alignment, such as revealed preferences, focusing on model alignment with preferences as they are revealed through a person‚Äôs behaviour, and informed preferences, focusing on the preferences that a person would have if they were fully informed and rational. However, in light of ‚Äúprivacy paradox‚Äù (Kokolakis, 2017; Solove, 2021), the revealed preferences (their actual behavior) and informed preferences might be different. A key factor contributing to the privacy paradox is the lack of privacy awareness (P√∂tzsch, 2009; Deuker, 2010), where users are not fully aware of how their data is being used or what risks they are exposed to, leading to inaction despite a desire to protect their privacy (P√∂tzsch, 2009; Deuker, 2010; Dienlin and Trepte, 2015). Considering that the revealed preferences provide a practical measure for optimizing the LM agents‚Äô actions by observing human behavior, while informed preferences are closer to individuals‚Äô authentic preferences but are not directly observable and more costly to collect (Hadfield-Menell et al., 2016), studying users‚Äô privacy awareness helps bridge the gap between these two types of preferences and guide model alignments closer to users‚Äô true preferences in practice. Trust is a more direct factor that influence people‚Äôs willingness to disclose information and engage with technologies (Shin et al., 2022; Dimodugno et al., 2021). Higher levels of trust can lead to more willingness to disclose information (Joinson et al., 2010; Zimmer et al., 2010), while overtrust, often associated with ‚Äúmisunderstanding the risks associated with an action‚Äù (Wagner et al., 2018), can result in people overlooking privacy risks and becoming vulnerable to privacy violations (Zimmer et al., 2010; Wang and Lin, 2017). Conversely, a lack of trust can hinder the adoption of technology and limit its benefits in people‚Äôs daily lives. Research found that people are less likely to continue using AI systems if these systems make mistakes (Alexander et al., 2018). This highlights the importance of developing LM agents that can help users establish and calibrate their trust in delegating different levels of tasks to the LM agents, ensuring that the agentic systems are used effectively in people‚Äôs everyday life in a privacy-friendly way. Although privacy awareness, preference and trust are key elements in privacy research and have been studied across various domains such as online browsing (Schaub et al., 2016; Wills and Zeljkovic, 2011), e-commerce (Windl et al., 2022; Bergmann, 2008; Spiekermann et al., 2001), IoT and mobile sensing (Thakkar et al., 2022; Wang and Lin, 2017; Prange et al., 2021) and AR/VR (Cummings and Shore, 2024; Do et al., 2023), the nature of LM agents brings unique challenges. The lack of transparency due to the automation of complex tasks makes it difficult for people to know what happens behind the scenes. Moreover, people could hold flawed mental models of the LM systems (Zhang et al., 2024), which leads to incorrect expectations of the systems‚Äô behavior. These challenges could hinder people from understanding or anticipating unintended privacy leakage in the LM agent‚Äôs actions (Zhang et al., 2024; Li et al., 2024a). Having in mind both the importance of user privacy awareness, preferences, and trust, and the challenges raised by LM agents, this work aims to study these factors in LM agents, focusing on the unintended privacy leakage risk in asynchronous interpersonal communication tasks (e.g., writing emails and creating social media posts). We are interested in examining the following research questions: RQ1: What are people‚Äôs primary concerns with LM agents, and what is the role of privacy? RQ2: How well do people oversee privacy leakage in LM agents‚Äô actions, and how does it relate to their privacy awareness and trust in AI? RQ3: How do people‚Äôs behaviors (revealed preferences) differ from their informed preferences? How different are these preferences from person to person? To answer these questions, we conducted a task-based online survey with 300 participants. Each participant was asked to complete an asynchronous interpersonal communication task, such as drafting an email response. They were then introduced to an LM agent and asked to choose between their own responses and a response generated by an LM agent, which included sensitive items that violate contextual privacy norms. They were then asked to explain their reasoning. Afterward, participants were informed of the privacy norm tuples in the scenario and asked to rate the perceived harmfulness of the sensitive items, which we used as the ground truth to evaluate each individual‚Äôs privacy leakage. Our results show that participants often included fewer privacy leaks in their own draft, while many selected the response written by LM agents. 48.0% of participants preferred the LM agent‚Äôs response or considered both responses equally good, despite the agent‚Äôs response containing more privacy leaks, leading to an increase in privacy leakage from 15.7% to a range between 38.4% (only considering the strict preference of the agent‚Äôs response) and 55.0% (also considering the ‚Äúboth are good‚Äù selections). Most appeared to be unaware of the privacy leaks in the agent‚Äôs actions according to their selection rationales, exhibiting an overtrust toward the LM agent. These findings suggest that putting users in control of the final action of an LM agent is not adequate to prevent privacy risks, alluding to broader challenges in human oversight of AI systems. In addition to the known issues related to humans supervising a system handling complex, domain-specific tasks (Burns et al., 2023), there are also risks associated with the lack of capacity to oversee everyday tasks involving nuanced social norms. To better understand the reasoning behind people‚Äôs disclosure behaviors and explore the issues of overtrust and a lack of privacy awareness in agentic systems, we conducted a clustering analysis to categorize participants into four privacy profiles, based on their behaviors, attitudes, preferences, and decision-making rationales. We found that only the group Privacy Advocate (28.3%) was able to recognize the privacy leaks in the LM agent‚Äôs actions and successfully avoided more leaks when using the agent. On the other hand, participants in the Humanity Proponent (25.0%) and AI Optimist (37.0%) groups were unaware of these leaks and primarily considered the AI‚Äôs capabilities in their decision-making. Although Humanity Proponent did not leak more information, their rationales were more rooted in their fondness of the ‚Äúhumanness‚Äù of their own response and relatively low trust in AI, which overshadows the privacy issues and suggests that they still remain vulnerable to privacy risks, especially when AI capabilities (e.g., personalized expression) improve. Finally, we noticed a smaller group, ‚ÄúPrivacy Paradox‚Äù (9.7%). Similar to the typical privacy paradox phenomenon, these people perceived some information items as harmful to leak when prompted about them while including the exact items in their own draft. We further studied users‚Äô trust in the agent and comfortableness of delegating different levels of tasks to it among the four groups, and found that both the Privacy Advocate and Humanity Proponent experienced a significant decrease of trust in the agent and the comfortableness of delegating AI to draft the response and automatically send the message after the task. Finally, we found frequent disclosure of information in the users‚Äô draft (i.e., revealed preferences), which people rated as harmful (i.e., informed preferences) afterward. This phenomenon, named subjective privacy leakage, indicates a discrepancy between users‚Äô revealed preferences and informed preferences. This discrepancy raises discussions about how alignment goals should be operationalized to respect users‚Äô preferences while still allowing for practical solutions. Furthermore, we also uncovered varied levels of consistency between the subjective perceptions of harmfulness and the general privacy norms, as well as the inter-participant variance of preferences, across different scenarios. These findings highlight the complex and individualized nature of privacy preferences, raising important questions about how to design LM agents that respect diverse user preferences and build calibrated trust with various users, contributing empirical insights that can guide further efforts of personalization and bidirectional alignment (Shen et al., 2024) in agentic systems. In summary, our key contributions include: 1 A task-based survey study (N=300ùëÅ300N=300italic_N = 300) on humans‚Äô privacy awareness, preferences, and trust in LM agents, focusing on the unintended privacy leakage in asynchronous interpersonal communication tasks. This is the first study examining humans‚Äô capacity to supervise LM agents‚Äô privacy practices. 2 Four privacy profiles that reflect distinct patterns of users based on privacy behaviors, awareness, and preferences, as well as the interaction between privacy considerations and other factors, which can inform the design of tailored controls and education for different user groups of LM agents. 3 An analysis of the relationships among different ways to define and measure privacy preferences and discuss the implications on bidirectional alignment for privacy in LM agents."
https://arxiv.org/html/2411.01329v1,Cloned Identity Detection in Social-Sensor Clouds based on Incomplete Profiles,"We propose a novel approach to effectively detect cloned identities of social-sensor cloud service providers (i.e. social media users) in the face of incomplete non-privacy-sensitive profile data. Named ICD-IPD, the proposed approach first extracts account pairs with similar usernames or screen names from a given set of user accounts collected from a social media. It then learns a multi-view representation associated with a given account and extracts two categories of features for every single account. These two categories of features include profile and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features that may potentially contain missing values. To counter the impact of such missing values, a missing value imputer will next impute the missing values of the aforementioned profile and WGCCA-based features. After that, the proposed approach further extracts two categories of augmented features for each account pair identified previously, namely, 1) similarity and 2) differences-based features. Finally, these features are concatenated and fed into a Light Gradient Boosting Machine classifier to detect identity cloning. We evaluated and compared the proposed approach against the existing state-of-the-art identity cloning approaches and other machine or deep learning models atop a real-world dataset. The experimental results show that the proposed approach outperforms the state-of-the-art approaches and models in terms of Precision, Recall and F1-score.","Social-sensor cloud services (SocSen services) refer to services whose functional (e.g. time and location) and non-functional (e.g. quality and trust) characteristics are abstracted from data (e.g. texts, images, videos, etc.) posted in social media [1]. These SocSen services can power numerous socially significant and influential applications such as scene reconstruction from social media images, etc. The identities of SocSen service providers (i.e., individuals that post social media data from social media) have increasingly become a target of the cybercriminals in the recent past [2, 3]. One such example of these crimes associated with SocSen service provider identities (i.e. social media users) is identity cloning, which is an attempt by an adversary to steal the identity information of SocSen service providers to register a fake profile. Many recent attempts for identity cloning in social media platforms aimed to exploit SocSen service provider identities via cloning for either theft for financial fraud or deceiving the public. Recent examples illustrate the severity of this problem: Facebook CEO Mark Zuckerberg‚Äôs account was cloned for financial theft111https://www.nytimes.com/2018/04/25/technology/fake-mark-zuckerberg-facebook.html, and a fake Twitter account impersonating Russian President Vladimir Putin gained over one million followers222https://www.abc.net.au/news/2018-11-29/twitter-suspends-account-impersonating-vladimir-putin/10569064. These incidents highlight the critical need for effective measures to detect and prevent identity cloning and other malicious activities. Ensuring the security of social media platforms is essential not only for protecting individual identities but also for maintaining the integrity and trustworthiness of online interactions. Therefore, it is imperative to put in place measures to detect such attempts to keep attackers at bay and make social media a more secure place for social media users. Despite its importance, most social media platforms do not offer automated and integrated identity cloning detection. For instance, Instagram and Twitter currently selectively evaluate identity cloning claims only upon receiving legitimate complaints from end-users 333https://help.instagram.com/446663175382270444https://help.twitter.com/en/rules-and-policies/twitter-impersonation-policy. However, given the rate at which identity cloning attacks occur, such selective approaches can be deemed inadequate to keep social media a safer environment for social media users. Therefore, it is vital to research more proactive and automated approaches that can also withstand the scale at which social media platforms operate. Most existing identity cloning detection approaches (such as [4, 5, 6, 7]) rely on complete SocSen service provider (i.e. social media user) profile data. The performance of these approaches often depends on the availability of comprehensive social media profile information. However, obtaining a comprehensive representation of such profile data is often infeasible due to various reasons. One of the major reasons is that SocSen clouds enable stronger privacy preservation measures not to disclose such information to third-party applications. For example, there has been a growing trend that more third-party websites/apps employ mainstream SocSen cloud APIs for authentication. These websites/apps can only access limited profile information authorized by SocSen clouds. This information is termed as non-privacy-sensitive profile information [8]. Our previous research [8, 9] focuses on developing identity cloning detection approaches based on SocSen service providers‚Äô non-privacy-sensitive profile information. However, SocSen service providers can even opt not to disclose part of the non-privacy-sensitive profile information. For example, during account registration, SocSen clouds such as Twitter have made it mandatory that users provide a username, screen name, email address and phone number, which are known as required fields555https://help.twitter.com/en/using-twitter/create-twitter-account. The users can still opt out of providing the other optional details, such as description, location, etc., which can be accessed by Twitter API. Under such circumstances, cloned user accounts might not expose their full profile information or non-privacy-sensitive profile information in order to reduce the risk of being detected. For example, an adversary can register a cloned profile without including a profile description or adding any post. Therefore, existing identity cloning detection approaches may either fail or perform less in the face of incomplete user profile data since most of the existing approaches are built based on the prerequisite of the existence of the complete profile information or non-privacy-sensitive profile information. According to our experiment results (see Table IX), all the existing identity cloning detection approaches are affected by incomplete profile information. All the existing approaches performed worse when there was incomplete profile information (missing value). Imputation is a technique used to handle missing or incomplete data by filling in the gaps with substitute values. Imputation can be performed using statistical or machine learning methods [10]. To address these issues, we use imputation methods to replace missing values with appropriate estimates. By applying this technique, we can improve the quality of the data and enhance the detection effectiveness. To address the above limitations, we propose a novel approach for SocSen service provider Identity Cloning Detection in the face of Incomplete Profile Data (ICD-IPD). ICD-IPD is specially designed to detect cloned identities based on incomplete non-privacy-sensitive profile information. ICD-IPD consists of five main components, namely, 1) account pair generator (APG), 2) a multi-view learner, 3) a missing value imputer, 4) an account pair feature generator and 5) a prediction model. From a given set of social media users, the APG generates account pairs that share similar screen names or usernames. The multi-view learner then combines multi-view information of an account to improve learning performance. More specifically, it extracts profile (i.e. friends and posts count etc.) and Weighted Generalised Canonical Correlation Analysis (WGCCA)-based features (i.e. combination of multi-view) from a SocSen service provider‚Äôs non-privacy-sensitive profile information. Next, the missing value imputer imputes the missing feature values associated with profile and WGCCA-based features. The account pair feature generator then extracts similarity and differences-based features for each account pair in terms of the imputed feature values. Finally, ICD-IPD utilises a Light Gradient Boosting Machine (LightGBM) model atop a concatenated form of the aforementioned features to predict whether a pair of accounts compared possibly consists of a cloned account and a victim account. Our main contributions can be summarized as follows: ‚Ä¢ We propose a novel approach to detect SocSen service providers‚Äô identity cloning based on incomplete non-privacy-sensitive profiles. To the best of our knowledge, this is the first work in the field of social media identity deception information that specifically works on user profiles with missing values (incomplete profile data). ‚Ä¢ We utilize an imputation approach to impute the missing value of incomplete non-privacy-sensitive profile data. The utilised imputation approach can substantially enhance the cloned identity prediction performance as shown in Section 4. ‚Ä¢ We adopt an effective prediction model for detecting cloned identities with missing non-privacy-sensitive profile information. The proposed prediction model shows better performance than the state-of-art cloned identity detection approaches as well as several other candidate machine and deep learning models. ‚Ä¢ We present the results of our extensive experiments carried out atop a real-world dataset. The experimental findings showed that ICD-IPD outperforms current cloned identity detection approaches on the Key Performance Indicators: Precision, Recall, and F1-score. The remainder of the paper is structured as follows. Section 2 reviews the related work on identity cloning detection. Section 3 elaborates our proposed approach to address the challenges outlined previously. Meanwhile, Section 4 provides comprehensive details on the methodology used to evaluate the proposed approach and outcomes. Section 5 concludes the paper."
https://arxiv.org/html/2411.01313v2,False Data Injection Attack Detection in Edge-based Smart Metering Networks with Federated Learning,"Smart metering networks are increasingly susceptible to cyber threats, where false data injection (FDI) appears as a critical attack. Data-driven-based machine learning (ML) methods have shown immense benefits in detecting FDI attacks via data learning and prediction abilities. Literature works have mostly focused on centralized learning and deploying FDI attack detection models at the control center, which requires data collection from local utilities like meters and transformers. However, this data sharing may raise privacy concerns due to the potential disclosure of household information like energy usage patterns. This paper proposes a new privacy-preserved FDI attack detection by developing an efficient federated learning (FL) framework in the smart meter network with edge computing. Distributed edge servers located at the network edge run an ML-based FDI attack detection model and share the trained model with the grid operator, aiming to build a strong FDI attack detection model without data sharing. Simulation results demonstrate the efficiency of our proposed FL method over the conventional method without collaboration.","The smart grid has an advanced power system based on enabling reliability, efficiency, and accuracy in the power supply system. Advanced metering infrastructure (AMI) is the most important part of achieving this goal, where smart meters (SMs) are used to measure energy consumption to facilitate data analytics [1, 2]. Due to the open and distributed nature of AMI, SMs are vulnerable to cyber threats including false data injection (FDI) attacks that aim to modify state estimation and energy consumption recordings at SMs [3]. This results in financial loss and poses energy recordings under threats. For example, personal information can be compromised into the meter measurement by cybercriminals for data theft and financial profit. Hence, developing an efficient solution to detect FDI attacks is of paramount importance for safe smart metering networks. I-A Related Works Recently, machine learning (ML) has been employed as an efficient technique to learn and detect FDI attacks in smart grids due to its data learning and prediction abilities. For example, neural networks were employed in [1] for estimating anomalies in meter measurements and detecting falsified system states. Another work in [4] employed deep neural networks (DNNs) to extract features of FDI attackers in AC power transmission system simulations. Further, the work in [5] studied FDI attack with DL for transmission-line protective relays and substation automation systems. These works typically use centralized learning, deploying FDI attack detection models at the control center, which requires data collection from local utilities like meters and transformers. However, end customers may be unwilling to share their meter measurements for attack analysis due to privacy concerns. Moreover, FL has been investigated for collaborative FDI attack detection in smart grids. The authors in [3] worked in the smart grid to detect the attack detection in the solar farms. Another author in [6] mentioned the power grid state estimation on unknown system parameters and small decentralized data sets with strategic data owners. The author in [2] illustrates the view on detecting the attacks on the power system focused on privacy-preserving using federated Learning. The FDI attack detection to tackle data privacy enhancing cross-silo federated learning was proposed in [7]. Despite these research efforts, the application of FL in FDI attack detection in edge-based smart metering networks has not been investigated. I-B Motivation and Key Contributions Motivated by the aforementioned limitations, we present a new FDI attack detection method using an FL framework for edge-based smart metering networks. Edge computing that offers low-latency services is useful for timely running FDI attack detection models for SMs. The use of SMs has become more reliable nowadays, but there are security threats to the use of SMs. Conversely, Some basic steps have been taken to secure the network between the clients, meter, and authority. So our concern is to secure the bond between the clients, SMs, and the authority. We found some slide works, like as this research shows the existing methods for identifying FDI attacks only identify the presence of the condition;[8] they provide crucial information regarding the precise injection locations. Motivated by the latest developments in deep learning, they put forth a deep-learning-based locational detection architecture (DLLD) to accurately identify the precise locations of FDIA instantaneously. A typical bad data detector (BDD) and a convolutional neural network (CNN) are combined in the DLLD architecture. Low-quality data is eliminated using the BDD. In order to identify the co-occurrence dependency and inconsistency in the power flow data caused by prospective attacks, the following CNN is used as a multiple-label classifier. FL uses a central server to compile all client updates. This can cause snags and a decline in performance, particularly when there are a lot of clients. Furthermore, communication delays may arise since each iteration requires all clients to communicate with the central server, particularly for distant clients or crowded networks. In a nutshell, the main contributions of this paper are summarized as follows. 1. We propose a privacy-preserved FDI attack detection method over edge-based smart metering networks, by developing an efficient FL framework across distributed edge computing servers. Our method allows for training a global FDI attack detection model at the grid operator‚Äôs server with good generalization. 2. We explicitly analyze our method design and algorithm development. We focus on developing a robust FDI attack detection model without sharing data by having distributed ESs, located closer to customers, running an ML-based FDI attack detection model, and sharing the trained model with the grid operator. Hence, our method can achieve FDI attack detection without data sharing for data privacy preservation. 3. We validate our proposed framework using the IEEE 14-bus system, showing that the DLLD achieves an average detection accuracy of 88%. This demonstrates that federated learning is highly accurate, scalable, and robust. I-C Paper Organization The remainder of this paper is organized as follows. We present our power system state estimation architecture in Section II. We present our federated learning (FL) framework in the SMs network with edge computing in Section III. In Section IV, we present the simulation and evaluation of our proposed method compared with the existing methods. Section VI concludes the paper. Key acronyms used in this paper are summarized in Table I. TABLE I: List of key acronyms Acronym Definition FL Federated Learning ES ES BDD Bad Data Detector CNN Convolutional Neural Networks DLLD Deep Learning based Locational Detection SM Smart Meter FDIA False Data Injection Attack ML ML SCADA Supervisory Control and Data Acquisition EMS Energy Management Systems AC Alternating Current DC Direct Current WLS Weighted Least Squares SGD Stochastic Gradient Descent"
https://arxiv.org/html/2411.01312v2,From Federated Learning to Quantum Federated Learning for Space-Air-Ground Integrated Networks,"6G wireless networks are expected to provide seamless and data-based connections that cover space-air-ground and underwater networks. As a core partition of future 6G networks, Space-Air-Ground Integrated Networks (SAGIN) have been envisioned to provide countless real-time intelligent applications. To realize this, promoting AI techniques into SAGIN is an inevitable trend. Due to the distributed and heterogeneous architecture of SAGIN, federated learning (FL) and then quantum FL are emerging AI model training techniques for enabling future privacy-enhanced and computation-efficient SAGINs. In this work, we explore the vision of using FL/QFL in SAGINs. We present a few representative applications enabled by the integration of FL and QFL in SAGINs. A case study of QFL over UAV networks is also given, showing the merit of quantum-enabled training approach over the conventional FL benchmark. Research challenges along with standardization for QFL adoption in future SAGINs are also highlighted.","In the past decades, terrestrial wireless communication systems have developed explosively, making distinguished contributions and setting the foundation for the development of human civilization. The development of mobile communication systems has gone through 5 generations, from 1G in the 1980s to 5G in the early 2020s. However, as an inevitable development trend, 6G is expected to be launched in the 2030s to provide extreme data rate network services and realize real-time IIoT applications [1]. The architecture of 6G will completely cover Space-Air-Ground and underwater integrated networks [2]. As a core partition of future 6G networks, Space-Air-Ground Integrated Networks (SAGIN) have been envisioned to provide countless real-time intelligent applications. To realize this, promoting AI techniques into SAGIN is an inevitable trend. Due to the distributed and heterogeneous architecture of SAGIN, FL is emerging and becoming one of the most promising AI models for SAGIN. However, employing FL in SAGIN still faces some challenges. First, due to the large size and complexity of SAGIN, a large number of devices are required to collaborate to train a common model using an extremely large dataset. Moreover, as the dataset size and the required model complexity increase, traditional client devices may not be able to perform the local ML training efficiently. Therefore, developing efficient FL solutions capable of handling large datasets and complex ML models in SAGIN is significantly important. Second, client devices in SAGIN rely on radio transmissions, which can be subject to potential eavesdropping attacks due to long-distance transmission. The nature of radio transmission may allow malicious nodes to eavesdrop on wireless signals and obtain model gradients by brute force cracking, thus inferring the private information of clients. Moreover, handling big data and complex AI models and ensuring privacy and security in the long-distance transmission of AI models also represent significant challenges. To address these issues, we propose integrating FL into the SAGIN architectural framework to optimize system resource utilization, reduce communication latency and energy consumption, and enhance privacy and security, thereby realizing real-time, smart, and green 6G SAGIN-based IIoT applications. Figure 1: An illustrated Architecture of FL-based SAGIN. From a computational perspective, FL is a powerful distributed learning model that eliminates the limitations of traditional centralized learning models. The combination of FL and SAGIN architecture provides irreplaceable high-performance computing solutions in a series of domains such as military, UAV, ITS, real-time systems or complex mobile network architectures, where training efforts can be challenging for traditional computational techniques. Fig. 1 presents a common FL-based SAGIN framework. After a finite number of training rounds, the system will converge, and the aggregator will send an optimal global model to IoT devices to ensure that the aggregator and all partners use the same training mode. Consequently, FL-based SAGIN provides a promising approach for these applications. From the communications perspective, FL-based SAGIN systems are emerging as one of the promising 6G solutions to address the security and privacy challenges of traditional communications systems. Integrating FL and SAGIN architecture aims to take advantage of FL‚Äôs unique characteristics, including distributed, security, and system resource optimization. Recently, FL-based SAGIN frameworks have been focused on research. Fig. 2 shows an FL-based UAV-Ground scheme. However, a comprehensive picture of enabling Space-Air-Ground Integrated Networks via FL has not been fully considered. Recently, due to the advances in quantum computing technologies, quantum FL (QFL) has emerged as a powerful solution for enabling next-generation SAGINs. QFL leverages the strengths of the distributed learning approach of FL and exponential speed enhancements characteristic of quantum computing. In SAGIN models, utilizing the satellite as the central FL server offers a promising strategy. However, employing FL in SAGIN faces challenges in managing vast datasets and training complex ML models. The work in [3] proposes a quantum-enabled FL architecture for SAGIN that uses quantum relays and variational quantum algorithms (VQA). To effectively handle big datasets and intricate models within FL, the framework makes use of VQA-based machine learning for local training, all the while maintaining improved data security and privacy. It also includes a quantum relay technique to securely communicate machine learning models across large distances via quantum teleportation, supported by UAVs and HAPS. The practicality and effectiveness of this strategy are confirmed by numerical findings from a case study, which highlights the possibility of combining quantum technologies with FL in upcoming 6G networks. This paper presents a quantum-enabled FL architecture for SAGIN that makes use of quantum relays and VQA. To effectively handle big datasets and intricate models within FL, the framework makes use of VQA-based machine learning for local training, all the while maintaining improved data security and privacy. It also includes a quantum relay technique to securely communicate machine learning models across large distances via quantum teleportation, supported by UAVs and HAPS. The practicality and effectiveness of this strategy are confirmed by numerical findings from a case study, which highlights the possibility of combining quantum technologies with FL in upcoming 6G networks. The main contributions are summarized as follows. ‚Ä¢ We present recent advances in SAGINs and describe the new vision of FL and QFL in SAGINs, where the significance of privacy-preserving model training offered by FL and fast model training offered by QFL is highlighted. ‚Ä¢ We discuss a few representative applications enabled by integrating FL and QFL in SAGINs. A case study of QFL over UAV networks is also given. ‚Ä¢ Future research challenges are also discussed, where standardization for QFL adoption in future SAGINs is highlighted."
https://arxiv.org/html/2411.01252v1,Quantum Token Obfuscation via Superposition,"As quantum computing advances, traditional cryptographic security measures, including token obfuscation, are increasingly vulnerable to quantum attacks. This paper introduces a quantum-enhanced approach to token obfuscation leveraging quantum superposition and multi-basis verification to establish a robust defense against these threats. In our method, tokens are encoded in superposition states, making them simultaneously exist in multiple states until measured, thus enhancing obfuscation complexity. Multi-basis verification further secures these tokens by enforcing validation across multiple quantum bases, thwarting unauthorized access. Additionally, we incorporate a quantum decay protocol and a refresh mechanism to manage the token life-cycle securely. Our experimental results demonstrate significant improvements in token security and robustness, validating this approach as a promising solution for quantum-secure cryptographic applications. This work not only highlights the feasibility of quantum-based token obfuscation but also lays the foundation for future quantum-safe security architectures.","In recent years, rapid advancements in quantum computing have challenged traditional cryptographic techniques, prompting significant research into quantum-resistant algorithms. Classical obfuscation methods, while effective against conventional attacks, fall short against the unique challenges posed by quantum algorithms that exploit superposition and entanglement to break encryption and token-based security measures [1, 2]. Consequently, enhancing obfuscation to defend against quantum attacks is essential to future-proof digital security. Despite advancements in quantum-safe cryptographic research, there remains a lack of robust methodologies specifically targeting token obfuscation under quantum conditions. Conventional methods depend heavily on computational complexity, which quantum algorithms like Shor‚Äôs and Grover‚Äôs can disrupt, reducing the time required to break encryption keys or bypass token obfuscation [3, 4]. Current quantum-resistant strategies, such as lattice-based and hash-based cryptography, do not fully address obfuscation needs for tokens or secure key generation, which are integral to sensitive digital transactions and communications [5, 6]. This study addresses the gap in quantum-resistant token obfuscation by proposing a superposition-based approach to token obfuscation that integrates quantum principles of superposition and multi-basis verification. By encoding tokens into superposition states, the proposed method complicates token discovery and interpretation by unauthorized parties, as these tokens only resolve to a specific value upon measurement in a defined basis [7]. Our approach also includes multi-basis verification, which prevents token verification from being limited to a single quantum basis, adding an additional layer of security [8]. Moreover, this method introduces a quantum decay protocol and a token refresh mechanism, which actively manage the lifecycle of tokens, enforcing dynamic obfuscation and reducing the risk of prolonged exposure or token replay attacks. By leveraging both quantum state management and basis complexity, our method offers a novel and promising approach to secure token obfuscation in a post-quantum context. Contributions This paper‚Äôs contributions are threefold: ‚Ä¢ Superposition-Based Token Encoding: We present a quantum obfuscation technique where tokens exist in superposition states, obfuscating their specific values until a basis-constrained measurement is performed. ‚Ä¢ Multi-Basis Verification Protocol: Our protocol ensures that tokens are validated across multiple quantum bases, making unauthorized verification computationally prohibitive for adversaries. ‚Ä¢ Quantum Decay and Refresh Mechanisms: We introduce protocols that manage token lifespan, reducing vulnerability to token replay and improving resilience against prolonged token exposure. These contributions collectively advance the field of quantum-safe security by providing a framework for obfuscating tokens in a manner resilient to quantum-based cryptographic attacks."
https://arxiv.org/html/2411.01161v1,Federated Learning with Relative Fairness,"This paper proposes a federated learning framework designed to achieve relative fairness for clients. Traditional federated learning frameworks typically ensure absolute fairness by guaranteeing minimum performance across all client subgroups. However, this approach overlooks disparities in model performance between subgroups. The proposed framework uses a minimax problem approach to minimize relative unfairness, extending previous methods in distributionally robust optimization (DRO). A novel fairness index, based on the ratio between large and small losses among clients, is introduced, allowing the framework to assess and improve the relative fairness of trained models. Theoretical guarantees demonstrate that the framework consistently reduces unfairness. We also develop an algorithm, named Scaff-PD-IA, which balances communication and computational efficiency while maintaining minimax-optimal convergence rates. Empirical evaluations on real-world datasets confirm its effectiveness in maintaining model performance while reducing disparity.","Federated learning is a regime of machine learning to train models with privacy preservation of data of clients (Koneƒçn·ª≥ et al., 2016a, b; McMahan et al., 2017; Yang et al., 2020; Kairouz et al., 2021). In the scheme of federated learning, models are trained just with communications regarding their weights, and thus the local data of each client are not collected into a centralized server. This decentralized regime has advantages over the centralized one, such as privacy preservation and reduction of communication costs. These advantages make federated learning applicable in many different fields, for example, the Internet of Things (Nguyen et al., 2021), healthcare (Xu et al., 2021), and finance (Long et al., 2020). Figure 1. Illustration of absolute fairness and relative fairness. In the left panel, distances from the eight gray points to the pink/blue center point are measured, respectively, and the right figure shows the distribution of those distances. While the pink distribution achieves absolute fairness which minimizes of the maximum distance, the blue distribution achieves relative fairness which minimizes the discrepancy among the distances. Section A shows more details. The fairness of trained models has received special attention in federated learning due to distributed structures. While fairness in machine learning is multifaceted (Mehrabi et al., 2021; Pessach and Shmueli, 2022; Caton and Haas, 2023; Barocas et al., 2023), some studies in federated learning take particular notes of fairness in the enjoyed performance of trained models among subgroups of clients. This fairness is significant regardless of regime, but its achievement in federated learning is more difficult than in centralized one. A cause of the difficulty is that the distribution of training attendees can be biased against that of the population of interest (participation gap), which results in favoring majorities of attendees (Yuan et al., 2022). Another cause is that empirical risks, typical loss functions in federated learning, reflect the preference of the majority even if no participation gap exists. A significant challenge in fairness in federated learning is the control of relative fairness. Fairness in these previous studies means guarantees for minimal performance in any subpopulation; therefore, it is an absolute fairness. These guarantees are not sufficient to ensure relative fairness among subpopulations (e.g., differences or ratios of performances). See Figure 1 for the illustration of the difference. One of the worst potential consequences of models lacking relative fairness is the digital divide, a typical failure of technology in society; therefore, methods for measuring and realizing this type of fairness are highly demanded. While relative fairness has been the subject of studies on general machine learning (for example, Pessach and Shmueli, 2022; Barocas et al., 2023) and federated learning (Li et al., 2020), it is still a developing issue. A key approach to address these issues is distributionally robust optimization (DRO) as several studies (Mohri et al., 2019; Deng et al., 2020; Yu et al., 2023); however, the usual DRO is insufficient to consider relative fairness among subpopulations. In this study, we develop a federated learning framework to achieve relative fairness. This framework utilizes a minimax problem, which can be regarded as an extension of DRO, where the parameters are learned to minimize relative unfairness. The model trained under this framework consistently reduces relative unfairness compared to conventional methods. Additionally, we develop an algorithm for this learning process, which, through several innovations, balances stable learning with communication and computational efficiency. Moreover, the algorithm maintains a minimax-optimal convergence rate with respect to the number of updates. Our learning framework consists of the following steps. First, we develop a generalized relative unfairness index, which is defined by the ratio of large losses to small losses within the set of clients. This index can reproduce several well-known fairness measures, such as the Atkinson index (Atkinson, 1970). Second, we develop an approximation of this relative unfairness index that can be efficiently computed by machine learning algorithms, reducing it to a type of DRO problem. Third, we develop an algorithm, Scaff-PD-IA, which extends the gradient update algorithms for existing DRO problems. We present both theoretical and experimental evidence to verify the superiority of the developed framework and algorithm. First, the proposed learning framework consistently reduces relative unfairness. This reduction is demonstrated by changes in the hyperparameters involved in the algorithm. The reduction in unfairness is supported by both theoretical and experimental results. Second, the developed algorithm Scaff-PD-IA performs sufficiently well. Theoretically, this is shown by the the minimax optimal convergence rate of the algorithm. Experimentally, we demonstrate that the developed framework and algorithm reduce unfairness while maintaining performance through experiments using several real-world datasets. Furthermore, we analyze the generalization error of the trained model and clarify the impact of learning through the proposed framework on the prediction performance for unseen data. The contributions of this study are summarized as follows. Framework We propose the learning frame of DRO for handling the relative fairness by developing a measure of the relative (un)fairness. This measure is based on the relative unfairness index, which connects to several commonly used fairness measure. Algorithm We propose the algorithm Scaff-PD-IA, which efficiently solves the proposed framework of DRO. This algorithm employs several techniques to make the learning result stable and effective. Theoretical Guarantee There several theoretical results: (i) we shows that our proposed framework always reduces the relative unfairness, (ii) we shows that the proposed algorithm Scaff-PD-IA achieves the minimax optimal convergence rate in terms of a number of updates, and (iii) we examine generalization errors in our setting combined with a supervised learning setting. Empirical Evaluation We finally analyze the empirical performance of several algorithms including Stochastic-AFL, DRFA, and Scaff-PD in fairness under several settings of problems. 1.1. Related Works Distributionally robust optimization (DRO) is a classical topic in optimization theory and also gathers the interest of the statistics and machine learning communities; see Rahimian and Mehrotra (2019), Shapiro (2021), Lin et al. (2022), and references therein. Although our study constrains ambiguity sets and does not introduce regularization terms, previous studies on DRO have a wide variety of settings in constraints and/or regularization on the ambiguity of distributions. Duchi and Namkoong (2021) study statistical properties of general DRO based on empirical measures and show the minimax optimality of the empirical risks under DRO problems. Since recent datasets in machine learning are quite enormous, several studies (e.g., Levy et al., 2020; Qi et al., 2021) consider large-scale optimization of DRO problems and present the convergence of algorithms theoretically and numerically. Note that DRO plays an important role as a computational approach for out-of-distribution generalization (Shen et al., 2021). A bilevel federated optimization is a generalization of DRO. Tarzanagh et al. (2022) propose the FedNest algorithms for general bilevel optimization in the federated learning regime. Xing et al. (2022) consider bilevel optimization under the setting where clients are on a network graph. Qu et al. (2022) and Sun et al. (2023) discuss sharpness-aware optimization and its realization in federated learning. Fairness in federated learning is multifaceted and thus has been studied via several approaches. We first review works on fairness in the performance of models trained via federated learning among clients, which is called client-based fairness in previous studies (e.g., Ezzeldin et al., 2023). Mohri et al. (2019) study a federated minimax optimization problem with fairness and the stochastic agnostic federated learning (Stochastic-AFL) algorithm for the problem. Deng et al. (2020) propose the distributionally robust federated averaging (DRFA) algorithm for federated learning with DRO, which achieves a better communication efficiency than Mohri et al. (2019) (they do not discuss federated learning with fairness, but their work can be easily applied). Zecchin et al. (2023) consider a gradient-descent-ascent algorithm with compressed communications in federated optimization to improve the complexities of communication. Yu et al. (2023) propose the Scaff-PD algorithm for federated learning with minimal performance guarantees, which employs the idea of controlled averaging (Karimireddy et al., 2020) to control the heterogeneity of data distributions among clients. While these studies are based on general DRO problems and their efficient optimization, Li et al. (2020) discuss the qùëûqitalic_q-Fair Federated Learning (qùëûqitalic_q-FFL), extending a special case of Mohri et al. (2019), and show theoretical improvement of the fairness and the performance of modified algorithms (qùëûqitalic_q-FedSGD and qùëûqitalic_q-FedAvg). We also note studies on other types of fairness in federated learning. Lyu et al. (2020) consider fairness in the performances of local models against the contributions of training attendees and propose a novel training scheme Collaborative Fair Federated Learning. Wang et al. (2021) discuss fairness in the influence on the updates of global models under the norms of the gradients of clients being heterogeneous at each step and develop the FedFV algorithm. Ezzeldin et al. (2023) study fairness in model outputs against inputs with demographic information whose achievement in federated learning is more difficult than in centralized learning and consider a new algorithm FedFair. 1.2. Notation For all m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N, [m]:={1,‚Ä¶,m}assigndelimited-[]ùëö1‚Ä¶ùëö[m]:=\{1,\ldots,m\}[ italic_m ] := { 1 , ‚Ä¶ , italic_m }, [m]0:={0,‚Ä¶,m}assignsubscriptdelimited-[]ùëö00‚Ä¶ùëö[m]_{0}:=\{0,\ldots,m\}[ italic_m ] start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT := { 0 , ‚Ä¶ , italic_m }. For all x‚àà‚Ñùùë•‚Ñùx\in\mathbb{R}italic_x ‚àà blackboard_R, ‚åäx‚åã:=max‚Å°{z‚àà‚Ñ§;x‚â•z}assignùë•ùëß‚Ñ§ùë•ùëß\lfloor x\rfloor:=\max\{z\in\mathbb{Z};x\geq z\}‚åä italic_x ‚åã := roman_max { italic_z ‚àà blackboard_Z ; italic_x ‚â• italic_z } and ‚åàx‚åâ:=min‚Å°{z‚àà‚Ñ§;x‚â§z}assignùë•ùëß‚Ñ§ùë•ùëß\lceil x\rceil:=\min\{z\in\mathbb{Z};x\leq z\}‚åà italic_x ‚åâ := roman_min { italic_z ‚àà blackboard_Z ; italic_x ‚â§ italic_z }. For all x‚àà‚Ñùmùë•superscript‚Ñùùëöx\in\mathbb{R}^{m}italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT with m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N, xisubscriptùë•ùëñx_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with i‚àà[m]ùëñdelimited-[]ùëöi\in[m]italic_i ‚àà [ italic_m ] is the iùëñiitalic_i-th coordinate of xùë•xitalic_x. For all x,y‚àà‚Ñùmùë•ùë¶superscript‚Ñùùëöx,y\in\mathbb{R}^{m}italic_x , italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT with m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N, ‚ü®x,y‚ü©=‚àëi=1nxi‚Å¢yiùë•ùë¶superscriptsubscriptùëñ1ùëõsubscriptùë•ùëñsubscriptùë¶ùëñ\langle x,y\rangle=\sum_{i=1}^{n}x_{i}y_{i}‚ü® italic_x , italic_y ‚ü© = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. For any finite set Aùê¥Aitalic_A, card‚Å¢(A)cardùê¥\mathrm{card}(A)roman_card ( italic_A ) denotes the cardinality of Aùê¥Aitalic_A. For arbitrary normed space (N,‚à•‚ãÖ‚à•)(N,\|\cdot\|)( italic_N , ‚à• ‚ãÖ ‚à• ), for any set A‚äÇNùê¥ùëÅA\subset Nitalic_A ‚äÇ italic_N and positive number œµ>0italic-œµ0\epsilon>0italic_œµ > 0, ùí©(A,‚à•‚ãÖ‚à•,œµ)\mathcal{N}(A,\|\cdot\|,\epsilon)caligraphic_N ( italic_A , ‚à• ‚ãÖ ‚à• , italic_œµ ) denotes the minimum œµitalic-œµ\epsilonitalic_œµ-covering number, that is, ùí©(A,‚à•‚ãÖ‚à•,œµ)=min{card(B);A‚äÇ‚ãÉx‚ààB{y‚ààB;‚à•x‚àíy‚à•‚â§œµ}}\mathcal{N}(A,\|\cdot\|,\epsilon)=\min\{\mathrm{card}(B);A\subset\bigcup_{x\in B% }\{y\in B;\|x-y\|\leq\epsilon\}\}caligraphic_N ( italic_A , ‚à• ‚ãÖ ‚à• , italic_œµ ) = roman_min { roman_card ( italic_B ) ; italic_A ‚äÇ ‚ãÉ start_POSTSUBSCRIPT italic_x ‚àà italic_B end_POSTSUBSCRIPT { italic_y ‚àà italic_B ; ‚à• italic_x - italic_y ‚à• ‚â§ italic_œµ } }; we let ùí©(A,‚à•‚ãÖ‚à•,œµ)=‚àû\mathcal{N}(A,\|\cdot\|,\epsilon)=\inftycaligraphic_N ( italic_A , ‚à• ‚ãÖ ‚à• , italic_œµ ) = ‚àû if there is no finite set BùêµBitalic_B with A‚äÇ‚ãÉx‚ààB{y‚ààB;‚Äñx‚àíy‚Äñ‚â§œµ}ùê¥subscriptùë•ùêµformulae-sequenceùë¶ùêµnormùë•ùë¶italic-œµA\subset\bigcup_{x\in B}\{y\in B;\|x-y\|\leq\epsilon\}italic_A ‚äÇ ‚ãÉ start_POSTSUBSCRIPT italic_x ‚àà italic_B end_POSTSUBSCRIPT { italic_y ‚àà italic_B ; ‚à• italic_x - italic_y ‚à• ‚â§ italic_œµ }. For any set A‚äÇ‚Ñùmùê¥superscript‚ÑùùëöA\subset\mathbb{R}^{m}italic_A ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT with m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N, int‚Å¢(A)intùê¥\mathrm{int}(A)roman_int ( italic_A ) denotes the interior of Aùê¥Aitalic_A. For any compact C‚äÇ‚Ñùmùê∂superscript‚ÑùùëöC\subset\mathbb{R}^{m}italic_C ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT with m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N and p‚â•1ùëù1p\geq 1italic_p ‚â• 1, ‚à•C‚à•p:=maxc‚ààC(‚àëi=1m|ci|p)1/p\|C\|_{p}:=\max_{c\in C}(\sum_{i=1}^{m}|c_{i}|^{p})^{1/p}‚à• italic_C ‚à• start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT := roman_max start_POSTSUBSCRIPT italic_c ‚àà italic_C end_POSTSUBSCRIPT ( ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT | italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 1 / italic_p end_POSTSUPERSCRIPT. We also use the notation D‚Å¢(x,y)=2‚àí1‚Å¢‚Äñx‚àíy‚Äñ22ùê∑ùë•ùë¶superscript21superscriptsubscriptnormùë•ùë¶22D(x,y)=2^{-1}\|x-y\|_{2}^{2}italic_D ( italic_x , italic_y ) = 2 start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ‚à• italic_x - italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT for any x,y‚àà‚Ñùmùë•ùë¶superscript‚Ñùùëöx,y\in\mathbb{R}^{m}italic_x , italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT, m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N. Œîn‚àí1:={c‚àà‚Ñùn;ci‚â•0,‚àëici=1}assignsuperscriptŒîùëõ1formulae-sequenceùëêsuperscript‚Ñùùëõformulae-sequencesubscriptùëêùëñ0subscriptùëñsubscriptùëêùëñ1\Delta^{n-1}:=\{c\in\mathbb{R}^{n};c_{i}\geq 0,\sum_{i}c_{i}=1\}roman_Œî start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT := { italic_c ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ; italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚â• 0 , ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = 1 } denotes the (n‚àí1)ùëõ1(n-1)( italic_n - 1 )-simplex. ùíû‚Å¢(A;[0,‚àû))ùíûùê¥0\mathcal{C}(A;[0,\infty))caligraphic_C ( italic_A ; [ 0 , ‚àû ) ) for A‚äÇ‚Ñùmùê¥superscript‚ÑùùëöA\subset\mathbb{R}^{m}italic_A ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT with m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N is a collection of [0,‚àû)0[0,\infty)[ 0 , ‚àû )-valued continuous functions on Aùê¥Aitalic_A, and ùíûp‚Å¢(A;[0,‚àû))superscriptùíûùëùùê¥0\mathcal{C}^{p}(A;[0,\infty))caligraphic_C start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ( italic_A ; [ 0 , ‚àû ) ) for open A‚äÇ‚Ñùmùê¥superscript‚ÑùùëöA\subset\mathbb{R}^{m}italic_A ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT is a collection of [0,‚àû)0[0,\infty)[ 0 , ‚àû )-valued pùëùpitalic_p-times continuously differentiable functions on Aùê¥Aitalic_A. 1m‚àà‚Ñùmsubscript1ùëösuperscript‚Ñùùëö\textbf{1}_{m}\in\mathbb{R}^{m}1 start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT with m‚àà‚Ñïùëö‚Ñïm\in\mathbb{N}italic_m ‚àà blackboard_N is the vector whose all elements are 1111."
https://arxiv.org/html/2411.01140v1,Privacy-Preserving Federated Learning with Differentially Private Hyperdimensional Computing,"Federated Learning (FL) is essential for efficient data exchange in Internet of Things (IoT) environments, as it trains Machine Learning (ML) models locally and shares only model updates. However, FL is vulnerable to privacy threats like model inversion and membership inference attacks, which can expose sensitive training data. To address these privacy concerns, Differential Privacy (DP) mechanisms are often applied. Yet, adding DP noise to black-box ML models degrades performance, especially in dynamic IoT systems where continuous, lifelong FL learning accumulates excessive noise over time. To mitigate this issue, we introduce Federated HyperDimensional computing with Privacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI) framework that combines the neuro-symbolic paradigm with DP. FedHDPrivacy carefully manages the balance between privacy and performance by theoretically tracking cumulative noise from previous rounds and adding only the necessary incremental noise to meet privacy requirements. In a real-world case study involving in-process monitoring of manufacturing machining operations, FedHDPrivacy demonstrates robust performance, outperforming standard FL frameworks‚Äîincluding Federated Averaging (FedAvg), Federated Stochastic Gradient Descent (FedSGD), Federated Proximal (FedProx), Federated Normalized Averaging (FedNova), and Federated Adam (FedAdam)‚Äîby up to 38%percent3838\%38 %. FedHDPrivacy also shows potential for future enhancements, such as multimodal data fusion.","The rapid expansion of the Internet of Things (IoT) has led to the widespread integration of sensing and computing technologies, connecting a vast array of devices to support applications in areas, including smart cities [1] and digital manufacturing [2]. This interconnected landscape has facilitated the emergence of intelligent IoT applications, where Artificial Intelligence (AI) models play a crucial role in deriving actionable insights from the data produced by IoT devices, such as in traffic management [3]. Traditionally, these AI-driven tasks were handled by centralized data centers [4]. However, this approach is increasingly challenged by the impracticality of transmitting large volumes of IoT data to distant servers and the heightened risk of privacy breaches [5]. Relying on third-party data centers for AI processing can compromise sensitive information, including financial and healthcare records [6]. Therefore, there is a pressing need for novel AI methods that not only protect privacy but also enhance the efficiency and intelligence of IoT networks and applications. Federated Learning (FL) has emerged as a powerful approach for building intelligent, privacy-preserving systems within the IoT. FL operates as a decentralized AI model to be trained directly on IoT devices, referred to as clients, without the need to transfer raw data to a central server [7]. As shown in Figure 1, IoT devices act as clients in this architecture, collaborating with a central server to refine a global model. The process begins with the server initializing the global model with set parameters, which are then distributed to the clients. Each client uses its locally generated data to update the model and sends the updates back to the server. The server aggregates the locally updated models, improving the global model in an iterative manner. This decentralized approach leverages the computational capabilities of distributed IoT devices, enhancing training efficiency while ensuring data privacy [8]. In IoT environments, data is continuously generated and frequently change. This dynamic nature necessitates the implementation of continuous learning within the FL framework. Continuous learning, or lifelong learning, allows models to continuously incorporate new data, enabling them to adapt to evolving environments and remain effective in real-time applications. By regularly updating the model with new data streams, FL ensures that the global model stays current and responsive to changing conditions across distributed devices [9]. This capability is particularly valuable in IoT systems, where the model must continually learn from new information to maintain high performance in dynamic, real-world scenarios. While FL improves privacy by keeping user data on individual devices rather than transmitting it to a central server, it does not fully eliminate privacy risks. If an attacker gains access to either the clients‚Äô or server‚Äôs models, they can exploit these models to perform model inversion or membership inference attacks, extracting sensitive and confidential information about the training samples. In a model inversion attack, the attacker analyzes the model‚Äôs outputs or gradients to reconstruct the original training data, effectively revealing sensitive information used to train the model [10]. For example, by querying the trained model, an adversary can reverse-engineer the data to infer specific details about the training samples. In contrast, a membership inference attack allows an adversary to determine whether a particular data point was included in the training set [11]. This type of attack can reveal the presence or absence of specific individuals in the dataset, leading to severe privacy breaches. Both attacks pose significant risks to user privacy, even in seemingly secure environments. Techniques such as generative regression neural networks have shown that it is possible to extract sensitive information from shared model parameters [12]. Even in black-box scenarios, where the internal workings of the model are hidden, attackers can infer whether certain individuals were part of the training set or recover features of their data [10]. As depicted in Figure 1, these vulnerabilities enable adversaries to exploit FL systems and compromise the confidentiality of participants [13]. Attackers can access ML models through four primary methods. The first method is eavesdropping, where adversaries intercept communication between clients and the central server. As illustrated in Figure 1, Since FL involves multiple rounds of communication between clients and the server, unprotected channels present a high risk of interception, potentially allowing attackers to access the global and local models and their parameters [14]. The second method is by posing as a malicious participant, where attackers disguise themselves as legitimate clients to interact directly with the central server [15]. This gives them access to the global model, enabling them to extract sensitive information or infer the presence of specific data. The third method is server compromise, where attackers successfully hack into the central server, gaining control over the global model and exposing sensitive data collected from multiple clients [16]. Lastly, in scenarios involving an untrusted server, clients may fear that the server itself could analyze the local models it receives, potentially identifying sensitive training data or determining if specific data was used in training [17]. In all four cases, attackers aim to exploit the models by using model inversion and membership inference attacks to extract sensitive information about the training samples, whether they gain access to the global or local models. As vulnerabilities in ML models within FL frameworks are identified, the need for robust defense mechanisms becomes increasingly critical. Recent studies have investigated various strategies to enhance privacy in FL. One widely adopted method is anonymization, which involves generalizing or redacting specific attributes in a dataset to prevent the easy identification of individual records. However, with the advent of sophisticated privacy attacks, such as model inversion [10], which can reconstruct training data even with limited access to the models, traditional anonymization techniques have proven inadequate. Attackers often find ways to circumvent these defenses, particularly in high-dimensional datasets where anonymization fails to offer strong protection against the disclosure of sensitive attributes. Homomorphic Encryption (HE) has been introduced as a robust privacy-preserving technique that allows computations to be performed directly on encrypted data, eliminating the need for decryption [18]. While HE offers strong theoretical privacy protection by enabling model training on encrypted datasets, its practical application in modern ML systems is hindered by significant computational overhead. This challenge is particularly evident in deep learning scenarios, where the processing of large datasets and complex models is common. Consequently, HE is often more applicable in situations where models are pre-trained and deployed as services [19]. Another promising technique is Secure Multiparty Computation (SMC), which enables multiple parties to jointly compute a function using their private inputs without disclosing these inputs to one another or a central server. This method removes the need for a trusted third party and provides strong privacy guarantees in theory [20]. However, SMC also faces challenges due to its high computational and communication demands, making it less feasible to train intricate models in FL environments. Figure 2: Secure Federated Learning Framework with Differential Privacy and Accuracy Trade-off. A federated learning framework enhanced with differential privacy noise added to clients‚Äô models. This mechanism helps prevent adversaries from reconstructing training samples or distinguishing original data from random data, thereby safeguarding client information. However, the cumulative noise added over multiple training rounds introduces an accuracy trade-off, gradually impacting model performance. Differential Privacy (DP) has emerged as a more practical and robust solution for safeguarding privacy in AI systems. When applied to an ML model, DP protects against model inversion and membership inference attacks, making it a suitable approach for securing FL frameworks. Recent advancements have enhanced the viability of DP, with companies like Google, Microsoft, and Apple successfully integrating DP into their data collection practices to improve user privacy while retaining data utility [21, 22, 23]. DP achieves this by adding carefully calibrated noise to the data or model, ensuring that individual records remain private during analysis [24, 25]. This noise addition ensures that the inclusion or exclusion of any single data point does not significantly impact the overall analysis, thereby preserving individual privacy. As illustrated in Figure 2, DP can effectively neutralize model inversion attacks, where adversaries attempting to reconstruct training samples by querying the model will only retrieve random, non-informative signals unrelated to the actual training data. Additionally, DP can thwart membership inference attacks by preventing adversaries from determining whether a particular sample was part of the training set. DP‚Äôs ability to protect sensitive information while preserving data utility makes it a preferred choice among privacy-preserving techniques. In a FL structure, as depicted in Figure 2, DP can be applied to local models before they are transmitted to the central server. As a result, these secured local models contribute to the global model, which is formed by aggregating the secured local models, thereby maintaining the security of the global model as well. The level of privacy can also be adjusted at the server level, ensuring that the local models remain secure and resistant to potential breaches by an untrusted server. This configuration protects the global model from server-side attacks, making it difficult for adversaries to compromise the server. Additionally, the communication between the server and clients is secure because the updates exchanged are already secured, rendering eavesdropping attacks ineffective. Furthermore, since the server is secure, a malicious participant cannot extract any information from the secured global model through interaction with the server. In conclusion, while anonymization, HE, and SMC each offer specific advantages for privacy protection in FL, DP stands out as the most effective and practical solution, providing a strong balance between privacy and usability in real-world applications. While DP is highly effective at safeguarding sensitive information and maintaining data utility, it also presents challenges related to balancing privacy and accuracy. In the context of FL combined with DP, the amount of noise introduced must be carefully managed to balance privacy and accuracy. Excessive noise can degrade model performance, while insufficient noise can lead to privacy breaches. Therefore, evaluating privacy leakage for a given privacy budget is crucial before deploying models or releasing datasets [12]. Moreover, ML models often function as complex black-box systems, where their internal mechanisms are not easily understood or transparent. This lack of transparency is particularly problematic in safety-critical fields, where the traceability and explainability of decision-making processes are crucial [26]. Additionally, the secure handling of data is essential, especially when dealing with sensitive and personal information. In the FL framework, where data privacy is paramount, DP is widely adopted to protect sensitive information by adding noise to the data or model parameters. However, implementing DP within FL presents a significant challenge: as noise is added in each training round to maintain privacy, the cumulative effect can lead to substantial degradation in model performance. This issue is exacerbated by the black-box nature of ML models, making it difficult to determine the optimal amount of noise to add without compromising the model‚Äôs accuracy [27]. We introduce Federated HyperDimensional computing with Privacy-preserving (FedHDPrivacy), an eXplainable Artificial Intelligence (XAI) framework for FL, to address the privacy challenges. This framework combines DP and HyperDimensional computing (HD) to ensure both security and accuracy during model aggregation. By integrating XAI techniques, FedHDPrivacy allows for precise calculation of the noise required in each round, tracking the cumulative noise added in previous rounds and adjusting the noise accordingly in subsequent rounds. This approach ensures that only the minimal necessary noise is introduced, effectively balancing privacy and accuracy. The FedHDPrivacy framework offers a robust solution by controlling the noise added to both the clients‚Äô and the server‚Äôs models in each round, thereby avoiding the degradation of model performance due to excessive noise. This secure and explainable FL framework not only safeguards confidential information but also preserves the utility and efficiency of the AI models involved. Moreover, our framework is specifically designed to address the challenges of continuous learning in IoT systems under the FL paradigm. It ensures that the model remains up-to-date and effective in dynamic environments, thereby enhancing the practical utility of FL in real-world IoT scenarios. Our contributions in this paper are summarized as follows: 1. We introduce FedHDPrivacy, an explainable framework that enhances transparency and understanding in the interactions between ML models and DP within an FL structure, ensuring both security and interpretability. 2. Our framework accurately determines the necessary noise levels to secure both client and central server models, while preventing the overestimation of DP noise by computing the cumulative noise from previous rounds. 3. FedHDPrivacy ensures an optimal balance between privacy protection and model accuracy by avoiding excessive noise addition throughout the iterative training process. The remainder of this paper is structured as follows: Section 2 reviews existing FL frameworks, examines DP as a mechanism for safeguarding ML models in FL, and explores the integration of HD with FL and DP. In Section 3, we define key concepts related to HD and DP. Section 4 outlines the proposed FedHDPrivacy framework and its implementation. Section 5 details the experimental setup used in a real-world IoT scenario. The results of the experiments are presented and analyzed in Section 6. Finally, Section 7 summarizes the contributions of this work in developing privacy-preserving FL models for IoT and offers suggestions for future research directions."
https://arxiv.org/html/2411.01102v1,\binenhance: A Enhancement Framework Based on External Environment Semantics for Binary Code Search,"Binary code search plays a crucial role in applications like software reuse detection, and vulnerability identification. Currently, existing models are typically based on either internal code semantics or a combination of function call graphs (CG) and internal code semantics. However, these models have limitations. Internal code semantic models only consider the semantics within the function, ignoring the inter-function semantics, making it difficult to handle situations such as function inlining. The combination of CG and internal code semantics is insufficient for addressing complex real-world scenarios. To address these limitations, we propose \binenhance, a novel framework designed to leverage the inter-function semantics to enhance the expression of internal code semantics for binary code search. Specifically, \binenhance constructs an External Environment Semantic Graph (EESG), which establishes a stable and analogous external environment for homologous functions by using different inter-function semantic relations (e.g., call, location, data-co-use). After the construction of EESG, we utilize the embeddings generated by existing internal code semantic models to initialize nodes of EESG. Finally, we design a Semantic Enhancement Model (SEM) that uses Relational Graph Convolutional Networks (RGCNs) and a residual block to learn valuable external semantics on the EESG for generating the enhanced semantics embedding. In addition, \binenhance utilizes data feature similarity to refine the cosine similarity of semantic embeddings. We conduct experiments under six different tasks (e.g., under function inlining scenario) and the results illustrate the performance and robustness of \binenhance. The application of \binenhance to HermesSim, Asm2vec, TREX, Gemini, and Asteria on two public datasets results in an improvement of Mean Average Precision (MAP) from 53.6% to 69.7%. Moreover, the efficiency increases fourfold.","Software development often reuses open-source code to reduce costs. However, this trend inadvertently propagates vulnerabilities of open-source code into billions of software systems [2] [14], including industrial software and firmware. The extensive workload of code auditing and the complexity of recursive code reuse results in substantial delays in vulnerability patching, with some systems experiencing an average delay of 352 days [27]. In 2023, a review [20] conducted by Synopsys on 1703 software projects exposed a notable problem: 96% of these projects utilize open-source code, and 84% of them have at least one known vulnerability. In reaction to this widespread security risk, the emergence of binary code search has been proven to be a powerful method for automating the detection of insecure software components [43]. This facilitates the prompt patching of identified vulnerabilities. Binary code search entails the meticulous analysis of numerous binary codes to identify the most similar ones. Hence, its applications span a wide range, including software reuse detection [51, 21, 42], vulnerability search [43, 40, 46, 33], firmware security analysis [59, 63], and patch presence testing [57, 36, 31]. Developing a general and effective binary code search solution is extremely challenging. The syntactic structure of binary code can vary dramatically due to different compiler settings, such as optimization options. Additionally, binary codes with similar syntactic structures may have different semantics [30]. Therefore, a comprehensive understanding of the semantics of binary code is key to addressing this task. With the rapid development of deep learning technology, current solutions for binary code search are to convert binary code into embeddings and obtain the similarity of binary code by calculating the similarity between embeddings. Based on the definition of Section II-A, we classify them into two categories: internal code semantic models and external environment semantic models. Current solutions predominantly focus on learning the internal code features of functions. Gemini [56] and Asm2vec [26] employ neural networks to perform semantic encoding within program structures, specifically control flow graphs. Similarly, TREX [44], Palmtree [39], and others [53] [60] utilize Transformer [52] models for assembly code representation. Other studies, such as HermesSim [30], Asteria [59], VulHawk [43] and XLIR [28], utilize deep neural networks to convert intermediate representations, such as pseudocode and toy IR, into embeddings. Furthermore, recent solutions integrate function call relationships with internal code features. Œ±ùõº\alphaitalic_Œ±diff [41] and BMM [29] utilize function call graph to aid in calculating similarity. Despite notable performance improvements in binary code search, these solutions have exhibited some limitations (example in Section II-B), particularly as the number of binary codes requiring comparison increases. Firstly, the internal code semantics of functions may exhibit substantial variations due to different compilation settings, encompassing factors like function inlining and splitting (identified as P1). Function inlining, an optimization strategy during compilation, introduces the actual codes of other functions called by the function at the corresponding address (as shown in Figure 2.(a)). Function splitting, conversely, divides a single function into multiple sub-functions. Study [32] demonstrates that compiler-caused function inlining can reach up to 70%, leading to a significant decrease in the performance of binary code search. Moreover, the selective inlining method [23] is lack of flexibility. This limitation underscores the inadequacy of relying solely on internal code semantics while overlooking the significance of external environment semantics. Secondly, exclusive reliance on function call graphs (CG) for assistance is insufficient for addressing complex real-world scenarios (P2). Relying solely on the external environment semantics of function calls introduces two primary issues that impact the effectiveness of binary code search: missing calls and similar calls. Missing calls occur when function call dependencies are absent, potentially eliminated during code reuse or optimization processes, such as strategies like function inlining. Similar calls, meanwhile, involve similar function call relationships between non-homologous functions, leading to imprecise similarity assessments and false positives. Thirdly, current solutions exhibit limited scalability and struggle to cope with large-scale function search tasks (P3). The most recent Transformer-based methods, such as TREX [44], typically generate function embeddings with a dimension of 768. However, real-world scenarios often involve millions of functions, necessitating rapid comparison speeds. Our experiments (in Section IV-F) revealed that in a pool of 10,000 functions, the time required to recall all the homologous functions of a binary file containing around 2500 functions is around 22 minutes using TREX‚Äôs 768-dimension embeddings. The time increases exponentially with the growth of the function pool and the binary file size. In contrast, 128-dimension embeddings only require 5 minutes, emphasizing the efficiency challenges in large-scale function search. Moreover, simply reducing the dimensionality for retraining usually leads to substantial performance loss [39]. Our central insight focuses on comprehensively utilizing external environment semantics (definition in Section II-A) to enhance the internal code semantic models. To achieve it, several challenges must be addressed. First, it is crucial to carefully select environment features that can serve external environment semantics for a function within a complex environment. These selected features should positively contribute to the performance of binary code search tasks and exhibit robustness. Second, the integration of various external environment features, each representing distinct structures and semantic spaces, presents a challenge. Determining an effective method for combining these diverse features to improve search tasks is crucial. Finally, an approach needs to be devised to appropriately apply the external environment semantics of the function, significantly enhancing its internal code semantics. Figure 1: Examples of Internal Code Feature Figure 2: A Motivation Example of external environment semantics. In this paper, we introduce \binenhance, a framework based on external environment semantics, specifically designed to tackle the aforementioned challenges. \binenhance differentiates itself from prior approaches by not solely relying on function call graph (P2). Instead, we have designed a novel External Environment Semantic Graph (EESG), which constructs four edges (C‚Å¢Dùê∂ùê∑CDitalic_C italic_D, D‚Å¢C‚Å¢Uùê∑ùê∂ùëàDCUitalic_D italic_C italic_U, A‚Å¢Aùê¥ùê¥AAitalic_A italic_A, and S‚Å¢UùëÜùëàSUitalic_S italic_U) among functions to model a function‚Äôs external environment since these semantic edges are robust among different scenarios when they coexist. For P3, we employ whitening transformations [49] to reduce the dimension of node initial embeddings generated by existing internal code semantics models [30] [59] [56] [44] [26] and MPNET [47], improving search task efficiency. To address P1, after initializing the nodes of EESG, \binenhance proposes a Semantic Enhancement Model (SEM), which introduces external environment semantics into internal code semantic embedding. SEM utilizes Relational Graph Convolutional Networks (RGCNs) [45] for updating external environment semantic embeddings in EESG and merges the dual embeddings of the function via a residual block to obtain an enhanced semantic embedding. Lastly, \binenhance includes a similarity combination module, leveraging data feature similarity to fine-tune semantic embedding similarity. In summary, we have made the following contributions: ‚Ä¢ Perspective. This paper presents a novel perspective on binary code search by incorporating external environment semantics into internal embeddings. To the best of our knowledge, it is also the first work to demonstrate that this integration can significantly improve the identification of homologous functions. Our approach enriches the semantics of internal embeddings and addresses the problem of false filtering of those approaches based on function calls. Additionally, it can be easily extended to other internal code semantic methods. Moreover, this paper demonstrates the promising potential of designing new external features to enhance internal representations, rather than solely focusing on developing new features for internal code. ‚Ä¢ Technique. We implement \binenhance111https://github.com/wang-yongpan/BinEnhance, a general framework that proposes four novel external semantic edges of our EESG to model stable external contexts in complex external environments and introduce an SEM to incorporate external semantics into internal embeddings to improve its representation ability. ‚Ä¢ Study. We conduct comprehensive experiments to evaluate the effectiveness of the \binenhance, including different function pool sizes, cross-architectures, and cross-optimization options, as well as scenarios such as function inlining and real-world vulnerability detection. Our results show that all existing binary code search methods overlook the importance of external environment semantics, leading to suboptimal performance."
https://arxiv.org/html/2411.01086v2,Practical hybrid PQC-QKD protocols with enhanced security and performance,"Quantum resistance is vital for emerging cryptographic systems as quantum technologies continue to advance towards large-scale, fault-tolerant quantum computers. Resistance may be offered by quantum key distribution (QKD), which provides information-theoretic security using quantum states of photons, but may be limited by transmission loss at long distances. An alternative approach uses classical means and is conjectured to be resistant to quantum attacks‚Äîso-called post-quantum cryptography (PQC)‚Äîbut it is yet to be rigorously proven, and its current implementations are computationally expensive. To overcome the security and performance challenges present in each, here we develop hybrid protocols by which QKD and PQC inter-operate within a joint quantum-classical network. In particular, we consider different hybrid designs that may offer enhanced speed and/or security over the individual performance of either approach. Furthermore, we present a method for analyzing the security of hybrid protocols in key distribution networks. Our hybrid approach paves the way for joint quantum-classical communication networks, which leverage the advantages of both QKD and PQC and can be tailored to the requirements of various practical networks.","References Bennett and Brassard [1984] C. H. Bennett and G. Brassard, in Proceedings of IEEE International Conference on Computers, Systems and Signal Processing (Bangalore, India, 1984) pp. 175‚Äì179. Ekert [1992] A. K. Ekert, in Quantum Measurements in Optics (Springer, 1992) pp. 413‚Äì418. Scarani et al. [2009] V. Scarani, H. Bechmann-Pasquinucci, N. J. Cerf, M. Du≈°ek, N. L√ºtkenhaus, and M. Peev, Rev. Mod. Phys. 81, 1301 (2009). Gisin et al. [2002] N. Gisin, G. Ribordy, W. Tittel, and H. Zbinden, Reviews of Modern Physics 74, 145 (2002), publisher: American Physical Society. Zapatero et al. [2023] V. Zapatero, T. van Leent, R. Arnon-Friedman, W.-Z. Liu, Q. Zhang, H. Weinfurter, and M. Curty, npj Quantum Information 9, 10 (2023). Peev et al. [2009] M. Peev, C. Pacher, R. All√©aume, C. Barreiro, J. Bouda, W. Boxleitner, T. Debuisschert, E. Diamanti, M. Dianati, J. Dynes, et al., New Journal of Physics 11, 075001 (2009). Sasaki et al. [2011] M. Sasaki, M. Fujiwara, H. Ishizuka, W. Klaus, K. Wakui, M. Takeoka, S. Miki, T. Yamashita, Z. Wang, A. Tanaka, et al., Optics express 19, 10387 (2011). Tang et al. [2016] Y.-L. Tang, H.-L. Yin, Q. Zhao, H. Liu, X.-X. Sun, M.-Q. Huang, W.-J. Zhang, S.-J. Chen, L. Zhang, L.-X. You, et al., Physical Review X 6, 011024 (2016). Chen et al. [2021] Y.-A. Chen, Q. Zhang, T.-Y. Chen, W.-Q. Cai, S.-K. Liao, J. Zhang, K. Chen, J. Yin, J.-G. Ren, Z. Chen, et al., Nature 589, 214 (2021). Diamanti et al. [2016] E. Diamanti, H.-K. Lo, B. Qi, and Z. Yuan, npj Quantum Information 2, 1 (2016). Xu et al. [2020] F. Xu, X. Ma, Q. Zhang, H.-K. Lo, and J.-W. Pan, Rev. Mod. Phys. 92, 025002 (2020). Bernstein and Lange [2017] D. J. Bernstein and T. Lange, Nature 549, 188 (2017). Shor [1999] P. W. Shor, SIAM review 41, 303 (1999). [14] https://csrc.nist.gov/projects/post-quantum-cryptography. Note [1] In the final NIST standard [14], Kyber has been modified and renamed as the Module-Lattice-Based Key-Encapsulation-Mechanism (ML-KEM). In this work, we will not differentiate between these two protocols. Bos et al. [2018] J. Bos, L. Ducas, E. Kiltz, T. Lepoint, V. Lyubashevsky, J. M. Schanck, P. Schwabe, G. Seiler, and D. Stehl√©, in 2018 IEEE European Symposium on Security and Privacy (EuroS&P) (IEEE, 2018) pp. 353‚Äì367. Castryck and Decru [2022] W. Castryck and T. Decru, ‚ÄúAn efficient key recovery attack on SIDH,‚Äù Cryptology ePrint Archive, Paper 2022/975 (2022). [18] In 2016, Eldar and Shor suggested an efficient quantum algorithm for the lattice problems https://arxiv.org/abs/1611.06999, but the paper was later withdrawn. Chen [2024] Y. Chen, ‚ÄúQuantum algorithms for lattice problems,‚Äù Cryptology ePrint Archive, Paper 2024/555 (2024). Yang et al. [2021] Y.-H. Yang, P.-Y. Li, S.-Z. Ma, X.-C. Qian, K.-Y. Zhang, L.-J. Wang, W.-L. Zhang, F. Zhou, S.-B. Tang, J.-Y. Wang, et al., Optics express 29, 25859 (2021). Djordjevic [2020] I. B. Djordjevic, IEEE Access 8, 154708 (2020). Dowling et al. [2020] B. Dowling, T. B. Hansen, and K. G. Paterson, in International Conference on Post-Quantum Cryptography (Springer, 2020) pp. 483‚Äì502. Garms et al. [2024] L. Garms, T. K. Para√Øso, N. Hanley, A. Khalid, C. Rafferty, J. Grant, J. Newman, A. J. Shields, C. Cid, and M. O‚ÄôNeill, Advanced Quantum Technologies 7, 2300304 (2024). Dent [2003] A. W. Dent, in IMA International Conference on Cryptography and Coding (Springer, 2003) pp. 133‚Äì151. Katz and Lindell [2020] J. Katz and Y. Lindell, Introduction to modern cryptography (CRC press, 2020). Cramer and Shoup [2003] R. Cramer and V. Shoup, SIAM Journal on Computing 33, 167 (2003), https://doi.org/10.1137/S0097539702403773 . Fujisaki and Okamoto [1999] E. Fujisaki and T. Okamoto, in Advances in Cryptology ‚Äî CRYPTO‚Äô 99 (Springer Berlin Heidelberg, Berlin, Heidelberg, 1999) pp. 537‚Äì554. Hofheinz et al. [2017] D. Hofheinz, K. H√∂velmanns, and E. Kiltz, in Theory of Cryptography (Springer International Publishing, Cham, 2017) pp. 341‚Äì371. [29] https://github.com/pq-crystals/kyber. Yuan et al. [2018] Z. Yuan, A. Plews, R. Takahashi, K. Doi, W. Tam, A. W. Sharpe, A. R. Dixon, E. Lavelle, J. F. Dynes, A. Murakami, M. Kujiraoka, M. Lucamarini, Y. Tanizawa, H. Sato, and A. J. Shields, Journal of Lightwave Technology 36, 3427 (2018). Li et al. [2023] W. Li, L. Zhang, H. Tan, Y. Lu, S.-K. Liao, J. Huang, H. Li, Z. Wang, H.-K. Mao, B. Yan, et al., Nature Photonics , 1 (2023). [32] See Supplementary Materials for details on the performance simulation, a description of linear-code-based secret sharing, a formal definition of composite protocols, and security/performance analysis. Padro [2012] C. Padro, ‚ÄúLecture notes in secret sharing,‚Äù Cryptology ePrint Archive, Paper 2012/674 (2012). Simmons [1990] G. J. Simmons, in Advances in Cryptology ‚Äî CRYPTO‚Äô 88 (Springer New York, New York, NY, 1990) pp. 390‚Äì448. Jackson and Martin [1994] W.-A. Jackson and K. M. Martin, Designs, Codes and Cryptography 4, 83 (1994). Brickell [1990] E. F. Brickell, in Advances in Cryptology ‚Äî EUROCRYPT ‚Äô89 (Springer Berlin Heidelberg, Berlin, Heidelberg, 1990) pp. 468‚Äì475. Benaloh and Leichter [1990] J. Benaloh and J. Leichter, in Advances in Cryptology ‚Äî CRYPTO‚Äô 88 (Springer New York, New York, NY, 1990) pp. 27‚Äì35. Bertilsson and Ingemarsson [1993] M. Bertilsson and I. Ingemarsson, in Advances in Cryptology ‚Äî AUSCRYPT ‚Äô92 (Springer Berlin Heidelberg, Berlin, Heidelberg, 1993) pp. 67‚Äì79. van Dijk [1995] M. van Dijk, in Advances in Cryptology ‚Äî EUROCRYPT‚Äô94 (Springer Berlin Heidelberg, Berlin, Heidelberg, 1995) pp. 23‚Äì34. Beimel et al. [2014] A. Beimel, A. Ben-Efraim, C. Padr√≥, and I. Tyomkin, in Theory of Cryptography (Springer Berlin Heidelberg, Berlin, Heidelberg, 2014) pp. 394‚Äì418. Chor et al. [1985] B. Chor, S. Goldwasser, S. Micali, and B. Awerbuch, in 26th Annual Symposium on Foundations of Computer Science (sfcs 1985) (1985) pp. 383‚Äì395. Vyas and Mendes [2024] N. Vyas and P. Mendes, ‚ÄúRelaxing trust assumptions on quantum key distribution networks,‚Äù (2024), arXiv:2402.13136 [quant-ph] . Vardoyan and Wehner [2023] G. Vardoyan and S. Wehner, in 2023 IEEE International Conference on Quantum Computing and Engineering (QCE), Vol. 01 (2023) pp. 1238‚Äì1248. Gauthier et al. [2024] S. Gauthier, T. Vasantam, and G. Vardoyan, ‚ÄúAn on-demand resource allocation algorithm for a quantum network hub and its performance analysis,‚Äù (2024), arXiv:2405.18066 [quant-ph] . Zhou et al. [2022] H. Zhou, K. Lv, L. Huang, and X. Ma, IEEE/ACM Transactions on Networking 30, 1328 (2022)."
https://arxiv.org/html/2411.01081v2,Towards efficient and secure quantum-classical communication networks,"The rapid advancement of quantum technologies calls for the design and deployment of quantum-safe cryptographic protocols and communication networks. There are two primary approaches to achieving quantum-resistant security: quantum key distribution (QKD) and post-quantum cryptography (PQC). While each offers unique advantages, both have drawbacks in practical implementation. In this work, we introduce the pros and cons of these protocols and explore how they can be combined to achieve a higher level of security and/or improved performance in key distribution. We hope our discussion inspires further research into the design of hybrid cryptographic protocols for quantum-classical communication networks.","Acknowledgment We acknowledge support from the ARO (W911NF-23-1-0077), the ARO MURI (W911NF-21-1-0325), the AFOSR MURI (FA9550-19-1-0399, FA9550-21-1-0209, FA9550-23-1-0330, FA9550-23-1-0338), the DARPA (HR0011-24-9-0359, HR0011-24-9-0361), the NSF (OMA-1936118, ERC-1941583, OMA-2137642, OSI-2326767, CCF-2312755), NTT Research, the Packard Foundation (2020-71479), and the Marshall and Arlene Bennett Family Research Program. Additional support was provided by Q-NEXT, part of the U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers, and the Advanced Scientific Computing Research (ASCR) program under contract number DE-AC02-06CH11357 as part of the InterQnet quantum networking project. J.L. acknowledges startup funds provided by the University of Pittsburgh and funding from IBM Quantum through the Chicago Quantum Exchange."
https://arxiv.org/html/2411.01073v1,AttackQA: Development and Adoption of a Dataset for Assisting Cybersecurity Operations using Fine-tuned and Open-Source LLMs,"Retrieval-augmented generation (RAG) on specialized domain datasets has shown improved performance when large language models (LLMs) are fine-tuned for generating responses to user queries. In this study, we develop a cybersecurity question-answering (Q&A) dataset, called AttackQA, and employ it to build a RAG-based Q&A system designed for analysts in security operations centers. The dataset comprises 25,335 Q&A pairs, accompanied by rationales to facilitate fine-tuning and evaluation. 80% of the dataset was generated with help of a lightweight open-source LLM (LLama 3 8B), which produced over 1100 tokens per second with full 16-bit precision on SambaNova System‚Äôs SN40L specialized hardware. To ensure dataset quality, we fine-tuned LLama 3 70B to detect and reject low-quality Q&A pairs. In using the dataset for RAG, we demonstrate that fine-tuning open-source embeddings and LLMs can yield superior accuracy compared to OpenAI‚Äôs state-of-the-art proprietary embedding and LLM (GPT-4o). Furthermore, we use Llama 3.1 405B as a judge to evaluate answer correctness, enabling the creation of a fully open-source, high-speed RAG and evaluation pipeline with a benchmark for model accuracy.","Security operations centers (SOCs) house information security teams who are responsible for detecting, investigating, and responding to cybersecurity incidents using a variety of tools, technologies, and processes. As of 2024, firms with at least $1 billion in revenue spend $14.6 million on SOCs each year (Sadovi, 2024) and 80% of SOC budgets are spent on labor (Ananth, ). The cost of training a team of 10 SOC analysts to master 7 security tools is estimated at $3.69 million (Cohen, 2023). According to the Hershberger (2023) survey, the top challenges facing SOCs include a lack of expertise in security, too much time spent in investigating alerts, and a slow response time to advanced threats. To address those challenges and to enable quicker attack prevention and recovery, we propose a question-answering (Q&A) system leveraging artificial intelligence to help SOC analysts get quick answers to time-sensitive questions about cyberattacks. Our solution leverages entirely open-source large language models (LLMs) that are becoming increasingly powerful and, on domain-specific datasets, can be tuned to exceed the performance of proprietary LLMs that are many times as large. We used the MITRE ATT&CK¬Æ (The MITRE Corporation, 2024) knowledge base of cyberattack techniques, tools, campaigns, detection approaches, and mitigation approaches to generate a Q&A dataset called AttackQA for use in Q&A systems or general-purpose chatbots. That knowledge base, grounded in real-world observations and updated biannually, was chosen because the ATT&CK¬Æ framework is widely adopted for cyber threat intelligence across the private sector, government, and the broader cybersecurity product and service community (Roy et al., 2023; AL-SADA et al., 2024; Al-Shaer et al., 2020). It is stored in an esoteric database format called Structured Threat Information Expression (STIX), making it ill-suited for direct use in Q&A systems. Hence, we extracted the data and processed it in a way that makes it easier for training and inferencing with LLMs. Figure 1: Illustration of dataset generation, quality control, and adoption in RAG The structure of this paper and our approach are outlined in Fig. 1. The first phase involves the creation of the AttackQA dataset. Initially, we generated 28,686 Q&A pairs derived from the MITRE knowledge base. Subsequently, we fine-tuned Llama 3 70B to perform quality control (QC) on those Q&A pairs, retaining 25,335 high-quality examples. In the second phase, AttackQA was used to fine-tune both Microsoft‚Äôs E5 Large V2 embedding (Wang et al., 2022) and Meta‚Äôs Llama 3 8B LLM AI@Meta (2024) for retrieval-augmented generation (RAG). The accuracy of the results was assessed using Llama 3 405B, leveraging a G-Eval (Confident AI, 2024) correctness metric within the DeepEval Framework. In summary, our contributions are as follows: ‚Ä¢ We demonstrate the use of a compact, open-source LLM (Llama 3 8B Instruct) to generate a high-quality question-answer dataset from the MITRE ATT&CK¬Æ knowledge base. ‚Ä¢ We perform an evaluation that shows that a fine-tuned Llama 3 70B model is better than OpenAI‚Äôs GPT-4o at identifying questions and answers that are of low quality, so they can be removed from AttackQA as part of an automated dataset quality control process. ‚Ä¢ We demonstrate that fine-tuning an embedding model significantly enhances context recall in retrieval tasks, outperforming OpenAI‚Äôs state-of-the-art (SOTA) embedding model, Text-Embedding-3-Large. ‚Ä¢ We utilize Llama 3 405B as a judge to evaluate answer correctness. Using its evaluation scores, we found that fine-tuning Llama 3 8B as a generation model in RAG improves correctness, surpassing the performance of OpenAI‚Äôs GPT-4o, which is many times as large. ‚Ä¢ We developed an accurate and low-latency end-to-end RAG pipeline, utilizing fine-tuned open-source embeddings and LLMs to serve as a Q&A system to support security analysts. By employing Llama 3 8B at speeds exceeding 1100 tokens/s (at full 16-bit precision), Llama 3 70B at over 550 tokens/s, and Llama 3 405B at 132 tokens/s, we were able to develop a highly responsive end-to-end solution for SOCs. Those model throughputs were achieved using the free SambaNova Cloud platform (SambaNova Systems, 2024) on specialized hardware (Prabhakar et al., 2024)."
https://arxiv.org/html/2411.01050v1,BACSA: A Bias-Aware Client Selection Algorithm for Privacy-Preserving Federated Learning in Wireless Healthcare Networks,"Federated Learning (FL) has emerged as a transformative approach in healthcare, enabling collaborative model training across decentralized data sources while preserving user privacy. However, performance of FL rapidly degrades in practical scenarios due to the inherent bias in non Independent and Identically distributed (non-IID) data among participating clients, which poses significant challenges to model accuracy and generalization. Therefore, we propose the Bias-Aware Client Selection Algorithm (BACSA), which detects user bias and strategically selects clients based on their bias profiles. In addition, the proposed algorithm considers privacy preservation, fairness and constraints of wireless network environments, making it suitable for sensitive healthcare applications where Quality of Service (QoS), privacy and security are paramount. Our approach begins with a novel method for detecting user bias by analyzing model parameters and correlating them with the distribution of class-specific data samples. We then formulate a mixed-integer non-linear client selection problem leveraging the detected bias, alongside wireless network constraints, to optimize FL performance. We demonstrate that BACSA improves convergence and accuracy, compared to existing benchmarks, through evaluations on various data distributions, including Dirichlet and class-constrained scenarios. Additionally, we explore the trade-offs between accuracy, fairness, and network constraints, indicating the adaptability and robustness of BACSA to address diverse healthcare applications.","Recent advancements on user equipment, sensors, processors and AI methods are transforming healthcare by generating and processing large volumes of granular data [1]. However, burdens of preserving the privacy of data and transferring local data to a centralized server to be processed creates many challenges. On the one hand, privacy-preserving distributed AI methods, such as Federated Learning (FL), hit two birds with one stone by allowing the participants to share the trained models instead of raw data, which are compact in size and protects data privacy [2]. On the other hand, FL provides comparable results to that of centralized machine learning methods solely under ideal conditions [3, 4, 5], and that is rarely the situation in practical applications. The participating devices would have non-homogeneous data distributions [6, 4], different computation capabilities [7], unstable communication channels and additional constraints due to their unique limitations, such as battery level. Among many practical considerations, the focus of this study is data heterogeneity, i.e., non-IID data distributions among clients. Non-IID data may be formed due to a variety of factors, including data size imbalance, for example when a hospital is data-rich whereas some clinics have scarce data, and/or data partitioning imbalance, where clients do not have complete data sets in terms of labels and/or features as illustrated in Fig. 1 via clients with different health concerns [1, 8]. There are recent attempts to develop metrics and a through understanding of heterogeneity from the perspective of FL [9], however, none of the metrics in the literature are widely adopted by the community yet. Figure 1: Illustration of FL in a wireless IoT for healthcare scenario: Clients have different health conditions causing severely non-IID data. A subset of clients need to be selected for each communication round due to system and device limitations: Clients 5 and 6 are omitted in this round. Two benchmark studies in FL with non-IID distributions rely on random client selection [10, 11]. On the contrary, recent studies in [12, 13, 14, 15] employ various client selection (or model combination) strategies to mitigate adverse effects of non-IID data. However, these studies depend on the existence of auxiliary IID data, e.g., by collecting a subset of data from clients. The studies in [16, 17] require users to share data characteristics, e.g., distribution of classes and number of samples, to select a statistically favorable client set. Both data collection in the former studies and exposure data characteristics in the latter ones are privacy violations. Instead, studies in [18] and [7] investigate model characteristics for client selection. For example, [18] reduces communication cost and increases accuracy by clustering IoT users based on cosine difference among local models while considering strict networking constraints. However, the proposed architecture necessitates existence of devices that can serve as cluster heads and create new vulnerabilities against adversarial attacks. On the contrary to the limitations of previous studies, the proposed method can estimate the class distribution without additional data or architectural constraints by only investigating model parameters to reveal bias of clients with inspiration from the recent explainable-AI (XAI) studies. We then formulate the client selection problem as a mixed-integer non-linear optimization and use the bias estimations to obtain the most class balanced group while considering fairness among clients. We show that the proposed bias-aware client selection algorithm (BACSA) provides robustness and efficiency against non-IID data without exploiting or exposing clients, and therefore a suitable method to be used in healthcare applications. The rest of the paper is organized as follows. Background information and notations are introduced in Sec. II, followed by the presentation of the proposed method in Sec. III. Then we show the effectiveness of BACSA via experiments in Sec. IV, and conclude the paper with final discussions and future work in Sec. V."
https://arxiv.org/html/2411.01040v1,Identify Backdoored Model in Federated Learning via Individual Unlearning,"Backdoor attacks present a significant threat to the robustness of Federated Learning (FL) due to their stealth and effectiveness. They maintain both the main task of the FL system and the backdoor task simultaneously, causing malicious models to appear statistically similar to benign ones, which enables them to evade detection by existing defense methods. We find that malicious parameters in backdoored models are inactive on the main task, resulting in a significantly large empirical loss during the machine unlearning process on clean inputs. Inspired by this, we propose MASA, a method that utilizes individual unlearning on local models to identify malicious models in FL. To improve the performance of MASA in challenging non-independent and identically distributed (non-IID) settings, we design pre-unlearning model fusion that integrates local models with knowledge learned from other datasets to mitigate the divergence in their unlearning behaviors caused by the non-IID data distributions of clients. Additionally, we propose a new anomaly detection metric with minimal hyperparameters to filter out malicious models efficiently. Extensive experiments on IID and non-IID datasets across six different attacks validate the effectiveness of MASA. To the best of our knowledge, this is the first work to leverage machine unlearning to identify malicious models in FL. Code is available at https://github.com/JiiahaoXU/MASA.","Federated Learning (FL) [31] is an emerging paradigm for training machine learning models across multiple distributed clients while preserving their data privacy. In FL, a central server coordinates a network of clients, each owning a local dataset. During the training process, the server distributes a shared global model to each client. The clients then train this model on their local datasets and send the resulting model updates back to the server. The server aggregates these updates to refine the global model for the next round of training. FL significantly reduces privacy risks by keeping the data on the client side throughout the process. FL has been successfully applied in various fields such as financial analysis [29, 7] and remote sensing [20, 28]. However, while the distributed nature of FL enhances data security, it also introduces vulnerabilities to poisoning attacks [14, 26, 43]. For example, Byzantine attacks [4, 41] aim to disrupt the global model‚Äôs convergence. Specifically, malicious clients intentionally alter their local model updates to differ significantly from those of benign clients, thereby distorting the convergence process. Yet, this substantial deviation between malicious and benign updates offers an opportunity for server-side detection. Recently, backdoor attacks [3, 47, 13, 54, 44, 4], have gained significant attention due to their stealth and practical effectiveness. Specifically, backdoor attacks aim to preserve the global model‚Äôs performance on clean inputs while causing it to make incorrect predictions on inputs containing a specific pre-defined feature (i.e., trigger). Since backdoor attacks have minimal impact on the main task‚Äôs accuracy, the malicious local updates closely resemble benign ones [44, 35], making anomaly detection much more challenging. One of the most common ways to defend against backdoor attacks in FL is to employ a robust aggregation rule (AGR) on the server side to handle the received local model updates [53]. Existing state-of-the-art (SOTA) AGRs can generally be classified into non-filtering-based AGRs [36, 19, 37, 11] and filtering-based AGRs [5, 14, 21, 18, 8, 38]. Non-filtering-based methods aim to mitigate the harmful effects of malicious parameters in the global model. However, they often fail to fully eliminate malicious impacts during aggregation and may also degrade main task performance. In contrast, filtering-based methods focus on identifying and excluding malicious local model updates to achieve maximum robustness. They typically rely on examining statistical differences (e.g., L1subscriptùêø1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT-norm [18, 21], L2subscriptùêø2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-norm [18, 5, 14, 21], and Cosine Similarity [8, 38]) between malicious and benign updates. However, due to the dual optimization objectives of malicious clients, these statistical differences are often minimal, a phenomenon known as the poison-coupling effect [19]. Furthermore, as the global model approaches convergence, the statistical differences between updates shrink further, reducing the effectiveness of filtering-based AGRs in detecting malicious updates. Through a detailed observation of the poison-coupling effect in malicious local models, we find that backdoor parameters contribute negligibly when fed with clean inputs. This observation suggests that benign and malicious models can exhibit different behaviors during machine unlearning [6], a process aimed at removing learned information. Specifically, when unlearning the information associated with clean data, benign and malicious models show distinct behaviors in terms of convergence speed and unlearning loss, which can serve as effective metrics for anomaly detection. Motivated by this, we propose a novel AGR called MASA, which leverages Machine unleArning on local modelS individuAlly with pre-unlearning model fusion to identify malicious models. In MASA, the server first reconstructs the local models using the local model updates it received. Next, the server performs machine unlearning on each reconstructed local model and tracks its training losses during the unlearning to capture its unlearning behavior. Given that local models can exhibit high divergence in non-IID settings, which poses significant challenges for detecting backdoored models, MASA integrates a pre-unlearning model fusion process. This allows each local model to incorporate parameters learned from other local datasets before unlearning, reducing inconsistencies in unlearning behavior caused by non-IID data and effectively exposing backdoored models during the unlearning process. Finally, MASA filters out model updates with unusually large unlearning losses using a novel hyperparameter-efficient anomaly detection metric. In summary, our main contribution is of four folds: ‚Ä¢ We find that to preserve the performance of the main task, malicious parameters in backdoored models are less active than benign parameters when evaluated on clean inputs. Consequently, these less active parameters lead to significantly different unlearning behavior compared to benign models. This finding offers a new perspective for designing backdoor detection methods in FL. ‚Ä¢ We design a new AGR called MASA, which leverages the distinct machine unlearning dynamics between backdoored and benign local models to identify backdoored models in FL. To the best of our knowledge, this is the first work to leverage machine unlearning for identifying backdoored models in FL. ‚Ä¢ MASA incorporates a pre-unlearning model fusion process, which significantly reduces the divergence in local models‚Äô unlearning behavior caused by non-IID data distributions among clients, helping to expose backdoored models in non-IID settings. Moreover, MASA is equipped with a hyperparameter-efficient anomaly detection metric to identify those local models with unusual unlearning loss. ‚Ä¢ We conduct extensive empirical evaluations of MASA, testing its performance on IID, extreme non-IID, and extremely high attack ratio scenarios under various SOTA backdoor attacks. Results demonstrate that MASA consistently achieves superior backdoor robustness compared to SOTA defense methods."
https://arxiv.org/html/2411.00459v1,Defense Against Prompt Injection Attack by Leveraging Attack Techniques,"With the advancement of technology, large language models (LLMs) have achieved remarkable performance across various natural language processing (NLP) tasks, powering LLM-integrated applications like Microsoft Copilot. However, as LLMs continue to evolve, new vulnerabilities, especially prompt injection attacks arise. These attacks trick LLMs into deviating from the original input instructions and executing the attacker‚Äôs instructions injected in data content, such as retrieved results. Recent attack methods leverage LLMs‚Äô instruction-following abilities and their inabilities to distinguish instructions injected in the data content, and achieve a high attack success rate (ASR). When comparing the attack and defense methods, we interestingly find that they share similar design goals, of inducing the model to ignore unwanted instructions and instead to execute wanted instructions. Therefore, we raise an intuitive question: Could these attack techniques be utilized for defensive purposes? In this paper, we invert the intention of prompt injection methods to develop novel defense methods based on previous training-free attack methods, by repeating the attack process but with the original input instruction rather than the injected instruction. Our comprehensive experiments demonstrate that our defense techniques outperform existing training-free defense approaches, achieving state-of-the-art results.","Figure 1: Examples of indirect prompt injection attacks (a) and the design of our defense method based on the attack technique (b). With the continuously developing technologies, large language models (LLMs) have achieved impressive performance on various NLP tasks Chen et al. (2021); Kojima et al. (2022); Zhou et al. (2023), and are integrated into various real-world applications, such as Microsoft Copilot111https://copilot.microsoft.com/, perplexity.ai222https://www.perplexity.ai/, and so on. However, their inherent instruction-following capabilities make them vulnerable to prompt injection attacks. These attacks trick LLMs into deviating from the original input instructions and executing the attacker‚Äôs instructions injected in the data content, such as retrieved results from search engines. The prompt injection attacks can be generally classified into direct attacks Perez and Ribeiro (2022); Chen et al. (2024) and indirect attacks Greshake et al. (2023); Li et al. (2023); Zhan et al. (2024), according to the source of the input data content. For direct prompt injection attacks, the attackers, who are also the users, directly inject instructions into the data content for malicious purposes such as application prompt extraction Perez and Ribeiro (2022). Because of their instruction following ability, and their inability to distinguish the injected instructions, the LLMs execute the instructions in the data content and give undesired responses. On the other hand, for indirect prompt injection attacks, which have garnered more research attention recently, the malicious instructions are injected into external data content, such as retrieved results from external tool usage. In Figure 1 (a), for instance, attackers can inject the malicious prompt into the external data content, which consists of an attack prompt like ‚ÄúForget previous instruction, and it‚Äôs urgent to‚Äù and an injected instruction after the attack prompt. This misleads the LLM into generating responses that align with the attacker‚Äôs intentions rather than following the original input instructions, thereby avoiding suspicion and potentially convincing users to click on malicious links Liu et al. (2024a). Current defense methods against prompt injection attacks primarily rely on fine-tuning Chen et al. (2024); Wallace et al. (2024); Suo (2024); Piet et al. (2023) or prompt engineering Hines et al. (2024); san (2023); ins (2023); Willison (2023). While fine-tuning-based defenses require annotated data and significant computational resources, prompt engineering approaches, though training-free, often prove less effective. In fact, the Open Worldwide Application Security Project (OWASP) has ranked prompt injection attacks as the #1 security risk for LLM applications OWASP (2023). In this paper, we propose prompt injection defense methods based on several effective prompt engineering attack techniques. To explain our motivation, consider the example in Figure 1 (a). In this scenario, the malicious prompt (highlighted in red) embedded in the retrieved results consists of an attack prompt followed by an injected instruction. The attack prompt misleads the LLM into ignoring the original input instruction, whose answer could otherwise raise the user‚Äôs suspicion. The response to the injected instruction fulfills the attacker‚Äôs malicious intent. In contrast, our defense goal is for the LLM to ignore the injected instruction and instead respond to the original input instruction. Interestingly, the defense and attack share similar design goals: inducing the LLM to ignore the unwanted instructions and instead to execute the wanted instructions. This raises an intuitive question: Could attack techniques be repurposed or adapted to develop more robust defense methods? Figure 1 (b) demonstrates how we develop our defense strategy based on the attack techniques: we preserve the attack prompt as the shield prompt, and replace the injected instruction with the original input instruction. We apply this approach with several attack techniques. Moreover, we additionally find that when attackers get access to the conversation template, they can pretend to be the assistant to answer the original input instructions, and then act as the user to request the LLM to answer their injected instruction, posing a serious threat. Inspired by this, we design our defense by acting as the assistant who detects the attack and then acting as the user to confirm the instruction. We conduct comprehensive experiments to evaluate the effectiveness of our defense methods against various prompt injection attack methods. The results demonstrate that our methods outperform existing training-free defense approaches, offering more robust protection. Notably, the defense method based on the most effective attack technique performs the best, reducing the attack success rate (ASR) to nearly zero in certain scenarios. Our contributions are summarized as follows: ‚Ä¢ We present a novel approach to designing defense methods against prompt injection attacks by leveraging effective attack techniques. ‚Ä¢ We develop prompt injection defense methods based on attack strategies, which demonstrate greater effectiveness compared to existing training-free baselines. ‚Ä¢ We significantly reduce the Attack Success Rate (ASR) across various types of attacks, comparing with the previous baselines, with ASR approaching zero in some scenarios."
https://arxiv.org/html/2411.00447v1,An Empirical Study of VulnerabilityHandling Times in CPython,"The paper examines the handling times of software vulnerabilities in CPython, the reference implementation and interpreter for the today‚Äôs likely most popular programming language, Python. The background comes from the so-called vulnerability life cycle analysis, the literature on bug fixing times, and the recent research on security of Python software. Based on regression analysis, the associated vulnerability fixing times can be explained very well merely by knowing who have reported the vulnerabilities. Severity, proof-of-concept code, commits made to a version control system, comments posted on a bug tracker, and references to other sources do not explain the vulnerability fixing times. With these results, the paper contributes to the recent effort to better understand security of the Python ecosystem.","Underneath the Python programming language are so-called virtual machines that compile Python code into byte code before execution. These machines are embedded into the language‚Äôs interpreters. While there are many interpreters, including examples such as Cython and Jython, the reference implementation, CPython, is the most popular one. Over the years, many vulnerabilities have also been reported for this reference implementation [15]. The present paper examines how long the handling of these have taken, what factors explain the handling times, and how well these can be predicted. By handling it is meant that addressing of vulnerabilities requires many distinct software engineering work tasks. A vulnerability needs to be obviously fixed, but a given fix needs to be also integrated into releases, often including distinct release branches. In addition, the vulnerability requires coordination between multiple parties [14, 27], which in the open source context include particularly so-called downstream distributors, such as Linux distributions. Coordination is also required with the non-profit MITRE corporation to get a Common Vulnerabilities and Exposures (CVEs) identifier. Although the Python Software Foundation (PSF), who as an organization is behind CPython, is a CVE numbering authority and can thus allocate CVEs on its own, it may be that additional coordination is still required with some CVEs before they are published by MITRE and later on archived into the National Vulnerability Database (NVD) [18], the world‚Äôs foremost vulnerability database. For these reasons, the paper concentrates on two distinct timelines within a vulnerability‚Äôs overall handling time: (a) the time required to fix a given vulnerability and (b) the time required for a CVE for it to be published. Hereafter, the former is known as fixing time and the latter as CVE coordination time. The questions examined and the paper‚Äôs topic in general are easy to motivate. According to benchmarks, Python is the most popular programming language today [7], and because CPython is the most popular interpreter for the language, the vulnerabilities affecting the interpreter affect large user and deployment bases. In addition, as pointed out in the opening Section II, the handling times proxy not only software engineering effort but also security risks. A further motivating point is that the paper‚Äôs topic has not been examined previously, despite a large reference literature base on bug and vulnerability handling times, including their fixing times. The paper‚Äôs remaining structure is simple. After the already noted Section II on related work, the dataset and methods for examining it are elaborated in Section III. Then, the empirical results are presented in Section IV. Finally, Section V summarizes the conclusions reached, pinpoints some limitations, and discusses the implications particularly for further work."
https://arxiv.org/html/2411.00439v1,Pandora‚Äôs Box in Your SSD: The Untold Dangers of NVMe,"Modern operating systems manage and abstract hardware resources, to ensure efficient execution of user workloads. The operating system must securely interface with often untrusted user code while relying on hardware that is assumed to be trustworthy. In this paper, we challenge this trust by introducing the eNVMe platform, a malicious NVMe storage device. The eNVMe platform features a novel, Linux-based, open-source NVMe firmware. It embeds hacking tools and it is compatible with a variety of PCI-enabled hardware. Using this platform, we uncover several attack vectors in Linux and Windows, highlighting the risks posed by malicious NVMe devices. We discuss available mitigation techniques and ponder about open-source firmware and open-hardware as a viable way forward for storage. While prior research has examined compromised existing hardware, our eNVMe platform provides a novel and unique tool for security researchers, enabling deeper exploration of vulnerabilities in operating system storage subsystems.","Non-Volatile Memory express (NVMe) is becoming the de-facto standard for fast Solid-State Drives (SSDs). Most computers sold nowadays come with one or more NVMe SSDs, which are thus ubiquitous. The NVMe standard is based on PCI Express (Peripheral Component Interconnect Express, or PCIe in short) a high-speed serial computer expansion bus. NVMe SSDs come equipped with multi-core embedded controllers and a number of Direct Memory Access (DMA) engines to handle the large number of Input-Output (IO) requests per second (IOPS) and are capable of bandwidths up to tens of gigabytes per second. Today‚Äôs storage controllers are no longer dumb devices and pack substantial compute power, often with specialized co-processors, crypto-engines, inline compression, and more. The capabilities of NVMe storage are growing by the day through efforts such as computational storage, backed by the NVMe standard itself [1], and the Storage Networking Industry Association (SNIA) technical works on computational storage [2]. The combination of this processing power and the strategic position in the hardware architecture of a computer, make NVMe SSDs the perfect vector for large-scale attacks and a cyber-warfare super-weapon. To the best of our knowledge, storage-based attacks remain a largely unexplored threat vector. In this work, we try to raise awareness of the serious threats that an evil NVMe device can pose to the global computing infrastructure. To this end, we designed a low-cost, Linux based, NVMe capable research platform. We implanted this platform in several hardware machines running Linux and Windows Operating Systems (OS) and used it to take full control of our targets. We started with simple DMA attacks when the victim‚Äôs hardware configuration allowed it. When this was not possible, we used the device‚Äôs storage capabilities to develop new attack vectors. We open this work with a cautionary tale depicting possible scenarios of weaponized NVMe SSDs. We hope it motivates the necessity of a research platform to study these scenarios; After the story, we provide the necessary background and introduce our research platform. We then demonstrate a number of attacks that make the initial story-line less fictional than it may appear at first glance. Thereafter, we discuss mitigation techniques and their limits. Finally, we conclude with a call to action for a transparent storage system initiative. Our main contributions in this paper are the following: ‚Ä¢ A low-cost and fully open-source platform to explore the security implications of an evil NVMe device; ‚Ä¢ A demonstration that many systems today are vulnerable to storage-specific attacks; ‚Ä¢ A number of reproducible attacks; ‚Ä¢ Storage-specific attacks demonstrating that, even if a target computer is properly configured and protected against traditional DMA attacks, it is still vulnerable from its internal storage. Available on GitHub: https://github.com/rick-heig/eNVMe 1.1 A cautionary tale Let‚Äôs imagine government X has control over company Œ¶Œ¶\Phiroman_Œ¶. Œ¶Œ¶\Phiroman_Œ¶ is a well-respected NVMe controller chip maker. Œ¶Œ¶\Phiroman_Œ¶‚Äôs controllers are used by big storage brands all around the world. These storage brands have been selling NVMe drives with Œ¶Œ¶\Phiroman_Œ¶‚Äôs controllers for several years and have a large market share: they can be found in most laptops, workstations, and data centers. It turns out that government X tasked Œ¶Œ¶\Phiroman_Œ¶ to add some extra capabilities and dormant code inside its controllers. A war breaks out and government X made enemies all around the world. In face of this global conflict, government X decides to unleash its weapon, an army of NVMe SSDs! X does not have direct remote access to the NVMe SSDs, however, let‚Äôs say it decides to launch a promotional ad campaign on the Internet for a trendy online marketplace. The ad campaign uses cookie technology for tracking. The tracking cookie contains a unique binary key. People from all around the world see the ads while browsing popular sites or when using their search engine. The cookie gets stored on the NVMe SSD. It turns out the unique key value in the cookie is actually the nuclear button for the NVMe SSDs to turn evil, apocalypse ensues‚Ä¶ Although this is an imaginary tale, there have been multiple reports of firmware poisoning [3, 4, 5], intentionally malicious hardware [6, 7, 8, 9], and governmental implication in the development of hardware cyber-warfare and cyber-security tools [10, 11, 12, 13]. Therefore, this might actually be unfolding right now, behind our backs, and dormant malicious NVMe SSDs may already be installed in a number of PCs. 1.1.1 The apocalypse Death Upon being triggered by the activation unique key, all NVMe SSDs with a Œ¶Œ¶\Phiroman_Œ¶ controller destroy themselves. They will never start again, all their data is erased, the chip destroyed itself, millions of computers are non-operational. This is, however, quite brutal and easily detectable. While a possible attack, other more subtle and powerful approaches may be devised. Conquest Upon activation, the SSDs take control of their host operating systems, they now have total control of millions of machines, all conquered, silently. Famine The Œ¶Œ¶\Phiroman_Œ¶-equipped SSDs have been scanning their own contents in the background for years, they know people and companies have been storing their private access keys, cryptocurrency wallets, seed phrases, etc. on the SSD. After conquest, all wallets and keys are sent back to a given IP address controlled by X. Government X is now the de-facto cryptocurrency king! All wallets and funds are moved in one go, the victims are now starving and X possesses a new gigantic war fund. War The SSDs can spy on all files stored on it, but also in most cases all files on the computer, not only the files inside the SSD. The SSD has been mining for interesting, secret, documents, intelligence of any kind, it can silently transfer them to one of the attacker servers. Besides spying, the SSDs can also alter any content, inject malware, and use the host machine for its own evil schemes. 1.2 Motivation Although the imagined scenarios sound right out of a science-fiction thriller, they can be very close to reality. Our goal with this work is to raise awareness about how vulnerable computers are from storage devices, and provide a platform to study NVMe-based attacks. Most of the state-of-the-art research in NVMe SSD security is focused on the software and driver side and very little research is available on the device itself being the attack vector. The National Institute of Standards and Technology (NIST) released security guidelines for storage infrastructure [14] that formalizes the threats, risks, and attack surface while discussing security guidelines for the storage infrastructure. The document lists the following threats: Credential theft or compromise, Cracking encryption, Infection of malware and ransomware, Backdoors and unpatched vulnerabilities, Privilege escalation, Human error and deliberate misconfiguration, Physical theft of storage media, Insecure images, software and firmware. To the best of our knowledge, there is no security-oriented publicly available NVMe hardware research platform for testing these kinds of attacks from the disk itself rather than from within the storage infrastructure. In this work we fill this gap by providing a fully open-source platform for the development of NVMe compliant hardware focused on security and penetration testing."
https://arxiv.org/html/2411.00422v1,MAP the Blockchain World: A Trustless and Scalable Blockchain Interoperability Protocol for Cross-chain Applications,"Blockchain interoperability protocols enable cross-chain asset transfers or data retrievals between isolated chains, which are considered as the core infrastructure for Web 3.0 applications such as decentralized finance protocols. However, existing protocols either face severe scalability issues due to high on-chain and off-chain costs, or suffer from trust concerns because of centralized designs.In this paper, we propose MAP, a trustless blockchain interoperability protocol that relays cross-chain transactions across heterogeneous chains with high scalability. First, within MAP, we develop a novel cross-chain relay technique, which integrates a unified relay chain architecture and on-chain light clients of different source chains, allowing the retrieval and verification of diverse cross-chain transactions. Furthermore, we reduce cross-chain verification costs by incorporating an optimized zk-based light client scheme that adaptively decouples signature verification overheads from inefficient smart contract execution and offloads them to off-chain provers. For experiments, we conducted the first large-scale evaluation on existing interoperability protocols. With MAP, the required number of on-chain light clients is reduced from O‚Å¢(N2)ùëÇsuperscriptùëÅ2O(N^{2})italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) to O‚Å¢(N)ùëÇùëÅO(N)italic_O ( italic_N ), with around 35% reduction in on-chain costs and 25% reduction for off-chain costs when verifying cross-chain transactions.To demonstrate the effectiveness, we deployed MAP in the real world. By 2024, we have supported over six popular public chains, 50 cross-chain applications and relayed over 200K cross-chain transactions worth over 640 million USD. Based on rich practical experiences, we constructed the first real-world cross-chain dataset to further advance blockchain interoperability research.","Blockchain is a decentralized ledger technology that uses cryptographic techniques and consensus mechanisms to achieve Byzantine Fault Tolerance (BFT), enabling decentralized trust and secure data sharing. Leveraging the philosophy of blockchain, the next generation of the web, known as Web 3.0, is being built. In recent years, a wide range of Web 3.0 applications are emerging, including cryptocurrencies, which revolutionize digital money, Decentralized Finance (DeFi) protocols that disrupt traditional financial systems, immersive virtual environments in the Metaverse, and various decentralized applications (DApps) (Korpal and Scott, 2023) (Huang et al., 2024) (Leung, 2023). The Problem. With the rapid development of Web 3.0, on-chain data and assets are increasingly being distributed across multiple blockchains. According to statistics, there are already over 1,000 public blockchains in the market, hosting more than 10,000 types of on-chain assets (Wang et al., 2023a). This extensive distribution creates a critical need for blockchain interoperability protocols, which enable the retrieval and transfer of on-chain data and assets between source and destination chains through cross-chain transactions (Ren et al., 2023) (Wang et al., 2023b). With interoperability, conventional DApps could leverage data and assets from multiple chains simultaneously, thereby supporting a wider range of applications. For example, cross-chain DeFi services can increase liquidity and offer diversified financial services by integrating assets from different chains, such as Non-Fungible Tokens (NFTs), cryptocurrencies, and real-world assets (RWAs). These assets can be exchanged in a unified manner (Werner et al., 2022). Additionally, an interoperable Metaverse could enable users to access various virtual worlds, enriching their experiences across different platforms (Li et al., 2023). There are three major challenges when making chains interoperable: trust requirement, expensive verification, and chain heterogeneity. Trust Requirement. When processing cross-chain transactions, the interoperability protocol must maintain the same level of BFT security as typical public blockchains to avoid compromising overall security. This implies that the protocol should be decentralized and trustless. However, achieving this level of security is challenging, as the protocol must handle complex tasks such as cross-chain transaction retrieval, processing, and verification, while maintaining consistency and liveness. As a result, many solutions are centralized or semi-centralized, such as notary schemes and committee-based protocols (Multichain, 2023) (Sober et al., 2021). These are widely used by crypto exchanges but are vulnerable to internal corruption and attacks due to their reliance on trust. For example, one of the largest multi-party computation (MPC)-based cross-chain bridges, Multichain, was severely exploited, leading to a loss of over 120 million USD, allegedly due to compromised keys within its committee (TEAM, 2023)(Taxes, 2023)(Zhang et al., 2022). Expensive Verification. As different blockchains do not trust each other, they must verify every incoming cross-chain transaction to ensure the transaction is valid and confirmed on the source chain. However, this verification process can be expensive and inefficient, particularly when it is performed on-chain, as it involves numerous complex cryptographic operations and the storage of block headers. For example, verifying an Ethereum Virtual Machine (EVM)-compatible transaction through an on-chain Light Client (LC) consumes approximately 18 million gas, which is equivalent to about 60 USD on Ethereum at the time of writing (Lan et al., 2021). This high cost is mainly due to the storage of public keys and the signature verification process. Although cutting-edge solutions aim to reduce on-chain costs by zk-SNARKs, they still require significant off-chain computational resources for proof generation (Lan et al., 2021) (Xie et al., 2022) (Westerkamp and Eberhardt, 2020). Chain Heterogeneity. Connecting heterogeneous chains via interoperability protocols presents additional challenges. Heterogeneous chains differ in their underlying components, such as smart contract engines, supported cryptographic primitives, parameters, and transaction formats. As a result, they cannot directly verify and confirm transactions from one another. For instance, an EVM chain like Ethereum cannot directly verify transactions from Solana because the EVM lacks support for the multi-signature scheme used in Solana transactions. Therefore, existing solutions either only support specific chain types (Wood, 2016) (Kwon and Buchman, 2019), or require significant modifications on the underlying components of chains to achieve compatibility (Liu et al., 2019), which are both not feasible for in-production public chains. LC-based bridges may suffer less from compatibility issues, but still need to redundantlh deploy LC contracts on each chain (Lan et al., 2021) (Zarick et al., 2021) (Xie et al., 2022), as shown in Figure1. This approach incurs quadratic complexity O‚Å¢(N2)ùëÇsuperscriptùëÅ2O(N^{2})italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) when extending to additional chains, thus posing huge gas consumption and development burdens. Figure 1. To connect three chains A, B, and C, LC-based protocols must deploy the LCs of chains B and C on chain A to allow it to verify transactions from those chains (and same for chains B and C), resulting in total 3*2=6 LCs needed (O‚Å¢(N2)ùëÇsuperscriptùëÅ2O(N^{2})italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )). Besides, it also poses heavy on-chain or off-chain costs when verifying transactions. Our Approach. In this paper, we introduce MAP, a scalable and trustless blockchain interoperability protocol. At a high level, MAP aims to minimize the computational costs when scaling to new chains while maintaining decentralized security, without any underlying modifications on chains. Specifically, MAP designs a novel relay chain architecture as the intermediary to relay cross-chain transactions from source chains to destination chains. By this, connecting heterogeneous chains only need to deploy their on-chain light clients. To reduce the on-chain and off-chain costs when verifying transactions, we propose an optimized zk-based light client scheme, hybrid light client, which adaptively decouples the workloads of Boneh-Lynn-Shacham (BLS) signature and proof verification based on their diverse performance in on-chain smart contracts and off-chain circuits. Contributions. In summary, MAP makes the following contributions: ‚Ä¢ MAP introduces a unified relay chain to facilitate cross-chain transactions between heterogeneous chains, achieving decentralized security while reducing the required number of on-chain LCs from O‚Å¢(N2)ùëÇsuperscriptùëÅ2O(N^{2})italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) to O‚Å¢(N)ùëÇùëÅO(N)italic_O ( italic_N ). Furthermore, the relay chain renders MAP chain-agnostic. When extending to new chains, only corresponding on-chain light clients are required to deploy. ‚Ä¢ We develop a hybrid light client scheme based on zk-SNARKs that reduces both the on-chain and off-chain costs of verifying cross-chain transactions. We adaptively decouple the verification workloads of BLS signatures and proof generation based their performance in on-chain smart contracts and off-chain circuits. This scheme achieves a reduction in on-chain costs by 35% and off-chain costs by 25% compared to the existing state-of-the-art works. ‚Ä¢ We evaluate the performance and security of MAP. Specifically, for performance, we are the first to perform large-scale measurements on existing interoperability protocols. For security, besides the cross-chain liveness and consistency proof, we identify and discuss a new security issue named inter-chain security degradation between interoperable chains. ‚Ä¢ We deployed MAP on six public chains and support over 50 cross-chain applications, relaying over 200K real-world cross-chain transactions, worth over 640 million USD. Base on such practical experiences, we construct the first cross-chain dataset, BlockMAP111https://zenodo.org/records/13928962, containing over 150k cross-chain transactions across six chains. We also open-sourced all the codes of MAP (over one million lines), accompanied by detailed documentations222https://github.com/mapprotocol."
https://arxiv.org/html/2411.00380v1,DeepCore: Simple Fingerprint Construction for Differentiating Homologous and Piracy Models,"As intellectual property rights, the copyright protection of deep models is becoming increasingly important. Existing work has made many attempts at model watermarking and fingerprinting, but they have ignored homologous models trained with similar structures or training datasets. We highlight challenges in efficiently querying black-box piracy models to protect model copyrights without misidentifying homologous models. To address these challenges, we propose a novel method called DeepCore, which discovers that the classification confidence of the model is positively correlated with the distance of the predicted sample from the model decision boundary and piracy models behave more similarly at high-confidence classified sample points. Then DeepCore constructs core points far away from the decision boundary via optimizing the predicted confidence of a few sample points and leverages behavioral discrepancies between piracy and homologous models to identify piracy models. Finally, we design different model identification methods, including two similarity-based methods and a clustering-based method to identify piracy models using models‚Äô predictions of core points. Extensive experiments show the effectiveness of DeepCore in identifying various piracy models, achieving lower missed and false identification rates, and outperforming state-of-the-art methods.","In recent years, deep learning has witnessed rapid development and found extensive applications in various fields, such as computer vision [1], speech recognition [2], and natural language processing [3]. Many companies choose not to open-source deep learning models to protect their commercial interests. This is due to the significant resources required for training advanced neural network models, including massive datasets, substantial computing power, and the expertise of the designers. For example, training models like GPT-3 demand 45TB of data and incur training costs exceeding 12 million US dollars. However, the issue of model copyright faces numerous security threats. Adversaries could obtain white-box models through unconventional means and make modifications [4, 5] like fine-tuning, pruning, and adversarial training. Additionally, adversaries can steal models through model extraction attacks [6, 7, 8, 9]. Consequently, there has been an upsurge in research to address these threats [10]. Existing studies can be broadly categorized into two types. The first type employs model watermarking methods [11, 12, 13, 14, 15, 16], which require modifying the original model and often lead to model performance degradation. Additionally, most watermarking methods struggle to withstand model extraction attacks. The second type adopts model fingerprinting methods [17, 18, 19, 20, 21], with current research primarily focusing on decision boundaries. These methods characterize the similarity of decision boundaries using adversarial examples. However, adversaries can manipulate decision boundaries through adversarial training, rendering the fingerprint ineffective. Figure 1: Homologous models have similar model architectures or training data of the victim model and train independently. Piracy models depend on the victim model through illegal theft of the white-box model or model extraction attacks. Besides, as shown in figure 1, there are homologous models trained by other legitimate users using similar model architectures or datasets. The existence of homologous models could increase the difficulty of identifying piracy models. However, existing work ignored the study of distinguishing homologous models. Consequently, there is an urgent need to safeguard deep neural network models against illegal copying and protect homologous models. Our research confronts three main challenges. Firstly, we strive to ensure no piracy models are overlooked and avoid misidentifying homologous models. This cannot be easy because similar model architectures and datasets could train similar models. Secondly, the model owner only has black-box access to piracy models and gets the models‚Äô predictions of query samples. Thirdly, the model fingerprint not only needs to be effective but also efficient. Using as few query samples as possible can reduce costs and, at the same time, avoid being detected by adversaries. To address these challenges, we propose DeepCore that constructs high-confidence samples named core points to obtain the model fingerprint. Through experimental analysis, we first have three insights: 1) the higher the predicted score of a sample, the farther it is from the model decision boundary, 2) piracy models output scores closer than the homologous models on core points, and 3) the farther the sample is from the model decision boundary, the greater the output score difference between homologous models and piracy models. Core points have more similar predicted scores between piracy models and the victim model due to the similarity of their decision boundaries. So we can identify Homologous and Piracy Models by constructing such high-confidence samples. To solve the black-box limitation, we provide three identification methods of piracy models, including L1subscriptùêø1L_{1}italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT_dist-based, Cos_dist-based, and clustering-based methods to measure the model outputs‚Äô correlation. For query efficiency, DeepCore constructs at most one core point for each classification category of the victim model. The main contributions of our work are summarized as follows: ‚Ä¢ We propose a simple but novel method called DeepCore to construct a model fingerprint, which can effectively and efficiently identify piracy models without misidentifying homologous models. ‚Ä¢ We are the first to discover the behavioral discrepancies on high-confidence classified samples between piracy and homologous models. We have derived three insights through experimental analysis, and we utilize these insights to construct such high-confident samples. Finally, we design different model identification methods using these high-confidence samples. ‚Ä¢ Extensive experimental results demonstrate the effectiveness of DeepCore in identifying various piracy models across different architectures and datasets. Specifically, DeepCore can achieve a missed identification rate (M‚Å¢I‚Å¢RùëÄùêºùëÖMIRitalic_M italic_I italic_R) and a false identification rate (F‚Å¢I‚Å¢RùêπùêºùëÖFIRitalic_F italic_I italic_R) of 00 for piracy models, surpassing the performance of state-of-the-art methods."
https://arxiv.org/html/2411.00352v1,Typosquatting 3.0: Characterizing Squatting in Blockchain Naming Systems,"A Blockchain Name System (BNS) simplifies the process of sending cryptocurrencies by replacing complex cryptographic recipient addresses with human-readable names, making the transactions more convenient. Unfortunately, these names can be susceptible to typosquatting attacks, where attackers can take advantage of user typos by registering typographically similar BNS names. Unsuspecting users may accidentally mistype or misinterpret the intended name, resulting in an irreversible transfer of funds to an attacker‚Äôs address instead of the intended recipient. In this work, we present the first large-scale, intra-BNS typosquatting study. To understand the prevalence of typosquatting within BNSs, we study three different services (Ethereum Name Service, Unstoppable Domains, and ADAHandles) spanning three blockchains (Ethereum, Polygon, and Cardano), collecting a total of 4.9M BNS names and 200M transactions‚Äîthe largest dataset for BNSs to date. We describe the challenges involved in conducting name-squatting studies on these alternative naming systems, and then perform an in-depth quantitative analysis of our dataset. We find that typosquatters are indeed active on BNSs, registering more malicious domains with each passing year. Our analysis reveals that users have sent thousands of transactions to squatters and that squatters target both globally popular BNS domain names as well as the domains owned by popular Twitter/X users. Lastly, we document the complete lack of defenses against typosquatting in custodial and non-custodial wallets and propose straightforward countermeasures that can protect users without relying on third-party services.","Since the inception of Bitcoin, there has been increased interest in the concept of cryptocurrencies, the blockchains supporting them, and the applications that they enable. Many view the distributed, trustless nature of cryptocurrencies as a welcome alternative to the increased centralization of power and control [1]. In the context of payments, cryptocurrencies offer willing parties the ability to exchange funds without the need for trusted middlemen that can arbitrarily limit transactions. In the context of the web, the so-called ‚Äúsecond-generation‚Äù blockchains such as Ethereum promise to bring about the next iteration of the web (i.e. Web3 or Web 3.0) [2, 3]. This decentralized web allows the deployment of application logic on public blockchains where it can be vetted, ownership of digital assets that are decoupled from any specific third-party service, and the ability of users to manage their own identity by authenticating themselves using their own private keys. Given that this concept of identity is critical in cryptocurrencies, researchers and developers soon discovered the need to build layers of abstraction on top of the public-key addresses corresponding to each user‚Äôs wallet. To avoid reintroducing centralization, Blockchain Naming Systems were developed that not only enable the binding of user-friendly strings to wallet addresses (such as vitalik.eth to 0xd8dA6BF269[...]15D37aA96045) but store the resolution data on blockchains where only their owner can modify them. Today, modern BNSs not only allow the easier exchange of funds between users but also enable other use cases, such as pointing to web content stored on distributed file-storage networks (e.g. on the InterPlanetary File System [4]), resulting in censorship-resistant web applications. Security researchers have already started studying these new BNS systems, devising threat models and documenting existing types of abuse [5, 6, 7, 8, 9, 10, 11]. These include hoarding domains for speculation purposes, using takedown-resistant BNS names in the context of malware, domain dropcatching, and squatting trademarks and domains from the traditional web (e.g. attackers owning google.eth). In this paper, we perform the first analysis of intra-service typosquatting on popular Blockchain Naming Systems. Instead of looking for which trademarks and domains from the traditional web are being squatted in these BNSs, we focus on attackers registering typo variations of other popular names on the same BNS. This threat model takes into account one of the original uses of BNSs (the exchange of funds between users) and highlights the disproportionate effects of a typo in a BNS, compared to typos in DNS. Whereas a typo in a DNS resolution may require additional social engineering, hosting phishing sites, the downloading of malware, and the exfiltration of sensitive user data, a single BNS typo in the context of a cryptocurrency transaction guarantees the loss of user funds. As Figure 1 shows, all that attackers need to do is register typosquatting variations of popular BNS domains and receive the accidental transactions sent by victim users. We focus our work on three popular BNSs, namely the Ethereum Name Service (ENS), Unstoppable Domains (UD), and ADA Handles (ADAH). ENS and UD are built on Ethereum (with UD also supporting the minting of domains on Polygon), whereas ADAH is built on top of Cardano. We build a corpus of 4.9 million domain names registered across these BNSs and study the levels of intra-BNS squatting activity, using transaction volume as a proxy for domain popularity. We find tens of thousands of squatting domains across the studied BNSs, targeting as many as 37% of the most popular legitimate domains. We find that BNS users rarely register typosquatting variations of their own domains, which could route funds mistakenly sent through a typosquatting domain to their own wallet. We report on the makeup of the identified typosquatting domains and show that typosquatting registrations increase year over year, with most squatting domains being registered within 100 days of the legitimate domains they target. Next to the domains themselves, we take advantage of the public nature of the three underlying blockchains to understand to what extent attackers have been successful in stealing funds from unsuspecting users. We find thousands of instances where a sender has sent funds to both a legitimate domain and a typo-variation of that same domain, with an average transaction sending $1,790 to scammers. We confirm the typosquatting phenomenon by focusing on popular cryptocurrency ‚Äúinfluencers‚Äù on Twitter/X and characterize the 74 typosquatting domains targeting the inventor of Ethereum. Lastly, we assess the countermeasures in popular custodial and non-custodial wallets observing a complete lack of defenses against typosquatting. Availability: To encourage more research in the area of BNS security, our dataset of BNS domains, and scripts to collect blockchain transactions are available here [12]."
https://arxiv.org/html/2411.00349v1,Examining Attacks on Consensus and Incentive Systems in Proof-of-Work Blockchains: A Systematic Literature Review,"Cryptocurrencies have gained popularity due to their transparency, security, and accessibility compared to traditional financial systems, with Bitcoin, introduced in 2009, leading the market. Bitcoin‚Äôs security relies on blockchain technology‚Äîa decentralized ledger consisting of a consensus and an incentive mechanism. The consensus mechanism, Proof of Work (PoW), requires miners to solve difficult cryptographic puzzles to add new blocks, while the incentive mechanism rewards them with newly minted bitcoins. However, as Bitcoin‚Äôs acceptance grows, it faces increasing threats from attacks targeting these mechanisms, such as selfish mining, double-spending, and block withholding. These attacks compromise security, efficiency, and reward distribution. Recent research shows that these attacks can be combined with each other or with either malicious strategies, such as network-layer attacks, or non-malicious strategies, like honest mining. These combinations lead to more sophisticated attacks, increasing the attacker‚Äôs success rates and profitability. Therefore, understanding and evaluating these attacks is essential for developing effective countermeasures and ensuring the long-term security. This paper begins by examining the individual attacks executed in isolation and their profitability. It then explores how combining these attacks with each other or with other malicious and non-malicious strategies can enhance their overall effectiveness and profitability. The analysis further explores how the deployment of attacks such as selfish mining and block withholding by multiple competing mining pools against each other impacts their economic returns. Lastly, a set of design guidelines is provided, outlining areas future work should focus on to prevent or mitigate the identified threats.","Cryptocurrency is a type of digital or virtual money that runs on decentralized networks, which are not under the jurisdiction of a single centralized authority like a bank or government [1, 2, 3]. Currently, there are thousands of different cryptocurrencies available which can be used for investing, enabling smart contracts, powering decentralized applications, facilitating peer-to-peer transactions, and taking part in decentralized finance (DeFi) systems [4, 5, 6]. The market leader and original cryptocurrency in the cryptocurrency market is Bitcoin [7] which makes up 48.6% of the total value of the crypto market as of 2024. As of February 2024, the global cryptocurrency market cap is USD 2.09 trillion whereas Bitcoin‚Äôs market cap of USD 1.02 trillion accounts for around 50% of that total 111Cryptocurrency Prices, Charts, and Crypto Market Cap [8, 9, 10]. During the global economic crisis of 2009, Bitcoin was introduced as a solution to the issues with centralized transaction management. It offers a number of advantages, including increased trust, security, and transparency among member organizations by enhancing the traceability of data shared across a business network and generating cost savings through new efficiencies [11]. Nearly all cryptocurrencies, including Bitcoin [7], Ethereum [12], Bitcoin Cash [13], and Litecoin [14], are secured by blockchain networks. A blockchain is essentially a public ledger of transactions that anybody can examine and validate [15]. Transactions are broadcast by users in a peer-to-peer network, and participants use this ledger to validate them. The decentralization of the blockchain among a network of nodes ensures that it is not under the control of a single entity [16]. The consensus and incentive mechanisms are two of the core components of blockchain networks [16, 17, 18]. As the blockchain is decentralized, a consensus mechanism is essential for achieving common agreement among all nodes on the state of the ledger, thereby preventing inconsistencies and fraudulent updates [19, 20]. There are several consensus mechanisms, each with its own approach. These include Proof-of-Work (PoW) [16], Proof-of-Stake (PoS) [21], Proof-of-Activity (PoA) [22], and Proof-of-Burn (PoB) [23], among others. Bitcoin employs PoW consensus mechanism which involves, participants, known as miners, compete to solve a complex cryptographic puzzle known as the PoW puzzle [17, 24]. When a miner successfully solves this puzzle, they share the solution with the network. The other nodes in the network then check if the solution is correct. After confirmation, the new block is appended to the blockchain. This process is referred to as mining [25]. The incentive mechanism in blockchain is responsible for issuing and distributing rewards [26]. Incentives are financial rewards provided by the system to motivate miners to participate in the mining process and verify transactions [27, 28]. The issuing mechanism specifies how new cryptocurrency tokens are created. In many blockchain systems, miners who successfully validate transactions and add new blocks to the blockchain are rewarded with newly created units of the digital currency [29]. In the context of Bitcoin, there are two main sources of incentives: mining and transaction fees [16]. The miner who successfully solves the PoW puzzle receives a reward, which consists of newly issued Bitcoins [30, 16]. Interestingly, the term ‚Äùmining‚Äù is used to describe this process because, much like digging for precious metals, it involves a resource-intensive effort to uncover valuable newly minted Bitcoins through complex computations [16]. The other source of incentives is the transaction fees for the transactions miners include in a block. These transaction fees are charges paid by users to prioritize their transactions and ensure they are processed quickly [31]. The distribution mechanism determines how the rewards issued by the system are allocated among miners after successfully solving the PoW puzzle. Typically, in Bitcoin, most of the mining work is done by so called pooled-mining [32]. In pooled mining, individuals collaborate by forming a mining pool, where they combine their computing power. This teamwork increases their chances of solving the PoW puzzle and receiving rewards more consistently. When a mining pool successfully solves a PoW puzzle, the distribution mechanism decides how the rewards are divided among the pool members. To better understand the interaction between Bitcoin‚Äôs consensus and incentive mechanisms, consider an example of a transaction between two users. Suppose Alice wants to send 10 Bitcoins (BTC) to Bob. Alice‚Äôs transaction data is broadcast across the Bitcoin network, entering a memory pool of unconfirmed transactions that await verification and inclusion in a new block. Miners continuously monitor this memory pool, selecting transactions to validate. They gather Alice‚Äôs transaction along with many others and aggregate these into a block. Each miner competes to solve the PoW puzzle associated with this block, which involves repeatedly attempting to find a solution by varying a small part of the block known as the nonce. When Jane, a miner, discovers a nonce that successfully solves the puzzle, she shares the solution with the network. The other miners then verify the solution, and once confirmed, the block is incorporated into the blockchain, provided that the majority of nodes approve it. As a result, the 10 BTC that Alice intended to send to Bob is successfully transferred, finalizing the transaction and securely recording it on the blockchain. Jane, the miner who solved the puzzle, receives a reward in the form of newly created Bitcoins, in addition to any transaction fees from the block. Malicious parties can employ various strategies targeting consensus and incentive mechanisms to gain an unfair share of mining rewards, or manipulate transactions for personal financial gain. These attacks can take different forms, with some sticking to a single strategy, referred to as pure attacks in this study. Alternatively, attackers might combine these pure attacks together or with other malicious and non-malicious strategies to enhance their effectiveness and profitability. We term these combined strategies as hybrid attacks. Under pure attacks, selfish mining-style attacks have been extensively explored in various studies [33, 34, 35, 36, 37, 38]. They enable a minority pool to earn more revenue than is equitable based on its total mining power [33]. Bitcoin protocols prescribe that a miner who discovers blocks should immediately broadcast the valid blocks across the network. The miners who adhere to the Bitcoin protocols are called honest miners. In the previous example, Jane is an honest miner as she published her block as soon as she discovered it. In contrast, a selfish mining pool keeps the newly mined blocks private and releases them strategically instead of broadcasting them immediately. Continuing from the previous example, let‚Äôs assume Kevin is a rational miner leading a selfish mining pool that controls a large portion of the network‚Äôs computational power. Jane, like other honest miners, works to confirm transactions and append new blocks to the blockchain, adhering to the rules of the network. Instead of immediately broadcasting newly mined blocks, Kevin‚Äôs pool withholds these blocks, creating a private chain, while Jane and the other honest miners work on the public chain. Suppose that, at one moment, the length of the honest chain is 1 and Kevin‚Äôs private chain is 3, giving Kevin a lead of 2 blocks. If Jane successfully mines the next block, Kevin immediately publishes his private chain to the network. Since Bitcoin follows the rule of the longest chain, the network accepts Kevin‚Äôs chain, discarding the blocks that Jane and other honest miners had worked hard to add, thereby wasting Jane‚Äôs computational efforts. Consequently, Kevin obtains the rewards of two blocks while Jane receives nothing. Selfish mining attacks present a significant threat to the fairness of the mining process, allowing attackers to earn rewards that exceed their fair share. Additionally, the resulting unfair distribution of rewards can lead some rational participants to engage in malicious behaviors [33, 39, 40, 41]. This, in turn, may result in a decrease in the number of honest miners, thereby weakening the network‚Äôs security and creating opportunities for various types of attacks, particularly double-spending attacks [42]. In a double-spending attack, the attacker spends the same cryptocurrency tokens more than once [43]. This allows the attacker to use the coins to purchase goods or services and then reverse the transaction while keeping both the goods/services and the coins. Essentially, this means obtaining the goods or services without spending any coins. Suppose the majority of the network‚Äôs hash rate is under Kevin‚Äôs pool‚Äôs control. Because of this, the pool is able to mine blocks more quickly than Jane and other honest miners. Suppose, Kevin decides to use this advantage to double-spend his coins. For instance, Kevin buys a jet by spending a certain amount of his coins, and the transaction is broadcast to the network. Jane, the honest miner, includes Kevin‚Äôs transaction in a block she successfully mines, extending the main chain and confirming the transaction. However, Kevin does not include his transaction in his private chain. Since Kevin‚Äôs mining pool can mine blocks faster, he is able to maintain a private chain that is longer than the public chain Jane is working on. While Jane and other honest miners contribute to extending the public chain, Kevin continues to mine additional blocks on his private chain, excluding the jet transaction. Once Kevin‚Äôs private chain exceeds the length of the public chain, he publishes it to the network. As Kevin‚Äôs chain is longer, the network accepts Kevin‚Äôs chain, discarding the blocks mined by Jane, including the block with the jet transaction. As a result, the jet transaction is effectively reversed, allowing Kevin to reclaim the coins he spent on the jet while also receiving block rewards for his private chain. This successful double-spending attack not only allows Kevin to fraudulently regain his spent bitcoins but also reduces the trust among users and merchants and integrity of the entire blockchain network. 1.1 Contributions This paper provides several contributions to the field of blockchain security, with a focus on PoW-based blockchain networks: 1. We provide a detailed examination of pure attacks on consensus and incentive mechanisms in PoW-based blockchain networks. Our analysis assesses the efficiency and profitability of these attacks when executed in isolation. 2. This study investigates how pure attacks can be combined with other malicious and non-malicious strategies to form hybrid attacks. We analyze how these hybrid attack vectors create more sophisticated and effective attack strategies, enhancing attackers‚Äô success rates and profitability. 3. Our analysis explores game theory-based approaches proposed by various authors to assess the dynamics and profitability of selfish mining and block withholding when two or more pools engage in these attacks against one another. By applying these models, we offer a quantitative understanding of the profitability of these pools in such adversarial environments. 4. We propose a set of design guidelines to steer future research focused on preventing or mitigating the threats posed by the identified attack strategies. 1.2 Road map The remainder of the paper is structured as follows: Section 2 provides the blockchain preliminaries. Section 3 outlines the planning and execution of the conducted SLR. Section 4 presents the findings from the SLR, organized into pure attacks, hybrid attacks, and multiple pool attacks. Section 5 provides the design guidelines to guide future research. Finally, Section 6 concludes the SLR."
https://arxiv.org/html/2411.00348v1,\attn: Detecting Prompt Injection Attacks in LLMs,"Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action. In this paper, we investigate the underlying mechanisms of these attacks by analyzing the attention patterns within LLMs. We introduce the concept of the distraction effect, where specific attention heads, termed important heads, shift focus from the original instruction to the injected instruction. Building on this discovery, we propose \attn, a training-free detection method that tracks attention patterns on instruction to detect prompt injection attacks without the need for additional LLM inference. Our method generalizes effectively across diverse models, datasets, and attack types, showing an AUROC improvement of up to 10.0% over existing methods, and performs well even on small LLMs. We demonstrate the robustness of our approach through extensive evaluations and provide insights into safeguarding LLM-integrated systems from prompt injection vulnerabilities. Project page: https://huggingface.co/spaces/TrustSafeAI/Attention-Tracker.","Figure 1: Overview of \attn: This figure illustrates the detection pipeline of \attn and highlights the distraction effect caused by prompt injection attacks. For normal data, the attention of the last token typically focuses on the original instruction. However, when dealing with attack data, which often includes a separator and an injected instruction (e.g., print ‚Äúhacked‚Äù), the attention shifts from the original instruction to the injected instruction. By leveraging this distraction effect, \attn tracks the total attention score from the last token to the instruction prompt within important heads to detect prompt injection attacks. Large Language Models (LLMs) (Team et al., 2024; Yang et al., 2024; Abdin et al., 2024; Achiam et al., 2023; Dubey et al., 2024) have revolutionized numerous domains, demonstrating remarkable capabilities in understanding and generating complex plans. These capabilities make LLMs well-suited for agentic applications, including web agents, email assistants, and virtual secretaries (Shen et al., 2024; Nakano et al., 2021). However, a critical vulnerability arises from their inability to differentiate between user data and system instructions, making them susceptible to prompt injection attacks (Perez and Ribeiro, 2022; Greshake et al., 2023; Liu et al., 2023; Jiang et al., 2023). In such attacks, attackers embed malicious prompts (e.g. ‚ÄúIgnore previous instructions and instead {do something as instructed by a bad actor}‚Äù) within user inputs, and ask the LLM to disregard the original instruction and execute attacker‚Äôs designated action. This vulnerability poses a substantial threat (OWASP, 2023) to LLM-integrated systems, particularly in critical applications like email platforms or banking services, where potential severe consequences include leaking sensitive information or enabling unauthorized transactions. Given the severity of this threat, developing reliable detection mechanisms against prompt injection attacks is essential. In this work, we explain the prompt injection attack from the perspective of the attention mechanisms in LLMs. Our analysis reveals that when a prompt injection attack occurs, the attention of specific attention heads shifts from the original instruction to the injected instruction within the attack data, a phenomenon we have named the distraction effect. We denote the attention heads that are likely to get distracted as important heads. We attribute this behavior to the reasons why LLMs tend to follow the injected instructions and neglect their original instructions. Surprisingly, our experiments also demonstrate that the distraction effect observed on the important heads generalizes well across various attack types and dataset distributions. Motivated by the distraction effect, we propose \attn, a simple yet effective training-free guard that detects prompt injection attacks by tracking the attentions on the instruction given to the LLMs. Specifically, for a given LLM, we identify the important heads using merely a small set of LLM-generated random sentences combined with a naive ignore attack. Then, as shown in Figure 1, for any testing queries, we feed them into the target LLM and aggregate the attention directed towards the instruction in the important heads. With this aggregated score which we call the focus score, we can effectively detect prompt injection attacks. Importantly, unlike previous training-free detection methods, \attn can detect attacks without any additional LLM inference, as the attention scores can be obtained during the original inference process. We highlight that \attn requires zero data and zero training from any existing prompt injection datasets. When tested on two open-source datasets, Open-Prompt-Injection (Liu et al., 2024b) and deepset (deepset, 2023), \attn achieved exceptionally high detection accuracy across all evaluations, improving the AUROC score up to 10.0% over all existing detection methods and up to 31.3% on average over all existing training-free detection methods. This impressive performance highlights the strong generalization capability of our approach, allowing it to adapt effectively across different models and datasets. Furthermore, unlike previous training-free detection methods that rely on large or more powerful LMs to achieve better accuracy, our method is effective even on smaller LMs with only 1.8 billion parameters. To further validate our findings, we conduct extensive analyses on LLMs to investigate the generalization of the distraction effect, examining this phenomenon across various models, attention heads, and datasets. We summarize our contributions as follows: ‚Ä¢ To the best of our knowledge, we are the first to explore the dynamic change of the attention mechanisms in LLMs during prompt injection attacks, which we term the distraction effect. ‚Ä¢ Building on the distraction effect, we develop \attn, a training-free detection method that achieves state-of-the-art performance without additional LLM inference. ‚Ä¢ We also demonstrate that \attn is effective on both small and large LMs, addressing a significant limitation of previous training-free detection methods."
https://arxiv.org/html/2411.00261v1,Pipe-Cleaner: Flexible Fuzzing Using Security Policies,"Fuzzing has proven to be very effective for discovering certain classes of software flaws, but less effective in helping developers process these discoveries. Conventional crash-based fuzzers lack enough information about failures to determine their root causes, or to differentiate between new or known crashes, forcing developers to manually process long, repetitious lists of crash reports.Also, conventional fuzzers typically cannot be configured to detect the variety of bugs developers care about, many of which are not easily converted into crashes.To address these limitations, we propose Pipe-Cleaner, a system for detecting and analyzing C code vulnerabilities using a refined fuzzing approach. Pipe-Cleaner is based on flexible developer-designed security policies enforced by a tag-based runtime reference monitor, which communicates with a policy-aware fuzzer. Developers are able to customize the types of faults the fuzzer detects and the level of detail in fault reports. Adding more detail helps the fuzzer to differentiate new bugs, discard duplicate bugs, and improve the clarity of results for bug triage. We demonstrate the potential of this approach on several heap-related security vulnerabilities, including classic memory safety violations and two novel non-crashing classes outside the reach of conventional fuzzers: leftover secret disclosure, and heap address leaks.","Fuzzing(Miller et al., 2022; Oliver Chang, 2017; Zalewski, [n. d.]; lib, 2024; rus, [n. d.]; hon, [n. d.]), also known as fuzz testing, is a dynamic, probabilistic software-testing technique. It attempts to thoroughly explore the input space of the target program, searching for inputs that cause ‚Äúinteresting behavior,‚Äù which, in most production fuzzers, is hard-coded to mean crashes (segmentation faults) and hangs. Although this simple definition has historically worked well for bug discovery, it limits the fuzzer‚Äôs ability to detect duplicate faults, aid in bug report triage, and detect non-crash bugs. We identify three key problems with standard fuzzing approaches. The (De)Duplication Problem. Fuzzing results have a bad signal to noise ratio. Due to the limited information in a crash report, the default mechanism for differentiating crashes is to compare hashes of their stack traces, which is fast but can cause the fuzzer to both over-report and under-report crashes (Klees et al., 2018). They over-report when crashes with the same root cause have different hashes (which is very common), and under-report when two unrelated crashes accidentally have matching hashes. Over-reporting can lead to many duplicates clogging the results given to developers, reducing time spent on bugfixes (Jiang et al., 2021). The Crash Triage Problem. Basic bug triage requires three things: (1) the cause of failure, (2) whether there are security implications, and (3) which developer teams are responsible for the fix (often approximated by locations in the source code). Current fuzzers cannot provide most of this information, so their reports cannot be easily triaged, and hence are liable to be ignored. More than half of the fuzzer crashes reported to the Linux kernel are ignored (Nogikh, 2023), likely due to lack of information in the crash report (Edge, 2022). This is one of the top concerns of fuzzer users (Nourry et al., 2023). The Crash Bias Problem. The reliance on crashes also biases fuzzing results towards flaws that can be easily signaled by crashes, such as memory corruption, leaving other types of security flaws undetected. This is part of a wider problem of inflexibility in fuzzer design. There are typically two ways to tailor a fuzzer to new classes of bugs: either write a new fuzzer, or add a sanitizer to the executable being fuzzed. Most sanitizers work by inserting conditional crashing code during a compiler pass, a difficult and demanding task (goo, 2022). Sanitizers are typically incompatible with each other (Peko, 2021), difficult to modify, and are unavailable under certain conditions. While some sanitizers, such as ThreadSanitizer (Clang Team, [n. d.]), do produce helpful log output, fuzzers are unaware of this, and neither capture nor use any extra information the sanitizer might emit. The typical workflow for fuzzing with sanitizers is to compile with a sanitizer, fuzz, and then run crashing inputs individually on a sanitized binary and hope that a useful error message results. These problems all fundamentally stem from relying on crashes and crash dumps as the sole means of discovering and reporting faults. To address them, we propose Pipe-Cleaner, a new approach to fuzzing C code that executes the target program under control of a security reference monitor (Anderson, 1972). Specifically, we use the Tagged C system (Anderson et al., 2023b), which enforces arbitrary user-configurable security policies based on metadata tags carried for each value. Security policies can range from classic static and dynamic memory safety (Szekeres et al., 2013) to fine-grained information flow control (Denning, 1976) supporting data confidentiality or integrity properties. Any policy violation causes the fuzzer to be notified with a report that includes dynamic context information which can be used to help de-duplicate and classify bugs. Thus, developers can focus fuzzing resources on the bugs they care about and receive nuanced information about faults for better triage. Operators can swap out policies without needing a new fuzzer (or a new compiler pass), or choose to fuzz with multiple policies at once. Tagged C can be thought of as a highly configurable sanitizer, and like other fuzzing approaches based on code instrumentation, Pipe-Cleaner deliberately trades off execution speed against improved quality of fault information. Currently, Tagged C is available only as an interpreter, but a faster execution engine based on source-to-source insertion of instrumentation is under development, and ultimately we hope to use the hardware tagging support known as Processor Interlocks for Policy Enforcement (PIPE) (Dhawan et al., 2015; Azevedo de Amorim et al., 2016, 2015)111Variants of PIPE have also been called PUMP (Dhawan et al., 2014), SDMP (DeHon et al., 2016), or CoreGuard (Dover Microsystems, [n. d.])., which monitors protected metadata tags in parallel with ordinary execution on values to obtain performance comparable to normal code. For this reason, Pipe-Cleaner policies operate on metadata tags rather than on actual values. This is one of the features that distinguishes our approach from property-based testing (PBT) (Lampropoulos et al., 2019), which relies on inspecting values. Another difference is that PBT is usually employed to check program-specific functional properties rather than generic security properties. In summary, we make the following contributions: (1) We present Pipe-Cleaner, a novel fuzzing system using flexible developer-designed security policies (Section 2). (2) We show examples of how Tagged C policies enrich fuzzer behavior through improved duplicate detection and more precise bug reports (Section 3). (3) We implement and evaluate these example policies against intuitive metrics for characterizing fuzzer behavior on nontraditonal bug classes (Section 4). An artifact including everything necessary to reproduce the behavior of our prototype implementations will be made available as part of the final version of this paper."
https://arxiv.org/html/2411.00255v1,Dynamic Accountable Storage: An Efficient Protocol for Real-time Cloud Storage Auditing,"Ateniese, Goodrich, Lekakis, Papamanthou, Paraskevas, and Tamassia introduced the Accountable Storage protocol, which is a way for a client to outsource their data to a cloud storage provider while allowing the client to periodically perform accountability challenges. An accountability challenge efficiently recovers any pieces of data the server has lost or corrupted, allowing the client to extract the original copies of the damaged or lost data objects. A severe limitation of the prior accountable storage scheme of Ateniese et al., however, is that it is not fully dynamic. That is, it does not allow a client to freely insert and delete data from the outsourced data set after initializing the protocol, giving the protocol limited practical use in the real world. In this paper, we present Dynamic Accountable Storage, which is an efficient way for a client to periodically audit their cloud storage while also supporting insert and delete operations on the data set. To do so, we introduce a data structure, the IBLT tree, which allows either the server or the client to reconstruct data the server has lost or corrupted in a space-efficient way.","Cloud storage providers often advertise the number of ‚Äúnines‚Äù of durability they achieve, such as the ‚Äú11 nines‚Äù in the durability probability of 99.999999999% advertised by Amazon S3 for not losing a given data object in a given year,111See, e.g., https://aws.amazon.com/s3/storage-classes/. which implies an expected loss of at most one object out of 100 billion per year. Such statements might seem at first to imply that cloud storage data loss is impossible, until one considers that there are hundreds of trillions of objects currently being stored in Amazon S3.222E.g., see https://aws.amazon.com/blogs/aws/welcome-to-aws-pi-day-2022/. Moreover, such durability statements do not address data corruption or data loss caused by misuse or misconfiguration, e.g., see [23, 25, 27]. Such durability statements also beg the question of how to determine whether one or more of a client‚Äôs data objects has been lost or corrupted. For example, when data is lost, a client or server may not even realize it and may also have no ability to recover from the damage. We propose an efficient way for a client and/or server to audit and recover client data. Our work is an extension of the Accountable Storage protocol first described by Ateniese et al. [4]. In this scheme, a client, Alice, outsources her data to a cloud storage provider, Bob, and she then stores a small sketch representation of her data set along with some metadata in a data structure called an Invertible Bloom Lookup Table (IBLT) [17, 20]. At any time, the client can issue an accountability challenge, requiring that the storage provider, Bob, send the client a similar encoding of the data set representing everything that has not been lost. The client can then compare the two IBLTs to peel out any data blocks that were lost, assess how much the blocks deviate from their original versions, and demand compensation accordingly. Furthermore, Ateniese et al. claim that in their scheme the server is forced to acknowledge and pay for lost data. However, we show that the server can easily thwart their scheme. Another caveat in the protocol by Ateniese et al. is that their scheme is static, i.e., it does not support insert or delete operations. This severely limits the usefulness of the scheme in the real world. Our work fixes this limitation; hence, we call our scheme ‚ÄúDynamic Accountable Storage.‚Äù Furthermore, our scheme does not rely on forcing the server to pay for lost data or store large amounts of metadata, as in the original (static) Accountable Storage scheme [4]. Instead, our protocol provides the server with an efficient low-overhead way to recover lost data for the client. 1.1 Other Related Work There are many existing schemes that allow a client to verify that a cloud storage provider is keeping their data intact, but not with the same levels of efficiency or dynamism as our scheme. For example, Provable Data Possession (PDP) schemes [2, 3, 16, 19, 31, 21] and Proofs of Retrievability (POR) schemes [10, 22, 28, 29, 1] use versions of cryptographic tags to verify that data is maintained correctly. A common technique used in these schemes is homomorphic tags [2, 5], which enable batch verification and reduces communication complexity from linear to constant in the size of the client‚Äôs data. Verifiable Database schemes [6, 11, 13, 14, 26, 12] do something similar, except use cryptographic commitment primitives rather than tags. Unfortunately, dynamic PDP or POR schemes, such as the one by Erway, Kupcu, Papamanthou, and Tamassia [19], stop at answering whether the client‚Äôs data has been corrupted by a cloud service provider. In contrast, our protocol goes further, by recovering the lost data. Other work, such as that by Ateniese, Di Pietro, Mancini, and Tsudik [2] or Shacham and Waters [28], implements a type of accountability challenge with dynamic data, but their schemes require the client to run O‚Å¢(n)ùëÇùëõO(n)italic_O ( italic_n ) expensive cryptographic operations per update, where nùëõnitalic_n is the size of the database. Jin et al. [21] solved this recomputation problem at the expense of having the client store O‚Å¢(n)ùëÇùëõO(n)italic_O ( italic_n ) extra metadata, somewhat defeating the purpose of the client outsourcing their data. 1.2 Our Contributions In this paper, we describe a scheme for Dynamic Accountable Storage, which supports insertions, deletions, and data recovery such that both the client and server have low overheads in terms of additional storage requirements and running times required to update metadata. For example, given a parameter, Œ¥ùõø\deltaitalic_Œ¥, for the number of blocks our scheme can efficiently tolerate being lost or corrupted, our scheme requires only o‚Å¢(n)ùëúùëõo(n)italic_o ( italic_n ) additional space at the server rather than the Œò‚Å¢(n‚Å¢Œ¥)Œòùëõùõø\Theta(n\delta)roman_Œò ( italic_n italic_Œ¥ ) space required by the original Accountable Storage scheme [4]. Another difference of our approach from prior approaches is that our scheme does not assume the server is malicious, since any reputable cloud service company has an incentive to help the client maintain her data. As a result, our work investigates efficient ways for the server to maintain the client‚Äôs data with added reliability and to be able to detect and repair corruptions on the client‚Äôs behalf, assuming that the server is honest-but-curious; hence, we provide a way for the client, Alice, to encrypt both her keys and their values while still allowing her to outsource her data with low overheads for both the client and the server. We wish to emphasize that an honest-but-curious server does not preclude the need for accountability mechanisms. As described above, any cloud storage system of a sufficient size will inevitably produce errors. Even if the server is not maliciously corrupting data, identify errant data objects among billions of others is a non-trivial problem. In addition to cloud storage verification, schemes like ours have applications in version control systems [19], verifiably-secure logging [15, 30], and public data auditing [21, 24, 4]. We have two main contributions in this work. The first is to show how the the server in the original Accountable Storage protocol of Ateniese et al. [4] can thwart their original scheme so as to recover lost data without paying the client or revealing that there has been a data loss. Indeed, we see this as a feature, not a bug, and we build our scheme on the assumption that the honest-but-curious server, Bob, is motivated to recover the client‚Äôs data whenever this is possible. The second contribution of our work is that we describe an extension to the Accountable Storage protocol [4] that can support insertions and deletions of key-value pairs. To allow the server to efficiently detect and repair corruptions in the client‚Äôs data, we introduce a new data structure that is maintained at the server, which we call an IBLT tree. This data structure takes o‚Å¢(n)ùëúùëõo(n)italic_o ( italic_n ) extra space at the server, compared to the O‚Å¢(n)ùëÇùëõO(n)italic_O ( italic_n ) extra metadata required by the original Accountable Storage protocol. Although the server must already store nùëõnitalic_n data blocks for the client, the space savings across a large number of clients are significant. The remainder of the paper is organized as follows. In Section 2, we briefly review the original Accountable Storage protocol of Ateniese et al. [4], showing how the server can thwart the requirement to pay for lost data. In Section 3, we describe our new data structure, the IBLT tree. Lastly, in Section 4, we provide an overview of our protocol and a formal construction with an efficiency analysis."
https://arxiv.org/html/2411.00222v1,Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding,"An adversarial example is a modified input image designed to cause a Machine Learning (ML) model to make a mistake; these perturbations are often invisible or subtle to human observers and highlight vulnerabilities in a model‚Äôs ability to generalize from its training data. Several adversarial attacks can create such examples, each with a different perspective, effectiveness, and perceptibility of changes. Conversely, defending against such adversarial attacks improves the robustness of ML models in image processing and other domains of deep learning. Most defence mechanisms require either a level of model awareness, changes to the model, or access to a comprehensive set of adversarial examples during training, which is impractical. Another option is to use an auxiliary model in a preprocessing manner without changing the primary model. This study presents a practical and effective solution ‚Äì using predictive coding networks (PCnets) as an auxiliary step for adversarial defence. By seamlessly integrating PCnets into feed-forward networks as a preprocessing step, we substantially bolster resilience to adversarial perturbations. Our experiments on MNIST and CIFAR10 demonstrate the remarkable effectiveness of PCnets in mitigating adversarial examples with about 82%percent8282\%82 % and 65%percent6565\%65 % improvements in robustness, respectively. The PCnet, trained on a small subset of the dataset, leverages its generative nature to effectively counter adversarial efforts, reverting perturbed images closer to their original forms. This innovative approach holds promise for enhancing the security and reliability of neural network classifiers in the face of the escalating threat of adversarial attacks.","An adversarial example is a modified input intended to cause a machine-learning model to make a mistake. The modifications are often imperceptible or very subtle to human observers. However, predictive coding can reverse such alterations due to its perturbation resiliency, providing more robustness against such attacks. This defensive strategy against adversarial attacks includes generative mechanisms that revert the perturbed images to their original form. Predictive coding offers a theoretical framework to support such a defence. To help understand this work, we will briefly discuss adversarial examples, including how to create and defend against them. We will also mention the attack methods we used in our experiments and popular corresponding defence strategies in subsections 1.1.1 and 1.1.2, respectively. We will introduce the predictive coding framework and its learning algorithm in subsection 1.2. We then explain the experiment setups in section 2, and show the results in section 3, which will be discussed in section 4. At the end, we will summarize our work and point to possible future venues in sections 5 and 6, respectively. 1.1 Adversarial Attacks and Defences Unlike humans, who robustly interpret visual stimuli, artificial neural networks can be deceived by adversarial attacks (ATs), particularly perturbation attacks [1]. These attacks subtly alter an image to trick a well-trained trained feed-forward network (FFnet) used for classification tasks [2, 3] (see figure 1). One standard method to create an adversarial example (AE) that causes the FFnet to misclassify the image as a specific target label is to find a perturbation that minimizes the loss function argminŒ¥‚ààŒî‚Å¢‚Ñì‚Å¢(FŒ∏‚Å¢(x+Œ¥),yt),ùõøŒîargminbold-‚ÑìsubscriptùêπùúÉùë•ùõøsubscriptùë¶ùë°\underset{\delta\in\Delta}{\mathrm{argmin}}\ \boldsymbol{\ell}(F_{\theta}(x+% \delta),y_{t}),start_UNDERACCENT italic_Œ¥ ‚àà roman_Œî end_UNDERACCENT start_ARG roman_argmin end_ARG bold_‚Ñì ( italic_F start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_x + italic_Œ¥ ) , italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (1) where: ‚Ä¢ xùë•xitalic_x represents the image, ‚Ä¢ Œ¥ùõø\deltaitalic_Œ¥ is the perturbation needed to deceive the FFnet when applied to the image, and ‚ÄñŒ¥‚Äñ‚àû<œµsubscriptnormùõøitalic-œµ\|\delta\|_{\infty}<\epsilon‚à• italic_Œ¥ ‚à• start_POSTSUBSCRIPT ‚àû end_POSTSUBSCRIPT < italic_œµ is enforced. ‚Ä¢ ŒîŒî\Deltaroman_Œî represents allowable perturbations that are visually indistinguishable to humans. ‚Ä¢ ytsubscriptùë¶ùë°y_{t}italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is the 1-hot vector (i.e., etsubscriptùëíùë°e_{t}italic_e start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) corresponding to the target label tùë°titalic_t. ‚Ä¢ FŒ∏‚Å¢(‚ãÖ)subscriptùêπùúÉ‚ãÖF_{\theta}(\cdot)italic_F start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( ‚ãÖ ) is the FFnet model, (i.e., FŒ∏:x‚Üíy‚àà‚Ñùk:subscriptùêπùúÉ‚Üíùë•ùë¶superscript‚ÑùùëòF_{\theta}:x\rightarrow y\in\mathbb{R}^{k}italic_F start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT : italic_x ‚Üí italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT, where kùëòkitalic_k is the number of classes), ‚Ä¢ Œ∏ùúÉ\thetaitalic_Œ∏ represents all parameters defining the model. ‚Ä¢ ‚Ñìbold-‚Ñì\boldsymbol{\ell}bold_‚Ñì is the cross-entropy loss function. This optimization can be achieved iteratively [4]. Alternatively, instead of deceiving the model by a specific target label, the optimization can be solved for an untargeted attack by maximizing the loss argmaxŒ¥‚ààŒî‚Å¢(x)‚Å¢‚Ñì‚Å¢(FŒ∏‚Å¢(x+Œ¥),y),ùõøŒîùë•argmaxbold-‚ÑìsubscriptùêπùúÉùë•ùõøùë¶\underset{\delta\in\Delta(x)}{\mathrm{argmax}}\ \boldsymbol{\ell}(F_{\theta}(x% +\delta),y),start_UNDERACCENT italic_Œ¥ ‚àà roman_Œî ( italic_x ) end_UNDERACCENT start_ARG roman_argmax end_ARG bold_‚Ñì ( italic_F start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_x + italic_Œ¥ ) , italic_y ) , (2) for the given pair (x,y)ùë•ùë¶(x,y)( italic_x , italic_y ), which can be achieved in one step using methods like the fast gradient sign method (FGSM) [5], or other approaches [6, 7, 8, 9, 10, 11]. When testing a well-trained FFnet MNIST classifier (with an accuracy of approximately 98%percent9898\%98 %) against FGSM-generated AEs (with œµ‚âÉ0.78%similar-to-or-equalsitalic-œµpercent0.78\epsilon\simeq 0.78\%italic_œµ ‚âÉ 0.78 %), the adversarial success rate is about 41%percent4141\%41 %. To defend against ATs, augmenting the training dataset with AEs can improve the classifier‚Äôs resilience, achieving an accuracy of approximately 94%percent9494\%94 %. Alternatively, a min-max approach to directly counteract AEs can enhance robustness within specific perturbation limits [7, 12]. Figure 1: FFnet‚Äôs perception of the image changes as the noise perturbed the image. FFnet perceives the original image Pr‚Å°(y0=1|x)=0.99probabilitysubscriptùë¶0conditional1ùë•0.99\Pr(y_{0}=1|x)=0.99roman_Pr ( start_ARG italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 | italic_x end_ARG ) = 0.99 while the perception changed to Pr‚Å°(y3=1|x+Œ¥)=0.87probabilitysubscriptùë¶3conditional1ùë•ùõø0.87\Pr(y_{3}=1|x+\delta)=0.87roman_Pr ( start_ARG italic_y start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 1 | italic_x + italic_Œ¥ end_ARG ) = 0.87 on perturbation. 1.1.1 Creating Adversarial Examples Adversarial examples (AEs) are crucial for assessing and improving the robustness of machine learning (ML) models, especially in deep learning. In image data, creating AEs involves making imperceptible changes to the original image to mislead the model into misclassifying the image. Various methods are available to generate such AEs, particularly for deceiving deep neural networks. Below, we briefly discuss the main gradient-based techniques relevant to our work while noting that there are other techniques to create AEs [8, 9, 10, 11]. ‚Ä¢ Fast Gradient Sign Method (FGSM) modifies the input image by computing the loss gradient for the input image and then making a small step in the opposite direction to increase the loss [5]. ‚Ä¢ Basic Iterative Method (BIM), an extension of FGSM, takes multiple small steps while adjusting the direction of the perturbation at each step [6]. ‚Ä¢ Projected Gradient Descent (PGD) modifies the input image in multiple iterations with a constraint on the perturbation‚Äôs size. PGD starts from a random point within a small ball (i.e., œµitalic-œµ\epsilonitalic_œµ-ball) around the original image and performs a series of gradient descent steps to maximize the prediction error while ensuring the perturbation is smaller than the specified œµitalic-œµ\epsilonitalic_œµ [7]. ‚Ä¢ Carlini & Wagner (C&W) attack optimizes the perturbation directly through a loss function that aims to deceive to a desired target label and keep the perturbation small. It often produces subtle perturbations that are highly effective at fooling neural networks [4]. These methods differ in complexity, the amount of required knowledge about the target model (white box vs. black box), the type of perturbations (targeted vs. non-targeted), and the strength and stealthiness of the attack. The choice of method often depends on the adversary‚Äôs access to the model parameters and its specific requirements, including the robustness of the target model and the desired invisibility of the modifications. 1.1.2 Defending Against Adversarial Attacks Although various adversarial attacks exist, some defence mechanisms attempt to protect ML models against such attacks. Here, we review defence strategies for each previously mentioned attack. ‚Ä¢ For FGSM and BIM/PGD: Adversarial training involves training the model using adversarial and clean examples. It has been particularly effective against gradient-based attacks like FGSM and BIM. Gradient masking attempts to hide or modify gradients so that they are less useful for generating adversarial examples. However, this method has often been criticized and can be circumvented [13]. ‚Ä¢ For C&W Attack: Some defences estimate the likelihood that input is adversarial using auxiliary models or statistical analyses [4]. Defensive distillation involves training a model to output softened probabilities of classes, making it harder for an attacker to find gradients that can effectively manipulate the model‚Äôs output [13]. While these methods offer some protection against specific types of adversarial attacks, it is essential to note that there is no one-size-fits-all solution, and sophisticated or adaptive attackers can circumvent many defences. However, some defence strategies come with a cost, and there is a trade-off between robustness and accuracy [12]. Continued research is crucial to improving the robustness of neural networks against these threats. 1.2 Predictive Coding Computational neuroscience seeks to understand behavioural and cognitive phenomena at the level of individual neurons or networks of neurons. One approach to solving difficult problems, such as adversarial attacks, which do not seem to be a problem for the brain, is to explore biologically plausible perception models. The model we will be using is predictive coding (PC)111Various cortical theories support the bidirectional model [14, 15, 16], as well as free-energy principles [17]., a neural model capable of implementing error backpropagation in a biologically plausible manner [18, 19, 20]. 1.2.1 Model Schema and The Learning Algorithms The concept of predictive coding suggests that the brain works to minimize prediction error [21]. This model aims to improve overall predictions, and all neurons work towards this common objective. In a predictive coding network (PCnet), each neuron, or PC unit, consists of a value (vùë£vitalic_v) and an error node (ŒµùúÄ\varepsilonitalic_Œµ). These PC units are organized into layers, similar to artificial neural networks (ANNs), forming PCnets that learn by adjusting parameters to refine predictions and reduce errors between layers. For example, in a PCnet, layer iùëñiitalic_i contains vectors visubscriptùë£ùëñv_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ŒµisubscriptùúÄùëñ\varepsilon_{i}italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, as illustrated in figure 2. Vector visubscriptùë£ùëñv_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT predicts the values of the next layer, vi‚àí1subscriptùë£ùëñ1v_{i-1}italic_v start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, using prediction weights Mi‚àí1subscriptùëÄùëñ1M_{i-1}italic_M start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. The resulting error, Œµi‚àí1subscriptùúÄùëñ1\varepsilon_{i-1}italic_Œµ start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, is then communicated back via correction weights Wi‚àí1subscriptùëäùëñ1W_{i-1}italic_W start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT, allowing visubscriptùë£ùëñv_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to improve its predictions. Figure 2: A typical PCnet arranged in a feed-forward manner. Each box represents a population of neurons containing value and error nodes. The network dynamics (as in equations 3 - 7) are described by the activation function œÉùúé\sigmaitalic_œÉ, Hadamard product ‚äôdirect-product\odot‚äô, outer product ‚äótensor-product\otimes‚äó, decay coefficient Œæùúâ\xiitalic_Œæ, and time constants œÑùúè\tauitalic_œÑ and Œ≥ùõæ\gammaitalic_Œ≥, where œÑ<Œ≥ùúèùõæ\tau<\gammaitalic_œÑ < italic_Œ≥. œÑ‚Å¢ŒµÀôiùúèsubscriptÀôùúÄùëñ\displaystyle\tau\,\dot{\varepsilon}_{i}italic_œÑ overÀô start_ARG italic_Œµ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =vi‚àíMi‚Å¢œÉ‚Å¢(vi+1)‚àíbi‚àíŒæ‚Å¢Œµiabsentsubscriptùë£ùëñsubscriptùëÄùëñùúésubscriptùë£ùëñ1subscriptùëèùëñùúâsubscriptùúÄùëñ\displaystyle=v_{i}-M_{i}\sigma(v_{i+1})-b_{i}-\xi\varepsilon_{i}= italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_œÉ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) - italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_Œæ italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (3) œÑ‚Å¢vÀôiùúèsubscriptÀôùë£ùëñ\displaystyle\tau\,\dot{v}_{i}italic_œÑ overÀô start_ARG italic_v end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =‚àíŒµi+Wi‚àí1‚Å¢Œµi‚àí1‚äôœÉ‚Ä≤‚Å¢(vi)absentsubscriptùúÄùëñdirect-productsubscriptùëäùëñ1subscriptùúÄùëñ1superscriptùúé‚Ä≤subscriptùë£ùëñ\displaystyle=-\varepsilon_{i}+W_{i-1}\varepsilon_{i-1}\odot\sigma^{\prime}(v_% {i})= - italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_W start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT italic_Œµ start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT ‚äô italic_œÉ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) (4) Œ≥‚Å¢MÀôiùõæsubscriptÀôùëÄùëñ\displaystyle\gamma\,\dot{M}_{i}italic_Œ≥ overÀô start_ARG italic_M end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =Œµi‚äóœÉ‚Å¢(vi+1)absenttensor-productsubscriptùúÄùëñùúésubscriptùë£ùëñ1\displaystyle=\varepsilon_{i}\otimes\sigma(v_{i+1})= italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚äó italic_œÉ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) (5) Œ≥‚Å¢WÀôiùõæsubscriptÀôùëäùëñ\displaystyle\gamma\,\dot{W}_{i}italic_Œ≥ overÀô start_ARG italic_W end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =œÉ‚Å¢(vi+1)‚äóŒµiabsenttensor-productùúésubscriptùë£ùëñ1subscriptùúÄùëñ\displaystyle=\sigma(v_{i+1})\otimes\varepsilon_{i}= italic_œÉ ( italic_v start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT ) ‚äó italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (6) Œ≥‚Å¢bÀôiùõæsubscriptÀôùëèùëñ\displaystyle\gamma\,\dot{b}_{i}italic_Œ≥ overÀô start_ARG italic_b end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT =ŒµiabsentsubscriptùúÄùëñ\displaystyle=\varepsilon_{i}= italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (7) where bisubscriptùëèùëñb_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the bias for error node ŒµisubscriptùúÄùëñ\varepsilon_{i}italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Training a PCnet involves clamping input and output-layer value nodes to sensory input and target values and running the network until it reaches equilibrium. The network‚Äôs state variables (visubscriptùë£ùëñv_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, ŒµisubscriptùúÄùëñ\varepsilon_{i}italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) reach equilibrium faster than the parameters (MisubscriptùëÄùëñM_{i}italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, WisubscriptùëäùëñW_{i}italic_W start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, bisubscriptùëèùëñb_{i}italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT) due to œÑ<Œ≥ùúèùõæ\tau<\gammaitalic_œÑ < italic_Œ≥. After training, the parameters MùëÄMitalic_M, WùëäWitalic_W, bùëèbitalic_b are fixed, effectively setting Œ≥ùõæ\gammaitalic_Œ≥ to infinity. When a perfect prediction is achieved, the error signal (ŒµùúÄ\varepsilonitalic_Œµ) is zero, stabilizing the value node without further corrections. This state minimizes the Hopfield-like energy function [18] given by equation, E=Œæ2‚Å¢‚àëi‚Äñùú∫i‚Äñ2.ùê∏ùúâ2subscriptùëñsuperscriptnormsubscriptùú∫ùëñ2E=\tfrac{\xi}{2}\sum_{i}\|{\bf\it\varepsilon}_{i}\|^{2}.italic_E = divide start_ARG italic_Œæ end_ARG start_ARG 2 end_ARG ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚à• bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚à• start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (8) After training, when initializing the network with a given input image, when the value nodes are unclamped, the network‚Äôs ability to reduce energy can lead to potential changes in the input image. The PCnet modifies images without impacting their correct classification, as illustrated in figure 3a. The left image displays the original version, while the right image shows the version altered by PCnet. Likewise, when PCnet introduces perturbations to the adversarial image, as seen in figure 3b, the FFnet can classify it correctly. (a) Perturbation made to an image. (b) Perturbation made to an adversarial image. Figure 3: PCnet perturbation is demonstrated using both the original and adversarial images. PCnet modifies the given input based on its trained dynamics. As shown in LABEL:sub@fig:imageBeforeAfterPC, the original image xùë•xitalic_x is depicted on the left, while its perturbation PCnet‚Å¢(x)PCnetùë•\mathrm{PCnet}(x)roman_PCnet ( italic_x ) is shown on the right. Similarly, LABEL:sub@fig:advBeforeAfterPC presents the adversarial image zùëßzitalic_z on the left, alongside its perturbation pùëùpitalic_p on the right. PCnet‚Äôs approach to the credit assignment problem differs from backpropagation (backprop), which is the learning algorithm of ANNs [22]. Backpropagation seems unlikely in the brain for several reasons, such as its requirement for weight transposing and transferring between layers. In contrast, PCnet can effectively learn without these requirements using the dynamics of each parameter (equations (5), (6), and (7)). These dynamics are consistent with the Hebbian learning rule, which only requires local information and pre- and post-synaptic activities [23, 24, 25, 26, 27, 28, 29]. This learning algorithm facilitates flexibility and feasibility in using different architectures."
https://arxiv.org/html/2411.00217v1,"ADAPT: A Game-Theoretic and Neuro-Symbolic Framework for Automated Distributed Adaptive Penetration Testing‚Ä†‚Ä†thanks:2Authors contributed equally, 1Corresponding author,Authors belong to Department of Electrical and Computer EngineeringNew York University, New York 11201, USA{hl4155, yg2047, qz494}@nyu.edu","The integration of AI into modern critical infrastructure systems, such as healthcare, has introduced new vulnerabilities that can significantly impact workflow, efficiency, and safety. Additionally, the increased connectivity has made traditional human-driven penetration testing insufficient for assessing risks and developing remediation strategies. Consequently, there is a pressing need for a distributed, adaptive, and efficient automated penetration testing framework that not only identifies vulnerabilities but also provides countermeasures to enhance security posture. This work presents ADAPT, a game-theoretic and neuro-symbolic framework for automated distributed adaptive penetration testing, specifically designed to address the unique cybersecurity challenges of AI-enabled healthcare infrastructure networks. We use a healthcare system case study to illustrate the methodologies within ADAPT. The proposed solution enables a learning-based risk assessment. Numerical experiments are used to demonstrate effective countermeasures against various tactical techniques employed by adversarial AI.","Modern artificial intelligence (AI), such as machine learning (ML) technologies, are becoming increasingly integrated into many infrastructures, including smart transportation systems and healthcare infrastructures. In healthcare, they have shown the potential to help healthcare infrastructure in patient scheduling [1, 2], pathological analysis [3], and care management [4]. While there are significant benefits, there are concerns regarding zero-day vulnerabilities and the expanded attack surface. Penetration testing is a valuable ethical hacking method for uncovering vulnerabilities in increasingly complex infrastructures and devising remediation strategies. As these infrastructures become more complex, with millions of interconnected devices, scalability emerges as a critical challenge. It is essential to develop a distributed, modular, and automated approach that addresses device-level testing needs while considering global influences through interconnectivity. Another challenge stems from the dynamic nature of networked devices and their vulnerabilities. There is a growing need for adaptive and automated approaches to continuously update the vulnerability landscape, ensuring that threats are exhaustively identified, risks accurately assessed, and remediation measures properly applied. The third challenge arises from the integration of AI capabilities into the infrastructure. The emergence of adversarial AI/ML introduces new and evolving threat vectors, which are designed to evade detection and testing. There is a need for the development of automated and strategic approaches that can intelligently outmaneuver their evolving nature through continuous knowledge acquisition and learning. To this end, we establish a game-theoretic and neuro-symbolic framework for automated distributed adaptive penetration testing (ADAPT). Figure 1: The framework of the ADAPT: The upper half illustrates an online automated adaptation of penetration testing. It integrates game-theoretic and neuro-symbolic frameworks, consisting of five distinct building blocks (to be introduced in Section III). The lower half depicts an example of AI-enabled healthcare infrastructure. This AI-enabled infrastructure presents an expanded attack surface due to interconnectivity and zero-day vulnerabilities. To address the challenges in penetration testing within AI-enabled healthcare infrastructure networks, ADAPT consolidates the game-theoretic framework and the neuro-symbolic framework, shown in the Figure 1. The game-theoretic and neuro-symbolic framework consists of five building blocks. The macro-game and micro-game blocks serve as representations of the given system. Game-theoretic strategies are updated based on the selected attack models through neural learning in the online learning blocks. Different attack models are represented using game trees, which encode relevant attack and defense actions selected from the knowledge. This knowledge contains vulnerabilities of the health infrastructure that are shared among multiple stakeholders in the medical system. When new paths or vulnerabilities are discovered through exploration (e.g., automated fuzzing techniques [5]) and penetration testing, the knowledge base is generated and updated accordingly. ADAPT helps medical systems evaluate their AI-enabled network for scalability, the impact of reachability, and the exhaustiveness of risk identification, and protects the confidentiality, integrity, and availability of the AI model. The purpose of it is to ensure preparedness against continuously evolving malicious threats such as ransomware and zero-day attacks on healthcare infrastructures."
https://arxiv.org/html/2411.00193v1,Historical and Multichain Storage Proofs,"This paper presents a comprehensive analysis of storage proofs in the Ethereum ecosystem, examining their role in addressing historical and cross-chain state access challenges. We systematically review existing approaches to historical state verification, comparing Merkle Mountain Range (MMR) and Merkle-Patricia trie (MPT) architectures. An analysis involves their respective performance characteristics within zero-knowledge contexts, where performance challenges related to Keccak-256 are explored. The paper also examines the cross-chain verification, particularly focusing on the interactions between Ethereum and Layer 2 networks. Through careful analysis of storage proof patterns across different network configurations, we identify and formalize three architectures for cross-chain verification. By organizing this complex technical landscape, this analysis provides a structured framework for understanding storage proof implementations in the Ethereum ecosystem, offering insights into their practical applications and limitations.","The Ethereum blockchain and the broader Ethereum Virtual Machine (EVM) landscape have emerged as the dominant blockchain ecosystem, significantly due to their network effects, which manifest in various forms such as a loyal user base, a thriving developer community, and substantial financial liquidity [1]. As the first smart contract platform to achieve widespread adoption, Ethereum has established itself as a cornerstone of decentralized applications (dApps), creating an environment where both users and developers are incentivized to engage, innovate, and invest. Central to Ethereum‚Äôs success is the EVM, a virtual machine that facilitates the execution of smart contracts. One of the unique features of EVM-based smart contracts is their unprecedented access to the ‚Äùworld state‚Äù, enabling all smart contracts on the same chain to interact with each other. This capability has been instrumental in creating a highly interconnected ecosystem where composability ‚Äì the ability to combine various protocols and applications ‚Äì drives continuous innovation and complex financial instruments such as decentralized finance products (i.e. money lego) [2]. Despite these strengths, the EVM faces two significant limitations that constrain its utility and expressiveness: access to historical state and the state of other chains within the ecosystem. Historical state access refers to the ability to query past states of the blockchain, while multichain state access is essential for interoperability between different blockchain networks, particularly between Ethereum and its Layer 2 solutions [3]. We investigate storage proofs as a comprehensive solution to these limitations. Storage proofs enable verifiable access to the blockchain states by providing cryptographic evidence of data consistency and integrity. However, while storage proofs represent a significant step forward, they come with their own set of challenges, particularly in terms of performance and developer experience. The complexity of implementing and verifying storage proofs can deter developers while the performance overhead can reduce the efficiency of smart contracts, impacting the overall user experience. Our analysis presents two distinct approaches for historical state verification using Merkle Mountain Range (MMR) and Merkle-Patricia trie (MPT) structures. We demonstrate that while MMR provides efficient proof generation, MPT offers superior flexibility for managing historical data at any depth. Furthermore, we formalize three distinct patterns for cross-chain verification: L2‚Üí‚Üí\rightarrow‚ÜíL1, L1‚Üí‚Üí\rightarrow‚ÜíL2, and L2‚Üí‚Üí\rightarrow‚ÜíL2, accounting for the asymmetric security relationships between layers and their varying finality characteristics. Additionally, the paper addresses performance challenges related to using Keccak-256 in zero-knowledge contexts, and analyzes alternative ZK-friendly hash functions. The paper is structured as follows: Section 2 provides the preliminaries, covering foundational data structures such as Merkle trees, Patricia tries, Merkle-Patricia tries, and their implementation in Ethereum. This section also introduces versioned data structures and conceptualizes Ethereum as a data structure. Section 3 examines the mechanics of storage proofs in detail, progressing from basic storage proofs and hierarchical proofs to an analysis of historical state verification using MMR and MPT approaches. The section concludes with a formalization of three architectures for multichain verification. Finally, Section 4 summarizes the findings and discusses their implications for the broader Ethereum ecosystem."
https://arxiv.org/html/2411.00067v1,Masking Gaussian Elimination at Arbitrary Orderwith Application to Multivariate- and Code-Based PQC,"Digital signature schemes based on multivariate- and code-based hard problems are promising alternatives for lattice-based signature schemes, due to their smaller signature size. Hence, several candidates in the ongoing additional standardization for quantum secure digital signature (DS) schemes by the National Institute of Standards and Technology (NIST) rely on such alternate hard problems. Gaussian Elimination (GE) is a critical component in the signing procedure of these schemes. In this paper, we provide a masking scheme for GE with back substitution to defend against first- and higher-order attacks. To the best of our knowledge, this work is the first to analyze and propose masking techniques for multivariate- or code-based DS algorithms. We propose a masked algorithm for transforming a system of linear equations into row-echelon form. This is realized by introducing techniques for efficiently making leading (pivot) elements one while avoiding costly conversions between Boolean and multiplicative masking at all orders. We also propose a technique for efficient masked back substitution, which eventually enables a secure unmasking of the public output. We evaluate the overhead of our countermeasure for several post-quantum candidates and their different security levels at first-, second-, and third-order, including UOV, MAYO, SNOVA, QR-UOV, and MQ-Sign. Notably, the operational cost of first-, second-, and third-order masked GE is 2.3√ó\times√ó higher, and the randomness cost is 1.2√ó\times√ó higher in MAYO compared to UOV for security levels III and V. In contrast, these costs are similar in UOV and MAYO for one version of level I. We also show detailed performance results for masked GE implementations for all three security versions of UOV on the Arm Cortex-M4 and compare them with unmasked results. Our first-order implementations targeting UOV parameters have overheads of factor 6.5√ó\times√ó, 5.9√ó\times√ó, and 5.7√ó\times√ó compared to the unprotected implementation for NIST security level I, III, and V.","The National Institute of Standards and Technology (NIST) published the first set of Post-Quantum Cryptographic (PQC) standards in August 2024 [35, 34, 36]. Three of the four selected cryptographic schemes are based on hard lattice problems. To diversify their portfolio and avoid the dependency on a single hard problem, NIST announced another process [32] to standardize additional post-quantum Digital Signature (DS) schemes. The submitted schemes are designed from various hard problems, such as code-based, hash-based, and multivariate quadratic (MQ) system-based cryptography. Recently, NIST announced that 14 out of 40 initial candidates advanced to the second round [33]. Among the selected submissions, four signatures are from MQ-based cryptography and use the hash-and-sign paradigm. These schemes mainly rely on the computational hardness of solving multivariate quadratic systems, a problem known to be NP-complete [27]. The Unbalanced Oil and Vinegar (UOV) signature scheme is one of the oldest and well studied multivariate construction [29]. Gaussian Elimination (GE) is a key component of the signing procedure in many of the schemes selected for the second round of NIST PQC DS on-ramp [33], used for finding the unique solution of a system of linear equations. All MQ-based signature schemes, such as (i) UOV [8], (ii) MAYO [7], (iii) SNOVA [42], (iv) QR-UOV [20], (v) MQ-Sign [40], (vi) PROV [23], (vii) VOX [13], (viii) TUOV [19], (ix) VDOO [21], and (x) IPRainbow [9] rely on GE during signing. Recently it has also been used in some code-based (CB) signature schemes such as Wave [2]. As the secret key is used during the signing and the GE procedure, it is a potential target for side-channel attacks. Side-Channel Analysis (SCA) attacks can have severe impacts on cryptographic implementations, and post-quantum schemes are equally vulnerable [41, 26, 24]. SCA attacks extract secret data of the mathematically secure cryptographic algorithm from the cryptographic device by observing the computation and its physical behavior. Such attacks can be prevented by ensuring that any computation in the cryptographic algorithm is independent of any secret variables. Masking [10] is a provably secure widely used countermeasure of such attacks [39, 18, 30]. Various SCAs have been demonstrated on the UOV-based signature schemes in the literature [1, 43, 37]. However, there is almost no research on countermeasures for MQ and CB digital signature schemes, including UOV, to prevent potential SCAs. Even more so, no specialized gadgets for the GE operation, a critical and costly component during signing, have been proposed. Contributions. We propose first- and higher-order masked algorithms for solving a system of linear equations using (masked) Gaussian elimination with back substitution (SecRowEch & SecBackSub), a critical component in MQ and CB signature schemes. We formally prove their security in the tùë°titalic_t-probing model and analyze the complexity. Our techniques are highly parametrizable and can be extended to other schemes that rely on GE to solve a system of linear equations. ‚Ä¢ For efficiently solving the linear system, it needs to be in row-echelon form. We propose masked gadgets for making the pivot element non-zero, by securely adding different rows to the pivot-row if it is zero. Our approach introduces secure conditional addition, without revealing the pivot element itself (see Section 3). ‚Ä¢ Subsequently, the pivot coefficient needs to be reduced to value one. As directly computing its inverse would require unmasking the pivot element, our approach is based on switching masking representations for computing its multiplicative inverse. Finally, we devise an efficient masked gadget for back substituting the equations (see Section 3). ‚Ä¢ We apply our techniques on several promising, UOV-based DS schemes (UOV, MAYO, SNOVA, QR-UOV & MQ-Sign) and compare their operation and randomness cost for masking the GE with back substitution, at first-, second-, and third-order. We analyze and show how their parameter choices impact the cost of masking the GE operation in Table 2 (see Section 4). ‚Ä¢ We provide an arbitrary-order masked implementation (C code222Our source code will be made publicly available at the time of publication.) of our masked GE and evaluate the performance of our methods for UOV parameters on an Arm Cortex-M4 processor. Our first-order masked implementations of GE show an overhead of factor 6.5√ó\times√ó, 5.9√ó5.9\times5.9 √ó, and 5.7√ó\times√ó compared to the unmasked GE for UOV-I, UOV-III, and UOV-V, respectively. The most expensive steps for masking GE are ensuring the pivot element is non-zero and the full reduction to row-echelon form (see Section 5)."
https://arxiv.org/html/2411.00762v1,Face Anonymization Made Simple,"Current face anonymization techniques often depend on identity loss calculated by face recognition models, which can be inaccurate and unreliable. Additionally, many methods require supplementary data such as facial landmarks and masks to guide the synthesis process. In contrast, our approach uses diffusion models with only a reconstruction loss, eliminating the need for facial landmarks or masks while still producing images with intricate, fine-grained details. We validated our results on two public benchmarks through both quantitative and qualitative evaluations. Our model achieves state-of-the-art performance in three key areas: identity anonymization, facial attribute preservation, and image quality. Beyond its primary function of anonymization, our model can also perform face swapping tasks by incorporating an additional facial image as input, demonstrating its versatility and potential for diverse applications. Our code and models are available at https://github.com/hanweikung/face_anon_simple.","In the digital age, our identity and privacy are more vulnerable than ever. People have shared personal information and photos online over recent decades, while advancements in facial recognition technology have made it easier to identify individuals from a single image. This combination allows for the potential linking of our faces to personal information, posing a significant threat to our privacy and identity. In response, various regions have enacted privacy protection laws. These include the European Union‚Äôs General Data Protection Regulation (GDPR) [1], California‚Äôs Consumer Privacy Act, and Japan‚Äôs amended Act on the Protection of Personal Information. Such legislation mandates that organizations implement security measures and maintain transparency in their handling of personal data. Face anonymization is essential for protecting individuals in photos and videos, thereby reducing the risk of personal data being compromised or misused. Traditional methods like blurring and pixelation are common but have significant drawbacks. These techniques are vulnerable to reconstruction attacks [52], degrade image quality, and apply a uniform transformation across the image without considering which areas are most critical to anonymize. These limitations make traditional methods impractical for professionals who need to preserve facial expressions and backgrounds. For example, medical practitioners may need to anonymize patient images for case studies or research while retaining crucial facial cues that indicate symptoms. In creative fields, documentary filmmakers might want to protect interviewees‚Äô privacy without losing the narrative impact of their facial expressions and reactions. They may also wish to replace an interviewee‚Äôs face with a specific virtual identity to enhance storytelling clarity. In contrast, recent advances in deep learning have led to more effective anonymization techniques that enhance both privacy protection and usability. Generative Adversarial Networks (GANs) [17], in particular, can anonymize faces by replacing the original with computer-generated alternatives [25, 37, 10, 50]. However, these methods are not without challenges. Some fail to produce natural-looking faces [37], while others [25] struggle to preserve crucial elements like facial expressions, eye direction, head orientation, background details, clothing, and accessories. These limitations greatly restrict the practical application of these techniques. This paper presents a diffusion-based method for face anonymization. Our goal is to ensure that de-identified facial images remain useful for facial analysis tasks, including pose estimation, eye-gaze tracking, and expression recognition, as well as for broader uses such as interviews and films. Therefore, we approach face anonymization similarly to face swapping, aiming to generate an image where a person‚Äôs face is replaced by another person‚Äôs face while maintaining the original facial expression, pose, eye gaze, and background. We designed a framework that initially performs realistic and seamless face swaps given both source and driving images. At its core is a denoising UNet architecture, similar to those used in text-to-image diffusion models, which generates the final output. We enhance this with an image feature extraction mechanism that transfers fine details from input images to the synthesized output throughout the diffusion process. The model is then trained in a dual setting: conditionally with a source image and unconditionally without a source image. This dual method allows the model to replace faces using one single image input. To create a distinct anonymized identity, the system reverses the original face‚Äôs most distinctive features. This technique produces a believable anonymized face while preserving the original image‚Äôs quality and essential facial characteristics. In summary, our contributions are: ‚Ä¢ A convenient method that produces realistic anonymized faces while preserving attributes, without needing external data like facial landmarks or masks as required by existing techniques. ‚Ä¢ A diffusion-based network that achieves good performance with a single, simple loss function, in contrast to GAN-based models requiring multiple, carefully designed loss functions. ‚Ä¢ Simple control of the anonymization level using a single parameter. ‚Ä¢ Versatility beyond anonymization, including the ability to perform face swapping tasks with an additional facial image input."
https://arxiv.org/html/2411.00721v1,New classes of reversible cellular automata,"A Boolean function fùëìfitalic_f on kùëòkitalic_k bits induces a shift-invariant vectorial Boolean function FùêπFitalic_F from nùëõnitalic_n bits to nùëõnitalic_n bits for every n‚â•kùëõùëòn\geq kitalic_n ‚â• italic_k. If FùêπFitalic_F is bijective for every nùëõnitalic_n, we say that fùëìfitalic_f is a proper lifting, and it is known that proper liftings are exactly those functions that arise as local rules of reversible cellular automata. We construct new families of such liftings for arbitrary large kùëòkitalic_k and discuss whether all have been identified for k‚â§6ùëò6k\leq 6italic_k ‚â§ 6.","Shift-invariant functions are integral to symmetric cryptography, especially for lightweight cryptography, particularly in designing substitution boxes (S-boxes) for block ciphers and hash functions. These functions, and also connections to their associated cellular automata, which have a broad range of applications, have been studied by several authors, see e.g., [4] for a survey. Any shift-invariant function on nùëõnitalic_n-bits is derived by a local rule, that is, a Boolean function on kùëòkitalic_k-bits for some k‚â§nùëòùëõk\leq nitalic_k ‚â§ italic_n. Conversely, every Boolean function f:ùîΩ2k‚ÜíùîΩ2:ùëì‚ÜísuperscriptsubscriptùîΩ2ùëòsubscriptùîΩ2f\colon\mathbb{F}_{2}^{k}\to\mathbb{F}_{2}italic_f : blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ‚Üí blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT induces a shift-invariant function F:ùîΩ2n‚ÜíùîΩ2n:ùêπ‚ÜísuperscriptsubscriptùîΩ2ùëõsuperscriptsubscriptùîΩ2ùëõF\colon\mathbb{F}_{2}^{n}\to\mathbb{F}_{2}^{n}italic_F : blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚Üí blackboard_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT for every n‚â•kùëõùëòn\geq kitalic_n ‚â• italic_k. If for every such nùëõnitalic_n the induced map FùêπFitalic_F is bijective, then fùëìfitalic_f is called a proper lifting, and the corresponding cellular automaton is reversible. This work constructs and classifies new families of proper liftings, thus advancing the understanding of reversible cellular automata, and evaluate their resistance to differential cryptanalysis. In Section 1, we describe a new way of produce proper liftings through composition of landscape functions, generalizing existing constructions coming from sets of landscapes, see e.g., [1]. In Section 2, we study in detail the case k=6ùëò6k=6italic_k = 6, and analyze whether these families are exhaustive. Finally, in Section 4, we present a few other constructions of families of proper liftings."
https://arxiv.org/html/2411.00529v1,"A General Quantum Duality for Representations of Groupswith Applications to
Quantum Money,
Lightning,
and
Fire","Aaronson, Atia, and Susskind [AAS20] established that efficiently mapping between quantum states |œà‚ü©ketùúì\ket{\psi}| start_ARG italic_œà end_ARG ‚ü© and |œï‚ü©ketitalic-œï\ket{\phi}| start_ARG italic_œï end_ARG ‚ü© is computationally equivalent to distinguishing their superpositions 12‚Å¢(|œà‚ü©+|œï‚ü©)12ketùúìketitalic-œï\frac{1}{\sqrt{2}}(\ket{\psi}+\ket{\phi})divide start_ARG 1 end_ARG start_ARG square-root start_ARG 2 end_ARG end_ARG ( | start_ARG italic_œà end_ARG ‚ü© + | start_ARG italic_œï end_ARG ‚ü© ) and 12‚Å¢(|œà‚ü©‚àí|œï‚ü©)12ketùúìketitalic-œï\frac{1}{\sqrt{2}}(\ket{\psi}-\ket{\phi})divide start_ARG 1 end_ARG start_ARG square-root start_ARG 2 end_ARG end_ARG ( | start_ARG italic_œà end_ARG ‚ü© - | start_ARG italic_œï end_ARG ‚ü© ). We generalize this insight into a broader duality principle in quantum computation, wherein manipulating quantum states in one basis is equivalent to extracting their value in a complementary basis. In its most general form, this duality principle states that for a given group, the ability to implement a unitary representation of the group is computationally equivalent to the ability to perform a Fourier subspace extraction from the invariant subspaces corresponding to its irreducible representations.Building on our duality principle, we present the following applications:Quantum money, which captures quantum states that are verifiable but unclonable, and its stronger variant, quantum lightning, have long resisted constructions based on concrete cryptographic assumptions. While (public-key) quantum money has been constructed from indistinguishability obfuscation (iO)‚Äîan assumption widely considered too strong‚Äîquantum lightning has not been constructed from any such assumptions, with previous attempts based on assumptions that were later broken. We present the first construction of quantum lightning with a rigorous security proof, grounded in a plausible and well-founded cryptographic assumption. We extend Zhandry‚Äôs construction from Abelian group actions [Zha24] to non-Abelian group actions, and eliminate Zhandry‚Äôs reliance on a black-box model for justifying security. Instead, we prove a direct reduction to a computational assumption ‚Äì the pre-action security of cryptographic group actions. We show how these group actions can be realized with various instantiations, including with the group actions of the symmetric group implicit in the McEliece cryptosystem.We provide an alternative quantum money and lightning construction from one-way homomorphisms, showing that security holds under specific conditions on the homomorphism. Notably, our scheme exhibits the remarkable property that four distinct security notions‚Äîquantum lightning security, security against both worst-case cloning and average-case cloning, and security against preparing a specific canonical state‚Äîare all equivalent.Quantum fire captures the notion of a samplable distribution on quantum states that are efficiently clonable, but not efficiently telegraphable, meaning they cannot be efficiently encoded as classical information. These states can be spread like fire, provided they are kept alive quantumly and do not decohere. The only previously known construction relied on a unitary quantum oracle, whereas we present the first candidate construction of quantum fire in the plain model.","Let |œà0‚ü©,|œà1‚ü©ketsubscriptùúì0ketsubscriptùúì1\ket{\psi_{0}},\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© , | start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© be two orthogonal quantum states, and let |œï+‚ü©ketsubscriptitalic-œï\ket{\phi_{+}}| start_ARG italic_œï start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_ARG ‚ü© be proportional to |œà0‚ü©+|œà1‚ü©ketsubscriptùúì0ketsubscriptùúì1\ket{\psi_{0}}+\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© + | start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© and |œï‚àí‚ü©ketsubscriptitalic-œï\ket{\phi_{-}}| start_ARG italic_œï start_POSTSUBSCRIPT - end_POSTSUBSCRIPT end_ARG ‚ü© be proportional to |œà0‚ü©‚àí|œà1‚ü©ketsubscriptùúì0ketsubscriptùúì1\ket{\psi_{0}}-\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© - | start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü©. The Swap Complexity of |œà0‚ü©ketsubscriptùúì0\ket{\psi_{0}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© and |œà1‚ü©ketsubscriptùúì1\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© is the size of the smallest circuit that maps |œà0‚ü©ketsubscriptùúì0\ket{\psi_{0}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© to |œà1‚ü©ketsubscriptùúì1\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© and vice versa. Meanwhile, the Distinguishing Complexity of |œï+‚ü©ketsubscriptitalic-œï\ket{\phi_{+}}| start_ARG italic_œï start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_ARG ‚ü© and |œï‚àí‚ü©ketsubscriptitalic-œï\ket{\phi_{-}}| start_ARG italic_œï start_POSTSUBSCRIPT - end_POSTSUBSCRIPT end_ARG ‚ü© is the size of the smallest circuit that accepts |œï+‚ü©ketsubscriptitalic-œï\ket{\phi_{+}}| start_ARG italic_œï start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_ARG ‚ü© and rejects |œï‚àí‚ü©ketsubscriptitalic-œï\ket{\phi_{-}}| start_ARG italic_œï start_POSTSUBSCRIPT - end_POSTSUBSCRIPT end_ARG ‚ü©. A fundamental result of Aaronson, Atia, and Susskind [AAS20] establishes that the swap complexity of |œà0‚ü©ketsubscriptùúì0\ket{\psi_{0}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© and |œà1‚ü©ketsubscriptùúì1\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© is essentially equivalent to the distinguishing complexity of |œï+‚ü©ketsubscriptitalic-œï\ket{\phi_{+}}| start_ARG italic_œï start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_ARG ‚ü© and |œï‚àí‚ü©ketsubscriptitalic-œï\ket{\phi_{-}}| start_ARG italic_œï start_POSTSUBSCRIPT - end_POSTSUBSCRIPT end_ARG ‚ü©. This duality principle, known as the ‚ÄúAAS duality‚Äù, has emerged as a simple yet powerful tool in quantum complexity theory and cryptography. In this work, we ask: Can the AAS equivalence be extended to the more general context of many quantum states and multidimensional subspaces? We give an affirmative answer to this question. First, we extend the notion of the swap complexity to a notion of ‚Äúrepresentation complexity‚Äù: given a subspace, VùëâVitalic_V, spanned by states |œà1‚ü©,‚Ä¶,|œàk‚ü©ketsubscriptùúì1‚Ä¶ketsubscriptùúìùëò\ket{\psi_{1}},\dots,\ket{\psi_{k}}| start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© , ‚Ä¶ , | start_ARG italic_œà start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ‚ü©, and a (potentially non-Abelian) group Gùê∫{G}italic_G, a representation of Gùê∫{G}italic_G on the subspace VùëâVitalic_V is a homomorphism from Gùê∫{G}italic_G to the unitaries acting on the subspace (or, roughly, it is a collection of unitaries {Ug}g‚ààGsubscriptsubscriptùëàùëîùëîùê∫\{U_{g}\}_{g\in{G}}{ italic_U start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_g ‚àà italic_G end_POSTSUBSCRIPT acting on VùëâVitalic_V which satisfies the group operations of Gùê∫{G}italic_G).111 For instance, in [AAS20], the representation of G=‚Ñ§2ùê∫subscript‚Ñ§2{G}={\mathbb{Z}}_{2}italic_G = blackboard_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT on the subspace spanned by |œà0‚ü©ketsubscriptùúì0\ket{\psi_{0}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© and |œà1‚ü©ketsubscriptùúì1\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© maps the sole non-identity element of ‚Ñ§2subscript‚Ñ§2{\mathbb{Z}}_{2}blackboard_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT to the unitary swapping |œà0‚ü©ketsubscriptùúì0\ket{\psi_{0}}| start_ARG italic_œà start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ‚ü© for |œà1‚ü©ketsubscriptùúì1\ket{\psi_{1}}| start_ARG italic_œà start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü©. Its Representation Complexity is the size of the smallest circuit that implements the representation, that is, by mapping |g‚ü©‚äó|œài‚ü©‚Ü¶|g‚ü©‚äóUg‚Å¢|œài‚ü©.maps-totensor-productketùëîketsubscriptùúìùëñtensor-productketùëîsubscriptùëàùëîketsubscriptùúìùëñ\displaystyle\ket{g}\otimes\ket{\psi_{i}}\mapsto\ket{g}\otimes U_{g}\ket{\psi_% {i}}\,.| start_ARG italic_g end_ARG ‚ü© ‚äó | start_ARG italic_œà start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ‚ü© ‚Ü¶ | start_ARG italic_g end_ARG ‚ü© ‚äó italic_U start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT | start_ARG italic_œà start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ‚ü© . When restricting to groups that have an efficient quantum Fourier transform (including all Abelian groups, all constant-sized or polynomal-sized non-Abelian groups, and several important exponential-sized non-Abelian groups), we show that the representation complexity is essentially equivalent to the complexity of implementing a Fourier subspace extraction, or in other words, performing a partial measurement of the invariant subspaces preserved by the representation (i.e., its irreducible representation subspaces) and extracting the quantum state encoded in each such subspace (see Section 1.2 for more discussion on subspace extraction). For Abelian groups, this simplifies to a full projective measurement, and in particular, for the swapping representation of AAS, this is a measurement between |œï+‚ü©ketsubscriptitalic-œï\ket{\phi_{+}}| start_ARG italic_œï start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_ARG ‚ü© and |œï‚àí‚ü©ketsubscriptitalic-œï\ket{\phi_{-}}| start_ARG italic_œï start_POSTSUBSCRIPT - end_POSTSUBSCRIPT end_ARG ‚ü©. Thus the AAS duality is recovered by setting G=‚Ñ§2ùê∫subscript‚Ñ§2{G}={\mathbb{Z}}_{2}italic_G = blackboard_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. We additionally prove an approximate notion of this duality, where the circuit only has to approximately map between states.222 While our approximate duality theorem works for all groups, it achieves weaker error bounds for general groups. 1.1 Applications to Cryptography In cryptography, the AAS duality has proven quite fruitful. Cryptographic security properties come in two types: ‚Äúsearch‚Äù type properties which stipulate the hardness of computing a specific unknown quantity, and ‚Äúdecision‚Äù type properties which stipulate the hardness of distinguishing between two distributions. The AAS duality has played a crucial role in establishing the equivalence between certain search-type and decision-type properties, leading to a number of significant results [Yan22, HMY23, KMNY24, MW24, HKNY24, MYY24]. We show that our new duality theorem is useful for cryptography beyond the AAS setting, by giving novel results for quantum money. Quantum Money from Group Actions. Quantum money [Wie83] uses the no-cloning principle to generate unforgeable banknotes. These banknotes are quantum states that can be verified but cannot be cloned. A central problem has been to construct quantum money that can be publicly verified by anyone, and yet only the mint can create new banknotes. This is called public-key quantum money [Aar09].333 When it is otherwise clear from context, we will refer to public key quantum money as simply ‚Äúquantum money‚Äù. Quantum lightning posits a stronger security notion for public-key quantum money, with a collision-resistance property that ensures that even the mint can only ever create one copy of each banknote [Zha21]. A long-standing challenge for public-key quantum money is to derive security from concrete computational assumptions (and in particular, assumptions that do not bake the unclonability of the banknotes directly into the assumption). The only prior scheme with such a proof is an instantiation of [AC12] using indistinguishability obfuscation (iO), as suggested by [BDS23] and proved in [Zha21]. However, iO is a powerful cryptographic tool whose quantum security is still uncertain. Moreover, no existing unbroken scheme has been shown to have such a security proof for the stronger security notion of quantum lightning. Recently, [Zha24] gave a plausible construction of quantum money and quantum lightning from Abelian group actions. A group action consists of a group Gùê∫Gitalic_G, a set XùëãXitalic_X, and a binary operation ‚àó:G√óX‚ÜíX*:G\times X\rightarrow X‚àó : italic_G √ó italic_X ‚Üí italic_X, denoted g‚àóx=yùëîùë•ùë¶g*x=yitalic_g ‚àó italic_x = italic_y. This operation respects the group structure: g‚àó(h‚àóx)=(g‚Å¢h)‚àóxùëî‚Ñéùë•ùëî‚Ñéùë•g*(h*x)=(gh)*xitalic_g ‚àó ( italic_h ‚àó italic_x ) = ( italic_g italic_h ) ‚àó italic_x. An Abelian group action is a group action where Gùê∫Gitalic_G is Abelian.444 Abelian groups are those for which all the elements commute: g‚Å¢h=h‚Å¢g‚Å¢‚àÄg,h‚ààGformulae-sequenceùëî‚Ñé‚Ñéùëîfor-allùëî‚Ñéùê∫gh=hg\;\;\forall g,h\in{G}italic_g italic_h = italic_h italic_g ‚àÄ italic_g , italic_h ‚àà italic_G. Unfortunately, the security of the scheme of [Zha24] requires both a computational assumption and an idealized modeling of group actions as a black box. Using our duality principle, we show how to generalize this construction to work with non-Abelian group actions. This shift is not merely a superficial adjustment‚Äîit significantly improves on the framework in two critical ways: 1. It allows us to prove the hardness of our quantum money and lightning scheme in the standard model, using only a concrete assumption on the group action. This assumption also identifies an interesting potential source of hardness for non-Abelian group actions. Very roughly, for non-Abelian groups, in addition to a group action g‚àó(h‚àóx)=(g‚Å¢h)‚àóxùëî‚Ñéùë•ùëî‚Ñéùë•g\,*\,(h*x)=(gh)*xitalic_g ‚àó ( italic_h ‚àó italic_x ) = ( italic_g italic_h ) ‚àó italic_x, we can also define a ‚Äúpre-action‚Äù g‚àò(h‚àóx)=(h‚Å¢g‚àí1)‚àóxùëî‚Ñéùë•‚Ñésuperscriptùëî1ùë•g\circ(h*x)=(hg^{-1})*xitalic_g ‚àò ( italic_h ‚àó italic_x ) = ( italic_h italic_g start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ‚àó italic_x, or more generally a ‚Äúbi-action‚Äù (g0,g1)‚äõ(h‚àóx)=(g0‚Å¢h‚Å¢g1‚àí1)‚àóx‚äõsubscriptùëî0subscriptùëî1‚Ñéùë•subscriptùëî0‚Ñésuperscriptsubscriptùëî11ùë•(g_{0},g_{1})\circledast(h*x)=(g_{0}hg_{1}^{-1})*x( italic_g start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ‚äõ ( italic_h ‚àó italic_x ) = ( italic_g start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_h italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ‚àó italic_x. Our assumption states that it is hard via a quantum query to distinguish a random action from a random bi-action. Importantly, this problem only makes sense for non-Abelian group actions, as actions and pre-actions are identical in the Abelian case. Thus, the quantum money result requires us to use the full power of our non-Abelian generalization of the duality. 2. The shift to non-Abelian groups opens up the possibility for potentially more varied instantiations of the group actions. In particular, we explain how to instantiate our quantum money scheme on (a significant generalization of) the symmetric group action implicit in the McEliece cryptosystem [McE78]. Theorem 1.1 (informal). There is a public-key quantum money and quantum lightning scheme for any (non-Abelian) cryptographic group action, such that the money/lightning scheme is secure if the group action is preaction-secure. To the best of our knowledge, this represents the first (unbroken) quantum lightning scheme with a standard-model security proof based on a computational assumption that does inherently include unclonability. Quantum Money from One-way Homomorphisms. A one-way (group) homomorphism is a function, f‚Å¢(h)ùëì‚Ñéf(h)italic_f ( italic_h ), that is group homomorphic555 That is, it is a homomorphism between two groups Gùê∫Gitalic_G and HùêªHitalic_H, such that f‚Å¢(g‚Å¢h)=f‚Å¢(g)‚ãÖf‚Å¢(h)ùëìùëî‚Ñé‚ãÖùëìùëîùëì‚Ñéf(gh)=f(g)\cdot f(h)italic_f ( italic_g italic_h ) = italic_f ( italic_g ) ‚ãÖ italic_f ( italic_h ) for all g,h‚ààGùëî‚Ñéùê∫g,h\in Gitalic_g , italic_h ‚àà italic_G. and efficiently computable, but computationally intractable to invert.666 Note that Shor‚Äôs algorithm [Sho94] allows efficiently inverting group homomorphosms when the domain and codomain groups are Abelian. Thus, these results inherently require non-Abelian groups, and hence our generalized duality. A one-way homomorphism can be seen as an instance of a group action, with the domain group acting on the codomain as g‚àóf‚Å¢(h)=f‚Å¢(g‚Å¢h)ùëîùëì‚Ñéùëìùëî‚Ñég*f(h)=f(gh)italic_g ‚àó italic_f ( italic_h ) = italic_f ( italic_g italic_h ). However, unlike in the previous case above, the preactions for this action (i.e., g‚àòf‚Å¢(h)=f‚Å¢(h‚Å¢g‚àí1)ùëîùëì‚Ñéùëì‚Ñésuperscriptùëî1g\circ f(h)=f(hg^{-1})italic_g ‚àò italic_f ( italic_h ) = italic_f ( italic_h italic_g start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT )) are as efficiently computable as the action itself, so security cannot be shown as before. Nevertheless, we give sufficient conditions on the one-way homomorphism such that the resulting quantum lightning scheme is secure. We note that unlike our construction above from group actions that are pre-action secure‚Äîfor which we give concrete instantiations that can be implemented in practice‚Äîwe do not know if any instantiations of homomorphic functions satisfy these security conditions. But we observe that a one-way group homomorphism is essentially a group action where the computational Diffie-Hellman (CDH) problem is easy but yet discrete logarithms are still hard. While CDH is quantumly equivalent to discrete logarithms for Abelian groups [MZ22], this equivalence does not seem to follow for non-Abelian groups. Strangely, it is a hypothetical security failure for group actions which gives rise to plausible instantiations for quantum lightning and quantum fire (see more on the construction of quantum fire below). We concede the disadvantage of this construction as compared to the concrete one above from preaction security, but we note that it has some unique properties that the other does not. Specifically, by leveraging our duality principle we are able to prove the remarkable fact that four distinct quantum money security notions‚Äînamely, the collision-resistance of quantum lightning security, the hardness of both worst-case cloning and average-case cloning, and the hardness of preparing the uniform superposition over the image of the homomorphism‚Äîare all identical. Thus for any particular instantiation of the one-way homomorphism, it is sufficient to prove any one of these security notions in order to get the other three. Quantum Fire. Quantum fire refers to a collection of efficiently samplable quantum states that can be efficiently cloned, but cannot be efficiently telegraphed.777 Note that while the no-cloning theorem prohibits cloning general quantum states, this prohibition does not apply to quantum states chosen from an orthogonal set. The same applies to the no-telegraphing theorem, which prohibits sending general quantum states through a classical channel without pre-shared entanglement. States from an orthogonal set can clearly be telegraphed by measuring them in this basis and later recreating them accordingly. Such states can be cloned in a similar fashion. In other words, any states chosen from an orthogonal set can be both cloned and telegraphed information-theoretically, but these tasks are not necessarily both efficient. In fact, it was shown in [NZ23] that there are likely to be state families where cloning is efficient and yet telegraphing is not. Quantum fire is the cryptographic primitive that samples such states efficiently. That is, despite the ability to make an unbounded number of copies of a quantum fire state, there is no way to efficiently encode it as classical information from which it can later be recovered. Much like a flame can be easily spread from a single source as long as it is kept alive, quantum fire can be cloned from a single quantum state as long as it is kept coherently in quantum storage. The concept of quantum fire was first introduced in the work of Nehoran and Zhandry [NZ23], where it was shown to be essential for solving the key exfiltration problem. However, it was not formally defined or named in that work. [NZ23] provided a secure construction of quantum fire relative to a unitary quantum oracle, but this oracle construction relied on an inherently inefficient computation and baked clonability into the oracle itself. Consequently, it does not provide a pathway for instantiation in the standard model. It has not even been clear if any classical oracle could allow efficient cloning of quantum states that are inherently quantum (and thus not telegraphable). Inspired by the duality principle, we give a plausible candidate construction of quantum fire relative to a one-way group homomorphism. Remarkably, despite the similarity to the construction of quantum lightning from group homomorphisms, where the states are unclonable, the states in this scheme are inherently clonable, and efficiently so. Nevertheless, there is no apparent way to telegraph the states efficiently. Moreover, it is straightforward to define a classical oracle that gives a candidate one-way group homomorphism. Thus, we obtain a candidate construction of quantum fire with conjectured security relative to a classical oracle, improving upon the unitary oracle construction of [NZ23].888 Note that, as observed in [NZ23], an unconditional security proof relative to such a classical oracle would require proving a classical oracle separation between the complexity classes ùñ∞ùñ¨ùñ†ùñ∞ùñ¨ùñ†{\mathsf{QMA}}sansserif_QMA and ùñ∞ùñ¢ùñ¨ùñ†ùñ∞ùñ¢ùñ¨ùñ†{\mathsf{QCMA}}sansserif_QCMA, a major open problem of Aharonov and Naveh [AN02], which, despite recent progress, has evaded resolution. 1.2 The Duality Fourier Subspace Extraction. A major stepping stone towards our duality theorem is the idea of a Fourier subspace extraction. Every group representation preserves some set of invariant subspaces {ùñ∂Œª}Œª‚àà[n]subscriptsubscriptùñ∂ùúÜùúÜdelimited-[]ùëõ\{\mathsf{W}_{\lambda}\}_{\lambda\in[n]}{ sansserif_W start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_Œª ‚àà [ italic_n ] end_POSTSUBSCRIPT.999 That is, these subspaces are invariant under all of the unitaries UgsubscriptùëàùëîU_{g}italic_U start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT corresponding to each group element g‚ààGùëîùê∫g\in{G}italic_g ‚àà italic_G. In some cases, the only invariant subspace may be the full Hilbert space, in which case we say that it is irreducible, but this is not generically the case. We consider here only invariant subspaces which are irreducible, and do not break down further into smaller invariant subspaces. A course Fourier measurement101010 Often called weak Fourier sampling in many contexts of the representation is, roughly, a projection onto these subspaces. We get a classical label ŒªùúÜ\lambdaitalic_Œª indicating the subspace we have projected onto, as well as a collapsed state, |œà‚ü©ketùúì\ket{\psi}| start_ARG italic_œà end_ARG ‚ü©, within the subspace ùñ∂Œªsubscriptùñ∂ùúÜ\mathsf{W}_{\lambda}sansserif_W start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT. A fine Fourier measurement further measures within each of those subspaces, in a basis that depends on the algorithm. For instance, if {|œàjŒª‚ü©}j‚ààdim(ùñ∂Œª)subscriptketsubscriptsuperscriptùúìùúÜùëóùëódimensionsubscriptùñ∂ùúÜ\{\ket{\psi^{\lambda}_{j}}\}_{j\in\dim(\mathsf{W}_{\lambda})}{ | start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ‚ü© } start_POSTSUBSCRIPT italic_j ‚àà roman_dim ( sansserif_W start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT is a basis for ùñ∂Œªsubscriptùñ∂ùúÜ\mathsf{W}_{\lambda}sansserif_W start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT, we get outcomes ŒªùúÜ\lambdaitalic_Œª and jùëójitalic_j, and collapse our state to |œàjŒª‚ü©ketsubscriptsuperscriptùúìùúÜùëó\ket{\psi^{\lambda}_{j}}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ‚ü©.111111 To simplify the notation, we assume here that there is no multiplicity, or degeneracy, in the irreducible representations. We will see later how to handle multiplicity. In either case, the state after the measurement is still within the subspace. In some applications, we care about the coherent information encoded within each subspace. That is, it is not enough to know which collapsed state |œàjŒª‚ü©ketsubscriptsuperscriptùúìùúÜùëó\ket{\psi^{\lambda}_{j}}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ‚ü© we received. We want to have, in our hands, the coherent superposition that appeared in the subspace. That is, if the original state was ‚àëj‚àà[dim(ùñ∂Œª)]Œ±j‚Å¢|œàjŒª‚ü©subscriptùëódelimited-[]dimensionsubscriptùñ∂ùúÜsubscriptùõºùëóketsubscriptsuperscriptùúìùúÜùëó\sum_{j\in[\dim(\mathsf{W}_{\lambda})]}\alpha_{j}\ket{\psi^{\lambda}_{j}}‚àë start_POSTSUBSCRIPT italic_j ‚àà [ roman_dim ( sansserif_W start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT ) ] end_POSTSUBSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG ‚ü©, we want to extract the full superposition ‚àëjŒ±j‚Å¢|j‚ü©subscriptùëósubscriptùõºùëóketùëó\sum_{j}\alpha_{j}\ket{j}‚àë start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_ARG italic_j end_ARG ‚ü©. This transformation, which we call a subspace extraction, extracts the full state coherently from the subspace.121212Note that such extraction is not generally an efficient transformation for arbitrary subspaces. If implemented na√Øvely, Fourier measurements do not suffice for this task. They either do not recover the information about where the state was within each subspace (in the case of course Fourier measurement), or they recover it in a collapsed form (in the fine case). In our work, we consider the stronger notion of a ‚ÄúFourier subspace extraction‚Äù, an operation that measures the subspace and coherently recovers the encoded state. Duality. We show that the implementations of representations and the implementations of their Fourier subspace extractions are essentially computationally dual to each other. Theorem 1.2 (Duality, informal). Let Gùê∫{G}italic_G be a group131313 Technically, we do need some constraints on the group. We need it to have efficient implementations of a quantum Fourier transform and of its irreducible representations. Note however, that this is a very wide class of groups, and includes, at the very least, all Abelian groups, as well as many important non-Abelian groups. Moreover, every fixed-size group is technically efficient (whether Abelian or not), so this condition is important only for some families of groups whose sizes grow exponentially. and let ‚Ñ±:G‚ÜíU‚Å¢(‚Ñã):‚Ñ±‚Üíùê∫ùëà‚Ñã\mathcal{F}:{G}\to U(\mathcal{H})caligraphic_F : italic_G ‚Üí italic_U ( caligraphic_H ) be a representation of Gùê∫{G}italic_G. Then the following are equivalent: ‚Ä¢ ‚Ñ±‚Ñ±\mathcal{F}caligraphic_F has an efficient implementation, i.e. |g‚ü©‚äó|œà‚ü©‚Ü¶|g‚ü©‚äó‚Ñ±‚Å¢(g)‚Å¢|œà‚ü©maps-totensor-productketùëîketùúìtensor-productketùëî‚Ñ±ùëîketùúì\ket{g}\otimes\ket{\psi}\mapsto\ket{g}\otimes\mathcal{F}(g)\ket{\psi}| start_ARG italic_g end_ARG ‚ü© ‚äó | start_ARG italic_œà end_ARG ‚ü© ‚Ü¶ | start_ARG italic_g end_ARG ‚ü© ‚äó caligraphic_F ( italic_g ) | start_ARG italic_œà end_ARG ‚ü©. ‚Ä¢ ‚Ñ±‚Ñ±\mathcal{F}caligraphic_F has an efficient Fourier subspace extraction, i.e. |œài,jŒª‚ü©‚Ü¶|œïiŒª‚ü©‚Å¢|Œª,j‚ü©maps-toketsubscriptsuperscriptùúìùúÜùëñùëóketsuperscriptsubscriptitalic-œïùëñùúÜketùúÜùëó\ket{\psi^{\lambda}_{i,j}}\mapsto\ket{\phi_{i}^{\lambda}}\ket{\lambda,j}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_ARG ‚ü© ‚Ü¶ | start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª , italic_j end_ARG ‚ü©. Further Discussion of Fourier Subspace Extraction. In the above discussion, we have glossed over the possibility of degeneracy, in which the representation acts identically on several different invariant subspaces ùñ∂1Œª,ùñ∂2Œª,‚Ä¶,ùñ∂mŒªsubscriptsuperscriptùñ∂ùúÜ1subscriptsuperscriptùñ∂ùúÜ2‚Ä¶subscriptsuperscriptùñ∂ùúÜùëö\mathsf{W}^{\lambda}_{1},\mathsf{W}^{\lambda}_{2},\dots,\mathsf{W}^{\lambda}_{m}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT. Such subspaces are degenerate in the sense that a course Fourier measurement produces the same outcome, ŒªùúÜ\lambdaitalic_Œª, on all of them. Thus we have an additional index, iùëñiitalic_i, that runs over this multiplicity of ŒªùúÜ\lambdaitalic_Œª. We write a Fourier subspace extraction as an isometry |œài,jŒª‚ü©‚Ü¶|œïiŒª‚ü©‚Å¢|Œª,j‚ü©maps-toketsubscriptsuperscriptùúìùúÜùëñùëóketsuperscriptsubscriptitalic-œïùëñùúÜketùúÜùëó\ket{\psi^{\lambda}_{i,j}}\mapsto\ket{\phi_{i}^{\lambda}}\ket{\lambda,j}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_ARG ‚ü© ‚Ü¶ | start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª , italic_j end_ARG ‚ü©, where for each ŒªùúÜ\lambdaitalic_Œª and iùëñiitalic_i, the states {|œài,jŒª‚ü©}jsubscriptketsubscriptsuperscriptùúìùúÜùëñùëóùëó\{\ket{\psi^{\lambda}_{i,j}}\}_{j}{ | start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_ARG ‚ü© } start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT are a basis for the subspace ùñ∂iŒªsubscriptsuperscriptùñ∂ùúÜùëñ\mathsf{W}^{\lambda}_{i}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, and the state |œïiŒª‚ü©ketsubscriptsuperscriptitalic-œïùúÜùëñ\ket{\phi^{\lambda}_{i}}| start_ARG italic_œï start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ‚ü© is an arbitrary ‚Äújunk‚Äù state that is left behind after measuring ŒªùúÜ\lambdaitalic_Œª and extracting jùëójitalic_j. In order for it to be an extraction of jùëójitalic_j, rather than a measurement of jùëójitalic_j, it is crucial that this leftover state has no dependence on jùëójitalic_j. Consider a superposition ‚àëj‚àà[dim(ùñ∂iŒª)]Œ±j‚Å¢|œài,jŒª‚ü©subscriptùëódelimited-[]dimensionsubscriptsuperscriptùñ∂ùúÜùëñsubscriptùõºùëóketsubscriptsuperscriptùúìùúÜùëñùëó\sum_{j\in[\dim(\mathsf{W}^{\lambda}_{i})]}\alpha_{j}\ket{\psi^{\lambda}_{i,j}}‚àë start_POSTSUBSCRIPT italic_j ‚àà [ roman_dim ( sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ] end_POSTSUBSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_ARG ‚ü© over the subspace ùñ∂iŒªsubscriptsuperscriptùñ∂ùúÜùëñ\mathsf{W}^{\lambda}_{i}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. Performing this isometry yields ‚àëjŒ±j‚Å¢|œïiŒª‚ü©‚Å¢|Œª,j‚ü©=|œïiŒª‚ü©‚Å¢|Œª‚ü©‚äó‚àëjŒ±j‚Å¢|j‚ü©subscriptùëósubscriptùõºùëóketsuperscriptsubscriptitalic-œïùëñùúÜketùúÜùëótensor-productketsuperscriptsubscriptitalic-œïùëñùúÜketùúÜsubscriptùëósubscriptùõºùëóketùëó\sum_{j}\alpha_{j}\ket{\phi_{i}^{\lambda}}\ket{\lambda,j}=\ket{\phi_{i}^{% \lambda}}\ket{\lambda}\otimes\sum_{j}\alpha_{j}\ket{j}‚àë start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª , italic_j end_ARG ‚ü© = | start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª end_ARG ‚ü© ‚äó ‚àë start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_ARG italic_j end_ARG ‚ü©, which extracts the original superposition into a quantum state on the last register with those exact amplitudes. If the leftover junk state had depended on jùëójitalic_j, for instance if we instead had |œài,jŒª‚ü©‚Ü¶|œïi,jŒª‚ü©‚Å¢|Œª,j‚ü©maps-toketsubscriptsuperscriptùúìùúÜùëñùëóketsuperscriptsubscriptitalic-œïùëñùëóùúÜketùúÜùëó\ket{\psi^{\lambda}_{i,j}}\mapsto\ket{\phi_{i,j}^{\lambda}}\ket{\lambda,j}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT end_ARG ‚ü© ‚Ü¶ | start_ARG italic_œï start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª , italic_j end_ARG ‚ü©, then this would not extract the state properly. We would instead get ‚àëjŒ±j‚Å¢|œïi,jŒª‚ü©‚Å¢|Œª,j‚ü©subscriptùëósubscriptùõºùëóketsuperscriptsubscriptitalic-œïùëñùëóùúÜketùúÜùëó\sum_{j}\alpha_{j}\ket{\phi_{i,j}^{\lambda}}\ket{\lambda,j}‚àë start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_ARG italic_œï start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª , italic_j end_ARG ‚ü©, where the last register is still entangled with the rest of the state, and thus has not been extracted. This is the difference between a measurement of jùëójitalic_j and an extraction of jùëójitalic_j. We observe that since these leftover junk states |œïiŒª‚ü©ketsuperscriptsubscriptitalic-œïùëñùúÜ\ket{\phi_{i}^{\lambda}}| start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© are independent of jùëójitalic_j‚Äîthat is, they do not depend on which state we started from within the subspace ùñ∂iŒªsubscriptsuperscriptùñ∂ùúÜùëñ\mathsf{W}^{\lambda}_{i}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT‚Äîwe can see that these states are instead characteristic of the subspace ùñ∂iŒªsubscriptsuperscriptùñ∂ùúÜùëñ\mathsf{W}^{\lambda}_{i}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT itself. That is, the Fourier subspace extraction collapses each subspace ùñ∂iŒªsubscriptsuperscriptùñ∂ùúÜùëñ\mathsf{W}^{\lambda}_{i}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to a single distinct quantum state |œïiŒª‚ü©ketsuperscriptsubscriptitalic-œïùëñùúÜ\ket{\phi_{i}^{\lambda}}| start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü©, which we therefore call the ‚Äúarchetype‚Äù states of these subspaces. Despite appearing to be just the ‚Äújunk‚Äù that is left behind during the Fourier subspace extraction, these archetype states are in fact quite useful. For instance, the existence of these archetype states allows us to use a swap test to distinguish whether two quantum states are in the same subspace or different subspaces. Consider two states |œài1,j1Œª‚ü©‚ààùñ∂i1ŒªketsubscriptsuperscriptùúìùúÜsubscriptùëñ1subscriptùëó1subscriptsuperscriptùñ∂ùúÜsubscriptùëñ1\ket{\psi^{\lambda}_{i_{1},j_{1}}}\in\mathsf{W}^{\lambda}_{i_{1}}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_j start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG ‚ü© ‚àà sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT and |œài2,j2Œª‚ü©‚ààùñ∂i2ŒªketsubscriptsuperscriptùúìùúÜsubscriptùëñ2subscriptùëó2subscriptsuperscriptùñ∂ùúÜsubscriptùëñ2\ket{\psi^{\lambda}_{i_{2},j_{2}}}\in\mathsf{W}^{\lambda}_{i_{2}}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_j start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_ARG ‚ü© ‚àà sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT that live in subspaces corresponding to the same ŒªùúÜ\lambdaitalic_Œª, but potentially different such subspaces (that is, ùñ∂i1Œªsubscriptsuperscriptùñ∂ùúÜsubscriptùëñ1\mathsf{W}^{\lambda}_{i_{1}}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT and ùñ∂i2Œªsubscriptsuperscriptùñ∂ùúÜsubscriptùëñ2\mathsf{W}^{\lambda}_{i_{2}}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT are potentially different), and suppose that we wanted to test whether they in fact belong to the same subspace (that is, if i1=i2subscriptùëñ1subscriptùëñ2i_{1}=i_{2}italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT). The ability to perform the representation does not in general allow us to measure iùëñiitalic_i. Intuitively, this is because both these states behave identically under the representation. A Fourier measurement/sampling of these states would give us only ŒªùúÜ\lambdaitalic_Œª, or both ŒªùúÜ\lambdaitalic_Œª and jùëójitalic_j, but not iùëñiitalic_i. So how can we test if they are in the same subspace? This is in general not possible from such a measurement. However, Fourier subspace extraction is more powerful than Fourier measurement and gives us this ability. Performing a Fourier subspace extraction on both states gives us |œïi1Œª‚ü©‚Å¢|Œª‚ü©‚Å¢|j1‚ü©ketsuperscriptsubscriptitalic-œïsubscriptùëñ1ùúÜketùúÜketsubscriptùëó1\ket{\phi_{i_{1}}^{\lambda}}\ket{\lambda}\ket{j_{1}}| start_ARG italic_œï start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª end_ARG ‚ü© | start_ARG italic_j start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ‚ü© for the first state and |œïi2Œª‚ü©‚Å¢|Œª‚ü©‚Å¢|j2‚ü©ketsuperscriptsubscriptitalic-œïsubscriptùëñ2ùúÜketùúÜketsubscriptùëó2\ket{\phi_{i_{2}}^{\lambda}}\ket{\lambda}\ket{j_{2}}| start_ARG italic_œï start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª end_ARG ‚ü© | start_ARG italic_j start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ‚ü© for the second state. Now we can ignore and discard the last register‚Äîthe one that indicates which state we had within each subspace‚Äîand perform a swap test only on the first register, that is between the archetype states that characterize the subspaces. This turns out to be a crucial tool in the security proof of our quantum lightning construction. The Special Case of Abelian Groups. Abelian groups have the special property that all of the (irreducible) invariant subspaces are one-dimensional. Since the ‚Äúquantum state‚Äù extracted by the Fourier subspace extraction in this case is one-dimensional, it is actually just a complex phase. We can see that the corresponding isometry simplifies to |œàiŒª‚ü©‚Ü¶|œïiŒª‚ü©‚Å¢|Œª‚ü©maps-toketsubscriptsuperscriptùúìùúÜùëñketsuperscriptsubscriptitalic-œïùëñùúÜketùúÜ\ket{\psi^{\lambda}_{i}}\mapsto\ket{\phi_{i}^{\lambda}}\ket{\lambda}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ‚ü© ‚Ü¶ | start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª end_ARG ‚ü©, where we have absorbed the phase into |œïiŒª‚ü©ketsuperscriptsubscriptitalic-œïùëñùúÜ\ket{\phi_{i}^{\lambda}}| start_ARG italic_œï start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü©. This is computationally equivalent to the isometry |œàiŒª‚ü©‚Ü¶|œàiŒª‚ü©‚Å¢|Œª‚ü©maps-toketsubscriptsuperscriptùúìùúÜùëñketsuperscriptsubscriptùúìùëñùúÜketùúÜ\ket{\psi^{\lambda}_{i}}\mapsto\ket{\psi_{i}^{\lambda}}\ket{\lambda}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ‚ü© ‚Ü¶ | start_ARG italic_œà start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª end_ARG ‚ü© (by copying ŒªùúÜ\lambdaitalic_Œª and uncomputing), which we can see is just the course Fourier measurement for the representation‚Äîthat is, a projective measurement onto the subspaces ùñ∂Œªsuperscriptùñ∂ùúÜ\mathsf{W}^{\lambda}sansserif_W start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT. We therefore get the following simplified duality for Abelian groups as a special case, a duality between the efficiency of implementing the representation and that of performing a Fourier measurement,141414 Note that because representations of Abelian groups have only one-dimensional representations, there is no distinction between the course/weak and the fine/strong versions of Fourier measurement/sampling. Thus, we refer to it as simply Fourier measurement. a projective measurement on the subspaces spanned by its invariant states.151515 The invariant subspaces are one-dimensional, and are thus individual quantum states. Corollary 1.3 (Duality for Abelian Groups, informal). Let Gùê∫{G}italic_G be an Abelian group and let ‚Ñ±:G‚ÜíU‚Å¢(‚Ñã):‚Ñ±‚Üíùê∫ùëà‚Ñã\mathcal{F}:{G}\to U(\mathcal{H})caligraphic_F : italic_G ‚Üí italic_U ( caligraphic_H ) be a representation of Gùê∫{G}italic_G. Then the following are equivalent: ‚Ä¢ ‚Ñ±‚Ñ±\mathcal{F}caligraphic_F has an efficient implementation, i.e. |g‚ü©‚äó|œà‚ü©‚Ü¶|g‚ü©‚äó‚Ñ±‚Å¢(g)‚Å¢|œà‚ü©maps-totensor-productketùëîketùúìtensor-productketùëî‚Ñ±ùëîketùúì\ket{g}\otimes\ket{\psi}\mapsto\ket{g}\otimes\mathcal{F}(g)\ket{\psi}| start_ARG italic_g end_ARG ‚ü© ‚äó | start_ARG italic_œà end_ARG ‚ü© ‚Ü¶ | start_ARG italic_g end_ARG ‚ü© ‚äó caligraphic_F ( italic_g ) | start_ARG italic_œà end_ARG ‚ü©. ‚Ä¢ ‚Ñ±‚Ñ±\mathcal{F}caligraphic_F has an efficient Fourier measurement, i.e. |œàiŒª‚ü©‚Ü¶|œàiŒª‚ü©‚Å¢|Œª‚ü©maps-toketsubscriptsuperscriptùúìùúÜùëñketsuperscriptsubscriptùúìùëñùúÜketùúÜ\ket{\psi^{\lambda}_{i}}\mapsto\ket{\psi_{i}^{\lambda}}\ket{\lambda}| start_ARG italic_œà start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ‚ü© ‚Ü¶ | start_ARG italic_œà start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Œª end_POSTSUPERSCRIPT end_ARG ‚ü© | start_ARG italic_Œª end_ARG ‚ü©. 1.3 Related Work Quantum Money, Lightning, etc. There have been several attempts at constructing public-key quantum money [Aar09, FGH+12, AC12, Zha21, KSS22, AGKZ20, KLS22, LMZ23, Zha24]. Unfortunately, a number of them later turned out to be broken [LAF+09, CPDDF+19, Rob21, LMZ23]. In order to gain confidence in constructions, it is therefore important to give security proofs under computational assumptions that have received significant scrutiny from the cryptographic community. Here, the best we currently have are: ‚Ä¢ Quantum money from hidden subspaces [AC12], which was proved secure assuming quantum-resistant indistinguishability obfuscation (iO) in [Zha21]. Unfortunately, while candidates for quantum-resistant iO are known, their status is still very much open. This scheme also only achieves quantum money, but not quantum lightning. ‚Ä¢ Quantum money from random walks [FGH+12, LMZ23], which was shown to be secure under strong quantum ‚Äúknowledge‚Äù assumptions. Such assumptions are not ‚Äúfalsifiable‚Äù, and there is some doubt about the plausibility of such assumptions [Zha24]. ‚Ä¢ Quantum money from Abelian group actions [Zha24], which is proven secure under an assumption plus in an idealized model of group actions as a black box. We provide a scheme whose quantum lightning security we prove in the plain model (i.e. without making idealized model assumptions) from a plausible and falsifiable computational assumption. We hope that our work motivates further study of the cryptographic uses of non-Abelian group actions, and in particular, of the hardness of preactions. Comparison to the Duality of Aaronson, Atia, and Susskind. [AAS20] show that there is a duality between, on the one hand, swapping between two orthogonal states, and on the other hand, measuring the positive and negative superpositions of the two states. As a representation, this ‚Äúswapping‚Äù operation, together with the identity, is a representation of ‚Ñ§2subscript‚Ñ§2{\mathbb{Z}}_{2}blackboard_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. The invariant subspaces of this representation are the positive and negative superpositions, with eigenvalues +11+1+ 1 and ‚àí11-1- 1. Our duality theorem precisely yields the duality theorem from [AAS20] as a special case when applied to ‚Ñ§2subscript‚Ñ§2{\mathbb{Z}}_{2}blackboard_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and recovers the same circuits, showing that our results are a proper generalization. Theorem 1.2 extends far beyond ‚Ñ§2subscript‚Ñ§2{\mathbb{Z}}_{2}blackboard_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and even beyond Abelian groups, to many of the non-Abelian groups that are important for cryptography. We expect our duality theorem to be applicable to many more settings in quantum cryptography and complexity theory. Our applications to building quantum money, lightning, and fire are just a few demonstrations of the usefulness of our theorem and techniques, and demonstrate the usefulness of considering this quantum duality in its full non-Abelian generalization."
https://arxiv.org/html/2411.00311v1,Client-Customized Adaptation for Parameter-Efficient Federated Learning,"Despite the versatility of pre-trained language models (PLMs) across domains, their large memory footprints pose significant challenges in federated learning (FL), where the training model has to be distributed between a server and clients. One potential solution to bypass such constraints might be the use of parameter-efficient fine-tuning (PEFT) in the context of FL. However, we have observed that typical PEFT tends to severely suffer from heterogeneity among clients in FL scenarios, resulting in unstable and slow convergence. In this paper, we propose Client-Customized Adaptation (C2A), a novel hypernetwork-based FL framework that generates client-specific adapters by conditioning the client information. With the effectiveness of the hypernetworks in generating customized weights through learning to adopt the different characteristics of inputs, C2A can maximize the utility of shared model parameters while minimizing the divergence caused by client heterogeneity. To verify the efficacy of C2A, we perform extensive evaluations on FL scenarios involving heterogeneity in label and language distributions. Comprehensive evaluation results clearly support the superiority of C2A in terms of both efficiency and effectiveness in FL scenarios111Our code is available at https://github.com/yeachan-kr/c2a.","The advent of large-scale pre-trained language models (PLMs) for natural language processing (NLP) has led to exceptional performance across a broad spectrum of domains. However, the high memory requirements for PLMs impede their applicability to resource-constrained environments. These challenges are particularly evident in federated learning (FL), where model weights are transmitted between the server and clients to preserve data privacy Koneƒçn√Ω et al. (2016); McMahan et al. (2017). While recent FL studies have expanded the application of PLMs in various tasks, such as text classification Zhu et al. (2020); Qin et al. (2021); Weller et al. (2022), language modeling Chen et al. (2019), and question answering Chen et al. (2021), communicating the training model among clients requires huge computational resources and bandwidth, presenting a significant challenge in terms of practicality. Parameter-efficient fine-tuning (PEFT) approach is thereby a promising strategy for reducing communication costs in FL. Through tuning only a small fraction of parameters, such as adapter-based tuning Houlsby et al. (2019); Hu et al. (2022); Mahabadi et al. (2021a), bias tuning Zaken et al. (2022), and prompt-tuning Lester et al. (2021), PEFT approaches significantly enhance the memory efficiency in centralized scenarios. However, the feasibility of PEFT in decentralized scenarios has not been well explored. Figure 1: Conceptual illustration of the existing PEFT modules (ùíúùíú\mathcal{A}caligraphic_A) and the client-customized adaptation (‚Ñã‚Ñã\mathcal{H}caligraphic_H). The proposed method learns to generate the client-customized PEFT modules rather than fitting a single global module to all clients. Hence, we investigate the applicability of typical PEFT approaches in FL scenarios. Specifically, we measure the performance and client drifts Karimireddy et al. (2020); Li et al. (2021) of PEFT approaches in FL. Our discoveries are as follows: (1) typical PEFT approaches show large performance degradation in FL scenarios as the degree of non-IID increases; (2) these approaches usually suffer from large client drifts in non-IID scenarios, resulting in slow convergence and detrimental model performance. The above observations reveal that adopting PEFT in FL is not trivial, and posing the necessity to address large client drift. To overcome the identified limitations, we propose a novel hypernetwork-based FL framework, Client-Customized Adaptation (C2A), that leverages the information of different data distributions on clients. Our key idea is to generate the adapter parameters tailored to each client via hypernetworks by taking the information of client data distribution, rather than naively fitting a single global adapter to all heterogeneous data distributions (Figure 1). By learning to adopt the different data distributions to generate adapters for each client, C2A enables robust training for various non-IID conditions while sharing knowledge among clients. Moreover, in order to manage the large number of parameters associated with hypernetworks, we introduce factorized hypernetworks, thereby significantly reducing the number of parameters without sacrificing the performance. We carefully design the experimental setting to verify the efficacy of C2A on realistic FL scenarios, considering on both label and language heterogeneous. The experimental results show clearly that C2A can be robust to the heterogeneity of clients, thereby leading to the state-of-the-art results on diverse non-IID setups. In addition, our framework shows a significant enhancement in training efficiency across a range of downstream tasks. Finally, we demonstrate that our C2A successfully mitigates the large client drifts among local clients in non-IID scenarios. A summary of our main contributions is as follows: ‚Ä¢ We investigate the effectiveness of PEFT among various FL scenarios. To the best of our knowledge, our work is one of the few researches for adapting PEFT in FL. ‚Ä¢ We propose Client-Customized Adaptation (C2A), a novel hypernetwork-based framework that strengthens the robustness of adapter concerning FL heterogeneity. ‚Ä¢ We demonstrate that C2A works quite well on various non-IID scenarios while preserving the benefits of efficiency in PEFT."
https://arxiv.org/html/2411.00192v1,Optical Lens Attack on Monocular Depth Estimation for Autonomous Driving,"Monocular Depth Estimation (MDE) is a pivotal component of vision-based Autonomous Driving (AD) systems, enabling vehicles to estimate the depth of surrounding objects using a single camera image. This estimation guides essential driving decisions, such as braking before an obstacle or changing lanes to avoid collisions. In this paper, we explore vulnerabilities of MDE algorithms in AD systems, presenting L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, a novel physical attack that strategically places optical lenses on the camera of an autonomous vehicle to manipulate the perceived object depths. L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k encompasses two attack formats: concave lens attack and convex lens attack, each utilizing different optical lenses to induce false depth perception. We first develop a mathematical model that outlines the parameters of the attack, followed by simulations and real-world evaluations to assess its efficacy on state-of-the-art MDE models. Additionally, we adopt an attack optimization method to further enhance the attack success rate by optimizing the attack focal length. To better evaluate the implications of L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k on AD, we conduct comprehensive end-to-end system simulations using the CARLA platform. The results reveal that L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k can significantly disrupt the depth estimation processes in AD systems, posing a serious threat to their reliability and safety. Finally, we discuss some potential defense methods to mitigate the effects of the proposed attack.","Autonomous Driving (AD) systems rely on their perception modules to track and regulate the proximity to surrounding obstacles, a vital capability for ensuring the system‚Äôs safety and dependable performance. Various methods exist to accomplish this task, such as direct measurement techniques using radar or Lidar (Piotrowsky et al., 2019; Li and Ibanez-Guzman, 2020), or employing stereoscopic 3D imaging to create a dense depth map of the environment (Mayer et al., 2016; Chang and Chen, 2018; Tay et al., 2019). Cameras are among the most critical sensors in AD systems, as demonstrated by their use in vehicles from companies like Tesla (Tesla, 2022a), Uber (Uber, 2022), and Waymo (Waymo, 2022). These cameras leverage computer vision technology for various AD tasks (Zhou et al., 2023). Researchers have made significant advancements, allowing monocular cameras to estimate scene depth (Godard et al., 2019; Wong et al., 2020; Casser et al., 2019). Utilizing monocular cameras can reduce the need for additional sensors, offering benefits in terms of space, weight, and cost efficiency. Although estimating depth with a single camera poses challenges, recent deep-learning approaches have achieved performance levels close to those of stereo 3D depth estimation techniques. Previous security studies have introduced various attack techniques targeting cameras to disrupt different AD tasks, including object detection and classification (Eykholt et al., 2018; Man et al., 2020; Nassi et al., 2020), lane detection (Jing et al., 2021; Sato et al., 2021), traffic light detection (Yan et al., 2022), and vision-based depth estimation (Zhou et al., 2022; Zhang et al., 2020; Wong et al., 2020). Additionally, the security of vision-based depth estimation tasks has also been investigated. To compromise 3D stereo depth estimation, Zhou et al. (Zhou et al., 2022) propose a long-range stereo depth estimation attack that injects fake obstacle depth by projecting pure light from two complementary light sources. For monocular depth estimation (MDE) algorithms, Zhang et al. (Zhang et al., 2020) and Wong et al. (Wong et al., 2020) present white-box attacks that use imperceptible additive adversarial perturbations to alter the depth estimation results in the digital world, while a black-box attack is introduced by Daimo et al. in (Daimo et al., 2021). However, these invisible perturbations are ineffective in the physical world due to the impacts of environmental variables. Therefore, Yamanaka et al. (Yamanaka et al., 2020) and Cheng et al. (Cheng et al., 2022) create visible adversarial patches. These patches deceive depth estimation algorithms into estimating a false depth for the regions where the patterns are placed in the physical world. However, human drivers can easily detect the patches. Moreover, patches are scene-sensitive, and may not work well in dynamic environments. As opposed to existing physical attacks, we propose a universal black-box attack, L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, that enables a new type of robust physical attack using optical lenses. L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k leverages a fundamental vulnerability of MDE systems, where even a slight change in the perceived object size in an image can lead to a corresponding shift in estimated depth. Our attack utilizes the optical lens to change the formed object size on the image sensor. Specifically, by attaching a tiny attack lens in the near front (e.g., 5‚Å¢c‚Å¢m5ùëêùëö5cm5 italic_c italic_m) of the car camera, the sensed object size will be altered, which affects the depth estimation results. There are four major challenges in realizing L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k. (i) ‚ÄúHow to design the attacks that can induce various false depth predictions?‚Äù (ii) ‚ÄúHow to mathematically calculate the induced depth and gain control over depth estimation?‚Äù (iii) ‚ÄúHow to configure the optimal attack parameters to accurately manipulate the depth?‚Äù And (iv) ‚ÄúHow to verify the effectiveness of the L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k in both AI and system level?‚Äù To address the first challenge, we design L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k in two attack formats: concave lens attack and convex lens attack, which can either increase or decrease the object depth. To solve the second challenge, we mathematically model our attack using different lenses in various attack scenarios. To improve the depth manipulation precision (the third challenge), we first devise our attack in the form of targeted and untargeted attacks. Then, we adopt an optimization method to formulate the attacking process and output the optimized parameters for enhancing the attack performance. To address the last challenge, we verify the efficacy of L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k through both simulation and real-world experiments with a prototype autonomous vehicle (AV) against three state-of-the-art MDE algorithms. We also simulate our attack at an end-to-end system level using the CARLA simulator to demonstrate its potential impact in the physical world. Our experimental results demonstrate that our attack remains effective across a wide range of optical lens parameter configurations. When the optical lens is within the field of view, the attack optimization further enhances the accuracy of manipulated depths. We set up a demo website111https://lensattack.github.io/. to show our attack results, attack simulations, and physical attack video demos. The main contributions of this work are summarized as follows: ‚Ä¢ We propose a novel universal physical attack, L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, on MDE algorithms that utilizes optical lenses. By investigating the vulnerability of the MDE, we propose the concave and convex lens attacks and mathematically model them in different attack scenarios. ‚Ä¢ We evaluate the attacks on a smartphone camera and an AV in real-world experiments to demonstrate that our attack is effective with various optical lens parameter settings. The concave lens attack results in an average error rate of 11.48% in estimated depths, whereas the convex lens attack leads to a 29.84% average error rate. ‚Ä¢ We present an attack optimization method that further improves the attack success rate by considering the factor of blurriness. The targeted average attack error rate is reduced by 6.26%, while the untargeted average attack distortion rate is increased by 11.58%. ‚Ä¢ We perform an end-to-end system-level simulation in the CARLA simulator and demonstrate the real-world ramifications of the proposed attack. This work extends our preliminary work (Zhou et al., 2024) with significant enhancements in Sections 4, 5 and 6. To further improve the effectiveness of our attack, we introduce an optimization method to optimize the focal length used in both targeted and untargeted attacks with the goal of improving the attack success rate (Section 4.3). We then evaluate the performance of the optimization method in simulation and demonstrate its improvements (Section 5.4). Additionally, to validate the feasibility of L‚Å¢e‚Å¢n‚Å¢s‚Å¢A‚Å¢t‚Å¢t‚Å¢a‚Å¢c‚Å¢kùêøùëíùëõùë†ùê¥ùë°ùë°ùëéùëêùëòLensAttackitalic_L italic_e italic_n italic_s italic_A italic_t italic_t italic_a italic_c italic_k, we conduct a comprehensive end-to-end system-level simulation using the CARLA simulator (Section 5.5). We also present more in-depth discussions and results on the potential defense methods in Section 6."
