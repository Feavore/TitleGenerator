URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04905v1,OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models,"Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an “open cookbook” for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.","Large Language Models (LLMs) have achieved significant success in various domains (Wang et al., 2023; Que et al., 2024; Liu et al., 2024a; c; Wu et al., 2024), particularly in code-related tasks, revolutionizing the current paradigm of software development (Qian et al., 2024; Wang et al., 2024). Code-specific LLMs have emerged as a critical area within LLM research, with tools such as ChatGPT, Copilot, and Cursor reshaping the workflows of developers. Despite this, the performance of open-source LLMs focused on code (Li et al., 2023; Tao et al., ; Lozhkov et al., 2024a; Zhang et al., 2024a) still falls short compared to state-of-the-art LLMs (Hui et al., 2024; Zhu et al., 2024), largely because these leading models keep their training datasets—an essential factor in LLM development—proprietary. This lack of transparency limits the broader research community’s ability to establish strong baselines and gain deeper insights into the workings of top-tier code LLMs. To remedy the gap, we set forth three primary goals by releasing OpenCoder and its development material: (1) Firstly, we aim to provide scholars with a meticulously curated and fully transparent strong baseline code LLM for research on mechanical interpretability and the data distribution of code LLMs. (2) Secondly, we intend to conduct in-depth investigations into the pretrain and instruction data curation pipeline for the development of stronger code LLMs. (3) Thirdly, by enabling a detailed review of the development of the models, we hope to unlock more diverse customized solutions based on transparent code LLM. Through OpenCoder, we strive to stimulate and accelerate the growth of the open-source code LLM community. Our comprehensive set of controlled experiments highlights key design choices for data curation for top-tier code LLMs in different training stages: (1) During the pretraining phase, the importance of data cleaning is highlighted (Zhou et al., 2024), emphasizing the removal of non-informative data such as pure hexadecimal code and excessively short code snippets that do not contribute to the learning process. (2) The impact of deduplication is significant, with file-level deduplication proving to be more effective than repository-level deduplication by maintaining data diversity and enhancing model performance on downstream tasks (Li et al., 2023). (3) The influence of GitHub stars is also examined, revealing that filtering data based on Github star count can possibly reduce data diversity and affect the overall data distribution, contributing to a suboptimal result (Allal et al., 2023). (4) In the annealing phase, the use of high-quality data is crucial for further enhancing the model’s capabilities, indicating that data quality is more important than quantity in the later stages of model training. (5) Finally, during the instruction tuning phase, a two-stage instruction tuning strategy is shown to be effective, allowing the model to acquire broad capabilities initially and then refine them with code-specific tasks, resulting in improved performance on both theoretical and practical coding tasks. These five key points underscore the importance of data quality, diversity, and targeted enhancement strategies in developing a high-performing code generation model like OpenCoder. This work introduces the OpenCoder, a completely open-source Code LLM, built on the transparent data process pipeline and reproducible dataset. As shown in Table 1, We provide the open cookbook to build a code LLM from scratch by providing the data cleaning pipeline, reproducible pretraining dataset, large-scale SFT Corpus, and intermediate checkpoints. OpenCoder, through its meticulous data processing and advanced training methods, has surpassed expectations by achieving top-tier results on multiple code LLM evaluation benchmarks. The introduction of the open cookbook of code LLM is designed to push forward the field of code intelligence studies and to encourage its broad use in the community of code intelligence. Table 1: The comparison of released resources between our OpenCoder with other popular open-sourced code LLMs. HumanEval scores are reported for the corresponding chat models. Models Data Processing Pipeline Reproducible Pretraining Dataset Large-scale SFT Dataset (>>>1M) Intermediate Checkpoints Training Tokens HumanEval Pass@1 Open Model Weights & Reproducible Datasets OpenCoder-8B ✓ ✓ ✓ ✓ 2.5T 83.5 StarCoder2-15B ✓ ✓ ✗ ✗ 4.1T 72.6 Crystal-7B ✗ ✓ ✗ ✓ 1.3T 34.1 Open Model Weights CodeLlama-7B ✗ ✗ ✗ ✗ 2.5T 34.8 CodeGemma-7B ✗ ✗ ✗ ✗ 6.5T 56.1 DS-Coder-V2-Lite ✗ ✗ ✗ ✗ 10.2T 81.1 Yi-Coder-9B ✗ ✗ ✗ ✗ 6.0T 85.4 Qwen2.5-Coder-7B ✗ ✗ ✗ ✗ 23.5T 88.4"
https://arxiv.org/html/2411.02863v1,LoopSCC: Towards Summarizing Multi-branch Loops within Determinate Cycles,"Analyzing programs with loops is a challenging task, suffering from potential issues such as indeterminate number of iterations and exponential growth of control flow complexity. Loop summarization, as a static analysis method for concrete semantic interpretation, receives increasing focuses in the field of loop program analysis. By analyzing and representing the regularity in loop control flow, it produces symbolic expressions semantically equivalent to the loop program, enhancing the accuracy and efficiency of loop analysis. However, current loop summarization methods are only suitable for single-branch loops or multi-branch loops with simple cycles, without supporting complex loops with irregular branch-to-branch transitions. In this paper, we proposed LoopSCC, a novel loop summarization technique, to achieve concrete semantic interpretation on complex loop control flow. LoopSCC first utilizes an inside-out transformation to convert the nested loop into non-nested one. Then, it analyzes the control flow at the granularity of single-loop-path and applies the strongly connected components (SCC for short) for contraction and simplification, resulting in the contracted single-loop-path graph (CSG for short). Based on the control flow information provided by the CSG, we can convert the loop summary into a combination of SCC summaries. When an SCC contains irregular branch-to-branch transitions, we propose to explore a convergent range to identify the determinate cycles of different execution paths, referred as oscillatory interval. According to the analysis of oscillatory interval, the loop summarization composed of both iteration conditions and execution operations can eventually be derived recursively. Extensive experiments compared to six state-of-the-art loop interpretation methods are conducted to evaluate the effectiveness of LoopSCC. From the results, LoopSCC outperforms comparative methods in both interpretation accuracy and application effectiveness. Especially, LoopSCC achieves a 100% interpretation accuracy on public common-used benchmark. In addition, a systematical study for loop properties on three large-scale programs illustrates that LoopSCC presents outstanding scalability for real-world loop programs. The LoopSCC and experimental data are available at https://anonymous.4open.science/r/LoopSCC-386F.","Dominant software engineering analysis techniques, such as symbolic execution (Baldoni et al., 2018) and model checking (Clarke, 1997), require simulating the execution of each reachable path within the target program. In this process, complex program structures, represented by loops, raise significant challenges to execution-based analysis techniques. For instance, when dealing with complex loop structures, both symbolic execution and model checking suffer from serious path explosion where an infinite number of paths derived from the loop need to be executed, resulting in program crashes and unreasonable analysis. To address this challenge, a straightforward method is iteration limit, which limits the number of loop iterations, simulating the loop as a finite path and executing it sequentially (Biere et al., 1999) (Healy et al., 1998). However, such method inevitably results in significant information loss, leading to serious biases in both execution and analysis. Building on this, Saxena et al. (Saxena et al., 2009) has proposed loop-dependent code summarization, leveraging efficient static analysis techniques to obtain the runtime properties (particularly the execution results) of loops, without actual loop execution. Such static loop analysis methods can be classified into two main categories based on the semantic interpretation of the original code structure: ❶ Abstract interpretation designs new program structures to approximate the target loop logic of original program, where the abstract semantics interpreted by the newly designed structures that is a superset of the original semantics, ensuring that all the reachable states of the original loops are covered by the abstract interpretation. ❷ Concrete interpretation designs a computable mathematical model to interpret the program semantics of target loop logic in an accurate way, making the target program logic semantically equivalent to the designed model. In comparison, abstract interpretation is simpler to design and implement, offering a variety of variants on either structures or logic. Consequently, current loop analysis efforts (Ball et al., 2001) (Kincaid et al., 2017) tend to be built on the abstract interpretation. Nevertheless, abstract interpretation fails to fully represent the semantics of the original loop structures, as it suffers from similar information loss or redundancy as the aforementioned iteration limit method, leading to incomplete program analysis. On the contrary, concrete interpretation achieves comprehensive summarization of loops. As stated by the Rice theorem (Rice, 1953) and the halting problem (Davis, 2013), the computation of concrete semantics is proved as an undecidable problem, meaning that only certain types of loop structures allow for concrete semantic interpretation. For instance, representative concrete interpretations (Saxena et al., 2009) (Godefroid and Luchaup, 2011) tend to concentrate on single-branch loops since they are less affected by the undecidable computation. For multi-branch loops that involve irregular jumps between loop blocks, the concrete interpretation becomes exceptionally challenging as the undecidable program execution. Aiming at this challenge, formal-method based efforts (Xie et al., 2016)(Xie et al., 2017)(Blicha et al., 2022) provide valuable attempts at concrete semantic interpretation for the multi-branch loops, where specialized path structures, such as path dependency automaton (PDA), are proposed to capture the execution dependency between the paths and transform irregular loop paths into parameterized periodic iterations. By analyzing the parameter expressions (such as iteration counters), they are able to interpret the periodic execution of the proposed path structures and further produce the semantic summarization of the complex multi-branch loops. However, such parameterized periodic summarization often fails to satisfy the requirements for parameter inductiveness, known as the inductiveness trap, which leads to significant uncertainty in the summarized iteration cycles, blocking the interpretive computation of loop execution. In this work, instead of the traditional parameterized periodic summarization, we propose a novel loop summarization with determinate cycles to explicitly interpret the logical semantics within multi-branch loops, and further build a practical analysis framework LoopSCC for precise, efficient and generalized analysis of program semantic. First, to facilitate the summarization, the LoopSCC converts the target loop into a canonical form with single input and output, using an existing Gaussian Elimination based method (Ashcroft and Manna, 1979) (Ammarguellat, 1992). Then, based on the transitions among the blocks within loops, we construct a SPath graph to represent the fine-grained control flow of the loops. By simplifying the nodes in the SPath graph at the granularity of strong connected components (SCC for short), we can further obtain a directed acyclic graph focused on the SCC, referred to as contracted SPath graph (CSG for short). For a target program with a complex loop structure, the execution will iterate repeatedly inside SCC and possibly exhibit a certain periodicity when iterating sufficiently. To extract such periodicity, we proposed oscillatory interval to represent the iterations of SCC into a piecewise function calculation within a limited value scope. To determine the oscillatory interval within the target loop execution, we have proposed an iterative search algorithms. After that, the LoopSCC utilizes function operations such as addition and subtraction to extract the periodicity in the oscillatory interval. In particular, LoopSCC uses the pigeonhole principle (Trybulec, 1990) to derive the periodicity of discrete values directly. Finally, the target loop can be summarized by computing the result of periodic function extracted from the oscillatory interval. We evaluate the effectiveness of LoopSCC from different perspectives through extensive experiments. Firstly, we evaluate the summarization precision of LoopSCC compared with state-of-the-art baselines on public datasets, where LoopSCC achieves a 100% summarization accuracy, outperforming all the baselines. Secondly, we performed program verification using the benchmark SV-COMP 2024. The results indicate that LoopSCC correctly verifies 86% of the test cases, outperforming the best competing tool VeriAbsL (Darke et al., 2023) by 10.3%. Thirdly, LoopSCC is integrated into typical program analysis tools to test the support of code analysis. The results demonstrate that LoopSCC significantly improves the analysis efficiency and coverage. Finally, we systematically investigated the feasibility of using the LoopSCC to summarize loops with non-memory-related operations in three large open-source programs: Bitcoin, musl, and Z3. The results indicate that 81.5% of the loops can be summarized using the LoopSCC, highlighting its outstanding scalability. In summary, this work makes the following contributions: • We proposed LoopSCC, a novel loop summarization framework based on strongly connected components, along with a dynamic programming-based interpretation algorithm to handle the implicit relationships within SPath conditions. • We proposed the finite oscillatory interval for the code execution within loop structures, and conducted an in-depth analysis of the periodic variation patterns of the oscillatory interval, along with a concrete interpretation and computation scheme for the execution outcomes. • We conducted extensive comparative experiments on public datasets against state-of-the-art loop summarization methods, and the results demonstrate that the proposed LoopSCC is not only theoretically sound and effective but also significantly enhances practical code analysis techniques."
https://arxiv.org/html/2411.02318v2,Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast,"Static verification is a powerful method for enhancing software quality, but it demands significant human labor and resources. This is particularly true of static verifiers that reason about heap manipulating programs using an ownership logic. LLMs have shown promise in a number of software engineering activities, including code generation, test generation, proof generation for theorem provers, and specification generation for static verifiers. However, prior work has not explored how well LLMs can perform specification generation for specifications based in an ownership logic, such as separation logic.To address this gap, this paper explores the effectiveness of large language models (LLMs), specifically OpenAI’s GPT models, in generating fully correct specifications based on separation logic for static verification of human-written programs in VeriFast. Our first experiment employed traditional prompt engineering and the second used Chain-of-Thought (CoT) Prompting to identify and address common errors generated across the GPT models. The results indicate that GPT models can successfully generate specifications for verifying heap manipulating code with VeriFast. Furthermore, while CoT prompting significantly reduces syntax errors generated by the GPT models, it does not greatly improve verification error rates compared to prompt engineering.","Auto-active (Hoare-logic styled (Hoare, 1969), static) verifiers, such as Viper (Müller et al., 2016), Verus (Lattuada et al., 2023), Dafny (Leino, 2010), Gillian (Fragoso Santos et al., 2020), and VeriFast (Jacobs et al., 2011), are powerful as they can prove the absence of large classes of bugs in code. Ideally, users of such tools need only specify the intended behavior of their code on the code itself (as pre- and postconditions), and the tool will automatically provide feedback on whether or not the code is provably correct with respect to this behavior. In reality, auto-active verifiers require many more auxiliary specifications (such as loop invariants, lemmas, folds, unfolds, etc.) to achieve this goal, burdening their users. In recent years, large language models (LLMs) have been effective in generating code (Chen et al., 2022; Sarsa et al., 2022), test-cases (Deng et al., 2023; Lemieux et al., 2023; Rao et al., 2023; Schäfer et al., 2023; Wang et al., 2024; Xia et al., 2024), and proofs in theorem provers (proof assistants) (Zheng et al., 2023; Yang et al., 2024; Jiang et al., 2021; Welleck and Saha, 2023; First et al., 2023). LLMs have also been shown to be effective for generating specifications supported by auto-active verifiers (Ma et al., 2024; Kamath et al., 2023; Misu et al., 2024; He et al., 2024; Mugnier et al., 2024). However, related work has not explored whether or not off-the-shelf LLMs can generate specifications based on a permissions logic, such as separation logic (Reynolds, 2002), that can be verified by auto-active verifiers such as VeriFast, Gillian, and Viper. Thanks to such specifications, these verifiers do well at verifying programs that manipulate the heap for both memory safety and functional properties. But, permissions logic based specifications (auxiliary and non-auxiliary) are particularly cumbersome to write, because they must specify the shape of the heap alongside functional constraints. This leads to specifications containing a number of predicates that hide heap details; and as a result, numerous lemmas, folds, unfolds, and special loop invariants that are used to connect the content of these predicates. While such specifications are difficult to reason about, they are written in a patterned way that may be amenable to generation via LLMs. Therefore, this paper evaluates how effective LLMs are at generating specifications that can be verified by VeriFast (Jacobs et al., 2011), which supports separation logic based verification of C and Java code. We specifically target OpenAI’s GPT models in this preliminary work, and employ two different prompt engineering techniques on the models. We develop input-output pairs to prompt the models with and use as ground truth for the models’ output, respectively. The input-output pairs are generated from a subset of 150 publicly available, statically verified examples on VeriFast’s Github. The GPT models’ output after prompting is inspected manually for correctness compared to the ground truth and results are recorded. Results indicate that GPT models can generate specifications for verification with VeriFast using traditional prompt engineering. When they fail to generate correct specifications, errors range from syntax to deeper verification errors. The second prompting approach based on Chain of Thought Prompting reduced syntax error rates significantly, but not verification error rates."
https://arxiv.org/html/2411.00618v1,Visualizing the Evaluation of Functional Programs for Debugging,"In this position paper, we present a prototype of a visualizer for functional programs. Such programs, whose evaluation model is the reduction of an expression to a value through repeated application of rewriting rules, and which tend to make little or no use of mutable state, are amenable to visualization in the same fashion as simple mathematical expressions, with which every schoolchild is familiar. We show how such visualizations may be produced for the strict functional language OCaml, by direct interpretation of the abstract syntax tree and appropriate pretty-printing. We describe (and begin to address) the challenges of presenting such program traces in limited space and of identifying their essential elements, so that our methods will one day be practical for more than toy programs. We consider the problems posed by the parts of modern functional programming which are not purely functional such as mutable state, input/output and exceptions. We describe initial work on the use of such visualizations to address the problem of program debugging, which is our ultimate aim.","When we do mathematics on paper, we write an expression or equation and, through a series of legal transformations, produce a simpler one that, we hope, tells us what we want to know. It is the same with functional programming, but the semantics define more closely the order in which the expression is evaluated, choosing each transformation by inspection of the shape of the expression. Of course, this is not quite how the compiled code runs, but it is the mental model. So it is natural, when teaching students functional programming, to proceed by analogy to the mathematical model in which they are already well-practiced. Since computer languages must be more formal in their choice of evaluation order, we tend to underline the sub-expression being evaluated at each step. Given the function f for doubling a number, we might have: f 3 = 1 + 2 * 3 ⟹⟹\displaystyle\Longrightarrow⟹ 6 = 1 + 2 * 3 ⟹⟹\displaystyle\Longrightarrow⟹ 6 = 1 + 6 ⟹⟹\displaystyle\Longrightarrow⟹ 6 = 7 ⟹⟹\displaystyle\Longrightarrow⟹ false There are differences from mathematics, of course: the last step may be rather confusing to the schoolchild (our equals sign does not denote an equation as such, but a comparison operator). Such visualizations are longwinded to write on paper, for all but the least substantial programs. We should like to generate them by computer. In order to provide a tool useful to both learners and the everyday programmer, we will begin with a subset of a real language (rather than building our own toy one), extend it to work for the whole language, and integrate it properly with the toolchain as a first class citizen. We are writing, in essence, a step-by-step interpreter. What is the relevance to debugging? The dream of debugging is this: having observed a misbehaviour caused by a bug, we assemble all relevant information, both about the program source and the full trace of the program’s operation, and, describing ourselves concisely to the computer, we narrow the circumstances down, again and again reducing the search space, until we have the bug in our grasp, and understand it fully. The fix is then often easy. But we are very far from this dream, even today. In his introduction to a Special Issue of Communications of the ACM in 1997 “The Debugging Scandal and What to Do About It” [5], Lieberman writes: “Today’s commercial programming environments provide debugging tools that are little better than the tools that came with programming environments thirty years ago. It is a sad commentary on the state of the art that many programmers name “inserting print statements” as their debugging technique of choice.” We claim this is still largely true, twenty years later."
https://arxiv.org/html/2411.00589v1,Early Announcement: Parametricity for GADTs,"Relational parametricity was first introduced by Reynolds for System F. Although System F provides a strong model for the type systems at the core of modern functional programming languages, it lacks features of daily programming practice such as complex data types. In order to reason parametrically about such objects, Reynolds’ seminal ideas need to be generalized to extensions of System F. Here, we explore such a generalization for the extension of System F by Generalized Algebraic Data Types (GADTs) as found in Haskell. Although GADTs generalize Algebraic Data Types (ADTs) — i.e., simple recursive types such as lists, trees, etc. — we show that naively extending the parametric treatment of these recursive types is not enough to tackle GADTs. We propose a tentative workaround for this issue, borrowing ideas from the categorical semantics of GADTs known as (functorial) completion. We discuss some applications, as well as some limitations, of this solution.","Relational parametricity [8] is a key technique for reasoning about programs in strongly typed languages. It can be used to enforce invariants guaranteeing strong properties of programs, programming languages, and programming language implementations supporting parametric polymorphism. A polymorphic program is a program that can be applied to arguments and return results of different types; a parametric polymorphic program is a program that is not only polymorphic over all types, but is also defined by the same type-uniform algorithm regardless of the concrete type at which it is applied. Since parametric polymorphic programs cannot perform type-specific operations, the computational behaviors they can exhibit are actually quite constrained. Parametricity was originally put forth by Reynolds [8] for System F [3, 7], the formal calculus at the core of all polymorphic functional languages. It was later popularized as Wadler’s “theorems for free” [10], so-called because it allows the deduction of properties of programs in such languages solely from their types, i.e., with no knowledge whatsoever of the text of the programs involved. However, to get interesting free theorems, Wadler actually treats System F extended with built-in lists. Indeed, most of the free theorems in [10] are essentially naturality properties for polymorphic list-processing functions. It is easy to extend the techniques developed in [10] for handling lists to non-list algebraic data types (ADTs). Parametricity for such types can then be used to derive not just naturality (i.e., commutativity) properties, but also results — such as proofs of type inhabitance and correctness of the program optimization known as short cut fusion [2] — that go beyond simple naturality. In his original formulation, Reynolds gives each type expression of System F a relational interpretation defined inductively. Each type expression ΦΦΦroman_Φ with type variables α⁢₁,α⁢₂,⋯,α⁢ₙ𝛼₁𝛼₂⋯𝛼italic-ₙα₁,α₂,⋯,αₙitalic_α ₁ , italic_α ₂ , ⋯ , italic_α italic_ₙ thus gives, for each tuple R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG of relations R⁢ᵢ𝑅italic-ᵢRᵢitalic_R italic_ᵢ between types A⁢ᵢ𝐴italic-ᵢAᵢitalic_A italic_ᵢ and B⁢ᵢ𝐵italic-ᵢBᵢitalic_B italic_ᵢ, a relation Φ^⁢R¯^Φ¯𝑅\widehat{Φ}\,\overline{R}over^ start_ARG roman_Φ end_ARG over¯ start_ARG italic_R end_ARG between the type Φ⁢[A¯/α¯]Φdelimited-[]¯𝐴¯𝛼Φ[\overline{A}/\overline{\alpha}]roman_Φ [ over¯ start_ARG italic_A end_ARG / over¯ start_ARG italic_α end_ARG ] and Φ⁢[B¯/α¯]Φdelimited-[]¯𝐵¯𝛼Φ[\overline{B}/\overline{\alpha}]roman_Φ [ over¯ start_ARG italic_B end_ARG / over¯ start_ARG italic_α end_ARG ]. To capture the intended type-uniformity of System F’s polymorphic expressions, these relational interpretations are defined in such a way that every function f⁢:⁢∀⁢α¯.Φ⁢→⁢Ψformulae-sequence𝑓:∀¯𝛼Φ→Ψf\,\mathord{\mathchar 58\relax}\,∀\overline{\alpha}.Φ\textrightarrow Ψitalic_f : ∀ over¯ start_ARG italic_α end_ARG . roman_Φ → roman_Ψ, where ΦΦΦroman_Φ and ΨΨΨroman_Ψ are two type expressions in the same type variables α¯¯𝛼\overline{\alpha}over¯ start_ARG italic_α end_ARG, is parametric in the following sense: for each tuple of relations R¯¯𝑅\overline{R}over¯ start_ARG italic_R end_ARG, the pairs related by Φ^⁢R¯^Φ¯𝑅\widehat{Φ}\,\overline{R}over^ start_ARG roman_Φ end_ARG over¯ start_ARG italic_R end_ARG are sent by f𝑓fitalic_f to pairs related by Ψ^⁢R¯^Ψ¯𝑅\widehat{Ψ}\,\overline{R}over^ start_ARG roman_Ψ end_ARG over¯ start_ARG italic_R end_ARG. As mentioned above, better approximations of realistic programming languages result from adding built-in data types to System F. Each such added data type induces a type constructor, and this type constructor must also be given a relational interpretation. Wadler [10] considers the case of lists, which we review in detail in Section 2. To add a new inductive data type constructor T𝑇Titalic_T to an ambient parametric language in such a way that parametricity is preserved, the method is always the same: Define its relational interpretation as a (dependent) inductive family T^^𝑇\widehat{T}over^ start_ARG italic_T end_ARG with one data constructor c^^𝑐\widehat{c}over^ start_ARG italic_c end_ARG for each data constructor c𝑐citalic_c of T𝑇Titalic_T expressing precisely that c𝑐citalic_c is a parametric polymorphic function. The data constructors of such a data type’s relational interpretation thus make formal the intuitive type-uniformity required of its data constructors by the grammars of languages such as Haskell. The relational interpretation T^^𝑇\widehat{T}over^ start_ARG italic_T end_ARG captures the intuition that, if we regard data types as containers, then two data structures of (two instances of) T𝑇Titalic_T are related by T^⁢R^𝑇𝑅\widehat{T}\,Rover^ start_ARG italic_T end_ARG italic_R exactly when the data they store are related by R𝑅Ritalic_R. This intuition also requires that T^^𝑇\widehat{T}over^ start_ARG italic_T end_ARG preserves inclusion, i.e., that T^⁢R⊆T^⁢S^𝑇𝑅^𝑇𝑆\widehat{T}\,R\subseteq\widehat{T}\,Sover^ start_ARG italic_T end_ARG italic_R ⊆ over^ start_ARG italic_T end_ARG italic_S whenever R⊆S𝑅𝑆R\subseteq Sitalic_R ⊆ italic_S. Indeed, if two data structures are related by T^⁢R^𝑇𝑅\widehat{T}\,Rover^ start_ARG italic_T end_ARG italic_R, then the data they store are related by R𝑅Ritalic_R, and thus by S𝑆Sitalic_S, so the two data structures must be related by T^⁢S^𝑇𝑆\widehat{T}\,Sover^ start_ARG italic_T end_ARG italic_S. Fortunately, for lists and other ADTs, the relational interpretations defined in this way enjoy this crucial inclusion-preservation property. Here, we report our ongoing efforts to add the generalization of ADTs known as Generalized Algebraic Data Types (GADTs) to System F in such a way that parametricity is preserved. In doing so, we insist on understanding GADTs as types of data structures, i.e., as types of containers that can be filled with data. Since this entails in particular that GADTs are inductive data type constructors, we might expect that following the method outlined above will suffice. In Section 2, we show that naively doing so results in relational interpretations of GADTs that do not satisfy the inclusion-preservation property identified at the end of the preceding paragraph. This is problematic: if we are to understand GADTs as types of data structures, then they should certainly satisfy all properties — among them the inclusion-preservation property — expected of such types. In Section 3, we explore a promising approach to overcoming this issue. This approach consists in defining the relational interpretation of a GADT through that of its completion, an ADT-like type constructor that contains the original GADT. In Section 4 we offer some applications of parametricity for GADTs obtained using our proposed approach. In Section 5 we discuss some issues that arise when making our proposed approach precise. Doing so requires defining a source language (an extension of System F that allows for GADTs), a target language (a dependent type theory strong enough to encode relations), and interpretations of each type of the source language as both a type and a relation in the target language. We point out some difficulties in the design of the target language, and also offer some thoughts on how to resolve them. Throughout the paper, we use an Agda-like syntax to write examples of types and terms of the anticipated target language. We note, however, that this language might end up being very different from Agda’s type theory. In particular, this early announcement by no means reports on an attempt to formalize our work in a proof assistant. We are not the first to consider parametricity for GADTs. Very recent progress on the subject has been presented in [9]. Sieczkowski et al. construct there a parametric model of an extension of System F supporting GADTs, with the aim of deriving free theorems and representation independence results. However, their work differs drastically from the line of research presented here in several ways. First, the semantics presented by Sieczkowski et al. targets normalization-by-evaluation. By contrast, our work is in no way concerned with such methods. Second, Sieczkowski et al. make essential use of guarded recursion through a universe of step-indexed propositions equipped with a later modality (as exists, e.g., in Iris). By contrast, we are concerned only with structural recursion in this work. Third, Sieczkowski et al. insist on the importance of two particular rules of their type system: discriminability and injectivity of type constructors. By contrast, we are agnostic about such rules, thus accommodating more diverse host languages. Finally, and most importantly, the semantics of Sieczkowski et al. models parametricity for GADTs only in those type indices that are unconstrained, i.e., that can be promoted to parameters. In particular, their approach cannot handle free theorems such as the one presented in Section 4.1 for 𝖲𝖾𝗊𝖲𝖾𝗊\operatorname{\sf Seq}sansserif_Seq, since 𝗉𝖺𝗂𝗋𝗂𝗇𝗀𝗉𝖺𝗂𝗋𝗂𝗇𝗀\operatorname{\sf pairing}sansserif_pairing has a constrained instance of 𝖲𝖾𝗊𝖲𝖾𝗊\operatorname{\sf Seq}sansserif_Seq as return type. By contrast, we not only recognize the non-uniformity of GADTs acknowledged by Sieczkowski et al., but we also recognize that this break of uniformity is governed by uniform type constructors (namely, those constraining the instances of the return types of GADTs’ data constructors), and that this uniformity must be captured by parametric models of the language at play."
