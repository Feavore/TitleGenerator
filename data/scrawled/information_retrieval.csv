URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04677v1,Lightning IR: Straightforward Fine-tuning and Inference of Transformer-based Language Models for Information Retrieval,"A wide range of transformer-based language models have been proposed for information retrieval tasks. However, fine-tuning and inference of these models is often complex and requires substantial engineering effort. This paper introduces Lightning IR, a PyTorch Lightning-based framework for fine-tuning and inference of transformer-based language models for information retrieval. Lightning IR provides a modular and extensible architecture that supports all stages of an information retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. It is designed to be straightforward to use, scalable, and reproducible. Lightning IR is available as open-source: https://github.com/webis-de/lightning-ir.","Pre-trained transformer-based language models have become a cornerstone in information retrieval (IR) research (Lin et al., 2022). Many different architectures have been proposed, each with their own model implementation and training procedure. This plethora makes fine-tuning different model architectures cumbersome and comparing different model architectures even more so. However, all these models use the same basic building blocks, can be fine-tuned in the same way, and have only minor differences in their inference procedure. To unify the usage of transformer-based language models in IR we present the Lightning IR framework. Lightning IR builds on and extends PyTorch Lightning (Falcon and team, 2024) to provide several key features that set it apart from existing libraries for neural IR: (1) It is backbone agnostic, i.e., (almost) any HuggingFace (Wolf et al., 2020) transformer-based language model can be used. (2) It supports the entire pipeline of IR models, from fine-tuning and indexing to searching and re-ranking. (3) It is flexible and supports, for example, multi-vector or sparse bi-encoder models and pointwise or listwise cross-encoder models out-of-the-box. (4) It provides an easy-to-use API and CLI for fine-tuning and inference. (5) It is easily configurable, allowing for reproducible experiments and easy model comparison. In this paper, we compare Lightining IR to existing frameworks, we describe its features and API, and we demonstrate Lightning IR’s capabilities in a series of reproducibility experiments."
https://arxiv.org/html/2411.04602v1,Self-Calibrated Listwise Reranking with Large Language Models,"Large language models (LLMs), with advanced linguistic capabilities, have been employed in reranking tasks through a sequence-to-sequence approach. In this paradigm, multiple passages are reranked in a listwise manner and a textual reranked permutation is generated. However, due to the limited context window of LLMs, this reranking paradigm requires a sliding window strategy to iteratively handle larger candidate sets. This not only increases computational costs but also restricts the LLM from fully capturing all the comparison information for all candidates. To address these challenges, we propose a novel self-calibrated listwise reranking method, which aims to leverage LLMs to produce global relevance scores for ranking. To achieve it, we first propose the relevance-aware listwise reranking framework, which incorporates explicit list-view relevance scores to improve reranking efficiency and enable global comparison across the entire candidate set. Second, to ensure the comparability of the computed scores, we propose self-calibrated training that uses point-view relevance assessments generated internally by the LLM itself to calibrate the list-view relevance assessments. Extensive experiments and comprehensive analysis on the BEIR benchmark and TREC Deep Learning Tracks demonstrate the effectiveness and efficiency of our proposed method.","Text reranking is a fundamental task in the information retrieval (IR) area focusing on scoring and reranking a set of retrieved text candidates (e.g., passages and documents) on the input query (Zhao et al., 2022). In the real world, text reranking is generally an important intermediate stage in the widely used IR pipeline, underpinning numerous downstream tasks, such as question answering (Mao et al., 2021) and dialogue systems (Won et al., 2023). Concretely, this task aims to measure the semantic relevance of each text candidate with the input query, and then ranks all candidates in order of that (Nogueira and Cho, 2019; Zhuang et al., 2023). In recent years, with the exceptional problem-solving capabilities of large language models (LLMs) (Zhao et al., 2023), existing work has applied LLMs to the text reranking tasks (Sun et al., 2023; Zhuang et al., 2024b; Ma et al., 2024). Instead of individually computing the relevance score for all query-candidate pairs, LLM-based methods are capable of directly generating the permutation of reranked candidates in an autoregressive manner (Ma et al., 2023; Reddy et al., 2024). Such a listwise paradigm enables efficient one-pass reranking for all candidates, and can also leverage the strong generation capability of LLMs, achieving remarkable performance. Despite the success, limited by the input window length of LLMs, it is hard to apply listwise LLM-based rerankers into a large candidate set or long documents. Although existing work has proposed the sliding window strategy (Pradeep et al., 2023a, b) that splits the candidate set for multi-round ranking, the increased computational cost is also higher for real-world applications. Moreover, the sliding window strategy would cause only part of the whole candidate set to be ranked by LLM at the same time. As a result, the global ranking process will degrade into local ranking within the window, which not only restricts LLMs from fully comparing all candidates but also leads to potential risks of the influence from the initial input order. Considering these limitations, several efforts are made to optimize the sliding window strategy (Yoon et al., 2024; Parry et al., 2024) or the autoregressive generative reranking paradigm (Reddy et al., 2024). Nevertheless, as they rely on LLMs for language generation (ranked permutation), the shortcomings in efficiency and effectiveness are still hard to fully resolve. Figure 1. Illustration of the comparison between our proposed SCaLR and the typical listwise reranking approach based on eight candidates. In this paper, we aim to propose a novel method to enable LLMs to efficiently and effectively perform listwise reranking, as shown in Figure 1. Given the whole candidate set, our motivation is to explicitly compute the list-view relevance scores for the listwise input, instead of directly producing the textual reranking results via LLMs. In this way, the list-view relevance scores can be used for a global ranking of all candidates (in the same window or not), which breaks the shortcomings caused by the in-window local comparison. To assess the relevance, we add a projection layer into the decoder-only LLM, to map the last token representations of the candidate text into the score. For the given in-window candidates, we can obtain their relevance scores and utilize ranking objectives for training. However, the ranking objectives mainly focus on learning the comparison of all candidates, which would lead to biased scores that affect the global ranking performance, especially for the top or bottom candidates (with extreme scores of 1 or 0). To address it, we propose self-calibrated training that adjusts the list-view relevance score to better align with the self-generated point-view relevance score. The point-view relevance score is generated solely based on the query and a single candidate, which is relatively fair and provides a regularization for reducing the bias. In this way, we can make use of two views of relevance scores for supervising the training process. The list-view scores provide rich comparison information, and the point-view scores calibrate the possible bias in the list-view ones, both ensuring the global comparability of the relevance score. To this end, we design a Self-Calibrated Listwise Reranking method, termed SCaLR. First, we devise the relevance-aware listwise reranking framework by revising the autoregressive generation process of LLMs. Concretely, we add corresponding projection layers for generating list-view relevance scores. To reduce the computational cost, we design a special mask mechanism to guarantee that the list-view relevance scores can be computed by a one-pass encoding process (Zelikman et al., 2024). Based on this, we introduce the parallel context encoding into the decoder-only LLM architecture, enabling independent encoding of candidates to generate the point-view relevance scores. Second, we propose the self-calibration training strategy that aligns the list-view relevance score with the point-view relevance score. Specifically, we employ the multi-task learning framework to learn both list-view and point-view relevance scores, and propose an adaptive optimization strategy to consider the reliability of the point-view scores during calibration. Our main contributions are summarized as follows: • We propose a novel listwise reranking framework SCaLR based on explicit list-view relevance for ranking, which enhances the model efficiency while addressing the limitation of window-based local ranking strategies. • We employ parallel context encoding for accelerating candidate modeling and utilize self-generated point-view relevance to calibrate the list-view relevance, ensuring global comparability for evaluating on large candidate set. • Extensive experiments and analyses on the BEIR and TREC benchmarks demonstrate the superiority of the proposed approach from in-domain and out-of-domain evaluation over state-of-the-art methods."
https://arxiv.org/html/2411.04539v1,Best Practices for Distilling Large Language Models into BERT for Web Search Ranking,"Recent studies have highlighted the significant potential of Large Language Models (LLMs) as zero-shot relevance rankers. These methods predominantly utilize prompt learning to assess the relevance between queries and documents by generating a ranked list of potential documents. Despite their promise, the substantial costs associated with LLMs pose a significant challenge for their direct implementation in commercial search systems. To overcome this barrier and fully exploit the capabilities of LLMs for text ranking, we explore techniques to transfer the ranking expertise of LLMs to a more compact model similar to BERT, using a ranking loss to enable the deployment of less resource-intensive models. Specifically, we enhance the training of LLMs through Continued Pre-Training, taking the query as input and the clicked title and summary as output. We then proceed with supervised fine-tuning of the LLM using a rank loss, assigning the final token as a representative of the entire sentence. Given the inherent characteristics of autoregressive language models, only the final token </s> can encapsulate all preceding tokens. Additionally, we introduce a hybrid point-wise and margin MSE loss to transfer the ranking knowledge from LLMs to smaller models like BERT. This method creates a viable solution for environments with strict resource constraints. Both offline and online evaluations have confirmed the efficacy of our approach, and our model has been successfully integrated into a commercial web search engine as of February 2024.","Relevance ranking is a paramount challenge in web search systems. The objective of relevance ranking is to rank candidate documents based on their pertinence to a specified inquiry. These documents are usually culled from an extensive corpus by a retrieval module. Of late, the integration of pre-trained language models (PLMs) such as BERTDevlin et al. (2018), along with industry giants like Google111https://blog.google/products/search/search-language-understanding-bert/, Bing222https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/, and BaiduZou et al. (2021); Liu et al. (2021), has been massively adopted within industry web search systems, yielding commendable resultsZhuang et al. (2023). BERT models are adept at considering the entire context of a word by examining adjacent words, which is particularly beneficial for discerning the intent of search queries. The efficacy of IR dictates the system’s response time to inquiries of users, which predominantly contingent on the performance of ranking model The recent triumphs LLMsBrown et al. (2020) in natural language processing have ignited interest in their application to text ranking. Researchers have delved into prompting LLMs to undertake zero-shot unsupervised ranking employing pointwiseWang et al. (2023); Sachan et al. (2023), pairwiseSachan et al. (2022), or listwise approachesSun et al. (2023b). Although these have made notable strides, they have yet to fully harness the potential of LLMs. Moreover, there have been initiatives to train pointwise rankers in supervised settings, utilizing LLMs, as exemplified by RankLLaMAMa et al. (2023a). Despite the SOTA performance yielded by LLM rank models in experimental settings, their direct application in real-world search engines is impractical. Figure 1: The overview of Rank Distillation from LLM Decoder to BERT Encoder. To overcome the challenges of deploying LLMs online, this paper introduces a novel Rank Distillation framework (DisRanker) that combines the capabilities of LLMs with the agility of BERT. Distillation is renowned for enhancing the efficiency of various neural ranking modelsHofstätter et al. (2020). Simultaneously, knowledge distillation facilitates the transfer of discerning skills from the teacher model to more compact models, significantly reducing computational costs during online inference. Initially, we utilize clickstream data to propagate domain knowledge through Continued Pre-Training (CPT)Gupta et al. (2023), using queries as inputs to generate titles and summaries that have captured user interest. In a process similar to question-answering, the LLM develops a detailed understanding of the interaction between queries and documents. We then refine the LLM using a pairwise rank loss, employing the end-of-sequence token, </s>, to represent query-document pairs. While previous research on neural rank models primarily used a bidirectional encoder-only model like BERT, interpreting the [CLS] token as a comprehensive representation of the text input, the autoregressive nature of LLMs prompts us to introduce an end-of-sequence token for the input query and document to structure the input sequence. The latent state from the final layer corresponding to this token is considered the embodiment of the query and document relationship. Consequently, we integrate a dense layer to act as a relevance adjudicator, applying pairwise rank loss to fine-tune the LLM. The deployment of LLMs for ranking tasks still faces practical challenges, particularly regarding application efficacy and output consistency. In the next phase, we employ a hybrid approach using Point-MSEQin et al. (2021) and Margin-MSEHofstätter et al. (2020) losses to distill the LLM. Point-MSE calculates the absolute difference between the LLM teacher and the BERT student, while Margin-MSE introduces a form of regularization and encourages the student model to learn the relative ranking from the teacher. This approach prevents overfitting by not requiring the student model to exactly match the teacher’s scores but to maintain the order of the scores, which is essential for ranking tasks. Thus, the student model learns to emulate the teacher’s ranking behavior while being more lightweight and efficient, making it better suited for deployment in resource-constrained environments. The main contributions of this paper can be summarized as follows: • We present DisRanker, a novel Rank Distillation pipeline that seamlessly integrates LLM with BERT to enhance web search ranking. A comprehensive suite of offline and online evaluations substantiates the efficacy of DisRanker. • We propose a domain-specific continued pre-training methods which is beneficial for enhancing the performance of LLMs on text ranking tasks. Additionally, we contribute a hybrid approach that employs Point-MSE and Margin-MSE loss to refine the distillation of LLM."
https://arxiv.org/html/2411.04403v1,Towards Competitive Search Relevance For Inference-Free Learned Sparse Retrievers,"Learned sparse retrieval, which can efficiently perform retrieval through mature inverted-index engines, has garnered growing attention in recent years. Particularly, the inference-free sparse retrievers are attractive as they eliminate online model inference in the retrieval phase thereby avoids huge computational cost, offering reasonable throughput and latency. However, even the state-of-the-art (SOTA) inference-free sparse models lag far behind in terms of search relevance when compared to both sparse and dense siamese models. Towards competitive search relevance for inference-free sparse retrievers, we argue that they deserve dedicated training methods other than using same ones with siamese encoders. In this paper, we propose two different approaches for performance improvement. First, we introduce the IDF-aware FLOPS loss, which introduces Inverted Document Frequency (IDF) to the sparsification of representations. We find that it mitigates the negative impact of the FLOPS regularization on search relevance, allowing the model to achieve a better balance between accuracy and efficiency. Moreover, we propose a heterogeneous ensemble knowledge distillation framework that combines siamese dense and sparse retrievers to generate supervisory signals during the pre-training phase. The ensemble framework of dense and sparse retriever capitalizes on their strengths respectively, providing a strong upper bound for knowledge distillation. To concur the diverse feedback from heterogeneous supervisors, we normalize and then aggregate the outputs of the teacher models to eliminate score scale differences. On the BEIR benchmark, our model outperforms existing SOTA inference-free sparse model by 3.3 NDCG@10 score. It exhibits search relevance comparable to siamese sparse retrievers and client-side latency only 1.1x that of BM25.","Information retrieval(IR) and question answering(QA) are fundamental tasks in the realm of information processing, widely employed in various web applications. Lexical-based algorithms such as TF-IDF and BM25 were once the dominant approach. These algorithms utilize inverted indexes, which was proven to be efficient. However, due to issues such as vocabulary mismatch (Zhao and Callan, 2010) and the lack of contextual information, their semantic retrieval capabilities are limited. In contrast, siamese dense retrievers have overcome the limitations of traditional lexical-based methods and have become the mainstream approach for semantic retrieval (Reimers and Gurevych, 2019). Nonetheless, the ANN algorithm requires a substantial amount of memory leading to a significant trade-off between search relevance and resource consumption (Malkov and Yashunin, 2018; Jegou et al., 2010). The interpretability of dense retrievers is also questioned. Recent years, learned sparse retrieval is proposed to address this obstacle and gained increasing attention (Dai and Callan, 2020; Formal et al., 2021b, a). This approach predicts token weights based on their semantics with context information. It expands token set with generative models (Nogueira et al., [n. d.], 2019) or masked language model heads (Bai et al., 2020; Zhao et al., 2021; Formal et al., 2021b, a, 2022; Lassance and Clinchant, 2022; MacAvaney et al., 2020; Lassance et al., 2024), thereby addressing the vocabulary mismatch problem. Since sparse embeddings can be integrated with inverted indexes, the retrieval process of learned sparse models is highly efficient without compromising recall. Moreover, learned sparse retrieval offers better interpretability because the contribution of each token can be intuitively understood by human. Among the learned sparse models, the inference-free architecture is particularly attractive to search applications. This architecture degenerates online model inference for query encoding into simple tokenization, significantly reducing end-to-end search latency and the associated model deployment costs. Early work like DEEPCT (Dai and Callan, 2020) and Doc2Query (Nogueira et al., [n. d.]) attempted to associate additional information with the original documents. However, search relevance could not be trained in an end-to-end manner. Proposed by Formal et al. (2021a), the SPLADE-doc architecture has achieved SOTA performance among inference-free retrievers. It predicts the token weights and expands the tokens with similar semantics. The search relevance and sparsity are tuned via end-to-end training. However, even the latest SOTA inference-free model, SPLADE-v3-Doc (Lassance et al., 2024), exhibits a significant gap in search relevance when compared with siamese sparse retrievers. On the BEIR benchmark, the average NDCG@10 score of SPLADE-v3-Doc is 4.7 lower than siamese sparse retrievers of the same size and training method. This disparity hinders its application in actual production environments. In this paper, we focus on improving the search relevance of the inference-free sparse retriever through more effective training methodologies. The first challenge lies in the uniform penalty applied to all tokens by the FLOPS regularization (Paria et al., 2020). The purpose of the FLOPS loss is to eliminate unnecessary tokens to sparsify the document representation. However, the uniform penalty from FLOPS loss has a side effect: it prevents the model from assigning tokens with large weights. Consequently, for semantically rich tokens that are important to be retained in the representation, their importance will be underestimated. To mitigate this effect, we propose a simple yet effective enhancement called IDF-aware FLOPS. This loss adjusts the penalty imposed by the FLOPS loss on different tokens, making the penalty inversely proportional to the token’s IDF value. It reduces the regularization penalty on unique tokens and increases the penalty on frequent tokens. Through experiments, we demonstrate that IDF-aware FLOPS effectively improves the search relevance of the inference-free sparse retriever, and we discover that it effectively reduces the average FLOPS number for the retrieval process. Subsequently, the pre-training phase is also explored in this paper. We argue that although the commonly used contrastive InfoNCE loss (Chen et al., 2020) is able to enhance the alignment and uniformity of representations (Wang and Isola, 2020), these two targets are not applicable in inference-free models. Because for inference-free models, all semantics are only encoded at the model-side. And the search relevance can not be improved by aligning the document representation with the bag-of-word query representation. In contrast, knowledge distillation presents a more suitable approach for the training (Formal et al., 2021a). Hofstätter et al. (2020) proposed to train dense retrievers by conducting knowledge distillation from cross-encoder rerankers, and Formal et al. (2022, 2021a) applied this method to the fine-tuning of sparse retrievers. However, for large-scale pre-training datasets, the inference workload of the teacher model can reach 10 times or even larger. And the inference cost of cross-encoders is impractical at these settings, especially when in-batch negatives are utilized. In this paper, we propose to build a strong teacher model by assembling siamese dense retrievers and siamese sparse retrievers. Siamese retrievers have a heterogeneous and superior architecture compared with the inference-free architecture. And their inference cost is applicable for large-scale pre-training. Moreover, the ensemble of dense and sparse retrievers can further enhance the upper bound of knowledge distillation, enlarging the space for performance improvement of our model. During the assembling process, we normalize the scores for heterogeneous retrievers. This prevents one retriever from dominating the assembled result, further balancing the contribution of teacher models. We conduct experiments on 13 public datasets from the BEIR benchmark, and our model outperforms the existing SOTA inference-free sparse model by 3.3 average NDCG@10 scores. Its performance even surpasses many strong siamese retrievers. Our contributions can be summarized as follows: (1) We propose IDF-aware FLOPS, which effectively improves the search relevance and efficiency of inference-free sparse models. (2) We explore how to effectively pre-train inference-free sparse models and propose the ensemble teacher model of heterogeneous siamese models, which has reasonable inference costs and strong performance. (3) The zero-shot performance of our model outperforms the SOTA inference-free retriever by 3.3 NDCG@10 score. It also surpasses strong siamese retrievers including SPLADE-v3-DistilBERT and ColBERTv2. While its client-side latency is only 1.1x that of BM25."
https://arxiv.org/html/2411.04129v1,"AmazonQAC: A Large-Scale, Naturalistic Query Autocomplete Dataset","Query Autocomplete (QAC) is a critical feature in modern search engines, facilitating user interaction by predicting search queries based on input prefixes. Despite its widespread adoption, the absence of large-scale, realistic datasets has hindered advancements in QAC system development. This paper addresses this gap by introducing AmazonQAC, a new QAC dataset sourced from Amazon Search logs, comprising 395M samples. The dataset includes actual sequences of user-typed prefixes leading to final search terms, as well as session IDs and timestamps that support modeling the context-dependent aspects of QAC. We assess Prefix Trees, semantic retrieval, and Large Language Models (LLMs) with and without finetuning. We find that finetuned LLMs perform best, particularly when incorporating contextual information. However, even our best system achieves only half of what we calculate is theoretically possible on our test data, which implies QAC is a challenging problem that is far from solved with existing systems. This contribution aims to stimulate further research on QAC systems to better serve user needs in diverse environments. We open-source this data on Hugging Face at https://huggingface.co/datasets/amazon/AmazonQAC.","Query Autocomplete (QAC) is an important feature in nearly every modern search engine (Cai and de Rijke, 2016). As the user types out a search query, the QAC system’s aim is to provide a list of search term suggestions based on the partially typed query (the “prefix”). Ideally, the QAC system will provide the user’s intended query, which they can select, thereby saving them the effort of typing out the full query. Even where the user does not have a specific query in mind, QAC suggestions can help them formulate search queries that lead them to the results they are seeking. However, despite the importance of QAC, it is a comparatively under-explored task in research. The publicly available datasets tend to be derived from search query datasets (e.g. Patki et al., 2024; Maurya et al., 2023; Park and Chiba, 2017). However, these datasets do not contain the prefixes that users typed, so prefixes have to be synthetically constructed (Mitra and Craswell, 2015), greatly limiting the empirical value of these resources for QAC. In fact, we were unable to find any publicly available large scale QAC datasets beyond synthetically constructed ones from the AOL data release in 2006. In general, the lack of large-scale realistic benchmarks has hampered research on QAC; few tasks have as large a gap between their importance in real-world technologies and the amount of research devoted to them. Query ID Session ID Prefixes First Prefix Time Final Search Term Search Time 12 354 [s, si, sin, sink, sink r, sink ra, sink rac, sink rack] 2023-09-04T20:46:14.293Z sink rack for bottom of sink 2023-09-04 20:46:27 376 1886 [a, al, alu, alum, alumi, alumi, alumin, alumin, alumind, alumind] 2023-09-04T12:18:44.120Z aluminum free deodorant for men 2023-09-04 12:18:47 120259 5691 [t, tu, tup, tupe, tupelo, tupelo ] 2023-09-15T07:47:16.359Z tupelo honey 2023-09-15 07:47:20 983301 5691 [tupelo honey, tupeo honey, tupo honey, tuo honey, to honey] 2023-09-15T07:49:21.616Z honey 2023-09-15 07:49:27 Table 1: Illustrative AmazonQAC dataset examples. The examples contain the actual prefixes that users typed on the way to selecting a search term. The session IDs and timestamps support reconstructing search contexts. In this paper we aim to facilitate more research on QAC by releasing AmazonQAC, a QAC dataset collected from Amazon Search logs with participation and support from Amazon. AmazonQAC contains 395M anonymized examples, where an example consists of a submitted search term together with the sequence of prefixes that was typed to reach that search term, a session ID, a timestamp, and other metadata (Table 1). The session IDs and timestamps mean that multiple sequential user searches can be grouped together to form context, which has shown to be useful for QAC (e.g. Shokouhi, 2013; Bar-Yossef and Kraus, 2011). The dataset includes a test set of 20K examples from a later time period than the train set, designed to simulate a real-world deployment of a QAC system. We present the task description, analyze dataset statistics, describe evaluation metrics, and motivate an upperbound Success@10 score of 69.8% on our held-out test set. We also evaluate several baseline approaches on AmazonQAC: Prefix Trees, semantic retrieval, and Large Language Models (LLMs) with and without finetuning. We find that the QAC problem is not just a simple case of prefix-search term memorization, as conventional wisdom might imply, but rather that it is a complex recommendation problem that is significantly influenced by the user’s search context. Our best baseline system is a finetuned LLM that leverages session context, and it achieves Success@10 of 37, which is half of our upperbound. This indicates that the QAC problem is a difficult one not readily solved by current systems. We hope that releasing AmazonQAC will prompt further innovation in QAC systems and that our baseline systems help guide these research efforts."
https://arxiv.org/html/2411.04366v1,The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing,"We present “The Concatenator,” a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.’s “musaicing” (or “audio mosaicing”) technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger’s NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.","Concatenative synthesis, or audio mosaicing, is a data-driven approach to arrange granular fragments of audio samples, particularly using data sourced from the spectral-temporal features of a target sound. While granular synthesis systems typically rely on combinations of aleatoric parameterization, deterministic automation, and traditional synthesis modulation to achieve complex and evolving textures from sound fragments [28], concatenative synthesis algorithms utilize Music Information Retrieval technology to decide parameters such as the index, amplitude, and pitch of each sound fragment. Modern music producers are inundated by audio data. Services like Splice offer hundreds of thousands of samples readily available on the cloud, and Kontakt multi-sample libraries can often take up over 10gb of disk space to capture a single instrument. Music Producers generate plenty of their own audio data as well: stems, multi-tracks, long-form recordings, and mix variations account for a large portion of many a music producer’s audio collection. Recent software such as XO by XLN Audio, Sononym, and Ableton Live 12 offer automatic organization of audio files based on various tags and descriptors, but these implementations of MIR technology are more utilitarian than creative in their design and application. Meanwhile, concatenative synthesis options remain sparse since its conceptual inception [31]: Reformer by Krotos is designed to create foley designs, apps like Samplebrain and CataRT [32, 33] are lacking in critical musical areas such as pitch tracking, with the more advanced options having limited accessibility for artists, requiring prior knowledge of Max (FluCoMa, MuBu) or Python (Audioguide). The Concatenator advances concatenative synthesis in 3 major ways: 1) it is capable of accurately reproducing harmonic and percussive sounds using arbitrary corpora 2) in real-time at scale, 3) affording new levels of control and accessibility. Furthermore, unlike neural audio systems [5], it requires no training and can adapt to arbitrary corpora at runtime. The speed, ease, and scope of The Concatenator offers a fresh paradigm for music producers to interact creatively with their ever-expanding excess of audio data, leading to what we believe is a breakthrough in the field."
https://arxiv.org/html/2411.04228v1,dsld: A Socially Relevant Tool for Teaching Statistics,"The growing power of data science can play a crucial role in addressing social discrimination, necessitating nuanced understanding and effective mitigation strategies of potential biases. “Data Science Looks At Discrimination” (dsld) is an R and Python package designed to provide users with a comprehensive toolkit of statistical and graphical methods for assessing possible discrimination related to protected groups, such as race, gender, and age. Our software offers techniques for discrimination analysis by identifying and mitigating confounding variables, along with methods for reducing bias in predictive models.In educational settings, dsld offers instructors powerful tools to teach important statistical principles through motivating real-world examples of discrimination analysis. The inclusion of an 80-page Quarto book further supports users—from statistics educators to legal professionals—in effectively applying these analytical tools to real-world scenarios.","Statistics—the class students love to hate! It’s hard to think of a course less popular, yet required by more majors, than statistics. Recent studies have found a negative student perception of statistical courses, ranging from undergraduate to graduate level course work (Naidu and Arumugam, 2014; Dani and Al Quraan, 2023). Perhaps relabeling as “data science” will help a bit; however, the subject is badly in need of better motivation. To aid this, a number of remedies have been proposed, ranging from the flipped classroom (Kovacs et al., 2021) to stories involving Disney characters (Peters, 2019). The American Statistical Association also has suggestions (Carver et al., 2016). Our package, dsld (Data Science Looks at Discrimination), takes a different approach by appealing to students’ awareness of social issues (Bowen et al., 2017). The software provides both analytical and graphical/tabular methods for investigating possible bias related to race, gender, age, and other potential sensitive variables. Specifically, two broad categories are addressed: • Detection of discrimination: This section focuses on identifying and compensating for confounding variables. For instance, is there a gender gap in wages after taking into account confounders such as age, occupation, number of weeks worked, etc.? • Addressing bias in prediction: This section focuses on the reduction of bias in the context of predictive algorithms. For example, consider a tool to aid in granting loan applications. If an applicant’s race is included as a predictor – either explicitly or through proxy variables – how can one mitigate its effect? The value of the package is greatly enhanced via the use of a tightly integrated open source textbook, written in Quarto (Allaire et al., 2024). The book is not a user manual for the package, but instead serves as a detailed treatment of the statistical concepts themselves, illustrated with dsld examples. This is no toy. On the contrary, the package will be quite useful in social science research, internal HR analyses, and in discrimination litigation. Both parametric and nonparametric regression models are available. We note other R packages focusing on analysis of discrimination and related issues: divseg (Kenny, 2022) is concerned with urban racial segregation; genderstat (Arafin Ayon, 2023) is “…an exhaustive tool developed for the R…programming environment, explicitly devised to expedite quantitative evaluations in the field of gender studies;” segregation (Elbers, 2021) is a tool for the calculation of relationships in two-way contingency tables, including with grouping, with a typical intended use case being analysis of urban racial segregation. Several packages address the issue of fairness in prediction, including fairML (Scutari, 2023a); fairmodels (Wiśniewski and Biecek, 2021); and fairness (Kozodoi and V. Varga, 2021). The organization of the remainder of this paper is as follows: Section 2 introduces the package and Quarto book. Section 3 covers detection of discrimination, and Section 4 addresses reduction of bias in prediction. Finally, the paper concludes with a discussion and future perspectives in Section 5. Some notation: We have a response variable Y𝑌Yitalic_Y related to a vector of covariates X𝑋Xitalic_X, and a sensitive variable S𝑆Sitalic_S; the latter may be continuous, binary or categorical. Y𝑌Yitalic_Y can be continuous or binary, with coding 1 and 0 in the latter case. In predicting Y𝑌Yitalic_Y in a new case, the predicted value is denoted by Y^^𝑌\widehat{Y}over^ start_ARG italic_Y end_ARG."
https://arxiv.org/html/2411.04051v1,Reproducible Hybrid Time-Travel Retrieval in Evolving Corpora,"There are settings in which reproducibility of ranked lists is desirable, such as when extracting a subset of an evolving document corpus for downstream research tasks or in domains such as patent retrieval or in medical systematic reviews, with high reproducibility expectations. However, as global term statistics change when documents change or are added to a corpus, queries using typical ranked retrieval models are not even reproducible for the parts of the document corpus that have not changed. Thus, Boolean retrieval frequently remains the mechanism of choice in such settings.We present a hybrid retrieval system combining Lucene for fast retrieval with a column-store-based retrieval system maintaining a versioned and time-stamped index. The latter component allows re-execution of previously posed queries resulting in the same ranked list and further allows for time-travel queries over evolving collection, as web archives, while maintaining the original ranking. Thus, retrieval results in evolving document collections are fully reproducible even when document collections and thus term statistics change.","Ranked Information Retrieval has, de facto, permeated all areas of searching for information, users of nearly every search interface expecting that results are presented in some order of relevance to their query. The results returned by such an IR system to user queries are usually not reproducible when the IR system’s underlying document collection changes, as is the case for any live search platform. More concretely, when documents are added to a collection or updated, global term statistics for that collection (e.g., term frequency and inverse document frequency) change. Since in ranked retrieval these global statistics contribute to the computation of relevance scores, such changes cause different result rankings. While one can argue that this lack of retrieval reproducibility is not of concern in a general (web) search setting, there are cases where, given a set of documents as a result of a search process, these search results must be traceable or reproducible at a later point in time. This is especially true when the focus of the work is not the retrieval itself, but research based on the analysis of data that is the result of a (ranked) retrieval operation, i.e. that is, work with extracted subsets of documents from an evolving document corpus. See, for example, “Fig 1. Corpus analysis sequence” in (Murdock et al., 2017), where a ranked retrieval is the first step in a processing pipeline. The authors state that the same query, executed at a later time and on a document collection that has changed, has resulted in a different, larger set of documents, making the original result set not possible to recreate. Domains that rely on reproducible and explainable retrieval results include academic search, patent retrieval, and scientific analysis of evolving collections (e.g. social media and webpages). An example of such a domain is systematic literature reviews (SLR), which are a type of secondary study that summarizes primary studies to answer pre-specified research questions. To select candidate primary studies for summarization, systematic reviews rely on constructing Boolean queries that retrieve them (Hausner et al., 2015; Higgins et al., 2019). The usage of Boolean queries in SLR ensures that any search can be replicated and further allows to validate the findings of such a study. For this, the documents are further filtered based on the document timestamps, to allow for consistent retrieval in evolving document collections. Although Boolean retrieval allows for the replication and reproduction of search results, they usually return large sets of documents, of which many are not relevant to the user’s actual information need. Therefore, ranking documents according to their relevance is preferred, which is currently done with neural rerankers on the result set of a boolean query to prioritize documents for downstream tasks (Wang et al., 2023). Search systems like CORE (Knoth and Zdrahal, 2012), Google Scholar111https://scholar.google.com/, or Semantic Scholar (Jones, 2015) do not provide functionality that allow researchers working with scientific content to make the results of their queries reproducible. Using APIs to interact with such systems is another form of issuing queries for retrieving data, where there is no information on how the query is further processed nor on how the results are ranked, if at all. Reproducibility of IR ranked results, as argued for above, necessitates keeping track of all changes. This is a highly challenging task, considering the rate of change in the document collections222See “Overall edit volume” at https://en.wikipedia.org/wiki/Wikipedia:Statistics, or monthly submission on Arxivhttps://arxiv.org/stats/monthly_submissions. To overcome these limitations, when working on topics that rely on the analysis of scientific publications, researchers use Boolean retrieval tools from systems like Scopus333https://scopus.com/ or PubMed444https://www.ncbi.nlm.nih.gov/pubmed/, or more novel tools like Cruise Screening (Kusa et al., 2023), where tracking and sharing literature search results is possible. In this work, we present a hybrid IR system that combines a classic, rank-based retrieval engine with a temporal, column-store-based search engine that allows for reproducing search results. The remainder of this paper is structured as follows. Section 2, provides a brief review on reproducibility in IR, time-travel and temporal information retrieval, while it also introduces the Research Data Alliance Dynamic Data Citation Guidelines and column-store-based retrieval. Section 3 describes the architecture of our proposed hybrid system. Section 4 shows how we reproduce retrieval results. Section 5 evaluates indexing speed, query response times, score correctness, and storage overhead, followed by conclusions in Section 6. The system is released as Open Source Software555https://anonymous.4open.science/r/Hybrid-Information-Retrieval."
https://arxiv.org/html/2411.03957v1,Fine-Grained Guidance for Retrievers: Leveraging LLMs’ Feedback in Retrieval-Augmented Generation,"Retrieval-Augmented Generation (RAG) has proven to be an effective method for mitigating hallucination issues inherent in large language models (LLMs). Previous approaches typically train retrievers based on semantic similarity, lacking optimization for RAG. More recent works have proposed aligning retrievers with the preference signals of LLMs. However, these preference signals are often difficult for dense retrievers, which typically have weaker language capabilities, to understand and learn effectively. Drawing inspiration from pedagogical theories like Guided Discovery Learning, we propose a novel framework, FiGRet (Fine-grained Guidance for Retrievers), which leverages the language capabilities of LLMs to construct examples from a more granular, information-centric perspective to guide the learning of retrievers. Specifically, our method utilizes LLMs to construct easy-to-understand examples from samples where the retriever performs poorly, focusing on three learning objectives highly relevant to the RAG scenario: relevance, comprehensiveness, and purity. These examples serve as scaffolding to ultimately align the retriever with the LLM’s preferences. Furthermore, we employ a dual curriculum learning strategy and leverage the reciprocal feedback between LLM and retriever to further enhance the performance of the RAG system. A series of experiments demonstrate that our proposed framework enhances the performance of RAG systems equipped with different retrievers and is applicable to various LLMs.","Large language models (LLMs), such as GPT-4 (Achiam et al. 2023), have achieved impressive results across a wide range of language tasks (Brown et al. 2020; Kojima et al. 2022). However, despite their rapid recent development, the issue of hallucinations persists (Zhang et al. 2023b; Ji et al. 2023). Particularly in knowledge-intensive tasks (Kandpal et al. 2023), the generated content sometimes deviates from factual information, resulting in fabricated or inaccurate statements. Retrieval-Augmented Generation (RAG) is regarded as an effective approach to mitigate the issue of hallucination (Lewis et al. 2020; Borgeaud et al. 2022), by leveraging external corpora to assist LLMs in generating accurate factual information. In previous approaches, retrievers are typically trained on semantic similarity, which may not align well with LLMs. Recent efforts (Shi et al. 2023; Yu et al. 2023) have explored fine-tuning retrievers with the results generated by LLMs as supervision signals, often involving LLMs annotating preferred documents within the corpus. However, due to their weaker linguistic abilities, retrievers struggle to grasp the fine-grained preferences of LLMs, posing challenges for effective alignment. Drawing from educational theories such as discovery learning, we note that training retrievers directly on LLMs’ preferences resembles pure discovery learning, which lacks explicit hints about why certain documents benefit LLMs. Literature on this theory (Craig 1956; Kittell 1957; Piaget 1970; Mayer 2004) suggests that guided discovery—providing extra guidance and feedback—is often more effective than pure discovery for learning concepts and rules. Therefore, we propose the FiGRet framework, inspired by educational theories. We view the more linguistically capable LLM as the ’teacher’ and the smaller model (retriever) as the ’student’. We provide feedback and guidance on the retriever’s alignment through the four steps commonly found in guided discovery learning scenarios, as well as three objectives closely related to RAG performance. Specifically, the proposed framework follows four steps: establishing learning objectives, constructing guidance, student model training, and assessing performance. First, we identify three key, minimally overlapping factors affecting RAG performance as learning objectives: 1) Relevance: Retrieved documents must contain directly relevant information to ensure the LLMs can generate correct results. 2) Comprehensiveness: The completeness of information within retrieved documents influences the comprehensiveness of the LLM’s generated content. 3) Purity: The proportion of noise (irrelevant information) within a document impacts generation quality, as excessive noise can mislead the model or obscure relevant information. We refer to the proportion of non-noisy information within a document as its purity. These objectives guide the retriever towards our ultimate goal of aligning with LLMs preferences. Second, we guide the retriever to learn these objectives through a more granular perspective. While previous work focused on training at the document-level, the retriever already possesses substantial document-level understanding. To further enhance its capabilities, we shift from a document-centric to an information-centric perspective, viewing each document as a collection of information units. Our learning objectives can then be approximated as the accuracy, recall, and precision of all retrieved information units. Following common practices in guided discovery learning, we construct easy-to-understand examples with hints across these three objectives. This construction facilitates the less linguistically capable retriever in capturing and aligning with the LLMs’ complex preferences without relying on implicit learning from massive document pairs. Third, we adopt a dual curriculum learning approach for student models training, gradually increasing the difficulty of learning tasks as suggested in guided discovery practices (Shulman et al. 1966; Mayer 2004). Finally, we conduct performance assessment, and leverage well-learned and poorly-learned instances to further enhance the framework. Well-learned instances are used to optimize the teacher model’s guidance construction, while poorly-learned instances undergo additional learning. Our experiments show that various retrievers achieve performance improvements within our framework. Across tasks such as MMLU and open-domain QA, performance improvements are observed across different LLMs. Furthermore, we validate the learning effectiveness of the retrievers in the three objectives. We summarize our contributions as follows: • We propose a framework, FiGRet, inspired by educational theories, in which LLMs assist smaller models (retrievers) in learning by providing high-quality guidance, enabling the smaller models to more efficiently learn complex knowledge, such as LLMs’ preferences. • We construct guidance examples based on three key factors affecting RAG performance, adopting a fine-grained perspective to help retrievers align with LLMs’ preferences. • Our framework allows for feedback and guidance from black-box LLMs without needing access to their inference processes, simplifying deployment."
https://arxiv.org/html/2411.03881v1,Data Fusion of Synthetic Query VariantsWith Generative Large Language Models,"Considering query variance in information retrieval (IR) experiments is beneficial for retrieval effectiveness. Especially ranking ensembles based on different topically related queries retrieve better results than rankings based on a single query alone. Recently, generative instruction-tuned Large Language Models improved on a variety of different tasks in capturing human language. To this end, this work explores the feasibility of using synthetic query variants generated by instruction-tuned LLMs in data fusion experiments. More specifically, we introduce a lightweight, unsupervised, and cost-efficient approach that exploits principled prompting and data fusion techniques. In our experiments, LLMs produce more effective queries when provided with additional context information on the topic. Furthermore, our analysis based on four TREC newswire benchmarks shows that data fusion based on synthetic query variants is significantly better than baselines with single queries and also outperforms pseudo-relevance feedback methods. We publicly share the code and query datasets with the community as resources for follow-up studies.https://github.com/breuert/sigirap24","Test collection-based evaluations of IR experiments are typically done in accordance with the Cranfield paradigm. Usually, these experiments are implemented with topics that define the users’ information need in a structured way (Sanderson, 2010). In principle, there are different ways to generate queries from topic files. However, the de facto standard in most IR evaluations is the use of the topic’s title as the query string. While this approach allows better comparability between different systems, it completely neglects query variability and also the system behavior under the consideration of different topically or semantically related queries (Bailey et al., 2015). Including multiple query representations in the retrieval process bears a lot of potential to improve retrieval effectiveness. Empirically, the improvements by data fusion algorithms were shown in several works. However, earlier work either required crowdsourcing efforts to obtain real query variants from users (Bailey et al., 2016; Benham and Culpepper, 2017), which is costly and time-consuming, or generated synthetic query variants for which additional query modeling methods were necessary (Chakraborty et al., 2020; Breuer et al., 2022). Recently, generative instruction-tuned LLMs substantially improved on various language modeling tasks. Zero shot, few shot, or in-context learning (Brown et al., 2020) does not require additional task-specific fine-tuning but instead provides the task-agnostic language model with context information based on precise instructions and by learning from examples in the prompt, allowing the language model to grasp the task. These advances offer promising solutions to reduce efforts when in need of obtaining query variants. To this end, this work evaluates if synthetic query variants based on generative LLMs can be used to improve the retrieval effectiveness in data fusion experiments. Figure 1 provides a high-level illustration of our proposed methodology. Given the information need that is described by the topic file, we construct a topic-specific prompt that is given to the LLM, and the generated queries are then used to retrieve documents from the index, yielding multiple single rankings based on different query variants. Afterward, the single rankings are combined with data fusion techniques, resulting in the final ranking. Our results suggest that fused rankings of synthetic query variants based on LLMs and principled prompting can result in viable results that, in combination, yield better effectiveness than the same retrieval method would yield with a single query. While this work focuses on evaluating the retrieval effectiveness of the fused rankings, the proposed method could also be understood as a component of a Retrieval Augmented Generation (RAG) system where the ranking is an intermediate used to improve the quality of system outputs in a conversation with an agent. In such a conversational search setting, it is also feasible to obtain a more precise understanding of the user’s information need, similar to the topic file, making this approach a promising candidate for real-world applications, which we discuss later in the text. In sum, our contributions are threefold and include the following: • An unsupervised and lightweight retrieval approach based on data fusion of synthetic query variants, • experimental evaluations based on four different newswire datasets, including different prompting strategies and different numbers of fused query variants, • new datasets of query variants for TREC test collections of the respective tracks and open-source code of the experimental setup for better reproducibility and follow-up studies.111\faGithub https://github.com/breuert/sigirap24"
https://arxiv.org/html/2411.03906v1,Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System,"In this paper, we examine the impact of lexicalization on Question Answering over Linked Data (QALD). It is well known that one of the key challenges in interpreting natural language questions with respect to SPARQL lies in bridging the lexical gap, that is mapping the words in the query to the correct vocabulary elements. We argue in this paper that lexicalization, that is explicit knowledge about the potential interpretations of a word with respect to the given vocabulary, significantly eases the task and increases the performance of QA systems. Towards this goal, we present a compositional QA system that can leverage explicit lexical knowledge in a compositional manner to infer the meaning of a question in terms of a SPARQL query. We show that such a system, given lexical knowledge, has a performance well beyond current QA systems, achieving up to a 35.8%percent35.835.8\%35.8 % increase in the micro F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT score compared to the best QA system on QALD-9. This shows the importance and potential of including explicit lexical knowledge. In contrast, we show that LLMs have limited abilities to exploit lexical knowledge, with only marginal improvements compared to a version without lexical knowledge. This shows that LLMs have no ability to compositionally interpret a question on the basis of the meaning of its parts, a key feature of compositional approaches. Taken together, our work shows new avenues for QALD research, emphasizing the importance of lexicalization and compositionality.","Question Answering over Linked Data (QALD) [63] is the task of automatically mapping a natural language question to an executable SPARQL query such that relevant information can be retrieved from RDF data sources. One of the seven challenges [69] identified by the authors for the development of QALD systems is handling the lexical gap [69], which requires bridging the way users refer to certain natural language terms and the way they are modeled in a given knowledge base. Consider the question “Who is the mayor of Moscow?”. In this case, “mayor” needs to be interpreted with respect to DBpedia as dbo:leaderName111We use namespace prefixes that are defined as follows: dbr: http://dbpedia.org/resource/, dbo: http://dbpedia.org/ontology/, dbp: http://dbpedia.org/property/, rdfs: http://www.w3.org/2000/01/rdf-schema#, rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# to map the question correctly to the following SPARQL query: SELECT ?o WHERE { dbr:Moscow dbo:leaderName ?o } Another important aspect of QALD is the principle of compositionality. That is, the meaning of a complex expression is determined by the meanings of its parts and the way they are syntactically combined. In the context of QALD, a complex question is represented by a SPARQL query that involves more than one triple pattern, excluding the predicates rdf:type or rdfs:label. For example, the SPARQL query of the complex question “Who is the mayor of the capital of Russia?” is as follows: SELECT ?uri WHERE { dbr:Russia dbo:capital ?o . ?o dbo:leaderName ?uri }. To handle complex questions, the QALD system requires using compositional reasoning to obtain the answer, which includes multi-hop reasoning, set operations, and other forms of complex reasoning. Recent approaches based on machine learning models (e.g., deep neural networks [51, 34, 66, 45], Seq2Seq neural networks [52], transformers [44, 81, 43], subgraph embeddings [6], probabilistic graphical models [30], bi-directional LSTMs [31], and tree-LSTMs [2]) have achieved promising results, and are currently mostly limited to answering simple questions (i.e., only one triple excluding the predicates rdf:type and rdfs:label). To deal with complex queries, Hakimov et al. [32] have proposed an approach that uses Combinatory Categorial Grammar (CCG) [65] for syntactic representations and typed lambda calculus expressions [8] for semantic representations. Some approaches [68, 70] strongly resemble ours, as the motivation is very similar: using explicit lexical information and Dependency-based Underspecified Discourse Representation Structures (DUDES) [10, 14] for semantic composition. However, these approaches generate all possible combinations of SPARQL queries for a natural language sentence, providing no mechanism for disambiguation; therefore, they produce many logically incorrect SPARQL queries. Some QALD approaches [32, 19, 79, 7, 60] have made only limited use of lexicalization, while others [68, 22, 21] have used lexical knowledge but have not systematically investigated its impact. Recently, LLM-based approaches [41, 55, 4, 3, 53, 28, 29] have proven to be powerful tools for NLP tasks. In particular, ChatGPT [67, 24, 80] has been shown to be an alternative to traditional QALD approaches. To our knowledge, Generative Pretrained Transformer (GPT) models have not been tested for their ability to compositionally interpret a question based on the meaning of its parts or the impact of lexical knowledge on their performance. In this paper, we thus address three research questions and provide the corresponding contributions listed below: RQ1. How can a QA system leverage explicit lexical knowledge? Towards this goal, we present a new compositional QA system that relies on a dependency parse and bottom-up semantic composition. RQ2. What is the impact of explicitly given lexical knowledge? Our experimental results show that our compositional system reaches (micro) F1subscript𝐹1F_{1}italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT measures of 0.720.720.720.72 on the QALD-9 dataset, which outperforms existing state of the art systems on the task by far (+ 35%). RQ3. Can Large Language Models also leverage explicit lexical knowledge? Our experiments show that, when encoding lexical knowledge explicitly in the prompt, state-of-the-art LLMs can benefit from such knowledge, improving results. However, they are far from reaching improvements that match the performance of our compositional approach."
https://arxiv.org/html/2411.03624v1,SEGMN: A Structure-Enhanced Graph Matching Network for Graph Similarity Learning,"Graph similarity computation (GSC) aims to quantify the similarity score between two graphs. Although recent GSC methods based on graph neural networks (GNNs) take advantage of intra-graph structures in message passing, few of them fully utilize the structures presented by edges to boost the representation of their connected nodes. Moreover, previous cross-graph node embedding matching lacks the perception of the overall structure of the graph pair, due to the fact that the node representations from GNNs are confined to the intra-graph structure, causing the unreasonable similarity score. Intuitively, the cross-graph structure represented in the assignment graph is helpful to rectify the inappropriate matching. Therefore, we propose a structure-enhanced graph matching network (SEGMN). Equipped with a dual embedding learning module and a structure perception matching module, SEGMN achieves structure enhancement in both embedding learning and cross-graph matching. The dual embedding learning module incorporates adjacent edge representation into each node to achieve a structure-enhanced representation. The structure perception matching module achieves cross-graph structure enhancement through assignment graph convolution. The similarity score of each cross-graph node pair can be rectified by aggregating messages from structurally relevant node pairs. Experimental results on benchmark datasets demonstrate that SEGMN outperforms the state-of-the-art GSC methods in the GED regression task, and the structure perception matching module is plug-and-play, which can further improve the performance of the baselines by up to 25%.","Graph similarity learning refers to learning the similarity score between two graphs, having extensive applications, such as code detection (Dai 2023; Bibi et al. 2023), molecular graph similarity (Coupry and Pogány 2022) and image matching (Wang, Yan, and Yang 2021). Graph Edit Distance (GED) (Sanfeliu and Fu 1983) and Maximum Common Subgraph (MCS) (Bunke 1997) are two widely used measurements, where GED refers to the minimum number of operations that convert one graph to the other and MCS is the largest subgraph which is simultaneously isomorphic to both graphs. However, the computation of GED and MCS is attributed to NP-complete problem (Wang et al. 2021; Bai et al. 2021). Traditional algorithms like Hungarian (Riesen and Bunke 2009) and A* (Neuhaus, Riesen, and Bunke 2006) can compute GED accurately but at the cost of high computational complexity. The prevailing of Graph Neural Networks (GNNs) (Kipf and Welling 2016) promotes the development of deep graph similarity learning (Jiang, Ning, and Dong 2023). Early studies generally learn graph-level representation for comparison (Ktena et al. 2017). To achieve higher granularity matching through node-level comparison, subsequent methods adopt GNNs for node embedding, followed by various cross-graph matching strategies, which can be generally divided into two categories. One is cross-graph attention (Ling et al. 2021; Jin et al. 2022; Tao et al. 2023) that captures abundant node-graph interactions to provide each node in one graph with the other graph’s node representations. The other is to directly extract features of the similarity matrix by comparing the similarity score of each cross-graph node pair (Bai et al. 2019, 2020; Tan et al. 2023). Moreover, there appear a few methods constructing structure-enhanced representations for structure matching. Some adopt the attention mechanism including distance information to enhance node embeddings (Tan et al. 2023), and others encode edges to directly represent the connection condition (Tan et al. 2022; Roy et al. 2022). However, the edge embedding is only for graph-level representation or subgraph construction. Despite various matching strategies, existing methods have two main drawbacks. (1) Representation limitation. A comprehensive representation enables multi-view matching. Most methods only use simple node embeddings without highlighting the edge representation, which is crucial for structure matching. (2) Matching inadequacy. Most methods conduct diverse cross-graph matching strategies on node embeddings from GNN. However, the node embeddings are confined to intra-structures as message passing is applied to their respective graphs. Therefore, directly comparing cross-graph node embeddings lacks the matching perception of the whole graph pair structure, thus causing the unreasonable similarity score. Intuitively, the cross-graph structure relationship provides the inter-graph structure relationship and helps rectify the unreasonable similarity score. According to the cross-graph structure relationship, two cross-graph node pairs are believed structurally relevant when their corresponding nodes in the same graph are adjacent. The concrete cross-graph structure relationship can be described by the assignment graph detailed in Definition 3. A toy example is given in Fig. 1. In this example, the nodes of G1subscript𝐺1G_{1}italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and G2subscript𝐺2G_{2}italic_G start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are embedded in a latent space by a GNN, where node v1superscript𝑣1v^{1}italic_v start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT is close to node vasuperscript𝑣𝑎v^{a}italic_v start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT in the space as they are structurally similar. Their respective neighbor nodes v3superscript𝑣3v^{3}italic_v start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT and vcsuperscript𝑣𝑐v^{c}italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT should be relatively close from the view of the whole structure of the graph pair while they are initially not. Cross-graph node pair (v3,vc)superscript𝑣3superscript𝑣𝑐(v^{3},v^{c})( italic_v start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) and (v1,va)superscript𝑣1superscript𝑣𝑎(v^{1},v^{a})( italic_v start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ) are structurally relevant as v1superscript𝑣1v^{1}italic_v start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and v3superscript𝑣3v^{3}italic_v start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, vasuperscript𝑣𝑎v^{a}italic_v start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT and vcsuperscript𝑣𝑐v^{c}italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT are both adjacent in G1subscript𝐺1G_{1}italic_G start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and G2subscript𝐺2G_{2}italic_G start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. Based on the cross-graph structure relationship, the similarity score of (v3,vc)superscript𝑣3superscript𝑣𝑐(v^{3},v^{c})( italic_v start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) can be increased by simply aggregating the similarity score of (v1,va)superscript𝑣1superscript𝑣𝑎(v^{1},v^{a})( italic_v start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ), which is quite high. So the unreasonable similarity score of (v3,vc)superscript𝑣3superscript𝑣𝑐(v^{3},v^{c})( italic_v start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) can be rectified. Figure 1: An example of cross-graph structural correction. The figure shows a pair of graphs with their partial assignment graph and node embedding space. The closer the two embeddings are in the space, the more similar they are supposed to be. S1⁢asubscript𝑆1𝑎S_{1a}italic_S start_POSTSUBSCRIPT 1 italic_a end_POSTSUBSCRIPT, S3⁢csubscript𝑆3𝑐S_{3c}italic_S start_POSTSUBSCRIPT 3 italic_c end_POSTSUBSCRIPT represent the similarity scores of node pair (v1,va)superscript𝑣1superscript𝑣𝑎(v^{1},v^{a})( italic_v start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT ) and (v3,vc)superscript𝑣3superscript𝑣𝑐(v^{3},v^{c})( italic_v start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT , italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ). To overcome the above drawbacks in previous works, we design a structure-enhanced framework, which achieves structure matching from the perspective of both representation and matching. On one hand, we propose a dual embedding approach that adds the structure perspective from the adjacent edges of each node. Firstly, the node graph is transformed into its corresponding line graph (detailed in Definition 2) with edge features. Then, an edge-enhanced GCN is applied to the line graph to learn node embedding (i.e., edge embedding in the node graph). Finally, each node embedding is concatenated to the aggregated embeddings of its adjacent edges in the node graph to generate the ultimate dual embedding. On the other hand, we introduce a structure perception matching module based on the structure-enhanced algorithm to augment the cross-graph similarity matrix. The assignment graph is adopted to obtain the neighbor relationships of each cross-graph node pair in their respective graph, and the similarity score of each node pair is convolved with that of its adjacent node pairs in the assignment graph to enhance the structure matching between graphs. Our contributions can be summarized as follows: • A structure-enhanced graph matching framework is proposed, which fully utilizes the graph structure from the perspective of both representation and matching. • Instead of aggregating messages only from nodes, a dual embedding learning module is proposed which can additionally aggregate adjacent edge representation into each node for a structure-enhanced representation. • Different from previous methods, our proposed structure perception matching module introduces the cross-graph structure to enhance matching by aggregating the similarity score on the assignment graph. It is a plug-to-play module that can be applied to most GSC methods. • Experiments on the benchmark datasets show that our method outperforms the state-of-the-art methods. The effectiveness of the proposed modules is also verified."
https://arxiv.org/html/2411.03484v1,"Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature","Automated knowledge extraction from scientific literature can potentially accelerate materials discovery. We have investigated an approach for extracting synthesis protocols for reticular materials from scientific literature using large language models (LLMs). To that end, we introduce a Knowledge Extraction Pipeline (KEP) that automatizes LLM-assisted paragraph classification and information extraction. By applying prompt engineering with in-context learning (ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve chemical information from PDF documents, without the need for fine-tuning or training and at a reduced risk of hallucination. By comparing the performance of five open-source families of LLMs in both paragraph classification and information extraction tasks, we observe excellent model performance even if only few example paragraphs are included in the ICL prompts. The results show the potential of the KEP approach for reducing human annotations and data curation efforts in automated scientific knowledge extraction.","Reticular materials are a class of crystalline, porous materials made of molecular building blocks that are linked by strong chemical bonds [1]. They exhibit exceptional properties due to their highly porous structure, high surface area, tunable pore sizes and morphologies [2]. Their versatility is evidenced by a broad range of industrial applications, among them heterogeneous catalysis [3], energy storage [4], water treatment [5], chemical sensing [6], heat transfer [7], gas capture [8] and drug delivery [9]. Following recent advances in generative AI, several models have been proposed to explore the large chemical space covered by reticular materials [10, 11, 12, 13, 14]. These models aim to generate reticular structures with optimized properties. Such structures are hypothetical as they have not yet been synthesised and tested in the lab. Devising a synthesis protocol for computationally generated structures requires a subject matter expert (SME). This is, however a challenging task given the large number of possible structures. An AI model that correlates a computationally discovered material with a lab synthesis protocol is, therefore, highly desirable. A first step towards the creation of such a model is building a database of existing synthesis protocols. One approach for creating such database is applying information extraction techniques to the existing body of scientific literature. A large number of reticular materials have been reported in the literature alongside their respective synthesis protocols [15, 16]. It is worth noting, however, that overlapping discoveries are common, given that the same material can be produced by means of different synthesis protocols [17]. Transfer learning has been suggested as means to improve information extraction on existing corpora of scientific texts related to materials [18]. For example, fine-tuning techniques allow for adapting existing general-purpose AI models to specific tasks in domains for which comparatively little data exists. However, recent developments in LLMs have enabled information extraction based on prompt engineering and few-shot learning tasks [19]. In this paper, we propose using large language models (LLMs), without the need for additional training or fine-tuning, for extracting synthesis protocols of reticular materials from scientific literature, i.e., unstructured PDF documents. We use prompt engineering with in-context learning (ICL) [20] for providing in the prompt all the context needed by the LLM to process the instructions. Together with instructions and input data, we provide examples that guide the LLM output production. This technique reduces the risk of hallucination, since all the context needed to execute the instruction is provided within the prompt. Also, it accelerates the process of information extraction because it does not require SME-based annotation of thousands of sentences/paragraphs for fine-tuning the models. Our domain-independent Knowledge Extraction Pipeline (KEP) uses LLMs for extracting relevant information from PDF documents. The pipeline is composed of four main modules: (i) PDF extractor: processes the PDF to extract the text; (ii) Paragraph classification: processes the text in order to select only the relevant paragraphs (i.e., paragraphs that have the information the user is interested in); (iii) Information extraction: processes the relevant paragraphs and extract the relevant information; and (iv) Knowledge representation: interprets and assigns meaning to the information while representing the related knowledge. The pipeline uses LLMs with prompt-engineering and ICL in two modules, namely paragraph classification and information extraction, which are the focus of this paper. In addition, for identifying the best set of examples to be used in the prompts of these two modules, we propose the Examples selection phase. This phase measures the performance of the LLMs in a given task and, by using different sets of examples, identifies the set to be used for optimal LLM performance. We have used five families of LLMs in both paragraph classification and information extraction modules and have compared their performance. We note that these open-source LLMs are not domain-specific and were not fine-tuned for our tasks. Our experiments indicate that: (i) even without fine-tuning or training, some of these models have achieved high performance in case ICL was used to provide examples in the prompt; (ii) the examples used in the prompt affect model performance and, hence, must be chosen carefully; and (iii) the same set of examples may lead to varying results if used in different models. Some recent papers share our work’s objectives, however, they differ methodologically [19, 21, 22, 23]. For example, Polak et al. (2024) [19] reported a pipeline for extracting information from unstructured text in the material discovery domain using language models. However, the cited work focused on simple extraction tasks, e.g., material, value and unit, while our pipeline is aimed at complex information associated with synthesis protocols that require additional classification. Unlike in our approach which is based on few-shot prompts providing examples for facilitating the information extraction, the cited work applies zero-shot methods for determining the relevance of sentences or paragraphs. Huo et al. (2019) [21] introduced a semi-supervised machine learning approach for classifying inorganic materials synthesis steps in scientific papers. The authors used the Latent Dirichlet Allocation (LDA) unsupervised topic modeling algorithm for clustering terms that are typically used in synthesis descriptions. A random forest classifier, based on annotations of hundreds of paragraphs, categorized the occurring synthesis types. This approach also used a Markov chain for modeling the sequence of steps, creating flowcharts of synthesis procedures. In Kononova et al. (2019) [22], the authors generated a dataset with “codified recipes” for solid-state synthesis which was automatically extracted from scientific publications using traditional text mining and natural language processing approaches. The authors used the two-step paragraph classification approach described in Huo et al. (2019) [21] for finding paragraphs on solid-state synthesis. The extraction pipeline consisted of several algorithms (BiLSTM-CRF, Material Parser, etc.) for identifying materials related information, including synthesis steps and conditions. Compared to our method, the cited work required considerable annotation effort and employed a less straightforward extraction pipeline. We note that our method relies primarily on the LLM capabilities for text understanding, without specialized tokenizers or entity recognizers. Finally, Park et al. (2022) [23] created a four-step pipeline, with text extraction from XML/HTML or PDF files and classifying relevant paragraphs, performing named entity recognition and, a fully connected multi-layer with dropout as classifier. Another promising, less related approach is using “AI chatbot agents” for assisting materials scientists in specific pipeline tasks. In reference [24], the authors used prompt engineering for guiding a ChatGPT-based bot to extract MOF synthesis information from various sources. The authors leveraged a bot-like interface for answering questions about synthesis procedures and chemical reactions. In reference [25], the authors leveraged multiple AI assistants, such as LLMs and specific ML algorithms, as lab assistants to support a human SME, enabling productivity levels similar to those of an entire research team. While the approach was not fully automated, it provided a proof-of-concept of how language models can be leveraged for accelerating materials discovery. The remainder of this paper is organized as follows. Section 2 introduces the use case, Section 3 describes in details the pipeline applied to the use case and Section 4 presents our experiments. Section 5 concludes and presents some future work."
https://arxiv.org/html/2411.03364v1,DM4Steal: Diffusion Model For Link Stealing Attack On Graph Neural Networks,"Graph has become increasingly integral to the advancement of recommendation systems, particularly with the fast development of graph neural network (GNN). By exploring the virtue of rich node features and link information, GNN is designed to provide personalized and accurate suggestions. Meanwhile, the privacy leakage of GNN in such contexts has also captured special attention. Prior work has revealed that a malicious user can utilize auxiliary knowledge to extract sensitive link data of the target graph, integral to recommendation systems, via the decision made by the target GNN model. This poses a significant risk to the integrity and confidentiality of data used in recommendation system. Though important, previous works on GNN’s privacy leakage are still challenged in three aspects, i.e., limited stealing attack scenarios, sub-optimal attack performance, and adaptation against defense. To address these issues, we propose a diffusion model based link stealing attack, named DM4Steal. It differs previous work from three critical aspects. (i) Generality: aiming at six attack scenarios with limited auxiliary knowledge, we propose a novel training strategy for diffusion models so that DM4Steal is transferable to diverse attack scenarios. (ii) Effectiveness: benefiting from the retention of semantic structure in the diffusion model during the training process, DM4Steal is capable to learn the precise topology of the target graph through the GNN decision process. (iii) Adaptation: when GNN is defensive (e.g., DP, Dropout), DM4Steal relies on the stability that comes from sampling the score model multiple times to keep performance degradation to a minimum, thus DM4Steal implements successful adaptive attack on defensive GNN. Extensive experiments on eight benchmark datasets, three graph neural network models, and six attack scenarios show that DM4Steal achieves the state-of-the-art (SOTA) performance compared with four baselines in aspects of AUC (∼similar-to\sim∼ ×\times×1.12).","Graph-structured data, such as social networks [22], biological networks [13], and transportation networks [38, 18] possesses powerful characterization capabilities for representing complex relationships among various objects [37, 33, 34]. Due to an increasing number of graph-structured data being collected, graph mining plays a significant role in wide-range areas. Regarded as intellectual property of data holders, the preservation of sensitive data, e.g., private personal information and essential relations, within graph should be considered in practical scenarios. Such typical examples could be found in recommendation system [29] and online transaction system [16]. Recent work has revealed a heightened reliance on graph data for training graph neural network (GNN) [14, 32, 27], leading to the substantial incorporation of private data within graph neural networks. More specifically, a malicious user could reversely inference the graph data used in training by accessing the target graph neural network model and leveraging specific auxiliary data. For example, as shown in Figure 1, a malicious user could infiltrate a user’s credit rating system and utilizes auxiliary knowledge (e.g., partial graphs, user attributes, and shadow datasets) to deduce a transactional relationship between two users based on varying rating results. Great efforts have been made to compromise the privacy of GNN [6, 30]. According to different attack goals, current work can be roughly cast into four categories, i.e., membership inference attacks [6, 23, 12], attribute inference attacks [6], link stealing attacks [6, 11], and model stealing attacks [30]. In particular, the link stealing attack seeks to reconstruct the connections between various nodes in the target graph data using publicly available embedded data or labels. For instance, we consider a scenario depicted in Figure 1, where a bank employs a user transaction network to assess their creditworthiness. However, a malicious user could exploit the user ratings to illicitly obtain transaction records, resulting in privacy breaches and detrimental consequences. Since link stealing attack [6, 11] makes a practical threat, we predominantly focus on it. Figure 1: An example of link stealing attack on credit rating system. The system utilizes a user’s transaction network as well as the user’s properties to rate the user’s credit. With limited auxiliary knowledge (e.g., partial graphs, user attributes, and shadow datasets), a malicious attacker could successfully steal the user’s transaction network by query results of the credit rating system. Nowadays, numerous graph reconstruction attacks have been proposed to reveal the privacy leakage threat of GNN. Although most of them show satisfying attack performance in different attack scenarios respectively, they are still challenged by three objectives. (i) Generalizability: existing attack methods’ performance is specific to a particular scenario but difficult to transfer to others, leading to significant attack degradation or even failure. (ii) Effectiveness: existing methods through similarity [6] [11] (i.e., similarity of node embeddings) and influence [31] (i.e., influence between nodes) tend to falsely determine node pairs without links, such as second-order neighbors, as having links due to the feature aggregation of GNN, leading to low attack AUC. (iii) Adaptation: existing link inference attack methods generally overlook the privacy-preserving defense for GNN, hindering their usability to practical defensive GNN. To address these challenges, we propose a novel generative framework for link stealing attack based on diffusion model, dubbed as DM4Steal. Since the diffusion model has shown outstanding performance in two aspects, i.e., permutation invariance and noise adaptation, we believe the model is able to accurately capture the permutation invariance of graphs intuitively. And it is able to reconstruct the original graphs from graphs with noise. Specifically, to tackle challenge (i), we propose a novel training strategy for the diffusion model, which enables the graph embeddings obtained from the target model to be used to train the diffusion model in addition to the node features, allowing it to be applied to the attack scenarios of the six species. To address challenge (ii), we utilize the score matching mechanism, whose inference process is not affected by the target model aggregation mechanism can achieve permutation invariance, instead of the previous similarity as well as influence mechanisms to perform link inference on the target graphs, thus reducing the impact of the GNN aggregation mechanism, and making the reversed graph more precise. For the challenge (iii), due to the addition of Gaussian noise to the target image during the forward process of diffusion models, this can overshadow the noise introduced by privacy protection mechanisms, weakening the impact of privacy protection noise and thereby enhancing the adaptability of DM4Steal. The main contributions of this paper are summarized as follows: ∙∙\bullet∙ Problem: we propose six attack scenarios of link stealing attack on GNN by summarizing the auxiliary knowledge (i.e., node features, partial subgraph, and shadow datasets). These diverse scenarios cover possible practical threats of link stealing. ∙∙\bullet∙ Approach: to the best of our knowledge, DM4Steal is the first link stealing attack method that utilizes a diffusion model to learn graph network topology and reduce instability caused by similarity or improper thresholds, thus it holds the advance of generality, effectiveness, and adaptiveness. ∙∙\bullet∙ Strategy: in order to make DM4Steal applicable to all six attack scenarios, we proposed the graph diffusion model’s generation strategy to adaptively generate graphs based on different auxiliary knowledge. ∙∙\bullet∙ Evaluation: extensive experiments on three GNNs over eight real-world datasets demonstrate that DM4Steal achieves the state-of-the-art (SOTA) attack performance. Moreover, It can conducts effective link stealing attack on defensive GNNs as well."
https://arxiv.org/html/2411.03143v1,Self-supervised Hierarchical Representation for Medication Recommendation,"Medication recommender is to suggest appropriate medication combinations based on a patient’s health history, e.g., diagnoses and procedures. Existing works represent different diagnoses/procedures well separated by one-hot encodings. However, they ignore the latent hierarchical structures of these medical terms, undermining the generalization performance of the model. For example, “Respiratory Diseases”, “Chronic Respiratory Diseases” and “Chronic Bronchiti” have a hierarchical relationship, progressing from general to specific. To address this issue, we propose a novel hierarchical encoder named HIER to hierarchically represent diagnoses and procedures, which is based on standard medical codes and compatible with any existing methods. Specifically, the proposed method learns relation embedding with a self-supervised objective for incorporating the neighbor hierarchical structure. Additionally, we develop the position encoding to explicitly introduce global hierarchical position. Extensive experiments demonstrate significant and consistent improvements in recommendation accuracy across four baselines and two real-world clinical datasets.","Medication recommendation systems understand patients’ health status from electronic health records (EHRs) and tailor a safe medication combination. In EHRs, each patient’s medical history is documented as a series of visits, typically involving diagnoses, procedures and medications. Existing works often represent diagnoses and procedures with one-hot encodings. The model then learns their embeddings in a low-dimensional space from patient data [24, 19, 25]. This embedding method organizes symbolic objects (e.g., words, entities and concepts) and has proven successful in numerous applications [3, 4, 7]. In healthcare, a universal language—namely, a standard coding system—is used to categorize diseases and conditions hierarchically. This hierarchical structure enables constraints to be imposed on the embedding space, allowing the model to effectively learn commonalities within each disease category. Recent studies in medication recommendation mainly focus on integrating drug-related knowledge into the models, such as drug-drug interaction graphs [30, 31], molecular structures of drugs [24, 25], and medical knowledge graphs linking diseases with medications [28]. These methods improve the accuracy and safety of recommendations by enhancing medication representations. However, the representation of diagnoses and procedures, along with their hierarchical structures, remains underexplored. In medication recommendations, patient heterogeneity introduces a wide range of diagnoses and procedures, making it difficult for the model to effectively learn their feature representations from scratch. For example, in the clinical dataset MIMIC-III [9], there are only 5,449 patients with at least two visits, yet exhibit 4,491 unique diagnoses and 1,412 unique procedures. On the other hand, the current models employ pooling operations (e.g., sum-pooling) to aggregate multiple diagnosis/procedure features as patient representation. Then, the model is optimized to match patients with medications, overlooking the structural properties (hierarchies) of diagnosis and procedure. We present the theoretical analysis in the following section. Due to the above issues, this paper leverages prior hierarchies to construct hierarchical representations rather than learning their complicated relationships from data. In clinical practice, it is a well-recognized principle that patients with similar diagnoses are often prescribed similar medications. Motivated by this, we highlight the importance of the hierarchical structures of diagnoses and procedures and exploit them to obtain more accurate representations. Our approach is grounded in the use of standardized medical codes, in which diagnoses and procedures exhibit a tree-like structure (hierarchical structure) from general to specific, corresponding to their conceptions. To be more specific, each diagnosis and procedure is tagged by a unique code, i.e., ICD coding111The International Classification of Diseases (ICD) is an international classification system used by healthcare providers to code and categorize diseases, conditions, etc. in EHRs. However, current methods map diagnoses and procedures directly to one-hot encodings, which unfortunately disregard their hierarchical structure. To bridge the research gap, we propose the Hierarchical encoder named HIER for diagnosis and procedure representations guided by medical codes. We explicitly equip the model with hierarchical structures to enhance their representations and improve recommendation accuracy. Specifically, the proposed hierarchical encoder consists of two components: Relation Embedding and Position Encoding. The relation embedding aggregates their parental categories and is optimized with a self-supervised objective to capture neighbor hierarchical structure. The position encoding, on the other hand, is derived from segmented medical codes to introduce global hierarchical positions. Integrating these two components enables us to obtain hierarchical representations that are compatible with any medication recommendation model. The main contributions of this paper are as follows: • We demonstrate, for the first time, that state-of-the-art medication recommendation models fail to adequately capture the structural relations of diagnoses and procedures. • We introduce a novel hierarchical encoder, guided by medical codes, to integrate hierarchical structure into the representation of medical terms (e.g., diagnoses and procedures). This hierarchical representation is universal and independent of the training process. • We conduct comprehensive experiments to validate the effectiveness of our method, demonstrating its significant and consistent performance improvements across four baselines and two real-world datasets."
https://arxiv.org/html/2411.02992v1,Efficient and Effective Adaptation of Multimodal Foundation Models in Sequential Recommendation,"Multimodal foundation models (MFMs) have revolutionized sequential recommender systems through advanced representation learning. While Parameter-efficient Fine-tuning (PEFT) is commonly used to adapt these models, studies often prioritize parameter efficiency, neglecting GPU memory and training speed. To address this, we introduced the IISAN framework, significantly enhancing efficiency. However, IISAN was limited to symmetrical MFMs and identical text and image encoders, preventing the use of state-of-the-art Large Language Models. To overcome this, we developed IISAN-Versa, a versatile plug-and-play architecture compatible with both symmetrical and asymmetrical MFMs. IISAN-Versa employs a Decoupled PEFT structure and utilizes both intra- and inter-modal adaptation. It effectively handles asymmetry through a simple yet effective combination of group layer-dropping and dimension transformation alignment. Our research demonstrates that IISAN-Versa effectively adapts large text encoders, and we further identify a scaling effect where larger encoders generally perform better. IISAN-Versa also demonstrates strong versatility in our defined multimodal scenarios, which include raw titles and captions generated from images and videos. Additionally, IISAN-Versa achieved state-of-the-art performance on the Microlens public benchmark. We will release our code and datasets to support future research.","Recent advancements in recommendation algorithms have demonstrated that utilizing powerful large language models (LLMs) and vision encoders like GPT-4o111https://openai.com/index/hello-gpt-4o/, GPT-4 [1], DALL-E [2], Llama [3], and CLIP [4] can obtain a state-of-the-art recommendation performance [5, 6, 7, 8, 9]. These types of models typically combine a recommender model as the user encoder with multimodal foundation models (MFMs)222The definition of multimodal foundation models (MFMs) may vary. In this paper, we focus on separately pre-trained text and image encoders, following the approach in [10, 4], where the representations of image and text are learned in an end-to-end manner. Other MFM paradigms, such as [11, 12], which use a large language model (LLM) as the backbone and input image encoder features into the LLM, are beyond the scope of this paper. to encode the items. Many studies [5, 6, 7] have shown that fine-tuning the item encoder can lead to optimal performance. However, these approaches are highly inefficient due to the immense computational resources required to fine-tune such large encoders. A popular paradigm [13] for resolving the efficiency problem includes methods such as Adapter [14], LoRA [15], Bitfit [16], and Compacter [17]. These approaches integrate tunable neural network modules into the backbone multimodal foundation models, collectively referred to as embedded parameter-efficient fine-tuning (EPEFT) [18]. While EPEFT methods have gained popularity for addressing the efficiency of trainable parameters, they still face practical efficiency challenges, such as training time and GPU memory consumption. The introduction of the IISAN [18], with its decoupled structure and caching strategy for multimodal encoders, addresses these practical efficiency issues, outperforming both traditional full fine-tuning and EPEFT methods in terms of both performance and efficiency. We refer to this decoupled, parameter-efficient fine-tuning approach as DPEFT. It significantly reduces GPU memory usage by up to 15 times compared to FFT and 12 times compared to EPEFT (Adapter/LoRA). Additionally, it accelerates training time per epoch by up to 20 times compared to FFT and 16 times compared to EPEFT. IISAN leverages both intra- and inter-modal information from multimodal foundation models by exploiting the hidden states within the layers of backbone models to achieve fine-grained information merging. Despite these advancements, merging multimodal hidden states from two different encoders presents two limitations: (1) it can only be conducted for symmetrical multi-modal transformer encoders; (2) due to the first limitation, it is hard to explore whether scaling the text encoder in IISAN with recent state-of-the-art LLMs could yield performance improvements. These limitations do not align with the current research trend, where text encoders are often larger and more complex than visual transformers, as shown in Figure 1, which compares the state-of-the-art text and visual transformers. While vision transformers like ViT-e [19], ViT-G [20], and ViT-22B [21] have billions of parameters, they are all closed-source models. Therefore, the most commonly used vision transformers currently have fewer than 1 billion parameters [22]. Normally, larger pre-trained transformers generally offer better performance according to the scaling effect [23]. To maximize the potential of pre-trained models, it is advantageous to use larger models. However, finding a vision transformer that matches the size of a text encoder (LLM) is nearly impossible due to the disparity in model sizes. Therefore, addressing the issue of asymmetrical merging is of paramount importance. Figure 1: Comparisons of the model sizes of various popular text transformers and visual transformers using a logarithmic scale. The close-sourced is marked as ”Close-sourced” in the figure, the other are all open-sourced models. In this paper, we present an extension of the IISAN framework [18] called the Intra- and Inter-model Side Adapted Network for Versatile Multimodal Representation (IISAN-Versa). We specifically categorize IISAN-Versa into two variants: the symmetrical IISAN-Versa (IISAN-VS), which follows the standard IISAN configuration, and the newly proposed asymmetrical IISAN-Versa (IISAN-VA), which is capable of incorporating a larger text encoder to address asymmetry between text and vision pre-trained models. For IISAN-VA, we address the challenge of asymmetric multimodal backbone networks by proposing a simple yet effective strategy that combines group layer-dropping with dimension transformation alignment. The IISAN-VA variant, with a larger text encoder, significantly outperforms the standard symmetrical IISAN-VS, which uses BERT-base text encoders, while maintaining high efficiency. This demonstrates IISAN-Versa’s effectiveness in accommodating a larger and more powerful model. Moreover, we extend and validate the IISAN-Versa framework to a wider range of multimodal scenarios, particularly focusing on multimodal text, thereby validating its scalability. This extension involves the integration of diverse text forms (named multimodal text), such as titles, text captions from image covers, and video content, utilizing an open-source video recommendation dataset MicroLens [24]. We summarize the main changes made in this study compared to our published conference version in SIGIR2024 [18] as follows: • We revise the Introduction and Related Work sections to emphasize the motivation of achieving the versatility of the new IISAN-Versa, rather than focusing only on the efficiency of IISAN. • We expand the methodology section by categorizing IISAN [18] as the symmetrical IISAN-Versa (IISAN-VS) and introduced the asymmetrical IISAN-Versa (IISAN-VA) to incorporate LLMs as encoders. • We validate the effectiveness of the IISAN-VA, which incorporates state-of-the-art LLMs as text encoders, demonstrating significant performance improvements. Furthermore, we fully explore the effect of scaling laws on text encoders, observing that larger pre-trained text encoders typically lead to better performance. • To gain a clearer understanding of efficiency improvement during the training process, this paper analyzes it from the perspectives of forward and backward propagation. • We further validate IISAN-Versa’s performance on Microlens, a public multimodal recommendation benchmark, achieving state-of-the-art performance. • We also utilize pre-trained captioning models to generate multimodal texts from raw images and videos from Microlens and validated IISAN-Versa’s versatility in this multimodal text scenario. Our main contributions are the following: • We propose a versatile paradigm, IISAN-Versa, which adapts existing mainstream symmetric and asymmetric multimodal foundation models to enable efficient and effective multimodal sequential recommendation. It ensures the flexibility of multi-structure model adaptation and the efficiency of PEFT through minimal modifications of dimension transformation layers with LayerDrop. • We identify a scaling effect in our novel asymmetric IISAN-Versa by scaling the text encoder from a smaller language model to a larger language model. This provides a new inspiration that scaling LLMs with appropriate approaches, e.g. our IISAN-Versa, can effectively and efficiently improve multimodal recommendation. • We construct a new multimodal text recommendation scenario by extending the open-source Microlens dataset, i.e., generating captions from its original videos and images using a pre-trained video captioning generator, to validate the multi-scenario adaptation capabilities of the proposed IISAN-Versa. The new reconstructed dataset will be released to facilitate future research for the multimodal recommendation community."
https://arxiv.org/html/2411.02959v1,HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems,"Retrieval-Augmented Generation (RAG) has been shown to improve knowledge capabilities and alleviate the hallucination problem of LLMs. The Web is a major source of external knowledge used in RAG systems, and many commercial systems such as ChatGPT and Perplexity have used Web search engines as their major retrieval systems. Typically, such RAG systems retrieve search results, download HTML sources of the results, and then extract plain texts from the HTML sources. Plain text documents or chunks are fed into the LLMs to augment the generation. However, much of the structural and semantic information inherent in HTML, such as headings and table structures, is lost during this plain-text-based RAG process. To alleviate this problem, we propose HtmlRAG, which uses HTML instead of plain text as the format of retrieved knowledge in RAG. We believe HTML is better than plain text in modeling knowledge in external documents, and most LLMs possess robust capacities to understand HTML. However, utilizing HTML presents new challenges. HTML contains additional content such as tags, JavaScript, and CSS specifications, which bring extra input tokens and noise to the RAG system. To address this issue, we propose HTML cleaning, compression, and pruning strategies, to shorten the HTML while minimizing the loss of information. Specifically, we design a two-step block-tree-based pruning method that prunes useless HTML blocks and keeps only the relevant part of the HTML. Experiments on six QA datasets confirm the superiority of using HTML in RAG systems 111Code and datasets are available at https://github.com/plageon/HtmlRAG..","Large Language Models (LLMs) have been proven to have powerful capabilities in various natural language processing tasks (Patel et al., 2023; Ouyang et al., 2022; OpenAI, 2023). However, at the same time, LLMs show deficiencies such as forgetting long-tailed knowledge (Kotha et al., 2024), offering outdated knowledge (Amayuelas et al., 2024), and hallucination (Zhou et al., 2024c; Mallen et al., 2023; Min et al., 2023). Retrieval-augmented generation (RAG) utilizes a retrieval system to fetch external knowledge and augment the LLM. It has proved effective in mitigating hallucinations of LLMs (Zhou et al., 2024a; Ni et al., 2024). Many RAG systems, such as Perlexity (PerplexityAI, 2024) and SearchGPT (OpenAI, 2024), have been developed, and they commonly use Web search engines as the underlying retrieval systems. Figure 1. Information loss in HTML to plain text conversion. Traditional RAG pipelines typically use plain text as the format for retrieved knowledge (Wang et al., 2024c; Jin et al., 2024a). HTML documents from the Web are often converted into plain text and concatenated with the user’s query before being fed into the LLM. We found that converting HTML to plain text leads to the loss of structural and semantic information. Figure 1 illustrates that a web page containing tabular form becomes disordered when converted to plain text. Even worse, original HTML tags, such as “¡code¿” and “¡a¿”, denoting important information, are discarded during conversion. Thus, in this paper, we tend to investigate an intuitive idea: Can we take HTML as the format of external knowledge in RAG systems to preserve the information in HTML documents to a larger extent? Taking HTML as the format of external knowledge offers several advantages beyond preserving the information inherent in HTML documents. During pre-training, LLMs have encountered HTML documents (Gur et al., 2023; Groeneveld et al., 2024; Biderman et al., 2023), which means that they inherently possess the ability to understand HTML without requiring further fine-tuning (Zheng et al., 2024; Kim et al., 2023a). Recently, both proprietary and open source LLMs have begun to support increasingly longer input windows, making it feasible to input more extensive HTML documents (Zeng et al., 2024; Dong et al., 2023; Zhang et al., 2024). Furthermore, documents in Latex, PDF, and Word formats can be converted to HTML with minimal loss, expanding the potential application of HTML as the format of external knowledge (Wang et al., 2023; Bruce R. Miller, 2024; Williamson et al., 2024). However, employing HTML as the knowledge format for LLMs also presents the challenge of handling longer input sequences and noisy contexts. Our preliminary experiments show that a real HTML document from the Web contains over 80K tokens on average, among which over 90% of the tokens are CSS styles, JavaScript, Comments, or other meaningless tokens. Compared to the common maximum context window of current LLMs, which ranges from 32K to 128K, an individual document length of 80K is unacceptable. The noisy tokens. The aforementioned meaningless tokens in HTML documents can also affect the generation quality of LLMs. To solve this problem, in this paper, we devise a HTML Cleaning module to remove semantically irrelevant content in HTML documents, while keeping the main content intact. We also adjust the HTML tree structure without losing semantic information, for example, merging multiple layers of single nested HTML tags and removing empty tags. These processes reduce the length of the HTML to 6% of its original size. Even after cleaning, HTML documents remain relatively long (over 4K each) to LLMs. To shorten the input context and remove the noise contained in the original retrieved documents, existing RAG systems have utilized different types of post-retrieval result refiners (Zhou et al., 2024b; Jin et al., 2024b; Xu et al., 2024; Jiang et al., 2024). These refiners extract the relevant text chunks or key sentences from the documents, regarding the user’s query and LLMs’ preference, and discard other content. These plain-text-based refiners cannot be directly applied to HTML because simply chunking HTML without considering its structure may generate unreasonable chunks. Hence, we further design an HTML Pruning module, which functions upon the intrinsic tree structure of HTML. The pruning process is comprised of the following steps: (1) Building a Block Tree. Each HTML document can be parsed into a DOM tree (W3Schools, 2024). We do not simply prune HTML on the DOM tree because it is too finely-grained (Guo et al., 2022; Wang et al., 2022), which brings much computational cost. Instead, we propose to build a corresponding block tree, in which the original DOM tree nodes are merged into hierarchical blocks. The granularity of the block tree can be adjusted by the degree of merging. (2) Pruning Blocks based on Text Embedding. We then prune the block tree using an on-the-shelf embedding model, because it is a simple but effective way to calculate the block’s relevance scores with the user’s query based on their embedding similarity. We apply a greedy pruning algorithm that removes blocks with lower similarity scores, and gets a pruned block tree. However, we observe that the embedding model may fail to work well with the fine-grained blocks because embeddings learned for these small blocks are usually vague and inaccurate, so this pruning step is limited to coarse-grained block trees. (3) Generative Fine-grained Block Pruning. To prune the block tree further, we expand the leaf nodes of the pruned block tree and build a finer-grained block tree. Since the generative model has a longer context window, it can model the block tree globally and is not limited to modeling one block at a time. Thus we further develop a generative model to prune HTML over the fine-grained blocks. The generative model is supposed to calculate the score for each block, which is given by the generation probability of a unique sequence indicating the block. The sequence is given by the path of HTML tags, starting from the root tag and walking down to the block’s tag and text (e.g., “¡html¿¡body¿¡div¿¡p¿block content…”). Finally, according to the block scores, we apply a similar greedy pruning algorithm to get the final pruned HTML. We conduct extensive experiments on six datasets including ambiguous QA, natural QA, multi-hop QA, and long-form QA. Experimental results confirm the superiority of HTML as the format of external knowledge over plain text. Our contributions are threefold: (1) We propose to take HTML as the format of knowledge in RAG systems, which retains information of the original HTML; (2) We propose a simple but effective HTML cleaning algorithm; (3) We propose a two-stage HTML pruning algorithm. This can be applied to most RAG systems and strikes a balance between efficiency and effectiveness."
https://arxiv.org/html/2411.02790v1,Memory Augmented Cross-encoders for ControllablePersonalized Search,"Personalized search represents a problem where retrieval models condition on historical user interaction data in order to improve retrieval results. However, personalization is commonly perceived as opaque and not amenable to control by users. Further, personalization necessarily limits the space of items that users are exposed to. Therefore, prior work notes a tension between personalization and users’ ability for discovering novel items. While discovery of novel items in personalization setups may be resolved through search result diversification, these approaches do little to allow user control over personalization. Therefore, in this paper, we introduce an approach for controllable personalized search. Our model, CtrlCE presents a novel cross-encoder model augmented with an editable memory constructed from users historical items. Our proposed memory augmentation allows cross-encoder models to condition on large amounts of historical user data and supports interaction from users permitting control over personalization. Further, controllable personalization for search must account for queries which don’t require personalization, and in turn user control. For this, we introduce a calibrated mixing model which determines when personalization is necessary. This allows system designers using CtrlCE to only obtain user input for control when necessary. In multiple datasets of personalized search, we show CtrlCE to result in effective personalization as well as fulfill various key goals for controllable personalized search.","Personalized search powers several industry scale search systems for products (Aslanyan et al., 2020; Yu et al., 2021), movies (Ostuni et al., 2023), jobs (Ha-Thuc and Sinha, 2016), and web-search more broadly (Horling and Kulick, 2009). While personalization in search systems commonly increases the uptake of systems by users and improves the relevance of search results, personalization is commonly seen as opaque and failing to provide users with sufficient control over predictions (Eiband et al., 2019; Konstan and Terveen, 2021). Further, personalized ranking is more likely to prevent users from seeing the breadth of information in a document collection. Here, Chien and Danks (2023) note the tension between personalization and epistemic fairness, which ensures that all users are similarly exposed to the space of information. Similarly, Salehi et al. (2015) note that search personalization may prevent serendipitous discovery and exploration in important applications such as education. Such concerns about personalization have been addressed through two avenues. The limits to information diversity have been addressed through diversification of personalized search results (Radlinski and Dumais, 2006; Vallet and Castells, 2012). While this improves result diversity, it does little to improve user control or facilitate user-driven interaction and discovery (Ruotsalo et al., 2018). On the other hand, a small body of work has also provided users interactive control over personalized search through editable user profiles (Ahn et al., 2015; Zemede and Gao, 2017). However, this line of work has been limited to exploring simple term or entity based user profiles and has focussed on designing visualization interfaces for user control. Notably, while prior work on controllable personalization for search tasks has been limited, a larger line of work has explored control for personalized recommendation tasks (Balog et al., 2019; Konstan and Terveen, 2021). This work has shown control over personalized recommendations to improve user satisfaction and trust (Jin et al., 2017; Jannach et al., 2016). In this paper, we take inspiration from this work and extend the body of work on controllable personalized search. We introduce, CtrlCE, a controllable cross-encoder model personalized with a editable user profile for personalized search tasks. We outline the following goals for controllable personalized search: (1) to allow control, a user profile used for personalization must be transparent to users, (2) users should be able to express preferences through intuitive edits made to their user profile. Further, since search can be performed without any personalization a controllable model should make good predictions without personalization, (3) since only some queries are likely to require personalization (Ai et al., 2019; Teevan et al., 2008) a controllable model should highlight queries for which user profile edits would be meaningful, and (4) controllable personalization should not come at the expense of degraded performance. Specifically, our model to fulfill these goals is a cross-encoder augmented with a user profile realized as an editable memory constructed from historical user documents. For our editable user memory we explore two multi-vector user representations, an item level representation and a novel concept based representation introduced in recent work on controllable recommendation (Mysore et al., 2023). Both representations remain transparent to users, leverage the rich content of historical user documents, and support a intuitive set of user edits. Further, to ensure that user edits are only solicited when necessary, CtrlCE represents document relevance as a non-personalized query-document and a personalized user-document score. Then, we train a novel calibrated mixing model that in addition to intelligently combining these scores, also predicts which queries are likely to benefit from user edits to their profile (Figure 1). In experiments, we show that on four datasets of personalized search drawn from multiple domains CtrlCE outperforms standard personalization approaches based on dense retrieval as well as ensemble models, and non-personalized approaches spanning sparse, dense, and cross-encoder retrievers/re-rankers by 6.4-10.6% across evaluation metrics. Following this, we demonstrate that CtrlCE fulfills the goals of controllability through simulations and calibration evaluations. Finally, we present a case study showcasing the queries that CtrlCE highlights for user profile edits. To the best of our knowledge CtrlCE represents the first approach to controllable personalized search with performant cross-encoder models and extends the under-explored area of controllable personalized search. Code for CtrlCE will be released upon acceptance."
https://arxiv.org/html/2411.02695v1,JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase,"Knowledge Graphs have emerged as a compelling abstraction for capturing key relationship among the entities of interest to enterprises and for integrating data from heterogeneous sources. JPMorgan Chase (JPMC) is leading this trend by leveraging knowledge graphs across the organization for multiple mission critical applications such as risk assessment, fraud detection, investment advice, etc. A core problem in leveraging a knowledge graph is to link mentions (e.g., company names) that are encountered in textual sources to entities in the knowledge graph. Although several techniques exist for entity linking, they are tuned for entities that exist in Wikipedia, and fail to generalize for the entities that are of interest to an enterprise. In this paper, we propose a novel end-to-end neural entity linking model (JEL) that uses minimal context information and a margin loss to generate entity embeddings, and a Wide & Deep Learning model to match character and semantic information respectively. We show that JEL achieves the state-of-the-art performance to link mentions of company names in financial news with entities in our knowledge graph. We report on our efforts to deploy this model in the company-wide system to generate alerts in response to financial news. The methodology used for JEL is directly applicable and usable by other enterprises who need entity linking solutions for data that are unique to their respective situations.","Knowledge Graphs are being used for a wide range of applications from space, journalism, biomedicine to entertainment, network security, and pharmaceuticals. Within JP Morgan Chase (JPMC), we are leveraging knowledge graphs for financial applications such as risk management, supply chain analysis, strategy implementation, fraud detection, investment advice, etc. While leveraging a knowledge graph, Entity Linking (EL) is a central task for semantic text understanding and information extraction. As defined in many studies (Zhang et al. 2010; Eshel et al. 2017; Kolitsas, Ganea, and Hofmann 2018), in an EL task we link a potentially ambiguous Mention (such as a company name) with its corresponding Entity in a knowledge graph. EL can facilitate several knowledge graph applications, for example, the mentions of company names in the news are inherently ambiguous, and by relating such mentions with an internal knowledge graph, we can generate valuable alerts for financial analysts. In Figure 1, we show a concrete example in which the name “Lumier” has been mentioned in two different news items. “Lumier”s are two different companies in the real world, and their positive financial activities should be brought to the attention of different stakeholders. With a successful EL engine, these two mentions of “Lumier”s can be distinguished and linked to their corresponding entities in a knowledge graph. Figure 1: Example for Entity Linking Prior work on EL has been driven by a number of standard datasets, such as CoNLYAGO (Suchanek, Kasneci, and Weikum 2007), TAC KBP111https://www.ldc.upenn.edu/collaborations/current-projects/tac-kbp, DBpedia222https://wiki.dbpedia.org/develop/datasets, and ACE333https://catalog.ldc.upenn.edu/LDC2006T06. These datasets are based on Wikipedia, and are therefore, naturally coherent, well-structured and rich in context (Eshel et al. 2017). We face the following problems when we use these methods for entity linking for our internal knowledge graph: 1) Wikipedia does not cover all the entities of financial interest. For example, as of this writing, the startup “Lumier” mentioned in Figure 1 is not present in Wikipedia, but it is of high financial interest as it has raised critical investment from famous investors. 2) Lack of context information. Many pre-trained models achieve great performance by leveraging rich context data from Wikipedia (Ganea and Hofmann 2017). For JPMC internal data, we do not have information comparable to Wikipedia to support re-training or fine-tuning of existing models. To address the problems identified above, we built a novel entity linking system, JEL, to link mentions of company names in text to entities in our own knowledge graph. Our model makes the following advancements on the current state-of-the-art: 1) We do not rely on Wikipedia to generate entity embeddings. With minimum context information, we compute entity embeddings by training a Margin Loss function. 2) We deploy the Wide & Deep Learning (Cheng et al. 2016) to match character and semantic information respectively. Unlike other deep learning models (Martins, Marinho, and Martins 2019; Kolitsas, Ganea, and Hofmann 2018; Ganea and Hofmann 2017), JEL applies a simple linear layer to learn character patterns, making the model more efficient both in the training phase and inference phase."
https://arxiv.org/html/2411.02692v1,JPEC: A Novel Graph Neural Network for Competitor Retrieval in Financial Knowledge Graphs,"Knowledge graphs have gained popularity for their ability to organize and analyze complex data effectively. When combined with graph embedding techniques, such as graph neural networks (GNNs), knowledge graphs become a potent tool in providing valuable insights. This study explores the application of graph embedding in identifying competitors from a financial knowledge graph. Existing state-of-the-art(SOTA) models face challenges due to the unique attributes of our knowledge graph, including directed and undirected relationships, attributed nodes, and minimal annotated competitor connections. To address these challenges, we propose a novel graph embedding model, JPEC(JPMorgan Proximity Embedding for Competitor Detection), which utilizes graph neural network to learn from both first-order and second-order node proximity together with vital features for competitor retrieval. JPEC had outperformed most existing models in extensive experiments, showcasing its effectiveness in competitor retrieval.","Competitor retrieval is one of the most crucial use cases for financial organizations. Traditionally, it is mostly driven by multiple manual tasks involving collecting data and converting factors like revenue, products, pricing, marketing, and industry distributions. While manually gathered market data offer vital insights, they have limitations in terms of applicability and scalability. On the other hand, knowledge graphs can provide competitive clues by revealing meaningful connections, such supply-chain, between companies. Combined with graph embedding techniques, knowledge graphs can offer a structured and efficient approach for automatic and intelligent competitor retrieval. However, most SOTA graph embedding methods are sub-optimal for our task due to the complex structure of a real world knowledge graph(described in Section2). This paper introduces a novel graph neural network, JPEC, for competitor detection from a financial knowledge graph with various types of edges but limited labeled data."
https://arxiv.org/html/2411.02400v1,Decomposition Dilemmas: Does Claim Decomposition Boost or Burden Fact-Checking Performance?,"Fact-checking pipelines increasingly adopt the Decompose-Then-Verify paradigm, where texts are broken down into smaller claims for individual verification and subsequently combined for a veracity decision. While decomposition is widely-adopted in such pipelines, its effects on final fact-checking performance remain underexplored. Some studies have reported improvements from decompostition, while others have observed performance declines, indicating its inconsistent impact. To date, no comprehensive analysis has been conducted to understand this variability. To address this gap, we present an in-depth analysis that explicitly examines the impact of decomposition on downstream verification performance. Through error case inspection and experiments, we introduce a categorization of decomposition errors and reveal a trade-off between accuracy gains and the noise introduced through decomposition. Our analysis provides new insights into understanding current system’s instability and offers guidance for future studies toward improving claim decomposition in fact-checking pipelines.111Source code available at https://github.com/qishenghu/Decomp_Dilemmas","Figure 1: An overview of the Decompose-Then-Verify pipeline employed in this study, which comprises four key stages: decomposition, retrieval, verification, and aggregation of sub-claim results. This figure illustrates how different decomposition methods, such as FactScore (Min et al., 2023) and VeriScore (Song et al., 2024), can lead to divergent decomposing outcomes. In this example, FactScore generates ambiguous sub-claims, while VeriScore omits key information (e.g., “Ultimately, the success of the Su-57…”) from the input. Fact-checking is a critical task that typically involves evaluating the veracity of claims or reports. With the rise of large language models (LLMs), the scope of fact-checking task has expanded to include verifying content generated by LLMs (Sun et al., 2024). While progress has been made in reducing LLM hallucinations, the problem has not yet been resolved, presenting ongoing challenges in ensuring the factuality of LLM outputs (Huang et al., 2023). At the same time, many studies focus on LLM-driven automated fact-checking pipelines (Min et al., 2023; Wei et al., 2024; Chern et al., 2023), aiming to improve the efficiency of the fact-checking processes. A common design pattern in these LLM-driven pipelines is the Decompose-Then-Verify paradigm, as adopted in frameworks like FactScore (Min et al., 2023), FacTool (Chern et al., 2023) and VeriScore (Song et al., 2024). It involves decomposing input text into sub-claims (Decompose), retrieving supporting information (e.g., Wikipedia, Google Search) for each sub-claim, and using a verifier model to assess the veracity of each sub-claim (Verify). The results are then aggregated to produce a final verification. By breaking down complex inputs (decomposition), these pipelines are effective in pinpointing misinformation, enabling a nuanced determination of whether a text is supported, unsupported (Zhu et al., 2023; Zhao et al., 2024a; Li et al., 2024), or assigned a quantified score (Min et al., 2023; Wei et al., 2024). However, most existing studies primarily focus on the design of fact-checking pipeline architectures. While decomposition is commonly employed in these frameworks, the reliability of the decomposition process itself remains insufficiently investigated. Some studies show that the final predictions of these pipelines are sensitive to the decomposition outcomes (Jiang et al., 2024; Wanner et al., 2024). Additionally, it has been observed that a given decomposer does not consistently lead to performance improvements when paired with different verifiers. For example, Kamoi et al. (2023) report improvements using a pre-trained Natural Language Inference (NLI) model as the verifier. However, an ablation study by Minicheck (Tang et al., 2024), which employs the same dataset and decomposition method but different NLI models, does not observe consistent benefits from decomposition. Similar inconsistencies are observed in FELM (Zhao et al., 2024b), which assesses the effect of decomposing responses into segments and further into claims. The study reveals distinct differences between ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023) as verifiers. Specifically, ChatGPT’s accuracy improves with claim-level decomposition, while GPT-4’s performance declines. Despite observed inconsistencies, the underlying reasons remain unclear, and there is limited in-depth analysis of how decomposition affects downstream performance. In this study, we investigate the underlying causes of performance variability by addressing three key questions. First, what determines decomposition’s effect on fact-checking performance? Through experiments, we present performance variability across factors such as input complexity, decomposition method design, and verifier strength. Second, what errors may decomposition introduce? We categorize decomposition error types, analyze their distribution, and validate their usefulness through error reflection. Third, what explains the variability in fact-checking performance? Our analysis reveals a trade-off between the accuracy gains by decomposing inputs into manageable sub-claims and the noise introduced by retrieval and decomposition as the number of sub-claims increases. This trade-off offers an explanation for the observed variability and provides insights for guiding future fact-checking pipeline design. Here are some key takeaways: • Current popular LLM-driven decomposition methods struggle to consistently improve fact-checking performance across varying input granularities and verifier strengths. • Decomposition methods tend to introduce different errors depending on the objectives—prioritizing high atomicity may over-fragment simple facts, causing redundancy and ambiguity, while emphasizing verifiability can omit essential details. • Decomposition improves performance on simpler sub-claims by reducing complexity, notably benefiting weaker verifiers. However, for stronger verifiers, the marginal accuracy gain from decomposition may not counterbalance the increased noise. • Decomposition can improve the handling of complex inputs; however, while increasing the number of sub-claims may initially enhance performance, the additional noise introduced will gradually offset these gains, eventually leading to performance degradation."
https://arxiv.org/html/2411.02864v1,Graph-DPEP: Decomposed Plug and Ensemble Play for Few-Shot Document Relation Extraction with Graph-of-Thoughts Reasoning,"Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning capability on many NLP tasks. Recasting an NLP task into a text-to-text generation task is a common practice so that generative LLMs can be prompted to resolve it. However, performing document-level relation extraction (DocRE) tasks with generative LLM models is still challenging due to the structured output format of DocRE, which complicates the conversion to plain text. Limited information available in few-shot samples and prompt instructions induce further difficulties and challenges in relation extraction for mentioned entities in a document. In this paper, we represent the structured output as a graph-style triplet rather than natural language expressions and leverage generative LLMs for the DocRE task. Our approach, the Graph-DPEP framework is grounded in the reasoning behind triplet explanation thoughts presented in natural language. In this framework, we first introduce a “decomposed-plug"" method for performing the generation from LLMs over prompts with type-space decomposition to alleviate the burden of distinguishing all relation types. Second, we employ a verifier for calibrating the generation and identifying overlooked query entity pairs. Third, we develop ""ensemble-play"", reapplying generation on the entire type list by leveraging the reasoning thoughts embedded in a sub-graph associated with the missing query pair to address the missingness issue. Through extensive comparisons with existing prompt techniques and alternative Language Models (LLMs), our framework demonstrates superior performance on publicly available benchmarks in experiments.","Figure 1. An example of document relation extraction. Document-level relation extraction (DocRE) extracts relations among multiple entity pairs in a document, representing a more realistic and challenging task than sentence-level extraction (Ji et al., 2017; Alt et al., 2020). In DocRE, an entity can have multiple mentions scattered throughout a document, and relationships between entities can appear in multiple different sentences. We illustrate a running example in Figure 7. All the entities that occurred in the context are marked in bold. These relations can be identified by intra-sentence hints, like ""Jack Ganto was born in Norvelt"", which can be found in Sent#4. But ""Calumet is in the country, the United States"", which should be reasoned from Sent#0 and Sent#1, because Sent#0 indicates that Norvelt is in the United States and Sent#1 indicates Calumet and Norvelt are from the same community. Most traditional transform-based DocRE models (Ma et al., 2023; Tan et al., 2022a; Yan et al., 2021) filter information in a lengthy document by retrieving relevant sentences as the supporting evidence to identify the relation for a given query entity mention pair. The idea of supporting evidence retrieval inspires us to make reasoning thoughts when we recast the traditional DocRE into a generation task. Large language models (LLMs) like ChatGPT present remarkable success in generative and reasoning tasks. Reformulating the traditional NLP tasks into text-to-text generation formats to perform LLMs attracts intensive focus, especially under low-resource scenarios. For DocRe, annotation on evidence sentences is expensive, which restrains traditional model ability in real-world scenarios. Therefore, we leverage LLMs as the few-shot learner to address few-shot DocRE in a generative manner. However, existing LLMs are mostly pre-trained on unstructured data, which leads to poor performance when dealing with tasks that require a structured format. Prior studies (Wadhwa et al., 2023; Li et al., 2023) utilize graph-like triplet sets, code-style frames, and similar approaches. As we also utilize triplet notation for describing extracted relations, we can establish a semantically rich graph structure to store information in a format readable by large language models (LLMs). On the other hand, even with a reasonable number of few-shot examples, LLMs still face challenges in generating graph-structured data, and the outputs from LLMs remain error-prone. Moreover, compared with regular relation extraction (Wadhwa et al., 2023; Li et al., 2023), DocRE suffers from extraction on a large label space, which poses a significant challenge in response time and quality of LLMs. Even given several relation extraction examples for in-context learning, LLMs are susceptible to the risk of misinterpreting labels within the vast number of possibilities. In this paper, we investigate the end-to-end few-shot DocRE problem via a generative model and propose Graph-DPEP, a decomposed-plug and ensemble-play framework that allows self-verification on generation under relation graph-of-thoughts reasoning. To assist LLMs in distinguishing intensive labels when transferring a classification task into a generation task, the decomposed-plug component processes an LLM to make a generation on each single type. What’s more, we inspect the generation results with a verifier module for concerns such as repetition, irrelevance, incompleteness, and missing query pairs. The verifier incorporates calibration principles to refine the generation, addressing issues except for missingness, which is identified for subsequent adjustments to compensate for performance losses in this aspect. Reapplying LLM on the missing pairs can be facilitated by an association sub-graph that is relevant to entities in the missing query pairs. So, we employ the association sub-graph as the graph-of-thoughts to further aid reasoning on missing pairs’ relations from the entire type list rather single one in the decomposed plug, so-called ensemble play. We summarize our contributions as follows: • To our knowledge, we are the first to transfer DocRE from a classification task to a generation task by constructing a decomposed method for prompt engineering to address the huge relation type space. • We compare our approach with the best existing prompting techniques available to us and our Graph-DPEP method achieves the most promising recalls on the whole type space, especially for infrequent types. • We employ the latest LLMs for evaluation of the effectiveness of the prompt design in Graph-DPEP. The experiment results show the advantages of our decomposed-plug and ensemble-play method with a self-correctable verifier."
https://arxiv.org/html/2411.02851v1,"Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization","The goal of Multilingual Visual Answer Localization (MVAL) is to locate a video segment that answers a given multilingual question. Existing methods either focus solely on visual modality or integrate visual and subtitle modalities. However, these methods neglect the audio modality in videos, consequently leading to incomplete input information and poor performance in the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method that incorporates audio modality to augment both visual and textual representations for the MVAL task. Specifically, we integrate features from three modalities and develop three predictors, each tailored to the unique contributions of the fused modalities: an audio-visual predictor, a visual predictor, and a textual predictor. Each predictor generates predictions based on its respective modality. To maintain consistency across the predicted results, we introduce an Audio-Visual-Textual Consistency module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing each modality’s predictor to dynamically learn from the others. This collaborative learning ensures that the model generates consistent and comprehensive answers. Extensive experiments show that our proposed method outperforms several state-of-the-art (SOTA) methods, which demonstrates the effectiveness of the audio modality.","With the rapid expansion of the internet, an increasing number of users are turning to online platforms to seek medical advice by posing natural language questions (O’Donnell et al., 2023; Lim et al., 2022). Current online platforms typically fall into two categories: those that provide textual answers, which may be difficult for users to interpret, and those that offer visual answers, which are generally more intuitive and easier to follow (Tang et al., 2021b). However, the retrieved videos often contain substantial amounts of information irrelevant to the user’s query (Moon et al., 2023), which significantly hinders the efficiency of information retrieval (Zhang et al., 2023). In response to this challenge, the task of Visual Answer Localization (VAL) has been introduced (Weng and Li, 2023). Figure 1: (a) Overview of the audio-enhanced multilingual video answer localization task. (b) Difference between existing methods and our method. (c) Performance comparison diagram of visual-based, textual-based, and our method. Existing VAL approaches can be broadly categorized into visual-based (Tang et al., 2021a; Chen et al., 2020a) and textual-based methods (Li et al., 2023a; Weng and Li, 2023; Li et al., 2024b). Visual-based methods are effective in scenarios where subtitle text is sparse, but their performance tends to degrade significantly in other contexts. In contrast, textual-based methods excel when abundant subtitle text is available, as the semantic similarity between the question and subtitle is typically greater than between the question and the video (Li et al., 2024b). However, these methods often overlook audio, which plays a crucial role in complementing both visual and textual modalities. There is inherent consistency and complementarity among these modalities (Chen et al., 2023a), and harnessing this synergy can enhance both visual and textual modalities by integrating information from the audio. Incorporating audio thus addresses the performance limitations in VAL, particularly in video segments lacking subtitles (Liu et al., 2022; Chen et al., 2020b; Sun et al., 2024). To this end, we study the Audio-enhanced Multilingual Visual Answer Localization (AMVAL) which aims to locate video segments that answer a user’s natural language question, in either Chinese or English. By providing video segments with verbal explanations for medical guidance, this approach not only facilitates the learning of specific actions but also helps bridge language barriers, making the content accessible to people who speak different languages (Macedonia and Knösche, 2011; Diamond et al., 2020). However, a significant challenge lies in effectively integrating the three modalities and fully utilizing their individual strengths to tackle the AMVAL task. To address this challenge, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method for AMVAL, aimed at reducing cross-modal discrepancies and improving the accuracy of span localization by integrating audio modality. We designed a network architecture with three modality channels (audio, visual, and textual) to fully leverage the semantic information from each modality in the video, addressing the limitations of single-modality methods. Each channel is equipped with a corresponding predictor: an audio-visual predictor, a visual predictor, and a textual predictor. During joint training, distinct objectives are assigned to each predictor, enabling them to leverage the unique strengths of their respective modalities. To improve modality integration, we introduce an Audio-Visual-Textual consistency module, which employs a Dynamic Triangular Loss (DTL) function based on Intersection over Union (IoU). This loss function aligns the modalities by minimizing the discrepancies between each predictor’s output and the target answer, as well as between the outputs of the other two predictors. Our approach promotes mutual learning among the predictors to achieve consistent and cohesive multimodal representations. Our contributions are as follows: (1) We study the AMVAL and propose the AVTSL method, which is the first to introduce the audio modality for the AMVAL; (2) We designed an Audio-Visual-Text Consistency module, which leverages the consistency and complementarity between different modalities using the DTL loss function; (3) We conducted extensive experiments to demonstrate the effectiveness of the AVTSL method, where our method outperformed other state-of-the-art (SOTA) methods by incorporating the audio modality."
https://arxiv.org/html/2411.02831v1,Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults in an Emotion-Aware Chatbot,"ABSTRACT The research community has traditionally shown a keen interest in emotion modeling, with a notable emphasis on the detection aspect. In contrast, the exploration of emotion generation has received less attention.This study delves into an existing state-of-the-art emotional chatbot, EmoBot, designed for generating emotions in general-purpose conversations. This research involves a comprehensive examination, including a survey to evaluate EmoBot’s proficiency in key dimensions like usability, accuracy, and overall user satisfaction, with a specific focus on fault tolerance. By closely examining the chatbot’s operations, we identified some noteworthy shortcomings in the existing model. We propose some solutions designed to address and overcome the identified issues.","In recent years, there has been a growing emphasis on the study of social chatbots. Unlike older rule-based counterparts, modern chatbots, driven by deep learning, have shown significant improvements (Sutskever et al., 2014). In order to design a chatbot that provides a meaningful experience, we must first understand what expectations people have for this technology, and what opportunities are there for chatbots based on user needs (Zamora, 2017). As chatbots become more prevalent in areas like entertainment and customer service (Io and Lee, 2017), the focus has shifted to making them more emotionally responsive and human-like. The goal is to enable chatbots to engage in empathetic conversations, often assuming the role of a social companion, thereby positively impacting the well-being of individuals (Skjuve et al., 2021). Therefore, adaptability to various scenarios and meeting user needs are crucial for a social chatbot’s success.This study delves into Emobot, an emotional chatbot using the cognitive appraisal theory to generate emotions based on user responses (Ehtesham-Ul-Haque et al., 2024). Our analysis involves an exploration of user experiences with this chatbot. We examine user experiences with Emobot, identifying areas for improvement in its emotion generation. Our goal is to detect and address these issues, proposing practical solutions to enhance the chatbot’s performance. Our contribution in this light are as follows : • Conducted a survey to assess user experience and subsequently performed a qualitative analysis on the gathered data. • By examining the codebase and analysing user experince we have found some faults in the existing Emobot. • Proposed different solution approaches according to the shortcomings."
https://arxiv.org/html/2411.02791v1,Language Models and Cycle Consistency for Self-Reflective Machine Translation,"This paper introduces a novel framework that leverages large language models (LLMs) for machine translation (MT). We start with one conjecture: an ideal translation should contain complete and accurate information for a strong enough LLM to recover the original sentence. We generate multiple translation candidates from a source language A𝐴Aitalic_A to a target language B𝐵Bitalic_B, and subsequently translate these candidates back to the original language A𝐴Aitalic_A. By evaluating the cycle consistency between the original and back-translated sentences using metrics such as token-level precision and accuracy, we implicitly estimate the translation quality in language B𝐵Bitalic_B, without knowing its ground-truth. This also helps to evaluate the LLM translation capability, only with monolingual corpora. For each source sentence, we identify the translation candidate with optimal cycle consistency with the original sentence as the final answer. Our experiments demonstrate that larger LLMs, or the same LLM with more forward passes during inference, exhibit increased cycle consistency, aligning with the LLM model size scaling law [Kaplan et al. (2020)] and test-time computation scaling law [Snell et al. (2024)]. This work provide methods for, 1) to implicitly evaluate translation quality of a sentence in the target language, 2), to evaluate capability of LLM for any-to-any-language translation, and 3), how to generate a better translation for a specific LLM.","Machine Translation (MT) has been a cornerstone of natural language processing, facilitating globalization by cross-linguistic communication and democratizing newest information access to all population. In recent years, transformer-based large language models (LLMs) have fundamentally changed the field of natural language processing. Introduced by [Vaswani et al. (2017)], the transformer architecture facilitates parallel processing of input word tokens, significantly improving computational efficiency and successfully scales to unseen model size. Strong language capabilities beyond human imagination emerges from LLM scaling, and create an image of Silicon intelligence for the first time. Transformer-based LLMs can be categorized into several paradigms: encoder-only architectures, like BERT [Devlin (2018)], which focus on meaningful embeddings of input sequences, and don’t fit translation task; the rest two architectures, the encoder-decoder architectures, like T5 [Raffel et al. (2020)], which separately process input and output sequences, were born for translation; and decoder-only architectures, like GPT [Radford et al. (2019)], which generate text in an autoregressive manner. Although GPT is not initially trained specifically for translation tasks like T5, it is exceptionally well-suited for a wide range of natural language processing (NLP) tasks via supervised fine-tuning on downstream tasks, including translation, and provides applications to users via prompting [Brown et al. (2020)]. On a related subject, MT evaluation poses significant challenges as there is no unit test for human languages, like Python or Java. Machine-based evaluation metrics, such as BLEU [Papineni et al. (2002)], METEOR [Banerjee and Lavie (2005)], and TER [Snover et al. (2006)], provide quantitative assessments based on N-gram overlaps and edit distances. While these metrics offer consistency and objectivity, they often fail to fully capture the semantic adequacy and fluency of translations [Liu et al. (2022)], although not being a problem for LLMs as they in most cases generate fluent sentences. Higher-level criteria beyond these metrics, such as overall quality and asceticity, nuanced subtleties, professional terminologies and informal idioms, require evaluation from native speaker of the target language, which is expensive and impractical during online inference. Additionally, evaluation on low-resource languages [Luong et al. (2015)] poses further difficulties due to the data scarcity. The starting point of our work is a simple conjecture: a good translation, and the LLM who translated it, should be able to jointly recover the original sentence completely and accurately. This is natural from information theory view of point. For instance, if an English sentence is translated into French and then back to English, a high degree of similarity between the original and final English sentences indicates a more accurate and reliable translation. To prove this, we propose translation cycle consistency as a meaningful metric to evaluate translation quality without parallel corpora in source language A𝐴Aitalic_A and target language B𝐵Bitalic_B. By translating a sentence from language A𝐴Aitalic_A to language B𝐵Bitalic_B and then back to A𝐴Aitalic_A, we can quantitatively measure the alignment, similarity, or closeness, between the original and back-translated texts. This method not only economically scale MT assessments, it also streamlines the evaluation process, making it widely usable for offline or online evaluation. Cycle consistency brings chance for further improving MT through a self-reflective mechanism: to think ahead a few steps. If the evaluation of translation during inference is accurate enough, we can afford to generate multiple candidates and select the best one. Unlike LLM decoding techniques such as beam search, which select the most probable translation based on a search space over a few tokens, solely based the LLM itself, cycle consistency allows for a complete evaluation of all translated tokens, selecting the most coherent and accurate output, with a math formula-backed metric. This is analogous to AlphaGo, which simulates several future steps to determine the best possible action in the current move [Silver et al. (2016)]. In this paper, we formalize our idea, and empirically investigate the scaling effects, observing that larger models exhibit improved cycle consistency in translations [Chen et al. (2021)], which in turn proves that cycle consistency is a valid and novel metric for evaluation."
https://arxiv.org/html/2411.02607v1,Towards Context-Aware Adaptation in Extended Reality: A Design Space for XR Interfaces and an Adaptive Placement Strategy,"By converting the entire 3D space around the user into a screen, Extended Reality (XR) can ameliorate traditional displays’ space limitations and facilitate the consumption of multiple pieces of information at a time. However, if designed inappropriately, these XR interfaces can overwhelm the user and complicate information access. In this work, we explored the design dimensions that can be adapted to enable suitable presentation and interaction within an XR interface. To investigate a specific use case of context-aware adaptations within our proposed design space, we concentrated on the spatial layout of the XR content and investigated non-adaptive and adaptive placement strategies. In this paper, we (1) present a comprehensive design space for XR interfaces, (2) propose Environment-referenced, an adaptive placement strategy that uses a relevant intermediary from the environment within a Hybrid Frame of Reference (FoR) for each XR object, and (3) evaluate the effectiveness of this adaptive placement strategy and a non-adaptive Body-Fixed placement strategy in four contextual scenarios varying in terms of social setting and user mobility in the environment. The performance of these placement strategies from our within-subjects user study emphasized the importance of intermediaries’ relevance to the user’s focus. These findings underscore the importance of context-aware interfaces, indicating that the appropriate use of an adaptive content placement strategy in a context can significantly improve task efficiency, accuracy, and usability.","1 Related Work Previous work explored various innovative applications and design dimensions of XR to spatially place 2D & 3D objects and transition between them, visualize hierarchies, and provide persistent and portable presentation of the personal information [14, 20, 38, 16, 34]. Morrison et al. highlighted unique design elements within AR for enhancing accessibility for visually impaired children [45]. These studies underscore the broad design space of XR interfaces and the versatile and transformative applications that XR enables across various contexts. This work, investigates previous work and the design elements they utilized, providing a comprehensive XR design space. XR offers the potential to enable efficient information access, reduce cognitive load, and enhance user convenience compared to traditional methods such as mobile phones [55, 12, 43, 11, 40]. However, intrusive XR interfaces may result in challenges such as information overload and occlusion of important cues within the environment [31, 4], increase cognitive load and discomfort, and reduce the user’s situational awareness and performance [17, 25, 54]. Various approaches for intuitive and seamless integration of XR content into the environment have been extensively explored. For instance, to enhance efficiency and minimize intrusiveness, numerous designs adapt the XR content’s availability, transparency, placement, and Level of Detail (LoD) [15, 3, 46], as well as spatial layout and size [19, 9]. Lages & Bowman highlighted the significance of adapting the AR content placement strategy to avoid occlusions and accommodate activities like walking [33]. For adaptations to the XR content placement, the concept of the frame of reference, also referred to as fixation was introduced [20]. User-triggered adaptation through gaze, hand, and head-based inputs such as finger taps and handheld controllers are extensively explored for adjustments to the transparency, LoD, and spatial layout of XR content [19, 32, 50, 39, 48, 47]. In AR, for instance, many applications were designed to prioritize the real-world [11] by initially keeping the XR content hidden, in the peripheral, or at a lower LoD, and granting access to them through explicit interactions [49, 42, 46]. However, AR’s definition emphasizes the integration of the digital content into the real world [1], underscoring the significance of context awareness. While user-triggered adaptations offer control and predictability, they increase the user’s physical and mental workload of deciding when, what, and how to apply the adaptations [51]. Automatic XR adaptation can enhance efficiency and reduce workload compared to the user-triggered ones [11]. Numerous studies suggest rule-based approaches for XR adaptations. The significance of such rule-based adaptations in meeting the XR task requirements within various applications such as driving and conversation have been highlighted [4, 12]. To prevent occlusion issues, Ens et al. suggested a rule-based adaptive design to exclusively place the XR objects on empty surfaces [18]. Constraints, explicitly imposed by the users, were utilized as guidelines to group related XR objects together and prevent their occlusion within a rule-based view management [3]. Such rule-based adaptive approaches are highly tailored to specific use cases and applications.Even within the same application or use case, slight contextual deviations can cause a rule to fail, making it suitable only within unchanging contexts. This work proposes an adaptive placement strategy, applicable within changing contexts, to extract and utilize contextual information from the environment and user state to spatially place the XR content. Context refers to the external components that influence or relate to the user’s interactions with the interface [13]. In recent years, context-aware XR has become a focal point of research, promising the potential for “ubiquitous"" and “pervasive"" computing through AR [56, 26]. Contextual aspects such as user preferences, cognitive load, device profiles, task environment, semantic changes, and task-specific security parameters have been utilized for adaptations to the XR content’s appearance, LoD, frame of reference, and spatial layout [37, 9, 36]."
https://arxiv.org/html/2411.02571v1,MM-Embed: Universal MultimodalRetrieval with Multimodal LLMs,"State-of-the-art retrieval models typically address a straightforward search scenario, where retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of both text and image, but underperforms a smaller CLIP retriever in cross-modal retrieval tasks due to modality bias from MLLMs. To address the issue, we propose modality-aware hard negative mining to mitigate the modality bias exhibited by MLLM retrievers. Second, we propose to continually fine-tune the universal multimodal retriever to enhance its text retrieval capability while maintaining multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-art text retrieval model, NV-Embed-v1, on MTEB retrieval benchmark. Finally, we explore to prompt the off-the-shelf MLLMs as the zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g., text-image composed queries) are more complex and challenging to understand. These findings also pave the way to advance universal multimodal retrieval in the future. We release the model weights at: https://huggingface.co/nvidia/MM-Embed.","Information retrieval is crucial for a variety of downstream tasks, such as question answering (Kwiatkowski et al., 2019), fact-checking (Thorne et al., 2018), and retrieval-augmented generation (Lewis et al., 2020). Existing state-of-the-art retrievers often focus on narrow scenarios. For example, LLM-based retrievers (Wang et al., 2023; Lee et al., 2024; Meng et al., 2024; Moreira et al., 2024) are limited to text-to-text retrieval tasks, where both the query and the retrieved results are text-only. Recent work on multimodal retrieval (Zhang et al., 2024; Jiang et al., 2024) focuses on specific tasks and assumes a homogeneous document format. However, in real-world applications, documents and queries often consist of diverse formats or modalities, such as text, images, and interleaved text and images. To advance information retrieval and support broader search scenarios, this work explores the use of multimodal LLMs (MLLMs; Dai et al., 2024; Liu et al., 2023a; 2024) for universal multimodal retrieval, accommodating diverse user-instructed tasks with multimodal queries and documents, as illustrated in Figure 1. We first explore to fine-tune MLLM-based bi-encoder retrievers with instructions as a guide (Asai et al., 2023) on 16 multimodal retrieval tasks from M-BIER (Wei et al., 2023). We find that MLLM-based retrievers significantly outperform CLIP-based retrievers in the challenging tasks, where interleaved text–image queries are given, such as visual question answering and composed image retrieval (tasks 3 and 7 in Figure 1). However, MLLM-based retrievers underperform in cross-modal retrieval tasks due to the modality bias from MLLMs. That is, given a text-based query with the instruction to retrieve an image (e.g., task 9 in Figure 1), an MLLM-based retriever tends to retrieve a relevant text-only rather than documents with images, especially when we improve the MLLM-based retriever’s text retrieval capability. To address the issue, we propose modality-aware hard negative mining in Section 4.1.1 and continual text-to-text retrieval fine-tuning in Section 4.1.2. Our final retriever, coined MM-Embed, is the first state-of-the-art universal multimodal retriever while maintaining competitive text-to-text retrieval performance across diverse tasks. Finally, we explore to prompt MLLMs as zero-shot rerankers. Surprisingly, we find that the zero-shot MLLM-based rerankers can further boost retrieval accuracy in the tasks, where user queries are interleaved text–image and more challenging to understand. For example, in the composed image retrieval dataset, CIRCO (Baldrati et al., 2023), the zero-shot rerankers are able to refine the ranked lists and significantly boosts the accuracy (mAP@5) over 7 points from the existing state-of-the-art composed-image retriever (Zhang et al., 2024) and our universal multimodal retrievers. This finding indicates that there is still room for improvement in such challenging tasks in order to tackle universal multimodal retrieval. Also, knowledge distillation from zero-shot or few-shot MLLM-based rerankers to retrievers is a promising direction. We summarize our contributions as follows: i) We present a study on applying MLLMs to universal multimodal retrieval. ii) We are the first to build MLLM-based universal multimodal retrievers. Notably, our MM-Embed, initialized from the existing best-performing text retriever (NV-Embed-v1; Lee et al., 2024), not only achieves state-of-the-art results in universal multimodal retrieval benchmark, M-BEIR (Wei et al., 2023), but also surpasses NV-Embed-v1 in text-to-text retrieval tasks on MTEB. iii) We are the first work to explore prompting MLLMs for zero-shot reranking. With a zero-shot MLLM-based reranker, we are able to boost the ranking accuracy over 7 points upon state-of-the-art retrievers in the composed image retrieval task, CIRCO (Baldrati et al., 2023). We organize the rest of the paper as follows. We discuss related work in § 2. We introduce the definition of universal multimodal retrieval in § 3 and present the proposed method in § 4. We report experiment results in § 5 and conclude the paper in § 6. Figure 1: Illustration of universal multimodal retrieval, where diverse tasks with instructions, queries and documents with multimodal formats are supported. In this work, we explore to fine-tune MLLM-based universal multimodal retriever, MM-Embed, and prompt an MLLM for reranking."
https://arxiv.org/html/2411.02537v2,Inquire: A Natural WorldText-to-Image Retrieval Benchmark,"We introduce Inquire, a text-to-image retrieval benchmark fdesigned to challengemultimodal vision-language models. on expert-level queries Inquire includes iiNaturalist 2024 (iNat24) a new dataset of five million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identification, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) Inquire-Fullrank, a full dataset ranking task, and (2) Inquire-Rerank, a reranking task for refining top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that Inquire poses a significant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a significant margin for improvement. By focusing on scientifically-motivated ecological challenges, Inquire aims to bridge the gap between AI capabilities and the needs of real-world scientific inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.","Recent advances in multimodal learning have resulted in advanced models Radford et al. [2021], Liu et al. [2023], Achiam et al. [2023] that demonstrate remarkable generalization capabilities in zero-shot classification Radford et al. [2021], Zhai et al. [2023], visual question-answering (VQA) Li et al. [2022], Yu et al. [2022], Alayrac et al. [2022], Li et al. [2023], and image retrieval Yu et al. [2022], Li et al. [2023]. These models offer the potential to assist in the exploration, organization, and extraction of knowledge from large image collections. However, despite this success, there remains a significant gap in the evaluation of these models on domain-specific, expert-level queries, where nuanced understanding and precise retrieval are critical. Addressing this gap is essential for future deployment in specialized fields such as biodiversity monitoring and biomedical imaging, among other scientific disciplines. Previous studies of the multimodal capabilities of this new generation of models have primarily focused on the task of VQA. In VQA, it has been demonstrated that there remains a large performance gap between state-of-the-art models and human experts in the context of challenging perception and reasoning queries such as those found on college-level exams Yue et al. [2024], Zhong et al. [2023]. However, no such expert-level benchmark exists for image retrieval. The most commonly used text-to-image retrieval benchmarks are derived from image captioning datasets, and contain simple queries related to common everyday categories Young et al. [2014], Lin et al. [2014]. Current multimodal models achieve near perfect performance on some of these benchmarks, indicating that they no longer pose a challenge (e.g., BLIP-2 Li et al. [2023] scores 98.9 on Flickr30K Young et al. [2014] top-10). Existing retrieval datasets are generally small Philbin et al. [2007, 2008], Young et al. [2014], Lin et al. [2014], limited to a single visual reasoning task (e.g., landmark-location matching Philbin et al. [2007, 2008], Weyand et al. [2020]), and lack concepts that would require expert knowledge Philbin et al. [2007, 2008], Weyand et al. [2020], Young et al. [2014], Lin et al. [2014]. These limitations impede our ability to track and improve image retrieval capabilities. Figure 2: Category breakdown for the fine-grained queries that make up Inquire. Each query category falls under one of the following supercategories: Species, Context, Behavior, or Appearance. A domain that is well-suited for studying this problem is the natural world, where images collected by enthusiast volunteers provide vast and largely uncurated sources of publicly available scientific data. In particular, the iNaturalist iNa [b] platform contains over 180 million species images and contributes immensely to research in biodiversity monitoring Chandler et al. [2017], Lohan [2024]. These images also contain a wealth of “secondary data” not reflected in their species labels Pernat et al. [2024], including crucial insights into interactions, behavior, morphology, and habitat that could be uncovered through searches. However, the time-consuming and expert-dependent analysis needed to extract such information prevents scientists from taking advantage of this valuable data at scale. This cost is amplified as scientists typically want to retrieve multiple relevant images for each text query, so that they can track changes of a property over space and time Young et al. [2019]. This domain serves as an ideal testbed for expert image retrieval, as these images contain expert-level diverse and composite visual reasoning problems, and progress in this field will enhance impactful scientific discovery. In this work, we introduce Inquire, a new dataset and benchmark for expert-level text-to-image retrieval and reranking on natural world images. Inquire includes the iNat24 dataset and 250 ecologically motivated retrieval queries. The queries span 33,000 true-positive matches, pairing each text query with all relevant images that we comprehensively labeled among iNat24’s five million natural world images. iNat24 is sampled from iNaturalist iNa [b], and contains images from 10,000 different species collected and annotated by citizen scientists, providing significantly more data for researchers interested in fine-grained species classification. The queries contained within Inquire come from discussions and interviews with a range of experts including ecologists, biologists, ornithologists, entomologists, oceanographers, and forestry experts. Our evaluation of multimodal retrieval methods demonstrates that Inquire poses a significant challenge, necessitating the development of models able to perform expert-level retrieval within large image collections. A key finding from our experiments is that reranking, a technique typically used in text retrieval Nogueira and Cho [2019], Khattab and Zaharia [2020], Karpukhin et al. [2020], offers a promising avenue for improvement in image retrieval. We hope that Inquire will inspire the community to build next-generation image retrieval methods towards the ultimate goal of accelerating scientific discovery. We make Inquire, the iNat24 dataset, pre-computed outputs from state-of-the-art models, and code for evaluation available at https://inquire-benchmark.github.io/. Figure 3: Proportion of queries in Inquire associated with each iconic group of species. Table 1: Comparison to common datasets used to evaluate text-to-image retrieval Gadre et al. [2023]. Unlike other datasets, Inquire has significantly more images and many matches per query rather than exactly one. MpQ: Matches per query Dataset Images Queries MpQ Expert Flickr30k Young et al. [2014] 1,000 5k 1 ✗ COCO Lin et al. [2014] 5,000 25k 1 ✗ Inquire 5,000,000 250 1–1.5k ✓"
https://arxiv.org/html/2411.02284v1,Training on the Test Model:Contamination in Ranking Distillation,"Neural approaches to ranking based on pre-trained language models are highly effective in ad-hoc search. However, the computational expense of these models can limit their application. As such, a process known as knowledge distillation is frequently applied to allow a smaller, efficient model to learn from an effective but expensive model. A key example of this is the distillation of expensive API-based commercial Large Language Models into smaller production-ready models. However, due to the opacity of training data and processes of most commercial models, one cannot ensure that a chosen test collection has not been observed previously, creating the potential for inadvertent data contamination. We, therefore, investigate the effect of a contaminated teacher model in a distillation setting. We evaluate several distillation techniques to assess the degree to which contamination occurs during distillation. By simulating a “worst-case” setting where the degree of contamination is known, we find that contamination occurs even when the test data represents a small fraction of the teacher’s training samples. We, therefore, encourage caution when training using black-box teacher models where data provenance is ambiguous.","Neural ranking models applying contextualized representations are frequently more effective than their statistical counterparts in ad-hoc ranking tasks (Karpukhin et al., 2020; Formal et al., 2022, inter alia.). Learning from annotated examples enables a more precise approximation of relevance, though computation costs greatly increase. To partially reduce cost, a strong but large neural model can provide training data to a smaller student model in a semi-supervised fashion, often maintaining effectiveness while largely reducing latency (Hofstätter et al., 2020). This process, known as knowledge distillation, allows the increasing size of models to be of minimal concern, as knowledge distillation allows small student models to capture the effectiveness of a strong teacher model when served in production. The use of Large Language Models (LLMs) as strong teachers exemplifies these benefits as high-quality semi-supervised data can be collected in a seemingly zero-shot fashion and distilled into highly effective smaller models (Pradeep et al., 2023; Schlatt et al., 2024). Many strong models are either fully closed-source (OpenAI et al., 2024) or solely open-weighted (Touvron et al., 2023; Jiang et al., 2023); therefore, their training data is unknown. When researchers use these models as teachers in distillation, data provenance becomes important to precisely measure improvement due to novel contributions versus test set leakage. Because it is difficult to determine to what degree or in what form closed models have been exposed to a given test collection, we simulate a worst-case scenario to provide insights into test set leakage concerns for closed-source models. This scenario directly optimizes teacher ranking models over training data contaminated with common test sets. We then employ these contaminated models as semi-supervised training signals in several common ranking distillation settings. We observe significant improvements in the effectiveness of student models over contaminated teachers (compared to uncontaminated teachers), even when the test data leakage constitutes less than 0.1% of the total training examples. Therefore, we conclude that a semi-supervised training signal is sufficient to cause significant improvements over standard models on a contaminated benchmark. We find that both the explicit use of teacher output and RankNet-style order distillation lead to contamination. From this finding, we encourage caution when distilling from closed-source models and evaluating public benchmarks when data provenance is ambiguous. Even though we investigate through the lens of ranking tasks, our findings are closely related to many existing aspects of distillation in broader NLP due to the ever-increasing use of Plackett-Luce preference optimization in large language model alignment (Wei et al., 2022; Chung et al., 2022) and distillation (Tunstall et al., 2023), which could suffer from similar contamination."
https://arxiv.org/html/2411.02041v1,Enhancing ID-based Recommendation with Large Language Models,"Large Language Models (LLMs) have recently garnered significant attention in various domains, including recommendation systems. Recent research leverages the capabilities of LLMs to improve the performance and user modeling aspects of recommender systems. These studies primarily focus on utilizing LLMs to interpret textual data in recommendation tasks. However, it’s worth noting that in ID-based recommendations, textual data is absent, and only ID data is available. The untapped potential of LLMs for ID data within the ID-based recommendation paradigm remains relatively unexplored. To this end, we introduce a pioneering approach called ”LLM for ID-based Recommendation” (LLM4IDRec). This innovative approach integrates the capabilities of LLMs while exclusively relying on ID data, thus diverging from the previous reliance on textual data. The basic idea of LLM4IDRec is that by employing LLM to augment ID data, if augmented ID data can improve recommendation performance, it demonstrates the ability of LLM to interpret ID data effectively, exploring an innovative way for the integration of LLM in ID-based recommendation. Specifically, we first define a prompt template to enhance LLM’s ability to comprehend ID data and the ID-based recommendation task. Next, during the process of generating training data using this prompt template, we develop two efficient methods to capture both the local and global structure of ID data. We feed this generated training data into the LLM and employ LoRA for fine-tuning LLM. Following the fine-tuning phase, we utilize the fine-tuned LLM to generate ID data that aligns with users’ preferences. We design two filtering strategies to eliminate invalid generated data. Thirdly, we can merge the original ID data with the generated ID data, creating augmented data. Finally, we input this augmented data into the existing ID-based recommendation models without any modifications to the recommendation model itself. We evaluate the effectiveness of our LLM4IDRec approach using three widely-used datasets. Our results demonstrate a notable improvement in recommendation performance, with our approach consistently outperforming existing methods in ID-based recommendation by solely augmenting input data.","Recommender systems play a central role and have emerged as indispensable tools in online services (Covington et al., 2016; Guo et al., 2017; Dai et al., 2021; Yu et al., 2023b; Chen et al., 2023). They serve the critical function of offering personalized recommendations in the face of information overload, effectively aligning with user preferences across a range of tasks (Zhou et al., 2018; Chen et al., 2020b). While various recommendation tasks exist (Covington et al., 2016; Lin et al., 2021; Xi et al., 2023), including top-N recommendation, next-item recommendation, and rating prediction, the common approach involves learning user representations to model their preferences and intentions. These learned representations are subsequently employed to generate decisions regarding recommended items for users. In recent years, Large Language Models (LLMs) have exhibited remarkable proficiency in approximating human intentions and excelling in a wide array of tasks, including reasoning and decision-making (Huang and Chang, 2022; Brown et al., 2020b; Zhao et al., 2023). Inspired by the great success of Large Language Models (LLMs) (Brown et al., 2020a; Touvron et al., 2023; Chiang et al., 2023), exploring the potential of LLMs in recommendation is attracting attention (Li et al., 2023d; Bao et al., 2023; Geng et al., 2022a; Cui et al., 2022; Dai et al., 2023a; Zhang et al., 2024a; Zhao et al., 2024), especially driven by innate reasoning capabilities and approximating human intentions of LLMs. With the exploration of LLM-based recommendation (Gao et al., 2023; Hou et al., 2023; Liu et al., 2023b), this direction has emerged as a promising approach for the next-generation recommendation systems (Zeng et al., 2021; Liu et al., 2023c; Wang et al., 2023). Figure 1. An example of comparing different structures of LLM in Recommender Systems (RS). Figure 1(a) and (b) depict current approaches for incorporating LLM into recommendation models. Both methods utilize textual data as input, such as user and item profiles. Nevertheless, in the context of ID-based recommendations, solely ID data is available, devoid of any textual information. When dealing with recommendation scenarios that solely rely on ID data, we investigated the application of LLM for ID-based recommendation, as shown in Figure 1(c). In the context of recommender systems, accurately capturing patterns in user interaction data (ID data) is crucial for generating relevant recommendations. The core goal of recommendation algorithms is to predict the items a user is likely to engage with based on their interaction history. Over the years, various methods have been developed to improve recommendation performance, including Markov Chains (He and McAuley, 2016; Rendle et al., 2010), RNN/CNN models (He and McAuley, 2016; Rendle et al., 2010), self-attentive models (Kang and McAuley, 2018; Li et al., 2020; Zhao et al., 2021), and more recently, graph convolutional network (GCN)-based models (Wang et al., 2019; Chen et al., 2020a; Yu et al., 2022). These advancements stem from increasingly sophisticated architectures designed to better capture the complex relationships in interaction data. GCN-based models, for example, excel at capturing intricate relationships between users and items compared to traditional CNN-based models. However, the emergence of LLMs introduces a new paradigm with even greater capacity for modeling and reasoning, surpassing existing structures like GCNs (Minaee et al., 2024; Wang et al., 2024; Ren et al., 2024; Wu et al., 2024). Given LLMs’ ability to model and interpret complex data, they present a promising direction for processing user interaction data (ID data). Various studies have demonstrated that LLMs can successfully address graph-related tasks, which rely heavily on understanding and reasoning about relationships within ID data (Zhang et al., 2024b; Guo et al., 2023; Ye et al., 2024; Geng et al., 2022b; Sun et al., 2024). For instance, LLM4DyG (Zhang et al., 2024b) constructs prompts using node and edge IDs, which are then processed by LLMs for inference, effectively tackling graph-based tasks. This suggests that employing LLMs to model and interpret ID data is not only feasible but also a research direction worth exploring further. For the studies of LLMs in recommendations, the current approaches can be broadly categorized into two groups. As shown in Figure 1(a), the first category involves leveraging LLMs as enhanced feature extractors to improve the performance of recommendation models in terms of accuracy or interpretability (Li et al., 2023c, b; Liu et al., 2023a; Wei et al., 2023). For example, LLMRec (Wei et al., 2023) uses LLMs to explicitly model with three types of features, such as user profile. In this category, LLMs serve as tools to extract valuable features from textual data, which contribute to improving recommendation models. As shown in Figure 1(b), the second category employs LLMs as a powerful recommendation system to directly generate recommendation results, such as a list of recommended items (Hou et al., 2023; Sun et al., 2023; Dai et al., 2023b). LLM-driven recommendation systems leverage the inherent capabilities of LLMs to generate recommendations based on the textual data they process. It is worth noting that both of these approaches primarily rely on textual data, such as book/movie titles and knowledge labels. The textual data forms the foundation for constructing recommendation paradigms, capitalizing on the generalization capabilities of LLMs to enhance recommendation performance. However, in ID-based recommendation, textual data is absent, containing only core ID information—namely, user IDs and item IDs—without corresponding textual descriptions for users and items (Yuan et al., 2023; Rendle et al., 2012; Yuan et al., 2022). Therefore, instead of focusing on designing an LLM-based recommendation system utilizing textual data, we explore LLM utilizing pure ID data in ID-based recommendation, as depicted in Figure 1(c). Specifically, we investigate the following two questions: Q1: Can LLM feed ID data to generate results that meet recommendation requirements? Many studies have shown that the representations learned by LLMs are applicable to various tasks enriched with textual side information (Borisov et al., 2023; Carranza et al., 2023; Mysore et al., 2023). Nevertheless, in existing recommendation systems, countless models have been developed that rely solely on user/item IDs to generate high-quality recommendations, eschewing the use of any textual or content-based information (Rendle et al., 2009; He et al., 2020; Chen et al., 2020a; Yu et al., 2022; Sun et al., 2019). In this study, we leverage ID-based recommendation as a domain task to investigate the applicability of LLM when confronted with ID data. Before the emergence of large language models, there were already many pre-trained language models, such as BERT (Devlin et al., 2018) and T5 (Raffel et al., 2020). These pre-trained language models have been successfully applied to various recommendation tasks (Qu et al., 2019; Sakata et al., 2019; Yang et al., 2019a, b; Hou et al., 2022). For example, UniSRec(Hou et al., 2022) improving recommendation models using pre-trained language modelsr (e.g. BERT). P5 (Geng et al., 2022b) pretrains on an encoder–decoder Transformer model to effectively model user behavior sequences (i.e., ID sequences). The success of using pre-trained language models in recommendation tasks can be attributed to their inherent linguistic capabilities. These linguistic capabilities encompass various facets, including the storage of factual knowledge and model sequential data. Meanwhile, LLMs have witnessed superior outperformance compared to traditional pre-trained language models, so utilization of LLM for recommendation tasks is an appealing avenue, particularly in the context of countless ID-based recommendations. Q2: How can we effectively leverage LLM to enhance ID-based recommendation? ID-based recommendation contain several essential components, such as interaction data, network structure, and ranking loss functions (Rendle et al., 2009; Chen et al., 2020a; Yu et al., 2022). LLMs can assume different roles within the ID-based recommendation pipeline. A straightforward question arises about where to integrate LLM in an ID-based recommendation model. Consequently, depending on the specific role, we must design different paradigms for LLMs, such as fine-tuning and prompting. The primary focus is on how to adapt LLM for an ID-based recommendation model effectively. The utilization of core ID data in an ID-based recommendation model primarily relies on interaction data, which is abundant and readily accessible. If a large language model (LLM) demonstrates proficiency in processing and understanding ID data, it presents an opportunity to integrate LLM as an element of recommendation. Specifically, by augmenting interactive data within the ID-based recommendation model using LLM capabilities, we can enhance the precision of recommendations. In this paper, we seek to present innovative solutions for integrating LLM into ID-based recommender systems. The basic idea is to feed pure ID data from recommendation tasks into a pre-trained LLM. Suppose the output of the LLM is meaningful and can improve recommendation performance. In that case, it can reflect that the LLM can understand and process ID data within the context of recommendation tasks. Conversely, suppose the output of the LLM is random, inaccurate, or lacks meaningfulness, resulting in a deterioration in recommendation performance. In that case, the LLM still lacks the ability to understand ID data and recommendation tasks intuitively. Based on the above idea, we propose a novel framework, namely the Large Language Model for ID-based Recommendation (LLM4IDRec). We designed a prompt template to process ID data and ensure that the output meets the recommended requirements (Q1). Meanwhile, LLM4IDRec utilizes LLM to augment the ID data and then enhance the ID-based recommendation model (Q2). Specifically, the ID data is initially transformed into training data through a designed prompt template that is understandable to language models. These generative training data are fed into the pre-trained LLM and adopt the fine-tuning strategy. After training, the fine-tuned LLM can generate ID data, effectively augmenting the original ID data. LLM4IDRec represents a pioneering approach to enhancing the capabilities of ID-based recommendation models through the power of LLM. Our main contributions are summarized as follows: • We intend to explore the potential of LLM and investigate a key question: Can the LLM adapt to the ID-based recommendation? This is different from existing work centred around textual data. While prior studies predominantly utilize textual data, such as attribute or title, to represent items and users in prompts, ID-based recommendations lack such textual information, relying solely on ID data. This unexplored potential of LLMs in handling ID data within recommendation paradigms is a driving force behind their integration. We propose that prompting LLMs mainly with textual data overlooks their full potential in recommendation tasks. Instead, delving deeper into ID-based data patterns inherent in interaction data can unlock novel insights and improve recommendation precision, thus justifying the use of LLMs. • We propose a novel approach, LLM4IDRec, which leverages LLM to augment ID data and subsequently utilizes this augmented data to improve the performance of ID-based recommendations.Specifically, we design a tailored prompt template specifically for ID-based recommendation tasks. This template, combined with two methods for generating training data to capture both local and global ID data structures. Additionally, combining fine-tuning strategy and two filter strategies guide LLMs in capturing ID-based data and generating interaction data aligned with user preferences. These modules significantly enhance the effectiveness of LLM4IDRec. • In experiments conducted using three publicly available datasets, LLM4IDRec consistently outperforms existing ID-based recommendation methods by only augmenting the input ID data. These results demonstrate the rationality of LLM4IDRec and the feasibility of LLM in the ID-based recommendation."
https://arxiv.org/html/2411.01785v1,Transferable Sequential Recommendation via Vector Quantized Meta Learning,"While sequential recommendation achieves significant progress on capturing user-item transition patterns, transferring such large-scale recommender systems remains challenging due to the disjoint user and item groups across domains. In this paper, we propose a vector quantized meta learning for transferable sequential recommenders (MetaRec). Without requiring additional modalities or shared information across domains, our approach leverages user-item interactions from multiple source domains to improve the target domain performance. To solve the input heterogeneity issue, we adopt vector quantization that maps item embeddings from heterogeneous input spaces to a shared feature space. Moreover, our meta transfer paradigm exploits limited target data to guide the transfer of source domain knowledge to the target domain (i.e., learn to transfer). In addition, MetaRec adaptively transfers from multiple source tasks by rescaling meta gradients based on the source-target domain similarity, enabling selective learning to improve recommendation performance. To validate the effectiveness of our approach, we perform extensive experiments on benchmark datasets, where MetaRec consistently outperforms baseline methods by a considerable margin.","Thanks to recent advances in language modeling, sequential recommendation has experienced significant improvements in capturing user-item transition patterns [1, 2, 3, 4, 5, 6]. While sequential recommendation outperforms traditional methods, a common challenge is that well-trained models cannot be reused for an unseen domain. As such, transferable recommenders are proposed for quick adaptation to a different target domain [7, 8]. One popular approach is to leverage shared information across domains (e.g., shared items) to enhance adaptation performance [9, 10, 11]. Another stream of cross-domain recommendation aims at learning domain-invariant features [12, 13, 14]. However, the mentioned approaches assume (partially) overlapping user / item groups or require explicit correspondences. Therefore, they are inapplicable upon large source-target domain discrepancy [15]. Recently, transfer learning methods are proposed by utilizing auxiliary information, where descriptive input (e.g., item title) is encoded as features [16, 17, 18]. Yet current approaches may cause performance drops due to the over-emphasis of domain-specific features [19]. Additionally, such methods require additional input, rendering them less effective in text-scarce or sensitive domains. To generalize transferable sequential recommenders to universal model architectures and recommendation scenarios, we consider an ID-only, non-overlapping and multi-source transfer learning setting, where items are solely represented with IDs and user interaction histories are sequences of item IDs in chronological order. Moreover, we assume zero overlapping of shared information across domains, that is, the involved domains only comprise of mutually exclusive user and item groups. Consequently, our approach enables the transfer of knowledge from arbitrary source domains to a different target domain in spite of the input heterogeneity, which significantly extends the applicability of cross-domain recommendation. The primary challenge of this setting is twofold: (1) the disjoint input spaces and item-level differences can lead to alignment difficulties across different domains; and (2) user behavior patterns in source domains may differ from those in the target domain, potentially causing negative transfer (e.g., performance drop) upon large domain discrepancy. To this end, we propose vector quantized meta learning for universally transferable sequential recommenders (MetaRec). MetaRec can accommodate arbitrary recommender architecture and consists of: (1) vector quantization (VQ) and (2) meta transfer. VQ solves the input heterogeneity problem by mapping the original item embeddings to a shared feature space. Instead of introducing additional parameters, we apply weights from the target domain embedding table as codebook in VQ. Then, the output vectors in the aligned feature space are used as item features to predict the next interaction. Despite quantizing the item representations, transition patterns from the source domains may be of different similarity to the target domain. Therefore, we additionally design meta transfer that adaptively learns to transfer knowledge from data-intensive source domains to the data-scarce target domain. Specifically, we update the parameters with sampled source domain data (i.e., source tasks), followed by deriving meta gradients using sampled target domain examples. Based on source-target gradient similarity, we rescale the meta gradients to optimize the learning from different source tasks. As such, MetaRec learns domain-invariant features from source optimization paths with the objective of improving the target domain performance. We summarize our contributions below: 1. To the best of our knowledge, we are the first to propose a solution for cross-domain sequential recommendation based on an ID-only setting with disjoint item groups. 2. The proposed vector quantization maps item embeddings across domains to a well-aligned feature space. Moreover, our meta transfer ‘learns to transfer’ from multiple sources for improved target domain performance. 3. We demonstrate the effectiveness of our MetaRec with extensive experiments over multiple source-target dataset selections, where the proposed MetaRec consistently outperforms baseline methods with considerable improvements in recommendation performance."
https://arxiv.org/html/2411.01690v1,Co-clustering for Federated Recommender System,"As data privacy and security attract increasing attention, Federated Recommender System (FRS) offers a solution that strikes a balance between providing high-quality recommendations and preserving user privacy. However, the presence of statistical heterogeneity in FRS, commonly observed due to personalized decision-making patterns, can pose challenges. To address this issue and maximize the benefit of collaborative filtering (CF) in FRS, it is intuitive to consider clustering clients (users) as well as items into different groups and learning group-specific models. Existing methods either resort to client clustering via user representations—risking privacy leakage, or employ classical clustering strategies on item embeddings or gradients, which we found are plagued by the curse of dimensionality. In this paper, we delve into the inefficiencies of the K-Means method in client grouping, attributing failures due to the high dimensionality as well as data sparsity occurring in FRS, and propose CoFedRec, a novel Co-clustering Federated Recommendation mechanism, to address clients heterogeneity and enhance the collaborative filtering within the federated framework. Specifically, the server initially formulates an item membership from the client-provided item networks. Subsequently, clients are grouped regarding a specific item category picked from the item membership during each communication round, resulting in an intelligently aggregated group model. Meanwhile, to comprehensively capture the global inter-relationships among items, we incorporate an additional supervised contrastive learning term based on the server-side generated item membership into the local training phase for each client. Extensive experiments on four datasets are provided, which verify the effectiveness of the proposed CoFedRec. The implementation is available at https://github.com/Xinrui17/CoFedRec.","With the rapid development of e-commerce and digital services, people have become increasingly digital-centric (Lu et al., 2015). They now spend a significant amount of time online, exploring products, content, and services tailored to their interests. Traditional recommender systems (RS) (Qin et al., 2021; Adomavicius and Tuzhilin, 2005) have proven to be indispensable for e-commerce giants and various digital service providers. However, these systems usually operate by consolidating vast amounts of user data centrally, leading to potential privacy concerns. Federated learning (FL) (McMahan et al., 2017; Konečnỳ et al., 2016; Bonawitz et al., 2019) is a method where multiple clients collaboratively train a deep learning model using their local data. This decentralized approach promotes efficient information exchange and ensures that each participant’s data remains private, without being exposed to a central authority or other participants. The Federated Recommender System (FRS) (Yang et al., 2020; Wu et al., 2021) is built on this idea. FRS is a specialized implementation of FL for recommendation tasks. Instead of directly sending user interaction data to a central server, FRS processes the data locally on users’ devices and only the essential model updates are sent back to the central server for global aggregation. Unlike other applications of FL (Qayyum et al., 2022; Yang et al., 2021), where there are fewer clients and each client possesses a large amount of data from multiple individuals (known as cross-silo FL (Kairouz et al., 2021)), in FRS, each user acts as a client constituting only one single user’s profile (also known as cross-device FL (Karimireddy et al., 2021)). There is an increasing number of works (Ammad-Ud-Din et al., 2019; Lin et al., 2020a; Wu et al., 2022b) exploring solutions for FRS. A typical approach involves the utilization of FedAvg (McMahan et al., 2017) to generate a global model and then fine-tune the model on the client side (Zhang et al., 2023b). However, this single global aggregation is inherently designed for IID data. In practical scenarios, the data available on each device is generated or produced by users, usually non-IID (Ghosh et al., 2019), reflecting users’ different preferences or decision habits. To model the heterogeneity across the clients (users), there are works (Frisch et al., 2021; Luo et al., 2022; Yuan et al., 2023) that assume the whole population could be partitioned into distinct clusters or groups, characterized by analogous preferences. On the other hand, collaborative filtering (CF) (Mnih and Salakhutdinov, 2007; He et al., 2017) has proven successful in recommender systems whose power is confined in the federated setting where the entire dataset is not available. However, we can expect an increase in accuracy by finding out the neighbors of users through clustering and then gathering collaborative insights. In this light, learning a group-level model customized for each user group can boost the algorithm’s adaptability to heterogeneous clients’ data and the ability to transfer positive knowledge among clients by factoring in collaborative insights. In this paper, however, we observe that the widely deployed clustering method which groups the clients using a distance function applied to the updates uploaded by clients (Long et al., 2022; Pan et al., 2022; Sattler et al., 2020) in FL is inefficient in the FRS setting since querying neighbors of high quality is nearly impossible when the feature space is sparse. To address the challenges mentioned above, we propose a co-clustering mechanism CoFedRec for FRS to effectively group clients without accessing their profiles. The core insights come from (i) the heterogeneity across clients in FR (ii) the understanding of CF whose key idea is to predict the interests of a user by collecting preferences from many neighborhoods. Specifically, we turn to the experiment results to analyze the inherent limitations of the classical clustering strategies and introduce the co-clustering mechanism. In each communication round, the global aggregation is performed as a preliminary step to gather the global item relationship, which yields an item membership via the K-Means clustering technique upon the global item representation. The server algorithm then implements the co-clustering by computing similarity scores among clients regarding a selected item category, which allows a specific item category to cluster users into two distinct groups, the similar group, and the dissimilar group. Within the similar group, users tend to react similarly towards that type of item. All the clients in the similar group will update their item embedding network with the aggregated group model while those in the dissimilar group will retain their local model waiting for the subsequent communication rounds. In addition to the group model, the item membership will be distributed to all the clients. Inspired by the theory of Supervised Contrastive Learning (SCL) (Khosla et al., 2020), a local supervised contrastive term is integrated into the local training phase, ensuring that the locally learned item representations retain the global item insights. Our contributions are summarized as follows: • We analyze the failure of classical clustering technique K-Means in the federated recommendation setting and propose a novel co-clustering federated recommendation mechanism CoFedRec which groups users based on specific item categories within each communication round and generates an intelligent group model containing the collaborative information from the neighbors. Our proposed paradigm applies to different backbones. • We introduce a supervised contrastive term into the local training phases to encode the global item relationship in the user individual item network. This ensures that our proposed CoFedRec not only effectively leverages user collaborative information, but also seamlessly integrates global insights into the local training process. • We conduct extensive experiments on four real-world datasets with various settings, which demonstrate the effectiveness and rationality of CoFedRec."
https://arxiv.org/html/2411.01540v1,Efficient and Robust Regularized Federated Recommendation,"Recommender systems play a pivotal role across practical scenarios, showcasing remarkable capabilities in user preference modeling. However, the centralized learning paradigm predominantly used raises serious privacy concerns. The federated recommender system (FedRS) addresses this by updating models on clients, while a central server orchestrates training without accessing private data. Existing FedRS approaches, however, face unresolved challenges, including non-convex optimization, vulnerability, potential privacy leakage risk, and communication inefficiency. This paper addresses these challenges by reformulating the federated recommendation problem as a convex optimization issue, ensuring convergence to the global optimum. Based on this, we devise a novel method, RFRec, to tackle this optimization problem efficiently. In addition, we propose RFRecF, a highly efficient version that incorporates non-uniform stochastic gradient descent to improve communication efficiency. In user preference modeling, both methods learn local and global models, collaboratively learning users’ common and personalized interests under the federated learning setting. Moreover, both methods significantly enhance communication efficiency, robustness, and privacy protection, with theoretical support. Comprehensive evaluations on four benchmark datasets demonstrate RFRec and RFRecF’s superior performance compared to diverse baselines. The code is available to ease reproducibility111https://github.com/Applied-Machine-Learning-Lab/RFRec.","In recent years, recommender systems (RSs) (Resnick and Varian, 1997; Jannach et al., 2010; Naumov et al., 2019; Hu et al., 2008; Koren et al., 2009; Liu et al., 2023b; Lin et al., 2022; Zhang et al., 2023; Wang et al., 2023; Li et al., 2022, 2023a) have become prevalent in various practical scenarios, such as e-commerce, online movie or music platforms, providing personalized recommendations for users (Lu et al., 2015; Schafer et al., 1999; Celma, 2010; Liu et al., 2023d; Li et al., 2023b; Liu et al., 2023a; Lin et al., 2023). Matrix Factorization (MF)-based models (Hu et al., 2008; Koren et al., 2009; Koren, 2008; Mnih and Salakhutdinov, 2007), the most prevalent RS solutions, leverage a centralized learning paradigm that collects user data and trains the model in a central server. The centralized approach can address user preferences well and has achieved great success. However, there emerge increasing concerns from individual users about privacy issues during information storage and collection in central server (Yang et al., 2020, 2019). In response, the authorities are strengthening data privacy protection legally (Yang et al., 2019), such as issuing the General Data Protection Regulation (GDPR) (Regulation, 2018). Exploring a practical approach that can train the models without directly utilizing user data is urgent in the recommendation domain. Federated Learning (FL) (Konečnỳ et al., 2016a, b; McMahan et al., 2016) has proven its efficacy in addressing the aforementioned data-isolated problem. In contrast to the centralized learning paradigm, FL learns the optimal model by continuously communicating the model parameters or gradients between the local client (user) and central server without aggregating client data to the server. Therefore, FL is promising to be the solution to address the privacy issue in the recommendation. Recently, some efforts have been exerted to integrate the FL technique into recommender systems to address the privacy issue, named as federated recommender systems (FedRS) (Ammad-Ud-Din et al., 2019; Lin et al., 2020a; Chai et al., 2020; Muhammad et al., 2020; Lin et al., 2020b; Wu et al., 2021; Dolui et al., 2019; Sun et al., 2022). Using the same optimization formulation as centralized RS, mainstream FedRS methods (e.g., MF-based FedRS) learn the user and item latent feature vectors and utilize their inner product to predict the rating matrix. In the FL setting, the user feature vectors are updated only on the clients. In contrast, the gradients of item feature vectors are aggregated to the server for item feature matrix updating. This approach can train the recommendation model without accessing and utilizing user data, alleviating privacy concerns. However, we recognize that existing FedRS methods generally suffer from several deficiencies. 1) Non-convex Optimization: most FedRS methods solve the RS optimization problem formulated as the MF error minimization directly (Ammad-Ud-Din et al., 2019; Lin et al., 2020a; Chai et al., 2020), which is essentially non-convex. The non-convex optimization may converge to sub-optimal points, jeopardizing the recommendation performance. Moreover, constrained by the FL setting, the user and item feature vectors defined in the formulation must be updated in the client and server, respectively, which can lead to unstable model training. 2) Vulnerability: the existing methods leverage the alternating update rule, where the client and server highly depend on each other to update (Ammad-Ud-Din et al., 2019; Wu et al., 2021; Takács and Tikk, 2012). Therefore, the update process may terminate when some clients cannot participate due to network or device issues. 3) Privacy Leakage Risk: in the training process, the transmission of gradients may still lead to private information leaks (Chai et al., 2020). Specifically, knowing two continuous gradients of a user during the communication process, the server can deduce the user ratings by the algebra operation of these two gradients. Though privacy protection methods can be equipped to prevent privacy leakage, such as leveraging homomorphic encryption and generating pseudo items (Liu et al., 2023c; Chai et al., 2020; Minto et al., 2021), such methods bring considerable additional computational or communication costs. 4) Communication Inefficiency: FedRS methods have a low convergence rate due to the alternating update rule, no matter alternating least squares (ALS) (Takács and Tikk, 2012; Haldar and Hernando, 2009) or alternating stochastic gradient descent (SGD) (Bottou, 2010; Koren et al., 2009). Specifically, these methods converges to the optimal with ε𝜀\varepsilonitalic_ε-accuracy in at least O⁢(log⁡m+log⁡1ε)𝑂𝑚1𝜀O(\log m+\log\frac{1}{\varepsilon})italic_O ( roman_log italic_m + roman_log divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) iterations (Lee and Stöger, 2023; Jain et al., 2013; Takács and Tikk, 2012), where m𝑚mitalic_m is the number of items. Hence, the number of communication rounds of existing methods is also O⁢(log⁡m+log⁡1ε)𝑂𝑚1𝜀O(\log m+\log\frac{1}{\varepsilon})italic_O ( roman_log italic_m + roman_log divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ), which is tremendous when m𝑚mitalic_m is large. In this paper, we first propose a problem formulation for FedRS and reformulate its optimization as a convex problem, which guarantees that the model converges to global optimal. As a regularized empirical risk minimization (RERM) (Marteau-Ferey et al., 2019; Zhang and Xiao, 2017), it consists of cumulative local loss (task term) and global loss (regularization term). Under the guidance of this formulation, we design two methods, namely RFRec and its faster version, RFRecF. In particular, we propose a Regularized Federated learning method for Recommendation (RFRec) to solve this RERM problem in a local GD manner. It theoretically ensures strict convergence to optimal with a linear convergence rate, which has an edge on communication efficiency. In addition, to pursue further communication efficiency, we leverage a non-uniform SGD training manner to update the task term and regularization term stochastically (RFRecF). This optimization mechanism enables RFRecF to achieve fewer expected communication rounds per iteration, thereby reducing communication costs and striking a balance between performance and efficiency. The minimum communication iterations of both methods is O⁢(log⁡1ε)𝑂1𝜀O(\log\frac{1}{\varepsilon})italic_O ( roman_log divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ), which is independent of item number m𝑚mitalic_m and achieves a remarkable reduction over existing methods. Our main contributions can be summarized as follows: • We propose a convex optimization formulation of FedRS, formulated as a regularized empirical risk minimization (RERM) problem, which can guide FedRS methods design with theoretical support for convergence and communication efficiency. • We put forward RFRec to solve the proposed RERM problem and the faster version, RFRecF, for further communication efficiency. We theoretically and experimentally prove their robustness and privacy preservation capability, and they own a guarantee of advancing communication efficiency. • Extensive experiments are conducted on four public benchmark datasets, demonstrating the state-of-the-art performance of our proposed methods against advanced baselines. We also conduct in-depth analysis to verify the efficacy and efficiency."
https://arxiv.org/html/2411.01537v1,LinRec: Linear Attention Mechanism for Long-term Sequential Recommender Systems,"Transformer models have achieved remarkable success in sequential recommender systems (SRSs). However, computing the attention matrix in traditional dot-product attention mechanisms results in a quadratic complexity with sequence lengths, leading to high computational costs for long-term sequential recommendation. Motivated by the above observation, we propose a novel L2-Normalized Linear Attention for the Transformer-based Sequential Recommender Systems (LinRec), which theoretically improves efficiency while preserving the learning capabilities of the traditional dot-product attention. Specifically, by thoroughly examining the equivalence conditions of efficient attention mechanisms, we show that LinRec possesses linear complexity while preserving the property of attention mechanisms. In addition, we reveal its latent efficiency properties by interpreting the proposed LinRec mechanism through a statistical lens. Extensive experiments are conducted based on two public benchmark datasets, demonstrating that the combination of LinRec and Transformer models achieves comparable or even superior performance than state-of-the-art Transformer-based SRS models while significantly improving time and memory efficiency. The implementation code is available online at https://github.com/Applied-Machine-Learning-Lab/LinRec.","In recent years, sequential recommender systems (SRSs) have become an increasingly popular and widely-applied technology (Wang et al., 2015; He et al., 2018; Wang et al., 2019; Huang et al., 2018; Tang et al., 2019; Chen et al., 2022; Xie et al., 2022; Qiu et al., 2022; Zhang et al., 2021), with applications in various practical scenarios such as social media, e-commerce, and online movie platforms (Liu et al., 2023b; Ge et al., 2021; Fan et al., 2021; Liu et al., 2023a; Zhao, 2022; Zhao et al., 2021a, 2019, 2018a; Ren et al., 2022). In practice, users’ historical interaction sequences are typically long-term, which contain valuable yet unequal information (i.e., different interactions’ importance), including more and less recent interactions, for revealing users’ actual preferences (Zhao et al., 2022; Zhang et al., 2020b; Zou et al., 2020; Zhang et al., 2023a; Yang et al., 2023; Zhao et al., 2020a, 2018b, 2017, b; Zhang et al., 2023b, 2022; Zheng et al., 2022). Therefore, identifying important interactions while not losing valuable information from a sequence (Wang et al., 2019; Chen et al., 2018), thus learning better sequence representation for making next-item recommendations, leads to the problem of long-term sequential recommendation. Towards this purpose, the Transformer architecture (Vaswani et al., 2017) has gained significant attention since its capabilities for learning informative long-term sequential patterns among historical user-item interactions. The core component of Transformer-based models is the dot-product attention mechanism (Vaswani et al., 2017), which computes the corresponding attention matrix for distinguishing items’ importance by a dot-product operation between the query and key matrices (Query and Key for short), thus learning sequence representations. For example, BERT4Rec (Sun et al., 2019) uses multi-head self-attention and simultaneously calculates the attention score of all positions. FDSA (Zhang et al., 2019) introduces multiple attention blocks to depict the potential features. SASRec (Kang and McAuley, 2018) controls the predictions based on only a small amount of actions by adopting an attention mechanism. However, a significant limitation of these models arises when dealing with long-term sequences, where the sequence length N𝑁Nitalic_N is far greater than the item embedding size d𝑑ditalic_d (N≫dmuch-greater-than𝑁𝑑N\gg ditalic_N ≫ italic_d). Due to the dot-product operation in the attention layer, the computational and memory complexity of transformers are 𝒪⁢(N2)𝒪superscript𝑁2\mathcal{O}(N^{2})caligraphic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ), resulting in the complexity could be dramatically increased with the problem scale (i.e., sequence lengths) for long-term sequential recommendation (Li et al., 2023, 2022; Liang et al., 2023; Zhao et al., 2023). In view of the above limitations, several approaches have been proposed to address the high computational costs of Transformer models, including Fixed Patterns (FP), Combination of Patterns (CP), Learnable Patterns (LP), Neural Memory, Low-Rank Methods, Kernels, and Recurrence (Tay et al., 2022). Some of these categories, such as FP, LP, and low-Rank, are theoretically less complex than the standard dot-product attention and have been shown to effectively reduce memory and time costs in practice (Qiu et al., 2019; Parmar et al., 2018; Wang et al., 2020b; Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020; Wang et al., 2020a; Katharopoulos et al., 2020). Notably, there are several efficient transformer models with linear complexity, such as Linformer (Wang et al., 2020a), Linear Transformers (Katharopoulos et al., 2020), and Big Bird (Zaheer et al., 2020). However, these methods may not be well-suited for SRSs, particularly when dealing with long-term sequences, as they often introduce additional steps to improve efficiency while sacrificing accuracy and stability. For instance, to reduce the rank of the attention matrix, Linformer (Wang et al., 2020a) introduces the projection for both Query and Key, which results in additional complexity and impairs accuracy, jeopardizing recommendation performance. In this paper, we propose an efficient attention mechanism to address the issue of high complexity transformers with linear complexity, called L2-normalized Linear attention for long-term sequential Recommender systems (LinRec). Our proposed method aims to create a method for the long-term sequential recommender that not only retains the advantages of attention (such as high accuracy) but also significantly reduces the computational complexity. Additionally, LinRec can be conveniently transplanted to any transformer for sequential recommendation systems, providing flexibility and compatibility. Moreover, LinRec does not introduce additional steps, but directly enhances the attention layer to improve efficiency, preserving both effectiveness and stability. To be more specific, the proposed LinRec mechanism involves three key modifications compared to standard attention: (i) changing the dot-product order of the attention mechanism, (ii) using row-wise and column-wise normalization methods for Query (𝑸𝑸\boldsymbol{Q}bold_italic_Q) and Key (𝑲𝑲\boldsymbol{K}bold_italic_K) respectively, and (iii) adding an activation layer to 𝑸𝑸\boldsymbol{Q}bold_italic_Q and 𝑲𝑲\boldsymbol{K}bold_italic_K. These modifications enable LinRec to reduce the complexity from 𝒪⁢(N2)𝒪superscript𝑁2\mathcal{O}(N^{2})caligraphic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) to 𝒪⁢(N)𝒪𝑁\mathcal{O}(N)caligraphic_O ( italic_N ), while still preserving the attention property and providing sparsity. The major contributions to our work are four-fold: • We develop a novel L2 normalized linear attention (LinRec) for long-term sequential recommendations, which reduces the complexity of the attention mechanism from 𝒪⁢(N2)𝒪superscript𝑁2\mathcal{O}(N^{2})caligraphic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) to 𝒪⁢(N)𝒪𝑁\mathcal{O}(N)caligraphic_O ( italic_N ), while preserving the high accuracy of the attention mechanism; • We theoretically analyze the proposed LinRec mechanism, including its effectiveness and efficiency, justifying the correctness of our design choice. Moreover, we explain and factorize the attention operation from a statistical perspective, demonstrating the inherent relation of efficient Transformer with probabilities; • The proposed LinRec mechanism is generally applicable to most transformer models for SRSs, as it can be easily incorporated into existing transformers by replacing the standard dot-product attention, providing flexibility and compatibility; • Empirical evaluations are conducted on two public benchmark datasets (ML-1m and Gowalla), which demonstrates that LinRec possesses competitive or superior performance than representative Transformer-based SRS models, while greatly reducing computational and memory costs."
https://arxiv.org/html/2411.01457v1,Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential Recommendation,"Sequential recommendation (SR) systems excel at capturing users’ dynamic preferences by leveraging their interaction histories. Most existing SR systems assign a single embedding vector to each item to represent its features, and various types of models are adopted to combine these item embeddings into a sequence representation vector to capture the user intent. However, we argue that this representation alone is insufficient to capture an item’s multi-faceted nature (e.g., movie genres, starring actors). Besides, users often exhibit complex and varied preferences within these facets (e.g., liking both action and musical films in the facet of genre), which are challenging to fully represent. To address the issues above, we propose a novel structure called Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential Recommendation (FAME). We leverage sub-embeddings from each head in the last multi-head attention layer to predict the next item separately. This approach captures the potential multi-faceted nature of items without increasing model complexity. A gating mechanism integrates recommendations from each head and dynamically determines their importance. Furthermore, we introduce a Mixture-of-Experts (MoE) network in each attention head to disentangle various user preferences within each facet. Each expert within the MoE focuses on a specific preference. A learnable router network is adopted to compute the importance weight for each expert and aggregate them. We conduct extensive experiments on four public sequential recommendation datasets and the results demonstrate the effectiveness of our method over existing baseline models.","The explosion of information online presents users with a vast and ever-growing sea of items, from products (Geng et al., 2022) and apps (Cheng et al., 2016) to videos (Covington et al., 2016; Zhao et al., 2019). With limited time to explore everything, recommender systems (RS) have become crucial tools for helping users make efficient and satisfying choices. However, user interests are inherently dynamic, evolving over time and making it challenging for platforms to deliver consistently relevant recommendations (Wang et al., 2021). To address these challenges, sequential recommendation (SR) has emerged as a powerful technique. This approach leverages the sequential nature of user interactions, typically captured as sessions containing a series of recent item interactions, to predict the user’s next action (Fang et al., 2020; Wang et al., 2019). Figure 1. A motivation example. \Description A motivation example Mainstream SR systems assign a single embedding vector to each item, capturing its features. Recurrent neural networks (RNNs) (Hidasi et al., 2015a, b), attention-based models (Kang and McAuley, 2018; Sun et al., 2019; Zhang et al., 2019; Song et al., 2019), graph-based models (Wu et al., 2019; Chang et al., 2021; Yang et al., 2023; Guo et al., 2022), and others combine these item embeddings into a sequence representation vector to capture the user intent. This representation is used to predict the next item (e.g., selecting the item with the highest inner product with the sequence representation vector). However, a single embedding cannot well capture an item’s multifaceted nature (e.g., movie genres and starring) (Zhang et al., 2022; Choi et al., 2024). This is particularly problematic when different facets of an item can influence user intent. As illustrated in Figure 1, User 1’s watch history suggests a strong preference for action movies. In this case, recommending another action movie might be appropriate. Conversely, User 2’s movie choices range across genres but all feature Hugh Jackman. In this case, recommending another movie starring Hugh Jackman might be more relevant. These examples highlight how user interests can be dominated by a single facet (genre or actor) within a category (movie). Furthermore, in more realistic scenarios, users can have multiple preferences within a single facet. For example, a user might enjoy both action and musical movies in the facet of genre. Recognizing and addressing these diverse preferences within a sequence is crucial for generating more effective recommendations that cater to specific user interests (Choi et al., 2024). Failing to capture the dominant facet and the specific preferences within each facet can lead to suboptimal recommendations. This highlights the need for recommender systems that can effectively model the dynamic and multi-faceted nature of user interests. Existing research addresses user intent complexity by using hierarchical windows (Guo et al., 2022; Zhang et al., 2023) to capture multi-level user intents from recent items, or by utilizing item representations from multiple items in the sequence instead of using the last item’s representation only to recommend the next interacted item (Choi et al., 2024). However, these methods still neglect the multi-faceted nature of items themselves. To solve the aforementioned issues, we propose a novel structure called Facet-Aware Multi-Head Mixture-of-Experts Model for Sequential Recommendation (FAME). We leverage sub-embeddings from each head in the last multi-head attention layer to predict the next item separately. This approach captures the potential multi-faceted nature of items (e.g., genres, starring) without increasing model complexity. A gating mechanism integrates recommendations from each head and dynamically determines their importance. Furthermore, we introduce a Mixture-of-Experts (MoE) network: this network replaces the query matrix in the self-attention layer, enabling the model to disentangle various user preferences within each facet. Each expert within the MoE focuses on a specific preference within each facet. A learnable router network is adopted to compute the importance weight for each expert and aggregate them. (e.g., whether action or musical movies are the stronger preference). To summarize, our contributions in this paper are as follows: • We propose a Multi-Head Prediction Mechanism to enhance the recommendation quality. This design facilitates capturing the potential multi-facet features of the items without increasing space requirements or the number of parameters. • We propose a Mixture-of-Experts (MoE) network that improves user preference modeling by disentangling multiple preferences in each facet within a sequence. This module seamlessly integrates with existing attention-based models. • Our model demonstrates significant effectiveness compared to various baseline categories (sequential, pre-trained, multi-intent) on four public datasets."
https://arxiv.org/html/2411.01368v1,Combining Financial Data and News Articles for Stock Price Movement Prediction Using Large Language Models,"Predicting financial markets and stock price movements requires analyzing a company’s performance, historic price movements, industry-specific events alongside the influence of human factors such as social media and press coverage. We assume that financial reports (such as income statements, balance sheets, and cash flow statements), historical price data, and recent news articles can collectively represent aforementioned factors.We combine financial data in tabular format with textual news articles and employ pre-trained Large Language Models (LLMs) to predict market movements. Recent research in LLMs has demonstrated that they are able to perform both tabular and text classification tasks, making them our primary model to classify the multi-modal data. We utilize retrieval augmentation techniques to retrieve and attach relevant chunks of news articles to financial metrics related to a company and prompt the LLMs in zero, two, and four-shot settings. Our dataset contains news articles collected from different sources, historic stock price, and financial report data for 20 companies with the highest trading volume across different industries in the stock market. We utilized recently released language models for our LLM-based classifier, including GPT- 3 and 4, and LLaMA- 2 and 3 models.We introduce an LLM-based classifier capable of performing classification tasks using combination of tabular (structured) and textual (unstructured) data. By using this model, we predicted the movement of a given stock’s price in our dataset with a weighted F1-score of 58.5% and 59.1% and Matthews Correlation Coefficient of 0.175 for both 3-month and 6-month periods.","With the advancement of language models and their extensive use in the finance industry [1], these models are now applied in finance for various purposes. These include text classification tasks [2, 3], such as semantic analysis of news 111Referring to classifying news as positive or negative [4]. Investors also use language models like ChatGPT to enhance their financial understanding, and there are studies on financial chatbots and fine-tuning LLMs for financial advisement [5, 6]. Predicting market trends and making informed investment decisions has remained an important task in finance. One use of LLMs in this field is predicting if the price is going up or down in the future, known as Price Movement Prediction [7, 8, 9]. For this purpose, different studies utilize various sources of data to predict price movements. The performance of a stock in the future is typically influenced by all available information in the current state [10]. Numerous factors, including the company’s performance, industry dynamics, human responses, social media, news, and broader political and economic shifts, can impact market behavior. We aim to gather as much information as possible to represent these factors for the stock price movement task. This information can be in tabular or textual format; therefore, we aim to design an LLM-based model that can perform stock price movement classification using data in different formats. We also believe that prompting LLM with different sources of information can prevent hallucination. Major trend in the natural language processing research has shifted to finding the right setup to utilize pre-trained LLMs to solve problems instead of training new models. Additionally, fine-tuning language models requires significant time, computational resources, and a large dataset, therefore we will conduct this analysis in a zero to four-shot setting. The following paragraphs will include our sources of information: Financial Factors Every publicly traded company in the United States is mandated by the Securities and Exchange Commission (SEC) to disclose its financial information at the end of each quarter, and among these regulatory filings, the 10-K report stands out as a comprehensive document detailing the company’s financial performance, risks, and operational insights, reported on a quarterly basis. To include the company’s performance, we included the data available in financial reports (a.k.a. 10-K files) in our analysis. Historical Price The trend in historical price changes for a company can indicate potential future movements, making historical price data important for price movement prediction. News Investors often refer to social media and news, which significantly influence their investing behavior. News stories are the most easily accessible type of content on social media; therefore, we include information from news articles as one source of information in our analysis. We use retrieval methods to extract information from the news, summarize, and rank them based on relevance to provide to the model. Company’s Industry and Services/Products The company’s industry and services/products are important because different factors can affect one industry or one type of service/product. We include keywords of the company’s industry and services as input for our analysis. In our approach, we prompt pre-trained language models with the aforementioned information. We utilize a hypothetical user query to retrieve relevant information about a specific company at a specific time. For this purpose, we collect news articles and financial data for 20 highly traded stocks in the stock market. Our model and data will be available upon publication of this research paper. News Article Sample Title: Should You Invest in Apple (AAPL) Based on Bullish Wall Street Views? Description: Based on the average brokerage recommendation (ABR), Apple (AAPL)… Data-Time: April 12, 2024 at 6:30 AM Keywords: Strong Buy, Zacks Rank, brokerage firms, Apple,… Content: Investors often turn to recommendations made by Wall Street analysts… URL: https://news.google.com/articles/CBMiTWh0d… Title: Congress passes spending bill with TikTok ban on government devices Description: Congress passed a large spending package that includes a bill banning TikTok from being used on government devices and new filing fees for mergers. Data-Time: Dec 23, 2022 at 3:08 PM Keywords: Alphabet Class, Amazon, Meta, Apple, Joe Biden, Politics, Social media Content: URL: https://www.cnbc.com/2022/12/23/congress-passes-spending-bill-with-tiktok-ban-on-government-devices.htm Company Names (Tickers) Apple Inc. (AAPL), Amazon.com Inc. (AMZN), Alphabet Inc. (GOOGL), Microsoft Corporation (MSFT), Tesla, Inc. (TSLA), Facebook, Inc. (FB), Berkshire Hathaway Inc. (BRK.B), Johnson & Johnson (JNJ), Visa Inc. (V), JPMorgan Chase & Co. (JPM), Procter & Gamble Company (PG), Walmart Inc. (WMT), The Coca-Cola Company (KO), Netflix Inc. (NFLX), Pfizer Inc. (PFE), Walt Disney Company (DIS), Nvidia Corporation (NVDA), Alibaba Group Holding Limited (BABA), Adobe Inc. (ADBE), Mastercard Incorporated (MA) News Websites Africa.businessinsider.com, Aljazeera.com, Apnews.com, Benzinga.com, Businessinsider.com, CNBC.com, CNN.com, CFO.economictimes.indiatimes.com, Decrypt.co, Defenseworld.net, Edition.cnn.com, Economist.com, Epaper.financialexpress.com, Etfdailynews.com, Financialbuzz.com, Fool.com, Forbes.com, Foxbusiness.com, FT.com, Globenewswire.com, Investingnews.com, Investorideas.com, Kiplinger.com, Markets.businessinsider.com, Moneycontrol.com, Newswire.ca, Pennystocks.com, Prnewswire.com, SCMP.com, Stockmarket.com, Stocknews.com, Theatlantic.com, Theweek.com, UPI.com, TABLE I: Dataset Information Financial Variables Source Description Total Revenue (10K) Reports Aggregate amount of income generated from the sale of goods or services before deducting any expenses Net Income (10K) Reports Also known as profit or net earnings, is the total amount of revenue earned by a company after deducting all expenses Free Cash Flow (10K) Reports Represents the amount of cash generated by a company’s operations Total Assets (10K) Reports Combined value of all resources owned or controlled by a company Price Momentum Historical Pricing Data Price Momentum 6-12 Months — Measures the relative strength and direction of a stock’s price movement over the past months. Forward Return Historical Pricing Data Expected or projected return on an investment or asset over a future period. Used as target TABLE II: Descriptions for the financial data available in financial reports extracted from Income Statements, balance sheets, and cash flow statements."
https://arxiv.org/html/2411.01354v1,"A Comparative Study on Recommendation
Algorithms: Online and Offline Evaluations on a Large-scale Recommender System","Recommender systems are widely used AI applications designed to help users efficiently discover relevant items. The effectiveness of such systems is tied to the satisfaction of both users and providers. However, user satisfaction is complex and cannot be easily framed mathematically using information retrieval and accuracy metrics. While many studies evaluate accuracy through offline tests, a growing number of researchers argue that online evaluation methods such as A/B testing are better suited for this purpose.We have employed a variety of algorithms on different types of datasets divergent in size and subject, producing recommendations in various platforms, including media streaming services, digital publishing websites, e-commerce systems, and news broadcasting networks. Notably, our target websites and datasets are in Persian (Farsi) language.This study provides a comparative analysis of a large-scale recommender system that has been operating for the past year across about 70 websites in Iran, processing roughly 300 requests per second collectively. The system employs user-based and item-based recommendations using content-based, collaborative filtering, trend-based methods, and hybrid approaches. Through both offline and online evaluations, we aim to identify where these algorithms perform most efficiently and determine the best method for our specific needs, considering the dataset and system scale.Our methods of evaluation include manual evaluation, offline tests including accuracy and ranking metrics like hit-rate@k and nDCG, and online tests consisting of click-through rate (CTR). Additionally we analyzed and proposed methods to address cold-start and popularity bias.","Recommender Systems (RS) are one of the most common and significant services provided by information systems. Music and media streaming platforms, e-commerce, employment websites, and online news publishers all employ RS to display content more efficiently to their users (Gutiérrez et al., 2019; Ray et al., 2021; Huang et al., 2019). RS filter streams of information to present the user with related or personalized content. This filtering process is either focused on item similarity or users’ previous views, the first being item-based and the latter being user-based. More traditional methods, use CB and CF algorithms analyzing the contextual similarity and feedback datasets. Recently, reinforcement and deep learning-based methods are being utilized in the state-of-the-studies. A gap, however, is apparent between the practical and the academic environments, which this paper aims to cover via the evaluation of the algorithms in the practical environment. Parallel to “Recommendation with a Purpose” (Jannach and Adomavicius, 2016), our ultimate objective is to enhance the efficiency of methods in real-world environments. A key distinction between academic and practical settings lies in the evaluation approach. While offline tests, often used in academic research, focus on metrics like accuracy and diversity, online evaluations measure performance through metrics like click-through rate (CTR) and time spent on the platform (PPS). Rossetti et al. (Rossetti et al., 2016) have shown that offline metrics often fail to accurately predict algorithm performance in real-world scenarios. Another factor contributing to the gap between academic research and practical applications is the limited variety and scale of datasets used in academic studies. In practical environments, the attributes of recommended items and user behavior are dynamic across different contexts, leading to unpredictable feedback. A/B testing is a common online testing method. In A/B testing, users are presented with different sets of recommendations, which can result in some groups receiving superior suggestions while others receive inferior ones. The two groups are either exposed to the same algorithm with varying hyper-parameters or two distinct algorithms, typically with one key variant distinguishing the two groups. Online evaluations must be conducted continuously, with performance comparisons assessed over time, as the effectiveness of the recommendation system (RS) is dynamic and can fluctuate due to factors like users’ growing trust in the system or cold-start challenges. (Beel, 2017). This study was conducted in a company that provides RS as a service. It has been providing over 30 million users within the past year; processing with a load of 300 requests per second and maintains a clientele of approximately 70 websites. The service delivers user-based and item-based recommendation systems engaging content-based (CB), collaborative filtering (CF), trend-based, and some hybrids. We tested our RS on a range of platforms, including e-commerce sites, media streaming services, news broadcasting websites, and digital publishing networks. In our analysis, we accounted for the differences in type and scale of data across these websites, recognizing that various data sets can yield differing performance levels with different algorithms. We thoroughly explored multiple algorithms, made comparisons, and considered the contextual factors that influence their effectiveness. Our offline analysis includes ranking and accuracy metrics such as hit-rate@k and, nDCG, mainly used for hyper-parameter tuning. CTR is used as our online metric. Figure 1. The RS architecture is shown above; The green boxes are the services and the orange boxes are data-bases."
https://arxiv.org/html/2411.01304v1,Towards a Knowledge Graph for Teaching Knowledge Graphs,"This poster paper describes the ongoing research project for the creation of a use-case-driven Knowledge Graph resource tailored to the needs of teaching education in Knowledge Graphs (KGs). We gather resources related to KG courses from lectures offered by the Semantic Web community, with the help of the COST Action Distributed Knowledge Graphs and the interest group on KGs at The Alan Turing Institute. Our goal is to create a resource-focused KG with multiple interconnected semantic layers that interlink topics, courses, and materials with each lecturer. Our approach formulates a domain KG in teaching and relates it with multiple Personal KGs created for the lecturers. Poster Page (GitHub): https://naiayti.github.io/TeachingKnowledgeGraphs.io/","Is there really a problem? Teaching courses at the university level can be a demanding task; the field is radically evolving with topic trends changing each year, the material needs to be up-to-date, and finding teaching resources for a topic outside of one’s main expertise can be challenging. The challenge arises both from not having a central hub, which can assist in accessing educational resources, topics, and courses, and also from the pluralistic naming and nonuniform format of similar resources across different institutions. Moreover, in the courses offered by the semantic web community, we observe that equivalant courses cover diverse topics, and these courses often highly differ in naming, such as “Web AI” and “Knowledge Engineering”. Also, the educational material for lectures and labs is found in a large variety of modalities. From the COST DKG Workshop “Teaching Knowledge Graphs"", Malaga, September 2023, it was identified the necessity for providing a resource that could highlight the topics and material taught in KG courses and a platform in which lectures could connect and exchange materials. Therefore, the proposal for a teaching Knowledge Graph (KG) resource was highlighted. Who cares? The primary beneficiaries are the lecturers who teach Knowledge Graph (KG) courses, and students who follow these courses. Those can benefit from a teaching KG in multiple use cases, as indicated in Table 1, as the resource is designed with the intention of supporting teaching and learning in the discovery of related topics, courses, datasets and educational materials related to KGs. By enhancing the teaching process, the lecturers can leverage the resource to better explain sub-concepts, demonstrate practical applications, and provide richer educational experiences. Additionally, the students’ learning is enhanced as they can have access to a central hub with well-structured information about KG courses, and leverage similar content to their course to gain a broader perspective about their study. Subsequently, the teaching KG resource can function as a digital library with content quality assurance, where educational materials are organized by level and topic, and interconnected with the lecturers. Table 1: The use cases presented as competency questions Competency Questions For (sub)topic X Who is teaching X? Which are the materials for X? Which are the prerequisites of X? Which are the labs for X? For course Y Who is the target audience for Y? Which educational level does Y target? Who is teaching Y? Which slides are linked to Y? Which labs are part of Y? Which courses are similar to Y? Which are suggested resources for Y? For dataset D Which exercises exist for D? Which courses use D? For material M Which courses use a material similar to M? How much similar is M to another material? Which topics does M cover? Is M open access? Moreover, the Semantic Web (SW) community needs a methodology for creating an educational teaching KG. This need is two folded; at first, a methodology generated from the community will benefit educators and practitioners. A SW framework for creating educational teaching KGs will grant educators access to the latest advancements that utilize the state-of-the-art standards, and fully explore the capabilities offered by the SW. On the other hand, as education is evolving, the demand for semantic solutions is prominent highlighting the need for the involvement of the SW into the theoretical foundation of learning and teaching applications. Ok, but isn’t it already done? All we know, we are the first working towards a teaching KG for KG courses. However, our contribution does not rely only on this. As Semantic Web technologies have been widely used in the education domain, ontologies are commonly used for semantically enhancing e-learning applications [1, 2]. Additionally, educational ontologies have been deployed for KG construction [3] and recently, a few educational-purpose KGs have been created for a plethora of applications [4, 5, 6, 7, 8, 9, 10, 11, 12]. Moreover, Personal Knowledge Graphs (PKGs) have recently been developed for educational applications [13, 14, 15], where they utilise semantic resources, such as linking entities to encyclopedic KGs. Nonetheless, they are not used in connection to a bigger domain or encyclopedic KG as they do not exchange information, such as updating information about the triples in the encyclopedic or domain KG. However, none of these approaches goes beyond the factual knowledge representation and limits each resource description to high-level connections, such as which resource is taught first and who is the resource’s author. To the best of our knowledge, we are the first to aim to extract knowledge from each resource, such as topics from educational material, and represent them as new knowledge in the KG. This is significantly important, as one of our main contributions lies in defining a new way of creating educational KGs. We achieve this by building a use-case-driven and resource-focused KG that offers a novel representation of the data it contains. The new representation goes beyond the factual knowledge and extraction of metadata, and further includes the statistical analysis and document information retrieval outcomes as part of the KG entities and properties. An example of this case is applying topic modelling to lecture notes [16], and enhancing the teaching KG with new entities as subtopics extracted from the lecture notes. Is there a big picture or you just want a paper published? Our effort is a stepping stone towards a big, multi-lingual, multi-modal, and high quality content educational teaching KG that can serve as the basis of future educational applications in AI and beyond [17]. As KGs can assist AI applications in education to offer more personalised and better learning experience to the learners [4], our goal is to enhance teachers experience by creating the basis for educational teaching KGs focused on quality content. Also, as quality education is underlined as a Sustainability Goal [18], raises the duty of setting standards for educational Semantic Web applications and KG resources in education. Consequently, we guarantee the quality of our resource by gathering topic descriptions and courses from the experts who teach them. Fine, but how are you going to make it? We reuse Semantic Web vocabularies, and standards to develop the teaching KG [19]. Our resource aims to interlink topics, courses, and educational material in order to enable teachers to enhance their courses and help students access multiple resources relevant to their learning goals. Driven by the needs of the users and adding granularity to the representation of material in our teaching KG resource, we aim to contribute both to the Semantic Web advancements as well as set the basis for advanced learning analytics applications. The teaching KG consists of two parts, the domain model and the user model, which are developed as extensions of the EduCOR ontology [1] and PKGs. At first, we create a domain model, the teaching KG resource, which extends the EduCOR ontology. The domain model contains the topics taught, the educational material, such as the lecture notes and labs, and the courses as sequences of the material. Moreover, we model each lecturer as a user model via PKGs [14] which have been shown capable of enabling smart learning environment analytics [20]. Each user model contains the lecturer identification information and is connected with the lecturer course in the domain model. Aren’t there any limitations? There are plenty of limitations to consider when building teaching KGs. Firstly, not all materials are open source and accessible, as frequently educational content stays hidden behind paywalls or requires institutional access. To address this, we develop the PKGs so the users can access the specific materials from each instructor and gain access to that knowledge. Moreover, the diversity in educational material and non-standardized course descriptions make automation the biggest challenge. The material and courses are present in various formats, which make their integration, interlinking, and maintenance a non-trivial task that we aim to tackle by parsing the text that can be extracted from each resource using symbolic and sub-symbolic techniques [21]."
https://arxiv.org/html/2411.01182v1,Graph Cross-Correlated Network for Recommendation,"Collaborative filtering (CF) models have demonstrated remarkable performance in recommender systems, which represent users and items as embedding vectors. Recently, due to the powerful modeling capability of graph neural networks for user-item interaction graphs, graph-based CF models have gained increasing attention. They encode each user/item and its subgraph into a single super vector by combining graph embeddings after each graph convolution. However, each hop of the neighbor in the user-item subgraphs carries a specific semantic meaning. Encoding all subgraph information into single vectors and inferring user-item relations with dot products can weaken the semantic information between user and item subgraphs, thus leaving untapped potential. Exploiting this untapped potential provides insight into improving performance for existing recommendation models. To this end, we propose the Graph Cross-correlated Network for Recommendation (GCR), which serves as a general recommendation paradigm that explicitly considers correlations between user/item subgraphs. GCR first introduces the Plain Graph Representation (PGR) to extract information directly from each hop of neighbors into corresponding PGR vectors. Then, GCR develops Cross-Correlated Aggregation (CCA) to construct possible cross-correlated terms between PGR vectors of user/item subgraphs. Finally, GCR comprehensively incorporates the cross-correlated terms for recommendations. Experimental results show that GCR outperforms state-of-the-art models on both interaction prediction and click-through rate prediction tasks.","Collaborative filtering (CF) models have delivered remarkable recommendation performance by representing each user and item as embedding vectors, where the similarity between users/items is reflected in their embeddings [1, 2]. Consequently, an intuitive paradigm is to infer the potential relation between users and items by assessing the correlation of their embedding vectors, such as through the dot product or multilayer perceptrons. A typical way to obtain these embeddings is to directly apply the matrix factorization on the user-item interaction matrix [3, 4]. Moreover, advancements involve utilizing neural networks to learn interactions [1, 5] or incorporating additional regularizers [6, 7]. As research progresses, the question of how to learn powerful and meaningful user/item embeddings has gained significant interest from both the academic and industrial communities. Recently, there has been a surge in CF-based recommendation models that harness the powerful modeling capabilities of Graph Neural Networks (GNNs) on graph-structured data. These models leverage graph convolutional processes to extract meaningful graph information from user-item behavior graphs progressively gathering information from distant neighbors within the user/item subgraphs [8, 9, 10]. Embedding representations for each user and item are derived after each round of graph convolution, converging into a single super vector across layers that encapsulates their interaction patterns. Subsequently, they follow and employ the conventional approach of computing the dot product of the super vectors to infer the relationship between a given user-item pair. Representatively, NGCF [8] utilizes a concatenation operation to construct the super vectors, while LightGCN [9] employs a weighted sum. This research direction has consistently demonstrated state-of-the-art performance [9, 11, 12]. Figure 1: An illustration of the user-item graph and the different semantic meanings of each hop of neighbors in the user/item subgraph. However, as shown in Figure 1, the recommendation graphs constructed from historical user-item interactions are heterogeneous bipartite graphs [13], where each hop of neighbor in the user-item subgraphs carries a specific semantic meaning. For instance, the first-hop neighbors of a given user comprise its historical interacted items, and the second-hop neighbors are relevant users with similar interests who have interacted with common items. Therefore, explicitly considering whether a user has interacted with similar items or whether its relevant users are interested in the target item can provide valuable information. Simply encoding all subgraph information into single vectors and inferring the user-item relation through dot products will undermine and weaken the semantic information between the user and item subgraphs, thereby wasting untapped potential. By exploiting these potentials, we can not only develop advanced recommendation models but also improve the performance of existing CF models. Moreover, current GNNs adopt recursive graph convolution to aggregate neighbor embeddings layer by layer [8, 9, 14], resulting in an indistinguishable combination of information from different neighbor hops. This blending of information results in impure embeddings for each hop of neighboring nodes. Based on these observations, the traditional paradigm of using a single vector representation holds untapped potential in improving recommendation performance. Therefore, it is promising to provide non-recursive plain graph representation methods to obtain pure embeddings for each hop of neighbors, enabling the representation of a given user/item subgraph while explicitly extracting and considering all cross-correlations between user/item subgraphs. Although showing promising prospects, it remains the following challenges: First, lacking suitable paradigms. Due to the graph convolution and single user-item vector inference architecture in current models, it is not applicable and non-trivial to extract pure embeddings for each hop of neighbors and comprehensively capture meaningful correlations between these embeddings in existing model paradigms. Moreover, ensuring design universality. Recommendation models in different stages of the recommender system exhibit various characteristics. For instance, the models used in the system’s ranking stage and prediction stage have different model architectures and computational efficiency requirements. Consequently, the designed model should adapt to the mainstream recommendation tasks and can plugin into current recommender models. To this end, in this paper, we investigate how to explore a general graph-based recommendation paradigm to exploit the full potential of embedding vectors in user subgraphs and item subgraphs. Specifically, we aim to investigate the following three research questions: (1) How can we obtain pure embedding vectors for each hop of neighbors and represent the given user/item subgraph with more than a single vector? (2) How to construct and model meaningful cross-correlation terms between the vectors of user subgraphs and item subgraphs? (3) How to design a flexible paradigm for different recommendation tasks and can be equipped with current recommenders? By studying these research questions, The main contributions of this paper are organized as follows: • We propose a general graph-based recommendation paradigm called Graph Crossed-correlated Network for Recommendation (GCR), which represents user/item subgraphs as multiple vectors and explicitly extracts and comprehensively considers the cross-correlation between a pair of user-item subgraphs. GCR is not only an advanced recommendation model but also provides the flexibility to improve the performance of existing recommendation models. • We develop a novel non-recursive plain graph representation architecture for subgraph representation in GCR, called Plain Graph Representation (PGR). Different from the previous graph convolution process in the recommendation, PGR first unfolds all neighbors and then aggregates each hop of neighbors, respectively. • We further provide Cross-Correlated Aggregation (CCA) to explicitly extract and comprehensively consider all cross-correlations between the vectors of the hops of user and item subgraphs. We also theoretically illustrate that GCR has significantly higher flexibility than existing state-of-the-art graph-based recommendation models. • Extensive experimental results on both public datasets and the industrial dataset demonstrate that GCR significantly outperforms the state-of-the-art models. Moreover, replacing the dot product or MLP layer in baseline models with GCR can also significantly improve the recommendation performance of both interaction prediction and click-through rate prediction tasks."
https://arxiv.org/html/2411.01178v1,LLM4PR: Improving Post-Ranking in Search Engine with Large Language Models,"Alongside the rapid development of Large Language Models (LLMs), there has been a notable increase in efforts to integrate LLM techniques in information retrieval (IR) and search engines (SE). Recently, an additional “post-ranking” stage is suggested in SE to enhance user satisfaction in practical applications. Nevertheless, research dedicated to enhancing the post-ranking stage through LLMs remains largely unexplored. In this study, we introduce a novel paradigm named Large Language Models for Post-Ranking in search engine (LLM4PR), which leverages the capabilities of LLMs to accomplish the post-ranking task in SE. Concretely, a Query-Instructed Adapter (QIA) module is designed to derive the user/item representation vectors by incorporating their heterogeneous features. A feature adaptation step is further introduced to align the semantics of user/item representations with the LLM. Finally, the LLM4PR integrates a learning to post-rank step, leveraging both a main task and an auxiliary task to fine-tune the model to adapt the post-ranking task. Experiment studies demonstrate that the proposed framework leads to significant improvements and exhibits state-of-the-art performance compared with other alternatives.","Figure 1. Illustration of the search process, encompassing matching, ranking and post-ranking stages. Conventional search engines (SE) and information retrieval (IR) systems are composed of multiple stages including matching and ranking, in which the relevant items are retrieved in matching stages and sorted by relevance in ranking stage. However, in practical applications, the relevance score of a single item is not the sole measure for practical search engines. On e-commerce platforms, the search engine is required not only to provide relevant results but also to maximize users’ purchase rate by presenting the product list. Meanwhile, in the short video search scenario, the search platform also attempts to maximize users’ play time during the entire search session. To this end, a post-ranking stage is suggested in practical applications. As depicted in Figure 1, the post-ranking stage severs as the final stage in search engines. This stage considers multiple attributes (item relevance score, user click/purchase rate, etc.) as well as mutual influences between items, and deliver the final list to optimally enhances user experience in the search session. In IR and SE, the classic Learning-To-Rank (LTR) method formulates a composite objective encompassing item relevance and click/purchase to model the post-ranking stage. Recently, numerous additional efforts have been dedicated to studying this search topic(Liu et al., 2022; Ai et al., 2019; Zhuang et al., 2018; Gong et al., 2020; Abdool et al., 2020; Pang et al., 2020; Huzhang et al., 2023). Concretely, SetRank (Pang et al., 2020) proposes the self-attention method to model mutual influences between items, and DLCM (Ai et al., 2018) suggests a RNN-based approach. PRM (Pei et al., 2019), PRS (Feng et al., 2021a) and PIER (Shi et al., 2023) suggest permutation-based two-stage processes for list generation and evaluation. Another pioneering stream of studies on post-ranking stages focus on the generative method. Seq2slate(Bello et al., 2018) and GRN (Feng et al., 2021b) introduce the generative method in post-ranking stage in recommendation scenario111In recommender system, “re-ranking” refers to “post-ranking” stage. While in information retrieval, ”re-ranking” denotes the ”ranking”. In case of confusion, we call the last stage in search engine as “post-ranking” in this paper. . Regarding the increasing popularity of generative methods, especially its significant success in Large Language Models (LLMs), the generative and LLM-based approaches emerge as a promising avenue for optimizing post-ranking stage in search engine. The advances in Large Language Models (LLMs) have achieved remarkable success in Natural Language Processing (NLP) and IR tasks (Vaswani et al., 2017; Devlin et al., 2019; Touvron et al., 2023a; Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023b), and it has been prompting increasing integration between LLM and practical applications such as search engine (SE). Currently, most existing LLM methods for SE primarily focus on augmenting document retrieval (Bonifacio et al., 2022; Jeronymo et al., 2023; Dai et al., 2023b; Ma et al., 2023a; Wang et al., 2024), document ranking/re-ranking (Qin et al., 2023; Sun et al., 2023b) and modeling relevance between queries and items (Zhuang et al., 2023; Qin et al., 2023; Sachan et al., 2022; Cho et al., 2023). However, despite significant efforts being directed towards matching (i.e., item retrieval) and ranking within SE, the post-ranking stage has been overlooked. Consequently, the explorations of LLMs for the post-ranking stage in SE remain scarce. To implement LLMs for the post-ranking process in search engine, several challenges have yet to be overcome. Issue 1: Heterogeneous Features Input. The input of post-ranking model commonly encompasses heterogeneous features including item descriptions, category ID features, item-level statistical features, and the output (i.e., numerical embedding) from the up-stream (i.e., ranking) stage. However, current LLMs are specifically designed to process semantic textual input only. Directly inputting these feature embeddings into the LLM could potentially lead to confusion, as these embeddings lack semantic context. Therefore, developing a mechanism to seamlessly incorporate these diverse features into the LLM input and align the feature representation with the LLM remains a significant challenge. Issue 2: Task Specification for Post-Ranking. Existing LLMs are designed for general purposes (i.e.conversation, question and answering), exhibiting limited capacities for the post-ranking task in practical applications. Thus, it is essential to explore appropriate adjustments to enhance the LLM’s ability for optimizing the post-ranking task. To address these issues, we propose an LLM-based framework for Post-Ranking in search engine, named LLM4PR, comprising a Query-Instructed Adapter (QIA) component and a backbone LLM. To handle the challenge of heterogeneous features (Issue 1), we propose a feature adaptation step to integrate diverse features. The QIAs take user/item side features as input, and combine various features in a query-instructed manner. Specifically, within the QIAs, the search query assigns distinct weights to diverse feature domains via the attention mechanism and derive the representation vectors for the user/item. Moreover, a template-based generation task is designed to achieve the semantic alignment between the user/item representations and the LLM. Particularly, we freeze the LLM and train the QIAs to generate semantic user/item representations that can be decoded by the LLM. To tackle the post-ranking task challenge (Issue 2), we design a main task and an auxiliary task to fine-tune the LLM4PR to learn to post-rank, named learning to post-rank step. The main task involves taking candidate item list as input and instructing LLM4PR to predict the target post-ranking order in a generative manner. To produce satisfying post-ranking lists, the model is required to assess the quality of various candidate item lists. To this end, we propose an auxiliary task that compares a pair of candidate lists and predicts the superior one. The auxiliary task serves as a supplement to the main task, supporting the model in developing an insight for judging the quality of candidate lists. In the training stage, both the main task and the auxiliary task are trained simultaneously. During the inference stage, we utilize the main task to generate the final post-ranking results. Overall, the contributions of this paper can be summarized as follows: • Firstly, to the best of our knowledge, our LLM4PR is the first LLM-based framework that optimizing the post-ranking stage for search engines. This framework leverages the robust capabilities of LLM and generates the final results straightforwardly. • Secondly, the proposed LLM4PR suggests a novel QIA component and feature adaptation step to align heterogeneous features. Additionally, it introduces an efficient template-based learning to post-rank step for model tuning. • Last but not least, the effectiveness of the proposed LLM4PR is fully validated through comprehensive experiment studies."
https://arxiv.org/html/2411.01039v1,Enhancing Question Answering Precision with Optimized Vector Retrieval and Instructions,"Question-answering (QA) is an important application of Information Retrieval (IR) and language models, and the latest trend is toward pre-trained large neural networks with embedding parameters. Augmenting QA performances with these LLMs requires intensive computational resources for fine-tuning. We propose an innovative approach to improve QA task performances by integrating optimized vector retrievals and instruction methodologies. Based on retrieval augmentation, the process involves document embedding, vector retrieval, and context construction for optimal QA results. We experiment with different combinations of text segmentation techniques and similarity functions, and analyze their impacts on QA performances. Results show that the model with a small chunk size of 100 without any overlap of the chunks achieves the best result and outperforms the models based on semantic segmentation using sentences. We discuss related QA examples and offer insight into how model performances are improved within the two-stage framework.","Accurately answering queries utilizing large-scale datasets has become an important task in the fast evolving filed of natural language processing (NLP). Recent advancements in machine learning and artificial intelligence have led to the development of sophisticated models that can parse, follow, and respond to natural language queries with unprecedented accuracy. Among these, the Retrieval-Augmented Generation (RAG) architecture represents a significant leap forward, combining the strengths of information (vector) retrieval and generative models for QA applications(Siriwardhana et al., 2023). This paper introduces a novel methodology that leverages the RAG architecture, aiming to enhance QA precision through optimized vector retrieval and improved context construction."
https://arxiv.org/html/2411.00786v1,Interpret and Control Dense Retrieval with Sparse Latent Features,"Dense embeddings deliver strong retrieval performance but often lack interpretability and controllability. This paper introduces a novel approach using sparse autoencoders (SAE) to interpret and control dense embeddings via the learned latent sparse features. Our key contribution is the development of a retrieval-oriented contrastive loss, which ensures the sparse latent features remain effective for retrieval tasks and thus meaningful to interpret. Experimental results demonstrate that both the learned latent sparse features and their reconstructed embeddings retain nearly the same retrieval accuracy as the original dense vectors, affirming their faithfulness. Our further examination of the sparse latent space reveals interesting features underlying the dense embeddings and we can control the retrieval behaviors via manipulating the latent sparse features, for example, prioritizing documents from specific perspectives in the retrieval results.","In the realm of information retrieval, dense embeddings derived from large language models (LLMs) have achieved state-of-the-art performances Khattab and Zaharia (2020); Reimers (2019). While these representations offer remarkable accuracy in matching queries to documents, their “black-box” nature poses challenges in applications that demand transparency and control, such as retrieval in bias-sensitive tasks, where users may need to understand the rationale behind the retrieved results and adjust the process to ensure fairness. In contrast, in bag-of-word base sparse retrieval, each dimension is a meaningful word, allowing users to see why certain documents are retrieved, and making it intuitive for users to revise their query keywords to control the retrieval results. Interpretability and controllability are important for building trust with users and facilitate the wide adoption of search technologies Croft et al. (2010). In this paper, we present a novel approach that leverages sparse autoencoders (SAE) to interpret and control dense retrieval systems. Sparse autoencoders have recently been used to improve the interpretability of LLMs by transforming neuron activation patterns into sparse dictionaries (Bricken et al., 2023; Templeton et al., 2024). We upgrade this approach to dense embeddings, incorporating a retrieval-oriented recovery loss which ensures the extracted sparse features remain faithful for retrieval, forming the basis of our interpretability analysis. Our experiments demonstrate the success of this approach. Retrieval using the learned latent sparse features and their reconstructed embeddings both recover the majority of the original dense retrieval accuracy on the MsMarco and Beir benchmarks, ensuring that these features offer genuine interpretability rather than an illusion. Then we explore the interpretability of these sparse features with Neuron to Graph (N2G) approach (Foote et al., 2023), and discover that various fine-grained concepts have been captured in the latent sparse space. To understand controllability through latent features, we conduct quantitative studies by amplifying query-relevant features, which successfully improved retrieval accuracy on the manipulated embeddings, both on the query side and the document side. Then, we perform case studies on multi-perspective queries and confirm that selectively manipulating sparse features from a specific perspective causes the reconstructed embeddings to prioritize documents from that perspective during retrieval. Our source code and extracted features are available at GitHub 111https://github.com/cxcscmu/embedding-scope."
https://arxiv.org/html/2411.00784v1,Fire: Fact-checking with Iterative Retrieval and Verification,"Fact-checking long-form text is challenging, and it is therefore common practice to break it down into multiple atomic claims. The typical approach to fact-checking these atomic claims involves retrieving a fixed number of pieces of evidence, followed by a verification step. However, this method is usually not cost-effective, as it underutilizes the verification model’s internal knowledge of the claim and fails to replicate the iterative reasoning process in human search strategies. To address these limitations, we propose Fire, a novel agent-based framework that integrates evidence retrieval and claim verification in an iterative manner. Specifically, Fire employs a unified mechanism to decide whether to provide a final answer or generate a subsequent search query, based on its confidence in the current judgment. We compare Fire with other strong fact-checking frameworks and find that it achieves slightly better performance while reducing large language model (LLM) costs by an average of 7.6 times and search costs by 16.5 times. These results indicate that Fire holds promise for application in large-scale fact-checking operations. Our code is available at https://github.com/mbzuai-nlp/fire.git.","“Every man has a right to his opinion, but no man has a right to be wrong in his facts.” - Bernard M. Baruch Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks, including both language comprehension and generation (Zhao et al., 2023; Xie et al., 2023a). Consequently, LLMs are now widely applied in various domains (Xie et al., 2023b), and many users increasingly rely on the information they provide. However, this reliance is problematic, as LLMs are capable of producing outputs that are highly confident but factually incorrect, highlighting the critical need for robust fact-checking systems (Akhtar et al., 2023). However, fact-checking the entire output of LLMs in a single step is highly challenging. To address this, Min et al. (2023) proposed decomposing the content into multiple atomic claims, each of which can be individually verified. While this approach simplifies the fact-checking process, assessing the veracity of these atomic claims remains complex, especially when many require sourcing evidence from the web. Indeed, identifying the most relevant evidence online is a key challenge in fact-checking pipelines (Wang et al., 2023). To address this issue, conventional methods, such as FacTool and Factcheck-GPT (Chern et al., 2023; Wang et al., 2023), frame the problem as a question-answering task, as illustrated on the left side of Figure 1. In these approaches, an LLM is prompted to generate N relevant questions, which are then used as search queries by a web search tool. The search results serve as evidence for LLM to determine the factuality of the claim. However, we argue that this process is inefficient in two key aspects. First, it underutilizes the internal knowledge already embedded in LLMs during pre-training. For claims involving common knowledge or widely known events, the LLM could confidently assess the claim without relying on external information. Second, generating multiple search queries concurrently does not align with the typical human reasoning process during search (Hu et al., 2023). Humans tend to begin with an initial query, gather information, and then refine their perspective on the claim, which often leads to the formulation of more effective follow-up queries. Figure 1: Comparisons between Fire and previous frameworks. Previous frameworks typically treat web search and claim verification as distinct processes. In contrast, Fire integrates interactive retrieval and verification. To address this gap, we introduce Fact-checking with Iterative Retrieval and VErification (Fire), an innovative agent-based framework that integrates both the internal knowledge of LLMs and external knowledge sources by unifying the verification process and search query generation into a single step. As illustrated on the right side of Figure 1, Fire employs a mechanism to decide whether to produce the final answer or generate a new search query, continuing the evidence-seeking process. This decision is based on the model’s confidence in its judgment. The closest related work to us is Safe (Wei et al., 2024), depicted in the center of Figure 1. Their method generates web search queries iteratively and subsequently verifies whether the entire retrieved evidence supports the claim. However, this approach lacks flexibility, as it treats evidence retrieval and claim verification as distinct processes, requiring a predetermined fixed number of searches regardless of the claim’s complexity. In contrast, our approach integrates evidence retrieval and claim verification into an iterative framework, encouraging the language model to verify based on its own knowledge and conduct searches only when necessary. Our experiments demonstrate that our method significantly reduces the computational costs of LLMs by an average factor of 7.6, as well as search-related costs by a factor of 16.5, all while maintaining fact-checking performance. In summary, our contributions are as follows: • We present Fire, a simple yet effective interactive framework for fact-checking. Through extensive experiments conducted across multiple datasets, we demonstrate that our framework significantly reduces the LLM computational and search costs, making it a better option for large-scale production. • Our ablation studies demonstrate that the step-by-step reasoning process enhances the model’s confidence in fact-checking, particularly with GPT-4o-mini. For GPT-4o, we observed a similar trend; however, the effect was not as pronounced as that seen with GPT-4o-mini. • We conducted an error analysis and identified several quality issues in the current benchmark datasets, including the presence of ungrounded claims. Additionally, the strict reasoning capabilities of the LLM may incorrectly classify some debatable claims as non-factual."
https://arxiv.org/html/2411.01611v1,Stochastic Communication Avoidance for Recommendation Systems,"One of the major bottlenecks for efficient deployment of neural network based recommendation systems is the memory footprint of their embedding tables. Although many neural network based recommendation systems could benefit from the faster on-chip memory access and increased computational power of hardware accelerators, the large embedding tables in these models often cannot fit on the constrained memory of accelerators. Despite the pervasiveness of these models, prior methods in memory optimization and parallelism fail to address the memory and communication costs of large embedding tables on accelerators. As a result, the majority of models are trained on CPUs, while current implementations of accelerators are hindered by issues such as bottlenecks in inter-device communication and main memory lookups. In this paper, we propose a theoretical framework that analyses the communication costs of arbitrary distributed systems that use lookup tables. We use this framework to propose algorithms that maximize throughput subject to memory, computation, and communication constraints. Furthermore, we demonstrate that our method achieves strong theoretical performance across dataset distributions and memory constraints, applicable to a wide range of use cases from mobile federated learning to warehouse-scale computation. We implement our framework and algorithms in PyTorch and achieve up to 6×6\times6 × increases in training throughput on GPU systems over baselines, on the Criteo Terabytes dataset.","A significant portion of machine learning research has advanced due to better memory and computational speeds of accelerators, alongside faster interconnects and more efficient parallelization in large systems. However, accelerators often have limited memory compared to CPUs, rendering many memory-intensive algorithms infeasible for deployment. One approach to mitigate this issue is to increase memory, but this cannot keep up with the rapid growth of machine learning models. An alternative is to develop new parallelization strategies that balance memory usage and communication, as explored in strategies like those in [1, 2, 3]. Another optimization strategy involves quantization [4, 5, 6], which aims to minimize the memory footprint and computational requirements without significantly impacting accuracy. Embedding tables, which map sparse categorical features to dense vectors [7, 8], are often prime targets for quantization due to their large sizes. By quantizing the embeddings to lower bit-width representations in DLRMs [9], such as 4-bit [10, 11], or performing tensor train decomposition [12], memory usage can be significantly reduced, making it more feasible to train and inference. These methods primarily focus on reducing the embedding size [13], while another solution is to modify the training process to save memory. Examples of this approach include reversible networks [14], which change the model’s structure, and techniques like Checkmate [15], which alter the model’s execution pattern by adding additional operations to backpropagation to decrease the number of intermediate values that need to be stored in memory. Recent algorithms have significantly increased the scale of NLP and CV models by reducing the memory demands per GPU [16, 17, 18], allowing the use of accelerators for extremely large models. However, these methods struggle with models that have large embedding tables, which are not easily managed by pipeline parallelism and remain large in parameter count. Data parallelism also falls short as it is better suited for compute-heavy tasks rather than memory-intensive embedding operations. Furthermore, techniques like recomputation or checkpointing do not suit embeddings well, as their high memory cost does not justify the modest savings from managing intermediate activations. This forces the use of model parallelism, but oftentimes the number of accelerators required to fit the large embedding tables is too great to make model parallelism a financially viable solution. These embeddings can often be terabytes large, but as observed in practice, they are not accessed uniformly at random. In real datasets, the access pattern of these embeddings varies, generally with a small portion of embeddings being accessed far more frequently than others[19]. Existing methods [20] have explored the usage of distributed communication to decrease the communication cost, but the theoretical bounds of communication efficiency has not been analyzed in previous work. In addition, there has been no existing work exploring how various methods of distributing embeddings across GPUs and CPUs impact the system performance. To summarize, our contributions are as follows: 1. We develop a simple framework to calculate the expected communication cost under a training regime. And we expand this framework to address considerations such as determining the optimal levels of communication and caching, as well as methods for adjusting them accordingly. 2. We use the above framework to obtain communication strategies that minimize expected communication costs without caching. We demonstrate that these methods also decrease main memory I/O proportionally to the decrease in communication. 3. We demonstrate how assumptions about ML training motivate different strategies for caching through empirical analysis with our framework. 4. We extensively test our algorithms on a variety of datasets and models. Furthermore, we also test various theoretical distributions and observe that our algorithms can generalize well, with performance gains on a wide range of potential synthetic datasets."
https://arxiv.org/html/2411.01561v1,Multimodal Graph Neural Network for Recommendation with Dynamic De-redundancy and Modality-Guided Feature De-noisy,"Graph neural networks (GNNs) have become crucial in multimodal recommendation tasks because of their powerful ability to capture complex relationships between neighboring nodes. However, increasing the number of propagation layers in GNNs can lead to feature redundancy, which may negatively impact the overall recommendation performance. In addition, the existing recommendation task method directly maps the preprocessed multimodal features to the low-dimensional space, which will bring the noise unrelated to user preference, thus affecting the representation ability of the model. To tackle the aforementioned challenges, we propose Multimodal Graph Neural Network for Recommendation (MGNM) with Dynamic De-redundancy and Modality-Guided Feature De-noisy, which is divided into local and global interaction. Initially, in the local interaction process,we integrate a dynamic de-redundancy (DDR) loss function which is achieved by utilizing the product of the feature coefficient matrix and the feature matrix as a penalization factor. It reduces the feature redundancy effects of multimodal and behavioral features caused by the stacking of multiple GNN layers. Subsequently, in the global interaction process, we developed modality-guided global feature purifiers for each modality to alleviate the impact of modality noise. It is a two-fold guiding mechanism eliminating modality features that are irrelevant to user preferences and captures complex relationships within the modality. Experimental results demonstrate that MGNM achieves superior performance on multimodal information denoising and removal of redundant information compared to the state-of-the-art methods.","Multimodal recommendation methods (MRs) are essential in the field of Artificial Intelligence (AI) and are widely applied in scenarios, i.e., healthcare [1, 2], online platform [3, 4, 5], news recommendation [6, 7], and social media [8, 9]. It uses multimodal signals to bootstrap collaborative signals to better capture the user’s interests and provide recommendations that are better suited to the user’s needs. Current MRs typically consist of three main steps: multimodal information fusion, collaborative filtering, and prediction. Figure 1: Illustration of the proposed model. It is structured into global and local interaction. Firstly, global interaction leverages modality features to guide the elimination of redundant information irrelevant to user preferences. Secondly, a trainable transformation matrix captures complex relationships within the modality, providing global preference information. For local interaction, the model addresses feature redundancy from GNNs by penalizing over correlated features. Predictions are made by calculating the weighted sum of global and local interaction information. This approach captures both global and local user interests, offering a comprehensive characterization of user preferences. Early MRs often linearly integrate individual modality signals into collaborative signals to establish user and item representation models. These methods primarily address low-order user-item interactions, neglecting higher-order relationship modeling. Additionally, different users have different preferences for various modalities. Given the inherent advantage of graph neural networks (GNNs) in capturing neighbor signals, applying GNNs to MRs is a natural choice. Specifically, GNNs aggregate neighbor information to capture more interaction details. Despite the commendable performance achieved by existing models, they still struggle with the following challenges: Many existing GNNs-based recommendation models contain only two layers of GNNs, this is because the effect of extracting interactive behavioral features or multimodal features does not keep going up as the number of GNNs layers continue to be stacked. Some researchers ascribe this phenomenon to the over-smoothing problem [10, 11, 12, 13], where node representations gradually become similar with the increase in layers. But Wu et al. [14] argue that as the number of GNNs layers increases, excessive feature correlations emerge, leading to performance degradation. This is manifested in the increasing redundancy among node representation features, which adversely affects the quality of learned embeddings. Existing models typically preprocess multimodal information through pretraining and then directly map embeddings to a smaller dimensionality [15, 16, 17]. However, directly mapping modality information from a high dimensional space to a low dimensional space will retain unnecessary noise information with greater probability. The introduction of modality noise information can directly affect the quality of the embedding and ultimately the expression of user preferences."
https://arxiv.org/html/2411.01376v1,Multi-Channel Hypergraph Contrastive Learning for Matrix Completion,"Rating is a typical user explicit feedback that visually reflects how much a user likes a related item. The (rating) matrix completion is essentially a rating prediction process, which is also a significant problem in recommender systems. Recently, graph neural networks (GNNs) have been widely used in matrix completion, which captures users’ preferences over items by formulating a rating matrix as a bipartite graph. However, existing methods are susceptible due to data sparsity and long-tail distribution in real-world scenarios. Moreover, the messaging mechanism of GNNs makes it difficult to capture high-order correlations and constraints between nodes, which are essentially useful in recommendation tasks. To tackle these challenges, we propose a Multi-Channel Hypergraph Contrastive Learning framework for matrix completion, named MHCL. Specifically, MHCL adaptively learns hypergraph structures to capture high-order correlations between nodes and jointly captures local and global collaborative relationships through attention-based cross-view aggregation. Additionally, to consider the magnitude and order information of ratings, we treat different rating subgraphs as different channels, encourage alignment between adjacent ratings, and further achieve the mutual enhancement between different ratings through multi-channel cross-rating contrastive learning. Extensive experiments on five public datasets demonstrate that the proposed method significantly outperforms the current state-of-the-art approaches.","Recently, the exponential growth of information has made matrix completion essential for e-commerce and social media platforms (Zhang et al., 2019; Ricci et al., 2010; Xia et al., 2020). These systems typically process two types of user feedback: explicit and implicit (Chen and Wang, 2022). Explicit feedback directly captures user preferences—for example, ratings on a matrix, where each entry quantifies how much a user likes a specific item. In contrast, implicit feedback indirectly reflects preferences, typically inferred as positive signals. This includes behavioral data such as click records, items added to shopping carts, and user favorites (Palomares et al., 2018; Raza and Ding, 2019). Matrix completion is a fundamental problem with various applications, ranging from missing traffic data imputation, protein relation prediction, image restoration, to recommender systems. Specifically, in recommender systems, the aim is to predict missing entries in a partially observed user-item matrix using available explicit or implicit feedback. Matrix factorization (Candes and Recht, 2012; Kalofolias et al., 2014; Xu et al., 2013) is a traditional approach to matrix completion that decomposes the user-item rating matrix into latent factors representing users and items. In recent years, GNNs have gained prominence in recommender systems (Monti et al., 2017; Shen et al., 2021; Zhang and Chen, 2019; Zhang et al., 2022). These methods model the rating matrix as a bipartite graph, reformulating the matrix completion problem as a link prediction task on this graph. Inductive matrix completion methods, such as IGMC (Zhang and Chen, 2019), utilize GNNs to learn local graph patterns from 1-hop subgraphs around target edges, enabling generalization to unseen nodes. Building on this idea, GIMC (Zhang et al., 2022) introduces edge embeddings and hyperbolic geometry to further enhance the modeling of bipartite graphs. Moreover, some review-based models (Wu et al., 2019; Gao et al., 2020a; Shuai et al., 2022) leverage auxiliary review information to enrich the learning of embeddings, complementing the structure-based methods. Despite their remarkable success, existing GNN-based matrix completion methods face several key challenges. First, existing GNN-based methods struggle to capture high-order correlations and complex constraints due to their reliance on existing edges in bipartite graphs, which often require stacking multiple layers and can introduce noise, leading to over-smoothing issues (Min et al., 2020). Second, these methods are inherently vulnerable to data sparsity and long-tail distributions (Singh, 2020), where a few highly active nodes dominate interactions, resulting in popularity bias and diminished effectiveness for less active nodes. Many approaches also overlook the magnitude and ordinal nature of ratings. For example, a rating of ‘4’ is closer to ‘5’ than to ‘1’. Although some recent methods (Zhang and Chen, 2019; Shen et al., 2021) incorporate regularization to address certain limitations, they often fail to fully account for these aspects within the model design. Motivated by the need to address the aforementioned challenges, we propose a novel Multi-Channel Hypergraph Contrastive Learning (MHCL) framework for matrix completion. First, we partition the bipartite graph into multiple subgraphs based on different types of rating edges using a subgraph partition and graph convolution module. During training, an adaptive hypergraph structure learning module dynamically learns hypergraph structures for these subgraphs at different layers, capturing high-order correlations between nodes to overcome the first challenge. Second, we introduce a multi-channel cross-rating contrastive learning module that treats each rating subgraph as a distinct channel, promoting mutual enhancement by aligning adjacent views. Third, to address the data sparsity and long-tail distribution challenge, we design an attention-based cross-view fusion module that strengthens interactions between different rating channels. Finally, an attention-based cross-view aggregation module learns node representations collaboratively from both local and global perspectives, fusing them into the final representation. MHCL demonstrates significant performance improvements, achieving up to +9.04% lift in MSE over SOTA baselines for matrix completion and a +10.31% increase in NDCG@10 for recommendation tasks on the ML-100K dataset. To sum up, this paper makes the contributions as follows: • MHCL leverages hypergraph learning to partition the user-item relationship graph into distinct subgraphs, representing various types of rating edges. It dynamically learns hypergraph structures to capture higher-order relationships during training. • MHCL introduces a multi-channel cross-rating contrastive learning, treating different rating subgraphs as unique perspectives. This approach promotes alignment between adjacent perspectives, enhancing the modeling of interrelationships between diverse rating types and improving prediction accuracy. • MHCL includes an attention-based cross-view aggregation module to integrate information from various rating subgraphs. It employs contrastive learning to collaboratively learn node representations from both local and global perspectives, ultimately fusing them to obtain the final node representations. This multi-view approach comprehensively captures node features, making contributions to improving recommendations. • We conduct comprehensive experiments to evaluate the performance of our MHCL model on matrix completion and recommendation tasks, along with diverse baselines, using multiple benchmark datasets."
https://arxiv.org/html/2411.01135v2,Music Foundation Model as Generic Booster for Music Downstream Tasks,"We demonstrate the efficacy of using intermediate representations from a single foundation model to enhance various music downstream tasks. We introduce SoniDo, a music foundation model (MFM) designed to extract hierarchical features from target music samples. By leveraging hierarchical intermediate features, SoniDo constrains the information granularity, leading to improved performance across various downstream tasks including both understanding and generative tasks. We specifically evaluated this approach on representative tasks such as music tagging, music transcription, music source separation, and music mixing. Our results reveal that the features extracted from foundation models provide valuable enhancements in training downstream task models. This highlights the capability of using features extracted from music foundation models as a booster for downstream tasks. Our approach not only benefits existing task-specific models but also supports music downstream tasks constrained by data scarcity. This paves the way for more effective and accessible music processing solutions.","A foundation model is a pre-trained model developed on a large-scale dataset that can be adapted for a variety of downstream tasks (Bommasani et al., 2021). Several language processing models (Radford et al., 2021; Brown et al., 2020; Devlin et al., 2018; Team et al., 2024) are considered foundation models due to their ability to unify all language tasks as sequence prediction tasks, effectively addressing multiple tasks with a single model. These foundation models have gained significant attraction and are widely used in everyday applications. In contrast, a powerful music foundation model capable of handling various music downstream tasks for music production is lacking. We categorize the tasks that a music foundation model primarily addresses into two types: understanding tasks, such as tagging and transcription, and generative tasks, such as mixing and mastering. Several multi-task models have been proposed as potential music foundation models (Li et al., 2023a; Yang et al., 2023; Copet et al., 2023; Agostinelli et al., 2023). However, this approach necessitates the inclusion of the desired tasks during the training phase. A notable strategy to overcome this limitation is to inject features extracted from a pre-trained large-scale model into smaller back-end models for downstream tasks that were not seen during training. This ensemble approach, which combines a large-scale model with various smaller models, can effectively function as a music foundation model. The codified audio language modeling (CALM) framework proposed by Castellon et al. (2021) is the first work in this direction, utilizing the intermediate representations from Jukebox (Dhariwal et al., 2020) to tackle music information retrieval (MIR) tasks, covering most music understanding tasks. Beyond MIR,Donahue et al. (2022) leveraged representations from Jukebox for melody transcription. Other studies have followed this approach to address time-invariant MIR tasks using the latest generative models based on residual quantized variational Autoencoders (RQ-VAEs) (Zeghidour et al., 2022; Défossez et al., 2023), enhancing the state-of-the-art (SOTA). However, these applications remain limited to music understanding tasks. Li et al. (2023b) expanded the focus to include music source separation, a generative task, but encountered instability issues during training. The performance of this extension does not yet match that of the baselines mentioned by Mitsufuji et al. (2022). An extensive overview of related work can be found in Appendix A. We extended the methodology from MIR tasks to generic music downstream tasks and hypothesize that the representation structure of foundation models is crucial in this case. Specifically, we propose that hierarchical representations, which divide information of varying granularity into different levels of embedding, are expected to provide efficient information hierarchy for all downstream tasks including both understanding and generative tasks. We empirically verify this hypothesis in Section 4. In contrast, music foundation models that have been applied to boost music downstream tasks do not have such a hierarchical structure. For example, Jukebox (Castellon et al., 2021; Donahue et al., 2022) is trained to have multi-level representation inspired from hierarchical latent representation (Razavi et al., 2019); however, each level is independently trained. RQ-VAEs (Yang et al., 2023; Li et al., 2023b) learn factorized representation that has a self-organized coarse-to-fine structure, however, they are not hierarchical. In accordance with the aforementioned hypothesis, we outline this study as follows. We propose and train our music foundation model, SoniDo (meaning sound in Spanish), on a high-fidelity internal dataset***The rights of this internal dataset are trained on licensed content only. Except for as specifically authorized by the rights owner, the rights owner expressly prohibits and has opted out of any text or data mining, web scraping or similar, reproductions, extractions or uses, of its content for any purposes, including in relation to training, developing, or commercializing any Al System. to establish a task-agnostic feature extraction pipeline. SoniDo is a generative model consisting of a multi-level transformer with a multi-level hierarchical encoder. With proper pre-processing, we infuse its intermediate representation as features to task-specific models on various music downstream tasks with data augmentation. Moreover, for understanding tasks, we proposed an on-the-fly data augmentation method called token-out to avoid the overfitting problem. We conducted a performance evaluation by benchmarking with representative tasks from understanding to generative tasks: music tagging, music transcription, music source separation, and music mixing. Table 1: Performance overview of applying extracted features to various music downstream tasks. Bold: best, underline: second best. Results obtained with different evaluation protocols are marked with asterisks. Downstream Task Dataset Metric SoniDo MusicGen Small MusicGen Large CALM w/ Jukebox-5B MERT Task-Specific SOTA Multi-task Music Tagging MusicTagATune ROC-AUC 91.7 90.4 90.5 91.5 91.3 92.0 (Huang et al., 2022a) mAP 41.5 38.8 39.0 41.4 40.2 38.4 Pitch Estimation Nsynth Acc. 93.8 93.3 92.8 91.6 94.4 89.2 (McCallum et al., 2022) Instrument Classification Acc. 78.0 71.9 74.2 70.4 72.6 78.2 (Wang et al., 2022) Emotion Regression EmoMusic Averaged R2superscript𝑅2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 64.7 45.6 46.2 66.9 68.0 63.0* (Castellon et al., 2021) Key Detection GiantSteps Weighted Acc. 63.5 65.2 62.4 66.7 65.6 79.6 (Castellon et al., 2021) Genre Classification GTZAN Acc. 80.7 75.2 70.3 79.7 79.3 83.5 (McCallum et al., 2022) Singer Identification VocalSet Acc. 87.0 82.3 83.3 82.6 87.1 80.3 (Modrzejewski et al., 2023) Technique Identification Acc. 74.4 66.1 63.9 76.7 76.9 65.6 (Yamamoto et al., 2022) Music Transcription MAPS Frame F1 83.92 82.94 81.53 - - 82.89 (Toyama et al., 2023) Note F1 86.45 85.97 85.14 - - 85.14 Note w/ Offset F1 68.27 68.27 66.28 - - 66.34 Note w/ Offset & Velocity F1 51.34 50.42 48.69 - - 48.20 Source Separation MUSDB18 SDR (bass) 9.50 8.86 8.17 4.9 5.6 11.31 (Lu et al., 2024) SDR (drums) 8.65 8.03 7.50 4.1 3.6 9.49 SDR (other) 5.91 5.59 5.54 2.7 3.0 7.73 SDR (vocals) 8.07 7.57 7.66 5.1 5.3 10.66 MDXDB21 hidden SDR (bass) 8.14 7.44 7.40 - - 7.86 (Rouard et al., 2023) SDR (drums) 8.16 8.31 7.37 - - 7.89 SDR (other) 5.21 5.26 4.93 - - 5.09 SDR (vocals) 8.04 7.81 7.73 - - 7.70 Music Mixing MDXDB21-dry hidden Stereo-Invariant 79.86 87.27 87.32 - - 82.09 (Martínez-Ramírez et al., 2022) Spectralmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.221 0.229 0.228 - - 0.193 Panningmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.175 0.244 0.219 - - 0.179 Dynamicmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.064 0.072 0.073 - - 0.070 Loudnessmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.171 0.148 0.132 - - 0.152 The encoder design of SoniDo is inspired by Jukebox but makes the representation hierarchical by enforcing the fine level to be conditioned by the coarse levels using a hierarchical autoencoder framework called hierarchically quantized VAE (HQ-VAE) (Takida et al., 2024). We then use a transformer-based multi-level auto-regressive model to characterize the probability mass of learnt HQ-VAE embeddings. We extract features from the intermediate representation of SoniDo by first converting input audio with the encoder into tokens, feeding them into the transformers, and extracting the intermediate output from the midst layer. We refer to these extracted features as SoniDo features. As shown in Table 1, we test our SoniDo’s feature injection for several music downstream tasks. To the best of our knowledge, this is the first study on enhancing both understanding and generative tasks with the intermediate representation from a single model. We briefly list the major findings of this study: 1. We empirically show that, with a generative model that is established on hierarchical representation, its intermediate representation can serve as generic booster of various music downstream tasks. 2. We verify that the extracted intermediate representation is beneficial for music understanding tasks even with only an extra shallow back-end network. The extension of the shallow network with attention layers leads to further improvement. 3. We show that the extracted intermediate representation is beneficial for enhancing task-specific models, through the applications to both understanding and generative tasks. 4. Several of the above improvements in each task category result in new SOTA scores. The summary of our results is shown in Table 1."
https://arxiv.org/html/2411.00788v1,KeyInst: Keyword Instruction for Improving SQL Formulation in Text-to-SQL,"Text-to-SQL parsing involves the translation of natural language queries (NLQs) into their corresponding SQL commands. A principal challenge within this domain is the formulation of SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. However, the intrinsic disparity between the NLQ and the SQL poses a significant challenge. In this research, we introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by Large Language Models (LLMs). KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query, thus facilitates a smoother SQL query formulation process. We explore two strategies for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. The former first generates KeyInst for question, which are then used to prompt LLMs. The latter employs a fine-tuned model to concurrently generate KeyInst and SQL in one step. We developed StrucQL, a benchmark specifically designed for the evaluation of SQL formulation. Extensive experiments on StrucQL and other benchmarks demonstrate that KeyInst significantly improves upon the existing Text-to-SQL prompting techniques.","The task of Text-to-SQL parsing, which aims at translating natural language questions into executable SQL queries, has gained increasing attention in recent years, as it can help non-expert users quickly access information in the database without the need for technical background Deng et al. (2021); Yu et al. (2020); Rajkumar et al. (2022); Ni et al. (2023). Text-to-SQL parsing faces two main challenges: schema linking and SQL formulation. Schema linking involves identifying the pertinent tables and columns in a database schema in response to an NLQ. SQL formulation refers to generating SQL queries that are not only syntactically correct but also semantically aligned with the natural language input. This paper primarily focuses on the challenge of SQL formulation. Currently, most Text-to-SQL prompting methods induce Large Language Models (LLMs) to generate the target SQL directly using In-context Learning (ICL) Nan et al. (2023); Pourreza and Rafiei (2024a); Tan et al. (2024). However, the vast difference between natural language queries (NLQ) and SQL hinders precise query formulation. In previous works, the skeleton-aware decoder Li et al. (2023) was proposed to alleviate this challenge by initially generating an SQL skeleton followed by the full query. An SQL skeleton is a basic framework of an SQL query consisting of SQL operators, without specific details such as column names, table names, or conditions. Incorporating SQL skeleton in prompting has also proven to be effective Gao et al. (2023); Guo et al. (2023). In this work, we also use the SQL structure as a central element in SQL formulation, with a particular emphasis on identifying key SQL operators. For instance, in translating the NLQ ""List the customers’ first and last names from the 10 least expensive invoices"", accurately identifying ORDER BY and LIMIT is crucial for formulating the correct SQL query. Figure 1: Graphical illustration of KeyInst and its applications: A. An example of schema, question, KeyInst, and SQL, B. The pipeline approach of KeyInst application, C. The single-pass approach of KeyInst application. We introduce Keyword Instruction (KeyInst), a novel method designed to enhance SQL formulation by LLMs. KeyInst essentially provides guidance on pivotal SQL keywords likely to be part of the final query. Recognizing that SQL queries corresponding to different NLQs require distinct keywords, KeyInst adapts dynamically to each query. An example of KeyInst in action is depicted in Figure 1A, demonstrating how it analyzes a given NLQ and deduces the critical SQL keywords. This strategy effectively narrows the gap between NLQ and SQL, facilitating a smoother SQL query formulation process. While KeyInst significantly aids in SQL query formulation, further exploration is needed on its generation and integration into Text-to-SQL parsing. We present two approaches for KeyInst generation: a model fine-tuning method and an ICL-based method. The former fine-tunes a model to produce KeyInsts for specific queries, while the latter prompts LLMs to generate KeyInsts through ICL Brown et al. (2020). For the application of KeyInst in Text-to-SQL tasks, we also investigate two strategies. The first strategy prompts LLMs to produce SQL queries with KeyInst. The second strategy is a fine-tuning strategy that generates SQL queries directly following KeyInst generation, treating KeyInst creation as a preliminary reasoning step. To amalgamate KeyInst generation and application within Text-to-SQL, we introduce a two-fold strategy. The pipeline approach initially generates KeyInst using either the fine-tuned or ICL-based method, followed by prompting LLMs with the generated KeyInst, as illustrated in Figure 1B. Conversely, the single-pass approach employs a fine-tuned model to concurrently generate KeyInst and SQL in one step, as depicted in Figure 1C. Several benchmarks, such as Spider Yu et al. (2018) and Bird Li et al. (2024b), have been developed to assess Text-to-SQL systems. However, these benchmarks focus on overall parsing performance and lack mechanisms for isolating evaluations of semantic linking and SQL formulation. To specifically assess SQL formulation capabilities, a new benchmark called StrucQL (Structural Benchmark for Text-to-SQL) has been developed, derived from Spider. In StrucQL, questions and schemas are simplified: questions explicitly mention schema items, and irrelevant tables and columns are omitted from the schema. This simplification makes schema linking straightforward, shifting the primary challenge to SQL formulation. Consequently, StrucQL serves as an effective tool for evaluating SQL formulation proficiency in Text-to-SQL systems. KeyInst was assessed on StrucQL and other benchmarks, with outcomes indicating that keyword instructions are a valuable intermediary for Text-to-SQL parsing, whether applied independently or in conjunction with other techniques. The main contributions of this work are summarized as follows: • We propose KeyInst, a keyword instruction tailored for each Text-to-SQL task, to alleviate SQL formulation challenge. We offer two approaches for integrating KeyInst into Text-to-SQL parsing: a pipeline strategy and a single-pass strategy. • The StrucQL benchmark was developed to specifically assess the SQL formulation abilities of Text-to-SQL systems. By simplifying questions and schemas, StrucQL eliminates schema linking challenges, focusing evaluation on SQL formulation performance. • Comprehensive experiments across various benchmarks were conducted. The findings demonstrate that KeyInst significantly improves upon the existing state-of-the-art Text-to-SQL prompting techniques, showcasing its effectiveness and potential."
https://arxiv.org/html/2411.00677v1,Making Sense of Metadata Mess: Alignment & Risk Assessment for Diatom Data Use Case,"Biologists study Diatoms, a fundamental algae, to assess the health of aquatic systems. Diatom specimens have traditionally been preserved on analog slides, where a single slide can contain thousands of these microscopic organisms. Digitization of these collections presents both metadata challenges and opportunities. This paper reports on metadata research aimed at providing access to a digital portion of the Academy of Natural Sciences’ Diatom Herbarium, Drexel University. We report results of a 3-part study covering 1) a review of relevant metadata standards and a microscopy metadata framework shared by Hammer et al., 2) a baseline metadata alignment mapping current diatom metadata properties to standard metadata types, and 3) a metadata risk analysis associated with the course of standard data curation practices. This research is part of an effort involving the transfer of these digital slides to an new system, DataFed, to support global accessible. The final section of this paper includes a conclusion and discusses next steps.","Creating and managing high quality metadata is essential for supporting digital life-cycle management and the FAIR principles [17]. These aims inform a current initiative involving the Academy of Natural Sciences’ Diatom Herbarium (ANS Diatom Herbarium), Drexel University. Diatoms are a fundamental algae, which remove carbon dioxide from the atmosphere through photosynthesis. Researchers focused on diatoms collect and preserve water samples on slides. They use microscopy technology to observe diatom microorganisms. Today, the ANS Diatom Herbarium contains well over 300,000 slides and is recognized as one of the most extensive diatom collections in the United States. In 2009, with the support of the U.S. National Science Foundation, the creators of this collection undertook a project to digitize 6,000 slides, a subset of the collection. The aim was to make the collection more globally accessible. Indeed, the digitization of these diatom slides was an important step toward increased collection access and data sharing, although this activity revealed a number of significant challenges related to collection organization and curation. The main challenges stem from the fact that a diatom specimen is microscopic. Biological database software generally assumes that a slide simply captures a single specimen or organism, while a diatom slide may contain thousands of microscopic specimens. Given this challenge and the absence of a unified technology supporting project needs, the ANS team was required to develop a system with a unique combination of microscope hardware, digitization hardware, and digitization software. The workflows, metadata gathering, and processing activities were conducted in house, as software libraries accounting for this specific use case are not available. In early 2024, a new effort was launched to enhance access to the ANS Diatom Herbarium and address these noted challenges. This current initiative involves information and computer scientists and diatom experts, with key aims to: 1) advance the management of the ANS Diatom Herbarium, and 2) support global access to this unique collection. The collaboration is connected with a more recent NSF project, ‘Development of a Platform for Accessible Data-Intensive Science and Engineering.’ A major component of the larger NSF project is to develop a metadata infrastructure for porting the digital collection to a developing system, called DataFed [15]. This paper discusses the current metadata research activities specific to the ANS Diatome Herbarium collection. We report on our review of metadata standards, a baseline metadata alignment, and a metadata risk analysis. The sections that follow cover these three activities, followed by a discussion and a conclusion."
https://arxiv.org/html/2411.00676v1,Enhancing Semantic Interoperability Across Materials Science With HIVE4MAT,"HIVE4MAT is a linked data interactive application for navigating ontologies of value to materials science. HIVE enables automatic indexing of textual resources with standardized terminology. This article presents the motivation underlying HIVE4MAT, explains the system architecture, reports on two evaluations, and discusses future plans.","Data infrastructure advancements over the last two decades have supported the development of a rich ecosystem of machine readable semantic vocabularies across many domains. These advances have enabled a number of materials science initiatives, e.g.[1, 2] to leverage individual ontologies for data organization, access, interoperability, and support for the FAIR data principles[3]. Even with this progress, limited attention has been directed to exploring the broad array of ontologies and other semantic systems both within materials science and across interconnected disciplines. One likely reason for this condition is the general evolution of ontologies. Ontological development and use-case scenarios represent a first-phase priority, while human and financial resources for exploring the larger spectrum of semantic systems require more time and investment. Another factor is limited availability of applications that provide researchers and data custodians capacity to easily explore and use of the broader spectrum of relevant semantics systems. Research is necessary to allow for greater exploration of a wide array of semantic systems of value to materials science researchers for organizing and providing access to their data assets and other scientific and scholarly outputs. The Helping Interdisciplinary Vocabulary Engineering for Materials Science (HIVE4MAT) application seeks to address this challenge. HIVE4MAT allows exploration multiple ontologies in a single environment through search, automatic indexing, as well as targeted browsing of an individual ontology’s simple, taxonomic structure. This paper reports on HIVE4MAT research development, the support for breaking down research silos of terminologies, baseline evaluations, and future plans."
https://arxiv.org/html/2411.00556v1,LLM-KT: A Versatile Framework for Knowledge Transfer from Large Language Models to Collaborative Filtering††thanks:The work was partially prepared within the framework of the Basic Research Program at the National Research University Higher School of Economics (HSE).,"We present LLM-KT, a flexible framework designed to enhance collaborative filtering (CF) models by seamlessly integrating LLM (Large Language Model)-generated features. Unlike existing methods that rely on passing LLM-generated features as direct inputs, our framework injects these features into an intermediate layer of any CF model, allowing the model to reconstruct and leverage the embeddings internally. This model-agnostic approach works with a wide range of CF models without requiring architectural changes, making it adaptable to various recommendation scenarios.Our framework is built for easy integration and modification, providing researchers and developers with a powerful tool for extending CF model capabilities through efficient knowledge transfer. We demonstrate its effectiveness through experiments on the MovieLens and Amazon datasets, where it consistently improves baseline CF models. Experimental studies showed that LLM-KT is competitive with the state-of-the-art methods in context-aware settings but can be applied to a broader range of CF models than current approaches.","Many recommender systems use Collaborative Filtering (CF) methods to model user preferences and match items to them [1, 2, 3]. However, these models often struggle to understand nuanced relationships and adapt to dynamic user-item interactions [4, 5]. To tackle this issue, applying Large Language Models (LLMs) for recommendations has been actively studied since LLMs offer new ways to represent knowledge with their strong reasoning capabilities. As a result, current studies have integrated LLMs into various stages of recommender systems, from open-world knowledge generation [6, 7] to candidate ranking [8, 9]. Since LLMs are expensive to use, recently, several works proposed to directly use LLM for improving the quality of CF models by performing knowledge transfer (e.g., KAR [10], LLM-CF [9]). They create textual features from reasoning chains of LLM and integrate them as input to CF models. However, such an approach limits their applicability to only context-aware models, making their direct usage impossible for other types of CF models that don’t handle input features. Given these limitations, we developed a method that extends the applicability of knowledge transfer from LLMs to a broader range of CF models. We introduce “LLM-KT”, a novel framework that facilitates seamless integration with various CF models and provides a robust environment for testing and modifying the approach. Our framework enables efficient knowledge transfer by embedding LLM-generated features into the intermediate layers of CF models, training the models to reconstruct these features as a pretext task internally. This process allows the CF model to develop a more refined understanding of user preferences, resulting in more accurate recommendations. Experiments on two well-known benchmarks demonstrate that the proposed method significantly improves the performance of CF models (+ up to 21% improvement in NDCG@10) while applying to a broader range of models than existing approaches and achieving results comparable to the state-of-the-art KAR [10] in context-aware setting."
https://arxiv.org/html/2411.00395v1,DivNet: Diversity-Aware Self-Correcting Sequential Recommendation Networks,"As the last stage of a typical recommendation system, collective recommendation aims to give the final touches to the recommended items and their layout so as to optimize overall objectives such as diversity and whole-page relevance. In practice, however, the interaction dynamics among the recommended items, their visual appearances and meta-data such as specifications are often too complex to be captured by experts’ heuristics or simple models. To address this issue, we propose a diversity-aware self-correcting sequential recommendation networks (DivNet) that is able to estimate utility by capturing the complex interactions among sequential items and diversify recommendations simultaneously. Experiments on both offline and online settings demonstrate that DivNet can achieve better results compared to baselines with or without collective recommendations.","Modern recommendation systems are complex, often consisting of multiple stages including matching (a.k.a. retrieval), coarse-to-fine-grain tiered ranking, and collective recommendation systems. Given user queries (e.g., a search query or a user session), the matching module quickly identifies a moderate-sized set of relevant candidates from a vast amount (millions or billions) of items in the inventory. Tiered ranking modules then estimate item-wise scores, such as CTR and CVR, based on which items will be ranked w.r.t. their relevancy to the target user. Before surfacing the recommendations to the user, a collective recommendation module is usually employed to optimize the value delivery. Without this module, the recommendation systems may suffer from severe problems, such as the discrepancy between training and inference, lack of diversity and so on. The discrepancy between training and inference means the model loss considers interactions between ranked items when training while the item is scored point-wisely or the context for this item has changed during inference. This discrepancy can lead to unreliable prediction performance. Given certain queries, similar items usually tend to have high scores, resulting in over-concentration of recommended items. Recently, collective recommendation module (Steck, 2018; Nassif et al., 2018; Teo et al., 2016; Wilhelm et al., 2018) is introduced into recommendation systems to further optimize the item layout, which can boost the performances of recommendation system, such as improving user satisfactions and click conversion rates. Collective recommendations try to improve the whole-page attraction by modeling the item interaction dynamics. Collective recommendation poses unique technical challenges. (I) First, complex interactions exist between items in different positions. Similar items shown in top positions may decrease or increase the click probability of subsequent items (which we call a self-exciting process between items). These depend on subtle causalities that it’s hard to define explicitly using rules beforehand. For instance, some users may prefer a diverse recommendation set while a concentrated one is preferred for other users. Besides, distinct combinations of items result in different user experiences. Items combinations with compatible colors or text descriptions can grasp users’ attention which in turn improves the click-through rate. Items with a short distance have stronger interactions than those with a long one. (II) Secondly, sequence exposure bias also exists as only a small fraction of possible item combinations have been shown to users, which leads to the estimating of out-of-distribution item combinations unreliable. Recent literature (Steck, 2018) addresses this issue by introducing Bayesian prior distribution of users’ preferences into the optimizing objective. The resulting recommendation sets are close to prior distributions of users’ preferences. However, it requires enough data accumulation to estimate an accurate prior distribution which is unsuitable for scenarios, like cold-start recommendations and short-term advertising or marketing activities. Nassif et al. (Nassif et al., 2018; Teo et al., 2016) try to balance the click-through rate and item diversity by designing a sub-modular function. Similarly, Wilhelm et al. (Wilhelm et al., 2018) employ determinantal point processes to diversify YouTube recommendation by computing pair-wise item similarity, which is inefficient to capture high-order item interactions and is computationally expensive. All the above methods assume the element-wise utility of items remain unchanged regardless of the contextual items. These methods are inflexible to model complex interactions within items, between items and users, items and positions. The PRM (Pei et al., 2019) employs a transformer structure to learn the global interactions from all the combinations of item pairs on the list. Seq2Slate (Bello et al., 2018) employs pointer-net (Vinyals et al., 2015) (a sequence-to-sequence network) to capture item interactions where a RNN encoder embeds all items and a RNN decoder iteratively select items using attention mechanism. The PRM and Seq2Slate is not diversity-aware and doesn’t model diversity explicitly. To address the above concerns, we propose a self-correcting sequential recommendation networks (DivNet) which can estimate the item utility sequentially and diversify recommendation set simultaneously. The DivNet firstly projects items and user features with contextual items into a hidden space using self-attention networks. Then DivNet sequentially selects items by considering influences of previously-selected items and passes its influence to subsequent items. During this selecting process, the DivNet estimates the item utility and computes the similarity between candidates and existing items. The self-correcting module aims at maximizing total utility and improve diversity among selected items. To reduce the effect of exposure bias, we let the DivNet search in the nearby region of original exposed sequences by adding a supervised regularization. We conduct enormous ablation tests in industrial applications. Results of offline evaluation and deployment on commercial platforms demonstrate its superior performances compared to baselines with/without collective recommendations. What’s more, the proposed method is practical as it can support large-scale queries in real-world recommendation systems."
https://arxiv.org/html/2411.00341v1,"A Survey on Bundle Recommendation: Methods, Applications, and Challenges","In recent years, bundle recommendation systems have gained significant attention in both academia and industry due to their ability to enhance user experience and increase sales by recommending a set of items as a bundle rather than individual items. This survey provides a comprehensive review on bundle recommendation, beginning by a taxonomy for exploring product bundling. We classify it into two categories based on bundling strategy from various application domains, i.e., discriminative and generative bundle recommendation. Then we formulate the corresponding tasks of the two categories and systematically review their methods: 1) representation learning from bundle and item levels and interaction modeling for discriminative bundle recommendation; 2) representation learning from item level and bundle generation for generative bundle recommendation. Subsequently, we survey the resources of bundle recommendation including datasets and evaluation metrics, and conduct reproducibility experiments on mainstream models. Lastly, we discuss the main challenges and highlight the promising future directions in the field of bundle recommendation, aiming to serve as a useful resource for researchers and practitioners. Our code and datasets are publicly available at https://github.com/WUT-IDEA/bundle-recommendation-survey.","Recommendation systems have become essential tools for alleviating information overload. Over the years, item recommendation systems have been widely applied to recommend item to user or users across various domains, achieving significant success (Gomez-Uribe and Hunt, 2016; Das et al., 2007; Briand et al., 2021; Cheuque et al., 2019; Gao et al., 2023a). For example, in news recommendation, users’ reading history and interests are analyzed to recommend relevant articles, thereby increasing user engagement and satisfaction (Das et al., 2007; Liu et al., 2010; Phelan et al., 2009; Okura et al., 2017). In e-commerce, recommendation systems have become a cornerstone for boosting sales conversion rates by recommending products related to users’ browsing or purchase history, as evidenced by the sophisticated algorithms used by platforms like Amazon (McAuley et al., 2015a). Similarly, video streaming platforms like YouTube employ deep learning techniques to ensure users can discover videos or series they are interested in, significantly increasing watch time and user retention (Covington et al., 2016). Netflix’s recommendation system stands out as a prime example of leveraging complex algorithms and data analysis to provide highly personalized viewing recommendations, which has significantly enhanced user experience and subscription retention (Gomez-Uribe and Hunt, 2016). Music streaming services like Spotify use collaborative filtering (Schafer et al., 2007; Sarwar et al., 2001; Breese et al., 1998) and content-based methods (Lops et al., 2011; Pazzani and Billsus, 2007; Lu et al., 2015) to recommend individual tracks tailored to users’ preferences, improving user satisfaction and engagement (Koren et al., 2009; van den Oord et al., 2013). Additionally, social media platforms such as Facebook and Twitter utilize recommendation systems to deliver personalized content to users, thereby boosting user interaction and time spent on the platform (Guy et al., 2010; Kazai et al., 2016). Even in the travel and hospitality industry, recommendation systems help users find ideal destinations, accommodations, and activities based on their past behaviors and preferences (Linden et al., 2003). Existing recommendation systems, however, still face challenges in addressing the diverse and personalized needs of users, despite significant progress in various application domains. Particularly, traditional single-item recommendation may not meet users’ comprehensive needs when users need to make choices across multiple categories or items. For example, a music lover may prefer recommendations that include a curated album from a favorite artist or a list of music songs within a specific genre that matches his or her mood. As a result, bundle recommendation system (Bundle RS) has emerged (Chang et al., 2020; Wei et al., 2023; Zhao et al., 2022; Ma et al., 2022, 2024a; Jeon et al., 2024), which extends the traditional item recommendation to a set of items that are likely to interest the user when considered together. These approaches not only enhance user satisfaction by providing more holistic recommendations but also have the potential to increase the economic value for providers through up-selling and cross-selling opportunities. Ubiquity of Bundle RS We conduct a high-level overview of recommendation systems, including item recommendation, group recommendation, bundle recommendation, and complex set recommendation. As shown in Table 1, item recommendation is to recommend a single item to an individual user based on their preferences, known as 1-to-1 recommendation. Group recommendation is to recommend an item to a group of users, referred to as 1-to-N recommendation (Wang et al., 2021a, 2022; Wang and Li, 2021; Yang et al., 2024a; Zhao et al., 2024). Complex set recommendation is more complicated, recommending a set of items to a group of users, called N-to-N recommendation, such as recommending a travel package to a tour group. Unlike the above three tasks, the focus of this survey is on bundle recommendation, which recommends sets or bundles of items to a user, termed N-to-1 recommendation. This task has been emergingly applied in various sectors such as e-commerce, entertainment, and tourism due to its ability to satisfy and enhance user experience and drive business value. Consequently, bundle recommendation has become a critical task in modern recommendation systems. Table 1. Recommendation System Tasks Task Output Alias Item Recommendation A single item to an individual user 1-to-1 Group Recommendation A single item to a group of users 1-to-N Bundle Recommendation A set of items to a single user N-to-1 Complex Set Recommendation A set of items to a group of users N-to-N For illustration, several example bundles in the domain of Product, Clothing, Food, Entertainment, Health etc. are depicted in Figure 1. In e-commerce, Bundle RS assists retailers in offering product bundles tailored to specific customer needs and preferences, thereby enhancing the shopping experience and boosting sales (Liu et al., 2017, 2014; Wan et al., 2018; Zhu et al., 2014). For instance, when an electronics enthusiast is looking to buy an iPhone, Bundle RS might recommend complementary items such as AirPods, Apple Watch, and 3-in-1 charger. The electronic product bundles are often sold at a discount, further encouraging the purchase. Similarly, in fashion outfit recommendation (Liu et al., 2012; Han et al., 2017; Hu et al., 2015; McAuley et al., 2015b; Chen et al., 2019a; Li et al., 2020b; Lu et al., 2019), the fourth example of Figure 1 shows an outfit, including blouse, skirt, heels and a handbag. Suppose that a customer is shopping online with an intent, e.g., purchasing fashion clothing for a party, and finds this well-matched outfit appealing. In this case, the customer may purchase this outfit. \Description Different application domains of bundles Figure 1. Different Application Domains of Bundles In the entertainment industry, streaming services like Netflix, Spotify, and NetEase use Bundle RS to recommend sets of movies, TV shows, or songs, providing users with a more comprehensive and enjoyable viewing or listening experience (Gomez-Uribe and Hunt, 2016; Cao et al., 2017; Chang et al., 2020; He et al., 2020b; Yang et al., 2019). Similarly, platforms like Youshu and Goodreads provide lists of books created by readers, leveraging Bundle RS to offer curated book recommendations (Chen et al., 2019b; He et al., 2019b; Liu et al., 2014). These bundles enhance the reading experience by recommending books that complement each other, whether based on themes, genres, or reader preferences. Meal recommendation (Agapito et al., 2017; Elsweiler and Harvey, 2015; Li et al., 2022a, 2023b, 2023a) is another application of bundle recommendation in food scenarios. For instance, a user looking for a dinner idea might receive a recommendation for a three-course meal featuring a broccoli salad, pasta, and an ice cream cake for dessert. This holistic approach to meal planning simplifies decision-making and can inspire users to try new recipes and ingredients. In the realm of travel package recommendation (Herzog et al., 2019; Lim et al., 2015, 2018), suppose that a family is planning a vacation to an unfamiliar city, Bundle RS will recommend a travel package that includes an ordered set of Point of Interest (POIs) that align with their interest preferences and trip constraints. This makes trip planning more convenient and ensures a memorable and well-rounded travel experience. Since bundles are ubiquitous across various application domains, Bundle RS is widely used in e-commerce, entertainment, and travel industries and so on. Increasing Importance of Bundle RS Bundle RS has become crucial in enhancing user experiences and boosting business success. In a world saturated with choices, users often feel overwhelmed by the need to make decisions. By offering personalized bundles of items, such as book lists, song lists, meals, or travel package, Bundle RS simplifies decision-making and provides cohesive, enjoyable experiences for users. This task not only increases user satisfaction and loyalty but also drives higher sales, as seen in e-commerce where tailored product bundles are used to meet specific user needs and preferences. Furthermore, Bundle RS can create highly targeted bundles that evolve over time, aligning with changing preferences and market trends by analyzing user historical data. Research community has also extensively explored Bundle RS, contributing to its development and refinement in both methodology and practice. As technology and user expectations continue to evolve, Bundle RS remains crucial in delivering comprehensive, personalized recommendations. Motivation of this Survey With the growing demand and development of bundle recommendation across various domains, numerous studies have been conducted to improve its performance. Initially, Zhu et al. (Zhu et al., 2014) proposed the bundle recommendation problem and tried to resolve it by minimizing the cost or maximize the revenue of a bundle (Zhu et al., 2014; Beladev et al., 2016). Later, association rule mining was utilized in (Guo-rong and Xi-zheng, 2006; Fang et al., 2018) for bundle generation and recommendation. When a certain number of user-bundle interactions were available, an intuitive solution was to treat the bundle as a single 'item' and apply traditional collaborative filtering methods. Prior work (Rendle et al., 2010) just ignored the affiliated items of the bundle and just used an id to represent a bundle. Recognizing the importance of affiliated items, some works (Cao et al., 2017; Chen et al., 2019b) tried to capture additional user-item interaction and bundle-item affiliation relations. Over time, the bundle view and item view were gradually introduced in bundle recommendation research (Chang et al., 2020; Ma et al., 2022) etc. This shift led to an increasing amount of research focused on these two views within bundle recommendation (Ke et al., 2023b, a; Ren et al., 2023) etc. Considering the increasing attention, emerging challenges, and urgent need for innovative breakthroughs on bundle recommendation, we realize it is the right time to present a survey of this area, providing a systematic review of the methods, applications, and challenges in Bundle RS. Uniqueness of this Work Given the significance and popularity of recommendation research, there are more and more surveys reviewing RS from different perspectives. These include explainable recommendation (Zhang and Chen, 2020), knowledge-based recommendation (Tarus et al., 2018), recommendation methods based on deep learning and reinforcement learning (Chen et al., 2023b), accuracy-oriented recommendation modeling (Wu et al., 2023b), bias and debias in RS (Chen et al., 2023a), fairness in RS (Deldjoo et al., 2024; Wang et al., 2023b), and so on. A survey on bundle recommendation by (Li et al., 2020a) published in 2020 primarily reviews the main model methods of bundle RS, including integer programming, association analysis, and both traditional and deep learning-based recommendation techniques. These methods have provided effective solutions in specific application scenarios. Along with the development of new techniques such as graph learning (Chang et al., 2020; Li et al., 2023b; Wei et al., 2023), contrastive learning (Zhao et al., 2022; Ma et al., 2024a, 2022), and knowledge distillation (Ren et al., 2023), the research on bundle recommendation has made further progress in the past few years. Therefore, different from existing surveys, our survey offers a more comprehensive and up-to-date review on bundle recommendation, covering the datasets across various bundle recommendation scenarios, both discriminative and generative bundle recommendation tasks, representation learning approaches and strategies, as well as interaction prediction and bundle generation methods. This survey benefits for researchers and practitioners who want to keep up with the state-of-the-art research in the field of bundle recommendation. By doing so, we deliver the following contributions in this survey: • Based on demands of different application scenarios, i.e., selecting existing bundles or generating new bundles, we introduce a taxonomy for bundle recommendation tasks: discriminative bundle recommendation and generative bundle recommendation. By introducing this clear distinction, we provide two general frameworks tailored to each category and help to identify the appropriate approach for a given task. • To categorize and summarize the existing technologies on discriminative bundle recommendation, we present a comprehensive review that covers representation learning approaches and strategies from bundle and item levels, as well as interaction modeling methods. This detailed review guides researchers to explore more effective solutions in this filed. • To offer a thorough understanding of current generative bundle recommendation, we conclude representation learning methods from item level, and bundle generation approaches. This survey helps researchers identify key methodologies and inspires the development of more effective generative models for bundle recommendation. • A summary of resources for bundle recommendation is provided, including datasets and evaluation metrics. Moreover, we conduct reproducibility experiments on discriminative and generative bundle recommendation models. • We discuss the main challenges and future directions in bundle recommendation from four perspectives. By highlighting these challenges, we aim to guide future research towards overcoming these obstacles and advancing the field. Method of Paper Collection Our survey focuses on reviewing bundle recommendation from the perspective of recommending existing bundles (discriminative) or new bundles (generative), so we retrieved top conferences such as SIGIR, KDD, WWW, RecSys, WSDM, etc., and top journals such as IEEE Trans. Knowl. Data Eng., ACM Trans. Inf. Syst., ACM Trans. Knowl. Discov. Data and so on. Leveraging scholarly databases like dblp and Google Scholar, we systematically conducted searches employing specific keywords like ”bundle recommend”, ”package recommend”, ”list recommend” , ”next basket recommend” to search related work. In order to make the retrieved papers more comprehensive, we also used keywords that related to bundle recommendation in different domains, such as ”meal recommend”, ”outfit recommend”, to get more papers. Then, based on the above retrieved papers related to bundle recommendation, we illustrate the statistics of them according to the publication time and publisher, as shown in Figure 2(a) and Figure 2(b), respectively. (a) The statistics of papers with the publication year (b) Distribution of papers Figure 2. The statistics and distribution of papers related to bundle recommendation \Description The statistics and distribution of papers related to bundle recommendation Survey Organization This survey is structured into seven sections and is organized as follows: Section 2 introduces the definitions and taxonomy of bundle recommendation. Section 3 discusses the discriminative bundle recommendation task, reviews the representation learning techniques and strategies from bundle and item levels, and summarizes several approaches of interaction modeling. Section 4 explores the generative bundle recommendation task, and reviews representation learning from item level and bundle generation methods. Section 5 summarizes the resources of bundle recommendation and conducts reproducibility experiments. Section 6 discusses the challenges and trends. And Section 7 concludes the survey."
https://arxiv.org/html/2411.00331v1,Beyond Utility: Evaluating LLM as Recommender,"With the rapid development of Large Language Models (LLMs), recent studies employed LLMs as recommenders to provide personalized information services for distinct users. Despite efforts to improve the accuracy of LLM-based recommendation models, relatively little attention is paid to beyond-utility dimensions. Moreover, there are unique evaluation aspects of LLM-based recommendation models, which have been largely ignored. To bridge this gap, we explore four new evaluation dimensions and propose a multidimensional evaluation framework. The new evaluation dimensions include: 1) history length sensitivity, 2) candidate position bias, 3) generation-involved performance, and 4) hallucinations. All four dimensions have the potential to impact performance, but are largely unnecessary for consideration in traditional systems. Using this multidimensional evaluation framework, along with traditional aspects, we evaluate the performance of seven LLM-based recommenders, with three prompting strategies, comparing them with six traditional models on both ranking and re-ranking tasks on four datasets. We find that LLMs excel at handling tasks with prior knowledge and shorter input histories in the ranking setting, and perform better in the re-ranking setting, beating traditional models across multiple dimensions. However, LLMs exhibit substantial candidate position bias issues, and some models hallucinate non-existent items much more often than others. We intend our evaluation framework and observations to benefit future research on the use of LLMs as recommenders. The code and data are available at https://github.com/JiangDeccc/EvaLLMasRecommender.","The applications of recommendation systems (RSs) are becoming increasingly widespread. Meanwhile, the emergence of Large Language Models (LLMs) and their outstanding performance on various NLP tasks (Brown et al., 2020; Kojima et al., 2022; Chowdhery et al., 2024; Zhang et al., 2022) have garnered great attention, creating a growing interest in applying LLMs to RSs as well. LLMs can be implemented in various stages of the RS pipeline, e.g., feature engineering (Liu et al., 2024; Wu et al., 2024) and feature encoding (Wang et al., 2022; Rajput et al., 2023). In this paper, we focus on the application of LLMs directly as recommenders. This approach introduces more significant changes to the traditional recommendation paradigm and thus may have more unknown impacts. Previous work has explored the performance of LLMs as recommenders along multiple conventional evaluation dimensions. Palma et al. (Palma et al., 2023) conduct a detailed comparison of the performance of ChatGPT with that of various traditional models, focusing on recommendation accuracy, diversity, popularity bias, and novelty. FairLLM (Zhang et al., 2023a), CFaiRLLM (Deldjoo and di Noia, 2024) and FairEvalLLM (Deldjoo, 2024a) concentrate on user-side fairness, while IFairLRS (Jiang et al., 2024) and (Deldjoo, 2024b) cast a light on item-side fairness. Although this previous work covers many conventional dimensions, they are under different settings and no single effort has comprehensively assessed all of these aspects. Moreover, we hold that these conventional concerns cannot fully reflect the performance of LLMs as recommenders because many novel characteristics introduced by LLMs are not considered by these conventional dimensions. Recommendations by LLMs differ from those by traditional models. LLMs have strong zero-shot (Wang and Lim, 2023), textual and generative (Zhang et al., 2022; Chowdhery et al., 2024) capabilities, whereas traditional models are highly data-dependent and generally lack robust text-processing abilities. Therefore, LLM recommenders may vary in terms of generalization abilities and show different performance when textual information can be more efficiently incorporated. Additionally, LLMs exhibit certain issues that traditional models do not, such as position bias (Wang et al., 2023b) and hallucinations (Xu et al., 2024b; Zhang et al., 2023c), which introduce new challenges. In this paper, we propose a multidimensional evaluation framework, including two conventional dimensions, utility and novelty, and four our proposed LLM-related dimensions. We call attention to the four additional evaluation dimensions: 1) history length sensitivity: delving deeper into the generalization capabilities, 2) candidate position bias: quantifying the issue of position bias, 3) generation-involved performance: evaluating the textual and generative capabilities, 4) hallucination: focusing on the hallucination problem of LLMs. Among them, dimensions 1 and 2 can also be evaluated for traditional models, but these issues are less relevant for them. Dimensions 3 and 4 are unique to LLMs as recommenders. By introducing these LLM-centric evaluation dimensions, we can gain a more comprehensive understanding of LLM recommendations and their differences from traditional recommendation models. With this framework, we conduct a multidimensional evaluation of seven LLMs, with three prompting strategies, exposing areas where LLMs excel and where they do not. In the ranking setting, LLMs demonstrate better novelty and excel in the domains where they possess more extensive knowledge and the cold-start scenario in terms of accuracy. LLM-generated profiles can capture key patterns of the user history. However, candidate position bias is significant, damaging recommendation quality. Hallucinations occur, posing threats to the user experience. In the re-ranking setting, LLMs show more outstanding performance than traditional recommenders in nearly all conventional dimensions with any history length. Though candidate position bias can still harm performance, the problem is partially alleviated compared to the ranking setting. Our main contributions are as follows: • We define and explore four evaluation dimensions beyond utility and novelty thoroughly based on the strengths and weaknesses of LLMs to observe how the characteristics of LLMs can impact recommendation performance. • We propose a reproducible, multidimensional evaluation framework for LLM-based recommenders, covering both ranking and re-ranking tasks. With this framework, we evaluate seven LLMs using three prompting strategies, including in-context learning method, and compare them against six traditional models across four datasets. • In the four LLM-related dimensions, we gain seven interesting observations, providing a better understanding of LLMs as recommenders for future researches."
https://arxiv.org/html/2411.00744v1,CORAG: A Cost-Constrained Retrieval Optimization System for Retrieval-Augmented Generation,"Large Language Models (LLMs) have demonstrated remarkable generation capabilities but often struggle to access up-to-date information, which can lead to hallucinations. Retrieval-Augmented Generation (RAG) addresses this issue by incorporating knowledge from external databases, enabling more accurate and relevant responses. Due to the context window constraints of LLMs, it is impractical to input the entire external database context directly into the model. Instead, only the most relevant information, referred to as “chunks”, is selectively retrieved. However, current RAG research faces three key challenges. First, existing solutions often select each chunk independently, overlooking potential correlations among them. Second, in practice, the utility of chunks are “non-monotomic”, meaning that adding more chunks can decrease overall utility. Traditional methods emphasize maximizing the number of included chunks, which can inadvertently compromise performance. Third, each type of user query possesses unique characteristics that require tailored handling—an aspect that current approaches do not fully consider.To overcome these challenges, we propose a cost-constrained retrieval optimization system CORAG for retrieval-augmented generation. We employ a Monte Carlo Tree Search (MCTS)-based policy framework to find optimal chunk combinations sequentially, allowing for a comprehensive consideration of correlations among chunks. Additionally, rather than viewing budget exhaustion as a termination condition, we integrate budget constraints into the optimization of chunk combinations, effectively addressing the non-monotonicity of chunk utility. Furthermore, by designing a configuration agent, our system predicts optimal configurations for each query type, enhancing adaptability and efficiency. Experimental results indicate an improvement of up to 30% over baseline models, underscoring the framework’s effectiveness, scalability, and suitability for long-context applications.","Although LLMs have demonstrated exceptional capabilities in generation tasks, they often struggle with accessing up-to-date information, which can lead to hallucinations (Huang et al., 2023; Xu et al., 2024). To address these challenges, RAG has emerged as a crucial solution. By integrating external data sources into LLM, RAG can provide more accurate, relevant, and up-to-date information. Nowadays, RAG has been widely studied in the context of LLMs especially for tasks requiring update external knowledge such as question answering task (Asai et al., 2023; Sarthi et al., 2024; Microsoft, 2024), medical information retrieval (Singhal et al., 2022; Alkhalaf et al., 2024), and time series analysis (Ravuru et al., 2024; Jin et al., 2023; Ye et al., 2024). External data sources are often extremely large, making it impractical to input them directly into the LLM. To address this issue, data is typically split into disjoint chunks and stored in a vector database, and then users query the most useful chunks to construct prompts for LLMs. Therefore, designing efficient and accurate structures and algorithms to search for the most relevant chunks has become a prominent research topic and has been widely studied in both the database (Xue et al., 2024; Zhao et al., 2024c) and machine learning communities (Asai et al., 2023; Yu et al., 2024a; Wang et al., 2024b). Figure 1. Example of chunks combination order. However, there are three key challenges in the existing approaches. Challenge 1: Correlations between chunks Currently, two primary methods are used to identify the most relevant chunks. The first approach formulates the problem as a approximate k-nearest neighbor (AKNN) task (Zhang et al., 2024; Yin et al., 2024), where each chunk is assigned a score, and the approxiamte top-k𝑘kitalic_k chunks ranked by score are selected. The second approach clusters the chunks, returning all chunks within the most relevant clusters in response to a query (Microsoft, 2024; Sarthi et al., 2024). However, both methods overlook potential correlations between chunks: the first approach disregards correlations entirely, while the second approach accounts for them only superficially by treating all chunks within each cluster as equally relevant. As a result, when multiple chunks convey similar or overlapping information, these methods introduce substantial redundancy in the selected chunks. For example, as illustrated in Figure 1, when querying the height and history of the Eiffel Tower, if each chunk is treated independently, a greedy method would select chunks χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ1subscript𝜒1\chi_{1}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT since they have the top two scores. However, both chunks only provide historical information, which is insufficient to fully address the query. To better address the query, it is necessary to include a chunk with constructor’s name, such as χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT. On the other hand, the clustering approach would return all of χ1,χ2,χ3subscript𝜒1subscript𝜒2subscript𝜒3\chi_{1},\chi_{2},\chi_{3}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_χ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT, and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, resulting in redundancy. An optimal solution would instead select χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT, as they provide the required information without redundancy. Additionally, research (Lu et al., 2021; Jiang et al., 2024; Yu et al., 2024b) has shown that the order of chunks influences LLM performance, a factor that existing methods also overlook. Following the example of the Eiffel Tower, when chunks χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT are selected, placing χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT first yields a higher score compared with the reverse order will have a better performance. However, determining the optimal chunk combination order is a challenging task since both of them require a search space growing exponentially with the number of available chunks. In this paper, we further demonstrate that this problem is NP-hard (see Section 2.1). Challenge 2: Non-monotonicity of utility Current solutions operate on the assumption that including more chunks will always yield better final results. Specifically, in the AKNN-based approach, exactly k𝑘kitalic_k chunks are selected deterministically each time. In the clustering-based approach, a distance threshold between clusters and the query is set, and all clusters within this threshold are returned. Both of them return as many chunks as possible. However, in practice, the utility of chunks is not monotonic. More specifically, excessive chunks can dilute key information by adding marginally relevant content, creating noise that reduces clarity. Additionally, conflicting or nuanced differences across chunks may confuse the model, lowering response quality. For example, as illustrated in Figure 1, when χ3subscript𝜒3\chi_{3}italic_χ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and χ4subscript𝜒4\chi_{4}italic_χ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT are selected, adding the chunk χ1subscript𝜒1\chi_{1}italic_χ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT decreases utility, highlighting that utility scores are often non-monotonic in practice. Challenge 3: Diversity of queries: User queries come in different types, each requiring its own ranking strategy due to their unique characteristics (Zhao et al., 2024a). In current RAG systems, the utility scores of chunks often are determined by the assigned reranker model. So far, various reranker models exist, but we observe that their performance varies significantly across different query types, and no single fixed reranker model consistently outperforms the others across all query variations (see our experiments in Section 6.3.4 for more details). Current methods (Lyu et al., 2024; Zhao et al., 2024b) typically rely on static reranker models for ranking chunks, lacking the flexibility to adapt to varying query contexts. Problem Statement: Is there a RAG system that fully considers correlations between chunks and the non-monotonicity of utility while being adaptable to all types of queries? 1.1. Our Contributions In this paper, we answer this question in the affirmative, by proposing a novel MCTS based policy tree framework to optimize chunk retrieval in RAG systems. In summary, our contributions can be summarized as follows: • We propose the first RAG framework that considers the chunk combination order for the RAG task. Instead of considering each chunk independently or at the cluster level, we use MCTS to help search the optimal chunk combination order sequentially. The high-level idea is as follows: First, we initialize the root node. Then, in an iterative process, we expand the tree by selecting the highest utility node and computing its expended nodes’ utilities. After each expansion, we update the utilities throughout the entire policy tree. During this process, the decision at each iteration depends on the chunks already selected, allowing us to fully consider the correlations between chunks. Moreover, MCTS reduces the exponential search space to linear, and we apply parallel expansion techniques to further enhance computational efficiency. With such designs, we address Challenge 1. • In contrast to prior RAG frameworks that consider the exhaustion of the budget as one of termination conditions, we propose a novel formulation wherein budget constraints are integrated into the process of optimizing chunk combinations to fully consider the non-monotonicity of utility of chunks thereby addressing Challenge 2. Moreover, by prioritizing high-relevance, low-cost chunks and factoring in token length, we further reduce computational costs. • We propose a contrastive learning-based agent that dynamically adjusts MCTS configurations per query, adapting reranker models and configurations to the specific query domain. This approach tailors retrieval for dynamic, domain-specific queries with flexibility and robustness, addressing Challenge 3. • Additionally, we conducted comprehensive experiments, comparing our framework with several state-of-the-art methods. The results validate the effectiveness, efficiency, and scalability of our approach, also showing a performance improvement of 30% over the baseline."
https://arxiv.org/html/2411.00469v1,MIRFLEX: Music Information Retrieval Feature Library for Extraction,"This paper introduces an extendable modular system that compiles a range of music feature extraction models to aid music information retrieval research. The features include musical elements like key, downbeats, and genre, as well as audio characteristics like instrument recognition, vocals/instrumental classification, and vocals gender detection. The integrated models are state-of-the-art or latest open-source. The features can be extracted as latent or post-processed labels, enabling integration into music applications such as generative music, recommendation, and playlist generation. The modular design allows easy integration of newly developed systems, making it a good benchmarking and comparison tool. This versatile toolkit supports the research community in developing innovative solutions by providing concrete musical features.","Music Information Retrieval (MIR) is a complex field focused on computational analysis and processing of musical data, with tasks like similarity estimation, genre classification, and recommendation. While recent advances in machine learning have led to powerful feature extraction methods, the fragmented nature of these tools poses challenges for researchers who must integrate multiple disparate systems. To address this, we present MIRFLEX, a unified feature extraction library designed for MIR research. MIRFLEX offers a diverse set of extractors covering key musical aspects such as key, beats, and genre, using both signal processing and machine learning techniques to generate comprehensive audio representations. The primary objectives of this work are threefold: 1. To offer a centralized and easily accessible collection of feature extraction tools, reducing the burden on researchers to implement and integrate disparate feature extraction techniques. 2. To provide a comprehensive feature set that captures the multifaceted nature of musical data, enabling researchers to explore a wide range of music-related applications and queries. 3. To contribute to the advancement of music information retrieval research by facilitating the rapid prototyping and development of new applications that leverage easily accessible and readily available musical features. The proposed feature extraction library is available at 111https://github.com/AMAAI-Lab/megamusicaps"
https://arxiv.org/html/2411.00451v1,Improving Few-Shot Cross-Domain Named Entity Recognition by Instruction Tuning a Word-Embedding based Retrieval Augmented Large Language Model,"Few-Shot Cross-Domain NER is the process of leveraging knowledge from data-rich source domains to perform entity recognition on data-scarce target domains. Most previous state-of-the-art (SOTA) approaches use pre-trained language models (PLMs) for cross-domain NER. However, these models are often domain specific. To successfully use these models for new target domains, we need to modify either the model architecture or perform model fine-tuning using data from the new domains. Both of these result in the creation of entirely new NER models for each target domain which is infeasible for practical scenarios. Recently, several works have attempted to use LLMs to solve Few-Shot Cross-Domain NER. However, most of these are either too expensive for practical purposes or struggle to follow LLM prompt instructions. In this paper, we propose IF-WRANER (Instruction Finetuned Word-embedding based Retrieval Augmented large language model for Named Entity Recognition), a retrieval augmented LLM, finetuned for the NER task. By virtue of the regularization techniques used during LLM finetuning and the adoption of word-level embedding over sentence-level embedding during the retrieval of in-prompt examples, IF-WRANER is able to outperform previous SOTA Few-Shot Cross-Domain NER approaches. We have demonstrated the effectiveness of our model by benchmarking its performance on the open source CrossNER dataset, on which it shows more than 2% F1 score improvement over the previous SOTA model. We have deployed the model for multiple customer care domains of an enterprise. Accurate entity prediction through IF-WRANER helps direct customers to automated workflows for the domains, thereby reducing escalations to human agents by almost 15% and leading to millions of dollars in yearly savings for the company.","Named Entity Recognition (NER) (Chinchor and Robinson, 1997) is a key process in information extraction, designed to identify and categorize entities in natural language into predefined entity types. Due to the large variations in entities and the way they are used across domains, NER has been a challenging task in NLP. Most traditional NER models require large volumes of labelled data for training (Wang et al., 2022; Yu et al., 2020; Wang et al., 2020b; Li et al., 2022a). However, collecting large volumes of labeled data is both costly and time consuming. Therefore, we need a model that can perform NER on multiple domains with minimal labeled examples from that domain. To tackle this problem, several solutions have been proposed, which attempt to transfer knowledge from data rich source domain to perform NER on data-scarce target domain. This is referred to as Few-Shot Cross-Domain NER. The traditional way of solving this involves training PLMs with entity-tagged source domain data, followed by fine-tuning them on target domain data, thereby transferring knowledge from source to target domain. This approach fails to address the semantic gap that may exist between source and target domains. To address this, some previous studies utilize adding auxiliary objects (Liu et al., 2020a; Wang et al., 2020a) or designing new model architectures (Jia et al., 2019; Liu et al., 2020a; Jia and Zhang, 2020) to train with both source and target domain data. Liu et al., leverages continued pretraining with target domain data for a better understanding of domain-specific data. Another line of research (Zheng et al., 2022; Hu et al., 2022) focuses on modeling the label relationship across domains to improve label information transfer. Specifically, LANER (Hu et al., 2022) utilizes an architecture to better leverage semantic relationships among domain labels to increase cross-domain performance. Most Few-Shot Cross-Domain NER models however, have one or both of the following weaknesses: • Models like LANER (Hu et al., 2022), have architectures that are very specific to the source and target domain pairs. Making these models work well for a new target domain requires tweaking the architecture. • Other approaches require finetuning of model on target domain data. This is not feasible in real life scenarios due to computational resource and time crunch. Our approach involves a single model architecture, finetuned only using entity tagged source domain data. For adaptation to target domain, our model does not require further fine-tuning. Recently, with the advent of generative AI, many researchers have tried to use LLMs to solve the Few Shot Cross-Domain NER problem. GPT-NER (Wang et al., 2023) and PromptNER (Ashok and Lipton, 2023) have experimented with different LLM prompting strategies for the task with varying degrees of success. GPT-NER further demonstrates that using the Retrieval Augmented Generation (RAG) (Lewis et al., 2020) framework to select the in-prompt examples further boosts NER performance. One common theme that has emerged from these works is that most of these approaches demonstrate good performance with GPT4 as the backbone LLM. Open source alternatives typically fall well short of SOTA performance as they do not seem to closely follow the prompt instructions. This is a serious problem for real world scenarios as the use of proprietary software like GPT4 can be cost-prohibitive, especially for applications operating at scale. In our approach, we finetune open source LLMs (Touvron et al., 2023), so that they can follow domain specific prompt instructions for the NER task. Like GPT-NER (Wang et al., 2023), we too utilise the RAG framework for selecting in-prompt examples. We store labelled domain data and their vector embeddings in a vector store and extract relevant domain examples from the store during inference based on similarity between the inference query embedding and the embeddings stored. Most applications using the RAG framework use sentence level embeddings for similarity score calculations. In our work, we show that for the NER task, retrieving examples based on word-level embedding similarity performs much better than that based on sentence-level embedding similarity."
https://arxiv.org/html/2411.00262v1,Content Aware Analysis of Scholarly Networks:A Case Study on CORD19 Dataset,"This paper investigates the relationships among key elements of scientific research network, namely articles, researchers, and journals. We introduce a novel approach to use semantic information through the HITS algorithm based propagation of topic information in the network. The topic information is derived by using the Named Entity Recognition and Entity Linkage. In our case, MedCAT is used to extract the topics from the CORD19 Dataset, which is a corpus of academic articles about COVID-19 and coronavirus scientific network. Our approach focuses on the COVID-19 domain, utilizing the CORD-19 dataset to demonstrate the efficacy of integrating topic-related information within the citation framework. Through the application of a hybrid HITS algorithm, we show that incorporating topic data significantly influences article rankings, revealing deeper insights into the structure of the academic community.","The core elements of scientific research include articles, researchers, and institutions. Since scientific research is the cumulative effort of researchers to increase the understanding of the world around us, the relationships between these elements are as important as the scientific results themselves. Gaining insight into the relationship between the core elements of scientific research can be useful for a variety of purposes, such as guiding scientific effort toward better use of resources, inferring comparative results between fields of research, better representing the importance of certain research fields and research groups. Current scientific literature continues to grow at a rapid pace every day. Let alone being able to follow the growth of communities in which we are not a part, it has become very difficult to even find conferences, journals or other prominent studies in our field. Naturally, examining the academic community in detail becomes a great burden for most young or experienced researchers, which results in missing out promising researchers and useful works. To overcome this problem, most researchers represent the scientific literature as a wide network consists of different entities such as researchers, institutes, etc. In this paper, we aimed to analyze current literature and demonstrate different approaches to this problem with some practical applications. The academic writers, their studies and the citation connection between them composes the scientific community, which forms a wide network of authors and articles. Authors are identified as the entities that creates the knowledge in the community through the articles they have published. The citation network which is derived from the published work is the most common representation of this knowledge, which is very simple yet effective to analyze the communities. Social Network Analysis is a way of measuring and mapping various aspects of relationships between different entities such as people, organizations and groups Sweet [2018]. At first step, we started our analysis from simple representation of the network which is a graph of authors and articles. Then, we focus on the possible interpretations of the centrality metrics, PageRank and its variations in the real world scenarios. Apart from the approach above, it is clear that the proposed citation network lacks of the semantic meaning of the published works. Also representation of the topics is missing in the constructed graph of authors and their articles. Even though some solutions based on Natural Language Processing are available in the literature, most of these works require to process and analyze the content of the published articles via their open access files or abstracts. So, we propose a new and robust method to represent and retrieve the data in scientific network by considering the topics as an entity in the research graph. The topics are derived from a pipeline based on Named Entity Recognition and Knowledge Base of the relevant graph. Naturally, we have to focus on a specific domain to use the knowledge base effectively. Eventually, we showed the applicability of our method on a chosen domain and dataset, which are COVID-19 and CORD19 Dataset. Our motivation in this paper is to develop a way to provide the researchers with quantifiable information about the relationships between these elements so that it can be used for such purposes. This quantifiable information includes graph measures of individual elements of a graph as well as the graph measures of the whole graph. Briefly, we propose a pipeline to create, analyze and store the research network which consists of authors, articles, named entities and relationships between them."
https://arxiv.org/html/2411.00163v1,PSL: Rethinking and Improving Softmax Loss from Pairwise Perspective for Recommendation,"Softmax Loss (SL) is widely applied in recommender systems (RS) and has demonstrated effectiveness. This work analyzes SL from a pairwise perspective, revealing two significant limitations: 1) the relationship between SL and conventional ranking metrics like DCG is not sufficiently tight; 2) SL is highly sensitive to false negative instances. Our analysis indicates that these limitations are primarily due to the use of the exponential function. To address these issues, this work extends SL to a new family of loss functions, termed Pairwise Softmax Loss (PSL), which replaces the exponential function in SL with other appropriate activation functions. While the revision is minimal, we highlight three merits of PSL: 1) it serves as a tighter surrogate for DCG with suitable activation functions; 2) it better balances data contributions; and 3) it acts as a specific BPR loss enhanced by Distributionally Robust Optimization (DRO). We further validate the effectiveness and robustness of PSL through empirical experiments. The code is available at https://github.com/Tiny-Snow/IR-Benchmark.","Nowadays, recommender systems (RS) have permeated various personalized services [1, 2, 3, 4]. What sets recommendation apart from other machine learning tasks is its distinctive emphasis on ranking [5]. Specifically, RS aims to retrieve positive items in higher ranking positions (i.e., giving larger prediction scores) over others and adopts specific ranking metrics (e.g., DCG [6] and MRR [7]) to evaluate its performance. The emphasis on ranking inspires a surge of research on loss functions in RS. Initial studies treated recommendation primarily as a classification problem, utilizing pointwise loss functions (e.g., BCE [8], MSE [9]) to optimize models. Recognizing the inherent ranking nature of RS, pairwise loss functions (e.g., BPR [10]) were introduced to learn a partial ordering among items. More recently, Softmax Loss (SL) [11] has integrated contrastive learning paradigms [12, 13], augmenting positive items as compared with negative ones, achieving state-of-the-art (SOTA) performance. While SL has proven effective, it still suffers from two limitations: 1) SL can be used to approximate ranking metrics, e.g., DCG and MRR [11, 14], but their relationships are not sufficiently tight. Specifically, SL uses the exponential function exp⁡(⋅)⋅\exp(\cdot)roman_exp ( ⋅ ) as the surrogate activation to approximate the Heaviside step function in DCG, resulting in a notable gap, especially when the surrogate activation takes larger values. 2) SL is sensitive to noise (e.g., false negatives [15]). Gradient analysis reveals that SL assigns higher weights to negative instances with large prediction scores, while the weights are rather skewed and governed by the exponential function. This characteristic renders the model highly sensitive to false negative noise. Specifically, false negative instances are common in RS, as a user’s lack of interaction with an item might stem from unawareness rather than disinterest [16, 17, 18]. These instances would receive disproportionate emphasis, potentially dominating the training direction, leading to performance degradation and training instability. To address these challenges, we propose a new family of loss functions, termed Pairwise Softmax Loss (PSL). PSL first reformulates SL in a pairwise manner, where the loss is applied to the score gap between positive-negative pairs. Such pairwise perspective is more fundamental to recommendation as the ranking metrics are also pairwise dependent. Recognizing that the primary weakness of SL lies in its use of the exponential function, PSL replaces this with other surrogate activations. While this extension is straightforward, it brings significant theoretical merits: • Tighter surrogate for ranking metrics. We establish theoretical connections between PSL and conventional ranking metrics, e.g., DCG. By choosing appropriate surrogate activations, such as ReLU or Tanh, we demonstrate that PSL achieves a tighter DCG surrogate loss than SL. • Control over the weight distribution. PSL provides flexibility in choosing surrogate activations that control the weight distribution of training instances. By substituting the exponential function with an appropriate surrogate activation, e.g., ReLU or Tanh, PSL can mitigate the excessive impact of false negatives, thus enhancing robustness to noise. • Theoretical connections with BPR loss. Our analyses reveal that optimizing PSL is equivalent to performing Distributionally Robust Optimization (DRO) [19] over the conventional pairwise loss BPR [10]. DRO is a theoretically sound framework where the optimization is not only on a fixed empirical distribution but also across a set of distributions with adversarial perturbations. This DRO characteristic endows PSL with stronger generalization and robustness against out-of-distribution (OOD), especially given that such distribution shifts are common in RS, e.g., shifts in user preference and item popularity [16, 20, 21]. Our analyses underscore the theoretical effectiveness and robustness of PSL. To empirically validate these advantages, we implement PSL with typical surrogate activations (Tanh, Atan, ReLU) and conduct extensive experiments on four real-world datasets across three experimental settings: 1) IID setting [22] where training and test distributions are identically distributed [23]; 2) OOD setting [24] with distribution shifts in item popularity; 3) Noise setting [15] with a certain ratio of false negatives. Experimental results demonstrate the superiority of PSL over existing losses in terms of recommendation accuracy, OOD robustness, and noise resistance."
https://arxiv.org/html/2411.00041v1,NeuroSym-BioCAT: Leveraging Neuro-Symbolic Methods for Biomedical Scholarly Document Categorization and Question Answering,"The growing volume of biomedical scholarly document abstracts presents an increasing challenge in efficiently retrieving accurate and relevant information. To address this, we introduce a novel approach that integrates an optimized topic modelling framework, OVB-LDA, with the BI-POP CMA-ES optimization technique for enhanced scholarly document abstract categorization. Complementing this, we employ the distilled MiniLM model, fine-tuned on domain-specific data, for high-precision answer extraction. Our approach is evaluated across three configurations: scholarly document abstract retrieval, gold-standard scholarly documents abstract, and gold-standard snippets, consistently outperforming established methods such as RYGH and bio-answer finder. Notably, we demonstrate that extracting answers from scholarly documents abstracts alone can yield high accuracy, underscoring the sufficiency of abstracts for many biomedical queries. Despite its compact size, MiniLM exhibits competitive performance, challenging the prevailing notion that only large, resource-intensive models can handle such complex tasks. Our results, validated across various question types and evaluation batches, highlight the robustness and adaptability of our method in real-world biomedical applications. While our approach shows promise, we identify challenges in handling complex list-type questions and inconsistencies in evaluation metrics. Future work will focus on refining the topic model with more extensive domain-specific datasets, further optimizing MiniLM and utilizing large language models (LLM) to improve both precision and efficiency in biomedical question answering.","With around 2.5 million new research contributions every year [1] specifically the rapidly growing biomedical research, the need for efficient and accurate information retrieval methods has become increasingly critical. The total volume of scholarly documents and the complexity of biomedical queries present substantial challenges in extracting relevant answers from vast repositories of knowledge. As the field advances, researchers are often faced with the daunting task of sifting through an extensive array of documents to obtain precise information. This highlights the necessity for robust answer extraction and document categorization methods that can enhance the accessibility of vital information. To address these challenges, this research proposes a neuro-symbolic approach that combines optimized topic modelling with advanced machine learning techniques, effectively integrating symbolic reasoning with neural representations for enhanced document retrieval and answer extraction. Specifically, we explore the efficacy of our method through three distinct configurations: the utilization of scholarly document abstract retrieval methods, golden scholarly documents abstract, and golden snippets. Our method evaluations demonstrate that our topic model-based document categorization outperforms existing methods, such as RYGH and bio-answer finder, which utilize a complex blend of techniques like BM25 [2], ElasticSearch [3], and various transformer models [4]. This suggests that a simpler yet fine-tuned approach can lead to more effective and cost-efficient solutions for biomedical information retrieval. The primary research question in this investigation was: How can optimized scholarly document abstract categorization and answer extraction methodologies improve the accuracy and efficiency of information retrieval in the biomedical domain? Addressing this question is vital, as it not only enhances the precision of answer extraction but also reduces the cognitive load on researchers seeking relevant information. Furthermore, our findings reveal that even distilled smaller language models like MiniLM [5] can effectively extract answers when fine-tuned on domain-specific data, particularly when focused on scholarly document abstracts rather than complete documents. While the comparison with the use of Large Language Models [6] instead of the smaller MiniLM is also an interesting research avenue, it is deemed out of the scope of this research. Overall our promising results suggest a potential shift in focus for future biomedical information retrieval methods, advocating for strategies that emphasize the utility of concise scholarly abstracts. In summary, the contributions of this research comprise: 1. A novel neuro-symbolic answer extraction methodology that combines optimized topic modelling and advanced machine learning techniques, effectively addressing the challenges in biomedical information retrieval, particularly in extracting answers from an expanding corpus of scholarly documents abstract. 2. A novel answer extraction methodology that combines optimized topic modelling and advanced machine learning techniques, effectively addressing the challenges in biomedical information retrieval, particularly in extracting answers from an expanding corpus of scholarly documents abstract. 3. A comprehensive evaluation of the proposed method across three configurations— scholarly document abstract retrieval, golden scholarly documents abstract, and golden snippets—demonstrating its superior performance over existing methods like RYGH and bio-answer finder, while highlighting the advantages of simplicity and domain-specific fine-tuning. 4. Insights into the effective use of distilled models, specifically MiniLM, for accurate answer extraction when fine-tuned on domain-specific data, along with recommendations for future biomedical information retrieval methods to focus on concise scholarly document abstracts for enhanced efficiency and accuracy. methods Phase Approach bio-answerfinder A, B Bio-AnswerFinder, ElasticSearch, Bio-ELECTRA, ELECTRA, BioBERT, SQuAD, wRWMD, BM25, LSTM, T5 bioinfo A, B BM25, ElasticSearch, distant learning, DeepRank, universal weighting passage mechanism (UPWM), PARADE-CNN, PubMedBERT LaRSA A, B ElasticSearch, BM25, SQuAD, Macro Passage Ranking, BioBERT, BoolQA, BART ELECTROBERT A, B ELECTRA, ALBERT, BioELECTRA, BERT NEUROSYM-BIOCAT A, B OVB-LDA, CMA-ES, MiniLM RYGH A BM25, BioBERT, PubMedBERT, T5, BERTMeSH, SciBERT gsl A BM25, BERT, dual-encoder BioNIR B sBERT, distance metrics KU-methods B BioBERT, data augmentation MQ B tf-idf, sBERT, DistilBERT Ir_sys B BERT, SQuAD1.0, SpanBERT, XLNet, PubMedBERT, BioELECTRA, BioALBERT, BART UDEL-LAB B BioM-ALBERT, Bio-ELECTRA, SQuAD MQU B BART, summarization NCU-IISR/AS-GIS B BioBERT, BERTScore, SQuAD, logistic-regression Table 1: Approaches used by BioASQ 10b Participants."
