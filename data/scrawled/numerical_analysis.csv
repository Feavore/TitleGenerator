URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04840v1,Localized KBO with genetic dynamics for multi-modal optimization,"In this paper, we introduce a novel approach to multi-modal optimization by enhancing the recently developed kinetic-based optimization (KBO) method with genetic dynamics (GKBO). The proposed method targets objective functions with multiple global minima, addressing a critical need in fields like engineering design, machine learning, and bioinformatics. By incorporating leader-follower dynamics and localized interactions, the algorithm efficiently navigates high-dimensional search spaces to detect multiple optimal solutions. After providing a binary description, a mean-field approximation is derived, and different numerical experiments are conducted to validate the results.","Recently, several numerical methods leveraging collective dynamics for minimizing non-convex high-dimensional functions have been developed, [7, 8, 9, 13, 14, 15, 18, 19, 23]. These methods, also known as gradient-free methods, offer the advantage of enhancing the efficiency of traditional gradient-based numerical optimization techniques, such as the Newton method or stochastic gradient descent method, [11]. While these methods can be efficient when gradients are easily computed, they are less practical in scenarios where the function evaluations, which require the gradients computation, are costly or unreliable. In contrast, particle methods explore the search space without needing gradient computations, making them suitable for applications like machine learning and engineering, where simulating physical processes can be expensive. Thus, particle methods provide a more robust approach to find optimal solutions, with an enhanced potential to locate global minima rather than being restricted to local minima. Additionally, the introduction of follower-leader dynamics has been shown to be an effective tool in global optimization problems, allowing a strong speed up of the convergence process, [2, 4, 28, 10, 5]. Indeed, particle methods are characterized by an exploration phase which slows down the convergence, especially with high diffusion parameter values. By assuming that only a part of the population (followers) is involved in the exploration process, while the remaining one (leaders) quickly reach consensus on the estimated position of the global minimizer, it is possible to achieve faster convergence. With this work, we aim at extending the KBO algorithm enhanced by genetic dynamics (GKBO) introduced in [4], to incorporate the capability of exploring objective functions with multiple global minima. Multi-modal optimization is a critical area in numerical optimization where the goal is to identify multiple optimal solutions within an objective function that possesses several global minima. Various approaches, such as the ones described in [32], aim to maintain diversity and prevent premature convergence. Additionally, consensus-based optimization (CBO) methods, like the one introduced in [21], offer another effective strategy for handling multiple global minima. This is in contrast to traditional optimization, which typically seeks a single optimal solution. Multi-modal optimization is particularly relevant in fields such as engineering design, machine learning, and bioinformatics, where multiple equally good solutions can exist, each offering unique advantages. A variety of techniques have been proposed for multi-modal optimization, with evolutionary algorithms such as the ones proposed in [22, 25]. Additionally, polarized CBO methods [12] have been introduced to further enhance multi-modal optimization. Here, the dynamics is localized using a certain kernel, and particles, instead of being attracted by a common weighted mean, drift towards a weighted mean that emphasizes nearby particles more heavily. In this way, the convergence to a unique global minimizer is prevented, and multiple global minima can be detected. Similarities are shared with models for opinion formation or collective motion, where agents influence strongly their nearest neighbours, leading to the formation of different consensus points or clusters [3, 17]. The purpose of this paper is to show how to incorporate a similar dynamics in the GKBO algorithm to allow particles to concentrate over the different global minima. The GKBO algorithm belongs to the class of consensus based algorithms, catching ideas from genetic algorithms. The basic assumption is to have a population divided into two groups, namely leaders and followers, similar to the parents and children in the genetic dynamics which mimics biological evolution [26, 27, 33]. In genetic algorithms, individuals (parents) from the current population are chosen based on their objective values (genetic information) and combined to create the next generation (offspring). This selection process is typically guided by the principle of survival of the fittest, leading the system to progressively approach an optimal solution over successive generations. In this framework, leaders are selected in a manner similar to parents in genetic algorithms, while the generation of children is replaced by followers gradually moving in direction of the leaders’ positions. Then, the dynamics of classical KBO is split between the leaders, which move toward the global optimum estimate x^^𝑥\hat{x}over^ start_ARG italic_x end_ARG, and the followers, which explore the minimization landscape [7, 34]. In particular, each agent is identified by a certain position and label (x,λ)𝑥𝜆(x,\lambda)( italic_x , italic_λ ). If an agent is in the follower status, then λ=0𝜆0\lambda=0italic_λ = 0. On the contrary, if it is in the leaders status, then λ=1𝜆1\lambda=1italic_λ = 1. In particular two agents with state (x,λ)𝑥𝜆(x,\lambda)( italic_x , italic_λ ) and (x∗,λ∗)subscript𝑥subscript𝜆(x_{*},\lambda_{*})( italic_x start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT , italic_λ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT ) are supposed to interact as follows x′=x+(νF⁢(x∗−x)+σF⁢D⁢(x)⁢ξ)⁢λ∗⁢(1−λ)+νL⁢(x^⁢(t)−x)⁢λ,superscript𝑥′𝑥subscript𝜈𝐹subscript𝑥𝑥subscript𝜎𝐹𝐷𝑥𝜉subscript𝜆1𝜆subscript𝜈𝐿^𝑥𝑡𝑥𝜆\displaystyle x^{\prime}=x+(\nu_{F}(x_{*}-x)+\sigma_{F}D(x)\xi)\lambda_{*}(1-% \lambda)+\nu_{L}(\hat{x}(t)-x)\lambda,italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_x + ( italic_ν start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT - italic_x ) + italic_σ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT italic_D ( italic_x ) italic_ξ ) italic_λ start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT ( 1 - italic_λ ) + italic_ν start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ( over^ start_ARG italic_x end_ARG ( italic_t ) - italic_x ) italic_λ , (1.1) x∗′=x∗,superscriptsubscript𝑥′subscript𝑥\displaystyle x_{*}^{\prime}=x_{*},italic_x start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_x start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT , where x′superscript𝑥′x^{\prime}italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT denotes the post interaction position, νF,νL,σFsubscript𝜈𝐹subscript𝜈𝐿subscript𝜎𝐹\nu_{F},\nu_{L},\sigma_{F}italic_ν start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT , italic_ν start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT , italic_σ start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT are positive parameters balancing the exploration and attraction dynamics, ξ𝜉\xiitalic_ξ is a random perturbation term, D⁢(x)𝐷𝑥D(x)italic_D ( italic_x ) is a diffusion matrix and x^⁢(t)^𝑥𝑡\hat{x}(t)over^ start_ARG italic_x end_ARG ( italic_t ) denotes the estimated position of the global minimizer at time t𝑡titalic_t computed according to the Laplace principle [16]. Agents can vary their status in time accordingly to different strategies. In this work we focus on a particular approach, similar to the selection process between parents and children in the genetic algorithm [26]. More in details, agents who are in the best position over the cost function are assumed to be leaders, while the others are in the followers status. Consensus-based methods have proven effective for non-convex objectives, but they are limited by their ability to approximate only a single minimizer [20]. To address these limitations, our approach involves establishing distinct clusters, each associated to a certain leader, and to compute distinct estimates of each minimizer across the clusters, based on topological interactions between followers and leaders. The rest of the paper is organized as follows. In Section 2 we introduce the localised version of the GKBO algorithm, focusing on the description of the rules that govern the dynamics, and of the label switching procedure. In Section 3 we derive the mean field limit, assuming that both the leaders and followers population admit a continuous limit, and in particular the two equations governing the evolution of the followers and leaders densities functions. In Section 5 we introduce the numerical methods showing how to implement a Nanbu type algorithm which mimics the dynamics of the introduced method. In Section 6, we perform different simulations, showing the efficiency of the proposed algorithm in terms of success rate and iteration number, and comparing it with cluster-based algorithms."
https://arxiv.org/html/2411.04701v1,A high-order accurate moving mesh finite element method for the radial Kohn–Sham equation,"In this paper, we introduce a highly accurate and efficient numerical solver for the radial Kohn–Sham equation. The equation is discretized using a high-order finite element method, with its performance further improved by incorporating a parameter-free moving mesh technique. This approach greatly reduces the number of elements required to achieve the desired precision. In practice, the mesh redistribution involves no more than three steps, ensuring the algorithm remains computationally efficient. Remarkably, with a maximum of 13131313 elements, we successfully reproduce the NIST database results for elements with atomic numbers ranging from 1111 to 92929292.","Kohn–Sham density functional theory (KSDFT) is the most commonly used electronic structure method in condensed matter physics and a widely adopted approach in quantum chemistry [13, 9]. In KSDFT, the electronic structure of a system is obtained by solving the Kohn–Sham equations, which pose a nonlinear eigenvalue problem. Despite being a fundamental task, solving these equations presents numerical challenges. One challenge arises from the singularities in the external potential near the nucleus, which cause the wavefunctions to vary smoothly between atoms but exhibit sharp changes close to the nucleus [6]. A common approach to addressing numerical challenges in the core region is the use of pseudopotentials [20], where solving the atomic Kohn–Sham equation is crucial. Beyond pseudopotential generation [18], the atomic Kohn–Sham equation is also essential for computing atomic properties [14] and constructing methods such as linearized augmented plane waves (LAPW) [24] and linearized muffin-tin orbitals (LMTO) [25]. Various numerical methods have been developed to solve the radial Kohn–Sham equation, including the finite difference method [21], finite element methods [22, 4, 16, 17, 3], the B-spline method [23], and the integral equation approach [27], etc. Of these, the high-order finite element method stands out for its exponential convergence with respect to polynomial order [3] and its mesh flexibility. Currently, most finite element methods employ either a uniform mesh or a radial mesh. Uniform meshes are inefficient in the core region due to the high element density required, whereas radial meshes offer more flexibility. The most commonly used radial mesh is the exponential mesh, which, through careful adjustment of its parameters, can achieve high accuracy with relatively few elements. However, despite this flexibility, radial meshes can be cumbersome to work with due to the numerous user-defined parameters involved. In this work, we propose a moving mesh finite element method for solving the radial Kohn–Sham equation, which achieves high accuracy with a minimal number of elements by utilizing a moving mesh strategy, without requiring user-defined parameters. The strategy is based on the equidistribution principle introduced by de Boor [5], which involves placing mesh points so that a measure of the solution error is evenly distributed across each subinterval. This principle has proven highly effective for moving mesh methods [26, 10]. The key ingredient of the moving mesh method is the monitor function, which controls the redistribution of the mesh, typically defined by the singularity of the problem. In this work, the monitor function is designed as the sum of the curvatures of the wavefunctions for the radial Kohn–Sham equation. This leads to a moving mesh equation, which in its discretized form becomes a tridiagonal linear system, and can be solved efficiently using methods like the Thomas algorithm. The solution of this equation provides the new mesh distribution. Since the Kohn–Sham equation is nonlinear, a precise update of the solution is unnecessary—interpolating the solution from the old mesh to the new one is sufficient. The new mesh is then used to solve the Kohn–Sham equation, and the moving mesh process is repeated until the solution converges. In practice, starting from a uniform mesh, convergence can be achieved within three iterations. To achieve high accuracy, the radial Kohn–Sham equation is discretized using a high-order finite element method. Numerical experiments on the iron atom demonstrate the superiority of this approach over lower-order methods. For example, using a uniform mesh with polynomial order p=3𝑝3p=3italic_p = 3, achieving a precision of 10−6superscript10610^{-6}10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT Hartree requires 4600 elements. In contrast, the same accuracy is achieved with only 119 elements when p=10𝑝10p=10italic_p = 10, highlighting the necessity of employing high-order methods. To further enhance efficiency, the moving mesh strategy is applied. This technique adaptively concentrates mesh points where higher resolution is needed, significantly reducing the number of elements. For instance, with p=3𝑝3p=3italic_p = 3, the number of elements required to achieve 10−6superscript10610^{-6}10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT accuracy decreases from 4600 to 143 when using a moving mesh. Similarly, with p=10𝑝10p=10italic_p = 10, the element count drops from 119 to just 10, illustrating the effectiveness of the moving mesh approach. The computational performance is further improved by solving the generalized eigenvalue problem using the locally optimal block preconditioned conjugate gradient (LOBPCG) method [11]. To accelerate convergence, we adopt a preconditioner following the approach in [1]. This preconditioner consists of a discretized Laplacian and a shift operator with respect to the mass matrix, which dramatically reduces the number of LOBPCG iterations from thousands to just a few, significantly improving the computational efficiency. We validate our method by comparing the results with the NIST database [15] for elements ranging from hydrogen (H) to uranium (U), covering atomic numbers Z=1𝑍1Z=1italic_Z = 1 to Z=92𝑍92Z=92italic_Z = 92. The numerical results show excellent agreement with the NIST data for both total and orbital energies. Remarkably, these results are obtained using no more than 13 elements, with polynomial order p=10𝑝10p=10italic_p = 10, demonstrating the accuracy and efficiency of the proposed method. The rest of this paper is structured as follows. Section 2 introduces the radial Kohn–Sham equation and the finite element method. Section 3 outlines the moving mesh strategy. Section 4 presents the numerical results, and we conclude in Section 5."
https://arxiv.org/html/2411.04661v1,"A novel splitting strategy to accelerate solving generalized eigenvalue problem from Kohn–Sham density functional theory111Submitted date: November 7, 2024","In this paper, we propose a novel eigenpair-splitting method, inspired by the divide-and-conquer strategy, for solving the generalized eigenvalue problem arising from the Kohn-Sham equation. Unlike the commonly used domain decomposition approach in divide-and-conquer, which solves the problem on a series of subdomains, our eigenpair-splitting method focuses on solving a series of subequations defined on the entire domain. This method is realized through the integration of two key techniques: a multi-mesh technique for generating approximate spaces for the subequations, and a soft-locking technique that allows for the independent solution of eigenpairs. Numerical experiments show that the proposed eigenpair-splitting method can dramatically enhance simulation efficiency, and its potential towards practical applications is also demonstrated well through an example of the HOMO-LUMO gap calculation. Furthermore, the optimal strategy for grouping eigenpairs is discussed, and the possible improvements to the proposed method are also outlined.","The Kohn–Sham density functional theory plays an important role within the realms of quantum physics, condensed matter, and computational chemistry [1]. It provides an effective approach to determine the ground state of a quantum system containing Nn⁢u⁢csubscript𝑁𝑛𝑢𝑐N_{nuc}italic_N start_POSTSUBSCRIPT italic_n italic_u italic_c end_POSTSUBSCRIPT nuclei and Ne⁢l⁢esubscript𝑁𝑒𝑙𝑒N_{ele}italic_N start_POSTSUBSCRIPT italic_e italic_l italic_e end_POSTSUBSCRIPT electrons by solving the No⁢c⁢csubscript𝑁𝑜𝑐𝑐N_{occ}italic_N start_POSTSUBSCRIPT italic_o italic_c italic_c end_POSTSUBSCRIPT lowest eigenpairs (εi,ψi),i=1,…,No⁢c⁢cformulae-sequencesubscript𝜀𝑖subscript𝜓𝑖𝑖1…subscript𝑁𝑜𝑐𝑐{(\varepsilon_{i},\psi_{i})},{i=1,\dots,N_{occ}}( italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , italic_i = 1 , … , italic_N start_POSTSUBSCRIPT italic_o italic_c italic_c end_POSTSUBSCRIPT from the Kohn–Sham (KS) equation, which is given by (−12⁢∇2+Vext+VHar+Vxc)⁢ψi⁢(𝐫)=εi⁢ψi⁢(𝐫).12superscript∇2subscript𝑉extsubscript𝑉Harsubscript𝑉xcsubscript𝜓𝑖𝐫subscript𝜀𝑖subscript𝜓𝑖𝐫\left(-\frac{1}{2}\nabla^{2}+V_{\mathrm{ext}}+V_{\mathrm{Har}}+V_{\mathrm{xc}}% \right)\psi_{i}(\mathbf{r})=\varepsilon_{i}\psi_{i}(\mathbf{r}).( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∇ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_V start_POSTSUBSCRIPT roman_ext end_POSTSUBSCRIPT + italic_V start_POSTSUBSCRIPT roman_Har end_POSTSUBSCRIPT + italic_V start_POSTSUBSCRIPT roman_xc end_POSTSUBSCRIPT ) italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_r ) = italic_ε start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_ψ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_r ) . (1) In this equation, No⁢c⁢csubscript𝑁𝑜𝑐𝑐N_{occ}italic_N start_POSTSUBSCRIPT italic_o italic_c italic_c end_POSTSUBSCRIPT is defined as Ne⁢l⁢e/2subscript𝑁𝑒𝑙𝑒2N_{ele}/2italic_N start_POSTSUBSCRIPT italic_e italic_l italic_e end_POSTSUBSCRIPT / 2 for even Ne⁢l⁢esubscript𝑁𝑒𝑙𝑒N_{ele}italic_N start_POSTSUBSCRIPT italic_e italic_l italic_e end_POSTSUBSCRIPT and (Ne⁢l⁢e+1)/2subscript𝑁𝑒𝑙𝑒12(N_{ele}+1)/2( italic_N start_POSTSUBSCRIPT italic_e italic_l italic_e end_POSTSUBSCRIPT + 1 ) / 2 for odd, representing the number of occupation orbitals. The external potential Vext⁢(𝐫)subscript𝑉ext𝐫V_{\mathrm{ext}}(\mathbf{r})italic_V start_POSTSUBSCRIPT roman_ext end_POSTSUBSCRIPT ( bold_r ) is characterized by the sum of the Coulombic interactions between the electrons and the nuclei positioned at {𝐑I}I=1,…,Nn⁢u⁢csubscriptsubscript𝐑𝐼𝐼1…subscript𝑁𝑛𝑢𝑐\{\mathbf{R}_{I}\}_{I=1,\dots,N_{nuc}}{ bold_R start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_I = 1 , … , italic_N start_POSTSUBSCRIPT italic_n italic_u italic_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT with respective charges {ZI}I=1,…,Nn⁢u⁢csubscriptsubscript𝑍𝐼𝐼1…subscript𝑁𝑛𝑢𝑐\{Z_{I}\}_{I=1,\dots,N_{nuc}}{ italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_I = 1 , … , italic_N start_POSTSUBSCRIPT italic_n italic_u italic_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT, expressed as Vext⁢(𝐫)=−∑I=1Nn⁢u⁢cZI|𝐑I−𝐫|.subscript𝑉ext𝐫superscriptsubscript𝐼1subscript𝑁𝑛𝑢𝑐subscript𝑍𝐼subscript𝐑𝐼𝐫V_{\mathrm{ext}}(\mathbf{r})=-\sum_{I=1}^{N_{nuc}}\frac{Z_{I}}{|\mathbf{R}_{I}% -\mathbf{r}|}.italic_V start_POSTSUBSCRIPT roman_ext end_POSTSUBSCRIPT ( bold_r ) = - ∑ start_POSTSUBSCRIPT italic_I = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_n italic_u italic_c end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG italic_Z start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT end_ARG start_ARG | bold_R start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT - bold_r | end_ARG . The Hartree potential term VHar⁢(𝐫)subscript𝑉Har𝐫V_{\mathrm{Har}}(\mathbf{r})italic_V start_POSTSUBSCRIPT roman_Har end_POSTSUBSCRIPT ( bold_r ) encapsulates the classical electrostatic repulsion among the electrons. Lastly, Vxc⁢(𝐫)subscript𝑉xc𝐫V_{\mathrm{xc}}(\mathbf{r})italic_V start_POSTSUBSCRIPT roman_xc end_POSTSUBSCRIPT ( bold_r ) denotes the exchange-correlation potential, which accounts for the quantum mechanical effects of exchange and correlation that distinguish the Kohn-Sham non-interacting fictitious system from the actual many-electron system. Due to the lack of the analytical solution for the KS equation, it becomes necessary to employ numerical methods to obtain the solution of this equation. There exists several challenges in solving the KS equation. Firstly, the singularities in external potential term must be carefully addressed. The pseudopotential approaches [2] provide a way to circumvent the singularities by dividing electrons into valence electrons and inner core electrons, thereby reducing the atom to an inoic core that interacts with the valence electrons. However, under extreme environments, pseudopotential approaches may lead to mispredictions [3, 4]. Consequently, all-electron calculations that treat the external potential exactly are often necessary. To handle the singularities, the adaptive finite element (AFE) methods which enable different mesh sizes are favored [5, 6]. Additionally, AFE methods have become increasingly attractive for electronic structure calculations in recent decades due to their ability to manage non-periodic boundary conditions and complex computational domains [7, 8, 9, 10, 11, 12, 13, 14, 15]. Secondly, the KS equation is essentially an eigenvalue problem. As the size of the quantum system increases, the diagonalization and orthogonalization of the eigenvalue problem become computationally expensive [16]. Specifically, the size of the discretized system and the number of required eigenpairs both increase, making the calculation more challenging. Many efforts have been made to reduce the computational cost, such as orthogonalization-free methods [17, 18] and the divide-and-conquer (DAC) method [19, 20, 21]. In the latter approach, the global system is divided into several subsystems, and each subsystem is solved separately. This involves the decomposition of the computational domain. Inspired by this, we propose an alternative approach to divide the global system. Instead of decomposing the domain, we split the problem based on the eigenpairs. Specifically, the KS equation is decomposed into several subequations, each associated with a group of the eigenpairs. These subequations are solved independently to obtain the corresponding eigenpairs. We refer to this approach as the splitting method. The splitting of the eigenpairs based on the difference of the regularity among the corresponding wavefunctions, similar to the idea behind pseudopotentials. Specifically, the wavefunctions of inner core electrons vary more rapidly than those of valence electrons. Moreover, the decay of valence wavefunctions is slower than that of inner core wavefunctions. Therefore, when applying the adaptive finite element discretization to the KS equation, the mesh should effectively capture all the wavefunctions. Although the finite element space constructed on such a mesh is able to capture the variations for all wavefunctions, it may not be the most optimal space for any certain wavefunction. Based on this observation, we aim to tailor the discretized space for each group of wavefunctions which share similar regularity, meaning the errors of these waveftunctions are similar. This idea can be implemented using the multi-mesh adaptive strategy [22, 23]. Unlike the method that solves all equations on the same mesh (hereafter referred to as the single-mesh method), the multi-mesh adaptive method (abbreviated as the multi-mesh method) solves different variables with varying regularities on different meshes within the same computational domain. The mesh quality for a specific variable is improved through a mesh adaptation process similar to that used in the single-mesh method. In this study, the multi-mesh method is implemented based on the framework proposed in [22]. Within this framework, mesh adaptation is achieved by locally refining or coarsening elements, a process known as hℎhitalic_h-adaptation. Specifically, a posteriori error estimation is employed to guide the adaptations for all the meshes. The error indicator for each mesh is derived from the respective eigenpairs. It is noteworthy that the multi-mesh method has been used to handle KS wavefunctions and the Hartree potential [23], but this work differs in its approach to managing the KS wavefunctions. Several numerical challenges remain in the splitting method for the KS equation, including the splitting strategy and the solution of eigenvalue problems on different spaces. While one could further divide the eigenpairs into more groups beyond just valence and inner core electrons, we found that further splitting can negatively impact the convergence of the self-consistent iteration and increase storage requirements. For solving the eigenvalue problems, we indeed need to solve two separate problems. In this work, we use the LOBPCG method [24] to solve the discretized eigenvalue problem. For the first problem, we use traditional methods to find the smallest eigenpairs, while for the second problem, we need to solve for the middle eigenvalues, which is more complex. We employ the soft-locking strategy [25] to address this issue. Additionally, the orthogonality of the wavefunctions must be carefully maintained. Since the wavefunctions belong to different finite spaces, orthogonality can be compromised, which often occurs in simulations. The loss of orthogonality primarily arises from the interpolation and projection processes. However, the resulting physical quantities, such as the total energy, are not significantly affected by this loss, as verified by several examples. To further ensure the validity of our results, we present a technique to guarantee orthogonality at a negligible cost. In the next section, we introduce the KS model and the finite element discretizations. Section 3 details the splitting method and the associated numerical challenges. Section 4 presents various numerical examples. Finally, we conclude the paper with a summary of our work."
https://arxiv.org/html/2411.04643v1,"A Micro-Macro Decomposition-Based Asymptotic-Preserving Random
Feature Method for Multiscale Radiative Transfer Equations","This paper introduces the Asymptotic-Preserving Random Feature Method (APRFM) for the efficient resolution of multiscale radiative transfer equations. The APRFM effectively addresses the challenges posed by stiffness and multiscale characteristics inherent in radiative transfer equations through the application of a micro-macro decomposition strategy. This approach decomposes the distribution function into equilibrium and non-equilibrium components, allowing for the approximation of both parts through the random feature method (RFM) within a least squares minimization framework. The proposed method exhibits remarkable robustness across different scales and achieves high accuracy with fewer degrees of freedom and collocation points than the vanilla RFM. Additionally, compared to the deep neural network-based method, our approach offers significant advantages in terms of parameter efficiency and computational speed. These benefits have been substantiated through numerous numerical experiments conducted on both one- and two-dimensional problems.","The radiative transfer equation (RTE) is the governing equation that models the propagation and interactions of radiation or particles within participating media [1, 2]. It is a fundamental integro-differential equation in various fields, including astrophysics, radiative transfer [3], neutron transport [4] and optical tomography [5, 6, 7, 8], etc. In recent years, there has been significant interest in devising accurate and efficient methods for solving the multiscale radiative transfer equation. The primary bottleneck in numerically resolving the radiative transfer equation stems from the high dimensionality of phase space, stiffness of collision terms, and multiscale features, among others. Various numerical methods have been developed in the field of computational methods for the radiative transfer equation, which can generally be categorized into two classes: deterministic methods and stochastic simulation methods. One of the most popular deterministic methods is the discrete ordinates/velocity method (DOM/DVM) [4, 9], sometimes referred to as the SNsubscript𝑆𝑁S_{N}italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT method. The DOM discretizes the angular variable and solves the RTE along the discrete directions. Spherical harmonics methods possess the advantage of rotational invariance and are widely used in solving the radiative transfer equation [10, 11]. Note that the scale parameter (Knudsen number) in the radiative transfer equation can vary significantly, ranging from the kinetic regime to the diffusive regime, therefore numerical methods should be able to handle the multiscale nature of the radiative transfer equation. To tackle this challenge, two major categories of methods have been developed, namely, the domain decomposition-based methods [12] and the asymptotic-preserving schemes [13]. Domain decomposition-based methods decompose the domain into different regions, where different differential equations are solved in each region with an interface condition to couple them. The asymptotic-preserving schemes aim to design numerical methods that are uniformly stable and accurate, regardless of the scale parameter. For stochastic simulation methods, the direct simulation Monte Carlo (DSMC) method is widely used for solving the radiative transfer equation [14]. There are also some references on solving the radiative transfer equation, interested readers can refer to [15, 16, 17, 18, 19, 20, 21] for more details. In the past few years, deep learning methods have shown great potential in solving high-dimensional and complex PDE problems, due to their strong fitting ability and generalization ability. Researchers have developed some deep learning-based methods for solving the radiative transfer equation and kinetic equations [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]. Although deep learning methods have achieved great success in solving PDEs, they still face challenges in terms of interpretability, generalization, and computational efficiency. For example, the training time for deep learning methods can be very long, and the accuracy of the solution may be less than satisfactory. The random feature method (RFM) bridges the gap between deep learning methods and traditional numerical methods, and is effective in solving various PDEs [39, 40, 41, 42]. The RFM approximates the solution of the PDE by a linear combination of random feature functions and Partition of Unity (PoU) functions, and the coefficients are determined by a least squares minimization problem. In particular, the random feature functions are constructed by two-layer neural networks with fixed parameters in the hidden layers and the PoU functions are constructed by tensor product of univariate functions. Benefiting from the construction of the random feature functions and least squares minimization problem, the RFM can achieve high accuracy and efficiency in solving PDEs. In this paper, we start by introducing the RFM for solving the radiative transfer equation and find that the vanilla random feature method has tremendous difficulty in resolving small scales. To address this issue, we propose the Asymptotic-Preserving Random Feature Method (APRFM) for solving the multiscale radiative transfer equation. The APRFM is designed to effectively handle the multiscale nature of the radiative transfer equation by utilizing a micro-macro decomposition approach. Our method can approximate the solution of the radiative transfer equation by decomposing the distribution function into equilibrium and non-equilibrium components and approximating both parts through the RFM within a least squares minimization framework. The proposed method demonstrates superior robustness across varying scales compared to the vanilla RFM and is more efficient than the previous deep learning methods. The rest of the paper is organized as follows. In Section 2, we gave a brief introduction to the radiative transfer equation and the random feature method. Besides, we demonstrate the difficulty of the vanilla random feature method in resolving small scales. Section 3 is the main part of the paper, where we propose the Asymptotic-Preserving Random Feature Method. In Section 4, we present numerical results for both one- and two-dimensional problems to validate the effectiveness of our method. Finally, we conclude the paper in the last section."
https://arxiv.org/html/2411.04591v1,Compatible finite element interpolated neural networks,"We extend the finite element interpolated neural network (FEINN) framework from partial differential equations (PDEs) with weak solutions in H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT to PDEs with weak solutions in H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) or H⁢(div)𝐻divH(\textbf{div})italic_H ( div ). To this end, we consider interpolation trial spaces that satisfy the de Rham Hilbert subcomplex, providing stable and structure-preserving neural network discretisations for a wide variety of PDEs. This approach, coined compatible FEINNs, has been used to accurately approximate the H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) inner product. We numerically observe that the trained network outperforms finite element solutions by several orders of magnitude for smooth analytical solutions. Furthermore, to showcase the versatility of the method, we demonstrate that compatible FEINNs achieve high accuracy in solving surface PDEs such as the Darcy equation on a sphere. Additionally, the framework can integrate adaptive mesh refinements to effectively solve problems with localised features. We use an adaptive training strategy to train the network on a sequence of progressively adapted meshes. Finally, we compare compatible FEINNs with the adjoint neural network method for solving inverse problems. We consider a one-loop algorithm that trains the neural networks for unknowns and missing parameters using a loss function that includes PDE residual and data misfit terms. The algorithm is applied to identify space-varying physical parameters for the H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) model problem from partial or noisy observations. We find that compatible FEINNs achieve accuracy and robustness comparable to, if not exceeding, the adjoint method in these scenarios.","Conventional numerical discretisations of partial differential equations rely on a partition of the domain (mesh) to approximate a continuous problem. The most widely used discretisation methods are the finite element method (FEM), the finite difference method (FDM), and the finite volume method (FVM). Among these, FEM is very popular due to its flexibility to handle complex geometries, ability to achieve high-order accuracy, and suitability to tackle mixed formulations. Additionally, it is supported by a solid mathematical foundation [Ern2021], ensuring stability and accuracy. Over the past few decades, optimal FEM solvers for both linear and nonlinear PDEs have been developed, effectively exploiting the capabilities of large-scale supercomputers [Badia2016]. Various physical phenomena in science and engineering can be modelled using PDEs. As a classical example, the Poisson equation, with weak solution in H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT, describes phenomena such as heat conduction. The Maxwell’s equations, with weak solution in H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ), succinctly state the fundamentals of electricity and magnetism, while the Darcy equation, with weak flux and pressure solution in H⁢(div)𝐻divH(\textbf{div})italic_H ( div ) and L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT spaces, respectively, describes fluid flow through porous media. These spaces are connected through the differential operators grad, curl, and div, such that applying an operator to functions in one space maps surjectively onto the kernel of the next operator in the sequence, thereby forming a de Rham complex [Arnold2006]. Compatible FEs are structure-preserving discretisation methods for solving these equations. They search for approximate solutions in subspaces that, at the discrete level, preserve the same relation via differential operators as their continuous, infinite-dimensional counterparts, thus forming a discrete de Rahm complex. A common property of these complexes is the gradual reduction of inter-element continuity requirements from one space to the next: the discrete subspaces are made of piecewise polynomials that are continuous across cell boundaries in the case of H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT (thus having a well defined global weak gradient), only tangential or only normal components are continuous for H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) and H⁢(div)𝐻divH(\textbf{div})italic_H ( div ), respectively (thus having well-defined global weak curl and div, respectively), and completely discontinuous across cell boundaries for L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Although quite established in several application areas, compatible FEs have become increasingly popular, e.g., for the simulation of geophysical flows in the context of atmospheric and ocean modelling, mostly because their ability to effectively address the issue of spurious numerical waves present in other numerical schemes, while allowing for conservation of energy and other quantities; see [Cotter2023] for a recent survey. For a given polytope (e.g., a triangle or a quadrilateral) and polynomial order, there are several possible choices for the discrete subspaces that lead to the above discrete complex structure. In this work we leverage first-kind Nédélec [Nedelec1980] and Raviart-Thomas (RT) [Raviart1977] vector-valued FEs for H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) and H⁢(div)𝐻divH(\textbf{div})italic_H ( div ) spaces, respectively (apart from the more standard continuous and discontinuous Lagrangian nodal FEs for H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT spaces, respectively). These FEs define the local polynomial bases and degrees of freedom in the form of integrals (moments) over mesh edges, faces, and cells so as to ensure the above mentioned continuity constraints across element boundaries. We note that Nédélec FEs are particularly well-suited for Maxwell’s equations as, in contrast to nodal FEs, they avoid spurious solutions, even in the case of domains with re-entrant corners or edges [Costabel2002]. They can accurately approximate discontinuous fields due to large jumps in the physical properties of the material, and are better understood mathematically than other discretisation methods for the Maxwell’s equations [Monk2019]. Deep learning techniques, especially NNs, have gained significant popularity over the last few years for solving PDEs. The main idea is that one seeks an approximation to the solution of a PDE from a trial finite-dimensional nonlinear manifold (e.g., a neural network) as opposed to a finite-dimensional linear space as in FEM. One of the most notable methods is PINNs [Raissi2019]. Instead of solving algebraic systems of equations to find an approximate solution, PINNs minimise the strong PDE residual evaluated at a set of randomly sampled collocation points. \Acppinn have demonstrated relative success for forward (where only the solution/state is unknown) and inverse (where incomplete data, e.g., physical coefficients, is supplemented with observations of the state) PDE-constrained problems (see, e.g., [Pang2019, Yang2021]). The VPINN method [Kharazmi2021] utilises a loss function based on the variational or weak form of the PDE in order to weaken the regularity requirements on the solution. \Acpvpinn support hℎhitalic_h-refinement through domain decomposition and p𝑝pitalic_p-refinement via projection onto higher-order polynomial spaces. One of the issues in VPINNs and PINNs is the difficulties associated to the accurate integration of NNs (and their derivatives) [Magueresse2024]. To address the integration challenge in VPINNs, the interpolated (IVPINN) method [Berrone2022] proposes using polynomial interpolations of NNs as trial functions in the variational formulation. \Acpivpinn reduce computational costs compared to VPINNs and demonstrate significantly higher accuracy, especially when the solution is singular. Despite the improvements, these variational methods make use of the Euclidean norm of the discrete residual, which is not an appropriate measure of the error, since the residual is a functional in the dual space. As a result, the analysis for IVPINNs is sub-optimal. Another challenge in PINN-based methods is the imposition of essential boundary conditions [Sukumar2022]. These methods either impose the boundary conditions weakly through a penalty term [Raissi2019] or Nitsche’s method [Magueresse2024], or rely on a combination of two functions: an offset function that satisfies the boundary conditions and a distance function that vanishes at the boundary [Berrone2022]. On the other hand, the so-called interpolated (FEINN) method, proposed in [Badia2024], aims to find a function among all possible realisations of the nonlinear NN manifold whose interpolation onto a trial FE space minimises a discrete dual norm, over a suitable test FE space, of the weak residual functional associated to the PDE. The FEINN method imposes essential boundary conditions more naturally at the FE space level by interpolating the NNs onto a FE space that (approximately) satisfies the Dirichlet boundary conditions. The integration of the loss function can efficiently be handled using a Gaussian quadrature, as in FEM. The dual norm of the residual is an accurate measure of the error and the analysis in [Badia2024] shows that the interpolation of the NNs onto the FE space is a stable and accurate approximation of the solution. We refer to [Rojas2024-it] for the use of the dual norm of the residual in VPINNs. \Acp feinn have demonstrated exceptional performance over FEM when the target solution is smooth: the trained NNs outperform FEM solutions by several orders of magnitude in terms of L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT and H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT errors, even with complex geometries. With minimal modifications, FEINNs can also be extended to solve inverse problems and exhibit comparable performance to the adjoint-based NN approach [Badia2024]. Besides, when combined with adaptive meshing techniques, hℎhitalic_h-adaptive FEINNs [Badia2024adaptive] can efficiently solve PDEs featuring sharp gradients and singularities while unlocking the nonlinear approximation power of discrete NN manifold spaces. The trained NNs show potential to achieve higher accuracy than FEM, particularly when the solution is not singular [Badia2024adaptive]. Most existing NN discretisation PDE solvers are developed for problems with weak solutions in H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT, such as the Poisson equation [Magueresse2024, Badia2024adaptive], or in H1×L2superscript𝐻1superscript𝐿2H^{1}\times L^{2}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT × italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT, such as the Navier-Stokes equations [Cai2021, Cheng2021, Pichi2023]. However, there are limited studies attacking problems with weak solutions in H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) or H⁢(div)𝐻divH(\textbf{div})italic_H ( div ). In [Baldan2021], the authors employ NNs to solve 1D nonlinear magneto quasi-static Maxwell’s equations in frequency or time domains. Similar to PINNs, MaxwellNet [Lim2022] trains a CNN using the strong PDE residual of the Maxwell’s equations as the loss function. The authors in [Baldan2023] address inverse Maxwell’s problems by integrating a hypernetwork into the PINN framework. This addition enables the trained hypernetwork to act as a parametrised real-time field solver, allowing rapid solutions to inverse electromagnetic problems. There are even fewer works on NN-based solvers for the Darcy equation. The physics-informed (PICNN) method proposed in [Zhang2023] uses a CNN to simulate transient two-phase Darcy flows in homogeneous and heterogeneous reservoirs. To ensure flux continuity, they adopt a FVM to approximate the PDE residual in the loss function. In [He2020], the authors employ multiple NNs to approximate both the unknown parameters and states of the inverse Darcy systems. Their study shows that PINNs offer regularisation and decrease the uncertainty in NN predictions. Another interesting topic is the development of NN solvers for PDEs posed over immersed manifolds, e.g., the Darcy equation defined on a sphere. There are a few works on PINNs for surface PDEs in the literature. In [Fang2020], the authors extend PINNs to solve the Laplace-Beltrami equation on 3D surfaces. The authors in [Hu2024] utilise a single-hidden-layer PINNs to solve the Laplace-Beltrami and time-dependent diffusion equations on static surfaces, as well as advection-diffusion equations on evolving surfaces. Another related work is [Bihlo2022], where the authors apply PINNs to solve the shallow-water equations on the sphere. In this work, we integrate compatible FEs, i.e., spaces that form a discrete the Rham complex [Arnold2006], into the FEINN method proposed in [Badia2024]. This integration enables us to solve PDEs with weak solutions in H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) or H⁢(div)𝐻divH(\textbf{div})italic_H ( div ). We refer to this method as compatible FEINNs. The novelty of compatible FEINNs over the standard FEINN method in [Badia2024] lies in introducing curl/div-conforming trial FE spaces to interpolate NNs, ensuring the desired structure-preserving properties across element boundaries. The residual minimisation framework underlying FEINNs allows for some flexibility in selecting the trial and test FE spaces provided that an inf-sup compatibility condition is satisfied among them; see [Badia2024adaptive] for the numerical analysis of the method. As we showed in [Badia2024] and further confirmed in this paper for the problems at hand, a proper choice of these spaces can lead to significant improvements in the accuracy of the solution and the convergence of the optimiser. As an evidence on the soundness of this approach, we also showcase its applicability to solve PDEs posed over immersed manifolds. We interpolate NN vector-valued fields living in ambient space (and thus not necessarily tangent to the manifold) onto FE spaces made of functions constrained to be tangent to the manifold by construction. Strikingly, NNs trained on a coarse mesh and lower-order bases are able to provide several orders of magnitude higher accurate solutions to the problem when interpolated onto a FE space built out of finer meshes and higher order bases. Besides, this method, which resembles TraceFEM [Olshanskii2017] (as it seeks an approximation of the surface PDE on a finite-dimensional space of functions living on a higher dimensional space), does not need stabilisation terms in the loss function to enforce tangentiality, as it relies on a surface FE interpolation on the tangent space. A comprehensive set of numerical experiments is conducted to demonstrate the performance of compatible FEINNs in forward and inverse problems involving problems in H⁢(curl)𝐻curlH(\textbf{curl})italic_H ( curl ) and H⁢(div)𝐻divH(\textbf{div})italic_H ( div ). The rest of the paper is organised as follows. In Sect. 2, we introduce model problems for the Maxwell’s equations and Darcy equation considered in this article, their compatible FE discretisation, and the compatible FEINN method in both forward and inverse scenarios. Sect. 3 starts with a brief discussion on the implementation, and then presents the numerical results of the forward and inverse experiments. Finally, we conclude the paper in Sect. 4."
https://arxiv.org/html/2411.04497v1,"Uniformly higher order accurate schemes for dynamics of charged particles under fast oscillating magnetic fields111Submitted to arXiv on November 7, 2024.Funding:The first author was funded by the Ministry of Education, Government of India under the Prime Minister’s Research Fellowship/grant PM/MHRD-19-17567.03. The last author benefits from fundings by the European Union via the Euratom Research and Training Programme (Grant Agreement No 101052200 EUROFusion). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Commission. Neither the European Union nor the European Commission can be held responsible for them.","This work deals with the numerical approximation of plasmas which are confined by the effect of a fast oscillating magnetic field (see [12]) in the Vlasov model. The presence of this magnetic field induces oscillations (in time) to the solution of the characteristic equations. Due to its multiscale character, a standard time discretization would lead to an inefficient solver. In this work, time integrators are derived and analyzed for a class of highly oscillatory differential systems. We prove the uniform accuracy property of these time integrators, meaning that the accuracy does not depend on the small parameter ε𝜀\varepsilonitalic_ε. Moreover, we construct an extension of the scheme which degenerates towards an energy preserving numerical scheme for the averaged model, when ε→0→𝜀0\varepsilon\to 0italic_ε → 0. Several numerical results illustrate the capabilities of the method.","The confinement of charged particles system is of important interest nowadays, in particular due to the application to fusion devices. The main strategy used in the machines like tokamaks or stellarators is to apply a strong magnetic field 𝐁⁢(t,x)𝐁𝑡𝑥{\bf B}(t,x)bold_B ( italic_t , italic_x ) (t𝑡titalic_t denotes the time variable and x𝑥xitalic_x the space variable) to confine the particles far from the boundary wall of the device. Indeed, due to the extreme temperature of plasma, it is crucial to avoid direct contact of the particles with wall material while allowing the plasma to reach fusion conditions. To confine particles, it is also possible to consider fast oscillating (in time) magnetic fields instead of strong ones. Indeed, the choice proposed in [12] is the following: 𝐁⁢(t,x)=θ⁢(t/ε)⁢B⁢(x)⁢b⁢(x),0<ε<<1formulae-sequence𝐁𝑡𝑥𝜃𝑡𝜀𝐵𝑥𝑏𝑥0𝜀much-less-than1{\bf B}(t,x)=\theta(t/\varepsilon)B(x)b(x),0<\varepsilon<\!\!<1bold_B ( italic_t , italic_x ) = italic_θ ( italic_t / italic_ε ) italic_B ( italic_x ) italic_b ( italic_x ) , 0 < italic_ε < < 1, where θ𝜃\thetaitalic_θ is a given P𝑃Pitalic_P-periodic function, B⁢(x)𝐵𝑥B(x)italic_B ( italic_x ) is a scalar positive function, and b⁢(x)𝑏𝑥b(x)italic_b ( italic_x ) is a unitary vector. This configuration may require less energy to design compared to the strong magnetic field configuration and it has been proven that this configuration has good confinement properties (see [12]). In this work, we are interested in the numerical approximation of the model obtained with this highly-oscillatory magnetic field. From the Vlasov equation (4) in a four-dimensional setting in phase space, the external magnetic field is supposed to be 𝐁⁢(t,x)=θ⁢(t/ε)⁢(0,0,B)T𝐁𝑡𝑥𝜃𝑡𝜀superscript00𝐵𝑇{\bf B}(t,x)=\theta(t/\varepsilon)(0,0,B)^{T}bold_B ( italic_t , italic_x ) = italic_θ ( italic_t / italic_ε ) ( 0 , 0 , italic_B ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT with B>0𝐵0B>0italic_B > 0. The electric field has to be decomposed into a gradient and rotational part so that we can formulate the corresponding characteristic equations as follows: x˙⁢(t)˙𝑥𝑡\displaystyle\dot{x}(t)over˙ start_ARG italic_x end_ARG ( italic_t ) =\displaystyle== v⁢(t),𝑣𝑡\displaystyle v(t),italic_v ( italic_t ) , (1) v˙⁢(t)˙𝑣𝑡\displaystyle\dot{v}(t)over˙ start_ARG italic_v end_ARG ( italic_t ) =\displaystyle== E⁢(t,x⁢(t))+B2⁢ε⁢θ′⁢(t/ε)⁢J⁢x⁢(t)+θ⁢(t/ε)⁢B⁢J⁢v⁢(t),t∈[0,T]𝐸𝑡𝑥𝑡𝐵2𝜀superscript𝜃′𝑡𝜀𝐽𝑥𝑡𝜃𝑡𝜀𝐵𝐽𝑣𝑡𝑡0𝑇\displaystyle E(t,x(t))+{\color[rgb]{0,0,0}\frac{B}{2\varepsilon}\theta^{% \prime}\left(t/\varepsilon\right)Jx(t)}+\theta(t/\varepsilon)BJv(t),\;\;t\in[0% ,T]italic_E ( italic_t , italic_x ( italic_t ) ) + divide start_ARG italic_B end_ARG start_ARG 2 italic_ε end_ARG italic_θ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_t / italic_ε ) italic_J italic_x ( italic_t ) + italic_θ ( italic_t / italic_ε ) italic_B italic_J italic_v ( italic_t ) , italic_t ∈ [ 0 , italic_T ] (2) where x⁢(t)∈ℝ2𝑥𝑡superscriptℝ2x(t)\in\mathbb{R}^{2}italic_x ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT denotes the position, v⁢(t)∈ℝ2𝑣𝑡superscriptℝ2v(t)\in\mathbb{R}^{2}italic_v ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT denotes the velocity of the particle, E:ℝ+×ℝ2→ℝ2:𝐸→subscriptℝsuperscriptℝ2superscriptℝ2E:\mathbb{R}_{+}\times\mathbb{R}^{2}\to\mathbb{R}^{2}italic_E : blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT × blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT denotes a given irrotational electric field (deriving from an electric potential E=−∇ϕ𝐸∇italic-ϕE=-\nabla\phiitalic_E = - ∇ italic_ϕ, with ϕ:ℝ+×ℝ2→ℝ:italic-ϕ→subscriptℝsuperscriptℝ2ℝ\phi:\mathbb{R}_{+}\times\mathbb{R}^{2}\to\mathbb{R}italic_ϕ : blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT × blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT → blackboard_R) and J𝐽Jitalic_J denotes the canonical 2×2222\times 22 × 2 symplectic matrix. Our goal is to construct and analyse efficient numerical schemes to approximate differential systems of the form (1)-(2). It is worth noticing that the system (1)-(2) is highly oscillatory in time ; in particular the second order derivatives of v𝑣vitalic_v is stiff which makes the derivation of higher order schemes challenging since the consistency error usually involves the time derivatives of the solution. To capture the correct dynamics with standard numerical strategies, the time step Δ⁢tΔ𝑡\Delta troman_Δ italic_t has to be smaller than ε∈(0,1]𝜀01\varepsilon\in(0,1]italic_ε ∈ ( 0 , 1 ], leading to very expensive methods. Thus, the first goal is to overcome the stiffness induced by the highly oscillatory character of the solution. In addition, the method has to be consistent with the asymptotic (or averaged) model obtained in the limit ε→0→𝜀0\varepsilon\to 0italic_ε → 0 (see [12]). These properties are usually referred in the literature as asymptotic-preserving or uniformly accurate strategies which ensures that the limit of the scheme is a scheme for the limit model. During the last decade, several works have been proposed to solve the highly-oscillatory equations efficiently and accurately. These schemes have been referred as uniformly accurate since the error can be proved to be uniform with respect to ε𝜀\varepsilonitalic_ε. One can quote the Duhamel based strategies applied for Klein-Gordon or Dirac equations [9, 14, 13], two-scale methods used for Vlasov equations [22, 18, 25, 24, 26, 17, 23, 16, 30, 15, 21, 33], micro-macro approach [19, 23, 20], integrator scheme for advection-diffusion equations [3, 2], multiscale decomposition or exponential wave integrators for Klein-Gordon, Dirac or Schrodinger equations [8, 4, 6, 7, 5]. In this work, we also consider a Duhamel based approach to design uniformly accurate numerical scheme for (1)-(2). A first order uniformly accurate scheme can be easily obtained by freezing the unknown in time and integrating the highly oscillating part exactly, which mainly consists of integrating the function θ𝜃\thetaitalic_θ. To derive higher order numerical methods, it is possible to recursively substitute adequately the Duhamel formula in itself, in the spirit of [14, 3]. In the linear context, we will consider the following general framework U˙⁢(t)=A⁢(t/ε)⁢U⁢(t)˙𝑈𝑡𝐴𝑡𝜀𝑈𝑡\dot{U}(t)=A(t/\varepsilon)U(t)over˙ start_ARG italic_U end_ARG ( italic_t ) = italic_A ( italic_t / italic_ε ) italic_U ( italic_t ) (3) with U⁢(t)∈ℝd𝑈𝑡superscriptℝ𝑑U(t)\in\mathbb{R}^{d}italic_U ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and A𝐴Aitalic_A a given d×d𝑑𝑑d\times ditalic_d × italic_d matrix-valued P𝑃Pitalic_P-periodic function. In this case, it is possible to derive arbitrarily higher order uniformly accurate explicit schemes. On top of that, a suitable use of the Duhamel formula enables to design (second order) midpoint uniformly accurate numerical schemes. Such schemes are particularly relevant for the configuration where the system (3) has a specific structure (Hamiltonian or A𝐴Aitalic_A skew-symmetric). Indeed, in this case, the averaged model derived from (3) becomes U¯˙⁢(t)=⟨A⟩⁢U¯⁢(t)˙¯𝑈𝑡delimited-⟨⟩𝐴¯𝑈𝑡\dot{\bar{U}}(t)=\langle A\rangle\bar{U}(t)over˙ start_ARG over¯ start_ARG italic_U end_ARG end_ARG ( italic_t ) = ⟨ italic_A ⟩ over¯ start_ARG italic_U end_ARG ( italic_t ) when ε→0→𝜀0\varepsilon\to 0italic_ε → 0, and midpoint method is the method of choice since it preserves some invariants (L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT norm or energy) exactly. Hence, our goal is to propose numerical schemes that are not only uniformly accurate, but also degenerate towards a midpoint scheme for the averaged model when ε→0→𝜀0\varepsilon\to 0italic_ε → 0 . Then, we investigate how to generalize the strategy to nonlinear systems of the form U˙⁢(t)=A⁢(t/ε)⁢U⁢(t)+g⁢(t,U⁢(t)).˙𝑈𝑡𝐴𝑡𝜀𝑈𝑡𝑔𝑡𝑈𝑡\dot{U}(t)=A(t/\varepsilon)U(t)+g(t,U(t)).over˙ start_ARG italic_U end_ARG ( italic_t ) = italic_A ( italic_t / italic_ε ) italic_U ( italic_t ) + italic_g ( italic_t , italic_U ( italic_t ) ) . Even if the first order case can be easily obtained, the extension to higher order requires more attention. Indeed, substituting the Duhamel formula in the nonlinear term leads to a highly oscillatory nonlinear term in g𝑔gitalic_g which is not tractable from a numerical point of view. A suitable expansion allows the oscillatory term to be explicitly revealed, enabling the development of higher order, uniformly accurate explicit schemes in the nonlinear context. However, the extension to midpoint strategy (which usually offers invariant preserving properties) turns out to be too costly in view of Vlasov applications since it involves nonlinear fixed point techniques. To overcome this drawback, we employ the SAV technique introduced in [31] to construct efficient structure-preserving numerical methods for nonlinear problems. After reformulating the system (1)-(2) using the SAV framework, we propose an efficient uniformly accurate midpoint numerical scheme which degenerates (when ε→0→𝜀0\varepsilon\to 0italic_ε → 0) towards a midpoint scheme for the averaged model of (1)-(2). It turns out that the averaged model enjoys a Hamiltonian structure (see [12]). Hence, a midpoint (with an explicit complexity) numerical scheme is particularly relevant since it exactly preserves the discrete energy. The rest of the paper is organized as follows. In Section 2, the models we are interested in are presented, together with their asymptotic limit. In section 3, the numerical schemes are presented in a linear context: higher order explicit schemes and second order midpoint schemes are introduced and their uniform accuracy is proved. In section 4.2, we extend the numerical scheme to the nonlinear case. In Section 5, we explain how these solvers can be integrated in Particle-In-Cell framework to approximate the nonlinear Vlasov-Poisson system. Finally, several numerical results are shown to illustrate the properties of the schemes in different contexts."
https://arxiv.org/html/2411.04140v1,Bayesian inference for geophysical fluid dynamics using generative models,"Data assimilation plays a crucial role in numerical modeling, enabling the integration of real-world observations into mathematical models to enhance the accuracy and predictive capabilities of simulations. This approach is widely applied in fields such as meteorology, oceanography, and environmental science, where the dynamic nature of systems demands continuous updates to model states. However, the calibration of models in these high-dimensional, nonlinear systems poses significant challenges. In this paper, we explore a novel calibration methodology using diffusion generative models. We generate synthetic data that statistically aligns with a given set of observations (in this case the increments of the numerical approximation of a solution of a partial differential equation). This allows us to efficiently implement a model reduction and assimilate data from a reference system state modeled by a highly resolved numerical solution of the rotating shallow water equation of order 104superscript10410^{4}10 start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT degrees of freedom into a stochastic system having two orders of magnitude less degrees of freedom. To do so, the new samples are incorporated into a particle filtering methodology augmented with tempering and jittering for dynamic state estimation, a method particularly suited for handling complex and multimodal distributions. This work demonstrates how generative models can be used to improve the predictive accuracy for particle filters, providing a more computationally efficient solution for data assimilation and model calibration.","Data assimilation is a technique in numerical modeling, in which real-world data is integrated into mathematical models to improve the accuracy and the predictive capabilities of simulations. This process has a wide range of applications in fields like meteorology, oceanography, or environmental science, where the dynamic nature of systems requires continual updates to model states based on incoming observations. However, the complexity of these systems often leads to challenges in accurately calibrating models, especially when dealing with sparse or noisy data. Traditionally, model calibration involves adjusting model parameters to align with observational data, a process that can be computationally intensive and complex due to the high dimensionality and the nonlinear character of many systems. In response to these challenges, recent advancements in machine learning, particularly in the realm of generative models, seem to offer promising solutions. Diffusion generative models, known for their capacity to capture complex data distributions and generate realistic samples, present a novel approach for model calibration. Our paper explores a novel methodology where a diffusion generative model is employed to calibrate a mathematical model prior to the commencement of the data assimilation process. By leveraging the diffusion model’s ability to generate synthetic data that is statistically consistent with observed samples from high-dimensional phenomena, we achieve a more accurate and robust initial calibration of the mathematical model. This enhances the subsequent data assimilation process, leading to improved model performance and predictive accuracy. Generative models are a class of machine learning models designed to generate new data samples from an unknown distribution which is typically available only through a dataset of samples. An important class of generative models are diffusion models which gradually transform the training data into samples from a well-known distribution, such as a Gaussian distribution, using a mechanism analogous to diffusion. In this process, the forward and the backward dynamics are learnt using a neural network. Once the learning is complete, samples from the unknown distribution are obtained by running the backward diffusion initiated from samples from the Gaussian distribution. In the following, we will use the language of stochastic nonlinear filtering to explain the data assimilation methodology in general, and how it applies to the application discussed in this paper in particular. Therefore, we let X𝑋Xitalic_X and Z𝑍Zitalic_Z be two processes defined on the probability space (Ω,ℱ,ℙ)Ωℱℙ(\Omega,\mathcal{F},\mathbb{P})( roman_Ω , caligraphic_F , blackboard_P ). The process X𝑋Xitalic_X is usually called the signal process or the truth and Z𝑍Zitalic_Z is the observation process. In this paper, X𝑋Xitalic_X is the pathwise solution of a rotating shallow water system (1) approximated using a high-resolution numerical method. The pair of processes (X,Z)𝑋𝑍(X,Z)( italic_X , italic_Z ) forms the basis of the nonlinear filtering problem which consists in finding the best approximation of the posterior distribution of the signal Xtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT given the observations Z1,Z2,…,Ztsubscript𝑍1subscript𝑍2…subscript𝑍𝑡Z_{1},Z_{2},\ldots,Z_{t}italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT 111For a mathematical introduction on the subject, see e.g. [1]. For an introduction from the data assimilation perspective of the filtering problem, see [18] and [15].. The posterior distribution of the signal at time t𝑡titalic_t is denoted by πtsubscript𝜋𝑡\pi_{t}italic_π start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. We let dXsubscript𝑑𝑋d_{X}italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT be the dimension of the state space and dZsubscript𝑑𝑍d_{Z}italic_d start_POSTSUBSCRIPT italic_Z end_POSTSUBSCRIPT be the dimension of the observation space. In many real-world applications, particularly those related to weather prediction, dXsubscript𝑑𝑋d_{X}italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT is very large dX=O⁢(109)subscript𝑑𝑋𝑂superscript109d_{X}=O(10^{9})italic_d start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = italic_O ( 10 start_POSTSUPERSCRIPT 9 end_POSTSUPERSCRIPT ). Performing DA on such large dimensional models requires the use of super-computers and it is not surprising that some of the most advanced super-computers are used in meteorological offices around the world. Here we are advocating a different approach. Rather than working with the full signal Xtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, we introduce an approximate model Xtcsubscriptsuperscript𝑋𝑐𝑡X^{c}_{t}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT that is computed on a much coarser grid. In the example below, the signal is denoted by Xtfsuperscriptsubscript𝑋𝑡𝑓X_{t}^{f}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT to emphasize the fact that it evolves on a finer grid than its proxy Xtcsubscriptsuperscript𝑋𝑐𝑡X^{c}_{t}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT which is constructed on a coarser grid. Of course, Xtfsuperscriptsubscript𝑋𝑡𝑓X_{t}^{f}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT and Xtcsubscriptsuperscript𝑋𝑐𝑡X^{c}_{t}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT will behave quite differently as the small scale effects will be lost on the coarser scale. This is where generative modeling comes into play. To account for the effect of the small scale we add a stochastic term to the equation satisfied by Xtcsubscriptsuperscript𝑋𝑐𝑡X^{c}_{t}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT. The stochastic term will need to be calibrated to data obtained be recording Xtfsuperscriptsubscript𝑋𝑡𝑓X_{t}^{f}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT. The calibration needs to be done before the data assimilation is applied. The two figures below illustrate this fact: Figure 1: Filtering and Calibration. Data assimilation and model calibration. Top: The predictive distribution is advanced using a forecast model and then updated via a nonlinear operation based on observed data. Bottom: Before time 00 the forecast model is calibrated. Filtering begins strictly after the calibration process is finshed. We need to make an important remark here: The calibration methodology requires full and accurate knowledge of the signal, more precisely its projection on the coarse grid. For synthetic data this can be easily achieved by running the model equation on a sufficiently large time interval and on the fine grid and then projecting it on the coarser grid. For real data, one would need to use the so-called re-analysis data. Such data is freely available, for example, from ECMWF, e.g. ERA5 data (see e.g. [11]). In [7] a generative model approach is used to approximate the posterior distribution directly, but in a procedure which must be performed offline. Repeating this iteratively would involve huge computational costs. In this work, we do not replace the two data assimilation steps (forecast and assimilation/update) using a generative model methodology, but rather use a diffusion model methodology to calibrate the signal before starting the data assimilation process. Let us explain briefly the methodology for the incorporation of the incomplete data. We will use particle filters. These are a class of filtering techniques which are adapted particularly to handle complex and multimodal distributions. Unlike traditional approaches, such as the Kalman filter, which assume linearity of the model and Gaussian noise, particle filters use a set of random samples/scenarios, or particles to represent the posterior distribution of a system’s state. Each particle carries a weight that reflects how well it fits the observed data. As new observations are assimilated, these particles are propagated through the model, and their weights are updated according to the likelihood of the observed data given the particle’s state: the particles that better match the observations are given higher weights, while those that are less accurate are down-weighted or discarded. Further details regarding this methodology can be found in [14] and in section 3.3 below. Figure 2: Classical Particle Filter. The resampling procedure ensures that particles with low weights are replaced with particles with higher weights. Following the resampling, an ensemble of equal-weighted particles is obtained. In high-dimensional spaces: one particle gains a weight close to one while all the others have weights close to zero and are discarded."
https://arxiv.org/html/2411.04686v1,Precision-Aware Iterative Algorithms Based on Group-Shared Exponents of Floating-Point Numbers,"Iterative solvers are frequently used in scientific applications and engineering computations. However, the memory-bound Sparse Matrix-Vector (SpMV) kernel computation hinders the efficiency of iterative algorithms. As modern hardware increasingly supports low-precision computation, the mixed-precision optimization of iterative algorithms has garnered widespread attention. Nevertheless, existing mixed-precision methods pose challenges, including format conversion overhead, tight coupling between storage and computation representation, and the need to store multiple precision copies of data. This paper proposes a floating-point representation based on the group-shared exponent and segmented storage of the mantissa, enabling higher bit utilization of the representation vector and fast switches between different precisions without needing multiple data copies. Furthermore, a stepped mixed-precision iterative algorithm is proposed. Our experimental results demonstrate that, compared with existing floating-point formats, our approach significantly improves iterative algorithms’ performance and convergence residuals.","Numerical simulations of many practical problems often rely on solving the linear equation system 𝑨⁢𝒙=𝒃𝑨𝒙𝒃\bm{A}\bm{x}=\bm{b}bold_italic_A bold_italic_x = bold_italic_b, where 𝑨𝑨\bm{A}bold_italic_A is an m×n𝑚𝑛m\times nitalic_m × italic_n sparse matrix, and 𝒙𝒙\bm{x}bold_italic_x and 𝒃𝒃\bm{b}bold_italic_b are dense vectors of sizes n𝑛nitalic_n and m𝑚mitalic_m, respectively. Iterative solvers such as conjugate gradient (CG) and generalized minimal residual (GMRES) methods are commonly used in practice. Sparse matrix-vector multiplication (SpMV) is a frequently used computational kernel in these algorithms, but it is also their primary performance bottleneck. The computational efficiency of SpMV is constrained by the memory access efficiency, making it a typical memory-bound kernel. With the continuous development of artificial intelligence technology, many computer hardware manufacturers have begun to support multiple precision data representations in their latest products. For example, NVIDIA’s latest H100 supports TF32, BF16, FP16, FP8 (E4M3 and E5M2), among others. This has inspired researchers to use mixed-precision techniques to optimize iterative algorithms. Compared to traditional iterative algorithms, which typically use a single floating-point format like FP64 or FP32, mixed-precision optimization introduces multiple-precision floating-point representations in iterative algorithms, reducing memory access and computational overhead, thereby improving the efficiency of iterative solving algorithms. As a key kernel of iterative algorithms, mixed-precision optimization of SpMV has attracted the attention of researchers [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The main idea behind these efforts is to select different-precision floating-point representations for non-zero elements (non-zeros) based on their magnitudes. For example, smaller values are stored in the single-precision floating-point format, while larger values are stored in the double-precision format, with both computations performed in double precision. In terms of mixed-precision optimization for iterative algorithms, current work mainly focuses on using different-precision floating-point representations in different statements or iterations [11, 12, 13, 14, 15]. Existing mixed-precision algorithms have improved the computational efficiency of iterative algorithms. However, using general-purpose floating-point representations introduces several challenges: (1) Lower utilization of binary bits: FP64 and FP32 have 11 and 8 exponent bits, respectively, and are two widely used floating-point representations. However, in specific applications, floating-point data may have particular numerical distributions, such as a small distribution range or most data being distributed around some points. In this case, the exponent field of the existing floating-point representations has a low binary bits utilization. (2) Tight coupling between storage and computation: In systems based on traditional floating-point formats, the precision used for storage is tightly coupled with the precision for computation, which sacrifices flexibility and limits the mixed-precision optimization of iterative algorithms. (a) Information entropy (b) Top-1 (c) Top-2 (d) Top-4 (e) Top-8 (f) Top-16 (g) Top-32 (h) Top-64 Figure 1: Non-zeros value distribution of sparse matrices. (a) Information entropy of non-zeros values, exponents, and mantissa. (b)-(h) Ratios of non-zeros corresponding to the top-k𝑘kitalic_k exponents. Based on the observations above and the observation that most non-zeros in a sparse matrix exhibit close or same exponents, this paper introduces an innovative mixed-precision iterative algorithm based on group-shared exponents. The core idea lies in the extraction and separate storage of shared exponents for a collection of floating-point numbers, thus freeing exponent bits for additional mantissa storage. This method boosts computational efficiency and improves the convergence of iterative computations with lower precision. This approach is practical because the separately stored shared exponents can be represented with more binary bits, preventing data overflow in computations. Moreover, by dedicating all bits, aside from the sign bit, to the mantissa representation, this approach improves the precision of data representation under bit-width constraints. The primary contributions of this paper include: • A new floating-point format is proposed, based on group-shared exponents and segmented storage of the tail, which eliminates the need for storing multiple copies of the same data in different precisions. • A stepped mixed-precision iterative algorithm optimization method based on the group-shared exponents’ floating-point format is proposed, which maintains the convergence property of the iterative algorithm while reducing the SpMV memory footprint overhead. • The optimization method for the mixed-precision iterative algorithm introduced in this paper has been validated on a GPU platform. Experimental results demonstrate that our proposed method improves the solving efficiency of the iterative algorithm and achieves similar residuals to high-precision iterative solvers. The remainder of this paper is organized as follows. Section II analyzes the numerical distribution of sparse matrices used in iterative algorithms. Section III describes our proposed new floating-point format and stepped mixed-precision iterative algorithm. Section IV evaluates the performance of the proposed approach. Section V discusses related work, and Section VI concludes the paper."
https://arxiv.org/html/2411.04352v1,The High-Order Magnetic Near-Axis Expansion: Ill-Posedness and Regularization,"When analyzing stellarator configurations, it is common to perform an asymptotic expansion about the magnetic axis. This so-called near-axis expansion is convenient for the same reason asymptotic expansions often are, namely, it reduces the dimension of the problem. This leads to convenient and quickly computed expressions of physical quantities, such as quasisymmetry and stability criteria, which can be used to gain further insight. However, it has been repeatedly found that the expansion diverges at high orders, limiting the physics the expansion can describe. In this paper, we show that the near-axis expansion diverges in vacuum due to ill-posedness and that it can be regularized to improve its convergence. Then, using realistic stellarator coil sets, we show that the near-axis expansion can converge to ninth order in the magnetic field, giving accurate high-order corrections to the computation of flux surfaces. We numerically find that the regularization improves the solutions of the near-axis expansion under perturbation, and we demonstrate that the radius of convergence of the vacuum near-axis expansion is correlated with the distance from the axis to the coils.","The design of stellarators is a computationally intensive task. The most basic problem in stellarator design – that of computing the magnetic field – requires solving the steady-state magnetohydrostatics equations (MHS). These equations are difficult to solve for reasons familiar to many problems in physics: they are nonlinear and three-dimensional. Popular MHS equilibrium solvers include VMEC (Hirshman, 1983), DESC (Dudt & Kolemen, 2020), and SPEC (Hudson et al., 2012), all of which take on the order of seconds to minutes to compute a single equilibrium. Beyond equilibrium solving, there are potentially many other stellarator objectives that are expensive to compute, with plasma stability metrics being a major example. When optimizing for stellarators, the costs of equilibrium and objective solving can limit the speed of the overall design process. This, in combination with the high dimensionality of specifying 3D fields, motivates a need for simpler alternatives. Recently, the near-axis expansion (Mercier, 1964; Solov’ev & Shafranov, 1970) has gained traction as an alternative to full 3D MHS solvers. The near-axis expansion works by asymptotically expanding all of the relevant plasma variables (such as magnetic coordinates, pressure, rotational transform, and plasma current) in the distance from the magnetic axis, which is assumed to be small relative to a characteristic magnetic scale length. The resulting equations are a hierarchy of one-dimensional ODEs, which can be solved orders of magnitude faster than 3D equilibria. This allows for one to quickly find large numbers of optimized stellarators (Landreman, 2022; Giuliani, 2024), something that was previously unavailable to the stellarator community. In addition to the speed, the near-axis expansion has other benefits. For instance, in Garren & Boozer (1991) it was shown that quasisymmetry imposes more constraints than free parameters in the expansion, leading to the conjecture that non-axisymmetric but perfectly quasisymmetric stellarators cannot exist. Many objectives have been defined and computed for the near-axis expansion, including quasisymmetry (Landreman & Sengupta, 2019), quasi-isodynamicity (Mata et al., 2022), isoprominence (Burby et al., 2023), and Mercier and magnetic-well conditions for stability (Landreman & Jorge, 2020; Kim et al., 2021). There is evidence that other higher-order effects such as ballooning and linear gyrokinetic stability could be investigated as well (Jorge & Landreman, 2020). The near-axis expansion has also been combined with a type of quadratic flux minimizing surfaces and coil optimization to create free-field optimized QA equilibria (Giuliani, 2024). In sum, the connection between easily expressed objectives, a relatively low-dimensional equilibrium description, and fast computation has led to the increased use axis expansion. However, the near-axis expansion is not without drawbacks. The primary drawback is fundamental: the expansion has limited accuracy far from the axis. For instance, in the “far-axis” regime, there can be large errors in the magnetic shear and magnetic surfaces can self-intersect (Landreman, 2022). The paper by Jorge & Landreman (2020) also indicates that higher-order terms may be needed for stability; such as magnetic curvature terms. Unfortunately, attempts to use higher-order terms have resulted in divergent asymptotic series, limiting the accuracy to small plasma volumes. Most series go to first, second, or sometimes third order in the distance from the axis in the relevant quantities, with any more terms typically reducing accuracy rather than improving it. Therefore, if we want to include more physics objectives over larger volumes in the near-axis expansion, we must overcome the issue of series divergence. Unfortunately, the issue of divergence is confounded by many of the assumptions that can be incorporated into the near-axis expansion. The most extreme case is that of QA stellarators, where it has been shown that the system of equations for QA is overdetermined beyond third order in the expansion. Obviously, unless one relaxes the problem (e.g. via anisotropic pressure; Rodríguez & Bhattacharjee, 2021), one cannot generally ask for a convergent QA near-axis expansion in such a circumstance. In the simpler case of non-quasisymmetric stellarators with smooth pressure gradients and nested surfaces, it is still unknown whether there are non-axisymmetric solutions to MHS (Grad, 1967; Constantin et al., 2021a). Recent work has found that perturbing for small force (Constantin et al., 2021b) or non-flat metrics (Cardona et al., 2023) allow for integrable solutions, but currently, there is no guarantee of solutions of MHS, let alone convergent asymptotic expansions. So, to begin the task of building convergent numerical methods for the near-axis expansion, we focus on a problem we know is solvable: Laplace’s equation for a vacuum magnetic potentials following Jorge et al. (2020). This can be solved in direct (Mercier) coordinates (Mercier, 1964) with no assumption of nested surfaces. Additionally, because solutions of Laplace’s equation are real analytic, there exist near-axis expansions of the equation that converge within a neighborhood of the axis. Despite these guarantees, even the near-axis expansion of Laplace’s equation diverges. In this paper, we show that the vacuum near-axis expansion diverges for a reason: namely that Laplace’s equation as a near-axis expansion is ill-posed (§3, following background in §2). To address this issue, we introduce a small regularization term to Laplace’s equation and expand to find a regularized near-axis expansion. We do this by including a viscosity term to Laplace’s equation that damps the highly oscillatory unstable modes responsible for the ill-posedness. By appropriately bounding the input of the near-axis expansion in a Sobolev norm, we prove that this term results in a uniformly converging near-axis expansion within a neighborhood of the axis. Following the theory, we describe a pseudo-spectral method for finding solutions to the near-axis expansion to arbitrary order in §4. In §5, we use the numerical method to show two examples of high-order near-axis expansions: the rotating ellipse and Landreman-Paul (Landreman & Paul, 2022). We find that the near-axis expansion magnetic field, rotational transform, and magnetic surfaces can converge accurately near the axis for unperturbed initial data. The region of convergence is observed to be dictated by the distance from the magnetic axis to the coils. Then, by perturbing the on-axis inputs, we show that the regularized expansion obeys Laplace’s more accurately farther from the axis. Finally, we conclude in §6."
https://arxiv.org/html/2411.04026v1,Space-Time Spectral Element Tensor Network Approach for Time Dependent Convection Diffusion Reaction Equation with Variable Coefficients,"In this paper, we present a new space-time Petrov-Galerkin-like method. This method utilizes a mixed formulation of Tensor Train (TT) and Quantized Tensor Train (QTT), designed for the spectral element discretization (Q1-SEM) of the time-dependent convection-diffusion-reaction (CDR) equation. We reformulate the assembly process of the spectral element discretized CDR to enhance its compatibility with tensor operations and introduce a low-rank tensor structure for the spectral element operators. Recognizing the banded structure inherent in the spectral element framework’s discrete operators, we further exploit the QTT format of the CDR to achieve greater speed and compression. Additionally, we present a comprehensive approach for integrating variable coefficients of CDR into the global discrete operators within the TT/QTT framework. The effectiveness of the proposed method, in terms of memory efficiency and computational complexity, is demonstrated through a series of numerical experiments, including a semi-linear example.","1.1 Model Time-dependent 3D Convection-Reaction-Diffusion Problem This paper develops a space-time spectral elements method for tensor network low-rank numerical solution of the time-dependent 3D convection diffusion reaction (CDR) equation with Dirichlet boundary conditions of the type, ∂u∂t−∇⋅(κ⁢(t,𝐱)⁢∇u)+𝐛⁢(t,𝐱)⋅∇u+c⁢(t,𝐱)⁢u=f⁢(t,𝐱)⁢in⁢[0,T]×Ω,u=g⁢(t,𝐱)on⁢[0,T]×∂Ω,u⁢(t0=0,𝐱)=u0⁢(𝐱)in⁢Ω,formulae-sequence𝑢𝑡⋅∇𝜅𝑡𝐱∇𝑢⋅𝐛𝑡𝐱∇𝑢𝑐𝑡𝐱𝑢𝑓𝑡𝐱in0𝑇Ωformulae-sequence𝑢𝑔𝑡𝐱on0𝑇Ω𝑢subscript𝑡00𝐱subscript𝑢0𝐱inΩ\begin{split}\frac{\partial u}{\partial t}-\nabla\cdot\Big{(}{\kappa}(t,% \mathbf{x})\nabla u\Big{)}+\mathbf{b}(t,\mathbf{x})\cdot\nabla u+c(t,\mathbf{x% })u&=f(t,\mathbf{x})\ \text{in}\ [0,T]\times\Omega,\\ u&=g(t,\mathbf{x})\quad\text{on}\ [0,T]\times\partial\Omega,\\ u(t_{0}=0,\mathbf{x})&=u_{0}(\mathbf{x})\quad\text{in}\ \Omega,\end{split}start_ROW start_CELL divide start_ARG ∂ italic_u end_ARG start_ARG ∂ italic_t end_ARG - ∇ ⋅ ( italic_κ ( italic_t , bold_x ) ∇ italic_u ) + bold_b ( italic_t , bold_x ) ⋅ ∇ italic_u + italic_c ( italic_t , bold_x ) italic_u end_CELL start_CELL = italic_f ( italic_t , bold_x ) in [ 0 , italic_T ] × roman_Ω , end_CELL end_ROW start_ROW start_CELL italic_u end_CELL start_CELL = italic_g ( italic_t , bold_x ) on [ 0 , italic_T ] × ∂ roman_Ω , end_CELL end_ROW start_ROW start_CELL italic_u ( italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0 , bold_x ) end_CELL start_CELL = italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_x ) in roman_Ω , end_CELL end_ROW (1.1) where κ⁢(t,𝐱)𝜅𝑡𝐱{\kappa}(t,\mathbf{x})italic_κ ( italic_t , bold_x ), 𝐛⁢(t,𝕩):=[b1⁢(t,𝕩),b2⁢(t,𝕩),b3⁢(t,𝕩)]Tassign𝐛𝑡𝕩superscriptsubscript𝑏1𝑡𝕩subscript𝑏2𝑡𝕩subscript𝑏3𝑡𝕩𝑇\mathbf{b}(t,\mathbb{x}):=[b_{1}(t,\mathbb{x}),b_{2}(t,\mathbb{x}),b_{3}(t,% \mathbb{x})]^{T}bold_b ( italic_t , blackboard_x ) := [ italic_b start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_t , blackboard_x ) , italic_b start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_t , blackboard_x ) , italic_b start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( italic_t , blackboard_x ) ] start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT, and c⁢(t,𝐱)𝑐𝑡𝐱c(t,\mathbf{x})italic_c ( italic_t , bold_x ) are the nonlinear coefficients, and 𝐱=(x,y,z)𝐱𝑥𝑦𝑧\mathbf{x}=(x,y,z)bold_x = ( italic_x , italic_y , italic_z ). The computational domain in space Ω=ΩZ×ΩY×ΩX⊂ℝ3ΩsubscriptΩ𝑍subscriptΩ𝑌subscriptΩ𝑋superscriptℝ3\Omega=\Omega_{Z}\times\Omega_{Y}\times\Omega_{X}\subset\mathbb{R}^{3}roman_Ω = roman_Ω start_POSTSUBSCRIPT italic_Z end_POSTSUBSCRIPT × roman_Ω start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT × roman_Ω start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ⊂ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT is a three dimensional cube which is a Cartesian product of three intervals, and T𝑇Titalic_T is the final time-point. We have considered inhomogeneous boundary condition and initially the tensor-train space-time formats are developed for coefficients that are constants or allow separation of variables, and later extended for a general type of variable coefficients. 1.2 Classical Methods for Solving CDR Spectral element method (SEM) is a powerful technique for solving CDR with high accuracy [40]. By combining the geometric flexibility of finite elements with the exponential convergence of spectral methods, SEM can achieve excellent resolution of complex solutions [8]. Discontinuous Galerkin [41] and Petrov-Galerkin [7] methods are widely used for solving CDR, particularly in finite element and spectral methods [15]. These methods involve approximating the PDE solution by projecting it onto a finite-dimensional subspace of trial functions. The key idea is to ensure that the residual error is orthogonal to the chosen subspace of test functions. In the Galerkin approach, the continuous problem is reformulated into a discrete problem by selecting a finite number of trial functions to approximate the solution. These functions are typically chosen from a function space like polynomials or piecewise-defined functions. In the classical (Bubnov-)Galerkin method, the same set of functions is used for both the trial (approximation of the solution) and the test (weighting) functions, which ensures that the residual error is orthogonal to the space spanned by the trial functions. The PDE is transformed into its weak (integral) form by multiplying it by a test function and integrating over the domain. The weak form allows handling problems with lower regularity solutions and dealing with complex geometries. The Petrov-Galerkin (PG) method generalizes the Bubnov-Galerkin method by allowing test and trial spaces to differ. This added flexibility allows for improved stability and accuracy, especially for convection-dominated problems or where numerical instability is a concern. By selecting appropriate test functions, the Petrov-Galerkin method can introduce stabilization mechanisms, such as upwinding in the case of convection-dominated problems and reducing oscillations or instabilities. In the present work, we do not apply such stabilization mechanisms since our underlying problem is a diffusion-dominated problem. In fact, we employ the same discrete basis for both test and trial functions. Our method is following the Petrov-Galerkin framework because the underlying continuous function spaces differ, specifically in the norms employed; our stability analysis depends on this difference, and it is based on inf-sup condition [5, 43]. Space-time discretizations treat the spatial and temporal domains simultaneously with the same methodology, whereas time-marching schemes discretize the spatial domain with one methodology e.g., FEM, Nonconforming FEM, and recently introduced virtual element method(VEM), and employ a distinct time-stepping scheme (such as backward Euler) for the temporal discretization. Space-time methods usually demand more resources, especially in the memory required for the representation of the solution across a time slab, but they allow more flexibility in the temporal discretization, including the possibility of spatially-localized refinement in time. Space-time methods can also be easier to analyze and may have higher accuracy than a corresponding time-marching scheme. Our current numerical examples serve as a proof of concept for the new tensor method. In future work, we aim to tackle more numerically challenging scenarios where stability may be a concern. 1.3 Mitigating the Curse of Dimensionality The CDR equations model a wide range of phenomena in physics, chemistry, and engineering, which often demand substantial computational resources due to the need for fine spatial and temporal discretizations. Traditional approaches to solving such problems often lead to prohibitively large systems of equations, making them impractical for many real-world applications, since the number of grid points grows exponentially with the number of dimensions. In real-life applications modeled by time-dependent PDEs, such as full waveform inversion problems, the number of grid points required for solving the PDE can be as large as M=6.6×1010𝑀6.6superscript1010M=6.6\times 10^{10}italic_M = 6.6 × 10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT per time step. With a large number of time steps N=4×105𝑁4superscript105N=4\times 10^{5}italic_N = 4 × 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT , one must store a total of M⁢N𝑀𝑁MNitalic_M italic_N floating point numbers [24]. This phenomenon is known as the curse of dimensionality [4]. The curse of dimensionality leads to poor computational scaling in numerical algorithms and poses the main challenge in multidimensional numerical computations, irrespective of the specific problem. Notably, even exascale high-performance computing, with its optimization strategies, cannot overcome the curse of dimensionality. This phenomenon, forces algorithm developers to make hard choices—either to reduce the fidelity of the model using, e.g., reduced-order models (ROMs) or to repeat computation using checkpointing type strategies. ROMs focus on creating a reduced form of the PDE operator and are an excellent solution in some cases. However, they are often tailored for specific models and demand significant domain expertise. Furthermore, ROMs can be incompatible with legacy codes as they often require invasive modification of the simulation software. ROM development is an active area of research and can work well for solving certain classes of problems [26, 19]. As an alternative, tensor network techniques have shown promise in alleviating the curse of dimensionality associated with high-dimensional problems. Among these, the tensor train (TT) format, introduced by Oseledets and Tyrtyshnikov [30], has gained particular attention due to its ability to represent high-dimensional data efficiently while maintaining computational tractability. Here we present a novel approach that combines the high accuracy of space-time spectral element methods with the computational efficiency of TT decomposition for solving time-dependent CDR equations in three spatial dimensions. Our method exploits the inherent low-rank structure often present in the solutions of such equations to dramatically reduce the computational complexity and storage requirements. Moreover, given that the spectral element discrete operators have banded structures, we further exploit the Quantized Tensor Train (QTT) format for more economical and efficient solvers [23]. Specifically, we develop a mixed TT/QTT decomposition of the four-dimensional (three space plus time) space-time spectral element Petrov-Galerkin discretization, and demonstrate how to construct the TT and QTT representations of the spectral element discretization. Our results show that this TT/QTT-SEM-PG approach can achieve high accuracy while dramatically reducing the computational resources required, making it possible to solve previously intractable problems in CDR systems. The remainder of this paper is organized as follows. In Section 2, we derive the weak formulation of (1.1), and review some basic concepts for space-time spectral element methods, and introduce the discretization and its matrix formulation. Section 5 describes the assembly process that leads to the global linear system. We have reformulated this process to be more compatible with tensor operations. This reformulation is particularly important as it simplifies and optimizes the tensorization process. In Section 6, we present our mixed TT/QTT design of the numerical solution of the CDR equation and introduce our algorithms in TT/QTT format. In Section 7, we present our numerical results and assess the performance of our method. In Section 8, we offer our final remarks and discusses possible future work. Additional details on the CDR tensorization approach are provided in the appendices."
https://arxiv.org/html/2411.03607v1,Upper bound of high-order derivatives for Wachspress coordinates on polytopes††thanks:This work was supported by the National Natural Science Foundation of China under Grant No. 12171244.,"The gradient bounds [19, 27, 14] of generalized barycentric coordinates play an essential role in the H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm approximation error estimate of generalized barycentric interpolations. Similarly, the Hksuperscript𝐻𝑘H^{k}italic_H start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT norm, k>1𝑘1k>1italic_k > 1, estimate needs upper bounds of high-order derivatives, which are not available in the literature. In this paper, we derive such upper bounds for the Wachspress generalized barycentric coordinates on simple convex d𝑑ditalic_d-dimensional polytopes, d≥1𝑑1d\geq 1italic_d ≥ 1. The result can be used to prove optimal convergence for Wachspress-based polytopal finite element approximation of, for example, fourth-order elliptic equations.Another contribution of this paper is to compare various shape-regularity conditions for simple convex polytopes, and to clarify their relations using knowledge from convex geometry.","Generalized barycentric coordinates (GBCs) [13, 22, 36] have been used to construct various finite element methods (FEMs) on polytopal meshes [17, 18, 24, 25, 29, 30, 31, 32, 39]. Following the standard FEM theory [2, 6], FEM approximation error analysis relies on the interpolation error analysis. For second-order elliptic equations, Gillette, Rand and Bajaj [19] have shown that the H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm GBC interpolation error estimate can be derived using the gradient bounds of GBCs. They also gave the gradient bounds of three GBCs: the Wachspress coordinates, the Sibson coordinates and the harmonic coordinates, in two dimensions. The same authors then proved the gradient bound of the mean value coordinates in [27]. Later, Floater, Gillette and Sukumar [14] re-investigated the Wachspress coordinates, where they extended the gradient bound in [19] from 2222D planar polygons to simple convex d𝑑ditalic_d-dimensional polytopes. These results helped to establish the approximation error estimate of GBC-based FEM for second-order elliptic equations. With the development of high-order GBC-based polygonal elements [28, 16, 5], it is possible to construct GBC-based FEM for fourth-order or even higher order elliptic partial differential equations. A Morley-type GBC-based non-conforming FEM on polygonal meshes [34] has been developed for solving fourth-order plate bending problems. Its FEM approximation error analysis relies on the interpolation error analysis, which in turn relies on the upper bounds of second-order derivatives of GBCs. Such upper bounds were not available by the time when [34] was completed, which has left a gap in the theoretical error analysis. This motivates the current work. In this paper, we derive upper bounds of arbitrary order derivatives for Wachspress coordinates on simple convex d𝑑ditalic_d-dimensional polytopes. The result can be used to prove optimal convergence for Wachspress-based polytopal finite element approximation of fourth-order elliptic equations. Moreover, through this work, we hope to reveal more properties of the GBCs, which have found interesting applications in computer graphics and computational mechanics [22]. To derive the upper bounds, one may naturally think of extending the analysis of the gradient bound in [14] to high-order derivatives. Unfortunately, this turns out to be almost impossible, as the analysis in [14] is tailored to retrieve a sharp gradient bound, and thus highly technical. We propose different techniques to reach the goal. Similar to [14], our upper bounds depend on the diameter hKsubscriptℎ𝐾h_{K}italic_h start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT of polytope K𝐾Kitalic_K, and the minimal distance h∗subscriptℎh_{*}italic_h start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT between vertices and non-incident facets ((d−1)𝑑1(d-1)( italic_d - 1 )-dimensional faces) of K𝐾Kitalic_K. The bounds are sharp in asymptotic orders when h∗=O⁢(hK)subscriptℎ𝑂subscriptℎ𝐾h_{*}=O(h_{K})italic_h start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT = italic_O ( italic_h start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ), but not when h∗≪hKmuch-less-thansubscriptℎsubscriptℎ𝐾h_{*}\ll h_{K}italic_h start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT ≪ italic_h start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT. We shall compare various shape regularity assumptions for simple convex polytopes, and show that h∗=O⁢(hK)subscriptℎ𝑂subscriptℎ𝐾h_{*}=O(h_{K})italic_h start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT = italic_O ( italic_h start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) is a practically reasonable assumption in applications such as FEM. The paper is organized as follows. In Section 2, we describe the Wachspress coordinates in detail, and present some preliminary tools. In Section 3, we derive the upper bounds of arbitrary-order derivatives for Wachspress coordinates. In Section 4, we discuss the relation of geometric conditions for simple convex polytopes. Supporting numerical results, including an application on solving fourth-order plate bending problems, are presented in Section 5."
https://arxiv.org/html/2411.03534v1,"Spectral Transformation for the Dense
Symmetric Semidefinite Generalized Eigenvalue Problem","The spectral transformation Lanczos method for the sparse symmetric definite generalized eigenvalue problem for matrices A𝐴Aitalic_A and B𝐵Bitalic_B is an iterative method that addresses the case of semidefinite or ill conditioned B𝐵Bitalic_B using a shifted and inverted formulation of the problem. This paper proposes the same approach for dense problems and shows that with a shift chosen in accordance with certain constraints, the algorithm can conditionally ensure that every computed shifted and inverted eigenvalue is close to the exact shifted and inverted eigenvalue of a pair of matrices close to A𝐴Aitalic_A and B𝐵Bitalic_B. Under the same assumptions on the shift, the analysis of the algorithm for the shifted and inverted problem leads to useful error bounds for the original problem, including a bound that shows how a single shift that is of moderate size in a scaled sense can be chosen so that every computed generalized eigenvalue corresponds to a generalized eigenvalue of a pair of matrices close to A𝐴Aitalic_A and B𝐵Bitalic_B. The computed generalized eigenvectors give a relative residual that depends on the distance between the corresponding generalized eigenvalue and the shift. If the shift is of moderate size, then relative residuals are small for generalized eigenvalues that are not much larger than the shift. Larger shifts give small relative residuals for generalized eigenvalues that are not much larger or smaller than the shift.","The symmetric semidefinite generalized eigenvalue problem A⁢𝒗=λ⁢B⁢𝒗,𝒗≠𝟎formulae-sequence𝐴𝒗𝜆𝐵𝒗𝒗0A{\bm{v}}=\lambda B{\bm{v}},\qquad{\bm{v}}\neq{\bm{0}}italic_A bold_italic_v = italic_λ italic_B bold_italic_v , bold_italic_v ≠ bold_0 (1) for n×n𝑛𝑛n\times nitalic_n × italic_n symmetric A𝐴Aitalic_A and symmetric positive semidefinite or positive definite B𝐵Bitalic_B arises commonly in a number of application areas, notably in structural engineering, and has a long history in the research literature on numerical linear algebra. When B𝐵Bitalic_B is positive definite, the most commonly used algorithm for the dense problem is based on Cholesky factorization of B𝐵Bitalic_B. It is described in [22] and implemented in the LAPACK routine xSYGV [1]. However, since it involves forming a matrix from the possibly ill conditioned Cholesky factor, very little can be proven about its stability. In practice, it typically delivers small relative residuals for eigenvalues that are large in magnitude and larger relative residuals for small eigenvalues. The purpose of this paper is to describe a related algorithm that, instead of solving systems with the Cholesky factor of B𝐵Bitalic_B, solves systems using a symmetric factor of the shifted matrix A−σ⁢B𝐴𝜎𝐵A-\sigma Bitalic_A - italic_σ italic_B. With an appropriately chosen shift, the algorithm is amenable to error analysis and we provide a mixed forward/backward error bounds for a matrix decomposition for the shifted and inverted problem. This error analysis also provides useful residual bounds for the original problem (1). In the remainder of the introduction, we establish notation and survey existing methods. We assume here that A𝐴Aitalic_A and B𝐵Bitalic_B are real and symmetric, but the problem is not substantially different if they are complex and Hermitian. It is necessary to assume that 𝒩⁢(A)∩𝒩⁢(B)={𝟎},𝒩𝐴𝒩𝐵0{\cal N}(A)\cap{\cal N}(B)=\left\{{\bm{0}}\right\},caligraphic_N ( italic_A ) ∩ caligraphic_N ( italic_B ) = { bold_0 } , where 𝒩⁢(A)𝒩𝐴{\cal N}(A)caligraphic_N ( italic_A ) denotes the null space of A𝐴Aitalic_A, since otherwise every choice of λ𝜆\lambdaitalic_λ is a solution of (1) for some choice of 𝒗𝒗{\bm{v}}bold_italic_v. If 𝒗𝒗{\bm{v}}bold_italic_v is a null vector of B𝐵Bitalic_B, then we identify it as a generalized eigenvector for the generalized eigenvalue λ=∞𝜆\lambda=\inftyitalic_λ = ∞. We also assume throughout the paper that A≠0𝐴0A\neq 0italic_A ≠ 0 and B≠0𝐵0B\neq 0italic_B ≠ 0. Problem (1) can be reformulated as β⁢A⁢𝒗=α⁢B⁢𝒗,𝒗≠𝟎,formulae-sequence𝛽𝐴𝒗𝛼𝐵𝒗𝒗0\beta A{\bm{v}}=\alpha B{\bm{v}},\qquad{\bm{v}}\neq{\bm{0}},italic_β italic_A bold_italic_v = italic_α italic_B bold_italic_v , bold_italic_v ≠ bold_0 , (2) which puts A𝐴Aitalic_A and B𝐵Bitalic_B on a more equal footing, although we still require that B𝐵Bitalic_B specifically be semidefinite. In this formulation we refer to the pair (α,β)𝛼𝛽(\alpha,\beta)( italic_α , italic_β ) as a generalized eigenvalue for the matrix pair (A,B)𝐴𝐵(A,B)( italic_A , italic_B ). The generalized eigenvalues from (1) are given by λ=α/β𝜆𝛼𝛽\lambda=\alpha/\betaitalic_λ = italic_α / italic_β, with infinite generalized eigenvalues having the form (α,0)𝛼0(\alpha,0)( italic_α , 0 ). We favor (2) in the algorithm and in the final error bounds, but switch between (1) and (2) whenever it is convenient. Where the context is clear we omit “generalized” and refer simply to eigenvalues and eigenvectors. The symmetric definite generalized eigenvalue problem is known to have real eigenvalues. Applying a congruence to both A𝐴Aitalic_A and B𝐵Bitalic_B preserves both eigenvalues and symmetry. Given a diagonalizing congruence for which VT⁢A⁢V=DαandVT⁢B⁢V=Dβ,formulae-sequencesuperscript𝑉T𝐴𝑉subscript𝐷𝛼andsuperscript𝑉T𝐵𝑉subscript𝐷𝛽V^{\rm T}AV=D_{\alpha}\qquad\mbox{and}\qquad V^{\rm T}BV=D_{\beta},italic_V start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT italic_A italic_V = italic_D start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT and italic_V start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT italic_B italic_V = italic_D start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT , the eigenvalues λ𝜆\lambdaitalic_λ are the diagonal elements of Dβ−t⁢Dαsuperscriptsubscript𝐷𝛽𝑡subscript𝐷𝛼D_{\beta}^{-t}D_{\alpha}italic_D start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - italic_t end_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT and the eigenvectors are the columns of V𝑉Vitalic_V. Such a congruence can be shown to exist [18, 20] with a somewhat weaker assumption than positive definiteness of B𝐵Bitalic_B [10, 15], although positive semidefiniteness of B𝐵Bitalic_B is not by itself sufficient. Unfortunately, even when it exists, the diagonalizing congruence will not in general be orthogonal, which means that we cannot expect to compute such a decomposition through the stable application of rotations or reflectors. The most commonly used method for positive definite B𝐵Bitalic_B computes a diagonalizing congruence starting with the Cholesky factorization B=Cb⁢CbT𝐵subscript𝐶𝑏superscriptsubscript𝐶𝑏TB=C_{b}C_{b}^{\rm T}italic_B = italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT. We can then compute an eigenvalue decomposition Cb−T⁢A⁢Cb−1=U⁢Λ⁢UTsuperscriptsubscript𝐶𝑏T𝐴superscriptsubscript𝐶𝑏1𝑈Λsuperscript𝑈TC_{b}^{-{\rm T}}AC_{b}^{-1}=U\Lambda U^{\rm T}italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - roman_T end_POSTSUPERSCRIPT italic_A italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = italic_U roman_Λ italic_U start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT so that we have congruences UT⁢Cb−1⁢A⁢Cb−T⁢U=Λ,andUT⁢Cb−1⁢B⁢Cb−T⁢U=I.formulae-sequencesuperscript𝑈Tsuperscriptsubscript𝐶𝑏1𝐴superscriptsubscript𝐶𝑏T𝑈Λandsuperscript𝑈Tsuperscriptsubscript𝐶𝑏1𝐵superscriptsubscript𝐶𝑏T𝑈𝐼U^{\rm T}C_{b}^{-1}AC_{b}^{-{\rm T}}U=\Lambda,\qquad\mbox{and}\qquad U^{\rm T}% C_{b}^{-1}BC_{b}^{-{\rm T}}U=I.italic_U start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_A italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - roman_T end_POSTSUPERSCRIPT italic_U = roman_Λ , and italic_U start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_B italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - roman_T end_POSTSUPERSCRIPT italic_U = italic_I . The eigenvectors are then the columns of V=Cb−T⁢U𝑉superscriptsubscript𝐶𝑏T𝑈V=C_{b}^{-{\rm T}}Uitalic_V = italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - roman_T end_POSTSUPERSCRIPT italic_U. The apparent inverses, and all references to applying inverses in this paper, are short-hand for solving linear systems in an appropriately stable way. There are some nontrivial aspects to exploiting symmetry in forming Cb−T⁢A⁢Cbsuperscriptsubscript𝐶𝑏T𝐴subscript𝐶𝑏C_{b}^{-{\rm T}}AC_{b}italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - roman_T end_POSTSUPERSCRIPT italic_A italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT that are described in [20]. We refer to this as the standard method for (1). It is described more fully in [20, 22]. It is simple and efficient but when B𝐵Bitalic_B is ill conditioned it comes with no expectation of stability and is in fact known to give large residuals for eigenvalues that are of small magnitude relative to ‖Λ‖2subscriptnormΛ2\|\Lambda\|_{2}∥ roman_Λ ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. If the Cholesky factorization of B𝐵Bitalic_B is computed with diagonal pivoting and the Jacobi method is used to compute the decomposition U⁢Λ⁢UT𝑈Λsuperscript𝑈TU\Lambda U^{\rm T}italic_U roman_Λ italic_U start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT, then [6] shows that even if B𝐵Bitalic_B is ill-conditioned, the computed eigenvalues satisfy backward error bounds that are often much better than might be expected from consideration of only the condition number of B𝐵Bitalic_B. The analysis can be extended to eigenvalues computed using the Q⁢R𝑄𝑅QRitalic_Q italic_R algorithm if the initial tridiagonal reduction is performed using plane rotations. There are a number of other options. The Q⁢Z𝑄𝑍QZitalic_Q italic_Z algorithm [16] for the nonsymmetric generalized eigenvalue problem is backward stable. However it is significantly slower than the standard method for (1) and, when applied to a symmetric problem, the backward errors are not in general symmetric. As a consequence, the computed eigenvalues can be complex and specialized perturbation theory for the symmetric problem [21] is not applicable. Throwing out the potentially significant imaginary part of a sensitive computed eigenvalue does not fix the problem in a stable way. The methods of [8] and [19] attempt to deflate the problem in a way that removes infinite or very large generalized eigenvalues prior to inversion. Their stability depends on deflating with a threshold that does not modify the problem excessively while guaranteeing that only a well conditioned matrix needs to be inverted to obtain the computed finite eigenvalues. This involves balancing potentially conflicting requirements for stability and does not always give satisfactory results. A stable algorithm was presented in [5] with an error analysis giving entirely satisfactory bounds on relative residuals. The stability of the algorithm is unconditional, which is ideal. However it involves computing an ordinary eigenvalue decomposition and transformations used in deflation of an eigenvalue can increase the size of residuals. If a test on residuals fails in the course of the algorithm, an ordinary eigenvalue decomposition might need to be recomputed. The number of such decompositions that need to be computed was reported to be small in the numerical tests, and there were arguments given as to why this should be so. Nevertheless there is some uncertainty about how many eigenvalue decompositions are needed. Finally, and of particular importance to the approach taken here, there is substantial previous work on the sparse problem. The Lanczos algorithm can in principle be applied to Cb−T⁢A⁢Cb−1superscriptsubscript𝐶𝑏T𝐴superscriptsubscript𝐶𝑏1C_{b}^{-{\rm T}}AC_{b}^{-1}italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - roman_T end_POSTSUPERSCRIPT italic_A italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT to solve (1) while exploiting sparsity. However, this poses numerical difficulties that are similar to those encountered when computing a full eigenvalue decomposition of Cb−T⁢A⁢Cb−1superscriptsubscript𝐶𝑏T𝐴superscriptsubscript𝐶𝑏1C_{b}^{-{\rm T}}AC_{b}^{-1}italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - roman_T end_POSTSUPERSCRIPT italic_A italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT. The spectral transformation Lanczos method [7, 11] avoids these issues by applying the Lanczos method to CbT⁢(A−σ⁢B)−1⁢Cbsuperscriptsubscript𝐶𝑏Tsuperscript𝐴𝜎𝐵1subscript𝐶𝑏C_{b}^{\rm T}(A-\sigma B)^{-1}C_{b}italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT ( italic_A - italic_σ italic_B ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT. The merits of this algorithm, and a variant, were further described in [17]. The main contribution of this paper is the observation that with a suitable shift, this same transformation conditionally stabilizes a simple direct method for (1). In terms of stability and efficiency, the algorithm inhabits a middle ground between the standard Cholesky based algorithm and that of [5]. Relative to [5], we compromise on stability in having conditions on the shift attached to bounds on the magnitude of residuals. However, the algorithm is easily implemented using factorizations already implemented in LAPACK using level-3 BLAS operations and is much closer in cost to the Cholesky-based method. Furthermore, it is typically possible to choose a single moderately sized shift that will work well for all computed eigenvalues. However for computed eigenvectors and shifts of large magnitude, there are additional restrictions related to the magnitude of the shift relative to the magnitudes of the eigenvalues of interest. In the remainder of the paper, we derive the algorithm, prove error bounds, and present the results of numerical experiments that illustrate the key features of the bounds. In §2 we give a full description of the algorithm, which is motivated by a simple lemma that later provides a basis for the error analysis. We also highlight other properties of the algorithm that will be of importance for the analysis. A forward/backward stability result for a matrix decomposition for the shifted and inverted problem analysis is given in §3. In §4 and §5 we give bounds for residuals associated with the problem (2) in the distinct cases of moderate and large shifts. The results of the numerical experiments are given in §6. Results are summarized in §7."
https://arxiv.org/html/2411.03523v1,Hamiltonian Monte Carlo methods for spectroscopy data analysis,"We present a scalable Bayesian framework for the analysis of confocal fluorescence spectroscopy data, addressing key limitations in traditional fluorescence correlation spectroscopy methods. Our framework captures molecular motion, microscope optics, and photon detection with high fidelity, enabling statistical inference of molecule trajectories from raw photon count data, introducing a superresolution parameter which further enhances trajectory estimation beyond the native time resolution of data acquisition. To handle the high dimensionality of the arising posterior distribution, we develop a family of Hamiltonian Monte Carlo (HMC) algorithms that leverages the unique characteristics inherent to spectroscopy data analysis. Here, due to the highly-coupled correlation structure of the target posterior distribution, HMC requires the numerical solution of a stiff ordinary differential equation containing a two-scale discrete Laplacian. By considering the spectral properties of this operator, we produce a CFL-type integrator stability condition for the standard Störmer-Verlet integrator used in HMC. To circumvent this instability we introduce a semi-implicit (IMEX) method which treats the stiff and non-stiff parts differently, while leveraging the sparse structure of the discrete Laplacian for computational efficiency. Detailed numerical experiments demonstrate that this method improves upon fully explicit approaches, allowing larger HMC step sizes and maintaining second-order accuracy in position and energy. Our framework provides a foundation for extensions to more complex models such as surface constrained molecular motion or motion with multiple diffusion modes.","Fluorescence spectroscopy studies light-matter interactions by means of specialized experiments [1, 22, 36, 37, 33]. Nowadays, various spectroscopic modalities are routinely used to identify chemical compounds [21, 28], characterize materials [46], image biological specimens [23, 26], and, most recently, also to quantify dynamics of moving particles [16, 17, 41, 42]. In particular, fluorescence correlation spectroscopy (FCS) combines unique optical devices and sampling electronics to record direct data at time scales on the order of one nanosecond which can resolve single photon emissions [22, 44, 36, 37, 33]. Such raw data, combined with mathematical analysis [16, 9, 27, 6], elucidate dynamics of moving molecules in chemical, biological and biochemical applications. A FCS experiment uses a confocal microscope to illuminate only a diffraction limited spot within the specimen of interest [45, 23, 47]. Subsequently, fluorescent particles, that absorb the illuminating light and re-emit it at a different color, are allowed to move within this volume. Optical filters are used to separate the emitted light and fast photon detectors are then employed to record them in real time. This way, a FCS experiment yields time series data of photon detections that are acquired typically at rates as fast as one sample per nano- to millisecond for time periods in excess of one minute. The resulting time series datasets are subsequently analyzed to estimate mainly diffusion coefficients that are characteristic of the motion of the fluorescent particles [16]. Data acquisition rates at the nanosecond scale across typical durations at or longer than a minute produce datasets with sizes on or above the order of 1010superscript101010^{10}10 start_POSTSUPERSCRIPT 10 end_POSTSUPERSCRIPT. Processing such high data volume requires careful design and implementation of statistical analysis techniques. Traditionally, data analysis in FCS is carried out by means of correlation functions that are derived based on simplistic theoretical models and fitted heuristically. Among many FCS simplifications the most important ones ignore detection noise and optics [9, 27, 31]. As shown in [16], such an approach has a degrading effect on the quality of the generated estimates that may be misleading or biased. In contrast, direct analysis methods using Markov state-space models [4, 30] in combination with Bayesian non-parametrics [11, 30] have recently been proposed as viable alternatives to correlative FCS analysis [16, 6]. By relying on realistic modeling representations without artificial simplifications, direct approaches yield reliable estimates with only a fraction of the data traditionally required for FCS applications. This is of paramount importance for biological FCS applications which seek to reduce photo-damage and so cannot afford excessively long data acquisition periods or exceedingly high illumination intensities as required for high signal-to-noise ratio (SNR) [3]. Furthermore, such specialized approaches are built on high fidelity representations of the experiment and allow for modifications and generalizations to non-standard setups. For instance, a faithful representation allows for modeling read-out noise and detectors [19, 20], complicated optics [13, 23, 34], multiple confocal volumes [18], as well as photo-physics [20, 40] which are common limiting factors to conventional FCS. The study in [16] provided a proof of principle for the direct analysis of raw spectroscopic data acquired in a typical FCS experiment. However, the numerical methods in [16] are limited to slowly diffusing particles (≲less-than-or-similar-to\lesssim≲10 μ𝜇\muitalic_μm2/sec) that may not be always given. In fact, biomolecules assessed in vitro or data of low SNR such as free fluorescent dyes and inorganic compounds assessed in vivo may exhibit diffusion coefficients as high as ≈\approx≈500 μ𝜇\muitalic_μm2/sec [42, 20, 42]. Extracting estimates of such high diffusion coefficients from raw spectroscopic data requires a more powerful computational framework than that proposed in [16]. In this study, we develop and investigate a novel framework for analyzing confocal fluorescence spectroscopy data. Specifically, we construct a generative probabilistic model of this spectroscopy method and examine the numerical aspects of Bayesian techniques used to characterize the conditional distribution of molecule trajectories given integrative photon count data [20, 19]. This requires solving a high-dimensional Bayesian inverse problem involving a target distribution exhibiting a complex covariance structure. Given the dimensionality of this distribution (detailed in section 2), standard Monte Carlo approaches, such as Random Walk Metropolis-Hastings, are unsuitable [25, 32]. Instead, we put forward a Hamiltonian Monte Carlo (HMC) scheme [2, 5, 29], which efficiently handles high-dimensional distributions. However, the computational demands are high because our approach requires repeatedly solving an ordinary differential equation (ODE). For typical fluorescence spectroscopy applications, the ODE is stiff, necessitating the use of ill-conditioned matrix operations for accurate numerical solutions. Our problem, typical of Bayesian inversion problems, involves a posterior distribution formed as the product of a likelihood function and a prior [30, 4, 32, 25]. This multiplicative structure naturally separates the ODE into stiff and non-stiff components. We analyze a solver that leverages this split structure and propose a new HMC-based inference method for fluorescence spectroscopy data, validated using in silico data mimicking realistic confocal fluorescence spectroscopy conditions. The remainder of this study is organized as follows. In section 2 we describe our model, the forward problem of generating synthetic data, and the inverse problem of estimating the diffusion coefficient of a molecular species given photon count data. The motivation for and description of our new HMC algorithms are also in section 2. We provide demonstrations and numerical results in section 3, and a discussion in section 4."
https://arxiv.org/html/2411.03411v1,A robust first order meshfree method fortime-dependent nonlinear conservation laws,"We introduce a robust first order accurate meshfree method to numerically solve time-dependent nonlinear conservation laws. The main contribution of this work is the meshfree construction of first order consistent summation by parts differentiations. We describe how to efficiently construct such operators on a point cloud. We then study the performance of such differentiations, and then combine these operators with a numerical flux-based formulation to approximate the solution of nonlinear conservation laws, with focus on the advection equation and the compressible Euler equations. We observe numerically that, while the resulting mesh-free differentiation operators are only O⁢(h12)𝑂superscriptℎ12O(h^{\frac{1}{2}})italic_O ( italic_h start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT ) accurate in the L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT norm, they achieve O⁢(h)𝑂ℎO(h)italic_O ( italic_h ) rates of convergence when applied to the numerical solution of PDEs.","1 Introduction Numerical methods for solving partial differential equations (PDEs) form the backbone of computational modeling and simulation efforts in science and engineering. The majority of numerical methods for PDEs rely on a representation of the domain as a mesh. However, solution quality and mesh quality are strongly related, such that poor quality meshes with irregularly shaped elements result in poorly approximated solutions [1, 27, 16]. This is especially problematic in 3D, where it is difficult to automatically and efficiently generate unstructured meshes with guaranteed element quality [4]. Meshfree methods encompass a broad class of numerical schemes intended to circumvent the mesh generation step. These methods range from particle-based methods to high order collocation type schemes [10]. However, a common issue faced by meshfree discretizations is balancing accuracy with stability and robustness. While there are a variety of methods for constructing accurate structure preserving mesh-based discretiztations [9, 21, 7], it is more difficult to ensure that meshfree methods are conservative and stable [28, 11]. In this paper, we present a method for constructing meshfree discretizations based on the enforcement of a summation-by-parts (SBP) property. This work is closely related to the formulation of [5], but exploits the fact that enforcing only first order accuracy constraints results in a simpler construction of meshfree operators [32]. These operators are then used to formulate a meshfree semi-discretization in terms of finite volume fluxes, as is done in [5]. If these fluxes are local Lax-Friedrichs fluxes with appropriate wave-speed estimates, the resulting discretization can be shown to be invariant-domain preserving under forward Euler time-stepping and a CFL condition. The paper proceeds as follows. In Section 2, we introduce the concept of summation by parts (SBP) operators. In Section 3, we introduce a methodology for constructing SBP and norm matrices given a point cloud, an adjacency matrix, and surface information (e.g., outward unit normals and boundary quadrature weights). In Section 4, we explore different ways of constructing the adjacency matrix and its effect on the behavior of the differentiation matrix. In Section 5, we discuss how to construct a numerical method for nonlinear conservation laws based on the meshfree operators, and in Section 6, we present some numerical results where we apply the method to the 2D linear advection and compressible Euler equations."
https://arxiv.org/html/2411.04119v1,Marcinkiewicz–Zygmund inequalitiesin quasi-Banach function spaces,"We obtain Marcinkiewicz–Zygmund (MZ) inequalities in various Banach and quasi-Banach spaces under minimal assumptions on the structural properties of these spaces. Our main results show that the Bernstein inequality in a general quasi-Banach function lattice X𝑋Xitalic_X implies Marcinkiewicz–Zygmund type estimates in X𝑋Xitalic_X. We present a general approach to obtain MZ inequalities not only for polynomials but for other function classes including entire functions of exponential type, splines, exponential sums, etc.","The classical Marcinkiewicz–Zygmund (MZ) inequality for trigonometric polynomials in Lp⁢(𝕋)subscript𝐿𝑝𝕋L_{p}({{\mathbb{T}}})italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( blackboard_T ), 𝕋=[0,2⁢π),𝕋02𝜋{{\mathbb{T}}}=[0,2\pi),blackboard_T = [ 0 , 2 italic_π ) , has the following form (1.1) C−1⁢(1N+1⁢∑k=0N|T⁢(tk)|p)1/p≤‖T‖Lp⁢(𝕋)≤C⁢(1N+1⁢∑k=0N|T⁢(tk)|p)1/p,superscript𝐶1superscript1𝑁1superscriptsubscript𝑘0𝑁superscript𝑇subscript𝑡𝑘𝑝1𝑝subscriptnorm𝑇subscript𝐿𝑝𝕋𝐶superscript1𝑁1superscriptsubscript𝑘0𝑁superscript𝑇subscript𝑡𝑘𝑝1𝑝C^{-1}\left(\frac{1}{N+1}\sum_{k=0}^{N}|T(t_{k})|^{p}\right)^{1/p}\leq\|T\|_{L% _{p}({{\mathbb{T}}})}\leq C\left(\frac{1}{N+1}\sum_{k=0}^{N}|T(t_{k})|^{p}% \right)^{1/p},italic_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_N + 1 end_ARG ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | italic_T ( italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) | start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 1 / italic_p end_POSTSUPERSCRIPT ≤ ∥ italic_T ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( blackboard_T ) end_POSTSUBSCRIPT ≤ italic_C ( divide start_ARG 1 end_ARG start_ARG italic_N + 1 end_ARG ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | italic_T ( italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) | start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 1 / italic_p end_POSTSUPERSCRIPT , where T𝑇Titalic_T is a trigonometric polynomial of degree at most n𝑛nitalic_n, 𝒯n={T:T⁢(x)=∑k=−nnck⁢ei⁢k⁢x}subscript𝒯𝑛conditional-set𝑇𝑇𝑥superscriptsubscript𝑘𝑛𝑛subscript𝑐𝑘superscript𝑒𝑖𝑘𝑥\mathcal{T}_{n}=\{T\,:\,T(x)=\sum_{k=-n}^{n}c_{k}e^{ikx}\}caligraphic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = { italic_T : italic_T ( italic_x ) = ∑ start_POSTSUBSCRIPT italic_k = - italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT italic_i italic_k italic_x end_POSTSUPERSCRIPT }, tk=2⁢π⁢kN+1subscript𝑡𝑘2𝜋𝑘𝑁1t_{k}=\frac{2\pi k}{N+1}italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = divide start_ARG 2 italic_π italic_k end_ARG start_ARG italic_N + 1 end_ARG, k=0,…,N𝑘0…𝑁k=0,\dots,Nitalic_k = 0 , … , italic_N, N={2⁢n,if 1<p<∞,[(2+ε)⁢n]+1,ε>0,if 0<p≤1 or p=∞,𝑁cases2𝑛if 1<p<∞,delimited-[]2𝜀𝑛1𝜀0if 0<p≤1 or p=∞,N=\left\{\begin{array}[]{ll}2n,&\hbox{if $1<p<\infty$,}\\ {[(2+\varepsilon)n]+1},\,\,\varepsilon>0,&\hbox{if $0<p\leq 1$ or $p=\infty$,}% \end{array}\right.italic_N = { start_ARRAY start_ROW start_CELL 2 italic_n , end_CELL start_CELL if 1 < italic_p < ∞ , end_CELL end_ROW start_ROW start_CELL [ ( 2 + italic_ε ) italic_n ] + 1 , italic_ε > 0 , end_CELL start_CELL if 0 < italic_p ≤ 1 or italic_p = ∞ , end_CELL end_ROW end_ARRAY and the constant C𝐶Citalic_C depends only on p𝑝pitalic_p and ε𝜀\varepsilonitalic_ε, cf. [61, Ch. X, § 7], [43]. This estimate plays a key role in the modern discretization theory and sampling recovery both in the theoretical and applied aspects. Various generalizations of the MZ inequality are known, see, e.g., [9, 18, 23, 31, 35, 39, 48, 60]. It is easy to see that (1.1) can be equivalently written as follows (1.2) C−1⁢‖∑k=0N|⁢T⁢(tk)⁢|χ(tk,tk+1)‖Lp⁢(𝕋)≤‖T‖Lp⁢(𝕋)≤C⁢‖∑k=0N|⁢T⁢(tk)⁢|χ(tk,tk+1)‖Lp⁢(𝕋).superscript𝐶1delimited-‖|superscriptsubscript𝑘0𝑁𝑇subscript𝑡𝑘subscriptdelimited-|‖subscript𝜒subscript𝑡𝑘subscript𝑡𝑘1subscript𝐿𝑝𝕋subscriptnorm𝑇subscript𝐿𝑝𝕋𝐶delimited-‖|superscriptsubscript𝑘0𝑁𝑇subscript𝑡𝑘subscriptdelimited-|‖subscript𝜒subscript𝑡𝑘subscript𝑡𝑘1subscript𝐿𝑝𝕋C^{-1}\bigg{\|}\sum_{k=0}^{N}|T(t_{k})|\chi_{(t_{k},t_{k+1})}\bigg{\|}_{L_{p}(% {{\mathbb{T}}})}\leq\|T\|_{L_{p}({{\mathbb{T}}})}\leq C\bigg{\|}\sum_{k=0}^{N}% |T(t_{k})|\chi_{(t_{k},t_{k+1})}\bigg{\|}_{L_{p}({{\mathbb{T}}})}.italic_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | italic_T ( italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) | italic_χ start_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( blackboard_T ) end_POSTSUBSCRIPT ≤ ∥ italic_T ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( blackboard_T ) end_POSTSUBSCRIPT ≤ italic_C ∥ ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | italic_T ( italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) | italic_χ start_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( blackboard_T ) end_POSTSUBSCRIPT . This formula shows that the Lpsubscript𝐿𝑝L_{p}italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT norm of a trigonometric polynomial is equivalent to the same norm of a simple function that interpolates the function at given points. This observation gives rise to the following natural question: Under what conditions on the space X𝑋Xitalic_X, the subspace Fn⊂Xsubscript𝐹𝑛𝑋F_{n}\subset Xitalic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ⊂ italic_X, and the points (xk)k=0Nsuperscriptsubscriptsubscript𝑥𝑘𝑘0𝑁(x_{k})_{k=0}^{N}( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT does the Marcinkiewicz–Zygmund inequality (1.3) C−1⁢‖∑k=0N|⁢F⁢(xk)⁢|χ(xk,tx+1)‖X≤‖F‖X≤C⁢‖∑k=0N|⁢F⁢(xk)⁢|χ(xk,xk+1)‖Xsuperscript𝐶1delimited-‖|superscriptsubscript𝑘0𝑁𝐹subscript𝑥𝑘subscriptdelimited-|‖subscript𝜒subscript𝑥𝑘subscript𝑡𝑥1𝑋subscriptnorm𝐹𝑋𝐶delimited-‖|superscriptsubscript𝑘0𝑁𝐹subscript𝑥𝑘subscriptdelimited-|‖subscript𝜒subscript𝑥𝑘subscript𝑥𝑘1𝑋C^{-1}\bigg{\|}\sum_{k=0}^{N}|F(x_{k})|\chi_{(x_{k},t_{x+1})}\bigg{\|}_{X}\leq% \|F\|_{X}\leq C\bigg{\|}\sum_{k=0}^{N}|F(x_{k})|\chi_{(x_{k},x_{k+1})}\bigg{\|% }_{X}italic_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∥ ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | italic_F ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) | italic_χ start_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_x + 1 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ≤ ∥ italic_F ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ≤ italic_C ∥ ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT | italic_F ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) | italic_χ start_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT hold for any F∈Fn𝐹subscript𝐹𝑛F\in F_{n}italic_F ∈ italic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT with a constant C𝐶Citalic_C independent of Fnsubscript𝐹𝑛F_{n}italic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and N𝑁Nitalic_N? In this paper we answer this question for a Banach and quasi-Banach function space X𝑋Xitalic_X, more general than Lpsubscript𝐿𝑝L_{p}italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT or even having different structure. Notations. First, we specify function spaces and (quasi)-norm, for which we generalize the MZ inequalities. Let X=X⁢(Ω,μ;q)𝑋𝑋Ω𝜇𝑞X=X(\Omega,\mu;q)italic_X = italic_X ( roman_Ω , italic_μ ; italic_q ), 0<q≤10𝑞10<q\leq 10 < italic_q ≤ 1, be a (quasi-)Banach function lattice on a domain Ω⊂ℝdΩsuperscriptℝ𝑑\Omega\subset{\mathbb{R}}^{d}roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT endowed with positive σ𝜎{\sigma}italic_σ-finite measure μ𝜇\muitalic_μ (cf. [2, Ch. 2]). In what follows, we will assume that the functional ∥⋅∥X:X↦ℝ+\|\cdot\|_{X}\,:\,X\mapsto{\mathbb{R}}_{+}∥ ⋅ ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT : italic_X ↦ blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT satisfies the following properties: 1)1)1 ) ‖f‖X=0subscriptnorm𝑓𝑋0\|f\|_{X}=0∥ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = 0 if and only if f=0𝑓0f=0italic_f = 0, 2)2)2 ) ‖λ⁢f‖X=|λ|⁢‖f‖Xsubscriptnorm𝜆𝑓𝑋𝜆subscriptnorm𝑓𝑋\|{\lambda}f\|_{X}=|{\lambda}|\|f\|_{X}∥ italic_λ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = | italic_λ | ∥ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT for all λ∈ℂ𝜆ℂ{\lambda}\in{\mathbb{C}}italic_λ ∈ blackboard_C, 3)3)3 ) ‖f+g‖Xq≤‖f‖Xq+‖g‖Xqsuperscriptsubscriptnorm𝑓𝑔𝑋𝑞superscriptsubscriptnorm𝑓𝑋𝑞superscriptsubscriptnorm𝑔𝑋𝑞\|f+g\|_{X}^{q}\leq\|f\|_{X}^{q}+\|g\|_{X}^{q}∥ italic_f + italic_g ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT ≤ ∥ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT + ∥ italic_g ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_q end_POSTSUPERSCRIPT, 4)4)4 ) if |f|≤|g|𝑓𝑔|f|\leq|g|| italic_f | ≤ | italic_g |, then ‖f‖X≤‖g‖Xsubscriptnorm𝑓𝑋subscriptnorm𝑔𝑋\|f\|_{X}\leq\|g\|_{X}∥ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ≤ ∥ italic_g ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT for all f,g∈X𝑓𝑔𝑋f,g\in Xitalic_f , italic_g ∈ italic_X, 5)5)5 ) χB∈Xsubscript𝜒𝐵𝑋\chi_{B}\in Xitalic_χ start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ∈ italic_X for all B⊂Ω𝐵ΩB\subset\Omegaitalic_B ⊂ roman_Ω with μ⁢(B)<∞𝜇𝐵\mu(B)<\inftyitalic_μ ( italic_B ) < ∞. If X𝑋Xitalic_X is a Banach space (when q=1𝑞1q=1italic_q = 1), then we will write X=X⁢(Ω,μ)=X⁢(Ω,μ;1)𝑋𝑋Ω𝜇𝑋Ω𝜇1X=X(\Omega,\mu)=X(\Omega,\mu;1)italic_X = italic_X ( roman_Ω , italic_μ ) = italic_X ( roman_Ω , italic_μ ; 1 ) and assume that X⊂L1𝑋subscript𝐿1X\subset L_{1}italic_X ⊂ italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. In this case, X′superscript𝑋′X^{\prime}italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT denotes the associate space of X𝑋Xitalic_X with the norm given by ‖g‖X′=sup‖f‖X≤1∫Ωg⁢(x)⁢f⁢(x)¯⁢𝑑μ⁢(x),subscriptnorm𝑔superscript𝑋′subscriptsupremumsubscriptnorm𝑓𝑋1subscriptΩ𝑔𝑥¯𝑓𝑥differential-d𝜇𝑥\|g\|_{X^{\prime}}=\sup_{\|f\|_{X}\leq 1}\int_{\Omega}g(x)\overline{f(x)}d\mu(% x),∥ italic_g ∥ start_POSTSUBSCRIPT italic_X start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = roman_sup start_POSTSUBSCRIPT ∥ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ≤ 1 end_POSTSUBSCRIPT ∫ start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT italic_g ( italic_x ) over¯ start_ARG italic_f ( italic_x ) end_ARG italic_d italic_μ ( italic_x ) , see [2, p. 12]. Recall that the space X𝑋Xitalic_X is called translation invariant if for all t∈Ω𝑡Ωt\in\Omegaitalic_t ∈ roman_Ω and f∈X𝑓𝑋f\in Xitalic_f ∈ italic_X, there hold f⁢(x+t)∈X𝑓𝑥𝑡𝑋f(x+t)\in Xitalic_f ( italic_x + italic_t ) ∈ italic_X and ∥f(⋅+t)∥X=∥f∥X\|f(\cdot+t)\|_{X}=\|f\|_{X}∥ italic_f ( ⋅ + italic_t ) ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = ∥ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT. Also, the space X𝑋Xitalic_X is called rearrangement invariant if ‖f‖X=‖g‖Xsubscriptnorm𝑓𝑋subscriptnorm𝑔𝑋\|f\|_{X}=\|g\|_{X}∥ italic_f ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT = ∥ italic_g ∥ start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT whenever μf⁢(x)=μg⁢(x)subscript𝜇𝑓𝑥subscript𝜇𝑔𝑥\mu_{f}(x)=\mu_{g}(x)italic_μ start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ( italic_x ) = italic_μ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( italic_x ), where μh⁢(x)=μ⁢({x∈Ω:|h⁢(x)|≥y})subscript𝜇ℎ𝑥𝜇conditional-set𝑥Ωℎ𝑥𝑦\mu_{h}(x)=\mu(\{x\in\Omega\,:\,|h(x)|\geq y\})italic_μ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_x ) = italic_μ ( { italic_x ∈ roman_Ω : | italic_h ( italic_x ) | ≥ italic_y } ) is the distribution function of hℎhitalic_h. In what follows, ℱnsubscriptℱ𝑛\mathcal{F}_{n}caligraphic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT denotes a subspace of X∩C⁢(ℝd)𝑋𝐶superscriptℝ𝑑X\cap C({\mathbb{R}}^{d})italic_X ∩ italic_C ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ), which depends on the parameter n𝑛nitalic_n, where n∈ℕorn⁢is a real positive number.𝑛ℕor𝑛is a real positive numbern\in\mathbb{N}\quad\text{or}\quad n\,\,\text{is a real positive number}.italic_n ∈ blackboard_N or italic_n is a real positive number . We note that for all classical examples of subspaces ℱnsubscriptℱ𝑛\mathcal{F}_{n}caligraphic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT (trigonometric and algebraic polynomials, splines, entire functions of exponential type) there holds the condition ℱm⊂ℱnsubscriptℱ𝑚subscriptℱ𝑛\mathcal{F}_{m}\subset\mathcal{F}_{n}caligraphic_F start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ⊂ caligraphic_F start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT for n≥m𝑛𝑚n\geq mitalic_n ≥ italic_m. However, we do not require this assumption in our main results. Throughout the paper, we will use the letter C𝐶Citalic_C to denote various positive constants, which may differ even within the same line where they appear. We also use the notation A≲B,less-than-or-similar-to𝐴𝐵\,A\lesssim B,italic_A ≲ italic_B , with A,B≥0𝐴𝐵0A,B\geq 0italic_A , italic_B ≥ 0, for the estimate A≤C⁢B,𝐴𝐶𝐵\,A\leq C\,B,italic_A ≤ italic_C italic_B , where C𝐶\,Citalic_C is a positive constant independent of the essential variables in A𝐴\,Aitalic_A and B𝐵\,Bitalic_B (usually, δ𝛿{\delta}italic_δ, n𝑛nitalic_n, and N𝑁Nitalic_N). If A≲Bless-than-or-similar-to𝐴𝐵\,A\lesssim Bitalic_A ≲ italic_B and B≲Aless-than-or-similar-to𝐵𝐴\,B\lesssim Aitalic_B ≲ italic_A simultaneously, we write A≍Basymptotically-equals𝐴𝐵\,A\asymp Bitalic_A ≍ italic_B and say that A𝐴\,Aitalic_A is equivalent to B𝐵\,Bitalic_B. As usual, for α∈ℤ+d𝛼superscriptsubscriptℤ𝑑{\alpha}\in{{\mathbb{Z}}}_{+}^{d}italic_α ∈ blackboard_Z start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, Dα=∂|α|1∂x1α1⁢…⁢∂xdαd.superscript𝐷𝛼superscriptsubscript𝛼1superscriptsubscript𝑥1subscript𝛼1…superscriptsubscript𝑥𝑑subscript𝛼𝑑D^{\alpha}=\frac{\partial^{|{\alpha}|_{1}}}{\partial x_{1}^{{\alpha}_{1}}\dots% \partial x_{d}^{{\alpha}_{d}}}.italic_D start_POSTSUPERSCRIPT italic_α end_POSTSUPERSCRIPT = divide start_ARG ∂ start_POSTSUPERSCRIPT | italic_α | start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG start_ARG ∂ italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT … ∂ italic_x start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_α start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT end_ARG . For δ>0𝛿0{\delta}>0italic_δ > 0, we denote Q⁢(x,δ)={y∈Ω:‖x−y‖∞≤δ}.𝑄𝑥𝛿conditional-set𝑦Ωsubscriptnorm𝑥𝑦𝛿Q(x,{\delta})=\{y\in\Omega\,:\,\|x-y\|_{\infty}\leq{\delta}\}.italic_Q ( italic_x , italic_δ ) = { italic_y ∈ roman_Ω : ∥ italic_x - italic_y ∥ start_POSTSUBSCRIPT ∞ end_POSTSUBSCRIPT ≤ italic_δ } . Structure of the paper. In Section 2 we establish MZ inequalities in Banach and quasi-Banach spaces: in Subsection 2.1, we investigate the boundedness of certain maximal and minimal functions in X𝑋Xitalic_X and then in Subsection 2.2 we apply these results to derive MZ inequalities in form (1.3). In Section 3, we obtain MZ-type inequalities with minimal number of nodes with the help of quadrature formulas. Sections 4 and 5 are devoted to MZ estimates for special and important examples, namely, for trigonometric and algebraic polynomials. Examples for other function systems are considered in Section 6. In Section 7, we consider some applications of MZ-type inequalities including the approximation of functions by sampling operators and polynomials in general Banach spaces, Nikolskii-type inequalities, and embedding theorems."
https://arxiv.org/html/2411.04081v1,A Multi-level Monte Carlo simulation for invariant distribution of Markovian switching Lévy-driven SDEs with super-linearly growth coefficients,"This paper concerns the numerical approximation for the invariant distribution of Markovian switching Lévy-driven stochastic differential equations. By combining the tamed-adaptive Euler-Maruyama scheme with the Multi-level Monte Carlo method, we propose an approximation scheme that can be applied to stochastic differential equations with super-linear growth drift and diffusion coefficients.","On a complete filtered probability space (Ω,ℱ,(ℱt)t≥0,ℙ)Ωℱsubscriptsubscriptℱ𝑡𝑡0ℙ(\Omega,\mathcal{F},(\mathcal{F}_{t})_{t\geq 0},\mathbb{P})( roman_Ω , caligraphic_F , ( caligraphic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT , blackboard_P ), we consider the d𝑑ditalic_d-dimensional process X=(Xt)t≥0=(X1,t,X2,t,…,Xd,t)t≥0𝑋subscriptsubscript𝑋𝑡𝑡0subscriptsubscript𝑋1𝑡subscript𝑋2𝑡…subscript𝑋𝑑𝑡𝑡0X=(X_{t})_{t\geq 0}=(X_{1,t},X_{2,t},\ldots,X_{d,t})_{t\geq 0}italic_X = ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT = ( italic_X start_POSTSUBSCRIPT 1 , italic_t end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT 2 , italic_t end_POSTSUBSCRIPT , … , italic_X start_POSTSUBSCRIPT italic_d , italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT solution to the following Lévy-driven stochastic differential equation with Markovian switching Xt=x0+∫0tb⁢(θs,Xs)⁢𝑑s+∫0tσ⁢(θs,Xs)⁢𝑑Ws+∫0tγ⁢(θs−,Xs−)⁢𝑑Zs,subscript𝑋𝑡subscript𝑥0superscriptsubscript0𝑡𝑏subscript𝜃𝑠subscript𝑋𝑠differential-d𝑠superscriptsubscript0𝑡𝜎subscript𝜃𝑠subscript𝑋𝑠differential-dsubscript𝑊𝑠superscriptsubscript0𝑡𝛾subscript𝜃limit-from𝑠subscript𝑋limit-from𝑠differential-dsubscript𝑍𝑠X_{t}=x_{0}+\int_{0}^{t}b(\theta_{s},X_{s})ds+\int_{0}^{t}\sigma(\theta_{s},X_% {s})dW_{s}+\int_{0}^{t}\gamma\left(\theta_{s-},X_{s-}\right)dZ_{s},italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_b ( italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) italic_d italic_s + ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_σ ( italic_θ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) italic_d italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT + ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_γ ( italic_θ start_POSTSUBSCRIPT italic_s - end_POSTSUBSCRIPT , italic_X start_POSTSUBSCRIPT italic_s - end_POSTSUBSCRIPT ) italic_d italic_Z start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , (1) where x0∈ℝdsubscript𝑥0superscriptℝ𝑑x_{0}\in\mathbb{R}^{d}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT; W=(Wt)t≥0=(W1,t,W2,t,…,Wd,t)t≥0𝑊subscriptsubscript𝑊𝑡𝑡0subscriptsubscript𝑊1𝑡subscript𝑊2𝑡…subscript𝑊𝑑𝑡𝑡0W=(W_{t})_{t\geq 0}=(W_{1,t},W_{2,t},\ldots,W_{d,t})_{t\geq 0}italic_W = ( italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT = ( italic_W start_POSTSUBSCRIPT 1 , italic_t end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT 2 , italic_t end_POSTSUBSCRIPT , … , italic_W start_POSTSUBSCRIPT italic_d , italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT is a d𝑑ditalic_d-dimensional standard Brownian motion; Z=(Zt)t≥0=(Z1,t,Z2,t,…,Zd,t)t≥0𝑍subscriptsubscript𝑍𝑡𝑡0subscriptsubscript𝑍1𝑡subscript𝑍2𝑡…subscript𝑍𝑑𝑡𝑡0Z=(Z_{t})_{t\geq 0}=(Z_{1,t},Z_{2,t},\ldots,Z_{d,t})_{t\geq 0}italic_Z = ( italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT = ( italic_Z start_POSTSUBSCRIPT 1 , italic_t end_POSTSUBSCRIPT , italic_Z start_POSTSUBSCRIPT 2 , italic_t end_POSTSUBSCRIPT , … , italic_Z start_POSTSUBSCRIPT italic_d , italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT is a d𝑑ditalic_d-dimensional centered pure jump Lévy process whose Lévy measure ν𝜈\nuitalic_ν satisfies ∫ℝd(1∧|z|2)⁢ν⁢(d⁢z)<+∞subscriptsuperscriptℝ𝑑1superscript𝑧2𝜈𝑑𝑧\int_{\mathbb{R}^{d}}(1\wedge|z|^{2})\nu(dz)<+\infty∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( 1 ∧ | italic_z | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) italic_ν ( italic_d italic_z ) < + ∞; θ=(θt)t≥0𝜃subscriptsubscript𝜃𝑡𝑡0\theta=(\theta_{t})_{t\geq 0}italic_θ = ( italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT is a Markov chain taking values in a finite state space S={1,…,N}𝑆1…𝑁S=\{1,\ldots,N\}italic_S = { 1 , … , italic_N }. We suppose that the generator Θ=(ϑi⁢j)N×NΘsubscriptsubscriptitalic-ϑ𝑖𝑗𝑁𝑁\Theta=(\vartheta_{ij})_{N\times N}roman_Θ = ( italic_ϑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_N × italic_N end_POSTSUBSCRIPT of θ𝜃\thetaitalic_θ is defined by ℙ⁢(θt+u=j|θt=i)={ϑi⁢j⁢u+o⁢(u)if ⁢i≠j1+ϑi⁢i⁢u+o⁢(u)if ⁢i=j,ℙsubscript𝜃𝑡𝑢conditional𝑗subscript𝜃𝑡𝑖casessubscriptitalic-ϑ𝑖𝑗𝑢𝑜𝑢if 𝑖𝑗1subscriptitalic-ϑ𝑖𝑖𝑢𝑜𝑢if 𝑖𝑗\mathbb{P}(\theta_{t+u}=j|\theta_{t}=i)=\begin{cases}\vartheta_{ij}u+o(u)&% \textnormal{if }i\neq j\\ 1+\vartheta_{ii}u+o(u)&\textnormal{if }i=j\end{cases},blackboard_P ( italic_θ start_POSTSUBSCRIPT italic_t + italic_u end_POSTSUBSCRIPT = italic_j | italic_θ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_i ) = { start_ROW start_CELL italic_ϑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT italic_u + italic_o ( italic_u ) end_CELL start_CELL if italic_i ≠ italic_j end_CELL end_ROW start_ROW start_CELL 1 + italic_ϑ start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT italic_u + italic_o ( italic_u ) end_CELL start_CELL if italic_i = italic_j end_CELL end_ROW , where t,u>0𝑡𝑢0t,u>0italic_t , italic_u > 0, ϑi⁢j≥0subscriptitalic-ϑ𝑖𝑗0\vartheta_{ij}\geq 0italic_ϑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ≥ 0 is the switching rate from state i𝑖iitalic_i to state j≠i𝑗𝑖j\neq iitalic_j ≠ italic_i, and ϑi⁢i=−∑j∈S,j≠iϑi⁢j.subscriptitalic-ϑ𝑖𝑖subscriptformulae-sequence𝑗𝑆𝑗𝑖subscriptitalic-ϑ𝑖𝑗\vartheta_{ii}=-\sum_{j\in S,j\not=i}\vartheta_{ij}.italic_ϑ start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT = - ∑ start_POSTSUBSCRIPT italic_j ∈ italic_S , italic_j ≠ italic_i end_POSTSUBSCRIPT italic_ϑ start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT . Let N⁢(d⁢t,d⁢z)=(N1⁢(d⁢t,d⁢z),…,Nd⁢(d⁢t,d⁢z))𝑁𝑑𝑡𝑑𝑧subscript𝑁1𝑑𝑡𝑑𝑧…subscript𝑁𝑑𝑑𝑡𝑑𝑧N(dt,dz)=(N_{1}(dt,dz),\ldots,N_{d}(dt,dz))italic_N ( italic_d italic_t , italic_d italic_z ) = ( italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_d italic_t , italic_d italic_z ) , … , italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_d italic_t , italic_d italic_z ) ) and N~⁢(d⁢t,d⁢z)~𝑁𝑑𝑡𝑑𝑧\widetilde{N}(dt,dz)over~ start_ARG italic_N end_ARG ( italic_d italic_t , italic_d italic_z ) be the Poisson random measure and compensated Poisson random measure associated to Z𝑍Zitalic_Z, respectively. We suppose that N1,…,Ndsubscript𝑁1…subscript𝑁𝑑N_{1},\ldots,N_{d}italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT are independent and let νi⁢(d⁢zi)⁢d⁢tsubscript𝜈𝑖𝑑subscript𝑧𝑖𝑑𝑡\nu_{i}(dz_{i})dtitalic_ν start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_d italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_d italic_t denote the intensity measure of Nisubscript𝑁𝑖N_{i}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. The Lévy-Itô decomposition of Z𝑍Zitalic_Z takes the form Zi,t=∫0t∫ℝ0zi⁢(Ni⁢(d⁢s,d⁢zi)−νi⁢(d⁢zi)⁢d⁢s),t≥0.formulae-sequencesubscript𝑍𝑖𝑡superscriptsubscript0𝑡subscriptsubscriptℝ0subscript𝑧𝑖subscript𝑁𝑖𝑑𝑠𝑑subscript𝑧𝑖subscript𝜈𝑖𝑑subscript𝑧𝑖𝑑𝑠𝑡0Z_{i,t}=\int_{0}^{t}\int_{\mathbb{R}_{0}}z_{i}(N_{i}(ds,dz_{i})-\nu_{i}(dz_{i}% )ds),\quad t\geq 0.italic_Z start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∫ start_POSTSUBSCRIPT blackboard_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_d italic_s , italic_d italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_ν start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_d italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_d italic_s ) , italic_t ≥ 0 . Moreover, we assume that three processes W𝑊Witalic_W, Z𝑍Zitalic_Z, and θ𝜃\thetaitalic_θ are mutually independent and adapted to the filtration (ℱt)t≥0subscriptsubscriptℱ𝑡𝑡0(\mathcal{F}_{t})_{t\geq 0}( caligraphic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT; for each θ∈S𝜃𝑆\theta\in Sitalic_θ ∈ italic_S, b(θ,.)b(\theta,.)italic_b ( italic_θ , . ), σ(θ,.)\sigma(\theta,.)italic_σ ( italic_θ , . ) and γ(θ,.)\gamma(\theta,.)italic_γ ( italic_θ , . ) are measurable functions. Stochastic differential equations (SDEs) have been used to model many random processes in biology, engineering, finance, and physics (see [14, 21]). In particular, SDEs driven by jumps with switching coefficients can be used to capture sudden changes in the dynamic of these processes (see [11, 29, 33, 1, 24, 23]). In these applications, one often wants to understand the long-time behavior of the systems by computing their invariant distribution. The partial differential equations approach may help to find such distributions for low dimensional SDEs with regular coefficients. One should use some simulation method for high dimensional SDEs with less regular coefficients (see [17, 20, 30, 31]). More precisely, let π𝜋\piitalic_π be the invariant measure, then for some function φ𝜑\varphiitalic_φ, π⁢(φ):=∫φ⁢(x)⁢π⁢(d⁢x)≈𝔼⁢[φ⁢(XT)]⁢ for ⁢T⁢ large enough.assign𝜋𝜑𝜑𝑥𝜋𝑑𝑥𝔼delimited-[]𝜑subscript𝑋𝑇 for 𝑇 large enough\pi(\varphi):=\int\varphi(x)\pi(dx)\approx\mathbb{E}[\varphi(X_{T})]\text{ for% }T\text{ large enough}.italic_π ( italic_φ ) := ∫ italic_φ ( italic_x ) italic_π ( italic_d italic_x ) ≈ blackboard_E [ italic_φ ( italic_X start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ] for italic_T large enough . Since the value of 𝔼⁢[φ⁢(XT)]𝔼delimited-[]𝜑subscript𝑋𝑇\mathbb{E}[\varphi(X_{T})]blackboard_E [ italic_φ ( italic_X start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ) ] is again not analytically tractable, one needs to approximate XTsubscript𝑋𝑇X_{T}italic_X start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT by X^Tsubscript^𝑋𝑇\hat{X}_{T}over^ start_ARG italic_X end_ARG start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT which can be simulated from equation (1) by a time discretization scheme. It is well-known that for classical approximation schemes, such as the Euler-Maruyama (EM) scheme, the error of the estimate depends on T𝑇Titalic_T and may go to ∞\infty∞ as T→∞→𝑇T\to\inftyitalic_T → ∞. Therefore, it would be useful to construct a numerical approximation scheme for XTsubscript𝑋𝑇X_{T}italic_X start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT such that the error of the estimate does not depend on T𝑇Titalic_T. An extensive body of literature exists on numerical SDE methods for SDEs (see [14, 21, 25]). Additionally, significant attention has been given recently to numerical methods for SDEs with non-Lipschitz coefficients. In particular, it has been demonstrated in [7] that for SDEs with super-linear growth coefficients, the classical Euler-Maruyama scheme may fail to converge in the Lpsuperscript𝐿𝑝L^{p}italic_L start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT-norm. To address this issue, various modified Euler-Maruyama schemes have been developed, including the tamed Euler-Maruyama scheme ([8, 27, 28, 2]) and the truncated Euler-Maruyama scheme ([19]). These schemes have been further developed for SDE with jumps and Markovian switching in [3, 16, 9, 22] In [5], Fang and Giles introduced an adaptive EM method and showed its convergence in L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-norm on the whole time interval [0,∞)0[0,\infty)[ 0 , ∞ ) when applying for a class of SDEs whose drift coefficient is polynomial growth Lipschitz continuous and diffusion coefficient is bounded and globally Lipschitz continuous. In [12, 13, 18, 32], the tamed-adaptive EM scheme was proposed by combining the ideas of the adaptive scheme in [5] with the tamed scheme introduced in [8]. It has been proven in [12, 13, 18, 32] that the tamed-adaptive EM scheme also converges in L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-norm on the whole time interval [0,∞)0[0,\infty)[ 0 , ∞ ) when applying for a class of SDEs whose drift and diffusion coefficients are locally Lipschitz and superlinearly growth. Furthermore, the tamed-adaptive Euler-Maruyama scheme was introduced in [12, 13, 18, 32] by combining the adaptive scheme from [5] with the tamed scheme from [8, 27, 28]. It has been shown in [12, 13, 18, 32] that the tamed-adaptive Euler-Maruyama scheme also converges in the L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-norm over the entire interval [0,∞)0[0,\infty)[ 0 , ∞ ) when applying for a class of SDEs whose drift and diffusion coefficients are locally Lipschitz and superlinearly growth. The purpose of this paper is to employ the tamed-adaptive Euler-Maruyama (TAEM) approximation scheme to construct an approximation for the invariant distribution of equation (1). Our approach is inspired by the works [5, 12, 13, 18, 32]. By adapting the methodology in [4, 5], we consider a Multi-level Monte Carlo scheme to estimate π⁢(φ)𝜋𝜑\pi(\varphi)italic_π ( italic_φ ), where π𝜋\piitalic_π represents the invariant distribution of the process X𝑋Xitalic_X. This scheme applies to any time-discretization approximation of X𝑋Xitalic_X that satisfies a uniform-in-time estimate for the L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-norm of the error (see condition (6)). Subsequently, we present the TAEM scheme and demonstrate its strong convergence over both finite and infinite time intervals. Notably, this is the first paper to study long-time approximation for multidimensional Markovian switching SDEs with superlinear growth diffusion coefficients, even in the case of SDEs without jump components. The structure of this paper is as follows: Section 2 introduces a set of conditions on the coefficients of equation (1) along with moment estimates for the exact solution. Section 3 establishes the existence and uniqueness of the invariant measure and then proposes a Multi-level Monte Carlo approximation scheme for it. In Section 4, we present the TAEM scheme, analyzing its convergence over both finite and infinite time intervals. Finally, Section 5 provides a numerical analysis of the TAEM scheme."
https://arxiv.org/html/2411.04010v1,Laplace transform based quantum eigenvalue transformation via linear combination of Hamiltonian simulation,"Eigenvalue transformations, which include solving time-dependent differential equations as a special case, have a wide range of applications in scientific and engineering computation. While quantum algorithms for singular value transformations are well studied, eigenvalue transformations are distinct, especially for non-normal matrices. We propose an efficient quantum algorithm for performing a class of eigenvalue transformations that can be expressed as a certain type of matrix Laplace transformation. This allows us to significantly extend the recently developed linear combination of Hamiltonian simulation (LCHS) method [An, Liu, Lin, Phys. Rev. Lett. 131, 150603, 2023; An, Childs, Lin, arXiv:2312.03916] to represent a wider class of eigenvalue transformations, such as powers of the matrix inverse, A−ksuperscript𝐴𝑘A^{-k}italic_A start_POSTSUPERSCRIPT - italic_k end_POSTSUPERSCRIPT, and the exponential of the matrix inverse, e−A−1superscript𝑒superscript𝐴1e^{-A^{-1}}italic_e start_POSTSUPERSCRIPT - italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT. The latter can be interpreted as the solution of a mass-matrix differential equation of the form A⁢u′⁢(t)=−u⁢(t)𝐴superscript𝑢′𝑡𝑢𝑡Au^{\prime}(t)=-u(t)italic_A italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_t ) = - italic_u ( italic_t ). We demonstrate that our eigenvalue transformation approach can solve this problem without explicitly inverting A𝐴Aitalic_A, reducing the computational complexity.","Quantum computers are expected to solve certain computational problems much more efficiently than classical computers, such as factoring large integers [Sho97] and simulating the dynamics of quantum systems [BCK15, LC17, GSLW19, CST+21]. Large scale and high dimensionality appear ubiquitously in scientific and engineering computation and have posed significant challenges for classical computers, motivating the development of efficient quantum algorithms for such problems. Many scientific computing tasks can be expressed as eigenvalue transformations. Suppose A∈ℂN×N𝐴superscriptℂ𝑁𝑁A\in\mathbb{C}^{N\times N}italic_A ∈ blackboard_C start_POSTSUPERSCRIPT italic_N × italic_N end_POSTSUPERSCRIPT is diagonalizable as A=𝒱⁢D⁢𝒱−1𝐴𝒱𝐷superscript𝒱1A=\mathcal{V}D\mathcal{V}^{-1}italic_A = caligraphic_V italic_D caligraphic_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, where 𝒱𝒱\mathcal{V}caligraphic_V is an invertible matrix and D𝐷Ditalic_D is a diagonal matrix. Let h:ℂ→ℂ:ℎ→ℂℂh:\mathbb{C}\to\mathbb{C}italic_h : blackboard_C → blackboard_C be a function defined on the spectrum of A𝐴Aitalic_A. The task of quantum eigenvalue transformation is to encode the matrix h⁢(A):=𝒱⁢h⁢(D)⁢𝒱−1assignℎ𝐴𝒱ℎ𝐷superscript𝒱1h(A):=\mathcal{V}h(D)\mathcal{V}^{-1}italic_h ( italic_A ) := caligraphic_V italic_h ( italic_D ) caligraphic_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT (1) using a unitary matrix that can be efficiently implemented on a quantum computer, or to prepare a quantum state proportional to h⁢(A)⁢|ψ⟩ℎ𝐴ket𝜓h(A)\ket{\psi}italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ for an input state |ψ⟩ket𝜓\ket{\psi}| start_ARG italic_ψ end_ARG ⟩. The most well-known instance of this problem involves the linear time-independent differential equation d⁢u⁢(t)d⁢t=−A⁢u⁢(t),0≤t≤T.formulae-sequenced𝑢𝑡d𝑡𝐴𝑢𝑡0𝑡𝑇\frac{\,\mathrm{d}u(t)}{\,\mathrm{d}t}=-Au(t),\quad 0\leq t\leq T.divide start_ARG roman_d italic_u ( italic_t ) end_ARG start_ARG roman_d italic_t end_ARG = - italic_A italic_u ( italic_t ) , 0 ≤ italic_t ≤ italic_T . (2) The solution can be expressed as a matrix function u⁢(T)=e−A⁢T⁢u⁢(0)𝑢𝑇superscript𝑒𝐴𝑇𝑢0u(T)=e^{-AT}u(0)italic_u ( italic_T ) = italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT italic_u ( 0 ), so Eq. 2 can be solved by performing the matrix exponential e−A⁢T=𝒱⁢h⁢(D)⁢𝒱−1superscript𝑒𝐴𝑇𝒱ℎ𝐷superscript𝒱1e^{-AT}=\mathcal{V}h(D)\mathcal{V}^{-1}italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT = caligraphic_V italic_h ( italic_D ) caligraphic_V start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, which is an eigenvalue transformation with h⁢(z)=e−z⁢Tℎ𝑧superscript𝑒𝑧𝑇h(z)=e^{-zT}italic_h ( italic_z ) = italic_e start_POSTSUPERSCRIPT - italic_z italic_T end_POSTSUPERSCRIPT. Certain eigenvalue transformation problems, such as the matrix inverse A↦A−1maps-to𝐴superscript𝐴1A\mapsto A^{-1}italic_A ↦ italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, can be formulated as singular value transformation (SVT) problems.111Given a matrix A𝐴Aitalic_A with singular value decomposition A=V⁢Σ⁢W†𝐴𝑉Σsuperscript𝑊†A=V\Sigma W^{{\dagger}}italic_A = italic_V roman_Σ italic_W start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT, the singular value transformation with function hℎhitalic_h produces the matrix V⁢h⁢(Σ)⁢W†𝑉ℎΣsuperscript𝑊†Vh(\Sigma)W^{\dagger}italic_V italic_h ( roman_Σ ) italic_W start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT. The matrix inverse can be implemented by applying a singular value transformation on the hermitian conjugate of A𝐴Aitalic_A with function h⁢(x)=1/xℎ𝑥1𝑥h(x)=1/xitalic_h ( italic_x ) = 1 / italic_x, since (A†)−1=V⁢Σ−1⁢W†superscriptsuperscript𝐴†1𝑉superscriptΣ1superscript𝑊†(A^{{\dagger}})^{-1}=V\Sigma^{-1}W^{{\dagger}}( italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = italic_V roman_Σ start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_W start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT. This enables the usage of efficient quantum algorithms such as the quantum singular value transformation (QSVT) [GSLW19]. However, most eigenvalue transformations, including matrix exponentials for general matrices, cannot be reformulated in such a fashion. Other examples include powers of the matrix inverse A↦A−kmaps-to𝐴superscript𝐴𝑘A\mapsto A^{-k}italic_A ↦ italic_A start_POSTSUPERSCRIPT - italic_k end_POSTSUPERSCRIPT (k∈ℝ+𝑘subscriptℝk\in\mathbb{R}_{+}italic_k ∈ blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT), the exponential of the matrix inverse A↦e−A−1maps-to𝐴superscript𝑒superscript𝐴1A\mapsto e^{-A^{-1}}italic_A ↦ italic_e start_POSTSUPERSCRIPT - italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT, and the matrix sign function A↦sgn⁢(A−μ⁢I)maps-to𝐴sgn𝐴𝜇𝐼A\mapsto{\rm sgn}(A-\mu I)italic_A ↦ roman_sgn ( italic_A - italic_μ italic_I ) for a given μ∈ℝ𝜇ℝ\mu\in\mathbb{R}italic_μ ∈ blackboard_R, to name a few. Two primary resources should be accounted for in the cost of preparing h⁢(A)⁢|ψ⟩ℎ𝐴ket𝜓h(A)\ket{\psi}italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩: the number of queries to the matrix A𝐴Aitalic_A and the number of queries to the input state |ψ⟩ket𝜓\ket{\psi}| start_ARG italic_ψ end_ARG ⟩. 1.1 Related works We are aware of two strategies for implementing general quantum eigenvalue transformations. The first strategy is the contour integral-based formulation [TOSU21, TAWL21]. When the function hℎhitalic_h is analytic within a region enclosing all eigenvalues of A𝐴Aitalic_A in the complex plane, the contour integral technique can be described as an integral over matrix inverses. After discretization of the contour, the eigenvalue transformation h⁢(A)ℎ𝐴h(A)italic_h ( italic_A ) can be implemented using a linear combination of matrix inverses, where each matrix inverse can be implemented by solving a quantum linear system problem (QLSP), for which many algorithms including QSVT are available [HHL09, Amb12, CKS17, SSO19, LT20, AL22, CAS+22, Dal24, LS24b]. The second strategy for a general eigenvalue transformation uses Taylor expansion of h⁢(z)ℎ𝑧h(z)italic_h ( italic_z ) into a linear combination of monomials, and implements each matrix monomial using products of block encodings [GSLW19]. However, the monomial coefficients can be arbitrarily large even for well-behaved h⁢(z)ℎ𝑧h(z)italic_h ( italic_z ) such as Chebyshev polynomials, making the Taylor expansion approach highly inefficient. The recent development of quantum eigenvalue transformation (QEVT) by Low and Su [LS24a] overcomes this problem via a history state polynomial (HSP) using stable polynomial expansions. For instance, for a non-normal matrix A𝐴Aitalic_A with real eigenvalues within [−1,1]11[-1,1][ - 1 , 1 ], this approach introduces a Chebyshev history state to encode all Chebyshev polynomials up to a specified order simultaneously. When the eigenvalues are not real but lie within the unit circle, the method constructs a Faber polynomial approximation via a Faber polynomial history state. Using these history state polynomials, the eigenvalue transformation can be implemented again by solving a QLSP. Both the contour integral method and the history state polynomial method described above can be used to implement the matrix exponential e−A⁢Tsuperscript𝑒𝐴𝑇e^{-AT}italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT. However, the connection between the matrix exponential and the differential equation Eq. 2 allows us to employ special techniques for this particular matrix eigenvalue problem. One type of algorithm, called the linear system approach [Ber14, BCOW17, CL20, Kro23, BC22], involves discretizing the time interval [0,T]0𝑇[0,T][ 0 , italic_T ] into L𝐿Litalic_L shorter intervals of length Δ⁢t=T/LΔ𝑡𝑇𝐿\Delta t=T/Lroman_Δ italic_t = italic_T / italic_L. This method uses a short-time propagator to propagate from one time interval to another and formulates a dilated linear system to encode the history state across the interval [0,T]0𝑇[0,T][ 0 , italic_T ]. While the best existing algorithm of this type [BC22] may offer a near-optimal number of queries to A𝐴Aitalic_A, it does not optimize the initial state preparation cost since the construction of such algorithm uses the quantum linear system algorithm in [CAS+22] as a subroutine, which requires multiple copies of the initial state. The time-marching method [FLT23] also employs a short-time propagator as in the linear system approach and directly encodes the matrix exponential e−A⁢Tsuperscript𝑒𝐴𝑇e^{-AT}italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT by performing repeated postselections of the short-time propagators. It can achieve optimal state preparation cost, but the complexity with respect to the number of queries to A𝐴Aitalic_A is sub-optimal. Furthermore, to our knowledge, neither the linear system approach nor the time-marching approach is well-suited for general eigenvalue transformations since they heavily rely on discretizing differential equations in the form of Eq. 2. The linear combination of Hamiltonian simulation (LCHS) method [ALL23] significantly simplifies the process of constructing a block encoding e−A⁢Tsuperscript𝑒𝐴𝑇e^{-AT}italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT when the matrix −A𝐴-A- italic_A is dissipative, in the sense that the Hermitian part L=(A+A†)/2𝐿𝐴superscript𝐴†2L=(A+A^{{\dagger}})/2italic_L = ( italic_A + italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ) / 2 of the matrix A𝐴Aitalic_A is a positive semidefinite matrix. The core of LCHS is an identity expressing e−A⁢Tsuperscript𝑒𝐴𝑇e^{-AT}italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT as a linear combination of a continuously parameterized family of Hamiltonian simulation problems, which can then be implemented using any quantum Hamiltonian simulation algorithm without resorting to specific short-time integrators. It also achieves optimal state preparation cost for matrix exponentiation. A closely related approach for implementing e−A⁢Tsuperscript𝑒𝐴𝑇e^{-AT}italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT is the Schrödingerisation method [JLY22], which converts a non-unitary differential equation into a dilated Schrödinger equation with an additional momentum dimension, subject to specific initial conditions. The approaches in [ALL23, JLY22] are first-order schemes, and the matrix query complexity scales linearly with 1/ϵ1italic-ϵ1/\epsilon1 / italic_ϵ, where ϵitalic-ϵ\epsilonitalic_ϵ is the target precision. Recently, LCHS has been generalized into a family of identities for expressing e−A⁢Tsuperscript𝑒𝐴𝑇e^{-AT}italic_e start_POSTSUPERSCRIPT - italic_A italic_T end_POSTSUPERSCRIPT with exponentially improved accuracy [ACL23]. This led to the first quantum algorithm to block-encode matrix exponentials with both optimal state preparation cost and near-optimal scaling in matrix queries across all parameters, including the target precision ϵitalic-ϵ\epsilonitalic_ϵ and simulation time T𝑇Titalic_T. The central question of this work is as follows: Can the linear combination of Hamiltonian simulation method be generalized to efficiently represent matrix eigenvalue transformations beyond matrix exponentiation? 1.2 Contribution and main idea In this work, we propose the following type of eigenvalue transformation. Let h⁢(z)ℎ𝑧h(z)italic_h ( italic_z ) be a function represented as the Laplace transform of a function g⁢(t)∈L1⁢(ℝ+)𝑔𝑡superscript𝐿1subscriptℝ{g}(t)\in L^{1}(\mathbb{R}_{+})italic_g ( italic_t ) ∈ italic_L start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ): h⁢(z)=∫0∞g⁢(t)⁢e−z⁢t⁢dt,Re⁡z≥0.formulae-sequenceℎ𝑧superscriptsubscript0𝑔𝑡superscript𝑒𝑧𝑡differential-d𝑡Re𝑧0h(z)=\int_{0}^{\infty}{g}(t)e^{-zt}\,\mathrm{d}t,\quad\operatorname{Re}z\geq 0.italic_h ( italic_z ) = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT italic_g ( italic_t ) italic_e start_POSTSUPERSCRIPT - italic_z italic_t end_POSTSUPERSCRIPT roman_d italic_t , roman_Re italic_z ≥ 0 . (3) If −A𝐴-A- italic_A is dissipative (i.e., all the eigenvalues of A𝐴Aitalic_A are in the right half-plane), then the eigenvalue transformation h⁢(A)ℎ𝐴h(A)italic_h ( italic_A ) takes the form h⁢(A)=∫0∞g⁢(t)⁢e−A⁢t⁢dt.ℎ𝐴superscriptsubscript0𝑔𝑡superscript𝑒𝐴𝑡differential-d𝑡h(A)=\int_{0}^{\infty}{g}(t)e^{-At}\,\mathrm{d}t.italic_h ( italic_A ) = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT italic_g ( italic_t ) italic_e start_POSTSUPERSCRIPT - italic_A italic_t end_POSTSUPERSCRIPT roman_d italic_t . (4) This is a linear combination of matrix exponentials, and each matrix exponential e−A⁢tsuperscript𝑒𝐴𝑡e^{-At}italic_e start_POSTSUPERSCRIPT - italic_A italic_t end_POSTSUPERSCRIPT can be further written as a linear combination of Hamiltonian simulation problems using the technique in [ACL23]. As a result, h⁢(A)ℎ𝐴h(A)italic_h ( italic_A ) is formulated as a double integral of continuously parameterized Hamiltonian simulation problems. We call this the Laplace transform based linear combination of Hamiltonian simulations (Lap-LCHS). Result 1 (Informal version of 4). Let A=L+i⁢H𝐴𝐿𝑖𝐻A=L+iHitalic_A = italic_L + italic_i italic_H, where L=A+A†2⪰0𝐿𝐴superscript𝐴†2succeeds-or-equals0L=\frac{A+A^{{\dagger}}}{2}\succeq 0italic_L = divide start_ARG italic_A + italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG ⪰ 0 and H=A−A†2⁢i𝐻𝐴superscript𝐴†2𝑖H=\frac{A-A^{{\dagger}}}{2i}italic_H = divide start_ARG italic_A - italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_i end_ARG are both Hermitian matrices. For a function h⁢(z)=∫0∞g⁢(t)⁢e−z⁢t⁢dtℎ𝑧superscriptsubscript0𝑔𝑡superscript𝑒𝑧𝑡differential-d𝑡h(z)=\int_{0}^{\infty}{g}(t)e^{-zt}\,\mathrm{d}titalic_h ( italic_z ) = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT italic_g ( italic_t ) italic_e start_POSTSUPERSCRIPT - italic_z italic_t end_POSTSUPERSCRIPT roman_d italic_t that can be expressed by a Laplace transformation with g⁢(t)𝑔𝑡g(t)italic_g ( italic_t ) being its inverse Laplace transform, we have h⁢(A)=∫0∞∫ℝf⁢(k)⁢g⁢(t)1−i⁢k⁢e−i⁢t⁢(k⁢L+H)⁢dk⁢dt.ℎ𝐴superscriptsubscript0subscriptℝ𝑓𝑘𝑔𝑡1𝑖𝑘superscript𝑒𝑖𝑡𝑘𝐿𝐻differential-d𝑘differential-d𝑡h(A)=\int_{0}^{\infty}\int_{\mathbb{R}}\frac{f(k){g}(t)}{1-ik}e^{-it(kL+H)}\,% \mathrm{d}k\,\mathrm{d}t.italic_h ( italic_A ) = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ∫ start_POSTSUBSCRIPT blackboard_R end_POSTSUBSCRIPT divide start_ARG italic_f ( italic_k ) italic_g ( italic_t ) end_ARG start_ARG 1 - italic_i italic_k end_ARG italic_e start_POSTSUPERSCRIPT - italic_i italic_t ( italic_k italic_L + italic_H ) end_POSTSUPERSCRIPT roman_d italic_k roman_d italic_t . (5) Here222In fact there exists a large family of kernel functions f⁢(k)𝑓𝑘f(k)italic_f ( italic_k ) that can make 1 hold (see 4). The one specified here is nearly optimal in the sense that it achieves almost the fastest poossible asymptotic decay along the real axis [ACL23, Proposition 7]. f⁢(k)=12⁢π⁢e−2β⁢e(1+i⁢z)β𝑓𝑘12𝜋superscript𝑒superscript2𝛽superscript𝑒superscript1𝑖𝑧𝛽f(k)=\frac{1}{2\pi e^{-2^{\beta}}e^{(1+iz)^{\beta}}}italic_f ( italic_k ) = divide start_ARG 1 end_ARG start_ARG 2 italic_π italic_e start_POSTSUPERSCRIPT - 2 start_POSTSUPERSCRIPT italic_β end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT ( 1 + italic_i italic_z ) start_POSTSUPERSCRIPT italic_β end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT end_ARG, β∈(0,1)𝛽01\beta\in(0,1)italic_β ∈ ( 0 , 1 ). Using 1, we construct an efficient quantum algorithm to implement the matrix function h⁢(A)ℎ𝐴h(A)italic_h ( italic_A ). We truncate and discretize the double integral in Eq. 5, approximating h⁢(A)ℎ𝐴h(A)italic_h ( italic_A ) by a weighted sum of Hamiltonian simulation operators e−i⁢t⁢(k⁢L+H)superscript𝑒𝑖𝑡𝑘𝐿𝐻e^{-it(kL+H)}italic_e start_POSTSUPERSCRIPT - italic_i italic_t ( italic_k italic_L + italic_H ) end_POSTSUPERSCRIPT for some specific choices of t𝑡titalic_t and k𝑘kitalic_k. Then our quantum Lap-LCHS algorithm implements each Hamiltonian simulation operator by the optimal time-independent Hamiltonian simulation algorithm based on QSVT [GSLW19], and performs the weighted sum using the linear combination of unitaries (LCU) technique [CW12, CKS17]. We establish a detailed complexity estimate for the Lap-LCHS algorithm. Here we present the cost for preparing the normalized state h⁢(A)⁢|ψ⟩∥h⁢(A)⁢|ψ⟩∥ℎ𝐴ket𝜓delimited-∥∥ℎ𝐴ket𝜓\frac{h(A)\ket{\psi}}{\mathopen{}\mathclose{{}\left\lVert h(A)\ket{\psi}}% \right\rVert}divide start_ARG italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ end_ARG start_ARG ∥ italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ ∥ end_ARG. We also analyze the cost of block encoding h⁢(A)ℎ𝐴h(A)italic_h ( italic_A ) in 6. Result 2 (Informal version of 7). Let A=L+i⁢H𝐴𝐿𝑖𝐻A=L+iHitalic_A = italic_L + italic_i italic_H, where L=A+A†2⪰0𝐿𝐴superscript𝐴†2succeeds-or-equals0L=\frac{A+A^{{\dagger}}}{2}\succeq 0italic_L = divide start_ARG italic_A + italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT end_ARG start_ARG 2 end_ARG ⪰ 0 and H=A−A†2⁢i𝐻𝐴superscript𝐴†2𝑖H=\frac{A-A^{{\dagger}}}{2i}italic_H = divide start_ARG italic_A - italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_i end_ARG are both Hermitian matrices. For a given function h⁢(z)ℎ𝑧h(z)italic_h ( italic_z ), we can prepare the state h⁢(A)⁢|ψ⟩/∥h⁢(A)⁢|ψ⟩∥ℎ𝐴ket𝜓delimited-∥∥ℎ𝐴ket𝜓h(A)\ket{\psi}/\mathopen{}\mathclose{{}\left\lVert h(A)\ket{\psi}}\right\rVertitalic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ / ∥ italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ ∥ with error at most ϵitalic-ϵ\epsilonitalic_ϵ using 𝒪~⁢(∥g∥L1⁢(ℝ+)∥h⁢(A)⁢|ψ⟩∥⁢(αA⁢T⁢(log⁡(1ϵ))1+o⁢(1)))~𝒪subscriptdelimited-∥∥𝑔superscript𝐿1subscriptℝdelimited-∥∥ℎ𝐴ket𝜓subscript𝛼𝐴𝑇superscript1italic-ϵ1𝑜1\widetilde{\mathcal{O}}\mathopen{}\mathclose{{}\left(\frac{\mathopen{}% \mathclose{{}\left\lVert{g}}\right\rVert_{L^{1}(\mathbb{R}_{+})}}{\mathopen{}% \mathclose{{}\left\lVert h(A)\ket{\psi}}\right\rVert}\mathopen{}\mathclose{{}% \left(\alpha_{A}T\mathopen{}\mathclose{{}\left(\log\mathopen{}\mathclose{{}% \left(\frac{1}{\epsilon}}\right)}\right)^{1+o(1)}}\right)}\right)over~ start_ARG caligraphic_O end_ARG ( divide start_ARG ∥ italic_g ∥ start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT end_ARG start_ARG ∥ italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ ∥ end_ARG ( italic_α start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT italic_T ( roman_log ( divide start_ARG 1 end_ARG start_ARG italic_ϵ end_ARG ) ) start_POSTSUPERSCRIPT 1 + italic_o ( 1 ) end_POSTSUPERSCRIPT ) ) queries to the matrix A𝐴Aitalic_A and 𝒪⁢(∥g∥L1⁢(ℝ+)∥h⁢(A)⁢|ψ⟩∥)𝒪subscriptdelimited-∥∥𝑔superscript𝐿1subscriptℝdelimited-∥∥ℎ𝐴ket𝜓\mathcal{O}\mathopen{}\mathclose{{}\left(\frac{\mathopen{}\mathclose{{}\left% \lVert{g}}\right\rVert_{L^{1}(\mathbb{R}_{+})}}{\mathopen{}\mathclose{{}\left% \lVert h(A)\ket{\psi}}\right\rVert}}\right)caligraphic_O ( divide start_ARG ∥ italic_g ∥ start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT end_ARG start_ARG ∥ italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ ∥ end_ARG ) queries to the input state |ψ⟩ket𝜓\ket{\psi}| start_ARG italic_ψ end_ARG ⟩. Here αA≥∥A∥subscript𝛼𝐴delimited-∥∥𝐴\alpha_{A}\geq\mathopen{}\mathclose{{}\left\lVert A}\right\rVertitalic_α start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ≥ ∥ italic_A ∥, g⁢(t)𝑔𝑡g(t)italic_g ( italic_t ) is the inverse Laplace transform of h⁢(z)ℎ𝑧h(z)italic_h ( italic_z ), and T𝑇Titalic_T is a truncation parameter such that ∥g∥L1⁢((T,∞))=𝒪⁢(ϵ⁢∥h⁢(A)⁢|ψ⟩∥)subscriptdelimited-∥∥𝑔superscript𝐿1𝑇𝒪italic-ϵdelimited-∥∥ℎ𝐴ket𝜓\mathopen{}\mathclose{{}\left\lVert{g}}\right\rVert_{L^{1}((T,\infty))}=% \mathcal{O}(\epsilon\mathopen{}\mathclose{{}\left\lVert h(A)\ket{\psi}}\right\rVert)∥ italic_g ∥ start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( ( italic_T , ∞ ) ) end_POSTSUBSCRIPT = caligraphic_O ( italic_ϵ ∥ italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ ∥ ). In 2, the query complexity to the matrix A𝐴Aitalic_A depends linearly on the truncation parameter T𝑇Titalic_T, which depends implicitly on the error ϵitalic-ϵ\epsilonitalic_ϵ. Specifically, T𝑇Titalic_T is chosen such that the integral ∫T∞|g⁢(t)|⁢dtsuperscriptsubscript𝑇𝑔𝑡differential-d𝑡\int_{T}^{\infty}|g(t)|\,\mathrm{d}t∫ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT | italic_g ( italic_t ) | roman_d italic_t is bounded by 𝒪⁢(ϵ⁢∥h⁢(A)⁢|ψ⟩∥)𝒪italic-ϵdelimited-∥∥ℎ𝐴ket𝜓\mathcal{O}(\epsilon\mathopen{}\mathclose{{}\left\lVert h(A)\ket{\psi}}\right\rVert)caligraphic_O ( italic_ϵ ∥ italic_h ( italic_A ) | start_ARG italic_ψ end_ARG ⟩ ∥ ). This may introduce an additional polynomial computational overhead with respect to ϵitalic-ϵ\epsilonitalic_ϵ if g⁢(t)𝑔𝑡g(t)italic_g ( italic_t ) decays only polynomially as t→∞→𝑡t\rightarrow\inftyitalic_t → ∞. Consequently, the overall matrix query complexity with respect to the error should be carefully analyzed on a case-by-case basis. The contour integral method, QEVT with history state polynomial, and Lap-LCHS each have different ranges of applicability where they are most efficient, so it is difficult to provide a succinct general comparison of these methods. As a rule of thumb, the contour integral method may require a part of the contour to be in the left half of the complex plane, so the cost can grow exponentially in T𝑇Titalic_T for h⁢(z)ℎ𝑧h(z)italic_h ( italic_z ) of the form in Eq. 3. According to [LS24a, Proposition 34], QEVT also requires the Hermitian part L𝐿Litalic_L of the matrix A𝐴Aitalic_A to be positive semi-definite, just as Lap-LCHS does. However, it may be possible to implement a broader range of transformations using QEVT, as it only requires the complex analyticity of the function h⁢(z)ℎ𝑧h(z)italic_h ( italic_z ) over a bounded domain and does not rely on any assumptions regarding the existence of its inverse Laplace transform. Both Lap-LCHS and QEVT can achieve near-optimal complexity with respect to the number of queries to A𝐴Aitalic_A, but Lap-LCHS has lower state preparation cost. Specifically, Lap-LCHS achieves the optimal state preparation cost, while the cost of QEVT involves additional logarithmic overhead in terms of the polynomial degree, even when combined with the recently developed tunable variable time amplitude amplification (VTAA) and block preconditioning techniques [LS24b]. Lap-LCHS is also simpler and might be more feasible in the early fault-tolerant regime by implementing the linear combination stochastically [WMB24, Cha24], while it remains unclear how to implement QEVT with near-optimal scaling on intermediate-term quantum computers due to the complexity of VTAA. To demonstrate the computational advantage of Lap-LCHS over other methods for performing eigenvalue transformations, we consider several specific instances. For all problems considered in this work, we find that Lap-LCHS achieves the best known complexity both in terms of the number of queries to A𝐴Aitalic_A and the initial state preparation oracle. The first example is the matrix function h⁢(z)=1z⁢(1−e−z⁢T)=∫0Te−z⁢s⁢ds,ℎ𝑧1𝑧1superscript𝑒𝑧𝑇superscriptsubscript0𝑇superscript𝑒𝑧𝑠differential-d𝑠h(z)=\frac{1}{z}(1-e^{-zT})=\int_{0}^{T}e^{-zs}\,\mathrm{d}s,italic_h ( italic_z ) = divide start_ARG 1 end_ARG start_ARG italic_z end_ARG ( 1 - italic_e start_POSTSUPERSCRIPT - italic_z italic_T end_POSTSUPERSCRIPT ) = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - italic_z italic_s end_POSTSUPERSCRIPT roman_d italic_s , (6) so that g⁢(s)𝑔𝑠g(s)italic_g ( italic_s ) is an indicator function on the interval [0,T]0𝑇[0,T][ 0 , italic_T ]. This corresponds to the simulation of a special inhomogeneous differential equation d⁢u⁢(t)d⁢t=−A⁢u⁢(t)+|ψ⟩,u⁢(0)=0,0≤t≤T.formulae-sequenced𝑢𝑡d𝑡𝐴𝑢𝑡ket𝜓formulae-sequence𝑢000𝑡𝑇\frac{\,\mathrm{d}u(t)}{\,\mathrm{d}t}=-Au(t)+\ket{\psi},\quad u(0)=0,\quad 0% \leq t\leq T.divide start_ARG roman_d italic_u ( italic_t ) end_ARG start_ARG roman_d italic_t end_ARG = - italic_A italic_u ( italic_t ) + | start_ARG italic_ψ end_ARG ⟩ , italic_u ( 0 ) = 0 , 0 ≤ italic_t ≤ italic_T . (7) Generalizing the homogeneous equation in Eq. 2, the inhomogeneous equation can model an external driving force. We apply the Lap-LCHS algorithm and find that in this case it is equivalent to combining the standard LCHS and variation of constants. This example is presented in Section 4.1. The second example is the power of matrix inverses h⁢(z)=(η+z)−pℎ𝑧superscript𝜂𝑧𝑝h(z)=(\eta+z)^{-p}italic_h ( italic_z ) = ( italic_η + italic_z ) start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT (8) for some η>0,p>0formulae-sequence𝜂0𝑝0\eta>0,p>0italic_η > 0 , italic_p > 0. This transformation is a generalization of the standard matrix inverse, and the power p𝑝pitalic_p does not need to be an integer. Fractional powers of matrices arise in certain Markov chain models and fractional differential equations [HL11]. Here the matrix function h⁢(A)=(η⁢I+A)−pℎ𝐴superscript𝜂𝐼𝐴𝑝h(A)=(\eta I+A)^{-p}italic_h ( italic_A ) = ( italic_η italic_I + italic_A ) start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT involves a shift by η𝜂\etaitalic_η. Notice that we can implement A−psuperscript𝐴𝑝A^{-p}italic_A start_POSTSUPERSCRIPT - italic_p end_POSTSUPERSCRIPT if all the eigenvalues of A+A†𝐴superscript𝐴†A+A^{\dagger}italic_A + italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT are positive. Then we can choose η𝜂\etaitalic_η to be the smallest eigenvalue of (A+A†)/2𝐴superscript𝐴†2(A+A^{\dagger})/2( italic_A + italic_A start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ) / 2 and consider the matrix function h⁢(A−η⁢I)ℎ𝐴𝜂𝐼h(A-\eta I)italic_h ( italic_A - italic_η italic_I ). This example is presented in Section 4.2. The third example is the simulation of a linear differential equation with a non-diagonal mass matrix: A⁢d⁢u⁢(t)d⁢t=−u⁢(t),0≤t≤T.formulae-sequence𝐴d𝑢𝑡d𝑡𝑢𝑡0𝑡𝑇A\frac{\,\mathrm{d}u(t)}{\,\mathrm{d}t}=-u(t),\quad 0\leq t\leq T.italic_A divide start_ARG roman_d italic_u ( italic_t ) end_ARG start_ARG roman_d italic_t end_ARG = - italic_u ( italic_t ) , 0 ≤ italic_t ≤ italic_T . (9) We consider two types of initial conditions, namely u⁢(0)=A−1⁢u0,andu⁢(0)=u0.formulae-sequence𝑢0superscript𝐴1subscript𝑢0and𝑢0subscript𝑢0u(0)=A^{-1}u_{0},\quad\mbox{and}\quad u(0)=u_{0}.italic_u ( 0 ) = italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and italic_u ( 0 ) = italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . (10) In both cases, we are given an oracle for preparing the state |u0⟩=u0/∥u0∥ketsubscript𝑢0subscript𝑢0delimited-∥∥subscript𝑢0\ket{u_{0}}=u_{0}/\mathopen{}\mathclose{{}\left\lVert u_{0}}\right\rVert| start_ARG italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ⟩ = italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / ∥ italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∥. Equation 9 has applications to evolutionary partial differential equations with time-space mixed derivatives in the form of ∂tℒx⁢u⁢(t,x)=−u⁢(t,x),subscript𝑡subscriptℒ𝑥𝑢𝑡𝑥𝑢𝑡𝑥\partial_{t}\mathcal{L}_{x}u(t,x)=-u(t,x),∂ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT italic_u ( italic_t , italic_x ) = - italic_u ( italic_t , italic_x ) , (11) where ℒx=∑k=0mak⁢∂xksubscriptℒ𝑥superscriptsubscript𝑘0𝑚subscript𝑎𝑘superscriptsubscript𝑥𝑘\mathcal{L}_{x}=\sum_{k=0}^{m}a_{k}\partial_{x}^{k}caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT = ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∂ start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT is a differential operator with respect to the spatial variable x𝑥xitalic_x. We can discretize the spatial variable to obtain a semi-discretized partial differential equation in the form of Eq. 9, where A𝐴Aitalic_A is the discrete version of ℒxsubscriptℒ𝑥\mathcal{L}_{x}caligraphic_L start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT. We apply the Lap-LCHS algorithm to Eq. 9 and analyze its complexity in Section 4.3. Remarkably, Lap-LCHS can solve Eq. 9 (i.e., implement the transformation e−T⁢A−1superscript𝑒𝑇superscript𝐴1e^{-TA^{-1}}italic_e start_POSTSUPERSCRIPT - italic_T italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT) without explicitly inverting the matrix A𝐴Aitalic_A, and achieves the best known complexity both in terms of the number of queries to A𝐴Aitalic_A and the initial state preparation oracle. The last example is the second-order differential equation d2⁢ud⁢t2=A⁢u,u⁢(0)=|u0⟩,u′⁢(0)=v0,formulae-sequencesuperscriptd2𝑢dsuperscript𝑡2𝐴𝑢formulae-sequence𝑢0ketsubscript𝑢0superscript𝑢′0subscript𝑣0\frac{\,\mathrm{d}^{2}u}{\,\mathrm{d}t^{2}}=Au,\quad u(0)=\ket{u_{0}},\quad u^% {\prime}(0)=v_{0},divide start_ARG roman_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u end_ARG start_ARG roman_d italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = italic_A italic_u , italic_u ( 0 ) = | start_ARG italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ⟩ , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( 0 ) = italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , (12) which has many applications such as wave equations. These second-order differential equations have a branch of the solution that grows in time. We apply Lap-LCHS to simulate the decaying branch of the solution. Note that if we first convert the second-order differential equation into a set of first-order differential equations and apply the matrix exponential solver, the cost will be dominated by the growing solution. Similarly to the first-order case, we may also consider the second-order differential equation with a non-diagonal mass matrix A⁢d2⁢ud⁢t2=u,u⁢(0)=|u0⟩,u′⁢(0)=v0.formulae-sequence𝐴superscriptd2𝑢dsuperscript𝑡2𝑢formulae-sequence𝑢0ketsubscript𝑢0superscript𝑢′0subscript𝑣0A\frac{\,\mathrm{d}^{2}u}{\,\mathrm{d}t^{2}}=u,\quad u(0)=\ket{u_{0}},\quad u^% {\prime}(0)=v_{0}.italic_A divide start_ARG roman_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u end_ARG start_ARG roman_d italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG = italic_u , italic_u ( 0 ) = | start_ARG italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG ⟩ , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( 0 ) = italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . (13) The results are presented in Section 4.4. 1.3 Discussion Despite significant recent advances in quantum algorithms for linear algebraic transformations, research on eigenvalue transformations for non-normal matrices remains in a relatively early stage. Consider for example Hamiltonian simulation, which is obviously an eigenvalue transformation problem. However, an efficient implementation of e−i⁢H⁢tsuperscript𝑒𝑖𝐻𝑡e^{-iHt}italic_e start_POSTSUPERSCRIPT - italic_i italic_H italic_t end_POSTSUPERSCRIPT using the contour integral method or the history state polynomial method is far from obvious, while Lap-LCHS simply reduces to the Hamiltonian simulation problem itself by construction. To our knowledge, when −A𝐴-A- italic_A satisfies the dissipative condition, Lap-LCHS obtains the lowest complexity among existing approaches for eigenvalue transformations. However, a central unifying structure akin to the qubitization framework that underpins QSVT and prior developments in singular value transformations has yet to be clearly identified for the eigenvalue transformation of non-normal matrices. Establishing such a unifying perspective could greatly enhance our understanding of quantum algorithms for matrix computation. 1.4 Organization The rest of this paper is organized as follows. First, in Section 2, we review two existing approaches for special eigenvalue transformations: QSVT for Hermitian matrix functions and LCHS for matrix exponentials. In Section 3, we establish the framework of Lap-LCHS, discuss the construction of our quantum algorithm, and present its complexity analysis. Then we discuss the applications of the Lap-LCHS algorithm in Section 4. Acknowledgements DA acknowledges support from the Department of Defense through the Hartree Postdoctoral Fellowship at QuICS. AMC acknowledges support from the Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Accelerated Research in Quantum Computing program, and from the National Science Foundation (NSF) through QLCI grant OMA-2120757. LL is a Simons investigator in Mathematics, and acknowledges support from the Challenge Institute for Quantum Computation (CIQC) funded by NSF through grant number OMA-2016245. LY acknowledges support from NSF grant DMS-2208163. LL and LY also acknowledge support from the U.S. Department of Energy, Office of Science, Accelerated Research in Quantum Computing Centers, Quantum Utility through Advanced Computational Quantum Algorithms, grant no. DE-SC0025572."
https://arxiv.org/html/2411.03929v1,Inexact block LU preconditioners for incompressible fluids with flow rate conditions,"When studying the dynamics of incompressible fluids in bounded domains the only available data often provide average flow rate conditions on portions of the domain’s boundary. In engineering applications a common practice to complete these conditions is to prescribe a Dirichlet condition by assuming a-priori a spatial profile for the velocity field. However, this strongly influence the accuracy of the numerical solution. A more mathematically sound approach is to prescribe the flow rate conditions using Lagrange multipliers, resulting in an augmented weak formulation of the Navier-Stokes problem.In this paper we start from the SIMPLE preconditioner, introduced so far for the standard Navier-Stokes equations, and we derive two preconditioners for the monolithic solution of the augmented problem. This can be useful in complex applications where splitting the computation of the velocity/pressure and Lagrange multipliers numerical solutions can be very expensive. In particular, we investigate the numerical performance of the preconditioners in both idealized and real-life scenarios. Finally, we highlight the advantages of treating flow rate conditions with a Lagrange multipliers approach instead of prescribing a Dirichlet condition.","Incompressible Navier-Stokes equations are commonly used in engineering applications to study the dynamics of viscous fluids. Given a bounded domain Ω⊂ℝ3Ωsuperscriptℝ3\Omega\subset\mathbb{R}^{3}roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT, classical boundary conditions impose three (one for each spatial dimension) point-wise data on the domain’s boundary ∂ΩΩ\partial\Omega∂ roman_Ω, typically prescribing the components of the velocity field (Dirichlet condition), of the Cauchy normal stress (Neumann condition), or a combination of the two (Robin condition). On the other hand, in some applications the only information available provides average conditions which are not enough to close the problem and thus need to be completed [10]. We will refer to them as defective boundary conditions. Some examples arise in blood-dynamics simulations, where clinical measures often provide information on either the flow rate or the mean pressure [40, 45], and flow in pipes simulations, where sensors measure the fluid flow rate [7, 37]. Moreover, in the geometric multiscale approach [38], where mathematical models with different spatial dimensions are coupled, the information provided by the lower-dimensional problem is not enough to close the higher-dimensional one [6, 32]. Regarding defective flow rate conditions, a widely employed strategy to close the problem, thanks to its practicality, is to prescribe a Dirichlet condition by assuming a-priori a spatial profile for the velocity field. In this case, the computational domain is often extended to reduce the impact of the chosen profile on the accuracy of the numerical solution. A more mathematically sound approach was proposed in [27], where the authors introduced a suitable variational formulation of the problem which includes the given flow rate data. However, this approach requires the definition of non-standard finite-dimensional subspaces which makes it problematic to implement in practice [46]. Hence, several alternative approaches have been proposed in literature [23] based on Lagrange multipliers [20, 45], control theory [21, 31], penalization technique [51] and the Nitsche method [29, 47]. The Lagrange multipliers approach was proposed in [20] and applied for a quasi-Newtonian Stokes problem [16], in a fluid-structure interaction framework [22] and in practical hemodynamic problems with patient-specific geometries [48, 11]. In this approach, flow rate boundary conditions are considered as a constraint for the solution and enforced using Lagrange multipliers, resulting in an augmented weak formulation of the Navier-Stokes problem. The problem is closed by assuming that, on the considered portion of the domain’s boundary, the Cauchy normal stress has zero tangential components and its normal component is constant in space. The resulting augmented problem can be solved by splitting the computation of the velocity/pressure fields and of the Lagrange multipliers in order to resort to available standard solvers for the solution of the Navier-Stokes step [45, 46]. However, iterative procedures based on this splitting can be very expensive in complex applications. In the recent work [28], the authors proposed monolithic block preconditioners based on inexact factorizations to deal with defective conditions arising from the coupling with lumped parameter models. In order to efficiently solve the augmented Finite Elements system, in this paper we consider a monolithic strategy and we propose a suitable block preconditioner for its efficient solution. Specifically, we start from the SIMPLE iterative solution strategy [35], studied as a preconditioner in [36, 33, 14, 15], and we extend it to the augmented flow rate defective case. To test the effectiveness of our proposal, we present several numerical results both in idealized and real-life scenarios. The outline of the paper is as follows: we provide a review of the augmented Navier-Stokes flow rate problem (Section 2) and of the SIMPLE preconditioner (Section 3); we propose an extension of the SIMPLE preconditioner (Section 4), discussing its formulation (Section 4.1) and introducing an efficient variant used in practice (Section 4.2); finally we provide some numerical results (Section 5), presenting the setting of our numerical experiments which involves a trivial extension of the SIMPLE preconditioner (Section 5.1), testing the performance of the proposed preconditioners for a varying number of Lagrange multipliers (Section 5.2) and in real-life hemodynamic scenarios (Section 5.3), and highlighting the advantages of treating defective flow rate condition with a Lagrange multipliers approach (Section 5.4)."
https://arxiv.org/html/2411.03599v1,Structure-preserving quantum algorithms for linear and nonlinear Hamiltonian systems,"Hamiltonian systems of ordinary and partial differential equations are fundamental across modern science and engineering, appearing in models that span virtually all physical scales. A critical property for the robustness and stability of computational methods in such systems is the symplectic structure, which preserves geometric properties like phase-space volume over time and energy conservation over an extended period. In this paper, we present quantum algorithms that incorporate symplectic integrators, ensuring the preservation of this key structure. We demonstrate how these algorithms maintain the symplectic properties for both linear and nonlinear Hamiltonian systems. Additionally, we provide a comprehensive theoretical analysis of the computational complexity, showing that our approach offers both accuracy and improved efficiency over classical algorithms. These results highlight the potential application of quantum algorithms for solving large-scale Hamiltonian systems while preserving essential physical properties.","Hamiltonian systems are foundational across various physical scales, from electron dynamics to the motion of atoms and molecules, and even to macroscopic models in continuum mechanics [31]. These systems are central to modern statistical mechanics [5, 32]. Moreover, Hamiltonian systems encompass numerous well-known examples across diverse scientific disciplines: from the Schrödinger equation governing electron dynamics in quantum mechanics to the cubic Schrödinger equation describing Bose-Einstein condensation; from Newton’s equations of motion in classical mechanics to Euler’s equation and the Korteweg–de Vries equation in fluid mechanics. Furthermore, Hamiltonian dynamics finds applications in elastodynamics within solid mechanics, Lotka-Volterra equations modeling population dynamics, and even the complex motions of celestial bodies. This ubiquity highlights the pivotal role of Hamiltonian dynamics in understanding natural phenomena across multiple scales and fields of study. Further, the underlying variational and geometric structures have given rise to significant advancements in mathematical theory. Mathematically, Hamiltonian systems are governed by the system of ODEs, involving generalized coordinates and momenta (𝒒(t),𝒑(t)∈ℝ2⁢d(\bm{q}(t),\bm{p}(t)\in\mathbb{R}^{2d}( bold_italic_q ( italic_t ) , bold_italic_p ( italic_t ) ∈ blackboard_R start_POSTSUPERSCRIPT 2 italic_d end_POSTSUPERSCRIPT {dd⁢t⁢𝒒=∇𝒑H,dd⁢t⁢𝒑=−∇𝒒H,\left\{\begin{aligned} \frac{d}{dt}\bm{q}=&\nabla_{\bm{p}}H,\\ \frac{d}{dt}\bm{p}=&-\nabla_{\bm{q}}H,\end{aligned}\right.{ start_ROW start_CELL divide start_ARG italic_d end_ARG start_ARG italic_d italic_t end_ARG bold_italic_q = end_CELL start_CELL ∇ start_POSTSUBSCRIPT bold_italic_p end_POSTSUBSCRIPT italic_H , end_CELL end_ROW start_ROW start_CELL divide start_ARG italic_d end_ARG start_ARG italic_d italic_t end_ARG bold_italic_p = end_CELL start_CELL - ∇ start_POSTSUBSCRIPT bold_italic_q end_POSTSUBSCRIPT italic_H , end_CELL end_ROW (1) where H⁢(𝒒,𝒑)𝐻𝒒𝒑H(\bm{q},\bm{p})italic_H ( bold_italic_q , bold_italic_p ) is known as the Hamiltonian. Hamiltonian dynamics, a cornerstone of classical physics, provides a powerful framework for describing a wide array of physical phenomena. The transition from a Hamiltonian H𝐻Hitalic_H to the ordinary differential equations (ODEs) in Eq. 1 embodies the celebrated Hamilton’s principle, equivalent to the fundamental least-action principle. This elegant formulation has been extended to infinite-dimensional systems, including partial differential equations (PDEs), with many further applications. The simulation of large-scale Hamiltonian systems remains a critical focus in scientific computing. A significant advancement in numerical algorithms is the development of symplectic integrators, which preserve the underlying geometric structure of these systems. Notably, symplectic integrators maintain energy accuracy over extended time periods, a crucial feature for long-term simulations. In contrast, conventional methods such as Taylor approximations and explicit Runge-Kutta schemes often lead to energy drift, compromising both the physical fidelity and long-term accuracy of the simulations. This preservation of geometric structures and essential physical properties underscores the importance of symplectic integrators in accurately modeling complex Hamiltonian systems. Thus the development of symplectic integrators has become a milestone in modern computational mathematics [21, 29, 23]. To illustrate the significance of symplectic integrators, Fig. 1 presents a comparative analysis of total energy conservation in a large-scale particle system. The simulation, conducted over 100,000 time steps, contrasts the performance of a symplectic integrator (Verlet’s method) against a standard Runge-Kutta method. The results vividly demonstrate the superior energy conservation properties of the symplectic approach. Figure 1: Total energy of a Lennard-Jones system with 2048 particles integrated in time for 100,000 steps. Quantum algorithms have shown remarkable potential for efficiently simulating high-dimensional systems. Notably, Hamiltonian simulation algorithms can model the unitary dynamics of the Schrödinger equation with exponential speedup relative to system dimension. This quantum advantage has been extended to classical systems of harmonic oscillators [4] and the acoustic wave equation [18], which are also Hamiltonian systems. Furthermore, quantum algorithms have been developed for simulating both linear and nonlinear ordinary differential equations (ODEs). Meanwhile, existing quantum algorithms have not explicitly addressed the symplectic structure inherent to Hamiltonian systems. This paper addresses this gap by incorporating symplectic integrators into quantum algorithms and analyzing how the geometric structure impacts computational complexity. It is important to emphasize that while existing works have largely concentrated on optimizing the query and gate complexity of quantum algorithms, preserving the fundamental physical properties—similar to the evolution of classical simulation algorithms—remains equally important. When resources are insufficient to reach the desired precision, the robustness of these algorithms becomes increasingly critical in ensuring reliable outcomes. thp Figure 2: A schematic illustration of the Carleman embedding for Hamiltonian systems. For linear Hamiltonian dynamics, we incorporate the Runge-Kutta Gauss methods [19] which automatically satisfies the symplectic property, while having the optimal order of accuracy for a fixed stage number. Theorem 1.1. Assume that 𝐱⁢(t)𝐱𝑡\bm{x}(t)bold_italic_x ( italic_t ) is the solution of a linear stable Hamiltonian system: 𝐱′=K⁢𝐱superscript𝐱′𝐾𝐱\bm{x}^{\prime}=K\bm{x}bold_italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_K bold_italic_x. The quantum algorithm produces a quantum state |ψ⟩=𝐱M‖𝐱M‖ket𝜓subscript𝐱𝑀normsubscript𝐱𝑀\ket{\psi}=\frac{\bm{x}_{M}}{\norm{\bm{x}_{M}}}| start_ARG italic_ψ end_ARG ⟩ = divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_ARG start_ARG ∥ start_ARG bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_ARG ∥ end_ARG, in which ‖𝐱M−𝐱⁢(T)‖<ϵ.normsubscript𝐱𝑀𝐱𝑇italic-ϵ\norm{\bm{x}_{M}-\bm{x}(T)}<\epsilon.∥ start_ARG bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT - bold_italic_x ( italic_T ) end_ARG ∥ < italic_ϵ . The mapping from the initial value 𝐱⁢(0)𝐱0\bm{x}(0)bold_italic_x ( 0 ) to 𝐱Msubscript𝐱𝑀\bm{x}_{M}bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT is symplectic. The algorithm involves O~⁢(T⁢‖K‖⁢κ⁢(V)2)~𝑂𝑇norm𝐾𝜅superscript𝑉2\tilde{O}(T\|K\|\kappa(V)^{2})over~ start_ARG italic_O end_ARG ( italic_T ∥ italic_K ∥ italic_κ ( italic_V ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) queries to K𝐾Kitalic_K, where in O~~𝑂\tilde{O}over~ start_ARG italic_O end_ARG we neglected logarithmic factors and κ⁢(V)𝜅𝑉\kappa(V)italic_κ ( italic_V ) is the condition number associated with the eigenvector matrix of K𝐾Kitalic_K. Meanwhile, at a high level, our algorithm for nonlinear dynamics is outlined in Fig. 2. Specifically, the solution of a Hamiltonian system corresponds to a symplectic map: 𝒙⁢(t)=S⁢𝒙⁢(0).𝒙𝑡𝑆𝒙0\bm{x}(t)=S\bm{x}(0).bold_italic_x ( italic_t ) = italic_S bold_italic_x ( 0 ) . We will show that the Carleman embedding implicitly induces a nonlinear symplectic map 𝒛=𝒉⁢(𝒙)𝒛𝒉𝒙\bm{z}=\bm{h}(\bm{x})bold_italic_z = bold_italic_h ( bold_italic_x ) that transforms the nonlinear dynamics to a linear Hamiltonian system for which the symplectic structure remains, which is denoted by S1subscript𝑆1S_{1}italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. Therefore, the original symplectic solution map is factored as follows, S=𝒉−1∘S1∘𝒉,𝑆superscript𝒉1subscript𝑆1𝒉S=\bm{h}^{-1}\circ S_{1}\circ\bm{h},italic_S = bold_italic_h start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∘ italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∘ bold_italic_h , (2) each of which is symplectic, a property that is invariant under composition. Our symplectic integrators, when applied to the Carleman system, introduce an approximate, but still symplectic, map for S1.subscript𝑆1S_{1}.italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . This result is summarized in the following. Theorem 1.2. Assume that the n𝑛nitalic_n dimensional nonlinear Hamiltonian system satisfies a no-resonance condition and a condition on the strength of the nonlinearity 𝖱r<1subscript𝖱𝑟1\mathsf{R}_{r}<1sansserif_R start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT < 1. Then there is a quantum algorithm that produces a quantum state |ψ⟩=𝐱M‖𝐱M‖ket𝜓subscript𝐱𝑀normsubscript𝐱𝑀\ket{\psi}=\frac{\bm{x}_{M}}{\norm{\bm{x}_{M}}}| start_ARG italic_ψ end_ARG ⟩ = divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_ARG start_ARG ∥ start_ARG bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT end_ARG ∥ end_ARG, in which ‖𝐱M−𝐱⁢(T)‖<ϵ.normsubscript𝐱𝑀𝐱𝑇italic-ϵ\norm{\bm{x}_{M}-\bm{x}(T)}<\epsilon.∥ start_ARG bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT - bold_italic_x ( italic_T ) end_ARG ∥ < italic_ϵ . The algorithm involves O~⁢(T1+log⁡(n⁢κ⁢(V))ϵlog⁡(n⁢κ⁢(V))⁢(‖F1‖+‖F2‖))~𝑂superscript𝑇1𝑛𝜅𝑉superscriptitalic-ϵ𝑛𝜅𝑉normsubscript𝐹1normsubscript𝐹2\tilde{O}\big{(}\frac{T^{1+\log(n\kappa(V))}}{\epsilon^{\log(n\kappa(V))}}(% \norm{F_{1}}+\norm{F_{2}})\big{)}over~ start_ARG italic_O end_ARG ( divide start_ARG italic_T start_POSTSUPERSCRIPT 1 + roman_log ( start_ARG italic_n italic_κ ( italic_V ) end_ARG ) end_POSTSUPERSCRIPT end_ARG start_ARG italic_ϵ start_POSTSUPERSCRIPT roman_log ( start_ARG italic_n italic_κ ( italic_V ) end_ARG ) end_POSTSUPERSCRIPT end_ARG ( ∥ start_ARG italic_F start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG ∥ + ∥ start_ARG italic_F start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ∥ ) ) queries to the coefficient matrices in the Hamiltonian. The mapping S𝑆Sitalic_S from the initial value 𝐱⁢(0)𝐱0\bm{x}(0)bold_italic_x ( 0 ) to 𝐱Msubscript𝐱𝑀\bm{x}_{M}bold_italic_x start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT is approximately symplectic in the sense that ST⁢J⁢S−J=O⁢(ϵ).superscript𝑆𝑇𝐽𝑆𝐽𝑂italic-ϵS^{T}JS-J=O(\epsilon).italic_S start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_J italic_S - italic_J = italic_O ( italic_ϵ ) . It is important to emphasize that the commonly cited term “Carleman linearization” is somewhat misleading: while the goal is to reduce the problem to a linear dynamical system, the underlying transformation itself is highly nonlinear. Therefore, we prefer the terminology “Carleman embedding”. Main contributions • We present a quantum algorithm for linear Hamiltonian systems with complexity that is optimal in both T𝑇Titalic_T and ϵitalic-ϵ\epsilonitalic_ϵ, while preserving the symplectic properties of the solution map. As a result, energy conservation is exactly maintained. • Our algorithm implements the symplectic collocation methods, which in practice typically outperforms explicit Runge Kutta methods or Taylor approximation methods. • Unlike the complexity estimates for general ODEs, our result does not depend on any time-varying parameters. • We extend this structure-preserving property to nonlinear Hamiltonian systems with polynomial nonlinearity by utilizing the Carleman embedding technique, and quantify how the symplectic properties are affected by the Carleman embedding. • We analyze the complexity of the quantum algorithm for the finite Carleman system. • We demonstrate that the Carleman embedding implicitly induces a symplectic map, which, under our conditions, reduces the nonlinear Hamiltonian dynamics to a linear Hamiltonian system. Consequently, our quantum algorithm preserves the symplectic property up to an error ϵitalic-ϵ\epsilonitalic_ϵ, which arises from the finite truncation of the Carleman system. Related works Numerous efficient quantum algorithms have been developed for solving linear ODE systems [3, 7, 8, 16, 20, 24, 26], with many of these algorithms reducing the problem to a Hamiltonian simulation framework—solving the unitary dynamics of a Schrödinger equation. These algorithms have been applied to harmonic oscillators [4] and the acoustic wave equation [18], which are two examples of classical Hamiltonian systems. Quantum algorithms in this framework offer exponential speedups with respect to problem dimensions. The recent work [27] has specifically considered Hamiltonian systems. However, quantum algorithms designed to handle Hamiltonian ODEs that preserve symplectic structures have not yet been explored in these prior works. In contrast, nonlinear Hamiltonian systems pose significantly more challenges due to their inherent complexity and interactions. One regime where quantum advantage has been identified is in dissipative dynamics using Carleman embedding [13], where the real part of the Jacobian’s eigenvalues is strictly negative [28]. Hamiltonian systems, due to their time-reversible nature, fall outside this regime. However, the authors’ recent work [35] bridges this gap by identifying new conditions under which quantum advantage remains achievable. This new condition will be used in the current paper as well. For general nonlinear dynamical systems, Brüstle and Wiebe [9] provided a lower bound on the computational complexity, showing that it scales exponentially with the time duration T𝑇Titalic_T. They also leveraged Carleman embedding to develop algorithms with the same complexity scaling."
https://arxiv.org/html/2411.03501v1,The Python LevelSet Toolbox (LevelSetPy),"This paper describes open-source scientific contributions in python surrounding the numerical solutions to hyperbolic Hamilton-Jacobi (HJ) partial differential equations viz., their implicit representation on co-dimension one surfaces; dynamics evolution with levelsets; spatial derivatives; total variation diminishing Runge-Kutta integration schemes; and their applications to the theory of reachable sets. They are increasingly finding applications in multiple research domains such as reinforcement learning, robotics, control engineering and automation. We describe the library components, illustrate usage with an example, and provide comparisons with existing implementations. This GPU-accelerated package allows for easy portability to many modern libraries for the numerical analyses of the HJ equations. We also provide a CPU implementation in python that is significantly faster than existing alternatives.","I Overview The reliability of the modern automation algorithms that we design has become paramount given the dangers that may evolve if nominally envisioned system performance falters. Even so, the need for scalable and faster numerical algorithms in software for verification and validation has become timely given the emergence of complexity of contemporary systems. The foremost open-source verification software for engineering applications based on Hamilton-Jacobi (HJ) equations [1, 2] and levelset methods [3, 4] is the CPU-based MATLAB®-implemented levelsets toolbox [5], developed before computing via graphical processing units (GPU) became pervasive. Since then, there has been significant improvements in computer hardware and architecture design, code parallelization algorithms, and compute-acceleration on modern GPUs. This paper describes a python-based GPU-accelerated scientific software package for numerically resolving generalized discontinuous solutions to Cauchy-type (or time-dependent) HJ hyperbolic partial differential equations (PDEs). HJ PDEs arise in many contexts including (multi-agent) reinforcement learning, robotics, control theory, differential games, flow, and transport phenomena. We focus on the numerical tools for safety assurance (ascertaining the freedom of a system from harm) in a verification sense in this paper. Accompanying the package are implicit calculus operations on dynamic codimension-one interfaces embedded within ℝnsuperscriptℝ𝑛\mathbb{R}^{n}blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT surfaces, and spatial and temporal discretization schemes for HJ PDEs. Furthermore, we describe explicit integration schemes including the Lax-Friedrichs, Courant-Friedrichs-Lewy (CFL), and total variation diminishing Runge-Kutta (or TVD-RK) conditioning schemes for HJ Hamiltonians of the form 𝑯⁢(𝒙,𝒑)𝑯𝒙𝒑\bm{H}(\bm{x},\bm{p})bold_italic_H ( bold_italic_x , bold_italic_p ), where 𝒙𝒙\bm{x}bold_italic_x is the state and 𝒑𝒑\bm{p}bold_italic_p is the co-state. Finally, extensions to reachability analyses for continuous and hybrid systems, formulated as optimal control or game theory problems using viscosity solutions to HJ PDEs are described. All data transfers to the GPU are based on CuPy [6] framework. In all, we closely follow the Python Enhancement Proposals (PEP) 8 style guide111Python PEP 8 style guide: peps.python.org/pep-0008/; however, in order not to break readability with respect to the original MATLAB®code, we err in consistency with the MATLAB®project layout. The Python package and installation instructions are available on the author’s github repository: levelsetpy. The CPU implementation (in Python) is on the cpu-numpy tree of the repository. Extensions to other python GPU programming language are straightforward (as detailed in the CuPy interoperability document). While our emphasis is on the resolution of safe sets in a reachability verification context, the applications of this package extend beyond control engineering."
https://arxiv.org/html/2411.03264v1,"A priori and a posteriori error estimates
of a DG-CG method for the wave equation in second order formulation","We establish fully-discrete a priori and semi-discrete in time a posteriori error estimates for a discontinuous-continuous Galerkin discretization of the wave equation in second order formulation; the resulting method is a Petrov-Galerkin scheme based on piecewise and piecewise continuous polynomial in time test and trial spaces, respectively. Crucial tools in the a priori analysis for the fully-discrete formulation are the design of suitable projection and interpolation operators extending those used in the parabolic setting, and stability estimates based on a nonstandard choice of the test function; a priori estimates are shown, which are measured in L∞superscript𝐿L^{\infty}italic_L start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT-type norms in time. For the semi-discrete in time formulation, we exhibit constant-free, reliable a posteriori error estimates for the error measured in the L∞⁢(L2)superscript𝐿superscript𝐿2L^{\infty}(L^{2})italic_L start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) norm; to this aim, we design a reconstruction operator into 𝒞1superscript𝒞1\mathcal{C}^{1}caligraphic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT piecewise polynomials over the time grid with optimal approximation properties in terms of the polynomial degree distribution and the time steps. Numerical examples illustrate the theoretical findings.AMS subject classification: 65M50; 65M60; 65J10.Keywords: wave equation; discontinuous Galerkin method; adaptive algorithm; a posteriori error analysis.","We establish fully-discrete a priori and semi-discrete in time a posteriori error estimates for a DG-CG (discontinuous Galerkin - continuous Galerkin) method, see, e.g., [32], approximating solutions to the wave equation in second order formulation, which are explicit in the spatial mesh size, the time steps, and the polynomial degrees. Formulation, a priori estimates, and minimal literature. The DG-CG method we are interested in is based on a second order formulation of the wave equation. Compared to several references where first order systems are considered [10, 18, 9, 20], the dimension of the corresponding discrete spaces is smaller. The method lies in between a fully DG and a 𝒞1superscript𝒞1\mathcal{C}^{1}caligraphic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT schemes in time: it employs piecewise and piecewise continuous polynomial in time and space; upwind terms involving first time derivatives in time of the trial functions are included in the formulation. The polynomial degree in time of the trial functions is larger by 1 than that for the test functions, leading to square systems for each time interval, which are solved sequentially as a time marching scheme. A key tool in deriving stability and a priori error estimates is the choice of an appropriate test function. For instance, in [32], a higher order fully-discrete version of the test function in [3] is used, leading to stability estimates in the L∞superscript𝐿L^{\infty}italic_L start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT-type norms in time. A posteriori estimates and minimal literature. A posteriori error estimates are well established for elliptic problems and a considerable amount of work is available for parabolic problems as well; on the other hand, hyperbolic (and in particular wave) problems are less explored. A posteriori error estimates for wave problems in second order formulation are studied in [1] and later rigorously analysed in [4]; first order systems are instead the topic of [23, 20, 29]; a posteriori error estimates measured in the L∞⁢(L2)superscript𝐿superscript𝐿2L^{\infty}(L^{2})italic_L start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) norm are investigated for different time stepping schemes in [11, 12, 14]. A posteriori error estimates in other norms are instead investigated, among others, in [7, 6]. Features of the DG-CG method. Compared to fully DG schemes in time, the DG-CG method involves fewer unknowns; in view of the a posteriori error analysis for the semi-discrete in time version, one can employ tools from the parabolic setting and deduce a posteriori bounds that are fully explicit with respect to the polynomial degree. On the other hand, compared to schemes with 𝒞1superscript𝒞1\mathcal{C}^{1}caligraphic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT in time trial functions, the DG-CG approach seems to be more suited to allow for mesh changes in space [21]; we are not aware of works in the literature with spatial mesh changes for 𝒞1superscript𝒞1\mathcal{C}^{1}caligraphic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT in time discretizations. First main contribution of the manuscript. We modify the analysis in [32] so as to have explicit dependence on the polynomial degree for the a priori analysis of the fully-discrete scheme. We consider uniform polynomial degree in the spatial discretization and possibly nonuniform polynomial degree in the time discretization. Static meshes in space are considered. Our analysis hinges upon deriving stability estimates for the scheme, which are explicit in the polynomial degrees; see Section 2.1. Based on such stability estimates and the properties of an integrated Thomeé operator discussed in Section 2.2, a priori error estimates are proven in Section 2.3. Convergence estimates are obtained in Section 2.4, which are explicit in the spatial mesh size, the time steps, and the polynomial degrees. Optimal convergence for the error measured with respect to L∞superscript𝐿L^{\infty}italic_L start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT-type norms in time is shown for sufficiently regular solutions and data for fixed polynomial degrees in time and space. Second main contribution of the manuscript. For the semi-discrete in time method, we design an error estimator satisfying a posteriori error estimates, with explicit dependence on the polynomial degree distribution in time. The crucial tool in a posteriori error estimates for time-dependent problems is the derivation of a reconstruction operator into smoother spaces. The original idea in the context of parabolic problems traces back to Makridakis and Nochetto [22]; the corresponding p𝑝pitalic_p-version analysis is detailed in [27] and later [17]. We design a related operator in the wave equation setting and derive p𝑝pitalic_p-optimal approximation estimates in several norms in Section 3.2; such an operator is instrumental for designing an error estimator that is reliable for the error measured in the L∞⁢(L2)superscript𝐿superscript𝐿2L^{\infty}(L^{2})italic_L start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) norm; see Section 3.3. The upper bound is explicit in the polynomial degree distribution and the time steps, without unknown constants. Since the test and trial test functions have different polynomial degrees in time, the a posteriori error bounds involve extra oscillation terms compared to the parabolic setting. To the best of our knowledge, we provide for the first time in the literature constant free, reliable a posteriori error estimates for a semi-discrete in time method for the approximation of solutions to the wave equation in second order formulation, which are explicit in the polynomial degree distribution in time and optimal in the time steps. The proposed analysis does not hinge upon any CFL condition, which is greatly advantageous for adaptivity whilst compared to methods based on explicit time stepping; there, for each mesh refinement, one has to check whether the resulting spatial mesh size is sufficiently small compared to the corresponding time step and the polynomial degree [16]. This improvement is even more effective for wave problems involving higher order spatial elliptic operators [24], where the CFL condition poses even stricter constraints on the spatial mesh size. List of the main results and advances. For the reader’s convenience, we list here the main results of the manuscript ([APRI] = a priori analysis; [APOS] = a posteriori analysis): • [APRI] Theorem 2.1 is concerned with constant free stability estimates for the DG-CG formulation, which are explicit with respect to the polynomial degrees in time and space employed throughout; • [APRI] Theorem 2.8 discusses a priori estimates that are explicit with respect to the spatial mesh size, the time step distribution, the spatial polynomial degree, and the polynomial degree distribution in time; • [APOS] we define an error estimator η𝜂\etaitalic_η in (70) for the semi-discrete in time formulation; • [APOS] corresponding reliability estimates with respect to the L∞⁢(L2)superscript𝐿superscript𝐿2L^{\infty}(L^{2})italic_L start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) norm of the error are displayed in Proposition 3.6. Functional setting. Standard notation is used throughout for Sobolev and Bochner spaces. Let D𝐷Ditalic_D be a Lipschitz domain in ℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, d=1𝑑1d=1italic_d = 1, 2222, and 3333, with boundary ∂D𝐷\partial D∂ italic_D. The space of Lebesgue measurable and square integrable functions over D𝐷Ditalic_D is L2⁢(D)superscript𝐿2𝐷L^{2}(D)italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_D ). The Sobolev space of positive integer order s𝑠sitalic_s is Hs⁢(D)superscript𝐻𝑠𝐷H^{s}(D)italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_D ). We endow Hs⁢(D)superscript𝐻𝑠𝐷H^{s}(D)italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_D ) with the inner product, seminorm, and norm (⋅,⋅)s,D,|⋅|s,D,∥⋅∥s,D.(\cdot,\cdot)_{s,D},\qquad\qquad\qquad{\left|{\cdot}\right|}_{s,D},\qquad% \qquad\qquad{\left\|{\cdot}\right\|}_{s,D}.( ⋅ , ⋅ ) start_POSTSUBSCRIPT italic_s , italic_D end_POSTSUBSCRIPT , | ⋅ | start_POSTSUBSCRIPT italic_s , italic_D end_POSTSUBSCRIPT , ∥ ⋅ ∥ start_POSTSUBSCRIPT italic_s , italic_D end_POSTSUBSCRIPT . Interpolation theory is used to construct Sobolev spaces of positive noninteger order; duality is used to define negative order Sobolev spaces. We shall be particularly using the space H−1⁢(D)superscript𝐻1𝐷H^{-1}(D)italic_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_D ), which is the dual of H01⁢(D)subscriptsuperscript𝐻10𝐷H^{1}_{0}(D)italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_D ); the duality pairing between the two spaces is ⟨⋅,⋅⟩⋅⋅\langle\cdot,\cdot\rangle⟨ ⋅ , ⋅ ⟩. A trace theorem holds true for the space Hs⁢(D)superscript𝐻𝑠𝐷H^{s}(D)italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_D ), 1/2<s<3/212𝑠321/2<s<3/21 / 2 < italic_s < 3 / 2. In particular, given s𝑠sitalic_s in the above range and g𝑔gitalic_g in Hs−12⁢(Γg)superscript𝐻𝑠12subscriptΓ𝑔H^{s-\frac{1}{2}}(\Gamma_{g})italic_H start_POSTSUPERSCRIPT italic_s - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT ( roman_Γ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ), being ΓgsubscriptΓ𝑔\Gamma_{g}roman_Γ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT any subset of ∂D𝐷\partial D∂ italic_D with positive measure in ∂D𝐷\partial D∂ italic_D, we define Hgs⁢(D,Γg):={v∈Hs⁢(D,ΓD)∣v|ΓD=g}.H^{s}_{g}(D,\Gamma_{g}):=\{v\in H^{s}(D,\Gamma_{D})\mid v_{|\Gamma_{D}}=g\}.italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( italic_D , roman_Γ start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) := { italic_v ∈ italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_D , roman_Γ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ) ∣ italic_v start_POSTSUBSCRIPT | roman_Γ start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_g } . The space of polynomials of nonnegative degree p𝑝pitalic_p over D𝐷Ditalic_D is ℙp⁢(D)subscriptℙ𝑝𝐷\mathbb{P}_{p}(D)blackboard_P start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ( italic_D ). Given 𝒳𝒳\mathcal{X}caligraphic_X a real Banach space with norm ∥⋅∥𝒳{\left\|{\cdot}\right\|}_{\mathcal{X}}∥ ⋅ ∥ start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT, an interval I𝐼Iitalic_I, and t𝑡titalic_t larger than or equal to 1111, we define Lt⁢(I;𝒳)superscript𝐿𝑡𝐼𝒳L^{t}(I;\mathcal{X})italic_L start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_I ; caligraphic_X ) as the Bochner space of measurable functions v𝑣vitalic_v from I𝐼Iitalic_I to 𝒳𝒳\mathcal{X}caligraphic_X such that the following quantity is finite: ‖v‖Lt⁢(I;𝒳):={(∫I‖v‖𝒳t)1t⁢d⁢tfor ⁢1≤t<∞ess supt∈I⁢‖v‖𝒳for ⁢t=∞.assignsubscriptnorm𝑣superscript𝐿𝑡𝐼𝒳casessuperscriptsubscript𝐼superscriptsubscriptnorm𝑣𝒳𝑡1𝑡𝑑𝑡for 1𝑡subscriptess sup𝑡𝐼subscriptnorm𝑣𝒳for 𝑡{\left\|{v}\right\|}_{L^{t}(I;\mathcal{X})}:=\begin{cases}\left(\int_{I}{\left% \|{v}\right\|}_{\mathcal{X}}^{t}\right)^{\frac{1}{t}}dt&\text{for }1\leq t<% \infty\\ \text{ess\ sup}_{t\in I}{\left\|{v}\right\|}_{\mathcal{X}}&\text{for }t=\infty% .\end{cases}∥ italic_v ∥ start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ( italic_I ; caligraphic_X ) end_POSTSUBSCRIPT := { start_ROW start_CELL ( ∫ start_POSTSUBSCRIPT italic_I end_POSTSUBSCRIPT ∥ italic_v ∥ start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_t end_ARG end_POSTSUPERSCRIPT italic_d italic_t end_CELL start_CELL for 1 ≤ italic_t < ∞ end_CELL end_ROW start_ROW start_CELL ess sup start_POSTSUBSCRIPT italic_t ∈ italic_I end_POSTSUBSCRIPT ∥ italic_v ∥ start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT end_CELL start_CELL for italic_t = ∞ . end_CELL end_ROW For s𝑠sitalic_s in ℕℕ\mathbb{N}blackboard_N, the space Hs⁢(I;𝒳)superscript𝐻𝑠𝐼𝒳H^{s}(I;\mathcal{X})italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_I ; caligraphic_X ) is the space of measurable functions v𝑣vitalic_v whose derivatives in time up to order s𝑠sitalic_s belong to L2⁢(I;𝒳)superscript𝐿2𝐼𝒳L^{2}(I;\mathcal{X})italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_I ; caligraphic_X ). For any real number s𝑠sitalic_s larger than or equal to 00, the space Hs⁢(I;𝒳)superscript𝐻𝑠𝐼𝒳H^{s}(I;\mathcal{X})italic_H start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT ( italic_I ; caligraphic_X ) is constructed using interpolation of integer order Bochner spaces. To avoid confusion, the seminorm symbol |⋅|{\left|{\cdot}\right|}| ⋅ | is only used to denote spatial seminorms. Seminorms in time are rather displayed as L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT norms of a suitable time derivative. The first and second derivative symbols are ⋅′superscript⋅′\cdot^{\prime}⋅ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and ⋅′′superscript⋅′′\cdot^{\prime\prime}⋅ start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT; time derivatives of order s𝑠sitalic_s larger than 2222 are displayed as ⋅(s)superscript⋅𝑠\cdot^{(s)}⋅ start_POSTSUPERSCRIPT ( italic_s ) end_POSTSUPERSCRIPT. The continuous problem. Let ΩΩ\Omegaroman_Ω be a polytopic, Lipschitz domain in ℝdsuperscriptℝ𝑑\mathbb{R}^{d}blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, d=1,2,3𝑑123d=1,2,3italic_d = 1 , 2 , 3; T𝑇Titalic_T a positive final time; QT:=(0,T]×Ωassignsubscript𝑄𝑇0𝑇ΩQ_{T}:=(0,T]\times\Omegaitalic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT := ( 0 , italic_T ] × roman_Ω the space–time cylinder; u0subscript𝑢0u_{0}italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT in H01⁢(Ω)superscriptsubscript𝐻01ΩH_{0}^{1}(\Omega)italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ); u1subscript𝑢1u_{1}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in L2⁢(Ω)superscript𝐿2ΩL^{2}(\Omega)italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ); f𝑓fitalic_f in L2⁢(0,T;H−1⁢(Ω))superscript𝐿20𝑇superscript𝐻1ΩL^{2}(0,T;H^{-1}(\Omega))italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( roman_Ω ) ). Given Δ𝐱⋅\Delta_{\mathbf{x}}\cdotroman_Δ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT ⋅ and ∇𝐱⋅\nabla_{\mathbf{x}}\cdot∇ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT ⋅ the spatial Laplace and gradient operator, we consider the following problem: find u:QT→ℝ:𝑢→subscript𝑄𝑇ℝu:Q_{T}\to\mathbb{R}italic_u : italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT → blackboard_R such that {u′′−Δ𝐱⁢u=fin ⁢QTu⁢(t,⋅)=0on ⁢(0,T]×∂Ωu⁢(0,⋅)=u0⁢(⋅)on ⁢Ωu′⁢(0,⋅)=u1⁢(⋅)on ⁢Ω.casessuperscript𝑢′′subscriptΔ𝐱𝑢𝑓in subscript𝑄𝑇𝑢𝑡⋅0on 0𝑇Ω𝑢0⋅subscript𝑢0⋅on Ωsuperscript𝑢′0⋅subscript𝑢1⋅on Ω\begin{cases}u^{\prime\prime}-\Delta_{\mathbf{x}}u=f&\text{in }Q_{T}\\ u(t,\cdot)=0&\text{on }(0,T]\times\partial\Omega\\ u(0,\cdot)=u_{0}(\cdot)&\text{on }\Omega\\ u^{\prime}(0,\cdot)=u_{1}(\cdot)&\text{on }\Omega.\\ \end{cases}{ start_ROW start_CELL italic_u start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT - roman_Δ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT italic_u = italic_f end_CELL start_CELL in italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_u ( italic_t , ⋅ ) = 0 end_CELL start_CELL on ( 0 , italic_T ] × ∂ roman_Ω end_CELL end_ROW start_ROW start_CELL italic_u ( 0 , ⋅ ) = italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( ⋅ ) end_CELL start_CELL on roman_Ω end_CELL end_ROW start_ROW start_CELL italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( 0 , ⋅ ) = italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( ⋅ ) end_CELL start_CELL on roman_Ω . end_CELL end_ROW (1) Introduce the spaces X:=H2⁢(0,T;H−1⁢(Ω))∩L2⁢(0,T;H01⁢(Ω))∩H1⁢(0,T;L2⁢(Ω)),Y:=L2⁢(0,T;H01⁢(Ω)),formulae-sequenceassign𝑋superscript𝐻20𝑇superscript𝐻1Ωsuperscript𝐿20𝑇subscriptsuperscript𝐻10Ωsuperscript𝐻10𝑇superscript𝐿2Ωassign𝑌superscript𝐿20𝑇subscriptsuperscript𝐻10ΩX:=H^{2}(0,T;H^{-1}(\Omega))\cap L^{2}(0,T;H^{1}_{0}(\Omega))\cap H^{1}(0,T;L^% {2}(\Omega)),\qquad\qquad Y:=L^{2}(0,T;H^{1}_{0}(\Omega)),italic_X := italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_H start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( roman_Ω ) ) ∩ italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) ) ∩ italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ) ) , italic_Y := italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) ) , and the bilinear form on Y×Y𝑌𝑌Y\times Yitalic_Y × italic_Y a⁢(u,v):=(∇𝐱u,∇𝐱v)0,Ω.assign𝑎𝑢𝑣subscriptsubscript∇𝐱𝑢subscript∇𝐱𝑣0Ωa(u,v):=(\nabla_{\mathbf{x}}u,\nabla_{\mathbf{x}}v)_{0,\Omega}.italic_a ( italic_u , italic_v ) := ( ∇ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT italic_u , ∇ start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT italic_v ) start_POSTSUBSCRIPT 0 , roman_Ω end_POSTSUBSCRIPT . We consider the following weak formulation of problem (1): {find ⁢u∈X⁢ such that ∫0T[⟨u′′,v⟩+a⁢(u,v)]⁢𝑑t=∫0T⟨f,v⟩⁢𝑑t∀v∈Yu⁢(0,⋅)=u0⁢(⋅)⁢ in ⁢H01⁢(Ω),u′⁢(0,⋅)=u1⁢(⋅)⁢ in ⁢L2⁢(Ω).casesfind 𝑢𝑋 such that otherwiseformulae-sequencesuperscriptsubscript0𝑇delimited-[]superscript𝑢′′𝑣𝑎𝑢𝑣differential-d𝑡superscriptsubscript0𝑇𝑓𝑣differential-d𝑡for-all𝑣𝑌otherwiseformulae-sequence𝑢0⋅subscript𝑢0⋅ in superscriptsubscript𝐻01Ωsuperscript𝑢′0⋅subscript𝑢1⋅ in superscript𝐿2Ωotherwise\begin{cases}\text{find }u\in X\text{ such that }\\ \int_{0}^{T}[\langle u^{\prime\prime},v\rangle+a(u,v)]dt=\int_{0}^{T}\langle f% ,v\rangle dt\qquad\qquad\forall v\in Y\\ u(0,\cdot)=u_{0}(\cdot)\text{ in }H_{0}^{1}(\Omega),\qquad u^{\prime}(0,\cdot)% =u_{1}(\cdot)\text{ in }L^{2}(\Omega).\end{cases}{ start_ROW start_CELL find italic_u ∈ italic_X such that end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT [ ⟨ italic_u start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT , italic_v ⟩ + italic_a ( italic_u , italic_v ) ] italic_d italic_t = ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ⟨ italic_f , italic_v ⟩ italic_d italic_t ∀ italic_v ∈ italic_Y end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL italic_u ( 0 , ⋅ ) = italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( ⋅ ) in italic_H start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ) , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( 0 , ⋅ ) = italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( ⋅ ) in italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ) . end_CELL start_CELL end_CELL end_ROW (2) Problem (1) is well posed; see, e.g., [25, Theorem 8.2-2]. To simplify the forthcoming analysis (especially the a posteriori error analysis in Section 3), we assume that f𝑓fitalic_f belongs to L2⁢(0,T;L2⁢(Ω))superscript𝐿20𝑇superscript𝐿2ΩL^{2}(0,T;L^{2}(\Omega))italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ) ). In [32], homogeneous Dirichlet and inhomogeneous Neumann boundary conditions are considered. This results in further complication in the analysis below. For this reason, we prefer to stick to the setting in (2). Spatial meshes, time grids, and polynomial degree distributions . We consider either a simplicial or tensor-product conforming mesh 𝒯hsubscript𝒯ℎ\mathcal{T}_{h}caligraphic_T start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT of ΩΩ\Omegaroman_Ω and a corresponding H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT-conforming Lagrangian finite element space Vhsubscript𝑉ℎV_{h}italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT of uniform order p𝐱subscript𝑝𝐱p_{\mathbf{x}}italic_p start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT. We assume the existence of a constant γ𝛾\gammaitalic_γ in (0,1)01(0,1)( 0 , 1 ) such that each element of 𝒯hsubscript𝒯ℎ\mathcal{T}_{h}caligraphic_T start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT is star-shaped with respect to a ball of radius larger than or equal to the diameter of that element; moreover, we assume quasi-uniformity of the mesh, i.e., given h1subscriptℎ1h_{1}italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and h2subscriptℎ2h_{2}italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT the diameters of two arbitrary elements with h1≤h2subscriptℎ1subscriptℎ2h_{1}\leq h_{2}italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≤ italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, one has h2≤γ⁢h1subscriptℎ2𝛾subscriptℎ1h_{2}\leq\gamma h_{1}italic_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ≤ italic_γ italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. 111In the a priori analysis, the only place where quasi-uniformity is used in lieu of local quasi-uniformity is while deriving estimates of the elliptic projector in the L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT norm; see displays (21) and (22) below. We further consider a decomposition 0=t0<t1<⋯<tN=T0subscript𝑡0subscript𝑡1⋯subscript𝑡𝑁𝑇0=t_{0}<t_{1}<\dots<t_{N}=T0 = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < ⋯ < italic_t start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT = italic_T of [0,T]0𝑇[0,T][ 0 , italic_T ] and introduce τn:=tn−tn−1assignsubscript𝜏𝑛subscript𝑡𝑛subscript𝑡𝑛1\tau_{n}:=t_{n}-t_{n-1}italic_τ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT := italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT for all n=1,…,N𝑛1…𝑁n=1,\dots,Nitalic_n = 1 , … , italic_N. With each time interval In:=(tn−1,tn]assignsubscript𝐼𝑛subscript𝑡𝑛1subscript𝑡𝑛I_{n}:=(t_{n-1},t_{n}]italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT := ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ], we associate a local polynomial degree pntsuperscriptsubscript𝑝𝑛𝑡p_{n}^{t}italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT; we collect such polynomial orders (in time) in the vector 𝐩tsuperscript𝐩𝑡\mathbf{p}^{t}bold_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT in ℕ+Nsuperscriptsubscriptℕ𝑁\mathbb{N}_{+}^{N}blackboard_N start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT, and set 𝐩nt:=pntassignsubscriptsuperscript𝐩𝑡𝑛superscriptsubscript𝑝𝑛𝑡\mathbf{p}^{t}_{n}:=p_{n}^{t}bold_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT := italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. For k𝑘kitalic_k in ℤℤ\mathbb{Z}blackboard_Z, 𝐩t+ksuperscript𝐩𝑡𝑘\mathbf{p}^{t}+kbold_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + italic_k is the vector of entries pnt+ksuperscriptsubscript𝑝𝑛𝑡𝑘p_{n}^{t}+kitalic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT + italic_k. For all n=1,…,N𝑛1…𝑁n=1,\dots,Nitalic_n = 1 , … , italic_N, we set (v′)+⁢(tn−1,⋅):=v|In′⁢(tn−1,⋅)(v^{\prime})^{+}(t_{n-1},\cdot):=v^{\prime}_{|I_{n}}(t_{n-1},\cdot)( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) := italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) and (v′)−⁢(tn−1,⋅):=v|In−1′⁢(tn−1,⋅)(v^{\prime})^{-}(t_{n-1},\cdot):=v^{\prime}_{|I_{n-1}}(t_{n-1},\cdot)( italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) := italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ); for all v𝑣vitalic_v piecewise continuous in time, we set v+⁢(tn−1,⋅):=v|In⁢(tn−1,⋅)v^{+}(t_{n-1},\cdot):=v_{|I_{n}}(t_{n-1},\cdot)italic_v start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) := italic_v start_POSTSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) and v−⁢(tn−1,⋅):=v|In−1⁢(tn−1,⋅)v^{-}(t_{n-1},\cdot):=v_{|I_{n-1}}(t_{n-1},\cdot)italic_v start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) := italic_v start_POSTSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ). We also define the tensor product space ℙpnt⁢(In;Vh):={vh,𝝉∈L2⁢(In;Vh)∣vh,𝝉=a⁢(x)⁢b⁢(t),a∈Vh,b∈ℙpnt⁢(In)}.assignsubscriptℙsuperscriptsubscript𝑝𝑛𝑡subscript𝐼𝑛subscript𝑉ℎconditional-setsubscript𝑣ℎ𝝉superscript𝐿2subscript𝐼𝑛subscript𝑉ℎformulae-sequencesubscript𝑣ℎ𝝉𝑎𝑥𝑏𝑡formulae-sequence𝑎subscript𝑉ℎ𝑏subscriptℙsuperscriptsubscript𝑝𝑛𝑡subscript𝐼𝑛\mathbb{P}_{p_{n}^{t}}(I_{n};V_{h}):=\{v_{h,\boldsymbol{\tau}}\in L^{2}(I_{n};% V_{h})\mid v_{h,\boldsymbol{\tau}}=a(x)b(t),\;a\in V_{h},\ b\in\mathbb{P}_{p_{% n}^{t}}(I_{n})\}.blackboard_P start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) := { italic_v start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT ∈ italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) ∣ italic_v start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT = italic_a ( italic_x ) italic_b ( italic_t ) , italic_a ∈ italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , italic_b ∈ blackboard_P start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) } . Throughout we assume that pnt≥2∀n=2,…,N.formulae-sequencesuperscriptsubscript𝑝𝑛𝑡2for-all𝑛2…𝑁p_{n}^{t}\geq 2\qquad\qquad\qquad\forall n=2,\dots,N.italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ≥ 2 ∀ italic_n = 2 , … , italic_N . The fully-discrete DG-CG method. Let u0,hsubscript𝑢0ℎu_{0,h}italic_u start_POSTSUBSCRIPT 0 , italic_h end_POSTSUBSCRIPT and u1,hsubscript𝑢1ℎu_{1,h}italic_u start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT be discretizations of u0subscript𝑢0u_{0}italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and u1subscript𝑢1u_{1}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT in Vhsubscript𝑉ℎV_{h}italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT. For instance, we pick u0,hsubscript𝑢0ℎu_{0,h}italic_u start_POSTSUBSCRIPT 0 , italic_h end_POSTSUBSCRIPT as the elliptic projection of u0subscript𝑢0u_{0}italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT defined in display (21) below; u1,hsubscript𝑢1ℎu_{1,h}italic_u start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT as the L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT projection of u1subscript𝑢1u_{1}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT onto Vhsubscript𝑉ℎV_{h}italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT. Other variants are possible, but are omitted here. We define Xh,𝝉:=u0,h+{uh,𝝉∈𝒞0(0,T;Vh)∣uh,𝝉∈|Inℙpnt(In;Vh)∀n=1,…,N}X_{h,\boldsymbol{\tau}}:=u_{0,h}+\{u_{h,\boldsymbol{\tau}}\in\mathcal{C}^{0}(0% ,T;V_{h})\mid u_{h,\boldsymbol{\tau}}{}_{|I_{n}}\in\mathbb{P}_{p_{n}^{t}}(I_{n% };V_{h})\;\forall n=1,\dots,N\}italic_X start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT := italic_u start_POSTSUBSCRIPT 0 , italic_h end_POSTSUBSCRIPT + { italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT ∈ caligraphic_C start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) ∣ italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_FLOATSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_FLOATSUBSCRIPT ∈ blackboard_P start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) ∀ italic_n = 1 , … , italic_N } and the upwind jump operator for the time derivative as [[uh,𝝉′⁢(tn−1)]]⁢(⋅)={uh,𝝉′(0,⋅)|I1−u1,h(⋅)if ⁢n=1uh,𝝉′(tn−1,⋅)|In−uh,𝝉′(tn−1,⋅)|In−1if ⁢n=2,…,N.\left[\!\left[u_{h,\boldsymbol{\tau}}^{\prime}(t_{n-1})\right]\!\right](\cdot)% =\begin{cases}u_{h,\boldsymbol{\tau}}^{\prime}{}_{|I_{1}}(0,\cdot)-u_{1,h}(% \cdot)&\text{if }n=1\\ u_{h,\boldsymbol{\tau}}^{\prime}{}_{|I_{n}}(t_{n-1},\cdot)-u_{h,\boldsymbol{% \tau}}^{\prime}{}_{|I_{n-1}}(t_{n-1},\cdot)&\text{if }n=2,\dots,N.\end{cases}[ [ italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ) ] ] ( ⋅ ) = { start_ROW start_CELL italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_FLOATSUBSCRIPT | italic_I start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_FLOATSUBSCRIPT ( 0 , ⋅ ) - italic_u start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT ( ⋅ ) end_CELL start_CELL if italic_n = 1 end_CELL end_ROW start_ROW start_CELL italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_FLOATSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_FLOATSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) - italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_FLOATSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT end_FLOATSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) end_CELL start_CELL if italic_n = 2 , … , italic_N . end_CELL end_ROW The DG-CG method, see, e.g., [32], reads as follows: find uh,𝝉subscript𝑢ℎ𝝉u_{h,\boldsymbol{\tau}}italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT in Xh,𝝉subscript𝑋ℎ𝝉X_{h,\boldsymbol{\tau}}italic_X start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT such that ∫In[(uh,𝝉′′,vh,𝝉)0,Ω+a⁢(uh,𝝉,vh,𝝉)]⁢𝑑t+([[uh,𝝉′]]⁢(tn−1,⋅),vh,𝝉+⁢(tn−1,⋅))0,Ω=(f,vh,𝝉)L2⁢(In;L2⁢(Ω))∀vh,𝝉∈ℙpnt−1⁢(In;Vh),∀n=1,…,N.\begin{split}&\int_{I_{n}}[(u_{h,\boldsymbol{\tau}}^{\prime\prime},v_{h,% \boldsymbol{\tau}})_{0,\Omega}+a(u_{h,\boldsymbol{\tau}},v_{h,\boldsymbol{\tau% }})]dt+(\left[\!\left[u_{h,\boldsymbol{\tau}}^{\prime}\right]\!\right](t_{n-1}% ,\cdot),v_{h,\boldsymbol{\tau}}^{+}(t_{n-1},\cdot))_{0,\Omega}\\ &\qquad=(f,v_{h,\boldsymbol{\tau}})_{L^{2}(I_{n};L^{2}(\Omega))}\qquad\qquad% \qquad\forall v_{h,\boldsymbol{\tau}}\in\mathbb{P}_{p_{n}^{t}-1}(I_{n};V_{h}),% \quad\forall n=1,\dots,N.\end{split}start_ROW start_CELL end_CELL start_CELL ∫ start_POSTSUBSCRIPT italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ( italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT , italic_v start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 0 , roman_Ω end_POSTSUBSCRIPT + italic_a ( italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT ) ] italic_d italic_t + ( [ [ italic_u start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ] ] ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) , italic_v start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) ) start_POSTSUBSCRIPT 0 , roman_Ω end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL = ( italic_f , italic_v start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ) ) end_POSTSUBSCRIPT ∀ italic_v start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT ∈ blackboard_P start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_V start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) , ∀ italic_n = 1 , … , italic_N . end_CELL end_ROW (3) The initial condition u0,hsubscript𝑢0ℎu_{0,h}italic_u start_POSTSUBSCRIPT 0 , italic_h end_POSTSUBSCRIPT is imposed strongly in Xh,𝝉subscript𝑋ℎ𝝉X_{h,\boldsymbol{\tau}}italic_X start_POSTSUBSCRIPT italic_h , bold_italic_τ end_POSTSUBSCRIPT; the initial condition u1,hsubscript𝑢1ℎu_{1,h}italic_u start_POSTSUBSCRIPT 1 , italic_h end_POSTSUBSCRIPT is imposed weakly through the upwind term at the initial time. Method (3) is solved time-slab by time-slab as a time marching scheme. Initial conditions on each time slab are assigned taking the values of the solution at the final time of the previous slab and upwinding the first time derivative. The existence and uniqueness of a solution and the data of method (3) follow, e.g., assuming sufficient smoothness of the solution to problem (2), showing stability estimates as those in Theorem 2.1 below (which imply uniqueness), and using the fact that on each time slab the linear system to solve is square (which entails that existence is equivalent to uniqueness). The semi-discrete in time DG-CG method. Define the space X𝝉:=u0+{u𝝉∈𝒞0(0,T;H01(Ω))∣u𝝉∈|Inℙpnt(In;H01(Ω))}.X_{\boldsymbol{\tau}}:=u_{0}+\{u_{\boldsymbol{\tau}}\in\mathcal{C}^{0}(0,T;H^{% 1}_{0}(\Omega))\mid u_{\boldsymbol{\tau}}{}_{|I_{n}}\in\mathbb{P}_{p_{n}^{t}}(% I_{n};H^{1}_{0}(\Omega))\}.italic_X start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT := italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + { italic_u start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT ∈ caligraphic_C start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) ) ∣ italic_u start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT start_FLOATSUBSCRIPT | italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_FLOATSUBSCRIPT ∈ blackboard_P start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) ) } . In Section 3 below, we prove constant free, reliable a posteriori error estimates for the time semi-discrete in time version of (3). More precisely, we look for u𝝉subscript𝑢𝝉u_{\boldsymbol{\tau}}italic_u start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT in X𝝉subscript𝑋𝝉X_{\boldsymbol{\tau}}italic_X start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT such that ∫In[(u𝝉′′,v𝝉)0,Ω+a⁢(u𝝉,v𝝉)]⁢𝑑t+([[u𝝉′]]⁢(tn−1,⋅),v𝝉+⁢(tn−1,⋅))0,Ω=(f,v𝝉)L2⁢(0,T;L2⁢(Ω))∀v𝝉∈ℙpnt−1⁢(In;H01⁢(Ω)),∀n=1,…,N,\begin{split}\int_{I_{n}}[(u_{\boldsymbol{\tau}}^{\prime\prime},v_{\boldsymbol% {\tau}})_{0,\Omega}+a(u_{\boldsymbol{\tau}},v_{\boldsymbol{\tau}})]dt&+(\left[% \!\left[u_{\boldsymbol{\tau}}^{\prime}\right]\!\right](t_{n-1},\cdot),v_{% \boldsymbol{\tau}}^{+}(t_{n-1},\cdot))_{0,\Omega}=(f,v_{\boldsymbol{\tau}})_{L% ^{2}(0,T;L^{2}(\Omega))}\\ &\forall v_{\boldsymbol{\tau}}\in\mathbb{P}_{p_{n}^{t}-1}(I_{n};H^{1}_{0}(% \Omega)),\quad\forall n=1,\dots,N,\end{split}start_ROW start_CELL ∫ start_POSTSUBSCRIPT italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ( italic_u start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT , italic_v start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT 0 , roman_Ω end_POSTSUBSCRIPT + italic_a ( italic_u start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT ) ] italic_d italic_t end_CELL start_CELL + ( [ [ italic_u start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ] ] ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) , italic_v start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ( italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , ⋅ ) ) start_POSTSUBSCRIPT 0 , roman_Ω end_POSTSUBSCRIPT = ( italic_f , italic_v start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 0 , italic_T ; italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ) ) end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL ∀ italic_v start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT ∈ blackboard_P start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) ) , ∀ italic_n = 1 , … , italic_N , end_CELL end_ROW (4) with u0subscript𝑢0u_{0}italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT imposed strongly in X𝝉subscript𝑋𝝉X_{\boldsymbol{\tau}}italic_X start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT and u1subscript𝑢1u_{1}italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT imposed weakly through upwinding. Let Π𝐩t−10subscriptsuperscriptΠ0superscript𝐩𝑡1\Pi^{0}_{\mathbf{p}^{t}-1}roman_Π start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT denote the piecewise L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT projector onto ℙpnt−1⁢(In;H01⁢(Ω))subscriptℙsuperscriptsubscript𝑝𝑛𝑡1subscript𝐼𝑛subscriptsuperscript𝐻10Ω\mathbb{P}_{p_{n}^{t}-1}(I_{n};H^{1}_{0}(\Omega))blackboard_P start_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) ) for all n=1,…,N𝑛1…𝑁n=1,\dots,Nitalic_n = 1 , … , italic_N. We can replace the right-hand side of (4) with the following expression: (f,v𝝉)L2⁢(In;L2⁢(Ω))=(Π𝐩t−10⁢f,v𝝉)L2⁢(In;L2⁢(Ω))∀n=1,…,N.formulae-sequencesubscript𝑓subscript𝑣𝝉superscript𝐿2subscript𝐼𝑛superscript𝐿2ΩsubscriptsubscriptsuperscriptΠ0superscript𝐩𝑡1𝑓subscript𝑣𝝉superscript𝐿2subscript𝐼𝑛superscript𝐿2Ωfor-all𝑛1…𝑁(f,v_{\boldsymbol{\tau}})_{L^{2}(I_{n};L^{2}(\Omega))}=(\Pi^{0}_{\mathbf{p}^{t% }-1}f,v_{\boldsymbol{\tau}})_{L^{2}(I_{n};L^{2}(\Omega))}\qquad\qquad\forall n% =1,\dots,N.( italic_f , italic_v start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ) ) end_POSTSUBSCRIPT = ( roman_Π start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT bold_p start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - 1 end_POSTSUBSCRIPT italic_f , italic_v start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_I start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ; italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ) ) end_POSTSUBSCRIPT ∀ italic_n = 1 , … , italic_N . (5) Structure of the remainder of the paper. We discuss stability and convergence estimates of the fully-discrete method (3) in Section 2, which are explicit in the spatial mesh size, the time steps, and the polynomial degrees. A posteriori error estimates for the semi-discrete in time method (4) are derived in Section 3. We assess the numerical findings with numerical experiments in Section 4, and draw some conclusions in Section 5."
https://arxiv.org/html/2411.03090v1,Adjoint lattice kinetic scheme for topology optimizationin fluid problems,Keywords Topology optimization ⋅⋅\cdot⋅ Lattice Kinetic Scheme ⋅⋅\cdot⋅ Unsteady problem ⋅⋅\cdot⋅ Thermal fluid,"Topology optimization is one of the structural optimization methods, and unlike size optimization or shape optimization which were previously proposed, the method can generate weight-reducing voids inside a structure. Consequently, it has attracted significant attention due to its potential to generate structures that are not based on human experience. Bendsøe and Kikuchi [4] were the first to introduce topology optimization into the structural mechanics field. Subsequently, Borravall and Petersson [6] pioneered the methodology in the fluid mechanics field by formulating it for a Stokes flow problem. Later, it was applied to steady-state Navier-Stokes laminar flow [13][25], turbulent flow [15][11], unsteady flow [17][10], forced convection [36][22] and natural convection problems [1]. Additionally, in more practical scenarios, this methodology was extended to three-dimensional heatsink design problems under natural convection [2], and the designed heatsink was successfully manufactured [18]. In the topology optimization process, it is often necessary to solve state equations many times, and the implicit scheme based on Finite Element Method (FEM) is commonly employed for this purpose. However, this approach requires solving large-scale simultaneous equations, which significantly increases computational costs. One potential solution to this challenge is to use Lattice Boltzman Method (LBM) [8], where fluid is modeled as a set of fictitious particles with finite velocity. The collisions and propagations of the particles are calculated using distribution functions, and macroscopic values are computed from their moments. Since LBM is a fully explicit scheme, it does not require solving large-scale simultaneous equations and is well-suited for parallel computing, which is a key trend in accelerating computations, as most calculations are performed independently at each lattice point. Pingen et al. [26] proposed a topology optimization method that solves the state field using LBM. Nevertheless, their approach still required solving large-scale simultaneous equations to compute design sensitivities. Subsequently, Yaji et al. [32] introduced a method that calculates design sensitivities at computational costs nearly equivalent to LBM by employing adjoint LBM [16], in which adjoint equations are discretized and solved using the same approach as LBM. This LBM-based topology optimization method has since been applied to various fields, including thermal fluid problems [33][12][19], unsteady problems [7][34][24], stabilized with the MRT model [21] and the porous model [20]. We also applied the topology optimization method using LBM to natural convection problems [28], during this process, memory usage became a critical issue, especially for unsteady problems. This is because, for unsteady problems, state field values for all time steps must be stored to compute design sensitivities. Additionally, in LBM, these values represent velocity distribution functions, which significantly increase memory requirements. It should be noted that this substantial memory consumption is not only observed in natural convection but also in forced convection. Several approaches have been proposed to reduce memory usage, including the checkpoint algorithm [14], which achieves this by recalculating state field values, the local-in-time method [35], which splits the time interval and approximates design sensitivities, and a hybrid approach [29] that combines these two methods. In addition, the “poor-man’s” approach [3], which simplifies the governing equations of thermal fluid with certain approximations, and a method that approximates specific three-dimensional problems to two-dimensional ones [37] were also proposed. A noteworthy development in this context is the Lattice Kinetic Scheme (LKS), proposed by Inamuro et al. [9], which reduces memory usage by computing exclusively with macroscopic quantities, such as pressure and fluid velocity, thereby eliminating the necessity of storing velocity distribution functions. This is accomplished by tuning LBM parameters and adding specific terms. In this study, we addressed the memory usage issue in topology optimization by employing LKS. A topology optimization method using LKS has already been proposed by Xie et al. [31]; however, their method closely resembles LBM for the following two reasons. First, in their method, adjoint equations are derived from the fully continuous Boltzman equation. During this process, the fluid velocity derivative terms in the local equilibrium distribution function, characteristic of LKS, are canceled out. As a result, the adjoint field value derivative terms corresponding to them are not explicitly present in the adjoint equations. Second, in their method, the form of the derived adjoint equations corresponds to the local equilibrium distribution function of LBM or LKS, and the adjoint equations are not described using only macroscopic values, as in LKS. In contrast, in the proposed adjoint equations in our study are derived from the discrete velocity Boltzman equations and they are discretized in the same manner as in LKS, consequently form of adjoint equations are described corresponding to LKS. The adjoint equations are solved in the same manner as the LKS, therefore, we refer to this method as the Adjoint Lattice Kinetic Scheme (ALKS). This paper is organized as follows: Section 2 introduces the general theory of topology optimization, as well as the LBM and LKS. The section also includes the derivation of the ALKS and validation through comparison with finite difference approximations, followed by a brief explanation of the optimization procedure. Section 3 presents numerical examples for both thermal and non-thermal problems, addressing both steady and unsteady scenarios. In these examples, we also compare the memory consumption of the proposed method, which uses the LKS approach, with that of the conventional method, based on the LBM. Finally, Section 4 concludes the paper by summarizing the key findings."
https://arxiv.org/html/2411.03029v1,A Linear-complexity Tensor Butterfly Algorithm for Compressing High-dimensional Oscillatory Integral Operators,"This paper presents a multilevel tensor compression algorithm called tensor butterfly algorithm for efficiently representing large-scale and high-dimensional oscillatory integral operators, including Green’s functions for wave equations and integral transforms such as Radon transforms and Fourier transforms. The proposed algorithm leverages a tensor extension of the so-called complementary low-rank property of existing matrix butterfly algorithms. The algorithm partitions the discretized integral operator tensor into subtensors of multiple levels, and factorizes each subtensor at the middle level as a Tucker-like interpolative decomposition, whose factor matrices are formed in a multilevel fashion. For a d𝑑ditalic_d-dimensional integral operator discretized into a 2⁢d2𝑑2d2 italic_d-mode tensor with n2⁢dsuperscript𝑛2𝑑n^{2d}italic_n start_POSTSUPERSCRIPT 2 italic_d end_POSTSUPERSCRIPT entries, the overall CPU time and memory requirement scale as O⁢(nd)𝑂superscript𝑛𝑑O(n^{d})italic_O ( italic_n start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ), in stark contrast to the O⁢(nd⁢log⁡n)𝑂superscript𝑛𝑑𝑛O(n^{d}\log n)italic_O ( italic_n start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_log italic_n ) requirement of existing matrix algorithms such as matrix butterfly algorithm and fast Fourier transforms (FFT), where n𝑛nitalic_n is the number of points per direction. When comparing with other tensor algorithms such as quantized tensor train (QTT), the proposed algorithm also shows superior CPU and memory performance for tensor contraction. Remarkably, the tensor butterfly algorithm can efficiently model high-frequency Green’s function interactions between two unit cubes, each spanning 512 wavelengths per direction, which represents over 512×512\times512 × larger problem sizes than existing algorithms. On the other hand, for a problem representing 64 wavelengths per direction, which is the largest size existing algorithms can handle, our tensor butterfly algorithm exhibits 200x speedups and 30×30\times30 × memory reduction comparing with existing ones. Moreover, the tensor butterfly algorithm also permits O⁢(nd)𝑂superscript𝑛𝑑O(n^{d})italic_O ( italic_n start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT )-complexity FFTs and Radon transforms up to d=6𝑑6d=6italic_d = 6 dimensions.","Oscillatory integral operators (OIOs), such as Fourier transforms and Fourier integral operators [32, 7], are critical computational and theoretical tools for many scientific and engineering applications, such as signal and image processing, inverse problems and imaging, computer vision, quantum mechanics, and analyzing and solving partial differential equations (PDEs). The development of accurate and efficient algorithms for computing OIOs has profound impacts on the evolution of the pertinent research areas including, perhaps mostly remarkably, the invention of the fast Fourier transform (FFT) by Cooley and Tukey in 1965 and the invention of the fast multipole method (FMM) by Greengard and Rokhlin in 1987, both of which were listed among the ten most significant algorithms discovered in the 20th century. Among existing analytical and algebraic methods for OIOs, butterfly algorithms [52, 46, 37, 36, 56] represent an emerging class of multilevel matrix decomposition algorithms that have been proposed for Fourier transforms and Fourier integral operators [8, 68, 67], special function transforms [63, 4, 54], fast iterative [53, 52, 47] and direct [24, 43, 25, 26, 59, 44, 60] solution of surface and volume integral equations for wave equations, high-frequency Green’s function ansatz for inhomogeneous wave equations [45, 41, 48], direct solution of PDE-induced sparse systems [42, 13], and machine learning for inverse problems [33, 35]. The (matrix) butterfly algorithms leverage the so-called complementary low-rank (CLR) property of the matrix representation of OIOs after proper row/column permutation. The CLR states that judiciously selected submatrices exhibit numerical low ranks, known as the butterfly ranks, which stay constant irrespective of the matrix sizes. This permits a multilevel sparse matrix decomposition requiring O⁢(n⁢log⁡n)𝑂𝑛𝑛O(n\log n)italic_O ( italic_n roman_log italic_n ) factorization time, application time, and storage units with n𝑛nitalic_n being the matrix size. Despite their low asymptotic complexity, the matrix butterfly algorithms oftentimes exhibit relatively large prefactors, i.e., constant but high butterfly ranks, particularly for higher-dimensional OIOs. Examples include Green’s functions for 3D high-frequency wave equations [59, 45], 3D Radon transforms for linear inverse problems [17], 6D Fourier–Bros–Iagolnitzer transforms for Wigner equations [15, 66], 6D Fourier transforms in diffusion magnetic resonance imaging [11] and plasma physics [18], 4D space-time transforms in quantum field theories [57, 49], and multi-particle Green’s functions in quantum chemistry [21]. For these high-dimensional OIOs, the computational advantage of the matrix butterfly algorithms over other existing algorithms becomes significant only for very large matrices. More broadly speaking, for large-scale multi-dimensional scientific data and operators, tensor algorithms are typically more efficient than matrix algorithms. Popular low-rank tensor compression algorithms include CANDECOMP/PARAFAC [30], Tucker [16], hierarchical Tucker [28], tensor train (TT) [55], and tensor network [12] decomposition algorithms. See references [34, 23] for a more complete review of available tensor formats and their applications. When applied to the representation of high-dimensional integral operators, tensor algorithms often leverage additional translational- or scaling-invariance property to achieve superior compression performance, including solution of quasi-static wave equations [65, 64, 22, 14], elliptic PDEs [3, 27], many-body Schrödinger equations [31], and quantum Fourier transforms (QFTs) [9]. That being said, most existing tensor decomposition algorithms will break down for OIOs due to their incapability to exploit the oscillatory structure of these operators; therefore, new tensor algorithms are called for. In this paper, we propose a linear-complexity, low-prefactor tensor decomposition algorithm for large-scale and high-dimensional OIOs. This new tensor algorithm, henceforth dubbed the tensor butterfly algorithm, leverages the intrinsic CLR property of high-dimensional OIOs more effectively than the matrix butterfly algorithm, which is enabled by additional tensor properties such as translational invariance of free-space Green’s functions and dimensional separability of Fourier transforms. The algorithm partitions the OIO tensor into subtensors of multiple levels, and factorizes each subtensor at the middle level as a Tucker-like interpolative decomposition, whose factor matrices are further constructed in a nested fashion. For a d𝑑ditalic_d-dimensional OIO (assuming d𝑑ditalic_d constant) discretized as a 2⁢d2𝑑2d2 italic_d-mode tensor with n𝑛nitalic_n being the size per mode, the factorization time, application time, and storage cost scale as O⁢(nd)𝑂superscript𝑛𝑑O(n^{d})italic_O ( italic_n start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ), and the resulting tensor factors have small multi-linear ranks. This is in stark contrast both to the O⁢(nd⁢log⁡n)𝑂superscript𝑛𝑑𝑛O(n^{d}\log n)italic_O ( italic_n start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_log italic_n ) scaling of existing matrix algorithms such as matrix butterfly algorithms and FFTs, and to the super-linear scaling of existing tensor algorithms. We mention that the linear complexity of the factorization time in our proposed algorithm is achieved via a simple random entry evaluation scheme, assuming that any arbitrary entry can be computed in O⁢(1)𝑂1O(1)italic_O ( 1 ) time. We remark that, for 3D high-frequency wave equations, the proposed tensor butterfly algorithm can handle discretized Green’s function tensors 512×512\times512 × larger than existing algorithms; on the other hand, for the largest sized tensor that can be handled by existing algorithms, our tensor butterfly algorithm is 200×200\times200 × faster than existing ones. Moreover, we claim that the tensor butterfly algorithm instantiates the first linear-complexity implementation of high-dimensional FFTs for arbitrary input data. 1.1 Related Work Multi-dimensional butterfly algorithms represent a version of matrix butterfly algorithms designed for high-dimensional OIOs [38, 10]. Instead of the traditional binary tree partitioning of the matrix rows/columns [52], these algorithms can be viewed as a modern version of [53] that permits quadtree and octree partitioning of the matrix rows/columns, which have been demonstrated on 2D and 3D OIOs. For a general d𝑑ditalic_d-dimensional OIO, the d𝑑ditalic_d-dimensional tree partitioning leads to a butterfly factorization with a d𝑑ditalic_d-fold reduction in the number of levels compared to the binary tree partitioning. However, we note that both the multi-dimensional and binary tree-based butterfly algorithms are still matrix-based algorithms that scale as O⁢(nd⁢log⁡n)𝑂superscript𝑛𝑑𝑛O(n^{d}\log n)italic_O ( italic_n start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_log italic_n ), as opposed to the proposed tensor algorithm that scales as O⁢(nd)𝑂superscript𝑛𝑑O(n^{d})italic_O ( italic_n start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ). Quantized tensor train (QTT) algorithms, or simply TT algorithms, are tensor algorithms well-suited for very high-dimensional integral operators. They have been proposed to compress volume integral operators [14] arising from quasi-static wave equations and static PDEs with O⁢(log⁡n)𝑂𝑛O(\log n)italic_O ( roman_log italic_n ) memory and CPU complexities. However, for high-frequency wave equations, the QTT rank scales proportionally to the wave number [14] leading to deteriorated CPU and memory complexities (see our numerical results in Section 4). Moreover, QTT has been proposed for computing FFT and QFT with O⁢(log⁡n)𝑂𝑛O(\log n)italic_O ( roman_log italic_n ) memory and CPU complexities [9]. However, after obtaining the QTT-compressed formats of both the volume-integral operator and the Fourier transform, the CPU complexity for contracting such a QTT compressed operator with arbitrary (i.e., non QTT-compressed) input data scales super-linearly. In contrast, our algorithm yields a linear CPU and memory complexity for the contraction operation. 1.2 Contents In what follows, we first review the matrix low-rank decomposition and butterfly decomposition algorithms in Section 2. In Section 3.1, we introduce the Tucker-like interpolative decomposition algorithm as the building block for the proposed tensor butterfly algorithm detailed in Section 3.2. The multi-linear butterfly ranks for a few special cases are analyzed in Section 3.2.1 and the complete complexity analysis is given in Section 3.2.2. Section 4 shows a variety of numerical examples, including Green’s functions for wave equations, Radon transforms, and uniform and non-uniform discrete Fourier transforms, to demonstrate the performance of matrix butterfly, tensor butterfly, Tucker and QTT algorithms. 1.3 Notations Given a scalar-valued function f⁢(x)𝑓𝑥f(x)italic_f ( italic_x ), its integral transform is defined as (1) g⁢(x)=∫yK⁢(x,y)⁢f⁢(y)⁢𝑑y𝑔𝑥subscript𝑦𝐾𝑥𝑦𝑓𝑦differential-d𝑦g(x)=\int_{y}K(x,y)f(y)dyitalic_g ( italic_x ) = ∫ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT italic_K ( italic_x , italic_y ) italic_f ( italic_y ) italic_d italic_y with an integral kernel K⁢(x,y)𝐾𝑥𝑦K(x,y)italic_K ( italic_x , italic_y ). The indexing of a matrix 𝐊𝐊\mathbf{K}bold_K is denoted by 𝐊⁢(i,j)𝐊𝑖𝑗\mathbf{K}(i,j)bold_K ( italic_i , italic_j ) or 𝐊⁢(t,s)𝐊𝑡𝑠\mathbf{K}(t,s)bold_K ( italic_t , italic_s ), where i,j𝑖𝑗i,jitalic_i , italic_j are indices and t,s𝑡𝑠t,sitalic_t , italic_s are index sets. We use 𝐊Tsuperscript𝐊𝑇\mathbf{K}^{T}bold_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT to denote the transpose of matrix 𝐊𝐊\mathbf{K}bold_K. For a sequence of matrices 𝐊1,…,𝐊nsubscript𝐊1…subscript𝐊𝑛\mathbf{K}_{1},\ldots,\mathbf{K}_{n}bold_K start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , bold_K start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, the matrix product is (2) ∏i=1n𝐊i=𝐊1⁢𝐊2⁢…⁢𝐊n,superscriptsubscriptproduct𝑖1𝑛subscript𝐊𝑖subscript𝐊1subscript𝐊2…subscript𝐊𝑛\prod_{i=1}^{n}\mathbf{K}_{i}=\mathbf{K}_{1}\mathbf{K}_{2}\ldots\mathbf{K}_{n},∏ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT bold_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_K start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_K start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT … bold_K start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , the vertical stacking (assuming the same column dimension) is (3) [𝐊i]i=[𝐊1;𝐊2;…;𝐊n],subscriptdelimited-[]subscript𝐊𝑖𝑖subscript𝐊1subscript𝐊2…subscript𝐊𝑛[\mathbf{K}_{i}]_{i}=[\mathbf{K}_{1};\mathbf{K}_{2};\ldots;\mathbf{K}_{n}],[ bold_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ bold_K start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; bold_K start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ; … ; bold_K start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] , and (4) diagi⁢(𝐊i)=diag⁢(𝐊1,𝐊2,…,𝐊n)subscriptdiag𝑖subscript𝐊𝑖diagsubscript𝐊1subscript𝐊2…subscript𝐊𝑛\text{diag}_{i}(\mathbf{K}_{i})=\text{diag}(\mathbf{K}_{1},\mathbf{K}_{2},% \ldots,\mathbf{K}_{n})diag start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) = diag ( bold_K start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_K start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , bold_K start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) is a block diagonal matrix with 𝐊isubscript𝐊𝑖\mathbf{K}_{i}bold_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT being the diagonal blocks. Given an L𝐿Litalic_L-level binary-tree partitioning 𝒯tsubscript𝒯𝑡\mathcal{T}_{t}caligraphic_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT of an index set t={1,2,⋯,n}𝑡12⋯𝑛t=\{1,2,\cdot\cdot\cdot,n\}italic_t = { 1 , 2 , ⋯ , italic_n }, any node τ𝜏\tauitalic_τ at each level is a subset of t𝑡titalic_t. The parent and children of τ𝜏\tauitalic_τ are denoted by pτsubscript𝑝𝜏p_{\tau}italic_p start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT and τcsuperscript𝜏𝑐\tau^{c}italic_τ start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT (c=1,2𝑐12c=1,2italic_c = 1 , 2), respectively, and τ=τ1∪τ2𝜏superscript𝜏1superscript𝜏2\tau=\tau^{1}\cup\tau^{2}italic_τ = italic_τ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ∪ italic_τ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. A multi-index 𝒊=(i1,⋯,id)𝒊subscript𝑖1⋯subscript𝑖𝑑\bm{i}=(i_{1},\cdot\cdot\cdot,i_{d})bold_italic_i = ( italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_i start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) is a tuple of indices, and similarly a multi-set 𝝉=(τ1,τ2,⋯,τd)𝝉subscript𝜏1subscript𝜏2⋯subscript𝜏𝑑\bm{\tau}=(\tau_{1},\tau_{2},\cdot\cdot\cdot,\tau_{d})bold_italic_τ = ( italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_τ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_τ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) is a tuple of index sets. We define (5) 𝝉k←t=(τ1,τ2,⋯,τk−1,t,τk+1,τk+2,⋯,τd).subscript𝝉←𝑘𝑡subscript𝜏1subscript𝜏2⋯subscript𝜏𝑘1𝑡subscript𝜏𝑘1subscript𝜏𝑘2⋯subscript𝜏𝑑\bm{\tau}_{k\leftarrow t}=(\tau_{1},\tau_{2},\cdot\cdot\cdot,\tau_{k-1},t,\tau% _{k+1},\tau_{k+2},\cdot\cdot\cdot,\tau_{d}).bold_italic_τ start_POSTSUBSCRIPT italic_k ← italic_t end_POSTSUBSCRIPT = ( italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_τ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_τ start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT , italic_t , italic_τ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT , italic_τ start_POSTSUBSCRIPT italic_k + 2 end_POSTSUBSCRIPT , ⋯ , italic_τ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) . Given a tuple of nodes (i.e. a multi-set) 𝝉=(τ1,τ2,⋯,τd)𝝉subscript𝜏1subscript𝜏2⋯subscript𝜏𝑑\bm{\tau}=(\tau_{1},\tau_{2},\cdot\cdot\cdot,\tau_{d})bold_italic_τ = ( italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_τ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_τ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) and a multi-index 𝒄=(c1,c2,⋯,cd)𝒄subscript𝑐1subscript𝑐2⋯subscript𝑐𝑑\bm{c}=(c_{1},c_{2},\cdot\cdot\cdot,c_{d})bold_italic_c = ( italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_c start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) with ci∈{1,2}subscript𝑐𝑖12c_{i}\in\{1,2\}italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ { 1 , 2 }, the children of 𝝉𝝉\bm{\tau}bold_italic_τ are denoted 𝝉𝒄=(τ1c1,τ2c2,⋯,τdcd)superscript𝝉𝒄superscriptsubscript𝜏1subscript𝑐1superscriptsubscript𝜏2subscript𝑐2⋯superscriptsubscript𝜏𝑑subscript𝑐𝑑\bm{\tau}^{\bm{c}}=({\tau_{1}}^{c_{1}},{\tau_{2}}^{c_{2}},\cdot\cdot\cdot,{% \tau_{d}}^{c_{d}})bold_italic_τ start_POSTSUPERSCRIPT bold_italic_c end_POSTSUPERSCRIPT = ( italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , italic_τ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT , ⋯ , italic_τ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) and the parents of τisubscript𝜏𝑖\tau_{i}italic_τ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i=1,2,⋯,d𝑖12⋯𝑑i=1,2,\cdot\cdot\cdot,ditalic_i = 1 , 2 , ⋯ , italic_d can be simply written as 𝒑𝝉=(pτ1,pτ2,⋯,pτd)subscript𝒑𝝉subscript𝑝subscript𝜏1subscript𝑝subscript𝜏2⋯subscript𝑝subscript𝜏𝑑\bm{p}_{\bm{\tau}}=(p_{\tau_{1}},p_{\tau_{2}},\cdot\cdot\cdot,p_{\tau_{d}})bold_italic_p start_POSTSUBSCRIPT bold_italic_τ end_POSTSUBSCRIPT = ( italic_p start_POSTSUBSCRIPT italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_τ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , ⋯ , italic_p start_POSTSUBSCRIPT italic_τ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT ). Similar to the above-described notations, we can replace the index i𝑖iitalic_i in [𝐊i]isubscriptdelimited-[]subscript𝐊𝑖𝑖[\mathbf{K}_{i}]_{i}[ bold_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and diagi⁢(𝐊i)subscriptdiag𝑖subscript𝐊𝑖\text{diag}_{i}(\mathbf{K}_{i})diag start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) with an index set τ𝜏\tauitalic_τ, a multi-index 𝒄𝒄\bm{c}bold_italic_c, or a multi-set 𝝉𝝉\bm{\tau}bold_italic_τ assuming certain predefined index ordering. Given complex-valued (or real-valued) functions f⁢(x)𝑓𝑥f(x)italic_f ( italic_x ) of d𝑑ditalic_d variables and integral operators K⁢(x,y)𝐾𝑥𝑦K(x,y)italic_K ( italic_x , italic_y ), the tensor representations of their discretizations are respectively denoted by 𝓕∈ℂn1×n2×⋯×nd𝓕superscriptℂsubscript𝑛1subscript𝑛2⋯subscript𝑛𝑑\bm{\mathcal{F}}\in\mathbb{C}^{n_{1}\times n_{2}\times\cdot\cdot\cdot\times n_% {d}}bold_caligraphic_F ∈ blackboard_C start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × ⋯ × italic_n start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and 𝓚∈ℂm1×m2×⋯×md×n1×n2×⋯×nd𝓚superscriptℂsubscript𝑚1subscript𝑚2⋯subscript𝑚𝑑subscript𝑛1subscript𝑛2⋯subscript𝑛𝑑\bm{\mathcal{K}}\in\mathbb{C}^{m_{1}\times m_{2}\times\cdot\cdot\cdot\times m_% {d}\times n_{1}\times n_{2}\times\cdot\cdot\cdot\times n_{d}}bold_caligraphic_K ∈ blackboard_C start_POSTSUPERSCRIPT italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_m start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × ⋯ × italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT × italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × ⋯ × italic_n start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, where n1,⋯,ndsubscript𝑛1⋯subscript𝑛𝑑n_{1},\cdots,n_{d}italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_n start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and m1,⋯,mdsubscript𝑚1⋯subscript𝑚𝑑m_{1},\cdots,m_{d}italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT are sizes of discretizations for the corresponding variables. In this paper, we use matricization to denote the reshaping of 𝓚𝓚\bm{\mathcal{K}}bold_caligraphic_K into a (Πk⁢mk)×(Πk⁢nk)subscriptΠ𝑘subscript𝑚𝑘subscriptΠ𝑘subscript𝑛𝑘(\Pi_{k}m_{k})\times(\Pi_{k}n_{k})( roman_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) × ( roman_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) matrix, and the reshaping of 𝓕𝓕\bm{\mathcal{F}}bold_caligraphic_F into a (Πk⁢nk)×1subscriptΠ𝑘subscript𝑛𝑘1(\Pi_{k}n_{k})\times 1( roman_Π start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) × 1 matrix. The entries of 𝓕𝓕\bm{\mathcal{F}}bold_caligraphic_F and 𝓚𝓚\bm{\mathcal{K}}bold_caligraphic_K are denoted by 𝓕⁢(𝒊)𝓕𝒊\bm{\mathcal{F}}(\bm{i})bold_caligraphic_F ( bold_italic_i ) (or equivalently 𝓕⁢(i1,i2,⋯,id)𝓕subscript𝑖1subscript𝑖2⋯subscript𝑖𝑑\bm{\mathcal{F}}(i_{1},i_{2},\cdot\cdot\cdot,i_{d})bold_caligraphic_F ( italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_i start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )) and 𝓚⁢(𝒊,𝒋)𝓚𝒊𝒋\bm{\mathcal{K}}(\bm{i},\bm{j})bold_caligraphic_K ( bold_italic_i , bold_italic_j ), respectively. Similarly the subtensors are denoted by 𝓕⁢(𝝉)𝓕𝝉\bm{\mathcal{F}}(\bm{\tau})bold_caligraphic_F ( bold_italic_τ ) (or equivalently 𝓕⁢(τ1,τ2,⋯,τd)𝓕subscript𝜏1subscript𝜏2⋯subscript𝜏𝑑\bm{\mathcal{F}}(\tau_{1},\tau_{2},\cdot\cdot\cdot,\tau_{d})bold_caligraphic_F ( italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_τ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , italic_τ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT )) and 𝓚⁢(𝝉,𝝂)𝓚𝝉𝝂\bm{\mathcal{K}}(\bm{\tau},\bm{\nu})bold_caligraphic_K ( bold_italic_τ , bold_italic_ν ). Given a d𝑑ditalic_d-mode tensor 𝓕∈ℂn1×n2×⋯×nd𝓕superscriptℂsubscript𝑛1subscript𝑛2⋯subscript𝑛𝑑\bm{\mathcal{F}}\in\mathbb{C}^{n_{1}\times n_{2}\times\cdot\cdot\cdot\times n_% {d}}bold_caligraphic_F ∈ blackboard_C start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × ⋯ × italic_n start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, the mode-j𝑗jitalic_j unfolding is denoted by 𝐅(j)∈ℂ(Πk≠j⁢nk)×njsuperscript𝐅𝑗superscriptℂsubscriptΠ𝑘𝑗subscript𝑛𝑘subscript𝑛𝑗\mathbf{F}^{(j)}\in\mathbb{C}^{(\Pi_{k\neq j}n_{k})\times n_{j}}bold_F start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT ∈ blackboard_C start_POSTSUPERSCRIPT ( roman_Π start_POSTSUBSCRIPT italic_k ≠ italic_j end_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) × italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, the mode-j𝑗jitalic_j tensor-matrix product of 𝓕𝓕\bm{\mathcal{F}}bold_caligraphic_F with a matrix 𝐗∈ℂm×nj𝐗superscriptℂ𝑚subscript𝑛𝑗\mathbf{X}\in\mathbb{C}^{m\times n_{j}}bold_X ∈ blackboard_C start_POSTSUPERSCRIPT italic_m × italic_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is denoted by 𝓨=𝓕×j𝐗𝓨subscript𝑗𝓕𝐗\bm{\mathcal{Y}}=\bm{\mathcal{F}}\times_{j}\mathbf{X}bold_caligraphic_Y = bold_caligraphic_F × start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_X, or equivalently 𝐘(j)=𝐅(j)⁢𝐗Tsuperscript𝐘𝑗superscript𝐅𝑗superscript𝐗𝑇\mathbf{Y}^{(j)}=\mathbf{F}^{(j)}\mathbf{X}^{T}bold_Y start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT = bold_F start_POSTSUPERSCRIPT ( italic_j ) end_POSTSUPERSCRIPT bold_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT."
https://arxiv.org/html/2411.02952v1,"A stabilized nonconforming finite element method for the surface
biharmonic problem††thanks:This study is supported in part by the National Natural
Science Foundation of China grant No. 12222101 and the Beijing Natural
Science Foundation No. 1232007.","This paper presents a novel stabilized nonconforming finite element method for solving the surface biharmonic problem. The method extends the New-Zienkiewicz-type (NZT) element to polyhedral (approximated) surfaces by employing the Piola transform to establish the connection of vertex gradients across adjacent elements. Key features of the surface NZT finite element space include its H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT-relative conformity and weak 𝑯⁢(div)𝑯div\bm{H}({\rm div})bold_italic_H ( roman_div ) conformity, allowing for stabilization without the use of artificial parameters. Under the assumption that the exact solution and the dual problem possess only H3superscript𝐻3H^{3}italic_H start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT regularity, we establish optimal error estimates in the energy norm and provide, for the first time, a comprehensive analysis yielding optimal second-order convergence in the broken H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm. Numerical experiments are provided to support the theoretical results.","Fourth-order partial differential equations (PDEs) on surfaces are widely applied in engineering and physics, including in thin shells [6], the surface Cahn-Hilliard equation [15], the surface Navier-Stokes equations [22], and biomembranes [16, 4]. In this paper, we consider the surface biharmonic problem as follows: (1) Δγ2⁢u=fon ⁢γ,∫γu⁢dσ=0.formulae-sequencesuperscriptsubscriptΔ𝛾2𝑢𝑓on 𝛾subscript𝛾𝑢differential-d𝜎0\Delta_{\gamma}^{2}u=f\quad\text{on }\gamma,\quad\int_{\gamma}u\mathrm{d}% \sigma=0.roman_Δ start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_u = italic_f on italic_γ , ∫ start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT italic_u roman_d italic_σ = 0 . Here, γ⊂ℝ3𝛾superscriptℝ3\gamma\subset\mathbb{R}^{3}italic_γ ⊂ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT is a compact, closed, and orientable two-dimensional surface without boundary, subject to certain smoothness conditions, and ΔγsubscriptΔ𝛾\Delta_{\gamma}roman_Δ start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT denotes the Laplace-Beltrami operator. The source term f𝑓fitalic_f satisfies the compatibility condition ∫γf⁢dσ=0subscript𝛾𝑓differential-d𝜎0\int_{\gamma}f\,\mathrm{d}\sigma=0∫ start_POSTSUBSCRIPT italic_γ end_POSTSUBSCRIPT italic_f roman_d italic_σ = 0. More specific assumptions and notation are given in Section 2. As a widely utilized discrete method for PDEs on surfaces, surface finite element methods (SFEMs) establish a Galerkin method on the polyhedral (or higher-order) approximated surface ΓhsubscriptΓℎ\Gamma_{h}roman_Γ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT of γ𝛾\gammaitalic_γ. Numerous studies have explored the SFEMs for second-order Laplace-Beltrami operator, which is discussed in review articles [13, 3] and their references. Studies utilizing SFEMs to address fourth-order problems include the use of second-order splitting techniques [14, 24] and the Hellan-Herrmann-Johnson mixed method, which is specifically designed for solving the surface Kirchhoff plate problem [25]. This latter method can also incorporate Gauss curvature terms to tackle surface biharmonic problem. Additionally, Larsson and Larson [19] proposed a scheme that combines continuous piecewise quadratic finite elements with an interior penalty formulation to address jumps in the normal component. In [5], continuous piecewise linear elements are employed to reconstruct gradients at vertices using weighted average methods, resulting in a continuous piecewise linear reconstructed gradient. In the error estimate of this method, the mesh is assumed to satisfy the 𝒪⁢(h2)𝒪superscriptℎ2\mathcal{O}(h^{2})caligraphic_O ( italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )-symmetry condition [27] to obtain the superconvergence property of the reconstructed gradient. In the variational formulation of (1), we require that the function values and the normal components of its gradient are continuous along any curve, that is, they belong to H2⁢(γ)superscript𝐻2𝛾H^{2}(\gamma)italic_H start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_γ ). However, achieving such strong conformity on the discrete surface ΓhsubscriptΓℎ\Gamma_{h}roman_Γ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT is generally not possible. This limitation arises because ΓhsubscriptΓℎ\Gamma_{h}roman_Γ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT is often only Lipschitz continuous, and the varying tangential directions of each discrete element prevent the gradient from maintaining continuity across the edges. On the other hand, for the planar biharmonic problem, it is common to use the Hessian operator in the variational formulation, which incorporates all second-order derivatives, rather than the Laplace operator. The Hessian operator clearly corresponds to a stronger energy norm, enhancing the stability of the method at both the continuous and discrete levels. However, on general surfaces, using the variational formulations of these two operators introduces additional terms related to Gaussian curvature [22]; see also the discussion in Remark 2.1. As a result, directly substituting the surface Hessian for the Laplace-Beltrami operator is not feasible, creating challenges in designing stable numerical methods. To address these challenges, in this paper, we extend the New-Zienkiewicz-type (NZT) element to surfaces. The NZT element, introduced by Wang, Shi, and Xu in [26], is a class of continuous finite elements with degrees of freedom (DoFs) that include vertex values and vertex gradients. On planar domains, the gradient of the NZT finite element space exhibits weak 𝑯⁢(div)𝑯div\bm{H}({\rm div})bold_italic_H ( roman_div ) properties, ensuring convergence for fourth-order problems. For handling tangential gradient DoFs on the discrete surface ΓhsubscriptΓℎ\Gamma_{h}roman_Γ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, we adopt a recent approach by Demlow and Neilan that provides a novel perspective on vector-valued nodal DoFs [11] in solving the surface Stokes problem. Specifically, we modify the NZT finite element space using the inter-element Piola transformation introduced in [11], which preserves the weak 𝑯⁢(divΓh;Γh)𝑯subscriptdivsubscriptΓℎsubscriptΓℎ\bm{H}({\rm div}_{\Gamma_{h}};\Gamma_{h})bold_italic_H ( roman_div start_POSTSUBSCRIPT roman_Γ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUBSCRIPT ; roman_Γ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) properties of the discrete space. Although adjacent elements on the discrete surface cannot share identical vertex gradient values, it can still be shown that the surface NZT finite element space achieves H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT-relative conformity. These properties of the discrete space enable the design of a nonconforming scheme with a parameter-free stabilization. The proposed nonstandard (nonconforming) finite element space presents several analytical challenges. To establish the approximation properties of the surface NZT finite element space, we carefully analyze the behavior of the tangential gradient and its Piola transformation from γ𝛾\gammaitalic_γ to ΓhsubscriptΓℎ\Gamma_{h}roman_Γ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, identifying the essential property in Lemma 2.4, which provides an 𝒪⁢(h2)𝒪superscriptℎ2\mathcal{O}(h^{2})caligraphic_O ( italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) approximation of the Piola transformation. For the error estimate, it is sufficient to assume that the solution has H3superscript𝐻3H^{3}italic_H start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT regularity, rather than requiring H4superscript𝐻4H^{4}italic_H start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT regularity [19, 5]. Furthermore, through detailed analysis, we show that in many error terms, replacing the finite element function with an interpolant from H3⁢(γ)superscript𝐻3𝛾H^{3}(\gamma)italic_H start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( italic_γ ) yields a higher-order estimate. Based on these analyses, the dual argument allows us to achieve a new second-order error estimate in the broken H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm. This paper is organized as follows: Section 2 introduces the preliminaries for the surface operator and its discretization, including the surface Piola transform used for both construction and analysis, as well as the planar NZT element. In Section 3, we present the stabilized nonconforming finite element method based on the surface NZT element for solving (1), together with the approximation properties of the finite element space and the stability of the numerical scheme. Section 4 demonstrates the optimal error estimates in both the energy norm and the broken H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm. Finally, numerical results are provided in Section 5. We shall use X≲Yless-than-or-similar-to𝑋𝑌X\lesssim Yitalic_X ≲ italic_Y (resp. X≳Ygreater-than-or-equivalent-to𝑋𝑌X\gtrsim Yitalic_X ≳ italic_Y) to denote X≤C⁢Y𝑋𝐶𝑌X\leq CYitalic_X ≤ italic_C italic_Y (resp. X≥C⁢Y𝑋𝐶𝑌X\geq CYitalic_X ≥ italic_C italic_Y), where C𝐶Citalic_C is a constant independent of the mesh size hℎhitalic_h. Additionally, X≃Ysimilar-to-or-equals𝑋𝑌X\simeq Yitalic_X ≃ italic_Y will signify that both X≲Yless-than-or-similar-to𝑋𝑌X\lesssim Yitalic_X ≲ italic_Y and X≳Ygreater-than-or-equivalent-to𝑋𝑌X\gtrsim Yitalic_X ≳ italic_Y hold."
https://arxiv.org/html/2411.02895v1,The shift-and-invert Arnoldi method for singular matrix pencils,"The numerical solution of singular generalized eigenvalue problems is still challenging. In Hochstenbach, Mehl, and Plestenjak, Solving Singular Generalized Eigenvalue Problems by a Rank-Completing Perturbation, SIMAX 2019, a rank-completing perturbation was proposed and a related bordering of the singular pencil. For large sparse pencils, we propose an LU factorization that determines a rank completing perturbation that regularizes the pencil and that is then used in the shift-and-invert Arnoldi method to obtain eigenvalues nearest a shift. Numerical examples illustrate the theory and the algorithms.","We consider the generalized eigenvalue problem (1) A⁢𝐱=λ⁢B⁢𝐱,𝐴𝐱𝜆𝐵𝐱A{\bf x}=\lambda B{\bf x},italic_A bold_x = italic_λ italic_B bold_x , where A,B∈ℂn×n𝐴𝐵superscriptℂ𝑛𝑛A,B\in\mathbb{C}^{n\times n}italic_A , italic_B ∈ roman_ℂ start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT. If the matrix pencil A−λ⁢B𝐴𝜆𝐵A-\lambda Bitalic_A - italic_λ italic_B is singular, i.e., det⁢(A−λ⁢B)≡0det𝐴𝜆𝐵0\mathrm{det}(A-\lambda B)\equiv 0roman_det ( italic_A - italic_λ italic_B ) ≡ 0, (1) is called a singular eigenvalue problem. In this case, the classical definition of eigenvalues becomes useless since any scalar λ0∈ℂsubscript𝜆0ℂ\lambda_{0}\in\mathbb{C}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ roman_ℂ would be an eigenvalue since det⁢(A−λ0⁢B)=0det𝐴subscript𝜆0𝐵0\mathrm{det}(A-\lambda_{0}B)=0roman_det ( italic_A - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_B ) = 0. Therefore, the meaningful (finite) eigenvalues [6], are defined as λ0∈ℂsubscript𝜆0ℂ\lambda_{0}\in\mathbb{C}italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ roman_ℂ such that rank⁢(A−λ0⁢B)<nrank⁢(A−λ⁢B)rank𝐴subscript𝜆0𝐵nrank𝐴𝜆𝐵\mathrm{rank}(A-\lambda_{0}B)<\mathrm{nrank}(A-\lambda B)roman_rank ( italic_A - italic_λ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT italic_B ) < roman_nrank ( italic_A - italic_λ italic_B ), where nrank⁢(A−λ⁢B):=maxλ∈ℂ⁡rank⁢(A−λ⁢B)assignnrank𝐴𝜆𝐵subscript𝜆ℂrank𝐴𝜆𝐵\mathrm{nrank}(A-\lambda B):=\max_{\lambda\in\mathbb{C}}\mathrm{rank}(A-% \lambda B)roman_nrank ( italic_A - italic_λ italic_B ) := roman_max start_POSTSUBSCRIPT italic_λ ∈ roman_ℂ end_POSTSUBSCRIPT roman_rank ( italic_A - italic_λ italic_B ) is called the normal rank of the matrix pencil A−λ⁢B𝐴𝜆𝐵A-\lambda Bitalic_A - italic_λ italic_B. Numerically, the normal rank can be computed as rank⁢(A−μ⁢B)rank𝐴𝜇𝐵\mathrm{rank}(A-\mu B)roman_rank ( italic_A - italic_μ italic_B ), where μ∈ℂ𝜇ℂ\mu\in\mathbb{C}italic_μ ∈ roman_ℂ is a randomly chosen scalar. According to the definition, we say that there is an eigenvalue at infinity if rank⁢(B)<nrank⁢(A−λ⁢B)rank𝐵nrank𝐴𝜆𝐵\mathrm{rank}(B)<\mathrm{nrank}(A-\lambda B)roman_rank ( italic_B ) < roman_nrank ( italic_A - italic_λ italic_B ). Such singular eigenvalue problems arise from many different areas. For instance, updating a finite element model to measured data can lead to a singular multiparameter eigenvalue problem which is further transformed into a singular generalized eigenvalue problem [4], as we show in Section 6.2. Also, the linearization of a polynomial multiparameter eigenvalue problem generates a singular problem [10]. Moreover, solving a rectangular eigenvalue problem can be formulated as a (square) singular pencil [7]. Numerical methods for (regular) matrix pencils are not suitable for singular pencils. We discuss a few options and difficulties. Staircase type methods are proposed to deflate the singular part of the pencil while keeping the regular part unaltered via a row and column compression process, and obtain a smaller regular problem [2, 14]. Then a numerical method for regular problems, such as the QZ method, is employed for the remaining regular part. Though this method is robust for small-scale problems, it can be quite time-consuming for large-scale problems. Moreover, a large number of rank decisions are involved, which can be tricky and unstable, especially after several iterations. Homotopy methods for generalized eigenvalue problems can obtain all eigenpairs [18, 11, 5]. However, for singular problems, they suffer from generating invalid paths; i.e., generated paths may diverge, which is significant for large-scale sparse problems. Moreover, tracing divergent paths generally takes more time than tracing valid ones and there is no way yet to tell which one is invalid before tracing. Also, it is difficult to separate the true eigenvalues from spurious ones since rank determination can be expensive for large-scale problems. Recently, a rank-completing method for singular problems was proposed in [8, 9]. A great advantage of this method is that it provides an efficient way to separate the true eigenvalues from the spurious ones. The main idea of the method is that, for a singular generalized eigenvalue problem, a rank-completing perturbation is applied and transforms the original matrix pencil into the bordered pencil (2) [A−λ⁢BW⁢(TA−λ⁢TB)(SA−λ⁢SB)⁢V∗0].matrix𝐴𝜆𝐵𝑊subscript𝑇𝐴𝜆subscript𝑇𝐵subscript𝑆𝐴𝜆subscript𝑆𝐵superscript𝑉0\begin{bmatrix}A-\lambda B&W(T_{A}-\lambda T_{B})\\ (S_{A}-\lambda S_{B})V^{*}&0\end{bmatrix}.[ start_ARG start_ROW start_CELL italic_A - italic_λ italic_B end_CELL start_CELL italic_W ( italic_T start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT - italic_λ italic_T start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL ( italic_S start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT - italic_λ italic_S start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT ) italic_V start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_CELL start_CELL 0 end_CELL end_ROW end_ARG ] . It is easily proved that there exists an algebraic set 𝕊⊂ℝ(n×(n−k))2𝕊superscriptℝsuperscript𝑛𝑛𝑘2\mathbb{S}\subset\mathbb{R}^{(n\times(n-k))^{2}}roman_𝕊 ⊂ roman_ℝ start_POSTSUPERSCRIPT ( italic_n × ( italic_n - italic_k ) ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT, such that for any V,W∈ℝn×(n−k)∖𝕊𝑉𝑊superscriptℝ𝑛𝑛𝑘𝕊V,W\in\mathbb{R}^{n\times(n-k)}\setminus\mathbb{S}italic_V , italic_W ∈ roman_ℝ start_POSTSUPERSCRIPT italic_n × ( italic_n - italic_k ) end_POSTSUPERSCRIPT ∖ roman_𝕊, the perturbed matrix pencil is regular and includes all regular eigenvalues. This observation led to numerical methods that solve the perturbed regular problem and then uses a test for selecting the regular eigenvalues. We show the properties of (2) in Section 2. Important to mention is that the pencil is regular and that the pencil prescribed eigenvalues determined by SA−λ⁢SBsubscript𝑆𝐴𝜆subscript𝑆𝐵S_{A}-\lambda S_{B}italic_S start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT - italic_λ italic_S start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT and TA−λ⁢TBsubscript𝑇𝐴𝜆subscript𝑇𝐵T_{A}-\lambda T_{B}italic_T start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT - italic_λ italic_T start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT, can be freely chosen. The other eigenvalues are spurious and can be detected by a simple test. See Section 2. In this paper, we aim at solving large scale singular eigenvalue problems, for which the isolation of the singular part is often not feasible, or may destroy the sparse structure of matrices A𝐴Aitalic_A and B𝐵Bitalic_B. There exist plenty of numerical methods for large scale regular eigenvalue problem (1). Here, we focus on the shift-and-invert Arnoldi method. For the singular generalized eigenvalue problem, there is no σ∈ℂ𝜎ℂ\sigma\in\mathbb{C}italic_σ ∈ roman_ℂ so that A−σ⁢B𝐴𝜎𝐵A-\sigma Bitalic_A - italic_σ italic_B is invertible and such problem is still a challenge. The approach consists of two components. First, we apply the shift-and-invert Arnoldi method to the bordered problem (2), with prescribed eigenvalues at infinity, since this is usually an unwanted eigenvalue. The (added) infinite eigenvalue has partial multiplicity one, and can therefore be eliminated using implicit restarting or a special inner product followed by a purification step [13]. The shift σ𝜎\sigmaitalic_σ in the shift-and-invert Arnoldi is set to zero to simplify notation. The use of dense V𝑉Vitalic_V and W𝑊Witalic_W in the border should be avoided for large scale problems. We present a strategy using a rank revealing LU factorization of the bordered matrix that leads to sparse matrices. The side effect of the LU factorization is that a suitable V𝑉Vitalic_V and W𝑊Witalic_W can be found as well, as well as the determination of the normal rank. That is, the LU factorization leads to sparse V𝑉Vitalic_V and W𝑊Witalic_W which makes the method suitable for large scale problems. Moreover, as shown in Section 5, the proposed method also applies to rectangular eigenvalue problems. The remainder of the paper is organized as follows. In Section 2, we introduce the background information and the spectral properties of bordered system. Section 3 presents the Arnoldi method for bordered system. In Section 4, a rank detection technique is employed based on a sparse LU decomposition. In Section 5, we extend our method and rank detection to rectangular problems. Numerical examples are presented in Section 6 to illustrate the theory and the proposed method. Finally, in Section 7, we conclude the paper and discuss future directions of research."
https://arxiv.org/html/2411.02646v1,A Discontinuous Galerkin Method for the Extracellular Membrane Intracellular Model,"We formulate and analyze interior penalty discontinuous Galerkin methods for coupled elliptic PDEs modeling excitable tissue, represented by intracellular and extracellular domains sharing a common interface. The PDEs are coupled through a dynamic boundary condition, posed on the interface, that relates the normal gradients of the solutions to the time derivative of their jump. This system is referred to as the Extracellular Membrane Intracellular model or the cell-by-cell model. Due to the dynamic nature of the interface condition and to the presence of corner singularities, the analysis of discontinuous Galerkin methods is non-standard. We prove the existence and uniqueness of solutions by a reformulation of the problem to one posed on the membrane. Convergence is shown by utilizing face-to-element lifting operators and notions of weak consistency suitable for solutions with low spatial regularity. Further, we present parameter-robust preconditioned iterative solvers. Numerical examples in idealized geometries demonstrate our theoretical findings, and simulations in multiple cells portray the robustness of the method.Key words. Electrophysiology, EMI model, dynamic boundary condition, interior penalty discontinuous Galerkin methods, low spatial regularity.MSC codes. 65N30, 65M60.","Electrophysiology models of excitable cells (such as cardiac cells or neurons) clasically consisted of the monodomain or bidomain equations [45, Ch 2.], [25]. These equations homogenize the tissue and assume coexistence of the extracellular, intracellular spaces and the cell membranes in every spatial point. In contrast, the geometry of each of these three domains is explicitly represented in the Extracellular Membrane Intracellular (EMI) system. In particular, this system models the electric potentials in the cell and its surrounding along with the trans–membrane potential. Consequently, the EMI-type models can naturally incorporate highly detailed reconstructions of biological tissues, e.g. [43], and numerical simulations may be used to shed light on previously unobservable physiological phenomena. For example, simulations of neuronal networks often exclusively model the membrane of the cells, connected through synapses. However, network behaviour may also be influenced by indirect cell communication through the extracellular space [4]. This ephaptic coupling can be captured using EMI-type models. We refer to the book [47] for more details, and to [3, 49] for the analysis of the EMI system. The EMI model is commonly discretized by the H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT-conforming continuous Galerkin (CG) elements, see e.g. [1, 46, 31]. The analysis of the CG formulation has been established in [24]. Recently, there has been a growing interest in using discontinuous Galerkin (DG) methods for the monodomain and bidomain equations [7], (Poisson)-Nernst-Planck equations modeling ionic transport [35, 36, 41] and the related EMI models of electrodiffusion [17]. We note that the EMI equations are closely related to the elliptic interface models of porous media flow in domains containing blocking fractures/barriers. However, compared to EMI, the latter models consist of adjacent domains and do not have the dynamic boundary condition on the interface. For these elliptic interface models, DG methods have been analyzed, for example in [29, 30] under the assumption of smooth solutions and in [12] under minimal regularity assumptions where the authors leverage the so-called medius analysis introduced in [27]. Here, we adopt a completely different approach following [22] to show convergence for solutions with H1+ssuperscript𝐻1𝑠H^{1+s}italic_H start_POSTSUPERSCRIPT 1 + italic_s end_POSTSUPERSCRIPT spatial regularity for 0<s<1/20𝑠120<s<1/20 < italic_s < 1 / 2. Our motivation for considering the nonconforming discretizations is threefold. First, DG schemes provide better local (element-wise) conservation properties. Second, implementation of CG for EMI requires that the finite element (FE) framework supports multimesh (mixed-dimensional) features, which might not be readily available. As an example, in the popular open-source finite element library FEniCS, preliminary support for such features has only recently been added [15]. Here, we argue that discretization by DG allows the EMI model to be implemented in any FE code that can handle standard discontinuous Galerkin methods. Finally, the design of solvers for the linear systems arising from CG discretization is still an active area of research [31, 33, 47, 10]. In particular, [47, Ch 6.] and [10] show that black-box multigrid solvers can fail to provide order-optimal solvers for certain parameter regimes. To this end, we shall below explore multigrid methods with DG. Main contributions. We are mainly concerned with the numerical analysis of the interior penalty DG method formulated for the EMI equations. In particular, the main challenges and contributions are summarized below. • We show existence and uniqueness of DG semi-discrete solutions in Section 3.2. Since the time-dependent term appears as an interface condition on the membrane, a reformulation of the model is required and is attained by introducing suitable lifting operators. The main result is in Theorem 3.6. The stability of the semi-discrete DG solution is established in Section 4. • We prove convergence of the semi-discrete solution under low spatial regularity assumptions where we work with H1+ssuperscript𝐻1𝑠H^{1+s}italic_H start_POSTSUPERSCRIPT 1 + italic_s end_POSTSUPERSCRIPT for s∈(0,1/2)𝑠012s\in(0,1/2)italic_s ∈ ( 0 , 1 / 2 ) functions with Laplacians in L2⁢(Ω)superscript𝐿2ΩL^{2}(\Omega)italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( roman_Ω ). This requires a delicate analysis relying on face-to-element lifting operators and weak notions of consistency established in [22]. This analysis is presented in Section 5 and the main result is Theorem 5.3. Further, we present and analyze a backward Euler DG discretization, see Section 6. • Finally, parameter robust preconditioners with respect to both the time step and the length of the domain are proposed. The convergence properties and the robustness of our solver are demonstrated in several examples in Section 7, including a physiologically relevant 3D simulation of 15 cardiac cells."
https://arxiv.org/html/2411.02356v1,Error Estimate for a Semi-Lagrangian Scheme for Hamilton–Jacobi Equations on Networks,"We examine the numerical approximation of time-dependent Hamilton–Jacobi equations on networks, providing a convergence error estimate for the semi-Lagrangian scheme introduced in [CarliniSiconolfi23], where convergence was proven without an error estimate. We derive a convergence error estimate of order one-half. This is achieved showing the equivalence between two definitions of solutions to this problem proposed in [ImbertMonneau17] and [Siconolfi22], a result of independent interest, and applying a general convergence result from [CarliniFestaForcadel20].","In this article, we focus on the numerical approximation of evolutive Hamilton–Jacobi (HJ) equations posed on networks. In recent years, the study of control problems on networks has gained significant attention due to its broad applicability in various domains, including data transmission, traffic management, flame propagation, and urban infrastructure planning, see [ImbertMonneauZidani12, CamilliFestaTozza17, CamilliCarliniMarchi18, AchdouMannucciMarchiTchou24]. The HJ equation has been extensively studied in classical settings, such as Euclidean unbounded or bounded domains. However, when posed on networks, the problem becomes significantly more challenging. Networks introduce unique complications, such as handling junction conditions at vertices, which represent the points where multiple edges meet. These challenges have led to the development of new mathematical techniques and suitable numerical methods. One key challenge in addressing HJ equations on networks is defining a well-posed solution, especially when each arc has unrelated Hamiltonians. This issue becomes particularly complex in evolutive problems, where discontinuities arise at the vertices due to the one-dimensional nature of the network. Properly handling these vertex discontinuities is critical for ensuring the well-posedness of the problem. Two main approaches have been proposed, both relying on the concept of flux limiters. The first approach, introduced by Imbert and Monneau in [ImbertMonneau17], addresses the difficulties at the vertices by employing special test functions. These functions are defined across the entire network and act simultaneously on all arcs adjacent to a given vertex. In contrast, the second approach, proposed by Siconolfi in [Siconolfi22], does not use special test functions. Instead, standard test functions are defined individually on each arc, acting separately and independently on them. The numerical approximation of the evolutive Hamilton–Jacobi equation in this context remains an active area of research. The inherent complexity of networks, especially the treatment of junctions, has resulted in the need for specialized approximation methods that can accurately and efficiently capture the dynamics. Several approaches have been proposed to tackle these challenges. In [CostesequeLebacqueMonneau14], a finite difference scheme was introduced, and convergence was proven under a Courant–Friedrichs–Lewy (CFL) condition. An error estimate for this scheme was presented in [GuerandKoumaiha19], achieving a convergence order of up to 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG, again under the CFL condition. In [CarliniFestaForcadel20], a semi-Lagrangian scheme was developed to approximate evolutive problems with Hamiltonians that also depend on the state variable. This approach proved particularly effective, as a convergence result was obtained without restrictions on the time step. Moreover, under the same time restrictions imposed in [CostesequeLebacqueMonneau14], the authors proved a rate of convergence of order 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG. More recently, in [CarliniSiconolfi23] a new semi-Lagrangian scheme was proposed, which exploits the viscosity solution framework introduced in [Siconolfi22]. In this framework, convergence is proven under the assumption that the ratio between the space step and the time step tends to zero, effectively an “inverse CFL condition”. This scheme is particularly appealing due to its ability to allow large time steps while remaining explicit. Additionally, its structure—where computations are performed separately on each arc—makes it highly suitable for parallelization. However, a convergence error estimate for this scheme has not yet been proved. In this paper, we aim to provide a convergence error estimate for the scheme proposed in [CarliniSiconolfi23]. To achieve this, we first prove the equivalence between the two definitions of viscosity solutions introduced in [ImbertMonneau17] and [Siconolfi22]. We believe that this equivalence is of independent interest, as it bridges two important frameworks. Furthermore, it allows us to prove a new convergence result and derive a convergence error estimate for the scheme in [CarliniSiconolfi23]. The error estimate is obtained under the same time step restriction as those required in [CostesequeLebacqueMonneau14] for convergence, and in [GuerandKoumaiha19] and [CarliniFestaForcadel20] for error estimates. Taking advantage of the equivalence between the two viscosity solution definitions, we apply the general convergence result of [CarliniFestaForcadel20], which provides an error estimate for any monotone, stable and consistent scheme, with the viscosity solution defined in [ImbertMonneau17]. The paper is organized as follows. In Section 2, we introduce the problem, present the main hypotheses, and provide the basic definitions. Section 3 contains one of the key results of the paper: the proof of the equivalence between the viscosity solution definitions given in [ImbertMonneau17] and [Siconolfi22]. In Section 4, we describe the numerical scheme proposed in [CarliniSiconolfi23] and present our second main result: an error estimate for the scheme under a time step restriction. Finally, in Section 5, we verify the numerical convergence in two test cases with different choices of time steps. Acknowledgments. The first two authors were partially supported by Italian Ministry of Instruction, University and Research (MIUR) (PRIN Project2022238YY5, “Optimal control problems: analysis, approximation”) and INdAM-research group GNCS (CUP__\__ E53C23001670001, “Metodi numerici innovativi per equazioni di Hamilton–Jacobi”). The first author was partially supported by KAUST through the subaward agreement ORA-2021-CRG10-4674.6. The third author is a member of the INdAM research group GNAMPA."
https://arxiv.org/html/2411.02333v1,Discrete the solving model of time-variant standard Sylvester-conjugate matrix equations using Euler-forward formula: An analysis of the differences between sampling discretion errors and space compressive approximation errors in optimizing neural dynamics,"Time-variant standard Sylvester-conjugate matrix equations are presented as early time-variant versions of the complex conjugate matrix equations. Current solving methods include Con-CZND1 and Con-CZND2 models, both of which use ode45 for continuous model. Given practical computational considerations, discrete these models is also important. Based on Euler-forward formula discretion, Con-DZND1-2i model and Con-DZND2-2i model are proposed. Numerical experiments using step sizes of 0.1 and 0.001. The above experiments show that Con-DZND1-2i model and Con-DZND2-2i model exhibit different neural dynamics compared to their continuous counterparts, such as trajectory correction in Con-DZND2-2i model and the swallowing phenomenon in Con-DZND1-2i model, with convergence affected by step size. These experiments highlight the differences between optimizing sampling discretion errors and space compressive approximation errors in neural dynamics.","Standard Sylvester-conjugate matrix equations (SSCME) [1] are the earliest version of complex conjugate matrix equations (CCME) [2]. And SSCME is time-invariant. In recent years of studies, Wu et al. provided matrix algebraic formula methods [3] and iterative solving methods [2] based on approximation theory. The essence of the iterative methods is to gradually approach the theoretical solution using multi-step computations. And time-variant standard Sylvester-conjugate matrix equations (TVSSCME) [4] is the time-variant extension of SSCME. TVSSCME is supplemented by the differences between differential algebra and linear algebra operations [5, 6]. The difference between SSCME and TVSSCME solutions is shown in Fig. 1. Unless otherwise specified, let O𝑂Oitalic_O represent “null matrix"", and only consider the unique theoretical solution X∗⁢(τ)superscript𝑋∗𝜏X^{\ast}(\tau)italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_τ ), same as below. (a)X⁢(τ)⁢F−A⁢X⁢(τ)−C=O𝑋𝜏𝐹𝐴𝑋𝜏𝐶𝑂X(\tau)F-AX(\tau)-C=Oitalic_X ( italic_τ ) italic_F - italic_A italic_X ( italic_τ ) - italic_C = italic_O, where τ→+∞→𝜏\tau\to+\inftyitalic_τ → + ∞, X⁢(τ)→X∗⁢(τ).→𝑋𝜏superscript𝑋∗𝜏X(\tau)\to X^{\ast}(\tau).italic_X ( italic_τ ) → italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_τ ) .(b)X⁢(τ)⁢F⁢(τ)−A⁢(τ)⁢X⁢(τ)−C⁢(τ)=O𝑋𝜏𝐹𝜏𝐴𝜏𝑋𝜏𝐶𝜏𝑂X(\tau)F(\tau)-A(\tau)X(\tau)-C(\tau)=Oitalic_X ( italic_τ ) italic_F ( italic_τ ) - italic_A ( italic_τ ) italic_X ( italic_τ ) - italic_C ( italic_τ ) = italic_O, where τ→+∞→𝜏\tau\to+\inftyitalic_τ → + ∞, X⁢(τ)→X∗⁢(τ).→𝑋𝜏superscript𝑋∗𝜏X(\tau)\to X^{\ast}(\tau).italic_X ( italic_τ ) → italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ( italic_τ ) . Figure 1: Differences between SSCME(a) and TVSSCME(b). TVSSCME is currently primarily solved using zeroing neural dynamics (ZND) models Con-CZND1 [4] and Con-CZND2 [4]. Above two models structure can be seen in Fig. 2. Random InputRandom InputRandom InputRandom Input⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{⋮⋮\vdots⋮…⋮⋮\vdots⋮⋮⋮\vdots⋮OutputOutputOutputOutput⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{ Hidden Complex Layer1 Input Real Layer Hidden Complex Layer2 Hidden Real Layer1 Output Real Layer Random InputRandom InputRandom InputRandom Input⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{…⋮⋮\vdots⋮⋮⋮\vdots⋮OutputOutputOutputOutput⋮⋮\vdots⋮n{⋮⋮\vdots⋮n{ Input Real Layer Hidden Real Layer1 Output Real Layer Figure 2: Different between Con-CZND1 [4] model and Con-CZND2 [4] model. 2 Con-CZND1 model. 2 Con-CZND2 model. However, Con-CZND1 model essentially approximates using the complex field error, while Con-CZND2 model approximates using the real field error. In ode45 [7] solver, Con-CZND2 model does not perform as well as Con-CZND1 model. Discrete neural dynamics is validated in previous studies to reduce the error between theoretical and numerical solutions [8]. Zhang et al. continued to develop discretion in the real field, progressing from Euler-forward formula [9, 10] to an 11-point sampling discretion [11, 12, 13]. However, there is no exploration of neural models for solving TVSSCME using sampling discretion in the existing literature. According to the known studies, the two continuous solution models, Con-CZND1 and Con-CZND2, show significant differences due to the approximation effects of the internal ode45 [14] solver. Additionally, Con-CZND1 model exhibits space compressive approximation phenomenon. Therefore, it is essential to rigorously establish a discrete neural dynamics model for TVSSCME. The rest of this article is organized as follows: Section 2 provides the definition of TVSSCME and supplementary knowledge. Section 3 defines the Con-DZND1-2i discrete solving model over the complex field and the Con-DZND2-2i discrete solving model over the real field. Section 4 presents simulations that validate the effectiveness of each model and compares their strengths and weaknesses. Sections 5 and 6 summarizes this article and suggests future directions. Before proceeding to the next section, the main contributions of this article are as follows: (1) Con-DZND1-2i model, which directly defines complex field error, and Con-DZND2-2i model, which maps to real field error, are proposed for solving TVSSCME. (2) Based on Euler-forward formula, both discrete models, Con-DZND1-2i and Con-DZND2-2i, which use different step sizes, can ultimately approximate the theoretical solution. (3) Con-DZND1-2i model defines complex field error, while Con-DZND2-2i model maps to real field error. These models highlight a significant difference between optimizing space compressive approximation errors and optimizing sampling discretion errors in neural network optimization. Both aspects should be considered from different perspectives."
https://arxiv.org/html/2411.02100v1,Fully consistent lowest-order finite element methods for generalised Stokes flows with variable viscosity,"Variable viscosity arises in many flow scenarios, often imposing numerical challenges. Yet, discretisation methods designed specifically for non-constant viscosity are few, and their analysis is even scarcer. In finite element methods for incompressible flows, the most popular approach to allow equal-order velocity-pressure interpolation are residual-based stabilisations. For low-order elements, however, the viscous part of that residual cannot be approximated, often compromising accuracy. Assuming slightly more regularity on the viscosity field, we can construct stabilisation methods that fully approximate the residual, regardless of the polynomial order of the finite element spaces. This work analyses two variants of this fully consistent approach, with the generalised Stokes system as a model problem. We prove unique solvability and derive expressions for the stabilisation parameter, generalising some classical results for constant viscosity. Numerical results illustrate how our method completely eliminates the spurious pressure boundary layers typically induced by low-order PSPG-like stabilisations.","Variable viscosity fields appear in various flow scenarios, such as when non-Newtonian behaviour, turbulence models or temperature gradients are considered. Nonetheless, in most of the Computational Fluid Dynamics literature, the assumption of constant viscosity is often taken for granted. Yet, it is not always straightforward to extend classical numerical techniques to fluid problems with non-constant viscosity. Even a simple, quasi-linear rheological law can lead to numerical instabilities, spurious boundary conditions or undesired matrix structures [1]. In that context, the literature on numerical methods for variable-viscosity flow problems has recently been expanding. Some examples are stabilisation methods [2, 3], fractional-step schemes [1, 4, 5, 6] and mixed formulations [7, 8]. For incompressible flows with variable viscosity, we have recently presented [2] an equal-order stabilised method that solves an old issue of residual-based methods: the weakened consistency and resulting numerical boundary layers observed when low-order elements are used [9]. When the polynomial order of the finite element spaces is smaller than the highest derivative in the equations, part of the stabilising residual vanishes, causing spurious pressure boundary layers [10]. Our modified approach, on the other hand, assumes Lipschitz continuity on the viscosity field to rewrite the viscous term using only first-order derivatives. The resulting formulation has a similar structure and cost as standard residual-based methods, but is fully consistent even for piecewise linear elements. Despite previous numerical experiments showcasing the accuracy and stability of the method [2, 11], an a-priori analysis has not been presented yet, which is thus the main goal of this work. We prove the unique solvability of the modified stabilisation, derive closed-form expressions for the stabilisation parameter and sketch convergence. Numerical examples showcase that our method indeed solves the issue of spurious boundary layers and loss of pressure accuracy on the boundaries."
https://arxiv.org/html/2411.01998v1,Adaptive neural network basis methods for partial differential equations with low-regular solutions,"This paper aims to devise an adaptive neural network basis method for numerically solving a second-order semilinear partial differential equation (PDE) with low-regular solutions in two/three dimensions. The method is obtained by combining basis functions from a class of shallow neural networks and the resulting multi-scale analogues, a residual strategy in adaptive methods and the non-overlapping domain decomposition method. At the beginning, in view of the solution residual, we partition the total domain ΩΩ\Omegaroman_Ω into K+1𝐾1K+1italic_K + 1 non-overlapping subdomains, denoted respectively as {Ωk}k=0KsuperscriptsubscriptsubscriptΩ𝑘𝑘0𝐾\{\Omega_{k}\}_{k=0}^{K}{ roman_Ω start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT, where the exact solution is smooth on subdomain Ω0subscriptΩ0\Omega_{0}roman_Ω start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and low-regular on subdomain ΩksubscriptΩ𝑘\Omega_{k}roman_Ω start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (1≤k≤K1𝑘𝐾1\leq k\leq K1 ≤ italic_k ≤ italic_K). Secondly, the low-regular solutions on different subdomains ΩksubscriptΩ𝑘\Omega_{k}roman_Ω start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (1≤k≤K1𝑘𝐾1\leq k\leq K1 ≤ italic_k ≤ italic_K) are approximated by neural networks with different scales, while the smooth solution on subdomain Ω0subscriptΩ0\Omega_{0}roman_Ω start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is approximated by the initialized neural network. Thirdly, we determine the undetermined coefficients by solving the linear least squares problems directly or the nonlinear least squares problem via the Gauss-Newton method. The proposed method can be extended to multi-level case naturally. Finally, we use this adaptive method for several peak problems in two/three dimensions to show its high-efficient computational performance.","In the past few years, the machine learning including neural networks and artificial intelligence (AI) has experienced rapid and significant advances in computer science and data analysis (cf. [27, 16]). This technology has represented a cornerstone of innovation across various industries, continues to transform how we live and work from enhancing everyday experiences to driving groundbreaking discoveries. More recently, the machine learning method has also become an important approach to numerically solving partial differential equations (PDEs), which is a heart topic in computational and applied mathematics. Initial exploration of such study can be traced back to significant works in the 1990s (cf. [7, 26]). The typical methods along this line include but not limited to the following methods. The Deep Ritz method was introduced in [13] for solving elliptic problems by combining the Ritz method and deep neural works (DNNs). The weak adversarial networks [44] is a machine learning method for solving a weak solution of a PDE based on its weak formulation and using the solution ansatz by DNNs (see also [4]). Another important classes of methods is the so-called Physics-Informed Neural Network (PINN) methods and their variants; we refer the reader to [33, 30, 32, 36, 23, 6, 25] for details. The applications of such methods in computational mathematics can be found in (cf.[12]). In all of the above machine learning methods, DNNs are used to parameterize the solution of PDEs and the related parameters are identified by minimizing an optimization problem formulated from the PDEs. The remarkable advantage of such methods connecting with DNNs is that they can overcome the so-called “curse of dimensionality” (cf. [17, 1, 2, 22, 24]). Furthermore, to improve computational efficiency, adaptive sampling techniques are combined with the PINN approach for solving PDEs with low-regular solutions (cf.[30, 38, 42, 15, 14]). Moreover, such techniques are also combined with the deep Ritz method for solving PDEs in [39]. Despite the DNN-based machine learning method can get over ”curse of dimensionality”, when solving PDEs in low dimensions, one is tempted to use shallow neural network as the solution asatz to balance computational accuracy and cost. Moreover, the weight/bias coefficients in the hidden layer are pre-set to random values and fixed, while the training parameters consist of the weight coefficients of output layer and they are obtained by using existing linear or nonlinear least squares solvers. In other words, one can use the shallow network to produce a basis of the underlying admissible space of the infinite-dimensional minimization problem associated with a PDE under discussion. An important advantage of such a basis is that it is mesh-free. We mention that randomness has been used in neural networks for a long time (cf. [35]). Randomized neural networks can be traced back to Turing’s unorganized machine and Rosenblatt’s perception [41, 34]. Later on, extreme Learning machine (ELM) was proposed in [20, 19, 18] for solving linear classification or regression problems by combining the previous random neural network with the linear least squares method. This method was also used for solving ordinary differential equations (cf. [43, 28]) and partial differential equations (PDEs) (cf. [37, 11]) . The ELM method was further combined with the non-overlapping domain decomposition technique to solve PDEs effectively (cf. [9, 8]). In [40], the method was even applied to solve high-dimensional partial differential equations. The combination of the random neural network and the partition of unity technique gives rise to the random feature method (cf. [5]). More recently, transferable neural networks (cf. [45]) generate hidden layer parameters by re-parameterizing the hidden neurons and using auxiliary functions to tune. It is proved that the hidden layer neurons are uniformly distributed within the unit cell. Numerical results indicate that the computational accuracy is high if the solution is smooth enough. However, the solution accuracy would deteriorate remarkably if the exact solution has low regularity. In this work, we are going to make use of the technique in constructing the basis functions from a shallow neural network, the non-overlapping domain decomposition (DDM), and the multi-scale neural networks [29] to produce an adaptive neural network basis (ANNB) method for solving second-order PDEs with low-regular solutions. The main idea of the ANNB method can be summarized in three steps. First of all, in view of the solution residual, we partition the total domain ΩΩ\Omegaroman_Ω into K+1𝐾1K+1italic_K + 1 non-overlapping subdomains, denoted respectively as Ω0,Ω1,⋯,ΩKsubscriptΩ0subscriptΩ1⋯subscriptΩ𝐾\Omega_{0},\Omega_{1},\cdots,\Omega_{K}roman_Ω start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , roman_Ω start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , roman_Ω start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT, where the solution of the target PDE is smooth on subdomain Ω0subscriptΩ0\Omega_{0}roman_Ω start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and is low-regular on each subdomain ΩksubscriptΩ𝑘\Omega_{k}roman_Ω start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (1≤k≤K1𝑘𝐾1\leq k\leq K1 ≤ italic_k ≤ italic_K). Secondly, the low-regular solutions on different subdomains ΩksubscriptΩ𝑘\Omega_{k}roman_Ω start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (1≤k≤K1𝑘𝐾1\leq k\leq K1 ≤ italic_k ≤ italic_K) are approximated by neural networks with different scales, while the smooth solution on subdomain Ω0subscriptΩ0\Omega_{0}roman_Ω start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is approximated by the initialized neural network. Thirdly, we determine the unknown coefficients by solving the linear least squares problems or the nonlinear least squares problems in view of the Gauss-Newton method. It is worth noting that the domain decomposition can be multi-leveled, though we only consider the two-level case for simplicity. Finally, we use this adaptive method to solve peak problems in two and three dimensions, which show the resulting numerical solution is high accurate. The rest of the paper is organized as follows. In section 2, we show how to construct neural network basis functions and a DDM-based neural network basis method (NNBM) for second-order linear and nonlinear PDEs. In section 3, we present the ANNB method in detail for solving second-order PDEs with low-regular solutions. In section 4, several numerical tests are provided to demonstrate the effectiveness of the ANNB method. Some conclusions are given in section 5. Finally, an appendix is provided to show the implementation details of our ANNM method."
https://arxiv.org/html/2411.01986v1,Randomized coupled decompositions,"Coupled decompositions are a widely used tool for data fusion. This paper studies the coupled matrix factorization (CMF) where two matrices X𝑋Xitalic_X and Y𝑌Yitalic_Y are represented in a low-rank format sharing one common factor, as well as the coupled matrix and tensor factorization (CMTF) where a matrix Y𝑌Yitalic_Y and a tensor 𝒳𝒳\mathcal{X}caligraphic_X are represented in a low-rank format sharing a factor matrix. We show that these problems are equivalent to the low-rank approximation of the matrix [X⁢Y]delimited-[]𝑋𝑌[X\ Y][ italic_X italic_Y ] for CMF, that is [X(1)⁢Y]delimited-[]subscript𝑋1𝑌[X_{(1)}\ Y][ italic_X start_POSTSUBSCRIPT ( 1 ) end_POSTSUBSCRIPT italic_Y ] for CMTF. Then, in order to speed up computation process, we adapt several randomization techniques, namely, randomized SVD, randomized subspace iteration, and randomized block Krylov iteration to the algorithms for coupled decompositions. We present extensive results of the numerical tests. Furthermore, as a novel approach and with a high success rate, we apply our randomized algorithms to the face recognition problem.","Coupled decompositions of multiple data sets are broadly used in different engineering disciplines as a tool for data fusion. They are utilized in the analysis of data coming from different sources, for example, to better describe data obtained by different technologies or methods. To name a few applications, coupled decompositions appear in chemometrics [23, 3, 2, 29], signal processing [30, 26], bioinformatics [5, 6], metabolimics [1, 34, 12], chromatography [25], etc. In this paper we apply them to the problem of face recognition. To the best of our knowledge, this is a novel approach. Standard procedure for face recognition algorithm is to calculate the mean face image, derive the covariance matrix, extract its eigenvectors, and use them to project the images onto lower-dimensional subspace, as explained in [35, 9, 33]. However, coupled decomposition bypasses this procedure and achieves dimensionality reduction by extracting the common part of the images. Coupled matrix factorization (CMF) [22] decomposes a set of matrices in a way that they are represented in a low-rank format sharing one common factor. For two matrices X∈ℝm×n1𝑋superscriptℝ𝑚subscript𝑛1X\in\mathbb{R}^{m\times n_{1}}italic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and Y∈ℝm×n2𝑌superscriptℝ𝑚subscript𝑛2Y\in\mathbb{R}^{m\times n_{2}}italic_Y ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT, their coupled rank-k𝑘kitalic_k approximation is given by X≈U⁢VTandY≈U⁢WT,formulae-sequence𝑋𝑈superscript𝑉𝑇and𝑌𝑈superscript𝑊𝑇X\approx UV^{T}\quad\text{and}\quad Y\approx UW^{T},italic_X ≈ italic_U italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT and italic_Y ≈ italic_U italic_W start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , (1.1) where U∈ℝm×k𝑈superscriptℝ𝑚𝑘U\in\mathbb{R}^{m\times k}italic_U ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_k end_POSTSUPERSCRIPT, V∈ℝn1×k𝑉superscriptℝsubscript𝑛1𝑘V\in\mathbb{R}^{n_{1}\times k}italic_V ∈ blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT × italic_k end_POSTSUPERSCRIPT, W∈ℝn2×k𝑊superscriptℝsubscript𝑛2𝑘W\in\mathbb{R}^{n_{2}\times k}italic_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × italic_k end_POSTSUPERSCRIPT. This is illustrated in Figure 1. We are going to show that the best coupled rank-k𝑘kitalic_k approximation (1.1) of X𝑋Xitalic_X and Y𝑌Yitalic_Y is equivalent to the best rank-k𝑘kitalic_k approximation of the matrix [X⁢Y]∈ℝm×(n1+n2)delimited-[]𝑋𝑌superscriptℝ𝑚subscript𝑛1subscript𝑛2[X\ Y]\in\mathbb{R}^{m\times(n_{1}+n_{2})}[ italic_X italic_Y ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × ( italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT. Hence, we are going to solve the approximation problem (1.1) using the singular value decomposition of [X⁢Y]delimited-[]𝑋𝑌[X\ Y][ italic_X italic_Y ]. ≈\approx≈X𝑋Xitalic_XandU𝑈Uitalic_UVTsuperscript𝑉𝑇V^{T}italic_V start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ≈\approx≈Y𝑌Yitalic_YU𝑈Uitalic_UWTsuperscript𝑊𝑇W^{T}italic_W start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT Figure 1. Graphical description of CMF. Big and complex data sets are very often represented by tensors instead of matrices. That is why, apart from the matrix-matrix case, there is also need for observing tensor-matrix factorization generalizing the problem (1.1). State-of-the-art algorithms for the coupled matrix and tensor factorization (CMTF) use the alternating least squares method [23, 4, 2, 3, 20, 21, 26, 24], which is an iterative method that may encounter convergence issues. We are going to show how CMTF of the tensor 𝒳∈ℝm×n2×n3𝒳superscriptℝ𝑚subscript𝑛2subscript𝑛3\mathcal{X}\in\mathbb{R}^{m\times n_{2}\times n_{3}}caligraphic_X ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT × italic_n start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and matrix Y∈ℝm×n𝑌superscriptℝ𝑚𝑛Y\in\mathbb{R}^{m\times n}italic_Y ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n end_POSTSUPERSCRIPT can be expressed in terms of CMF of the tensor matricization X(1)∈ℝm×n2⁢n3subscript𝑋1superscriptℝ𝑚subscript𝑛2subscript𝑛3X_{(1)}\in\mathbb{R}^{m\times n_{2}n_{3}}italic_X start_POSTSUBSCRIPT ( 1 ) end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT and matrix Y𝑌Yitalic_Y. Consequently, CMTF is simplified and solved using the singular value decomposition of the matrix [X(1)⁢Y]∈ℝm×(n2⁢n3+n)delimited-[]subscript𝑋1𝑌superscriptℝ𝑚subscript𝑛2subscript𝑛3𝑛[X_{(1)}\ Y]\in\mathbb{R}^{m\times(n_{2}n_{3}+n)}[ italic_X start_POSTSUBSCRIPT ( 1 ) end_POSTSUBSCRIPT italic_Y ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_m × ( italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_n start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT + italic_n ) end_POSTSUPERSCRIPT, instead of the alternating least squares technique. Since the aforementioned factorizations can be time consuming for large-scale matrices and tensors, we look into a way to overcome this issue. In the recent years, randomized algorithms experienced a breakthrough in the numerical linear algebra [8, 32, 7, 13, 14, 16]. Randomized algorithms are known to be significantly faster than deterministic algorithms and they are reliable in many applications. Here we explore several randomization techniques. The first one is an adaptation of the broadly used randomized SVD [8] to the case of coupled factorizations. Further on, we study randomized subspace iteration [8, 27] and randomized block Krylov iteration [27] for the case of coupled decompositions. We test these approaches on different examples, both for CMF and for CMTF, and compare the results mutually, as well as with the non-randomized algorithms. Apart from the synthetic numerical examples, we also test our algorithms on the face recognition problem. Given a database of faces, our randomized algorithms for coupled decompositions match a new face with a person from the database. Although this is not a traditional approach to face recognition, our algorithms demonstrated a very good success rate. The paper is divided as follows. In Section 2 we study coupled matrix factorization and introduce our basic algorithm (Algorithm 1). This algorithm is randomized in Subsection 2.1, while the numerical examples are given in Section 3. Then, we analyse coupled matrix and tensor factorization in Section 4. The Tucker tensor decomposition is examined in Subsection 4.2 and CP tensor decomposition in Subsection 4.3. The corresponding numerical examples are presented in Section 5. In Section 6 we apply our algorithms to the problem of face recognition. We end the paper with a short conclusion in Section 7."
https://arxiv.org/html/2411.01874v1,High order numerical methods for solving high orders functional differential equations,In this paper we construct high order numerical methods for solving third and fourth orders nonlinear functional differential equations (FDE). They are based on the discretization of iterative methods on continuous level with the use of the trapezoidal quadrature formulas with corrections. Depending on the number of terms in the corrections we obtain methods of O⁢(h4)𝑂superscriptℎ4O(h^{4})italic_O ( italic_h start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) and O⁢(h6)𝑂superscriptℎ6O(h^{6})italic_O ( italic_h start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) accuracy. Some numerical experiments demonstrate the validity of the obtained theoretical results. The approach used here for the third and fourth orders nonlinear functional differential equations can be applied to functional differential equations of any orders.,"Recently, in [6] we established the existence and uniqueness results and constructed a numerical method of second order of accuracy for solving the FDE u′′′=f⁢(t,u⁢(t),u⁢(φ⁢(t))),t∈(0,a)formulae-sequencesuperscript𝑢′′′𝑓𝑡𝑢𝑡𝑢𝜑𝑡𝑡0𝑎u^{\prime\prime\prime}=f(t,u(t),u(\varphi(t))),\quad t\in(0,a)italic_u start_POSTSUPERSCRIPT ′ ′ ′ end_POSTSUPERSCRIPT = italic_f ( italic_t , italic_u ( italic_t ) , italic_u ( italic_φ ( italic_t ) ) ) , italic_t ∈ ( 0 , italic_a ) (1) associated with the general linear-type two-point boundary conditions. The method is based on the use of Green function and trapezoidal formula for computing integrals at each iteration. We proved that the method is of O⁢(h2)𝑂superscriptℎ2O(h^{2})italic_O ( italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) accuracy. In the last year, Bica and Curila [2] constructed successive approximations for equivalent integral equation with the use of cubic spline interpolation at each iterative step for solving the equation (1) in [0,1]01[0,1][ 0 , 1 ] with the boundary conditions u⁢(0)=c1,u′⁢(0)=c2,u′⁢(1)=c3.formulae-sequence𝑢0subscript𝑐1formulae-sequencesuperscript𝑢′0subscript𝑐2superscript𝑢′1subscript𝑐3u(0)=c_{1},u^{\prime}(0)=c_{2},u^{\prime}(1)=c_{3}.italic_u ( 0 ) = italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( 0 ) = italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_u start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ( 1 ) = italic_c start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT . (2) They prove that the maximal order of convergence of the method is O⁢(h3)𝑂superscriptℎ3O(h^{3})italic_O ( italic_h start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). The proof of this result is very complicated. It should be said that the method used in [2] is the same method of iterated splines proposed in [3] for even order FDEs. There are also numerical examples which show that the method is of order O⁢(h2)𝑂superscriptℎ2O(h^{2})italic_O ( italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) for second order FDE and of order O⁢(h4)𝑂superscriptℎ4O(h^{4})italic_O ( italic_h start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) for fourth order FDE. In [3, 4] the authors proved the maximal order of convergence of the method for the fourth order FDE with the clamped boundary conditions is O⁢(h4)𝑂superscriptℎ4O(h^{4})italic_O ( italic_h start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ). Also, in [5] the authors used Catmull-Rom cubic spline interpolation at each iteration of the Green function technique to third order FDE and come to the same conclusion that the maximal order of convergence is O⁢(h3)𝑂superscriptℎ3O(h^{3})italic_O ( italic_h start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ). It should be emphasized that the approach to boundary value problems (BVPs) in [2, 3, 5] is based on the discretization of the iterative method applied to the equivalent nonlinear integral equation u⁢(t)=g⁢(t)+∫0aG⁢(t,s)⁢f⁢(s,u⁢(s),u⁢(φ⁢(s)))⁢𝑑s.𝑢𝑡𝑔𝑡superscriptsubscript0𝑎𝐺𝑡𝑠𝑓𝑠𝑢𝑠𝑢𝜑𝑠differential-d𝑠u(t)=g(t)+\int_{0}^{a}G(t,s)f(s,u(s),u(\varphi(s)))ds.italic_u ( italic_t ) = italic_g ( italic_t ) + ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT italic_G ( italic_t , italic_s ) italic_f ( italic_s , italic_u ( italic_s ) , italic_u ( italic_φ ( italic_s ) ) ) italic_d italic_s . Due to this approach, with the use of the cubic spline interpolation at each iteration the construction of the numerical algorithms and the proof of their convergence are very cumbersome and complicated. Differently from Bica, in [6] we constructed numerical method for solving the third order FDE (1) on the base of discretization of the iterative method on continuous level resulting from the operator equation for nonlinear term (or right hand side) of the equation. We proved that the method obtained by using trapezoidal quadrature formula at each iteration is of O⁢(h2)𝑂superscriptℎ2O(h^{2})italic_O ( italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ). In this paper, using trapezoidal quadrature formula with corrections we construct numerical methods of order O⁢(h4)𝑂superscriptℎ4O(h^{4})italic_O ( italic_h start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) and O⁢(h6)𝑂superscriptℎ6O(h^{6})italic_O ( italic_h start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ) for the equation (1) with the boundary conditions (2). We also construct numerical methods of high orders for a BVP for fourth order FDE. The technique used here is an further development of our method for designing high orders numerical methods for solving high orders ODEs [8, 9]. Some numerical examples confirm the theoretical results."
https://arxiv.org/html/2411.01862v1,"Collocation method for a functional equation arising in behavioral sciences111This is the accepted version of a paper published in Journal of Computational and Applied Mathematics 458 (2025), 116343 with DOI:https://doi.org/10.1016/j.cam.2024.116343","We consider a nonlocal functional equation that is a generalization of the mathematical model used in behavioral sciences. The equation is built upon an operator that introduces a convex combination and a nonlinear mixing of the function arguments. We show that, provided some growth conditions of the coefficients, there exists a unique solution in the natural Lipschitz space. Furthermore, we prove that the regularity of the solution is inherited from the smoothness properties of the coefficients.As a natural numerical method to solve the general case, we consider the collocation scheme of piecewise linear functions. We prove that the method converges with the error bounded by the error of projecting the Lipschitz function onto the piecewise linear polynomial space. Moreover, provided sufficient regularity of the coefficients, the scheme is of the second order measured in the supremum norm.A series of numerical experiments verify the proved claims and show that the implementation is computationally cheap and exceeds the frequently used Picard iteration by orders of magnitude in the calculation time. Keywords: functional equation, nonlocal equation, collocation method AMS Classification: 39B22, 65L60","We investigate a functional equation of the following form u⁢(x)=φ⁢(x)⁢u⁢(φ1⁢(x))+(1−φ⁢(x))⁢u⁢(φ2⁢(x))+f⁢(x),𝑢𝑥𝜑𝑥𝑢subscript𝜑1𝑥1𝜑𝑥𝑢subscript𝜑2𝑥𝑓𝑥u(x)=\varphi(x)u(\varphi_{1}(x))+(1-\varphi(x))u(\varphi_{2}(x))+f(x),italic_u ( italic_x ) = italic_φ ( italic_x ) italic_u ( italic_φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) ) + ( 1 - italic_φ ( italic_x ) ) italic_u ( italic_φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_x ) ) + italic_f ( italic_x ) , (1) where φ𝜑\varphiitalic_φ, φ1,φ2subscript𝜑1subscript𝜑2\varphi_{1},\varphi_{2}italic_φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and f𝑓fitalic_f are known coefficients satisfying certain growth and structure conditions to be given below. Note that, excluding the trivial case where φ1,φ2subscript𝜑1subscript𝜑2\varphi_{1},\varphi_{2}italic_φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are linear functions, the equation can be thought of as nonlocal in which the argument of the sought solution is mixed in a possibly nonlinear way. From another point of view, the above can be seen as a functional equation with two vanishing delays [12] (in contrast with the usual additive, or nonvanishing, delay of the form x−τ𝑥𝜏x-\tauitalic_x - italic_τ for some τ>0𝜏0\tau>0italic_τ > 0). This feature of the problem introduces some difficulties in both analytical and numerical treatment. For example, probably the simplest case where the delay is proportional is the functional pantograph equation [10] u⁢(x)=a⁢u⁢(b⁢x)+f⁢(x),a∈ℝ,0<b<1,formulae-sequence𝑢𝑥𝑎𝑢𝑏𝑥𝑓𝑥formulae-sequence𝑎ℝ0𝑏1u(x)=au(bx)+f(x),\quad a\in\mathbb{R},\quad 0<b<1,italic_u ( italic_x ) = italic_a italic_u ( italic_b italic_x ) + italic_f ( italic_x ) , italic_a ∈ blackboard_R , 0 < italic_b < 1 , (2) which already possesses a very rich structure and requires nontrivial techniques to analyse that attracted many researchers through several last decades [5]. There is also a broad interest in Volterra integral equations with nonvanishing delays [3] and its stochastic generalizations [18]. One of the motivations to investigate (1) is its emergence in behavioral sciences as a model of learning processes of various species [11]. The simplest one refers to the paradise fish [17] and is based on the experiment [6]. In this model, the coefficients have the form φ⁢(x)=x,φ1⁢(x)=1−α+α⁢x,φ2⁢(x)=β⁢x,f⁢(x)≡0,0<α≤β<1,formulae-sequence𝜑𝑥𝑥formulae-sequencesubscript𝜑1𝑥1𝛼𝛼𝑥formulae-sequencesubscript𝜑2𝑥𝛽𝑥formulae-sequence𝑓𝑥00𝛼𝛽1\varphi(x)=x,\quad\varphi_{1}(x)=1-\alpha+\alpha x,\quad\varphi_{2}(x)=\beta x% ,\quad f(x)\equiv 0,\quad 0<\alpha\leq\beta<1,italic_φ ( italic_x ) = italic_x , italic_φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) = 1 - italic_α + italic_α italic_x , italic_φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_x ) = italic_β italic_x , italic_f ( italic_x ) ≡ 0 , 0 < italic_α ≤ italic_β < 1 , (3) and boundary conditions u⁢(0)=0𝑢00u(0)=0italic_u ( 0 ) = 0, u⁢(1)=1𝑢11u(1)=1italic_u ( 1 ) = 1. Other generalizations to different species and learning processes were discussed, for example, in [8]. The equation similar to (1) but in the case of the paradise fish model (3) was initially explored in [13], employing the Schauder fixed point theorem to establish the existence of solutions. A crucial stipulation was introduced, necessitating that the solution be expressible through a specific power series. This matter was investigated in additional studies in [11], where the author demonstrated the existence and uniqueness using the Banach contraction principle and Picard’s iteration. Subsequently, the research into the existence and uniqueness results was expanded in [1]. In our previous paper [7] we investigated a general situation (1) with vanishing source and non-zero boundary conditions. We have proved the existence and uniqueness of the solution under some growth conditions on the coefficients. We have also noticed that, to the best of our knowledge, the only available numerical method for obtaining approximate solutions, Picard’s iteration, is impractical and requires long computation times. To aid in this, we have also proposed some analytical approximate solutions that provide decent accuracy. In this paper, we devise another way to obtain arbitrarily accurate numerical solutions to the general equation (1) using the collocation method. This numerical scheme is widely used to obtain numerical solutions to various differential and integral equations [2]. It has been successfully applied to the pantograph equation in [12] and later to its version with a general nonvanishing delay in [4]. In [15], authors considered a corresponding nonlinear problem and proved its solvability and convergence of the collocation scheme. The main result of this paper is the construction and proof of the convergence of a collocation numerical method to solve (1) (Section 3). Thanks to the auxiliary results on existence, uniqueness, and regularity (Section 2), we can obtain them with assumptions only on the coefficients. The unique solution lies in the Lipschitz space, and in order to find the error of the numerical scheme, it is necessary to obtain some new results concerning polynomial interpolation for functions of that regularity. Numerical calculations (Section 4) verify that the method is robust, fast and able to also treat equations of weaker regularity such as Hölder continuous. In any case, it outperforms the classical Picard iteration with a computational cost of O⁢(n2)𝑂superscript𝑛2O(n^{2})italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) with n→∞→𝑛n\rightarrow\inftyitalic_n → ∞ being the number of degrees of freedom. There are still several open and interesting problems regarding this method, and we outline them in Section 5."
https://arxiv.org/html/2411.01763v1,How Analysis Can Teach Us the Optimal Way to Design Neural Operators,"This paper presents a mathematics-informed approach to neural operator design, building upon the theoretical framework established in our prior work. By integrating rigorous mathematical analysis with practical design strategies, we aim to enhance the stability, convergence, generalization, and computational efficiency of neural operators. We revisit key theoretical insights, including stability in high dimensions, exponential convergence, and universality of neural operators. Based on these insights, we provide detailed design recommendations, each supported by mathematical proofs and citations. Our contributions offer a systematic methodology for developing neural operators with improved performance and reliability.","Neural operators have changed the way we approach problems involving mappings between infinite-dimensional function spaces, particularly in solving partial differential equations (PDEs) [1, 2, 3]. By extending the capabilities of neural networks from finite-dimensional data to function spaces, architectures such as the Fourier Neural Operator (FNO) and Deep Operator Network (DeepONet) have demonstrated significant success in approximating solution operators with significantly reduced computational costs. In our prior work, we developed a mathematical framework for analyzing neural operators, proving their stability, convergence properties, and capacity for universal approximation between function spaces [4]. We also established probabilistic bounds on generalization error, linking it to sample size and network capacity. Building upon this foundation, the primary objective of this paper is to translate these theoretical insights into actionable design recommendations for neural operators. By doing so, we aim to bridge the gap between theory and practice, suggesting better neural operator architectures and saving time in design. The remainder of the paper is organized as follows. In Section 2, we reinstate the theoretical results from the prior paper, including the definitions of neural operators and the key theorems related to the behaviors of neural operators. In Section 3, we present our detailed design recommendations, illustrating how each recommendation enhances neural operator performance. Finally, Appendix A contains the full proofs of all the theorems, lemmas, and propositions presented in this paper."
https://arxiv.org/html/2411.01746v1,"Entropy stable conservative flux form neural networks††thanks:\monthyeardateNovember 4, 2024","We propose an entropy-stable conservative flux form neural network (CFN) that integrates classical numerical conservation laws into a data-driven framework using the entropy-stable, second-order, and non-oscillatory Kurganov-Tadmor (KT) scheme. The proposed entropy-stable CFN uses slope limiting as a denoising mechanism, ensuring accurate predictions in both noisy and sparse observation environments, as well as in both smooth and discontinuous regions. Numerical experiments demonstrate that the entropy-stable CFN achieves both stability and conservation while maintaining accuracy over extended time domains. Furthermore, it successfully predicts shock propagation speeds in long-term simulations, without oracle knowledge of later-time profiles in the training data.","Hyperbolic systems of partial differential equations (PDEs) are fundamental in modeling the dynamics of various natural and engineered systems. These systems are characterized by their ability to describe wave propagation, making them essential for understanding phenomena involving the transport of quantities such as mass, momentum, and energy. They therefore play a pivotal role in the fields of climate science, oceanography, and sea ice dynamics [26]. For example, the shallow water equations are used extensively to model ocean currents and wave dynamics, helping to predict the movement of oceanic water masses and their impact on coastal regions [40]. Euler equations are similarly important in atmospheric dynamics models. Their general utility in large-scale flow patterns and wave phenomena make them fundamental to modeling weather and climate systems [19]. Iceberg and sea ice dynamics affect both oceanic and atmospheric systems, with hyperbolic PDEs playing roles in the modeling of iceberg motion [1] and in the elastic deformation of sea ice [14, 33]. It is therefore consistent to assume that unknown systems of hyperbolic PDEs govern the dynamics that describe intricate relationships between ice, ocean, and atmosphere. This investigation demonstrates that it may be possible to predict future behavior of these unknown systems from relevant data acquired in the past. The rigorous mathematical framework for hyperbolic systems of PDEs has enabled the development of accurate, efficient and robust numerical solvers for these complex processes when the governing PDEs are known. It is imperative for numerical schemes to closely mirror the mathematical structures of the underlying PDEs, as approximate solutions can develop singularities such as shocks, sharp gradients, and discontinuous derivatives, even when starting from smooth initial conditions. Due to their ability to resolve the locally complicated structures inherent in nonlinear hyperbolic PDEs, finite volume methods [26] and discontinuous Galerkin methods [18] are often adopted for long term model simulations [6, 7, 32, 42]. As large datasets pertaining to important state variables (e.g. temperature, pressure, and velocity fields) are more readily acquired, data-driven models are increasingly being used to accurately and efficiently learn the dynamics of unknown ordinary differential equation (ODE) and PDE models, as well as to predict their future behaviors. In this regard machine learning (ML) offers a compatible framework for various dynamical system prediction problems, originally in the context of ODEs [12, 13, 28], with some recent extensions to PDEs [11, 27, 41]. Analogous to the well known method-of-lines approach, most data-driven methods for learning dynamical systems construct neural networks to represent the right hand side of a semi-discrete ODE system, whose inputs are predictions at previous time steps, which is then followed by appropriate numerical time integration techniques [4, 8]. These approaches are considered as purely data-driven since no explicit spatial structural information is included. By contrast, methods that incorporate spatial derivatives such as gradients and Laplacians are designed to embed explicit spatial structural information for dynamical system discovery [29]. Methods that incorporate ideas from classical numerical conservation laws into neural network frameworks have been more recently developed to predict the long term behavior of hyperbolic conservation laws [3, 5, 39]. For example, the conservative flux form neural network (CFN) introduced in [5] learns the dynamics of unknown hyperbolic conservation laws by leveraging a finite volume structure. Similarly, RoeNet in [39] draws inspiration from the classical Roe scheme, which approximates the Riemann solver [34]. Notably, CFN and RoeNet both describe methods that extrapolate the solutions to a future time using training data obtained in some initial time interval. Based on the promising results achieved with CFN and RoeNet, this investigation expands on the theme of incorporating classical numerical conservation laws, specifically to include entropy stability conditions, into the neural network framework. We note that the method in [39] is reported to not “violate entropy”, but neither formal discussion nor computational evidence is provided. By contrast, in this investigation we intentionally consider entropy stability in the design of our neural network in the form of slope-limiting numerical methods. As a prototype we employ the second order accurate non-oscillatory Kurganov-Tadmor (KT) scheme [22], which uses a minmod slope limiter. We observe that much like what is provable for classical numerical methods for conservation laws [17, 25, 26], the combination of entropy stability, higher (here second) order accuracy, and slope-limiting within the neural network design yields better prediction capabilities for data-driven methods than the original CFN in [5], which takes none of these properties into account. In particular, by design the original CFN approach tends to preserve oscillations in the solution, and therefore cannot discern true oscillatory behavior from that induced by noise, in turn potentially leading to incorrect flux terms and non-physical solutions. By contrast the slope-limiting property of the entropy-stable CFN scheme proposed here effectively serves as a denoising procedure since it reduces oscillations in the solution. Moreover, since its construction is inherently tied to the spatial order of accuracy, slope limiting is more robust in providing the expected order of accuracy for the solution (in smooth regions) when compared to the original CFN approach. Our numerical experiments demonstrate that the entropy-stable CFN yields results that are both conservative and entropy-stable. It also correctly predicts the shock propagation speed in extended time domains without introducing non-physical oscillations into the solution. Importantly, these accurate predictions can be generated even when the training period does not contain a discontinuous profile. That is, the method does not require oracle knowledge of a later time solution space. Our new method is more efficient than techniques such as RoeNet [39], since it bypasses the need for computing a matrix pseudo-inverse, which has the added risk of generating instabilities during training and testing. Finally, our method’s network architecture is simpler and more compact, further enhancing efficiency. The rest of this paper is organized as follows. In Section 2 we review fundamental properties of numerical conservation laws and the entropy condition, as well as discuss three entropy-stable numerical methods that can potentially provide the building blocks for our new entropy-stable CFN, namely the KT, the Lax Wendroff and the modified Lax Wendroff schemes. We then construct the entropy-stable CFN in Section 3. In Section 4 we provide details for our experimental design and the metrics used to validate our results. Computational experiments are performed in Section 5, where we also analyze our method for its sensitivity to noise, training data resolution, and choice of parameters. Some concluding remarks and ideas for future investigations are given in Section 6."
https://arxiv.org/html/2411.01740v1,A conditional normalizing flow for domain decomposed uncertainty quantification,"In this paper we present a conditional KRnet (cKRnet) based domain decomposed uncertainty quantification (CKR-DDUQ) approach to propagate uncertainties across different physical domains in models governed by partial differential equations (PDEs) with random inputs. This approach is based on the domain decomposed uncertainty quantification (DDUQ) method presented in [Q. Liao and K. Willcox, SIAM J. Sci. Comput., 37 (2015), pp. A103–A133], which suffers a bottleneck of density estimation for local joint input distributions in practice. In this work, we reformulate the required joint distributions as conditional distributions, and propose a new conditional normalizing flow model, called cKRnet, to efficiently estimate the conditional probability density functions. We present the general framework of CKR-DDUQ, conduct its convergence analysis, validate its accuracy and demonstrate its efficiency with numerical experiments.","During the last two decades there has been a rapid development in numerical methods for uncertainty quantification (UQ). This explosion in interest has been driven by the need to increase the reliability of numerical simulations for complex engineering problems, e.g., partial differential equations (PDEs) with random inputs, inverse problems and optimization under uncertainty. In many UQ related areas, such as multidisciplinary optimization [1], subsurface hydrology [2], porous media flow modelling [3] and microscopic simulations [4], an essential problem is uncertainty propagation across different physical domains. For this purpose, decomposition based UQ methods have been actively developed. These include, for example, hybrid particle and continuum models [5], parallel domain decomposition methods for stochastic Galerkin [6], random domain decomposition with sparse grid collocation [7], coupling dimension reduction for polynomial chaos expansion [8, 9], stochastic model reduction based on low-rank separated representation [10], local polynomial chaos expansion [11, 12], importance sampling based decomposition [13, 14], and propagation methods of uncertainty across heterogeneous domains [15]. In [16], stochastic domain decomposition via moment minimization is proposed, where the moment minimizing interface condition is introduced to match the stochastic local solutions. A domain decomposition model reduction method for stochastic convection-diffusion equations is studied in [17], and a network UQ method for component-based systems is introduced in [18]. In addition, stochastic domain decomposition based on variable-separation is proposed in [19], and domain decomposition for Bayesian inversion is studied in [20]. In this work, we focus on the importance sampling based decomposition for uncertainty analysis [13, 14]. In [13], a distributed uncertainty analysis method for feed-forward multicomponent systems is introduced, which conducts local uncertainty analyses for individual local components and synthesizes them into a global system uncertainty analysis through importance sampling. Based on this idea, a domain-decomposed uncertainty quantification approach (DDUQ) for systems governed by PDEs is proposed in [14]. In DDUQ, non-overlapping domain decomposition is applied to decompose global PDEs into local problems posed on physical subdomains, and uncertainty analyses are performed for local problems with corresponding proposal probability density functions (PDFs). The target probability density function of the inputs for each local problem is estimated through a density estimation technique, where samples are generated through domain decomposition iterations with coupling surrogates. Then, importance sampling is utilized to assemble global uncertainty analysis results. Density estimation is a crucial step in DDUQ. Over the last few decades, various density estimation methods have been developed. The classical density estimation methods can be broadly categorized into parametric and non-parametric approaches. In parametric methods, the given data are assumed to follow specific distributions, such as Gaussian distributions and mixed Gaussian distributions [21, 22], and parameters in these distributions are typically determined through maximum likelihood estimation and expectation maximization [23]. This type of approaches is effective when the specified distributions can fit the underlying data well; otherwise, their performance degrades. For the non-parametric methods, they offer greater flexibility to fit the underlying data than parametric methods, as they are not restricted to a predetermined form of distributions. Techniques such as histogram estimation, kernel density estimation (KDE) [24], and k-nearest neighbor methods [25] are popular in this category. Although achieving great success, challenges still exist for these classic methods when the underlying data are high-dimensional. Recently, new deep learning-based techniques have shown great potential for high-dimensional density estimation, e.g., the autoregressive models [26, 27], variational autoencoders (VAEs) [28], generative adversarial networks (GANs) [29], normalizing flow models [30, 31, 32, 33] and diffusion models [34]. In this work, we are concentrated on KRnet [33], a normalizing flow model that incorporates the Knothe-Rosenblatt (KR) rearrangement [35], and it is applied to solve high-dimensional Fokker-Planck equations in [36]. The KRnet is constructed as an invertible block-triangular mapping between the target distribution and a prior distribution, and the estimated PDF can then be written explicitly by the change of variables. The aim of this work is to systematically improve DDUQ [14] through utilizing deep learning methods for both density estimation and coupling surrogate modelling. In the offline stage of DDUQ, each subdomain is assigned a local proposal input PDF, and Monte Carlo simulations are performed to generate samples of local outputs and coupling functions. Based on the samples, surrogates of the coupling functions, which are referred to as the coupling surrogates, are built using deep neural networks (DNNs) in this work. In the online stage, a large number of samples are generated from the given PDF of global system input parameters, and domain decomposition iterations are conducted with the coupling surrogates to compute target local input samples. While the importance sampling procedure is applied to re-weight the local output samples, the weights are computed based on conditional PDFs of the local input parameters in this work, rather than the joint PDFs in the original DDUQ. Estimating conditional PDFs is inspired by the specific structure of the target local input samples. The local input parameters are divided into system input parameters and interface parameters, where interface parameters are dependent on system input parameters (details are shown in Section 2.2). Furthermore, the PDFs of the system input parameters are given, and focusing on the conditional PDFs can avoid unnecessary complexity to estimate the joint PDFs. However, efficient estimation for conditional distributions is another challenge. To address this issue, we generalize KRnet to a conditional version, which is referred to as conditional KRnet (cKRnet) in the following. To summarize, the main contributions of this work are two-fold: first we propose cKRnet to estimate general conditional PDFs; second, we modify the importance sampling procedure in DDUQ through replacing the joint PDFs of local inputs by conditional PDFs, and efficiently apply cKRnet to estimate the conditional PDFs for the local inputs. This new version of DDUQ is referred to as cKRnet based domain decomposed uncertainty quantification (CKR-DDUQ) approach in the following. The rest of the paper is organized as follows. Section 2 presents the problem setup, reviews the DDUQ approach following [14], where the weights in the importance sampling procedure are reformulated. Then we propose the conditional KRnet (cKRnet) for conditional density estimation in Section 3. Section 4 presents our CKR-DDUQ approach and its convergence analysis. In Section 5, we demonstrate the effectiveness of CKR-DDUQ with numerical experiments, where both diffusion and Stokes problems are studied. Finally Section 6 concludes the paper."
https://arxiv.org/html/2411.01550v1,"A New Error Analysis for Finite Element Methods for Elliptic
Neumann Boundary Control Problems
with Pointwise Control Constraints",We present a new error analysis for finite element methods for a linear-quadratic elliptic optimal control problem with Neumann boundary control and pointwise control constraints. It can be applied to standard finite element methods when the coefficients in the elliptic operator are smooth and also to multiscale finite element methods when the coefficients are rough.,"Let Ω⊂ℝdΩsuperscriptℝ𝑑\Omega\subset\mathbb{R}^{d}roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT (d=2,3𝑑23d=2,3italic_d = 2 , 3) be a Lipschitz polyhedral domain with boundary ΓΓ\Gammaroman_Γ, yd,f∈L2⁢(Ω)subscript𝑦𝑑𝑓subscript𝐿2Ωy_{d},f\in{L_{2}(\Omega)}italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , italic_f ∈ italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_Ω ), ϕ1,ϕ2∈L2⁢(Γ)subscriptitalic-ϕ1subscriptitalic-ϕ2subscript𝐿2Γ\phi_{1},\phi_{2}\in{L_{2}(\Gamma)}italic_ϕ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ϕ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_Γ ), ϕ1≤ϕ2subscriptitalic-ϕ1subscriptitalic-ϕ2\phi_{1}\leq\phi_{2}italic_ϕ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≤ italic_ϕ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT on ΓΓ\Gammaroman_Γ, γ∈(0,1]𝛾01\gamma\in(0,1]italic_γ ∈ ( 0 , 1 ] be a positive constant, and (1.1) a⁢(y,z)=∫ΩA⁢∇y⋅∇z⁢d⁢x+∫Ωκ⁢y⁢z⁢𝑑x,𝑎𝑦𝑧subscriptΩ⋅𝐴∇𝑦∇𝑧𝑑𝑥subscriptΩ𝜅𝑦𝑧differential-d𝑥a(y,z)=\int_{\Omega}A\nabla y\cdot\nabla z\,dx+\int_{\Omega}\kappa yz\,dx,italic_a ( italic_y , italic_z ) = ∫ start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT italic_A ∇ italic_y ⋅ ∇ italic_z italic_d italic_x + ∫ start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT italic_κ italic_y italic_z italic_d italic_x , where the d×d𝑑𝑑d\times ditalic_d × italic_d symmetric coefficient matrix A⁢(x)𝐴𝑥A(x)italic_A ( italic_x ) is positive definite, and κ𝜅\kappaitalic_κ is a nonnegative bounded measurable function such that ‖κ‖L1⁢(Ω)>0subscriptnorm𝜅subscript𝐿1Ω0\|\kappa\|_{L_{1}(\Omega)}>0∥ italic_κ ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( roman_Ω ) end_POSTSUBSCRIPT > 0. More precisely, we assume that the components of A⁢(x)𝐴𝑥A(x)italic_A ( italic_x ) are Lebesgue measurable functions and there exist two positive numbers α≤β𝛼𝛽\alpha\leq\betaitalic_α ≤ italic_β such that (1.2) α⁢|ξ|2≤ξt⁢A⁢(x)⁢ξ≤β⁢|ξ|2∀x∈Ω,ξ∈ℝd.formulae-sequence𝛼superscript𝜉2superscript𝜉𝑡𝐴𝑥𝜉𝛽superscript𝜉2formulae-sequencefor-all𝑥Ω𝜉superscriptℝ𝑑\alpha|\xi|^{2}\leq\xi^{t}A(x)\xi\leq\beta|\xi|^{2}\qquad\forall\,x\in\Omega,% \;\xi\in\mathbb{R}^{d}.italic_α | italic_ξ | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_ξ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_A ( italic_x ) italic_ξ ≤ italic_β | italic_ξ | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∀ italic_x ∈ roman_Ω , italic_ξ ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . The model linear-quadratic Neumann boundary control problem (cf. [21, 29]) is to find (1.3) (y¯,u¯)=argmin(y,u)∈𝕂12⁢[‖y−yd‖L2⁢(Ω)2+γ⁢‖u‖L2⁢(Γ)2],¯𝑦¯𝑢subscriptargmin𝑦𝑢𝕂12delimited-[]superscriptsubscriptnorm𝑦subscript𝑦𝑑subscript𝐿2Ω2𝛾superscriptsubscriptnorm𝑢subscript𝐿2Γ2(\bar{y},\bar{u})=\mathop{\rm argmin}_{(y,u)\in\mathbb{K}}\frac{1}{2}\big{[}\|% y-y_{d}\|_{L_{2}(\Omega)}^{2}+\gamma\|u\|_{L_{2}(\Gamma)}^{2}\big{]},( over¯ start_ARG italic_y end_ARG , over¯ start_ARG italic_u end_ARG ) = roman_argmin start_POSTSUBSCRIPT ( italic_y , italic_u ) ∈ blackboard_K end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG [ ∥ italic_y - italic_y start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_Ω ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_γ ∥ italic_u ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_Γ ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , where (y,u)𝑦𝑢(y,u)( italic_y , italic_u ) belongs to 𝕂⊂H1⁢(Ω)×L2⁢(Γ)𝕂superscript𝐻1Ωsubscript𝐿2Γ\mathbb{K}\subset{H^{1}(\Omega)}\times{L_{2}(\Gamma)}blackboard_K ⊂ italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ) × italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_Γ ) if and only if (1.4) a⁢(y,z)=∫Ωf⁢z⁢𝑑x+∫Γu⁢z⁢𝑑s∀z∈H1⁢(Ω),formulae-sequence𝑎𝑦𝑧subscriptΩ𝑓𝑧differential-d𝑥subscriptΓ𝑢𝑧differential-d𝑠for-all𝑧superscript𝐻1Ωa(y,z)=\int_{\Omega}fz\,dx+\int_{\Gamma}uz\,ds\qquad\forall\,z\in{H^{1}(\Omega% )},\\ italic_a ( italic_y , italic_z ) = ∫ start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT italic_f italic_z italic_d italic_x + ∫ start_POSTSUBSCRIPT roman_Γ end_POSTSUBSCRIPT italic_u italic_z italic_d italic_s ∀ italic_z ∈ italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ) , and (1.5) ϕ1≤u≤ϕ2onΓ.formulae-sequencesubscriptitalic-ϕ1𝑢subscriptitalic-ϕ2onΓ\phi_{1}\leq u\leq\phi_{2}\quad\text{on}\quad\Gamma.italic_ϕ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ≤ italic_u ≤ italic_ϕ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT on roman_Γ . Here d⁢s𝑑𝑠dsitalic_d italic_s is the infinitesimal arc length (d=2𝑑2d=2italic_d = 2) or the infinitesimal surface area (d=3𝑑3d=3italic_d = 3). Remark 1.1. We follow the standard notation for differential operators, function spaces and norms that can be found for example in [10, 1, 5]. Remark 1.2. It is clear that the bilinear form a⁢(⋅,⋅)𝑎⋅⋅a(\cdot,\cdot)italic_a ( ⋅ , ⋅ ) is bounded on H1⁢(Ω)superscript𝐻1Ω{H^{1}(\Omega)}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ). From a generalized Poincaré-Friedrichs inequality (cf. [27]) we also have ‖v‖H1⁢(Ω)2≤CPF⁢(|v|H1⁢(Ω)2+‖κ⁢v‖L2⁢(Ω)2),superscriptsubscriptnorm𝑣superscript𝐻1Ω2subscript𝐶PFsuperscriptsubscript𝑣superscript𝐻1Ω2superscriptsubscriptnorm𝜅𝑣subscript𝐿2Ω2\|v\|_{H^{1}(\Omega)}^{2}\leq C_{\rm PF}\big{(}|v|_{H^{1}(\Omega)}^{2}+\|\sqrt% {\kappa}v\|_{L_{2}(\Omega)}^{2}\big{)},∥ italic_v ∥ start_POSTSUBSCRIPT italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_C start_POSTSUBSCRIPT roman_PF end_POSTSUBSCRIPT ( | italic_v | start_POSTSUBSCRIPT italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ∥ square-root start_ARG italic_κ end_ARG italic_v ∥ start_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( roman_Ω ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , which together with (1.2) implies the coercivity of a⁢(⋅,⋅)𝑎⋅⋅a(\cdot,\cdot)italic_a ( ⋅ , ⋅ ): (1.6) ‖v‖H1⁢(Ω)2≤CPF⁢max⁡(α−1,1)⁢a⁢(v,v)∀v∈H1⁢(Ω).formulae-sequencesuperscriptsubscriptnorm𝑣superscript𝐻1Ω2subscript𝐶PFsuperscript𝛼11𝑎𝑣𝑣for-all𝑣superscript𝐻1Ω\|v\|_{H^{1}(\Omega)}^{2}\leq C_{\rm PF}\max(\alpha^{-1},1)a(v,v)\qquad\forall% \,v\in{H^{1}(\Omega)}.∥ italic_v ∥ start_POSTSUBSCRIPT italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_C start_POSTSUBSCRIPT roman_PF end_POSTSUBSCRIPT roman_max ( italic_α start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , 1 ) italic_a ( italic_v , italic_v ) ∀ italic_v ∈ italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ) . Therefore (1.4) is uniquely solvable. Remark 1.3. We can interpret (1.4) as a steady state diffusion-reaction model for a chemical substance in a heterogeneous media with the source f𝑓fitalic_f and the boundary control (in a weak sense) (A⁢∇y)⋅𝒏=uonΓ,⋅𝐴∇𝑦𝒏𝑢onΓ(A\nabla y)\cdot\bm{n}=u\qquad\text{on}\quad\Gamma,( italic_A ∇ italic_y ) ⋅ bold_italic_n = italic_u on roman_Γ , where 𝒏𝒏\bm{n}bold_italic_n is the unit outer normal on ΓΓ\Gammaroman_Γ. There is a substantial literature (cf. the monographs [28, 22, 20] and the references therein) on the analysis of finite element methods for elliptic optimal control problems beginning with the pioneering work of Falk in [14]. Nevertheless the analyses in the literature are not directly applicable to multiscale finite element methods for problems with rough coefficients, i.e., when (1.2) is the only assumption on the coefficient matrix A⁢(x)𝐴𝑥A(x)italic_A ( italic_x ). Recently a new error analysis for distributed elliptic optimal control problems with pointwise control constraints was developed in [4, 6] that is applicable to standard finite element methods (when A𝐴Aitalic_A is smooth) and also to multiscale finite element methods (when A𝐴Aitalic_A is rough). Our goal in this paper is to develop a similar error analysis for the model Neumann boundary control problem defined by (1.3)–(1.5). The rest of the paper is organized as follows. We recall the properties of the continuous problem in Section 2 and introduce the discrete problem in Section 3. An abstract error analysis is given in Section 4, followed by application to problems with smooth coefficients in Section 5 and application to problems with rough coefficients in Section 6. We end with some concluding remarks in Section 7. Some estimates for the continuous problem are given in Appendix A. Throughout the paper we use C𝐶Citalic_C (with or without subscript) to denote a generic positive constant that can take different values at different occurrences."
https://arxiv.org/html/2411.01397v1,Automatic optimal-rate convergence of randomized nets using median-of-means,"We study the sample median of independently generated quasi-Monte Carlo estimators based on randomized digital nets and prove it approximates the target integral value at almost the optimal convergence rate for various function spaces. Contrast to previous methods, the algorithm does not require a prior knowledge of underlying function spaces or even an input of pre-designed (t,m,s)𝑡𝑚𝑠(t,m,s)( italic_t , italic_m , italic_s )-digital nets, and is therefore easier to implement. This study provides further evidence that quasi-Monte Carlo estimators are heavy-tailed when applied to smooth integrands and taking the median can significantly improve the error by filtering out the outliers.","Quasi-Monte Carlo (QMC) method [5] has been used extensively as an alternative to the usual Monte Carlo (MC) algorithm. Just like MC, QMC uses the average of n𝑛nitalic_n function evaluations as an estimate for the target integral value. The places of sampling are, however, carefully designed to explore the underlying sampling space efficiently. As a result, QMC is less sensitive to the curse of dimensionality compared to classical quadrature rules while enjoying a faster error convergence rate compared to MC. Traditionally, the error bound is given by the Koksma-Hlawka inequality [10], which proves a O⁢(n−1+ϵ)𝑂superscript𝑛1italic-ϵO(n^{-1+\epsilon})italic_O ( italic_n start_POSTSUPERSCRIPT - 1 + italic_ϵ end_POSTSUPERSCRIPT ) convergence rate for arbitrarily small ϵ>0italic-ϵ0\epsilon>0italic_ϵ > 0 when the integrand f𝑓fitalic_f has a finite total variation in the sense of Hardy and Krause. Later [15] introduces the method of scrambling, which randomizes the sampling points without breaking their design. Randomization boosts the convergence rate to O⁢(n−3/2+ϵ)𝑂superscript𝑛32italic-ϵO(n^{-3/2+\epsilon})italic_O ( italic_n start_POSTSUPERSCRIPT - 3 / 2 + italic_ϵ end_POSTSUPERSCRIPT ) as measured by the rooted mean squared error (RMSE) when f𝑓fitalic_f has continuous dominating mixed derivatives of order 1111. An issue with the usual QMC method is the convergence rate cannot be further improved even if f𝑓fitalic_f satisfies a stronger smoothness assumption [16]. To solve this issue, [4] constructs the higher order scrambled digital nets that attain a faster convergence rate for smooth enough f𝑓fitalic_f. A drawback with this design is the smoothness parameter is needed as an input for the construction, and still the algorithm fails to converge at the optimal rate once the smoothness of f𝑓fitalic_f exceeds the parameter we set. This paper offers a solution using the median trick. The median of QMC estimators is first analyzed in [16] and proven to converge at O⁢(n−c⁢log2⁡(n))𝑂superscript𝑛𝑐subscript2𝑛O(n^{-c\log_{2}(n)})italic_O ( italic_n start_POSTSUPERSCRIPT - italic_c roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_n ) end_POSTSUPERSCRIPT ) rate for any c<3⁢log⁡(2)/π2≈0.21𝑐32superscript𝜋20.21c<3\log(2)/\pi^{2}\approx 0.21italic_c < 3 roman_log ( 2 ) / italic_π start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≈ 0.21 when f𝑓fitalic_f is an one-dimensional analytic function over [0,1]01[0,1][ 0 , 1 ]. Later [18] generalizes this result to higher-dimensional f𝑓fitalic_f. [16] also proves a O⁢(n−α+ϵ)𝑂superscript𝑛𝛼italic-ϵO(n^{-\alpha+\epsilon})italic_O ( italic_n start_POSTSUPERSCRIPT - italic_α + italic_ϵ end_POSTSUPERSCRIPT ) convergence rate when f𝑓fitalic_f is α𝛼\alphaitalic_α-times continuously differentiable over [0,1]01[0,1][ 0 , 1 ]. While the rate is not optimal, it demonstrates a feature of automatic convergence rate speedup without the knowledge of the smoothness of f𝑓fitalic_f. In this paper, we will improve this result by both generalizing it to higher-dimensional f𝑓fitalic_f and proving a O⁢(n−α−1/2+ϵ)𝑂superscript𝑛𝛼12italic-ϵO(n^{-\alpha-1/2+\epsilon})italic_O ( italic_n start_POSTSUPERSCRIPT - italic_α - 1 / 2 + italic_ϵ end_POSTSUPERSCRIPT ) almost optimal rate. Our analysis will be focused on base-2 scrambled digital nets, although the idea can be generalized to other QMC methods. As an additional merit of the median trick, we do not need to input a pre-designed digital net to implement the algorithm. This paper is organized as following. Section 2 introduces necessary background knowledge and notation for later sections. Subsection 2.4 in particular explains how the median trick boosts the convergence rate. Section 3 is devoted to the study of convergence rate of the median estimator constructed by randomized digital nets under various smoothness assumptions. Subsection 3.1 studies integrands with bounded variation of order α𝛼\alphaitalic_α used by [4]. We are going to show the median estimator attains almost the same convergence rate as the higher order scrambled digital nets without knowing the smoothness parameter α𝛼\alphaitalic_α. The proof is rather short thanks to a key lemma proven in [4], and it illustrates the standard analysis scheme of the median estimator. Subsection 3.2 studies integrands whose dominating mixed derivatives of order α𝛼\alphaitalic_α are continuous with bounded fractional variation used by [3]. We will prove an almost 1/2121/21 / 2 convergence rate improvement over the optimal deterministic error rate achieved by the higher order scrambled digital nets. Subsection 3.3 studies integrands whose dominating mixed derivatives belong to the fractional Sobolev space used by [2] and [7]. Again we will show an almost 1/2121/21 / 2 convergence rate improvement over the optimal deterministic rate. The proof resembles that of Subsection 3.2, so we will omit the repeated part and highlight where a different analysis is required. Section 4 showcases many numerical experiments that test how well median estimators perform compared to higher order digital nets. Section 5 concludes the paper with a discussion of the limitations of our analysis as well as interesting research directions."
https://arxiv.org/html/2411.01085v1,Structure preserving discretization:A Berezin-Toeplitz Quantization viewpoint,"In this paper, we introduce a comprehensive axiomatization of structure-preserving discretization through the framework of commutative diagrams. By establishing a formal language that captures the essential properties of discretization processes, we provide a rigorous foundation for analyzing how various structures—such as algebraic, geometric, and topological features—are maintained during the transition from continuous to discrete settings. Specifically, we establish that the transition from continuous to discrete differential settings invariably leads to noncommutative structures, reinforcing previous observation on the interplay between discretization and noncommutativity. We demonstrate the applicability of our axiomatization by applying it to the Berezin-Toeplitz quantization, showing that this quantization method adheres to our proposed criteria for structure-preserving discretization. We establish in this setting a precise limit theorem for the approximation of the Laplacian by a sequence of matrix approximations. This work not only enriches the theoretical understanding of the nature of discretization but also sets the stage for further exploration of its applications across various discretization methods.","The discretization of continuous structures plays a pivotal role in both theoretical and applied mathematics, particularly in fields such as numerical analysis, computational physics, and data science. Traditional approaches to discretization often rely on heuristic or ad hoc methods, which can lead to inconsistencies and inefficiencies. Recent advances suggest that a more rigorous approach of discretization lies in preserving the essential features of the continuous structures while transitioning to a discrete framework. A key aspect of effective discretization is ensuring that the discrete model retains the underlying geometric and algebraic properties of the continuous one. Structure-preserving discretization methods have gained significant attention due to their ability to retain key properties of continuous systems, such as symmetries, conservation laws, and geometric structures, when transitioning to discrete models. State-of-the-art approaches include finite element exterior calculus (FEEC) [5, 1], which ensures stability and convergence in numerical approximations of partial differential equations by preserving the de Rham complex. Geometric integration methods, such as symplectic integrators, maintain the symplectic structure in Hamiltonian systems, ensuring long-term accuracy in energy conservation [4, 10]. Discrete exterior calculus (DEC) extends these ideas to computational geometry by discretizing differential forms while preserving topological properties like divergence and curl [12, 9]. Recent advances have also focused on the relation between noncommutative geometries and discrete differential structures [13, 14]. This in particular exemplified by the Berezin-Toeplitz quantization, which discretizes Poisson algebras while maintaining algebraic structures in the transition from continuous to discrete systems [15], providing a robust framework for discrete geometry applications. Although there are similarities among the different approaches of structure-preserving discretization, it is yet not clear how one can describe under the same unified theory, eclectic methods ranging from finite exterior calculus to quantization; we identify this as a gap in the theory. In this paper, we present an axiomatization of structure-preserving discretization by leveraging the language of commutative diagrams. Our primary objective is to formalize the process of discretization such that it maintains the integrity of underlying geometric and algebraic structures. We argue that by employing commutative diagrams, one can systematically capture and enforce the preservation of essential relationships between continuous and discrete models. A subsequent objective is to demonstrate that the structure-preserving discretization of differential structures, following the previous axioms, lead to noncommutative differential structure on operator algebras. Commutative diagrams provide a visual and conceptual tool from category theory that allow us to represent the interrelations between various mathematical structures and their discretizations. By constructing and analyzing these diagrams, we can ensure that the discretization process respects the morphisms and relationships inherent in the continuous setting. This approach not only facilitates a clearer understanding of the discretization process but also helps in developing algorithms that are both theoretically sound and practically effective. We demonstrate, by applying our axiomatization, that the discretization of a continuous differential structure is inherently given by a noncommutative geometry. This means that the resulting discrete structure is represented by operator algebras and the differentiation is realized by a commutator with a selfadjoint operator D𝐷Ditalic_D. The results presented is this work corroborate previous observations and constructions [20, 21]. The dictionary between continuous and discrete is identical to the dictionary between classical geometry and noncommutative one. This dictionary is summarized in the following table. Structure Continuous Discrete Space Smooth manifold M𝑀Mitalic_M simplex or graph Algebra Algebra of functions C∞⁢(M)superscript𝐶𝑀C^{\infty}(M)italic_C start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT ( italic_M ) Operator algebra 𝔅𝔅\mathfrak{B}fraktur_B Differential Differential form d⁢f𝑑𝑓dfitalic_d italic_f Commutator [D,F]𝐷𝐹[D,F][ italic_D , italic_F ] Integration ∫f⁢(x)⁢𝑑x𝑓𝑥differential-d𝑥\int f(x)dx∫ italic_f ( italic_x ) italic_d italic_x Trω⁢(F)subscriptTr𝜔𝐹\mathrm{Tr}_{\omega}(F)roman_Tr start_POSTSUBSCRIPT italic_ω end_POSTSUBSCRIPT ( italic_F ) Table 1. Noncommutative geometry: from continuous to discrete The Berezin-Toeplitz quantization is an archetypal example of deformation of a classical geometry given by a compact Poisson manifold (M,{⋅,⋅})𝑀⋅⋅(M,\{\cdot,\cdot\})( italic_M , { ⋅ , ⋅ } ) to noncommutative finite dimensional operator algebra (Mn⁢(ℂ),[⋅,⋅])subscript𝑀𝑛ℂ⋅⋅(M_{n}(\mathbb{C}),\left[\cdot,\cdot\right])( italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( blackboard_C ) , [ ⋅ , ⋅ ] ). Thus, it serves as a fundamental example of transition from a continuous to a discrete space. We use it in this work as a central application of our axiomatization in order to show that it satisfies the conditions of a structure preserving discretization. The following paper is divided as follows: in Section 2 we will define the key concepts and axioms that underpin our framework, in Section 3 we illustrate how commutative diagrams can be used to formalize the preservation of structure, and demonstrate how noncommutative geometry arise from a structure preserving discretization of a differential structure. In Section 5 we apply these results to the Berezin-Toeplitz quantization. Through this exploration, we hope to contribute to a deeper understanding of discretization methods and to provide a foundation for future research in this area."
https://arxiv.org/html/2411.01931v1,Differentially private and decentralized randomized power method,"The randomized power method has gained significant interest due to its simplicity and efficient handling of large-scale spectral analysis and recommendation tasks. As modern datasets contain sensitive private information, we need to give formal guarantees on the possible privacy leaks caused by this method. This paper focuses on enhancing privacy-preserving variants of the method. We propose a strategy to reduce the variance of the noise introduced to achieve Differential Privacy (DP). We also adapt the method to a decentralized framework with a low computational and communication overhead, while preserving the accuracy. We leverage Secure Aggregation (a form of Multi-Party Computation) to allow the algorithm to perform computations using data distributed among multiple users or devices, without revealing individual data. We show that it is possible to use a noise scale in the decentralized setting that is similar to the one in the centralized setting. We improve upon existing convergence bounds for both the centralized and decentralized versions. The proposed method is especially relevant for decentralized applications such as distributed recommender systems, where privacy concerns are paramount.","The randomized power method has attracted significant attention due to its simplicity and its efficiency in solving large-scale linear algebra problems in spectral analysis or recommendation. It is in particular useful for computing the principal singular vectors of a matrix. It has a straightforward implementation which runs with low computational overhead (nearly linear complexity), making it particularly suitable for analyzing large modern datasets. In large-scale machine learning systems, protecting user privacy is paramount. Datasets often contain sensitive personal or organizational information, so privacy has become a critical requirement. The standard randomized power method does not inherently provide privacy guarantees. While its output might seem less sensitive than the input data, there is no formal guarantee that private information embedded in the data cannot be inferred. To address and quantify privacy leakages, Differential Privacy (DP) has emerged as a powerful mathematical framework that provides formal guarantees and helps to mitigate potential privacy leaks of an algorithm. DP ensures that the output of an algorithm reveals little about any individual record in the input, even in the worst case. Several works have attempted to apply DP to the randomized power method. For instance, (Hardt & Price, 2014) and (Balcan et al., 2016) introduced differentially private versions of the method, and (Guo et al., 2021; Wang & Xu, 2020) explores federated approaches to private power iteration. Other close works explore centralized differentially private versions of PCA (Liu et al., 2022) and federated (Wang & Xu, 2020; Briguglio et al., 2023) and give optimal convergence bounds given certain assumptions. Despite these advancements, existing approaches suffer from several limitations. First, their performance heavily depends on the number of singular vectors being computed (Hardt & Price, 2014; Balcan et al., 2016; Guo et al., 2021; Liu et al., 2022; Wang & Xu, 2020), which impacts both utility and privacy guarantees. Second, they are primarily designed for centralized settings (Hardt & Price, 2014; Liu et al., 2022), where a trusted curator is assumed to hold the data. Moreover, some methods (Liu et al., 2022) make strong assumptions about the underlying data distribution (i.e. that the data is sub-Gaussian) which makes it harder to use these methods in practice. Some federated versions (Briguglio et al., 2023) claim to guarantee privacy by use of the federated setting, although it has been shown that decentralization does not offer privacy by design (Geiping et al., 2020). Finally, no fully decentralized versions exist to our knowledge. The previously proposed federated versions (Balcan et al., 2016; Guo et al., 2021; Wang & Xu, 2020) use public channel communication and ignore advances in Multi-Party-Computation (MPC). This makes them unsuitable for decentralized environments, such as distributed recommender systems and social networks, where data is partitioned across users and/or devices and communications are restricted to a predefined communication graph. 1.1 Contributions The core focus of this article is to introduce improved privacy-preserving randomized power methods and to extend them to a decentralized setting, while maintaining computational efficiency and performance. We propose new convergence bounds for the (decentralized) privacy-preserving power method; these show that our proposed algorithm has a weaker dependence on the number of singular vectors that are computed. We propose an improved differentially private version of the randomized power method, with tighter convergence bounds, reducing the noise required to achieve privacy and therefore improving the accuracy of the method. In the process of deriving these bounds, we present a new, more straightforward proof of a privacy result for the differentially private randomized power method.111This proof addresses some minor mistakes in a privacy proof from (Hardt & Roth, 2012, 2013) that have been reproduced in several follow-up works (Hardt & Price, 2014; Balcan et al., 2016). The modified result allows us to use a wider range of privacy parameters. Finally, we propose decentralized versions of the algorithms. These are particularly relevant in contexts where data is distributed across multiple users or devices, as is often the case in recommender systems. To address the privacy concerns of the users, without forcing them to rely on a trusted curator, we propose a federated privacy-preserving version of the method using Secure Aggregation (Bell et al., 2020), a form of Multi-Party Computation (MPC) and a fully decentralized version using GOPA (GOssip noise for Private Averaging) (Sabater et al., 2022). This allows users to keep their data local while contributing to a privacy-preserving global computation. Our decentralized approach preserves the effectiveness of the centralized version of the randomized power method while incorporating the privacy advantages of local differential privacy (DP). Notably, by employing Secure Aggregation, we are able to set the noise scale comparable to that of central DP, enhancing accuracy. This makes the proposed method a promising candidate for decentralized and privacy-conscious applications. Our contributions can be summarized as follows: 1. We introduce a novel privacy-preserving randomized power method that improves upon existing convergence bounds, with a reduced sensitivity to the number of singular vectors. 2. We extend the method to a decentralized setting, utilizing Secure Aggregation to ensure privacy in a distributed environment without the need for a trusted curator. 3. We provide new privacy proofs that correct errors in previous works, ensuring rigorous privacy guarantees. 4. We propose a new adjacency model to guarantee the differential privacy of our method, to open the door to new and more realistic use cases. 5. We present both runtime-dependent and runtime-independent bounds for the proposed methods, demonstrating their utility through theoretical analysis and empirical validation."
https://arxiv.org/html/2411.01924v1,Fairness-Utilization Trade-off in Wireless Networks with Explainable Kolmogorov-Arnold Networks,"The effective distribution of user transmit powers is essential for the significant advancements that the emergence of 6G wireless networks brings. In recent studies, Deep Neural Networks (DNNs) have been employed to address this challenge. However, these methods frequently encounter issues regarding fairness and computational inefficiency when making decisions, rendering them unsuitable for future dynamic services that depend heavily on the participation of each individual user. To address this gap, this paper focuses on the challenge of transmit power allocation in wireless networks, aiming to optimize α𝛼\alphaitalic_α-fairness to balance network utilization and user equity. We introduce a novel approach utilizing Kolmogorov-Arnold Networks (KANs), a class of machine learning models that offer low inference costs compared to traditional DNNs through superior explainability. The study provides a comprehensive problem formulation, establishing the NP-hardness of the power allocation problem. Then, two algorithms are proposed for dataset generation and decentralized KAN training, offering a flexible framework for achieving various fairness objectives in dynamic 6G environments. Extensive numerical simulations demonstrate the effectiveness of our approach in terms of fairness and inference cost. The results underscore the potential of KANs to overcome the limitations of existing DNN-based methods, particularly in scenarios that demand rapid adaptation and fairness.","The advent of 6G wireless networks heralds a new era of connectivity, promising to revolutionize sectors such as healthcare, education, logistics, and transportation [1]. These next-generation networks are poised to deliver unprecedented capabilities, including ultra-high data rates, massive device connectivity, and adaptive responses to highly dynamic environments [2]. Central to realizing these advancements is the efficient allocation of user transmit powers, a critical factor that directly influences network performance, user experience, and energy efficiency. In recent years, the complexity of this challenge has led researchers to explore innovative solutions leveraging Machine Learning (ML) techniques, with a particular focus on Deep Neural Networks (DNNs). Among the studies addressing the transmit power allocation problem in this rapidly evolving field, several notable approaches stand out. Nasir et al. [3] and Sheu et al. [4] employed Deep Q-Learning (DQL) to maximize the sum data rate of users, utilizing channel information as input. Li et al. [5] extended the same approach to a distributed setting. Jamous et al. [6] applied DQL to optimize transmission energy efficiency. Zhang et al. [7] innovated by using convolutional DNNs with users’ geographical information to maximize aggregate data rates. In a different approach, Zhang et al. [8] implemented Proximal Policy Optimization (PPO) with signal strength inputs to ensure predefined Signal-to-Interference-plus-Noise Ratio (SINR) thresholds. Huang et al. [9] also utilized PPO, focusing on maximizing the sum of data rates. While existing DNN-based methods have demonstrated considerable performance, they face significant challenges in two key areas: balancing network utilization with fairness, and achieving computational efficiency during inference. Most of the existing studies have primarily focused on system-wide performance indicators, such as aggregate data rates, often at the expense of equitable resource allocation among individual users. This oversight becomes particularly critical in the context of future services, where semantic-aware communication is expected, and ensuring fair participation for each user is essential to maintain the quality and diversity of outcomes, thereby mitigating potential biases from specific sources [10]. Furthermore, the DNN-based techniques prevalent in the literature are predominantly black-box models, necessitating complex computations for each inference. This computational intensity often results in prolonged inference times [11]. Such inefficiency is particularly problematic in the dynamic environments anticipated for 6G services, where rapid adaptation to changing environmental conditions is paramount. To address the gaps in existing research, this paper focuses on investigating the power allocation problem with the objective of optimizing α𝛼\alphaitalic_α-fairness. The α𝛼\alphaitalic_α-fairness metric offers a versatile framework for balancing the trade-off between fairness and utilization in resource allocation. By modulating α𝛼\alphaitalic_α, we can achieve various fairness objectives, providing a flexible approach suitable for dynamic future services. To tackle this problem, we employ a novel class of machine learning models known as Kolmogorov-Arnold Networks (KANs), which have been proposed as an alternative to conventional DNNs [12]. KANs are designed to approximate continuous multivariate functions using learnable activation functions within a relatively simple architecture, offering improved generalization capabilities. The reliance on these functions renders KANs fully explainable, significantly reducing the computational overhead typically associated with inference. This characteristic makes KANs particularly well-suited for time-sensitive and resource-constrained environments, offering an attractive solution for next-generation communication systems. The remainder of this paper is structured as follows. Section II presents the system model, provides a comprehensive problem formulation, and proves the NP-hardness of the considered problem. Section III elucidates the proposed KAN-based solution, encompassing its fundamental principles, as well as the proposed dataset generation and decentralized training algorithms. In Section IV, we present and analyze numerical results, with a particular focus on evaluating the efficiency of the proposed solution in terms of fairness and inference cost. Finally, Section V concludes the paper with a summary of our findings and closing remarks on the implications and potential future directions of this research."
https://arxiv.org/html/2411.01849v1,A tamed-adaptive Milstein scheme for stochastic differential equations with low regularity coefficients,"We propose a tamed-adaptive Milstein scheme for stochastic differential equations in which the first-order derivatives of the coefficients are locally Hölder continuous of order α𝛼\alphaitalic_α. We show that the scheme converges in the L2subscript𝐿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-norm with a rate of (1+α)/21𝛼2(1+\alpha)/2( 1 + italic_α ) / 2 over both finite intervals [0,T]0𝑇[0,T][ 0 , italic_T ] and the infinite interval (0,+∞)0(0,+\infty)( 0 , + ∞ ), under certain growth conditions on the coefficients.","This paper considers the numerical approximation for a one-dimensional process X=(Xt)t≥0𝑋subscriptsubscript𝑋𝑡𝑡0X=(X_{t})_{t\geq 0}italic_X = ( italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT which is a solution to the following stochastic differential equation (SDE) Xt=x0+∫0tμ⁢(Xs)⁢𝑑s+∫0tσ⁢(Xs)⁢𝑑Ws,x0∈ℝ,t≥0,formulae-sequencesubscript𝑋𝑡subscript𝑥0superscriptsubscript0𝑡𝜇subscript𝑋𝑠differential-d𝑠superscriptsubscript0𝑡𝜎subscript𝑋𝑠differential-dsubscript𝑊𝑠formulae-sequencesubscript𝑥0ℝ𝑡0X_{t}=x_{0}+\int_{0}^{t}\mu(X_{s})ds+\int_{0}^{t}\sigma(X_{s})dW_{s},\quad x_{% 0}\in\mathbb{R},\quad t\geq 0,italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_μ ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) italic_d italic_s + ∫ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_σ ( italic_X start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) italic_d italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R , italic_t ≥ 0 , (1) where W=(Wt)t≥0𝑊subscriptsubscript𝑊𝑡𝑡0W=(W_{t})_{t\geq 0}italic_W = ( italic_W start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ≥ 0 end_POSTSUBSCRIPT is a one-dimensional standard Brownian motion defined on a complete filtered probability space (Ω,ℱ,(ℱt),ℙ)Ωℱsubscriptℱ𝑡ℙ(\Omega,\mathcal{F},(\mathcal{F}_{t}),\mathbb{P})( roman_Ω , caligraphic_F , ( caligraphic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , blackboard_P ). Stochastic differential equations are widely employed in modeling systems across various fields, including biology, physics, and economics [1, 20, 24]. Due to the limited availability of explicit solutions, numerical approximation methods have become indispensable for many applications. The convergence rates of these numerical schemes typically depend on the growth conditions and smoothness of the coefficients. It is well-known that for SDEs whose coefficients μ𝜇\muitalic_μ and σ𝜎\sigmaitalic_σ satisfy global Lipschitz continuity and linear growth conditions, the Euler-Maruyama scheme has the order of strong convergence 1/2121/21 / 2. Moreover, if the coefficients of the SDE (1) satisfy the additional condition that μ𝜇\muitalic_μ is once differentiable and σ𝜎\sigmaitalic_σ is twice differentiable, then the Milstein scheme has the order of strong convergence 1111 (see [7, 14, 22]). However, recent studies [8] revealed that for SDEs with superlinear growth coefficients, classical explicit schemes like the Euler-Maruyama and Milstein methods may fail to converge in the Lpsubscript𝐿𝑝L_{p}italic_L start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT-norm. After that, numerous approximation methods have been proposed to address SDEs with super-linear growth coefficients. The tamed Euler-Maruyama scheme was introduced in [9] and further developed in [28, 29, 23]. The truncated Euler scheme was introduced in [21]. These schemes have the order of strong convergence 1212\frac{1}{2}divide start_ARG 1 end_ARG start_ARG 2 end_ARG when applied for some class of SDEs whose coefficients are super-linear growth and satisfy some one-sided Lipschitz and monotonicity conditions. Several implicit Milstein schemes have been developed for nonlinear SDEs, which strongly converge to the exact solution with an order of 1111 (see [6, 25, 32]). However, it is important to note that implicit approximation methods typically demand increased computational efforts and costs, as they require solving nonlinear algebraic equations at each time step. The tamed Milstein scheme, proposed in [30], is an explicit scheme that has the order of strong convergence 1111 for a class of SDEs with superlinear growth coefficients. Besides, the truncated Milstein scheme was first investigated in [21] for SDEs with commutative noise. It was shown in [4] that the truncated Milstein scheme has the order of strong convergence 1111 if both the drift and diffusion coefficients have continuous second-order derivatives and are polynomially bounded. Further developments of the tamed and truncated Milstein scheme can be found in [2, 4, 15, 16, 17, 18, 19, 27, 10], and references therein. An alternative to fixed-step schemes is the adaptive scheme, where every time step is determined by the current values of the underlying approximated process. Fang and Giles [3] introduced an adaptive Euler-Maruyama scheme designed for SDEs with polynomial growth, one-sided Lipschitz continuous drift, and bounded, globally Lipschitz continuous diffusion. Building on this, in [12, 13], a tamed-adaptive Euler-Maruyama scheme was proposed by combining the tamed Euler-Maruyama method with the adaptive approach. This scheme is applicable to SDEs with locally Lipschitz continuous and one-sided Lipschitz continuous drift terms, as well as locally (α+1/2)𝛼12(\alpha+1/2)( italic_α + 1 / 2 )-Hölder continuous diffusion coefficients. In [11], the authors employed a class of path-bounded time-stepping strategies, which reduce the step size as the solution stays near the boundary of a specified sphere, to develop an explicit adaptive Milstein method for SDEs whose coefficients do not satisfy the commutativity condition. They showed that this scheme achieves strong convergence of order 1111 for SDEs with one-sided Lipschitz continuous drift. In this paper, we introduce a tamed-adaptive Milstein scheme for equation (1), synthesizing key ideas from both the tamed Milstein scheme and adaptive schemes. The principal contribution of this work is the substantial relaxation of the regularity conditions required on the coefficients. Specifically, instead of assuming Lipschitz continuity, as in previous studies, we impose only local α𝛼\alphaitalic_α-Hölder continuity on the first derivatives of the coefficients μ𝜇\muitalic_μ and σ𝜎\sigmaitalic_σ. We establish that the tamed-adaptive scheme achieves strong convergence in the L2subscript𝐿2L_{2}italic_L start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT-norm with a rate of (1+α)/21𝛼2(1+\alpha)/2( 1 + italic_α ) / 2, illustrating the impact of the regularity of μ′superscript𝜇′\mu^{\prime}italic_μ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and σ′superscript𝜎′\sigma^{\prime}italic_σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT on the convergence rate of the Milstein scheme. Notably, when α=1𝛼1\alpha=1italic_α = 1, the strong convergence rate is shown to be optimal. Additionally, we analyze the strong convergence rate over both finite time intervals [0,T]0𝑇[0,T][ 0 , italic_T ] for a fixed T>0𝑇0T>0italic_T > 0, and the infinite time interval [0,∞)0[0,\infty)[ 0 , ∞ ). The study of approximation schemes over the infinite interval has only recently gained interest (see [3, 12, 13]). The integrability properties of both the exact and approximated solutions are essential in determining the strong convergence rate. The remainder of this paper is structured as follows. In Section 2, we outline the model assumptions and main results. Specifically, we define the tamed-adaptive Milstein scheme and present key theorems regarding moment estimates for the approximate process and the strong convergence of the scheme. All proofs are deferred to Section 3. In Section 4, we provide a numerical analysis of the tamed-adaptive Milstein scheme to illustrate the theoretical findings. Technical estimates for the increments of both the exact and approximate solutions are provided in Section 5."
https://arxiv.org/html/2411.01719v1,LES-SINDy: Laplace-Enhanced Sparse Identification of Nonlinear Dynamical Systems,"Sparse Identification of Nonlinear Dynamical Systems (SINDy) is a powerful tool for data-driven discovery of governing equations. However, it encounters challenges when modeling complex dynamical systems involving high-order derivatives or discontinuities, particularly in the presence of noise. These limitations restrict its applicability across various fields in applied mathematics and physics. To mitigate these, we propose Laplace-Enhanced SparSe Identification of Nonlinear Dynamical Systems (LES-SINDy). By transforming time-series measurements from the time domain to the Laplace domain using the Laplace transform and integration by parts, LES-SINDy enables more accurate approximations of derivatives and discontinuous terms. It also effectively handles unbounded growth functions and accumulated numerical errors in the Laplace domain, thereby overcoming challenges in the identification process. The model evaluation process selects the most accurate and parsimonious dynamical systems from multiple candidates. Experimental results across diverse ordinary and partial differential equations show that LES-SINDy achieves superior robustness, accuracy, and parsimony compared to existing methods.","The discovery of scientific laws from measurements is a significant intellectual milestone, and its motivation arises from the widespread occurrence of nonlinear dynamical systems in science and engineering. Understanding the governing equations, which often take the form of ordinary differential equations (ODEs), partial differential equations (PDEs), and stochastic differential equations (SDEs), is essential for accurate prediction, effective control, and informed decision-making [1, 2]. In many complex systems, the underlying dynamics remain poorly understood, which renders conventional modeling techniques based on first principles both challenging and, at times, intractable. To tackle the challenge of model discovery in dynamical systems, Sparse Identification of Nonlinear Dynamics (SINDy) [3] offers a data-driven solution. By the use of given measurements, SINDy constructs parsimonious models that capture the essential features of system dynamics without the need for detailed knowledge of the underlying physics. The strength of SINDy lies in its ability to identify sparse and interpretable models, based on the assumption that the system’s dynamics can be represented as a sparse linear combination of candidate functions. This process involves iterative optimization through sparse regression [4] and the selection of the most relevant terms from a comprehensive library, which enables the discovery of governing equations that are both accurate and physically meaningful. Building on the idea of using sparse regression techniques to discover nonlinear dynamical systems, extensive research has been conducted to enhance the SINDy framework for various objectives or to apply it across diverse domains. Schaeffer [5] employed lasso regression within SINDy to identify PDEs. Rudy et al. [6, 7] advanced the field by proposing PDE functional identification of nonlinear dynamics (PDE-FIND), which extends SINDy to discover dynamical systems from spatial time series measurements. Brunton et al. [8] and Kaiser el al. [9] focused on generalizing SINDy to include control inputs and facilitating its use in model predictive control. Zhang et al. [10] and Kaheman et al. [11] extended SINDy to identify implicit differential equations (DEs), thereby broadening its applicability. When modeling implicit DEs, criteria such as the Akaike information criterion (AIC) [12, 13] and Bayesian information criterion (BIC) [14, 15] are commonly employed to select the most accurate and parsimonious model among multiple candidates. To extract dynamical systems from complex, high-dimensional measurements, methods like autoencoders [16] and physics-informed neural networks [17, 18, 19] have effectively employed automatic differentiation to approximate system variables and identify significant derivative terms. More recent research has integrated neural networks with manifold theory to directly discover intrinsic state variables and their dynamics from time series measurements [20]. To enhance the utility of the SINDy framework in various scientific and engineering disciplines, the Python package PySINDy [21, 22] was developed to provide flexible tools for data-driven model discovery. With these advanced tools, data-driven methods can be applied effectively in diverse domains, including control systems [23, 24], fluid mechanics [25], biomechanics [26, 27], and chemistry [28]. Challenges and Motivations: While the SINDy framework is adaptable to various applications, ensuring robustness and accuracy is still a concern, particularly in the presence of high-order derivatives and discontinuities in the system dynamics [6]. The implementation of SINDy assumes that all terms in the library are smooth and differentiable. However, this assumption presents challenges when numerical differentiation methods, such as finite differences, are used to approximate high-order derivatives or discontinuous terms in the library. Consequently, identifying governing equations that include such terms becomes difficult, as such approximate errors can significantly impact the accuracy of the resulting sparse model. Furthermore, the presence of noisy measurements further exacerbates these issues, which potentially leads to instability and inaccuracy in the approximations. To address these challenges, researchers have explored various strategies, and one is to learn multiple SINDy models and effectively leverage them. For example, Ensemble-SINDy [29] discovered multiple SINDy models from different subsets of measurements simultaneously, and then aggregated the results to form the final model. Additionally, Uncertainty Quantification SINDy (UQ-SINDy) [30] introduced a probabilistic assessment of each candidate function, which was later incorporated with physical constraints [31]. By incorporating Bayesian methods to analyze multiple potential models, UQ-SINDy ensures more reliable model discovery and provides confidence intervals to quantify the certainty of the results. While these methods enhance robustness and reliability in model identification, they often become computationally intensive as the number of measurements accumulates. The computational cost is largely driven by the sparse regression process in SINDy, where the expense of matrix operations, like inversion, scales roughly cubicly with more measurements. Furthermore, the need for repeated sparse regression across numerous iterations or samples further amplifies the computational cost. An alternative strategy is to first transform the library from the time domain to another domain, such as the frequency or Laplace domain, where high-order derivatives and discontinuities are more easily handled. Sparse regression is then employed in the transformed domain. For instance, the Weak-SINDy [32, 33] transforms the SINDy library to the frequency domain using Fourier basis functions [34, 35], followed by sparse regression to produce the model. Intuitively. this approach is more efficient because the computational burden is no longer dictated by the sparse regression process. In the frequency domain, the matrix structure may be simplified and independent of the number of measurements (resolution-free), which makes matrix operation computationally advantageous. Inspired by the successful use of Laplace transformations in different machine-learning tasks [36, 37], we propose Laplace-Enhanced Sparse Identification of Nonlinear Dynamical Systems (LES-SINDy). This framework is designed to achieve robustness, efficiency, and accuracy in data-driven model discovery. Given time-series measurements, LES-SINDy first constructs candidate functions in the time domain and then subsequently transforms them to the Laplace domain using the Laplace transformation and integration by parts. After the transformation, a resolution-free sparse regression is employed to identify several potential candidate models. Lastly, the most appropriate model is selected from candidate models using a modified AIC. It is important to note that both LES-SINDy and Weak-SINDy aim to identify dynamical systems by transforming the library from the time domain to the frequency domain. However, we provide further insights into why LES-SINDy is a more effective option for discovering nonlinear dynamical systems, which will be further supported by the experimental results. One advantage of the Laplace transform over the Fourier transform is its ability to effectively handle functions that exhibit unbounded growth. The Fourier transform requires that all the candidate functions be absolutely integrable over the time domain, which might be problematic to approximate functions that grow exponentially or do not decay sufficiently. Consequently, there is no clear evidence to support Weak-SINDy can accurately identify dynamical systems with unbounded growth, such as (3) (Fig. 2(b)) and (9)-(10) (Fig. 3(b)). In contrast, the Laplace transform, with its exponentially decaying weighting factor, can handle such candidate functions by selecting s𝑠sitalic_s with an appropriately large real part. This makes the Laplace transform more suitable for analyzing candidate functions that do not satisfy the integrability conditions required by the Fourier transform. Moreover, the exponential weighting scheme in LES-SINDy can also benefit numerically simulated measurements, which are prone to accumulate errors over time. By assigning greater weight to function values at earlier times, the Laplace transform focuses on the more accurate, less error-prone parts of the simulation. This is in contrast to the Fourier transform, which may inadvertently introduce accumulated errors, particularly in the later stages of the simulation. Therefore, in scenarios where maintaining numerical stability is crucial, the Laplace transformation can be a more reliable choice. Contributions: We summarize the major contributions to this work as follows. 1. We propose the novel LES-SINDy framework for robust and accurate data-driven model discovery, which leverages the Laplace transform and integration by parts to transform the candidate functions from the time domain to the Laplace domain. 2. LES-SINDy addresses key challenges in existing methods by handling high-order derivatives, discontinuities, unbounded growth functions, and accumulated numerical errors. 3. LES-SINDy is “resolution-free”: it is unaffected by measurement resolution, while the computational burden in SINDy increases cubically with increasing measurements. 4. We test LES-SINDy across a diverse range of dynamical systems to validate its efficiency and robustness, including high-order ODEs, ODEs with discontinuous inputs, ODEs with trigonometric and hyperbolic functions, nonlinear ODE systems, and PDEs."
https://arxiv.org/html/2411.01649v1,Monolithic 3D numerical modeling of granular cargo movement on bulk carriers in waves,"A novel monolithic approach for simulating vessels in waves with granular cargo is presented using a Finite Volume framework. This model integrates a three-phase Volume of Fluid method to represent air, water, and cargo, coupled with a granular material model. The approach incorporates vessel dynamics by assuming rigid-body motion for the vessel’s empty hull. A three degrees of freedom rigid-body motion solver is applied for a 3D case study. The model also includes inviscid far-field boundary conditions facilitating the generation of linear waves approaching the vessel and applies a rigid-perfectly plastic material model for the granular phase. The model’s efficacy is demonstrated through a validation of the three-phase Volume of the Fluid method, a verification of the granular material model, and finally, a reconstruction of the ”Jian Fu Starïncident in a fully 3D simulation. This integrated approach is a feasibility study for investigating bulk carrier accidents, offering a powerful tool for maritime safety analysis and design optimization.","This paper introduces a fully coupled 3D three-phase model for cargo vessels transporting granular material in waves. The results of two validation and verification cases representing single dynamics of the coupled model and a complete 3D case application study rebuilding the conditions of a vessel sinking incident are presented. The background for the undertaken feasibility study is the accumulated accidents of bulk carriers transporting unsaturated ores between 2009 and 2019, summarized in table 1, which resulted in many casualties. Further ships transporting nickel or iron ore had to abort their voyage or take stability-enhancing measures to avoid capsizing and one of these incidents is described exemplarily in Lee [25]. A general description of the problem of carriage of nickel and iron ore on vessels can be found in Rose [31] and Munro et al. [29]. Also, historical accidents such as the sinking of the vessel ”Melanie Schulteïn 1952 (cf. Teutsch et al. [32]) could be related to cargo instability issues studied in this work. The effect that was held accountable for the numerous losses of bulk carriers is called liquefaction, which is a sudden viscous behavior of the loaded unsaturated granular material. Geotechnical liquefaction occurs when the forces resulting from pore pressure exceed the inter-particle forces and is mainly described in the context of earthquakes, e.g., Bian et al. [2], Di et al. [6, 7], Elgamal et al. [10] and Unno et al. [35]. The liquefaction of unsaturated granular material is a challenging mechanism to simulate as seen in Wobbes et al. [38] and Airey et al. [1]. Promising research, including the fluidization of granular material by pore gas pressure in the Finite Volume method, can be found in Chupin et al. [4]. Current research by Hoang et al. [15, 16] applying SPH-based methods to solve geomechanical problems could be extended for future applications on the application of bulk cargo transport. Based on the existing data from accidents and incidents, the cargo failure mechanism responsible for the incidents needs to be secured. Besides geotechnical liquefaction, cargo shifting and the so-called ”wet base”(cf. TWG [20], Rose [31]) or ”dynamic separation”(cf. Ferauge et al. [11]) mechanisms can lead to stability issues of the vessels. A ”wet baseöccurs if the water inside the cargo migrates towards the bottom and a fully saturated layer of cargo forms at the bottom of the cargo hold. The highly saturated bottom layer starts moving, induced by the vessel motion, and the more stable drier cargo slides around on the wet cargo layer. The theory of ”dynamic separationïs explained in Ferauge et al. [11] and assumes that a highly saturated cargo layer occurs on top of the cargo pile either by compression of the cargo, which pumps the internal water upwards or by external water coming overboard. Experience reports of captains suggest that water was found on top of the cargo piles and in the corners of the holds on vessels where unstable cargo led to emergency measures. Nevertheless, pure sliding of the iron and nickel ore cargo can also be the reason for the losses of the vessels since the material’s cohesion and repose angle depend on the cargo material’s saturation level. Most lost vessels are reported to have left from ports in regions where the monsoon rain was drenching the cargo before the vessel left. Sliding of the cargo can also occur in dry bulk cargos, e.g., wood or grains, and the presented method can be applied to incidents related to these cargoes as well, e.g., the incident of the ”Yong Fengïn 2021. Therefore, the present approach does not assume geotechnical liquefaction but uses a classical perfectly plastic material model described in [9] depending on the material’s cohesion and angle of repose to represent the cargo behavior. Applying material parameters responding to the material information given in the incident reports, this approach can represent the shifting of cargo during the incidents. Saturation-dependent cohesion and angle of repose can be applied to the presented model. Tabelle 1: List of bulk carriers carrying nickel or iron ore lost due to cargo shift/ liquefaction from 2009200920092009 until 2024202420242024 as reported in the ”Bulk Carrier Casualty Reportsöf INTERGARGO [17, 18]. Vessel name D⁢W⁢T𝐷𝑊𝑇DWTitalic_D italic_W italic_T [t] L⁢o⁢A𝐿𝑜𝐴LoAitalic_L italic_o italic_A [m] B𝐵Bitalic_B [m] year cargo loss of lives Black Rose 37657 187.7 28.4 2009 Indian iron ore fines 1 Asian Forest 14434 128.0 20.0 2009 Indian iron ore fines 0 Jian Fu Star 45108 189.8 31.3 2010 Indonesian nickel ore 13 Nasco Diamond 56893 185.6 32.3 2010 Indonesian nickel ore 22 Hong Wei 50149 189.8 32.3 2010 Indonesian nickel ore 10 Vinalines Queen 56040 190.0 32.3 2011 Indonesian nickel ore 22 Harita Bauxite 48891 192.0 32.0 2013 Indonesian nickel ore 15 Trans Summer 56824 190.0 32.0 2013 Indonesian nickel ore 0 Bulk Jupiter 56009 190.0 32.3 2015 Malaysian Bauxite 18 Emerald Star 57367 190.0 32.0 2017 Indonesian nickel ore 10 Nur Allya 52378 190.0 32.0 2019 Indonesian nickel ore 27 Other models have been developed to study the cargo behavior on bulk carriers, some of which are similar to the approach of the present model. In Zou et al. [44], a level-set-based Finite Volume (FV) free-surface method is used to study the sloshing of highly viscous fluids in rectangular tanks, missing a material model representing the granular phase. A Discrete Element Method (DEM) based model for simulating liquefaction on vessels has been developed by Ju et al. [23, 22]. This innovative approach provides a granular-level analysis of cargo behavior under maritime conditions. In a related study, Zhang et al. [43] employed a coupled non-Newtonian fluid model with a simplified body surface method to investigate vessel responses. Their research examined the intricate relationships between wave frequencies, amplitudes, and resulting ship motions. Wang et al. [37] applied the DEM methods to explore the influence of material parameters on cargo movement. This study elucidated the critical role of specific cargo properties in determining the risk and extent of potential shifts during transit. Complementing these efforts, Wu et al. [41] conducted a comprehensive analysis of liquefaction risk utilizing advanced 3D vessel simulations. These collective studies demonstrate the growing sophistication of computational models in addressing the challenges of maritime cargo transport. In the present work, these studies will be expanded by a fully 3D model of a vessel with granular cargo in wave conditions derived from an incident report. In this paper, the mathematical model and numerical method are first described. Validation cases of the three-phase and granular material models are presented, and the results of a 3D case study of the coupled problem are reported. As a 3D case study, the ”Jian Fu Starïncident on 27th October 2010 is chosen due to its representative vessel dimensions deduced from table 1 and the incident conditions are taken from the incident report of the ”Panama Maritime Authority”[19]."
https://arxiv.org/html/2411.01633v1,On the Limit of the Tridiagonal Model forβ𝛽\betaitalic_β-Dyson Brownian Motion,"In previous work, Edelman and Dumitriu provide a description of the result of applying the Householder tridiagonalization algorithm to a Gβ𝛽\betaitalic_βE random matrix. The resulting tridiagonal ensemble makes sense for all β>0𝛽0\beta>0italic_β > 0, and has spectrum given by the β𝛽\betaitalic_β-ensemble for all β>0𝛽0\beta>0italic_β > 0. Moreover, the tridiagonal model has useful stochastic operator limits introduced and analyzed by Edelman and Sutton, and subsequently analyzed in work by Ramirez, Rider, and Virág. In this work we analogously study the result of applying the Householder tridiagonalization algorithm to a Gβ𝛽\betaitalic_βE process which has eigenvalues governed by β𝛽\betaitalic_β-Dyson Brownian motion. We propose an explicit limit of the upper left k×k𝑘𝑘k\times kitalic_k × italic_k minor of the n×n𝑛𝑛n\times nitalic_n × italic_n tridiagonal process as n→∞→𝑛n\to\inftyitalic_n → ∞ and k𝑘kitalic_k remains fixed. We prove the result for β=1𝛽1\beta=1italic_β = 1, and also provide numerical evidence for β=1,2,4𝛽124\beta=1,2,4italic_β = 1 , 2 , 4. This leads us to conjecture the form of a dynamical β𝛽\betaitalic_β-stochastic Airy operator with smallest k𝑘kitalic_k eigenvalues evolving according to the n→∞→𝑛n\to\inftyitalic_n → ∞ limit of the largest, centered and re-scaled, k𝑘kitalic_k eigenvalues of β𝛽\betaitalic_β-Dyson Brownian motion.","Dyson Brownian motion (DBM) introduced by Freeman Dyson in 1962 [Dys62] is a widely studied stochastic model for a system of particles that possess pairwise repulsive behavior. For any β>0𝛽0\beta>0italic_β > 0, β𝛽\betaitalic_β-Dyson Brownian motion is the dynamics of a system {λi}i=1nsuperscriptsubscriptsubscript𝜆𝑖𝑖1𝑛\{\lambda_{i}\}_{i=1}^{n}{ italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT undergoing the following system of stochastic differential equations d⁢λi=(∑1≤j≠i≤n1λi−λj−∑i=1nλi2)⁢d⁢t+2β⁢d⁢Bj.𝑑subscript𝜆𝑖subscript1𝑗𝑖𝑛1subscript𝜆𝑖subscript𝜆𝑗superscriptsubscript𝑖1𝑛subscript𝜆𝑖2𝑑𝑡2𝛽𝑑subscript𝐵𝑗d\lambda_{i}=\left(\sum_{1\leq j\neq i\leq n}\frac{1}{\lambda_{i}-\lambda_{j}}% -\sum_{i=1}^{n}\frac{\lambda_{i}}{2}\right)dt+\sqrt{\frac{2}{\beta}}dB_{j}.italic_d italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( ∑ start_POSTSUBSCRIPT 1 ≤ italic_j ≠ italic_i ≤ italic_n end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_λ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT divide start_ARG italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ) italic_d italic_t + square-root start_ARG divide start_ARG 2 end_ARG start_ARG italic_β end_ARG end_ARG italic_d italic_B start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . (1.1) As long as λ1⁢(0),…,λn⁢(0)subscript𝜆10…subscript𝜆𝑛0\lambda_{1}(0),...,\lambda_{n}(0)italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 0 ) , … , italic_λ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( 0 ), are distinct, it is known that (1.1) has a unique strong solution and λ1⁢(t),…,λn⁢(t)subscript𝜆1𝑡…subscript𝜆𝑛𝑡\lambda_{1}(t),...,\lambda_{n}(t)italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_t ) , … , italic_λ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ( italic_t ) are a.s. distinct for all t>0𝑡0t>0italic_t > 0. Moreover β𝛽\betaitalic_β-DBM has a unique invariant distribution given by the β𝛽\betaitalic_β-ensemble (where 𝒵𝒵\mathcal{Z}caligraphic_Z is for normalization) 𝒵−1⁢∏i<j|λi−λj|β⁢∏i=1ne−λi2/4⁢d⁢xi.superscript𝒵1subscriptproduct𝑖𝑗superscriptsubscript𝜆𝑖subscript𝜆𝑗𝛽superscriptsubscriptproduct𝑖1𝑛superscript𝑒superscriptsubscript𝜆𝑖24𝑑subscript𝑥𝑖\mathcal{Z}^{-1}\prod_{i<j}|\lambda_{i}-\lambda_{j}|^{\beta}\prod_{i=1}^{n}e^{% -\lambda_{i}^{2}/4}dx_{i}.caligraphic_Z start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ∏ start_POSTSUBSCRIPT italic_i < italic_j end_POSTSUBSCRIPT | italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_λ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | start_POSTSUPERSCRIPT italic_β end_POSTSUPERSCRIPT ∏ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / 4 end_POSTSUPERSCRIPT italic_d italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . (1.2) In the random matrix context, for β=1,2,4𝛽124\beta=1,2,4italic_β = 1 , 2 , 4 β𝛽\betaitalic_β-DBM describes the evolution of the eigenvalues of matrices evolving according to the Gβ𝛽\betaitalic_βE process, the following matrix valued Ornstein-Uhlenbeck process d⁢M=−M⁢d⁢t+2⁢d⁢BG⁢β⁢E𝑑𝑀𝑀𝑑𝑡2𝑑subscript𝐵𝐺𝛽𝐸dM=-Mdt+\sqrt{2}dB_{G\beta E}italic_d italic_M = - italic_M italic_d italic_t + square-root start_ARG 2 end_ARG italic_d italic_B start_POSTSUBSCRIPT italic_G italic_β italic_E end_POSTSUBSCRIPT (1.3) which leaves the Gβ𝛽\betaitalic_βE distribution invariant [AGZ09, For10]. And as a consequence Gβ𝛽\betaitalic_βE matrices (alternatively known as Hermite ensembles) have eigenvalue distribution given by (1.2). For β=1,2,4𝛽124\beta=1,2,4italic_β = 1 , 2 , 4 the Gβ𝛽\betaitalic_βE matrices coincide with the highly studied Gaussian Orthogonal Ensemble (GOE), Gaussian Unitary Ensemble (GUE), and Gaussian Symplectic Ensemble (GSE) respectively. These three Gβ𝛽\betaitalic_βEs are described by the canonical random symmetric, Hermitian, and self-dual matrix models, respectively, endowed with Gaussian entries. For general β>0𝛽0\beta>0italic_β > 0, there is no immediate generalization. However, by applying the Householder tridiagonalization procedure used in numerical linear algebra on Gβ𝛽\betaitalic_βE matrices [Bau59, Wil62], one obtains the following symmetric tridiagonal model [DE02]: (N⁢(0,2)χβ⁢nχβ⁢nN⁢(0,2)χβ⁢(n−1)χβ⁢(n−1)⋱⋱⋱⋱⋱⋱N⁢(0,2)χ2⁢βχ2⁢βN⁢(0,2)χβχβN⁢(0,2)).matrix𝑁02subscript𝜒𝛽𝑛missing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionsubscript𝜒𝛽𝑛𝑁02subscript𝜒𝛽𝑛1missing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionsubscript𝜒𝛽𝑛1⋱⋱missing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpression⋱⋱⋱missing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpression⋱𝑁02subscript𝜒2𝛽missing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionsubscript𝜒2𝛽𝑁02subscript𝜒𝛽missing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionmissing-subexpressionsubscript𝜒𝛽𝑁02\begin{pmatrix}N(0,2)&\chi_{\beta n}&&&&&\\ \chi_{\beta n}&N(0,2)&\chi_{\beta(n-1)}&&&&\\ &\chi_{\beta(n-1)}&\ddots&\ddots&&&\\ &&\ddots&\ddots&\ddots&&\\ &&&\ddots&N(0,2)&\chi_{2\beta}&\\ &&&&\chi_{2\beta}&N(0,2)&\chi_{\beta}\\ &&&&&\chi_{\beta}&N(0,2)\end{pmatrix}.( start_ARG start_ROW start_CELL italic_N ( 0 , 2 ) end_CELL start_CELL italic_χ start_POSTSUBSCRIPT italic_β italic_n end_POSTSUBSCRIPT end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL italic_χ start_POSTSUBSCRIPT italic_β italic_n end_POSTSUBSCRIPT end_CELL start_CELL italic_N ( 0 , 2 ) end_CELL start_CELL italic_χ start_POSTSUBSCRIPT italic_β ( italic_n - 1 ) end_POSTSUBSCRIPT end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL italic_χ start_POSTSUBSCRIPT italic_β ( italic_n - 1 ) end_POSTSUBSCRIPT end_CELL start_CELL ⋱ end_CELL start_CELL ⋱ end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL ⋱ end_CELL start_CELL ⋱ end_CELL start_CELL ⋱ end_CELL start_CELL end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL ⋱ end_CELL start_CELL italic_N ( 0 , 2 ) end_CELL start_CELL italic_χ start_POSTSUBSCRIPT 2 italic_β end_POSTSUBSCRIPT end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL italic_χ start_POSTSUBSCRIPT 2 italic_β end_POSTSUBSCRIPT end_CELL start_CELL italic_N ( 0 , 2 ) end_CELL start_CELL italic_χ start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL italic_χ start_POSTSUBSCRIPT italic_β end_POSTSUBSCRIPT end_CELL start_CELL italic_N ( 0 , 2 ) end_CELL end_ROW end_ARG ) . (1.4) It was verified [DE02] that for all β>0𝛽0\beta>0italic_β > 0 one has (1.2) as the eigenvalue distribution for the tridiagonal matrix ensemble (1.4). In addition to providing a matrix model for the the β𝛽\betaitalic_β ensemble for all β𝛽\betaitalic_β, one can find stochastic differential operator limits (n→∞→𝑛n\to\inftyitalic_n → ∞) for the tridiagonal matrix model which provide further insight into quantities such as the largest eigenvalue governed by the β𝛽\betaitalic_β-Tracy Widom distribution. Over the past two decades the Gβ𝛽\betaitalic_βE tridiagonal model and its operator limits have been studied and applied extensively (e.g., [LP20, DE06, RRV11, DP12, AD14, KRV16, VV17, GS18, TZ+24]). \begin{overpic}[width=411.93767pt]{figures/intro.pdf} \put(82.5,34.4){\rotatebox{16.0}{$n$ large}} \put(92.0,35.3){\eqref{Tridiagonal Evolution Informal}} \put(48.0,16.5){$\longrightarrow$} \end{overpic} Figure 1: Householder Tridiagonalization (right) of a 10×10101010\times 1010 × 10 GOE process (left) In this work, we consider applying the Householder tridiagonalization procedure to the n×n𝑛𝑛n\times nitalic_n × italic_n Gβ𝛽\betaitalic_βE process, for each t𝑡titalic_t (depicted graphically in Figure 1). The resulting process is a real symmetric tridiagonal matrix valued stochastic process with eigenvalue distribution governed by Dyson Brownian motion. Since these tridiagonal matrices are symmetric, we can treat the entries as a system of 2⁢n−12𝑛12n-12 italic_n - 1 scalar stochastic processes. Although at a fixed time these processes are described simply as in (1.4), as processes they appear to be considerably more complicated. For instance, our numerical results in Section 4.2 indicate that the diagonal entries are not Gaussian processes despite their Gaussianity at a fixed time. Nevertheless, as the dimension of the matrices n→∞→𝑛n\to\inftyitalic_n → ∞, a fixed k×k𝑘𝑘k\times kitalic_k × italic_k upper-left corner submatrix process can be roughly described as below where the entries are independent to each other: (OU⁢(m)OU𝑚\text{OU}(m)OU ( italic_m ) is an Ornstein-Uhlenbeck process with parameters θ=m,σ=2⁢mformulae-sequence𝜃𝑚𝜎2𝑚\theta=m,\sigma=\sqrt{2m}italic_θ = italic_m , italic_σ = square-root start_ARG 2 italic_m end_ARG, OUk⁢(m)subscriptOU𝑘𝑚\textbf{OU}_{k}(m)OU start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_m ) is a length k𝑘kitalic_k vector of k𝑘kitalic_k independent OU⁢(m)OU𝑚\text{OU}(m)OU ( italic_m ) processes; refer to Section 2.1 for the notation) (2⁢OU⁢(1)β⁢‖𝐎𝐔n−1⁢(1)‖β⁢‖𝐎𝐔n−1⁢(1)‖2⁢OU⁢(3)⋱⋱⋱⋱⋱⋱β⁢‖𝐎𝐔n−k+1⁢(k−1)‖β⁢‖𝐎𝐔n−k+1⁢(k−1)‖2⁢OU⁢(2⁢k−1)).matrix2OU1𝛽normsubscript𝐎𝐔𝑛11missing-subexpression𝛽normsubscript𝐎𝐔𝑛112OU3⋱missing-subexpression⋱⋱⋱missing-subexpression⋱⋱missing-subexpressionmissing-subexpressionmissing-subexpression𝛽normsubscript𝐎𝐔𝑛𝑘1𝑘1missing-subexpressionmissing-subexpression𝛽normsubscript𝐎𝐔𝑛𝑘1𝑘12OU2𝑘1\begin{pmatrix}\sqrt{2}\mathrm{OU}(1)&\sqrt{\beta}||\mathbf{OU}_{n-1}(1)||&\\ \sqrt{\beta}||\mathbf{OU}_{n-1}(1)||&\sqrt{2}\mathrm{OU}(3)&\ddots\\ &\ddots&\ddots&\ddots\\ &\hskip 42.67912pt\ddots&\ddots\\ &&&\sqrt{\beta}||\mathbf{OU}_{n-k+1}(k-1)||\\ &&\sqrt{\beta}||\mathbf{OU}_{n-k+1}(k-1)||&\sqrt{2}\mathrm{OU}(2k-1)\end{% pmatrix}.( start_ARG start_ROW start_CELL square-root start_ARG 2 end_ARG roman_OU ( 1 ) end_CELL start_CELL square-root start_ARG italic_β end_ARG | | bold_OU start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ( 1 ) | | end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL square-root start_ARG italic_β end_ARG | | bold_OU start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ( 1 ) | | end_CELL start_CELL square-root start_ARG 2 end_ARG roman_OU ( 3 ) end_CELL start_CELL ⋱ end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL ⋱ end_CELL start_CELL ⋱ end_CELL start_CELL ⋱ end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL ⋱ end_CELL start_CELL ⋱ end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL end_CELL start_CELL square-root start_ARG italic_β end_ARG | | bold_OU start_POSTSUBSCRIPT italic_n - italic_k + 1 end_POSTSUBSCRIPT ( italic_k - 1 ) | | end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL end_CELL start_CELL square-root start_ARG italic_β end_ARG | | bold_OU start_POSTSUBSCRIPT italic_n - italic_k + 1 end_POSTSUBSCRIPT ( italic_k - 1 ) | | end_CELL start_CELL square-root start_ARG 2 end_ARG roman_OU ( 2 italic_k - 1 ) end_CELL end_ROW end_ARG ) . (1.5) Furthermore, from the central limit theorem, one can verify that the off-diagonal entries can be further simplified by the property 12⁢n⁢(‖𝐎𝐔n−j⁢(j)‖2−n)→OU⁢(2⁢j)→12𝑛superscriptnormsubscript𝐎𝐔𝑛𝑗𝑗2𝑛OU2𝑗\frac{1}{\sqrt{2n}}(||\mathbf{OU}_{n-j}(j)||^{2}-n)\to\mathrm{OU}(2j)divide start_ARG 1 end_ARG start_ARG square-root start_ARG 2 italic_n end_ARG end_ARG ( | | bold_OU start_POSTSUBSCRIPT italic_n - italic_j end_POSTSUBSCRIPT ( italic_j ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_n ) → roman_OU ( 2 italic_j ) as n→∞→𝑛n\to\inftyitalic_n → ∞. Thus our main result, Theorem 3.1, states that for the tridiagonalization of the n×n𝑛𝑛n\times nitalic_n × italic_n Gβ𝛽\betaitalic_βE process, denoting diagonal entries by aj⁢(t)subscript𝑎𝑗𝑡a_{j}(t)italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) and off-diagonal entries by bj⁢(t)subscript𝑏𝑗𝑡b_{j}(t)italic_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ), as n→∞→𝑛n\to\inftyitalic_n → ∞ aj⁢(t)subscript𝑎𝑗𝑡\displaystyle a_{j}(t)italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) →2⁢OU⁢(2⁢j−1),→absent2OU2𝑗1\displaystyle\to\sqrt{2}\mathrm{OU}(2j-1),→ square-root start_ARG 2 end_ARG roman_OU ( 2 italic_j - 1 ) , 1n⁢β⁢(bj⁢(t)2−β⁢n)1𝑛𝛽subscript𝑏𝑗superscript𝑡2𝛽𝑛\displaystyle\frac{1}{\sqrt{n}\beta}(b_{j}(t)^{2}-\beta n)divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_n end_ARG italic_β end_ARG ( italic_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_t ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_β italic_n ) →2⁢OU⁢(2⁢j),→absent2OU2𝑗\displaystyle\to\sqrt{2}\mathrm{OU}(2j),→ square-root start_ARG 2 end_ARG roman_OU ( 2 italic_j ) , for entries in the k×k𝑘𝑘k\times kitalic_k × italic_k upper-left corner submatrix process. Moreover all of these limiting Ornstein-Uhlenbeck processes are independent of each other. Although we only prove the statement for β=1𝛽1\beta=1italic_β = 1, the proof should easily generalize to the β=2,4𝛽24\beta=2,4italic_β = 2 , 4 cases, and our numerical evidences substantiate this claim. This result provides conjectures for simple tridiagonal models which have the same largest few eigenvalues as β𝛽\betaitalic_β-Dyson Browniant motion (namely extending the pattern (1.5) all the way to k=n𝑘𝑛k=nitalic_k = italic_n) along with a time evolving stochastic Airy operator (see Section 5). Unfortunately our result only concern a finite number of entry processes, so we cannot actually recover eigenvalue or operator limit information. Moreover, our numerical experiments do not yet provide strong evidence for these conjectures. In Section 5 we further discuss these conjectures and further open problems related to tridiagonal processes and their eigenvalues. In addition to proving the main theorem, we perform numerical experiments to support our result. Since we study large matrix processes our simulations are implemented on parallel machines to increase efficiency. See Section 4 for details. 1.1 Organization of the Paper Section 2 covers the notation and background needed to read the rest of the paper. In particular Sections 2.1 and 2.2 provide the notation and setup needed to understand the main result, numerical experiments, and future directions, while Sections 2.3, 2.4, 2.5, and 2.6, are only referred to during the proof of Theorem 3.1, which is the main result. In Section 3 we state the main result, Theorem 3.1, and outline its proof. Before proving Theorem 3.1, we provide numerical results in Section 4, and remarks on prior research in the areas, as well as potential future direction in Section 5. These two sections can be read independently of the proof of Theorem 3.1 which is deferred to Sections 6 and 7, as well as the Appendix which supplements Section 6."
https://arxiv.org/html/2411.01546v1,On the existence of extremal solutions for the conjugate discrete-time Riccati equation,"In this paper we consider a class of conjugate discrete-time Riccati equations (CDARE), arising originally from the linear quadratic regulation problem for discrete-time antilinear systems. Recently, we have proved the existence of the maximal solution to the CDARE with a nonsingular control weighting matrix under the framework of the constructive method. Our contribution in the work is to finding another meaningful Hermitian solutions, which has received little attention in this topic. Moreover, we show that some extremal solutions cannot be attained at the same time, and almost (anti-)stabilizing solutions coincide with some extremal solutions. It is to be expected that our theoretical results presented in this paper will play an important role in the optimal control problems for discrete-time antilinear systems.","A conjugate discrete-time algebraic Riccati equation (CDARE) is the matrix equation X=ℛ⁢(X):=AH⁢X¯⁢A−AH⁢X¯⁢B⁢RX−1⁢BH⁢X¯⁢A+H,𝑋ℛ𝑋assignsuperscript𝐴𝐻¯𝑋𝐴superscript𝐴𝐻¯𝑋𝐵superscriptsubscript𝑅𝑋1superscript𝐵𝐻¯𝑋𝐴𝐻\displaystyle X=\mathcal{R}(X):=A^{H}\overline{X}A-A^{H}\overline{X}BR_{X}^{-1% }B^{H}\overline{X}A+H,italic_X = caligraphic_R ( italic_X ) := italic_A start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT over¯ start_ARG italic_X end_ARG italic_A - italic_A start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT over¯ start_ARG italic_X end_ARG italic_B italic_R start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_B start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT over¯ start_ARG italic_X end_ARG italic_A + italic_H , (1a) or its equivalent expression X=AH⁢X¯⁢(I+G⁢X¯)−1⁢A+H,𝑋superscript𝐴𝐻¯𝑋superscript𝐼𝐺¯𝑋1𝐴𝐻\displaystyle X=A^{H}\overline{X}(I+G\overline{X})^{-1}A+H,italic_X = italic_A start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT over¯ start_ARG italic_X end_ARG ( italic_I + italic_G over¯ start_ARG italic_X end_ARG ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_A + italic_H , (1b) where A∈ℂn×n𝐴superscriptℂ𝑛𝑛A\in\mathbb{C}^{n\times n}italic_A ∈ blackboard_C start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT, B∈ℂn×m𝐵superscriptℂ𝑛𝑚B\in\mathbb{C}^{n\times m}italic_B ∈ blackboard_C start_POSTSUPERSCRIPT italic_n × italic_m end_POSTSUPERSCRIPT, R∈ℍm𝑅subscriptℍ𝑚R\in\mathbb{H}_{m}italic_R ∈ blackboard_H start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is nonsingular, H=C⁢CH∈ℍn𝐻𝐶superscript𝐶𝐻subscriptℍ𝑛H=CC^{H}\in\mathbb{H}_{n}italic_H = italic_C italic_C start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT ∈ blackboard_H start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT with C∈ℂn×n𝐶superscriptℂ𝑛𝑛C\in\mathbb{C}^{n\times n}italic_C ∈ blackboard_C start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT, I𝐼Iitalic_I is the identity matrix of compatible size, G:=B⁢R−1⁢BHassign𝐺𝐵superscript𝑅1superscript𝐵𝐻G:=BR^{-1}B^{H}italic_G := italic_B italic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_B start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT, RX:=R+BH⁢X¯⁢Bassignsubscript𝑅𝑋𝑅superscript𝐵𝐻¯𝑋𝐵R_{X}:=R+B^{H}\overline{X}Bitalic_R start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT := italic_R + italic_B start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT over¯ start_ARG italic_X end_ARG italic_B and the unknown X∈dom⁢(ℛ)𝑋domℛX\in\mathrm{dom}(\mathcal{R})italic_X ∈ roman_dom ( caligraphic_R ), respectively. Here, ℍℓsubscriptℍℓ\mathbb{H}_{\ell}blackboard_H start_POSTSUBSCRIPT roman_ℓ end_POSTSUBSCRIPT denotes the set of all ℓ×ℓℓℓ\ell\times\ellroman_ℓ × roman_ℓ Hermitian matrices and dom⁢(ℛ):={X∈ℍn|det(RX)≠0}assigndomℛconditional-set𝑋subscriptℍ𝑛subscript𝑅𝑋0\mathrm{dom}(\mathcal{R}):=\{X\in\mathbb{H}_{n}\,|\,\det(R_{X})\neq 0\}roman_dom ( caligraphic_R ) := { italic_X ∈ blackboard_H start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT | roman_det ( italic_R start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ) ≠ 0 }. We also notice that RXsubscript𝑅𝑋R_{X}italic_R start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT is nonsingular if and only if det(I+G⁢X¯)≠0𝐼𝐺¯𝑋0\det(I+G\overline{X})\neq 0roman_det ( italic_I + italic_G over¯ start_ARG italic_X end_ARG ) ≠ 0. A class of CDAREs (1) arises from the linear quadratic regulation (LQR) optimal control problem for the discrete-time antilinear system of the state space representation xk+1=A⁢x¯k+B⁢u¯k,k≥0,formulae-sequencesubscript𝑥𝑘1𝐴subscript¯𝑥𝑘𝐵subscript¯𝑢𝑘𝑘0x_{k+1}=A\overline{x}_{k}+B\overline{u}_{k},\quad k\geq 0,italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_A over¯ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_B over¯ start_ARG italic_u end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_k ≥ 0 , (2) where xk∈ℂnsubscript𝑥𝑘superscriptℂ𝑛x_{k}\in\mathbb{C}^{n}italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_C start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT is the state response and uk∈ℂmsubscript𝑢𝑘superscriptℂ𝑚u_{k}\in\mathbb{C}^{m}italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ∈ blackboard_C start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT is the control input. The main goal of this control problem is to find a state feedback control uk=−F⁢xksubscript𝑢𝑘𝐹subscript𝑥𝑘u_{k}=-Fx_{k}italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = - italic_F italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT such that the performance index 𝒥⁢(uk,x0):=∑k=0∞[xkHukH]⁢[HR]⁢[xkuk]assign𝒥subscript𝑢𝑘subscript𝑥0superscriptsubscript𝑘0matrixsuperscriptsubscript𝑥𝑘𝐻superscriptsubscript𝑢𝑘𝐻matrix𝐻missing-subexpressionmissing-subexpression𝑅matrixsubscript𝑥𝑘subscript𝑢𝑘\mathcal{J}(u_{k},x_{0}):=\sum_{k=0}^{\infty}\begin{bmatrix}x_{k}^{H}&u_{k}^{H% }\end{bmatrix}\begin{bmatrix}H&\\ &R\end{bmatrix}\begin{bmatrix}x_{k}\\ u_{k}\end{bmatrix}caligraphic_J ( italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) := ∑ start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT end_CELL start_CELL italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL italic_H end_CELL start_CELL end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL italic_R end_CELL end_ROW end_ARG ] [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] is minimized with H≥0𝐻0H\geq 0italic_H ≥ 0 and R>0𝑅0R>0italic_R > 0. If the antilinear system (2) is controllable [11, Theorem 12.7] or the matrix pair (A⁢A¯,[BA⁢B¯])𝐴¯𝐴matrix𝐵𝐴¯𝐵(A\overline{A},\begin{bmatrix}B&A\overline{B}\end{bmatrix})( italic_A over¯ start_ARG italic_A end_ARG , [ start_ARG start_ROW start_CELL italic_B end_CELL start_CELL italic_A over¯ start_ARG italic_B end_ARG end_CELL end_ROW end_ARG ] ) is stabilizable [12, Theorem 2], the optimal state feedback controller is uk∗:=−RX∗−1⁢BH⁢𝐙¯𝐌⁢A⁢xkassignsuperscriptsubscript𝑢𝑘superscriptsubscript𝑅subscript𝑋1superscript𝐵𝐻subscript¯𝐙𝐌𝐴subscript𝑥𝑘u_{k}^{*}:=-R_{X_{*}}^{-1}B^{H}\mathbf{\overline{Z}_{M}}Ax_{k}italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT := - italic_R start_POSTSUBSCRIPT italic_X start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT italic_B start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT over¯ start_ARG bold_Z end_ARG start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT italic_A italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT for k≥0𝑘0k\geq 0italic_k ≥ 0 and thus the minimum value of 𝒥⁢(uk∗,x0)=x0H⁢𝐙𝐌⁢x0𝒥superscriptsubscript𝑢𝑘subscript𝑥0superscriptsubscript𝑥0𝐻subscript𝐙𝐌subscript𝑥0\mathcal{J}(u_{k}^{*},x_{0})=x_{0}^{H}\mathbf{Z_{M}}x_{0}caligraphic_J ( italic_u start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_H end_POSTSUPERSCRIPT bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is achieved, where 𝐙𝐌:=max⁡(ℝ=∩ℕn)assignsubscript𝐙𝐌subscriptℝsubscriptℕ𝑛\mathbf{Z_{M}}:=\max(\mathbb{R}_{=}\cap\mathbb{N}_{n})bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT := roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) is the unique positive solution of the CDARE (1a). Here, the notation ℕnsubscriptℕ𝑛\mathbb{N}_{n}blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT collects all positive semidefinite matrices with size n𝑛nitalic_n and all Hermitian solution of CDARE (1) is denoted by ℝ=subscriptℝ\mathbb{R}_{=}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT. The positive definite solution of CDARE (1) has been an active area of research and CDARE (1) is also called the discrete-time algebraic anti-Riccati equation [10, 11]. For more details see [11, 12]. In the past few years, the authors [5, 6] discovered a way to construct the existence of the maximal solution 𝐗𝐌:=max⁡(ℝ=∩ℙ)assignsubscript𝐗𝐌subscriptℝℙ\mathbf{X_{M}}:=\max(\mathbb{R}_{=}\cap\mathbb{P})bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT := roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P ) for the CDARE (1) with G,H∈ℍn𝐺𝐻subscriptℍ𝑛G,H\in\mathbb{H}_{n}italic_G , italic_H ∈ blackboard_H start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. Here, the notation ℙℙ\mathbb{P}blackboard_P collects any Hermitian matrix X𝑋Xitalic_X such that RX>0subscript𝑅𝑋0R_{X}>0italic_R start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT > 0. More precisely, a class of the fixed-point iteration (FPI) has been utilized to compute 𝐗𝐌subscript𝐗𝐌\mathbf{X_{M}}bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT under the weaker assumptions. Further exploitation in the convergence result of the FPI appeared in recent work [6] or see the below Theorem 2.1. It is clear that 𝐗𝐌≥𝐙𝐌subscript𝐗𝐌subscript𝐙𝐌\mathbf{X_{M}}\geq\mathbf{Z_{M}}bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT ≥ bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT when G,H≥0𝐺𝐻0G,H\geq 0italic_G , italic_H ≥ 0 since ℕn⊆ℙsubscriptℕ𝑛ℙ\mathbb{N}_{n}\subseteq\mathbb{P}blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ⊆ blackboard_P. However, 𝐗𝐌−𝐙𝐌subscript𝐗𝐌subscript𝐙𝐌\mathbf{X_{M}}-\mathbf{Z_{M}}bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT - bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT may be indefinite in general if they exist. One aim of this work is to establish some appropriate conditions for the existence and relation to all meaningful solutions including 𝐗𝐌subscript𝐗𝐌\mathbf{X_{M}}bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT and 𝐙𝐌subscript𝐙𝐌\mathbf{Z_{M}}bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT. Based on these recent works, it is natural to ask whether another meaningful extremal solution 𝐗𝐦:=max⁡(ℝ=∩ℙ)assignsubscript𝐗𝐦subscriptℝℙ\mathbf{X_{m}}:=\max(\mathbb{R}_{=}\cap\mathbb{P})bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT := roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P ), 𝐘𝐌:=max⁡(ℝ=∩𝔼)assignsubscript𝐘𝐌subscriptℝ𝔼\mathbf{Y_{M}}:=\max(\mathbb{R}_{=}\cap\mathbb{E})bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT := roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E ) and 𝐘𝐦:=min⁡(ℝ=∩𝔼)assignsubscript𝐘𝐦subscriptℝ𝔼\mathbf{Y_{m}}:=\min(\mathbb{R}_{=}\cap\mathbb{E})bold_Y start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT := roman_min ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E ) exist. Here, the notation 𝔼𝔼\mathbb{E}blackboard_E collects any Hermitian matrix X𝑋Xitalic_X such that RX<0subscript𝑅𝑋0R_{X}<0italic_R start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT < 0. In analogy with some particular Hermitian solutions of discrete-time algebraic Riccati equation(DARE) [4, 5], another interesting issue including almost stabilizing, almost anti-stabilizing, positive semidefinite and negative semidefinite solutions will also be defined and investigated. For the sake of explanation, the maximal element of ℝ=∩ℕnsubscriptℝsubscriptℕ𝑛\mathbb{R}_{=}\cap\mathbb{N}_{n}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, the minimal element of ℝ=∩ℕnsubscriptℝsubscriptℕ𝑛\mathbb{R}_{=}\cap\mathbb{N}_{n}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, the maximal element of ℝ=∩−ℕn\mathbb{R}_{=}\cap-\mathbb{N}_{n}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ - blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, the minimal element of ℝ=∩−ℕn\mathbb{R}_{=}\cap-\mathbb{N}_{n}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ - blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, the maximal element of ℝ=∩𝔼subscriptℝ𝔼\mathbb{R}_{=}\cap\mathbb{E}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E and the minimal element of ℝ=∩𝔼subscriptℝ𝔼\mathbb{R}_{=}\cap\mathbb{E}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E are denoted by 𝐙𝐌subscript𝐙𝐌\mathbf{\mathbf{Z_{M}}}bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT, 𝐙𝐦subscript𝐙𝐦\mathbf{Z_{m}}bold_Z start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT, 𝐖𝐌subscript𝐖𝐌\mathbf{\mathbf{W_{M}}}bold_W start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT, 𝐖𝐦subscript𝐖𝐦\mathbf{W_{m}}bold_W start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT, 𝐗𝐚.𝐬.subscript𝐗formulae-sequence𝐚𝐬\mathbf{X_{a.s.}}bold_X start_POSTSUBSCRIPT bold_a . bold_s . end_POSTSUBSCRIPT and 𝐗𝐚.𝐚.𝐬.subscript𝐗formulae-sequence𝐚𝐚𝐬\mathbf{X_{a.a.s.}}bold_X start_POSTSUBSCRIPT bold_a . bold_a . bold_s . end_POSTSUBSCRIPT, respectively, if they exist. For more details about the specific notations in the paper, see Tables 2 and 3. As expected, we shall provide a constructive mathod to guarantee the existence of all extremal and almost (anti-)stabilizing solutions under reasonable assumptions, which generalizes the previously works. Two kinds of fixed-point iterations are proposed for finding all extremal solutions via the two auxiliary algebraic Riccati equations. Precisely, starting with X0∈ℍnsubscript𝑋0subscriptℍ𝑛X_{0}\in\mathbb{H}_{n}italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_H start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, the sequence {Xk}k=0∞superscriptsubscriptsubscript𝑋𝑘𝑘0\{X_{k}\}_{k=0}^{\infty}{ italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT generated by the following FPI of the form Xk+1=ℛ⁢(Xk),k≥0,formulae-sequencesubscript𝑋𝑘1ℛsubscript𝑋𝑘𝑘0X_{k+1}=\mathcal{R}(X_{k}),\quad k\geq 0,italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) , italic_k ≥ 0 , (3) will be used in the following content of the paper. With the help of the concept of FPI (3), it is shown in Theorem 3.1 of [6] that the existence of the maximal element of ℝ≤∩ℙsubscriptℝℙ\mathbb{R}_{\leq}\cap\mathbb{P}blackboard_R start_POSTSUBSCRIPT ≤ end_POSTSUBSCRIPT ∩ blackboard_P to the CDARE (1) can be constructed iteratively by FPI (3), which is exactly equal to 𝐗𝐌=max⁡(ℝ=∩ℙ)subscript𝐗𝐌subscriptℝℙ\mathbf{X_{M}}=\max(\mathbb{R}_{=}\cap\mathbb{P})bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT = roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P ). Analogous to Theorem 3.1 of [6], one can easily obtain the minimal element of ℝ≥∩𝔼subscriptℝ𝔼\mathbb{R}_{\geq}\cap\mathbb{E}blackboard_R start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT ∩ blackboard_E to the CDARE (1a) based on the framework of the FPI (3). Later in Section 2, we will describe in more detail. It is interesting to ask whether the minimal element 𝐗𝐦subscript𝐗𝐦\mathbf{X_{m}}bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT of ℝ=∩ℙsubscriptℝℙ\mathbb{R}_{=}\cap\mathbb{P}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P or the maximal element 𝐘𝐌subscript𝐘𝐌\mathbf{Y_{M}}bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT of ℝ=∩𝔼subscriptℝ𝔼\mathbb{R}_{=}\cap\mathbb{E}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E can be established in a unified framework of the fixed-point iteration (3)? The following example gives a counterexample to this question. Namely, there is a possibility that the CDARE (1) may has a minimal solution 𝐗𝐦subscript𝐗𝐦\mathbf{X_{m}}bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT for which 𝐗𝐦=min⁡(ℝ=∩ℙ)subscript𝐗𝐦subscriptℝℙ\mathbf{X_{m}}=\min(\mathbb{R}_{=}\cap\mathbb{P})bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT = roman_min ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P ). However, the FPI (3) with any initial X0subscript𝑋0X_{0}italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT doesn’t converge to 𝐗𝐦subscript𝐗𝐦\mathbf{X_{m}}bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT. Example 1.1. We consider a scalar CDARE (1) with n=m=1𝑛𝑚1n=m=1italic_n = italic_m = 1 of the form x=|a|2⁢x¯−|a|2⁢x¯2⁢|b|2r+|b|2⁢x¯+h=|a|2⁢x¯1+g⁢x¯+h,𝑥superscript𝑎2¯𝑥superscript𝑎2superscript¯𝑥2superscript𝑏2𝑟superscript𝑏2¯𝑥ℎsuperscript𝑎2¯𝑥1𝑔¯𝑥ℎx=|a|^{2}\bar{x}-\frac{|a|^{2}\bar{x}^{2}|b|^{2}}{r+|b|^{2}\bar{x}}+h=\frac{|a% |^{2}\bar{x}}{1+g\bar{x}}+h,italic_x = | italic_a | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT over¯ start_ARG italic_x end_ARG - divide start_ARG | italic_a | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT over¯ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT | italic_b | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_r + | italic_b | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT over¯ start_ARG italic_x end_ARG end_ARG + italic_h = divide start_ARG | italic_a | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT over¯ start_ARG italic_x end_ARG end_ARG start_ARG 1 + italic_g over¯ start_ARG italic_x end_ARG end_ARG + italic_h , where a=eπ6⁢i𝑎superscript𝑒𝜋6𝑖a=e^{\frac{\pi}{6}i}italic_a = italic_e start_POSTSUPERSCRIPT divide start_ARG italic_π end_ARG start_ARG 6 end_ARG italic_i end_POSTSUPERSCRIPT, b=eπ3⁢i𝑏superscript𝑒𝜋3𝑖b=e^{\frac{\pi}{3}i}italic_b = italic_e start_POSTSUPERSCRIPT divide start_ARG italic_π end_ARG start_ARG 3 end_ARG italic_i end_POSTSUPERSCRIPT, r=h=1𝑟ℎ1r=h=1italic_r = italic_h = 1 and thus g=|b|2/r=1𝑔superscript𝑏2𝑟1g=|b|^{2}/r=1italic_g = | italic_b | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_r = 1. Therefore, it has exactly two Hermitian solutions 𝐱𝐌=1+52>1−52=𝐱𝐦subscript𝐱𝐌152152subscript𝐱𝐦\mathbf{x_{M}}=\frac{1+\sqrt{5}}{2}>\frac{1-\sqrt{5}}{2}=\mathbf{x_{m}}bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT = divide start_ARG 1 + square-root start_ARG 5 end_ARG end_ARG start_ARG 2 end_ARG > divide start_ARG 1 - square-root start_ARG 5 end_ARG end_ARG start_ARG 2 end_ARG = bold_x start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT. Note that 𝐱𝐦subscript𝐱𝐦\mathbf{x_{m}}bold_x start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT and 𝐱𝐌subscript𝐱𝐌\mathbf{x_{M}}bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT both are elements of ℝ=∩ℙ={𝐱𝐦,𝐱𝐌}subscriptℝℙsubscript𝐱𝐦subscript𝐱𝐌\mathbb{R}_{=}\cap\mathbb{P}=\{\mathbf{x_{m}},\mathbf{x_{M}}\}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P = { bold_x start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT }. It is easy to check that the equivalent expressions of the sequence {xk}k=0∞superscriptsubscriptsubscript𝑥𝑘𝑘0\{x_{k}\}_{k=0}^{\infty}{ italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∞ end_POSTSUPERSCRIPT generated by the FPI (3) with any x0∈ℝsubscript𝑥0ℝx_{0}\in\mathbb{R}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_R can be rewritten in the explicit form xk=𝐱𝐌+5⁢(x0−𝐱𝐌)⁢tkx0−𝐱𝐦−(x0−𝐱𝐌)⁢tk,subscript𝑥𝑘subscript𝐱𝐌5subscript𝑥0subscript𝐱𝐌superscript𝑡𝑘subscript𝑥0subscript𝐱𝐦subscript𝑥0subscript𝐱𝐌superscript𝑡𝑘\displaystyle x_{k}=\mathbf{x_{M}}+\dfrac{\sqrt{5}(x_{0}-\mathbf{x_{M}})t^{k}}% {x_{0}-\mathbf{x_{m}}-(x_{0}-\mathbf{x_{M}})t^{k}},italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT + divide start_ARG square-root start_ARG 5 end_ARG ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT ) italic_t start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT end_ARG start_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - bold_x start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT - ( italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT ) italic_t start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT end_ARG , provided that xksubscript𝑥𝑘x_{k}italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT exists for k≥0𝑘0k\geq 0italic_k ≥ 0, where t=7−3⁢52∈(−1,1)𝑡735211t=\frac{7-3\sqrt{5}}{2}\in(-1,1)italic_t = divide start_ARG 7 - 3 square-root start_ARG 5 end_ARG end_ARG start_ARG 2 end_ARG ∈ ( - 1 , 1 ). It is immediate that the FPI (3) with any x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT converges to the maximal solution 𝐱𝐌subscript𝐱𝐌\mathbf{x_{M}}bold_x start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT. The above example observes that the existence of 𝐗𝐦subscript𝐗𝐦\mathbf{X_{m}}bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT (and similar to 𝐘𝐌subscript𝐘𝐌\mathbf{Y_{M}}bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT) is not ensured by a unifying treatment based the construction of FPI (3). This is the motivational thought that drives the study in this paper. As previously mentioned, the aim of the paper is to investigate the existence for the minimal element 𝐗𝐦subscript𝐗𝐦\mathbf{X_{m}}bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT of ℝ=∩ℙsubscriptℝℙ\mathbb{R}_{=}\cap\mathbb{P}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P, the maximal element 𝐘𝐌subscript𝐘𝐌\mathbf{Y_{M}}bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT of ℝ=∩𝔼subscriptℝ𝔼\mathbb{R}_{=}\cap\mathbb{E}blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E to CDARE (1), etc. Our contributions are summarized as follows. With the aid of two auxiliary equations, we have shown that CDARE in this class (1) has two extremal solutions 𝐗𝐦subscript𝐗𝐦\mathbf{X_{m}}bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT and 𝐘𝐌subscript𝐘𝐌\mathbf{Y_{M}}bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT and almost (anti-)stabilizing solutions under certain conditions. In addition, we prove that two extremal pairs (𝐗𝐌,𝐗𝐦)subscript𝐗𝐌subscript𝐗𝐦(\mathbf{X_{M}},\mathbf{X_{m}})( bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT , bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT ) and (𝐘𝐌,𝐘𝐦)subscript𝐘𝐌subscript𝐘𝐦(\mathbf{Y_{M}},\mathbf{Y_{m}})( bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT , bold_Y start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT ) cannot be simultaneously assigned to a CDARE (1). Another contribution in this paper is shown that the constructive proofs for the existence and uniqueness result of ten Hermitian solutions including 𝐙𝐌subscript𝐙𝐌\mathbf{\mathbf{Z_{M}}}bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT, 𝐙𝐦subscript𝐙𝐦\mathbf{Z_{m}}bold_Z start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT, 𝐖𝐌subscript𝐖𝐌\mathbf{\mathbf{W_{M}}}bold_W start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT and 𝐖𝐦subscript𝐖𝐦\mathbf{W_{m}}bold_W start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT to the CDARE (1) under some mild and reasonable assumptions. For the sake of clarity, Table 1 lists every iterative solution with suitable initial value coincides with extremal solution or almost (anti-)stabilizing solution, respectively, as we will discuss later. Extremal solutions General assumption FPI Convergence A∈ℂn×n,G,H∈ℍnformulae-sequence𝐴superscriptℂ𝑛𝑛𝐺𝐻subscriptℍ𝑛A\in\mathbb{C}^{n\times n},\,G,H\in\mathbb{H}_{n}italic_A ∈ blackboard_C start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT , italic_G , italic_H ∈ blackboard_H start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT region 𝐗𝐌=subscript𝐗𝐌absent\mathbf{X_{M}}=bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT = ℝ≤∩ℙ≠∅subscriptℝℙ\mathbb{R}_{\leq}\cap\mathbb{P}\neq\emptysetblackboard_R start_POSTSUBSCRIPT ≤ end_POSTSUBSCRIPT ∩ blackboard_P ≠ ∅ and Xk+1=ℛ⁢(Xk)subscript𝑋𝑘1ℛsubscript𝑋𝑘X_{k+1}=\mathcal{R}(X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 𝕌≥subscript𝕌\mathbb{U}_{\geq}blackboard_U start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT max⁡(ℝ=∩ℙ)subscriptℝℙ\max(\mathbb{R}_{=}\cap\mathbb{P})roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P ) 𝕋≠∅𝕋\mathbb{T}\neq\emptysetblackboard_T ≠ ∅ 𝐗𝐦=subscript𝐗𝐦absent\mathbf{X_{m}}=bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT = ℝ≤∩ℙ≠∅subscriptℝℙ\mathbb{R}_{\leq}\cap\mathbb{P}\neq\emptysetblackboard_R start_POSTSUBSCRIPT ≤ end_POSTSUBSCRIPT ∩ blackboard_P ≠ ∅, 𝕆≠∅𝕆\mathbb{O}\neq\emptysetblackboard_O ≠ ∅ Xk+1=−ℛ−1⁢(−Xk)subscript𝑋𝑘1superscriptℛ1subscript𝑋𝑘X_{k+1}=-{\mathcal{R}^{-1}}(-X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = - caligraphic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( - italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) −𝕍≥subscript𝕍-{\mathbb{V}}_{\geq}- blackboard_V start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT min⁡(ℝ=∩ℙ)subscriptℝℙ\min(\mathbb{R}_{=}\cap\mathbb{P})roman_min ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P ) and HA¯∈dom⁢(ℛ)¯subscript𝐻𝐴domℛ\overline{H_{A}}\in\mathrm{dom}(\mathcal{R})over¯ start_ARG italic_H start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT end_ARG ∈ roman_dom ( caligraphic_R ) 𝐘𝐌=subscript𝐘𝐌absent\mathbf{Y_{M}}=bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT = ℝ≥∩𝔼≠∅subscriptℝ𝔼\mathbb{R}_{\geq}\cap\mathbb{E}\neq\emptysetblackboard_R start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT ∩ blackboard_E ≠ ∅, 𝕆≠∅𝕆\mathbb{O}\neq\emptysetblackboard_O ≠ ∅ Xk+1=−ℛ−1⁢(−Xk)subscript𝑋𝑘1superscriptℛ1subscript𝑋𝑘X_{k+1}=-{\mathcal{R}^{-1}}(-X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = - caligraphic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( - italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) −𝕍≤subscript𝕍-{{\mathbb{V}}}_{\leq}- blackboard_V start_POSTSUBSCRIPT ≤ end_POSTSUBSCRIPT max⁡(ℝ=∩𝔼)subscriptℝ𝔼\max(\mathbb{R}_{=}\cap\mathbb{E})roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E ) and HA¯∈dom⁢(ℛ)¯subscript𝐻𝐴domℛ\overline{H_{A}}\in\mathrm{dom}(\mathcal{R})over¯ start_ARG italic_H start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT end_ARG ∈ roman_dom ( caligraphic_R ) 𝐘𝐦=subscript𝐘𝐦absent\mathbf{Y_{m}}=bold_Y start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT = ℝ≥∩𝔼≠∅subscriptℝ𝔼\mathbb{R}_{\geq}\cap\mathbb{E}\neq\emptysetblackboard_R start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT ∩ blackboard_E ≠ ∅ and Xk+1=ℛ⁢(Xk)subscript𝑋𝑘1ℛsubscript𝑋𝑘X_{k+1}=\mathcal{R}(X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 𝕌≤subscript𝕌\mathbb{U}_{\leq}blackboard_U start_POSTSUBSCRIPT ≤ end_POSTSUBSCRIPT min⁡(ℝ=∩𝔼)subscriptℝ𝔼\min(\mathbb{R}_{=}\cap\mathbb{E})roman_min ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_E ) 𝕋≠∅𝕋\mathbb{T}\neq\emptysetblackboard_T ≠ ∅ Extremal solutions General assumption FPI Convergence A∈ℂn×n,G,H∈ℕnformulae-sequence𝐴superscriptℂ𝑛𝑛𝐺𝐻subscriptℕ𝑛A\in\mathbb{C}^{n\times n},\,G,H\in\mathbb{N}_{n}italic_A ∈ blackboard_C start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT , italic_G , italic_H ∈ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT region 𝐙𝐌=subscript𝐙𝐌absent\mathbf{Z_{M}}=bold_Z start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT = 𝕋≠∅𝕋\mathbb{T}\neq\emptysetblackboard_T ≠ ∅ Xk+1=ℛ⁢(Xk)subscript𝑋𝑘1ℛsubscript𝑋𝑘X_{k+1}=\mathcal{R}(X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 𝕌≥subscript𝕌\mathbb{U}_{\geq}blackboard_U start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT max⁡(ℝ=∩ℕn)subscriptℝsubscriptℕ𝑛\max(\mathbb{R}_{=}\cap\mathbb{N}_{n})roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) 𝐙𝐦=subscript𝐙𝐦absent\mathbf{Z_{m}}=bold_Z start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT = ℝ≥∩ℕn≠∅subscriptℝsubscriptℕ𝑛\mathbb{R}_{\geq}\cap\mathbb{N}_{n}\neq\emptysetblackboard_R start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≠ ∅ Xk+1=ℛ⁢(Xk)subscript𝑋𝑘1ℛsubscript𝑋𝑘X_{k+1}=\mathcal{R}(X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) [0,H]0𝐻[0,H][ 0 , italic_H ] min⁡(ℝ=∩ℕn)subscriptℝsubscriptℕ𝑛\min(\mathbb{R}_{=}\cap\mathbb{N}_{n})roman_min ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) 𝐖𝐌=subscript𝐖𝐌absent\mathbf{W_{M}}=bold_W start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT = A𝐴Aitalic_A is nonsingular, Xk+1=−ℛ−1⁢(−Xk)subscript𝑋𝑘1superscriptℛ1subscript𝑋𝑘X_{k+1}=-{\mathcal{R}^{-1}}(-X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = - caligraphic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( - italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) [0,−H~]0~𝐻[0,-\widetilde{H}][ 0 , - over~ start_ARG italic_H end_ARG ] max(ℝ=∩−ℕn)\max(\mathbb{R}_{=}\cap-\mathbb{N}_{n})roman_max ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ - blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) and ℝ≥∩−ℕn≠∅\mathbb{R}_{\geq}\cap-\mathbb{N}_{n}\neq\emptysetblackboard_R start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT ∩ - blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≠ ∅ 𝐖𝐦=subscript𝐖𝐦absent\mathbf{W_{m}}=bold_W start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT = 𝕆≠∅𝕆\mathbb{O}\neq\emptysetblackboard_O ≠ ∅ Xk+1=−ℛ−1⁢(−Xk)subscript𝑋𝑘1superscriptℛ1subscript𝑋𝑘X_{k+1}=-{\mathcal{R}^{-1}}(-X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = - caligraphic_R start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( - italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) −𝕍¯≥subscript¯𝕍-\overline{\mathbb{V}}_{\geq}- over¯ start_ARG blackboard_V end_ARG start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT min(ℝ=∩−ℕn)\min(\mathbb{R}_{=}\cap-\mathbb{N}_{n})roman_min ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ - blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) Upper bounds General assumption FPI Convergence A∈ℂn×n,G,H∈ℍnformulae-sequence𝐴superscriptℂ𝑛𝑛𝐺𝐻subscriptℍ𝑛A\in\mathbb{C}^{n\times n},\,G,H\in\mathbb{H}_{n}italic_A ∈ blackboard_C start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT , italic_G , italic_H ∈ blackboard_H start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT region 𝐗𝐚.𝐬.=subscript𝐗formulae-sequence𝐚𝐬absent\mathbf{X_{a.s.}}=bold_X start_POSTSUBSCRIPT bold_a . bold_s . end_POSTSUBSCRIPT = 𝕋≠∅𝕋\mathbb{T}\neq\emptysetblackboard_T ≠ ∅, R>0𝑅0R>0italic_R > 0 Xk+1=ℛ⁢(Xk)subscript𝑋𝑘1ℛsubscript𝑋𝑘X_{k+1}=\mathcal{R}(X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 𝕌≥subscript𝕌\mathbb{U}_{\geq}blackboard_U start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT 𝐔⁢(ℝ=∩ℙ)𝐔subscriptℝℙ\mathbf{U}(\mathbb{R}_{=}\cap\mathbb{P})bold_U ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_P ) and H≥0𝐻0H\geq 0italic_H ≥ 0 𝐗𝐚.𝐚.𝐬.=subscript𝐗formulae-sequence𝐚𝐚𝐬absent\mathbf{X_{a.a.s.}}=bold_X start_POSTSUBSCRIPT bold_a . bold_a . bold_s . end_POSTSUBSCRIPT = ℛℛ\mathcal{R}caligraphic_R is order preserving, Xk+1=ℛ⁢(Xk)subscript𝑋𝑘1ℛsubscript𝑋𝑘X_{k+1}=\mathcal{R}(X_{k})italic_X start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = caligraphic_R ( italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) [0,H]0𝐻[0,H][ 0 , italic_H ] min⁡(ℝ=∩ℕn)subscriptℝsubscriptℕ𝑛\min(\mathbb{R}_{=}\cap\mathbb{N}_{n})roman_min ( blackboard_R start_POSTSUBSCRIPT = end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) H≥0𝐻0H\geq 0italic_H ≥ 0 and ℝ≥∩ℕn≠∅subscriptℝsubscriptℕ𝑛\mathbb{R}_{\geq}\cap\mathbb{N}_{n}\neq\emptysetblackboard_R start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT ∩ blackboard_N start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ≠ ∅ Table 1: The constructive iterations for the existence of ten specific Hermitian solutions The rest of this paper is organized as follows. In Section 2, some notations and preliminaries are first provided. Next, we review briefly recent work for which establish the existence of two extremal solutions 𝐗𝐌subscript𝐗𝐌\mathbf{X_{M}}bold_X start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT and 𝐘𝐦subscript𝐘𝐦\mathbf{Y_{m}}bold_Y start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT based on the framework of FPI (3). Especially, we introduce two auxiliary matrix equations in order to obtain another extremal solutions. Furthermore, the convergence region techniques are used to derive some iterative solutions. Section 3 devotes to the development of the dual-CDARE which guarantee the existence of two extremal solutions 𝐗𝐦subscript𝐗𝐦\mathbf{X_{m}}bold_X start_POSTSUBSCRIPT bold_m end_POSTSUBSCRIPT and 𝐘𝐌subscript𝐘𝐌\mathbf{Y_{M}}bold_Y start_POSTSUBSCRIPT bold_M end_POSTSUBSCRIPT, and the existence of another extremal solutions to CDAREs (1) will be addressed. Two subsets ℝ≤∩ℙsubscriptℝℙ\mathbb{R}_{\leq}\cap\mathbb{P}blackboard_R start_POSTSUBSCRIPT ≤ end_POSTSUBSCRIPT ∩ blackboard_P and ℝ≥∩𝔼subscriptℝ𝔼\mathbb{R}_{\geq}\cap\mathbb{E}blackboard_R start_POSTSUBSCRIPT ≥ end_POSTSUBSCRIPT ∩ blackboard_E have empty intersection will be discussed in Section 4 under reasonable hypotheses. The (almost) stabilizing solution and almost (anti-)stabilizing solution are studied in Section 5. Finally, we conclude the paper in Section 6."
https://arxiv.org/html/2411.01415v1,A gamma variate generator with shape parameter less than unity,"Algorithms for generating random numbers that follow a gamma distribution with shape parameter less than unity are proposed. Acceptance-rejection algorithms are developed, based on the generalized exponential distribution. The squeeze technique is applied to our method, and then piecewise envelope functions are further considered. The proposed methods are excellent in acceptance efficiency and promising in speed.","The gamma distribution is one of the most important probability distributions in statistics. It has applications in a broad range of fields in natural sciences, engineering, and social sciences. The probability density is defined by fGA⁢(x;α,λ)=1λα⁢Γ⁢(α)⁢xα−1⁢e−x/λ⁢(x≥0)subscript𝑓GA𝑥𝛼𝜆1superscript𝜆𝛼Γ𝛼superscript𝑥𝛼1superscript𝑒𝑥𝜆𝑥0\displaystyle f_{\rm GA}(x;\alpha,\lambda)=\frac{1}{\lambda^{\alpha}\Gamma(% \alpha)}x^{\alpha-1}e^{-x/\lambda}~{}~{}(x\geq 0)italic_f start_POSTSUBSCRIPT roman_GA end_POSTSUBSCRIPT ( italic_x ; italic_α , italic_λ ) = divide start_ARG 1 end_ARG start_ARG italic_λ start_POSTSUPERSCRIPT italic_α end_POSTSUPERSCRIPT roman_Γ ( italic_α ) end_ARG italic_x start_POSTSUPERSCRIPT italic_α - 1 end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - italic_x / italic_λ end_POSTSUPERSCRIPT ( italic_x ≥ 0 ) (1) where α𝛼\alphaitalic_α (>0absent0>0> 0) is the shape parameter, λ𝜆\lambdaitalic_λ (>0absent0>0> 0) the scale parameter, and Γ⁢(x)Γ𝑥\Gamma(x)roman_Γ ( italic_x ) the gamma function. Generating a gamma variate, a random number drawn from a gamma distribution, is an important and fundamental problem in statistical computing. Owing to its importance, various numerical algorithms have been developed since the early days of computer age [1, 3, 4, 8, 7, 9, 10, 6]. The nature of the gamma distribution is controlled by the shape parameter α𝛼\alphaitalic_α. Accordingly, we need to use different gamma variate generators. When 1<α1𝛼1<\alpha1 < italic_α, the distribution function increases from zero at x=0𝑥0x=0italic_x = 0, has a maximum at x=(α−1)⁢λ𝑥𝛼1𝜆x=(\alpha-1)\lambdaitalic_x = ( italic_α - 1 ) italic_λ, and then decays as x𝑥xitalic_x further increases. To generate a gamma variate with α>1𝛼1\alpha>1italic_α > 1, probably Marsaglia & Tsang [8]’s algorithm is one of the best, as detailed in literature [8, 10, 6]. When α=1𝛼1\alpha=1italic_α = 1, the distribution is reduced to the exponential distribution, and it is easy to generate a variate. When 0<α<10𝛼10<\alpha<10 < italic_α < 1, which is the range of our interest, the gamma distribution monotonically decreases from positive infinity at x=0𝑥0x=0italic_x = 0 to zero at x→∞→𝑥x\rightarrow\inftyitalic_x → ∞. To generate a gamma variate for 0<α<10𝛼10<\alpha<10 < italic_α < 1, various acceptance-rejection methods have been developed [1, 3, 4, 7]. Combining power-law and exponential distributions, Ahrens & Dieter [1] proposed a piecewise rejection method. Best [3] extended the Ahrens & Dieter [1] method, by adjusting a switching point and introducing a squeeze technique. Devroye [4] developed a different rejection method, based on the exponential power distribution. As of 2024, it is employed by a Python package, NumPy [2]. Using a generalized exponential distribution [5], Kundu & Gupta [7] developed a rejection method and its piecewise extensions. Finally, Tanizaki [9] developed a ratio-of-uniforms method, which works either for 0<α<10𝛼10<\alpha<10 < italic_α < 1 and for 1<α1𝛼1<\alpha1 < italic_α. In this note, we propose efficient gamma generators for 0<α<10𝛼10<\alpha<10 < italic_α < 1, advancing an earlier study [7]. In Section 2, we develop an acceptance-rejection algorithm, using the generalized exponential distribution. In Section 3, we construct fractional functions for the squeeze technique. In Section 4, we discuss piecewise envelope functions. In Section 5, we evaluate the performance of the proposed and previous methods. Section 6 contains discussions and the summary."
https://arxiv.org/html/2411.00688v1,Why do we regularise in every iterationfor imaging inverse problems?,"Regularisation is commonly used in iterative methods for solving imaging inverse problems. Many algorithms involve the evaluation of the proximal operator of the regularisation term in every iteration, leading to a significant computational overhead since such evaluation can be costly. In this context, the ProxSkip algorithm, recently proposed for federated learning purposes, emerges as an solution. It randomly skips regularisation steps, reducing the computational time of an iterative algorithm without affecting its convergence. Here we explore for the first time the efficacy of ProxSkip to a variety of imaging inverse problems and we also propose a novel PDHGSkip version. Extensive numerical results highlight the potential of these methods to accelerate computations while maintaining high-quality reconstructions. Keywords— Inverse problems, Iterative regularisation, Proximal operator, Stochastic optimimisation.","Inverse problems involve the process of estimating an unknown quantity 𝒖†∈𝕏superscript𝒖†𝕏\bm{u}^{\dagger}\in\mathbb{X}bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT ∈ blackboard_X from indirect and often noisy measurements 𝒃∈𝕐𝒃𝕐\bm{b}\in\mathbb{Y}bold_italic_b ∈ blackboard_Y obeying 𝒃=𝑨⁢𝒖†+𝜼𝒃𝑨superscript𝒖†𝜼\bm{b}=\bm{A}\bm{u}^{\dagger}+\bm{\eta}bold_italic_b = bold_italic_A bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT + bold_italic_η. Here 𝕏,𝕐𝕏𝕐\mathbb{X},\mathbb{Y}blackboard_X , blackboard_Y denote finite dimensional spaces, 𝑨:𝕏→𝕐:𝑨→𝕏𝕐\bm{A}:\mathbb{X}\rightarrow\mathbb{Y}bold_italic_A : blackboard_X → blackboard_Y is a linear forward operator, 𝒖†superscript𝒖†\bm{u}^{\dagger}bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT is the ground truth and 𝜼𝜼\bm{\eta}bold_italic_η is a random noise component. Given 𝒃𝒃\bm{b}bold_italic_b and 𝑨𝑨\bm{A}bold_italic_A, the goal is to compute an approximation 𝒖𝒖\bm{u}bold_italic_u of 𝒖†superscript𝒖†\bm{u}^{\dagger}bold_italic_u start_POSTSUPERSCRIPT † end_POSTSUPERSCRIPT. Since inverse problems are typically ill-posed, prior information about 𝒖𝒖\bm{u}bold_italic_u has to be incorporated in the form of regularisation. The solution to the inverse problem is then acquired by solving arg⁢min𝒖∈𝕏⁡𝒟⁢(𝑨⁢𝒖,𝒃)+α⁢ℛ⁢(𝒖).subscriptargmin𝒖𝕏𝒟𝑨𝒖𝒃𝛼ℛ𝒖\operatorname*{arg\,min}_{\bm{u}\in\mathbb{X}}\mathcal{D}(\bm{A}\bm{u},\bm{b})% +\alpha\mathcal{R}(\bm{u}).start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_u ∈ blackboard_X end_POSTSUBSCRIPT caligraphic_D ( bold_italic_A bold_italic_u , bold_italic_b ) + italic_α caligraphic_R ( bold_italic_u ) . (1) Here 𝒟𝒟\mathcal{D}caligraphic_D denotes the fidelity term, measuring the distance between 𝒃𝒃\bm{b}bold_italic_b and the solution 𝒙𝒙\bm{x}bold_italic_x under the operator 𝑨𝑨\bm{A}bold_italic_A. Regularisation term ℛℛ\mathcal{R}caligraphic_R promotes properties such as smoothness, sparsity, edge preservation, and low-rankness of the solution, and is weighted by a parameter α>0𝛼0\alpha>0italic_α > 0. Classical examples for ℛℛ\mathcal{R}caligraphic_R in imaging include the well-known Total Variation (TV), high order extensions [1], namely the Total Generalized Variation (TGV) [2], Total Nuclear Variation [3] and more general tensor based structure regularisation, [4]. In order to obtain a solution for (1), one employs iterative algorithms such as Gradient Descent (GD) for smooth objectives or Forward-Backward Splitting (FBS) [5] for non-smooth ones. Moreover, under the general framework min𝒙∈𝕏⁡f⁢(𝒙)+g⁢(𝒙),subscript𝒙𝕏𝑓𝒙𝑔𝒙\displaystyle\min_{\bm{x}\in\mathbb{X}}f(\bm{x})+g(\bm{x}),roman_min start_POSTSUBSCRIPT bold_italic_x ∈ blackboard_X end_POSTSUBSCRIPT italic_f ( bold_italic_x ) + italic_g ( bold_italic_x ) , (2) the Proximal Gradient Descent (PGD) algorithm, also known as Iterative Shrinkage Thresholding Algorithm (ISTA) and its accelerated version FISTA [6] are commonly used when f𝑓fitalic_f is a convex, L-smooth and g𝑔gitalic_g proper convex. Saddle-point methods such as the Primal Dual Hybrid Gradient (PDHG) [7] are commonly used for non-smooth f𝑓fitalic_f. A common property of most of these methods, see Algorithms 1–3 below, is the evaluation of proximal operators related to the regulariser in every iteration, which for τ>0𝜏0\tau>0italic_τ > 0 is defined as proxτ⁢ℛ⁢(𝒙):=arg⁢min𝒛∈𝕏⁡{12⁢‖𝒛−𝒙‖22+τ⁢ℛ⁢(𝒛)}.assignsubscriptprox𝜏ℛ𝒙subscriptargmin𝒛𝕏12superscriptsubscriptnorm𝒛𝒙22𝜏ℛ𝒛\mathrm{prox}_{\tau\mathcal{R}}(\bm{x}):=\operatorname*{arg\,min}_{\bm{z}\in% \mathbb{X}}\bigg{\{}\frac{1}{2}\|\bm{z}-\bm{x}\|_{2}^{2}+\tau\mathcal{R}(\bm{z% })\bigg{\}}.roman_prox start_POSTSUBSCRIPT italic_τ caligraphic_R end_POSTSUBSCRIPT ( bold_italic_x ) := start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ∈ blackboard_X end_POSTSUBSCRIPT { divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_z - bold_italic_x ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_τ caligraphic_R ( bold_italic_z ) } . (3) This proximal operator can have either a closed form solution, e.g., when ℛ(⋅)=∥⋅∥1\mathcal{R}(\cdot)=\|\cdot\|_{1}caligraphic_R ( ⋅ ) = ∥ ⋅ ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT or requires an inner iterative solver e.g., when ℛ⁢(𝒖)=TV⁢(𝒖)ℛ𝒖TV𝒖\mathcal{R}(\bm{u})=\mathrm{TV}(\bm{u})caligraphic_R ( bold_italic_u ) = roman_TV ( bold_italic_u ). 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0 2:Initialize: 𝒙0∈𝕏subscript𝒙0𝕏\bm{x}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,…,K−1𝑘0…𝐾1k=0,\ldots,K-1italic_k = 0 , … , italic_K - 1 do 4: 𝒙k+1=𝒙k−γ⁢∇f⁢(𝒙k)subscript𝒙𝑘1subscript𝒙𝑘𝛾∇𝑓subscript𝒙𝑘\bm{x}_{k+1}=\bm{x}_{k}-\gamma\nabla f(\bm{x}_{k})bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_γ ∇ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 5:end for Algorithm 1 GD PGD/ISTA/FBS 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0 2:Initialize: 𝒙0∈𝕏subscript𝒙0𝕏\bm{x}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,…,K−1𝑘0…𝐾1k=0,\ldots,K-1italic_k = 0 , … , italic_K - 1 do 4: 𝒙k+1=proxγ⁢g⁢(𝒙k−γ⁢∇f⁢(𝒙k))subscript𝒙𝑘1subscriptprox𝛾𝑔subscript𝒙𝑘𝛾∇𝑓subscript𝒙𝑘\bm{x}_{k+1}=\mathrm{prox}_{\gamma g}(\bm{x}_{k}-\gamma\nabla f(\bm{x}_{k}))bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT italic_γ italic_g end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_γ ∇ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) 5:end for Algorithm 2 PGD/ISTA/FBS 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0, t0=1subscript𝑡01t_{0}=1italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 2:Initialize: 𝒙0∈𝕏subscript𝒙0𝕏\bm{x}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,…,K−1𝑘0…𝐾1k=0,\ldots,K-1italic_k = 0 , … , italic_K - 1 do 4: tk+1=1+1+4⁢tk22subscript𝑡𝑘1114superscriptsubscript𝑡𝑘22t_{k+1}=\frac{1+\sqrt{1+4t_{k}^{2}}}{2}italic_t start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = divide start_ARG 1 + square-root start_ARG 1 + 4 italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG start_ARG 2 end_ARG, ak=tk−1tk+1subscript𝑎𝑘subscript𝑡𝑘1subscript𝑡𝑘1a_{k}=\frac{t_{k-1}}{t_{k+1}}italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = divide start_ARG italic_t start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT end_ARG 5: 𝒙¯k+1=𝒙k+ak⁢(𝒙k−𝒙k+1)subscript¯𝒙𝑘1subscript𝒙𝑘subscript𝑎𝑘subscript𝒙𝑘subscript𝒙𝑘1\bar{\bm{x}}_{k+1}=\bm{x}_{k}+a_{k}\left(\bm{x}_{k}-\bm{x}_{k+1}\right)over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) 6: 𝒙k+1=proxγ⁢g⁢(𝒙¯k+1−γ⁢∇f⁢(𝒙¯k+1))subscript𝒙𝑘1subscriptprox𝛾𝑔subscript¯𝒙𝑘1𝛾∇𝑓subscript¯𝒙𝑘1\bm{x}_{k+1}=\mathrm{prox}_{\gamma g}(\bar{\bm{x}}_{k+1}-\gamma\nabla f(\bar{% \bm{x}}_{k+1}))bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT italic_γ italic_g end_POSTSUBSCRIPT ( over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT - italic_γ ∇ italic_f ( over¯ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) ) 7:end for Algorithm 3 FISTA 1:Parameters: γ>0𝛾0\gamma>0italic_γ > 0, probability p>0𝑝0p>0italic_p > 0 2:Initialize: 𝒙0,𝒉0∈𝕏subscript𝒙0subscript𝒉0𝕏\bm{x}_{0},\bm{h}_{0}\in\mathbb{X}bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_italic_h start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ∈ blackboard_X 3:for k=0,1,…,K−1𝑘01…𝐾1k=0,1,\dotsc,K-1italic_k = 0 , 1 , … , italic_K - 1 do 4: 𝒙^k+1=𝒙k−γ⁢(∇f⁢(𝒙k)−𝒉k)subscript^𝒙𝑘1subscript𝒙𝑘𝛾∇𝑓subscript𝒙𝑘subscript𝒉𝑘\hat{\bm{x}}_{k+1}=\bm{x}_{k}-\gamma(\nabla f(\bm{x}_{k})-{\bm{h}_{k}})over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_γ ( ∇ italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 5: if Prob⁢(θk=1)=pProbsubscript𝜃𝑘1𝑝\mathrm{Prob}(\theta_{k}=1)=proman_Prob ( italic_θ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = 1 ) = italic_p then 6: 𝒙k+1=proxγp⁢g⁢(𝒙^k+1−γp⁢𝒉k)subscript𝒙𝑘1subscriptprox𝛾𝑝𝑔subscript^𝒙𝑘1𝛾𝑝subscript𝒉𝑘\bm{x}_{k+1}=\mathrm{prox}_{\frac{\gamma}{p}g}\bigg{(}\hat{\bm{x}}_{k+1}-\frac% {\gamma}{p}{\bm{h}_{k}}\bigg{)}bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT divide start_ARG italic_γ end_ARG start_ARG italic_p end_ARG italic_g end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT - divide start_ARG italic_γ end_ARG start_ARG italic_p end_ARG bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) 7: elsexk+1=x^k+1subscript𝑥𝑘1subscript^𝑥𝑘1\;\;\bm{x}_{k+1}=\hat{\bm{x}}_{k+1}bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT 8: end if 9: 𝒉k+1=𝒉k+pγ⁢(𝒙k+1−𝒙^k+1)subscript𝒉𝑘1subscript𝒉𝑘𝑝𝛾subscript𝒙𝑘1subscript^𝒙𝑘1{\bm{h}_{k+1}}={\bm{h}_{k}}+\frac{p}{\gamma}(\bm{x}_{k+1}-\hat{\bm{x}}_{k+1})bold_italic_h start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + divide start_ARG italic_p end_ARG start_ARG italic_γ end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) 10:end for Algorithm 4 ProxSkip The ProxSkip algorithm The possibility to skip the computation of the proximal operator in some iterations according to a probability p𝑝pitalic_p, accelerating the algorithms, without affecting convergence, was discussed in[8]. There, the ProxSkip algorithm was introduced to tackle federated learning applications which also rely on computations of expensive proximal operators. ProxSkip introduces a control variable 𝒉ksubscript𝒉𝑘\bm{h}_{k}bold_italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT, see Algorithm 4. When the proximal step is not applied, the control variable remains constant. Hence, if at iteration k𝑘kitalic_k, no proximal step has been applied previously, the accumulated error is passed to 𝒙k+1subscript𝒙𝑘1\bm{x}_{k+1}bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT without incurring an additional computational cost. If at the iteration k𝑘kitalic_k the proximal step is applied, the error is reduced and the control variable will be updated accordingly. In [8] it was shown that the ProxSkip converges provided that f𝑓fitalic_f in (2) is L-smooth and μ𝜇\muitalic_μ-strongly convex, and probability p𝑝pitalic_p satisfies p≥μ/L.𝑝𝜇𝐿p\geq\sqrt{\mu/L}.italic_p ≥ square-root start_ARG italic_μ / italic_L end_ARG . (4) In the case of equality in (4), the algorithm converges (in expectation) at a linear rate with γ=1L𝛾1𝐿\gamma=\frac{1}{L}italic_γ = divide start_ARG 1 end_ARG start_ARG italic_L end_ARG and the iteration complexity is 𝒪⁢(Lμ⁢log⁡(1ε))𝒪𝐿𝜇1𝜀\mathcal{O}(\frac{L}{\mu}\log(\frac{1}{\varepsilon}))caligraphic_O ( divide start_ARG italic_L end_ARG start_ARG italic_μ end_ARG roman_log ( divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) ). In addition, the total number of proximal evaluations (in expectation) are only 𝒪⁢(1p⁢log⁡(1ε))𝒪1𝑝1𝜀\mathcal{O}(\frac{1}{\sqrt{p}}\log(\frac{1}{\varepsilon}))caligraphic_O ( divide start_ARG 1 end_ARG start_ARG square-root start_ARG italic_p end_ARG end_ARG roman_log ( divide start_ARG 1 end_ARG start_ARG italic_ε end_ARG ) ). Our contribution Our aim is to showcase for the first time via extended numerical experiments the computational benefits of ProxSkip for a variety of imaging inverse problems, including challenging real-world tomographic applications. In particular, we show that ProxSkip can outperform the accelerated version of its non-skip analogue, namely FISTA. At the same time, we introduce a novel PDHGSkip version of the PDHG, Algorithm 6, which we motivate via numerical experiments. We anticipate that this will spark further research around developing skip-versions of a variety of proximal based algorithms used nowadays. For all our imaging experiments we consider the following optimisation problem that contains a quadratic distance term as the fidelity term, with the (isotropic) total variation as the regulariser, i.e., TV⁢(𝒖)=‖𝑫⁢𝒖‖2,1=∑|(𝑫y⁢𝒖,𝑫x⁢𝒖)|2=∑((𝑫y𝒖)2+(𝑫x𝒖)2,\mathrm{TV}(\bm{u})=\|\bm{D}\bm{u}\|_{2,1}=\sum|(\bm{D}_{y}\bm{u},\bm{D}_{x}% \bm{u})|_{2}=\sum\sqrt{((\bm{D}_{y}\bm{u})^{2}+(\bm{D}_{x}\bm{u})^{2}},roman_TV ( bold_italic_u ) = ∥ bold_italic_D bold_italic_u ∥ start_POSTSUBSCRIPT 2 , 1 end_POSTSUBSCRIPT = ∑ | ( bold_italic_D start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_u , bold_italic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT bold_italic_u ) | start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = ∑ square-root start_ARG ( ( bold_italic_D start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( bold_italic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT bold_italic_u ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , min𝒖∈𝕏⁡12⁢‖𝑨⁢𝒖−𝒃‖22+α⁢TV⁢(𝒖),subscript𝒖𝕏12superscriptsubscriptnorm𝑨𝒖𝒃22𝛼TV𝒖\min_{\bm{u}\in\mathbb{X}}\frac{1}{2}\|\bm{A}\bm{u}-\bm{b}\|_{2}^{2}+\alpha% \mathrm{TV}(\bm{u}),roman_min start_POSTSUBSCRIPT bold_italic_u ∈ blackboard_X end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG ∥ bold_italic_A bold_italic_u - bold_italic_b ∥ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_α roman_TV ( bold_italic_u ) , (5) where 𝑫⁢𝒖=(𝑫y,𝑫x)𝑫𝒖subscript𝑫𝑦subscript𝑫𝑥\bm{D}\bm{u}=(\bm{D}_{y},\bm{D}_{x})bold_italic_D bold_italic_u = ( bold_italic_D start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , bold_italic_D start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) is the finite difference operator under Neumann boundary conditions."
https://arxiv.org/html/2411.00575v1,A Semi-Discrete Optimal Transport Scheme for the 3D Incompressible Semi-Geostrophic Equations,"We describe a mesh-free three-dimensional (3D) numerical scheme for solving the incompressible semi-geostrophic equations, based on semi-discrete optimal transport techniques. These results generalise previous two-dimensional (2D) implementations. The optimal transport methods we adopt are known for their structural preservation and energy conservation qualities and achieve an excellent level of efficiency and numerical energy-conservation. We use this scheme to generate numerical simulations of an important benchmark problem. To our knowledge, this is the first fully 3D simulation of this particular cyclone, evidencing the model’s applicability to atmospheric and oceanic phenomena and offering a novel, robust tool for meteorological and oceanographic modelling.","In this paper we describe what is, to our knowledge, the first mesh-free three-dimensional numerical scheme providing simulations of incompressible semi-geostrophic atmospheric flows. In particular, we use this scheme to simulate the evolution of an isolated semi-geostrophic cyclone. The scheme we present is based on semi-discrete optimal transport techniques. A two-dimensional reduction of such flows has been previously considered and simulated in [2, 3, 5, 10]. While [2, 5] use fully discrete optimal transport with entropic regularisation, [10, 3] utilise semi-discrete optimal transport which provides a theoretical framework for the pioneering geometric method, introduced in [8]. However, to the authors knowledge, there do not exist any three-dimensional simulations, either realising the ideas of the geometric method or of any other approach. Our method to implement fully 3D computations is a substantial extension of the method presented in [10]. In addition to the novel spatial particle discretisation, achieved using semi-discrete optimal transport, we briefly investigate various explicit numerical methods to solve the temporal evolution, focusing on balancing efficiency with the relative error in the energy conservation. This exploration is novel in terms of integrating an ODE solver with an optimal transport solver but is still simple and straightforward. We then implement our schemes to provide insights into large-scale semi-geostrophic flows and into how they model the formation and development of atmospheric fronts and cyclones. In particular, we present the first full 3D simulation of the formation of an isolated cyclone, developing from the benchmark set of initial conditions given in [19]. 1.1. Background and motivation A central theme in the study of atmospheric and oceanic dynamics is the quest for models that are both mathematically and numerically tractable and that approximate accurately the fluid motion, at least within a set of specific physical constraints. An important system of equations satisfying these requirements is the semi-geostrophic (SG) system. This system, which is derived under the assumptions of hydrostatic and geostrophic balance, is a second-order accurate reduction of the full Euler system for inviscid fluids and is recognised for its effectiveness in modelling shallow, rotationally-dominated flows characterised by small Rossby numbers. This contrasts with the quasi-geostrophic (QG) limit which simplifies the dynamics by neglecting ageostrophic terms beyond the first order, whereas by retaining the second-order terms, the SG system more accurately represents ageostrophic flows and frontal dynamics [7]. We use the SG system to model the evolution of an isolated cyclone, starting from a standard initial profile proposed by Schär and Wernli [19]. Previous works, including those by Hoskins, Schär and Wernli, were constrained by the breakdown of the transformation between geostrophic and physical space in finite time, limiting the application of the SG system. In contrast, our approach leverages the optimal transport formulation, which overcomes this limitation and ensures that the transformation remains valid for all times - a fundamental advantage that extends beyond simply improving simulations. While existing studies focus on 2D computations of temperature and pressure evolution at the top and bottom boundaries, we go further by simulating the full 3D dynamics, capturing both the exterior surfaces and the interior of the domain. The base state of this initial condition is a symmetric baroclinic jet combined with a uniform barotropic shear component controlled by the shear parameter, A∈ℝ𝐴ℝA\in\mathbb{R}italic_A ∈ blackboard_R, and given in terms of the non-dimensionalised pressure by Φ¯⁢(𝐱)=12⁢(arctan⁡(x21+x3)−arctan⁡(x21−x3))−0.12⁢x2⁢x3−12⁢A⁢(x22−x32).¯Φ𝐱12arctangentsubscript𝑥21subscript𝑥3arctangentsubscript𝑥21subscript𝑥30.12subscript𝑥2subscript𝑥312𝐴superscriptsubscript𝑥22superscriptsubscript𝑥32\overline{\Phi}(\mathbf{x})=\frac{1}{2}\quantity(\arctan(\frac{x_{2}}{1+x_{3}}% )-\arctan(\frac{x_{2}}{1-x_{3}}))-0.12x_{2}x_{3}-\frac{1}{2}A\quantity(x_{2}^{% 2}-x_{3}^{2}).over¯ start_ARG roman_Φ end_ARG ( bold_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( start_ARG roman_arctan ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 1 + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_ARG end_ARG ) - roman_arctan ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 1 - italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT end_ARG end_ARG ) end_ARG ) - 0.12 italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_A ( start_ARG italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) . (1.1) Importantly, this base state is harmonic and, through the presence of arctangents, it encodes the ramp-like structure observed in the presence of weather fronts. This base state is then perturbed at the top and bottom of the domain via the perturbation function given by h⁢(x1,x2)=(1+(x10.5)2+(x20.5)2)−32−12⁢(1+(x1−10.5)2+(x20.5)2)−32−12⁢(1+(x1+10.5)2+(x20.5)2)−32.ℎsubscript𝑥1subscript𝑥2superscript1superscriptsubscript𝑥10.52superscriptsubscript𝑥20.523212superscript1superscriptsubscript𝑥110.52superscriptsubscript𝑥20.523212superscript1superscriptsubscript𝑥110.52superscriptsubscript𝑥20.5232\begin{split}h(x_{1},x_{2})&=\quantity(1+\quantity(\frac{x_{1}}{0.5})^{2}+% \quantity(\frac{x_{2}}{0.5})^{2})^{-\frac{3}{2}}-\frac{1}{2}\quantity(1+% \quantity(\frac{x_{1}-1}{0.5})^{2}+\quantity(\frac{x_{2}}{0.5})^{2})^{-\frac{3% }{2}}\\ &\quad\quad-\frac{1}{2}\quantity(1+\quantity(\frac{x_{1}+1}{0.5})^{2}+% \quantity(\frac{x_{2}}{0.5})^{2})^{-\frac{3}{2}}.\end{split}start_ROW start_CELL italic_h ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) end_CELL start_CELL = ( start_ARG 1 + ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG 0.5 end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 0.5 end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) start_POSTSUPERSCRIPT - divide start_ARG 3 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( start_ARG 1 + ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - 1 end_ARG start_ARG 0.5 end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 0.5 end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) start_POSTSUPERSCRIPT - divide start_ARG 3 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( start_ARG 1 + ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + 1 end_ARG start_ARG 0.5 end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( start_ARG divide start_ARG italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG start_ARG 0.5 end_ARG end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) start_POSTSUPERSCRIPT - divide start_ARG 3 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT . end_CELL end_ROW (1.2) This perturbation is applied only to the temperature, i.e. to the derivative of the pressure with respect to the vertical coordinate, x3subscript𝑥3x_{3}italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. The SG system was introduced by Eliassen in 1949 [11] and later revisited by Hoskins in the 1970s [13], and has played a pivotal role in particular in our understanding of large-scale (at length scales on the order of tens of kilometers) atmospheric dynamics and of atmospheric front formation. The usefulness of the SG equations has been exemplified in operational settings, such as their use as a diagnostic tool by the UK Met Office, underscoring their value in meteorological practice [6, 7]. Mathematically, this system came to prominence following the pioneering work of Brenier and Benamou [1], who showed how a formulation of the system in a different set of geostrophic variables is amenable to analysis using optimal transport techniques. This formulation is the one we use in this paper as the basis of our numerical investigation. Building on the foundational work of Eliassen and Hoskins, and continuing the research presented in [10] and iterated upon by [2, 5], our study seeks to further advance the numerical treatment of the SG model by simulating its flow in 3D geostrophic space. Specifically, we employ the damped Newton method recently developed by Kitagawa, Mérigot, and Thibert [14] to evaluate numerically the semi-discrete transport map, which is equivalent to computing an optimal Laguerre tessellation of the 3D space. This method represented a significant advancement for numerical semi-discrete optimal transport methods and it aligns with our goal to adopt a mathematically rigorous and consistent formulation of the geometric method first proposed by Cullen and Purser [8]. This approach is particularly desirable in view of its structural preservation qualities. Indeed, a crucial advantage of the semi-discrete optimal transport method over traditional finite element methods such as [20, 22] is its capacity to preserve the underlying structures of the equations being discretised. This means that solutions obtained through this method conserve total energy and simulate optimally mass-preserving flows within the fluid domain. Such characteristics are not only mathematically appealing but also crucial for the physical reliability of the simulations, especially when dealing with complex phenomena like frontal discontinuities. Our numerical solutions, thus, stand to offer an accurate conservation of total energy, mirroring the physical behaviours observed in natural fluid dynamics and thereby offering new insights into the understanding and prediction of atmospheric and oceanic phenomena. 1.2. Semi-geostrophic system in discrete geostrophic variables In this section we provide the mathematical background for the model and explain, in brief, its connection to optimal transport. Consider a compact convex set 𝒳⊂ℝ3𝒳superscriptℝ3\mathcal{X}\subset\mathbb{R}^{3}caligraphic_X ⊂ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. 𝒳𝒳\mathcal{X}caligraphic_X can be identified with the physical or fluid domain. Furthermore, consider an open set 𝒴⊆ℝ3𝒴superscriptℝ3\mathcal{Y}\subseteq\mathbb{R}^{3}caligraphic_Y ⊆ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT which can be identified with geostrophic space. The non-dimensionalised system of equations we discretise and solve numerically on the interval [0,τ]0𝜏[0,\tau][ 0 , italic_τ ] for τ∈ℝ+𝜏subscriptℝ\tau\in\mathbb{R}_{+}italic_τ ∈ blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT is given in [1] : ∂tαt+div⁢(αt⁢𝐯⁢[αt])=0,𝐯⁢[αt]=J⁢(id−T−1),J=(0−10100000),\begin{split}&\partial_{t}\alpha_{t}+\mathrm{div}(\alpha_{t}\mathbf{v}[\alpha_% {t}])=0,\\ &\mathbf{v}[\alpha_{t}]=J(\mathrm{id}-T^{-1}),\qquad J=\matrixquantity(0\hfil&% -1&0\\ 1&0&0\\ 0&0&0),\end{split}start_ROW start_CELL end_CELL start_CELL ∂ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + roman_div ( italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_v [ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) = 0 , end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL bold_v [ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = italic_J ( roman_id - italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) , italic_J = ( start_ARG start_ARG start_ROW start_CELL 0 end_CELL start_CELL - 1 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW start_ROW start_CELL 0 end_CELL start_CELL 0 end_CELL start_CELL 0 end_CELL end_ROW end_ARG end_ARG ) , end_CELL end_ROW (1.3) where α:[0,τ]→𝒫⁢(𝒴):𝛼→0𝜏𝒫𝒴\alpha:[0,\tau]\to\mathscr{P}(\mathcal{Y})italic_α : [ 0 , italic_τ ] → script_P ( caligraphic_Y ) is a probability measure-valued map such that αt=α⁢(t)∈𝒫⁢(𝒴)subscript𝛼𝑡𝛼𝑡𝒫𝒴\alpha_{t}=\alpha(t)\in\mathscr{P}(\mathcal{Y})italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_α ( italic_t ) ∈ script_P ( caligraphic_Y ) and T:𝒳→𝒴:𝑇→𝒳𝒴T:\mathcal{X}\to\mathcal{Y}italic_T : caligraphic_X → caligraphic_Y is the optimal transport map from 𝟙𝒳subscript1𝒳\mathbbm{1}_{\mathcal{X}}blackboard_1 start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT (the normalised Lebesgue measure on 𝒳𝒳\mathcal{X}caligraphic_X) to αtsubscript𝛼𝑡\alpha_{t}italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, the inverse of the potential vorticity. For the incompressible semi-geostrophic system the transport is required to be optimal with respect to a quadratic cost. In this case the optimal transport map is defined as T=arg⁢minT:𝒳→𝒴T#⁢𝟙𝒳=αt⁢∫𝒳‖𝐱−T⁢(𝐱)‖2⁢d𝐱,𝑇subscriptargmin:𝑇→𝒳𝒴subscript𝑇#subscript1𝒳subscript𝛼𝑡subscript𝒳superscriptnorm𝐱𝑇𝐱2𝐱\displaystyle T=\operatorname*{arg\,min}_{\begin{subarray}{c}T:\mathcal{X}\to% \mathcal{Y}\\ T_{\#}\mathbbm{1}_{\mathcal{X}}=\alpha_{t}\end{subarray}}\int_{\mathcal{X}}\|% \mathbf{x}-T(\mathbf{x})\|^{2}\,\differential\mathbf{x},italic_T = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT start_ARG start_ROW start_CELL italic_T : caligraphic_X → caligraphic_Y end_CELL end_ROW start_ROW start_CELL italic_T start_POSTSUBSCRIPT # end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT = italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ∫ start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT ∥ bold_x - italic_T ( bold_x ) ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_DIFFOP roman_d end_DIFFOP bold_x , where T#⁢𝟙𝒳subscript𝑇#subscript1𝒳T_{\#}\mathbbm{1}_{\mathcal{X}}italic_T start_POSTSUBSCRIPT # end_POSTSUBSCRIPT blackboard_1 start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT is the pushforward of the probability measure 𝟙𝒳subscript1𝒳\mathbbm{1}_{\mathcal{X}}blackboard_1 start_POSTSUBSCRIPT caligraphic_X end_POSTSUBSCRIPT under the map T𝑇Titalic_T [18]. This is the case that has been studied most extensively, and for which there is a fairly exhaustive theory [18]. In our particle discretisation of this problem the target measure, αtsubscript𝛼𝑡\alpha_{t}italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, becomes the discrete probability measure αtN=1N⁢∑iNδ𝐳i⁢(t)subscriptsuperscript𝛼𝑁𝑡1𝑁superscriptsubscript𝑖𝑁subscript𝛿superscript𝐳𝑖𝑡\alpha^{N}_{t}=\frac{1}{N}\sum_{i}^{N}\delta_{\mathbf{z}^{i}(t)}italic_α start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_δ start_POSTSUBSCRIPT bold_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( italic_t ) end_POSTSUBSCRIPT, where 𝐳isuperscript𝐳𝑖\mathbf{z}^{i}bold_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT are points in 𝒴⊂ℝ3𝒴superscriptℝ3\mathcal{Y}\subset\mathbb{R}^{3}caligraphic_Y ⊂ blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT. This yields an ODE which is given formally by 𝐳˙isuperscript˙𝐳𝑖\displaystyle\dot{\mathbf{z}}^{i}over˙ start_ARG bold_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT =𝐯⁢[αtN]⁢(𝐳i),absent𝐯delimited-[]superscriptsubscript𝛼𝑡𝑁superscript𝐳𝑖\displaystyle=\mathbf{v}[\alpha_{t}^{N}](\mathbf{z}^{i}),= bold_v [ italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ] ( bold_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) , 𝐯⁢[αtN]⁢(𝐳i)𝐯superscriptsubscript𝛼𝑡𝑁superscript𝐳𝑖\displaystyle\mathbf{v}\quantity[\alpha_{t}^{N}](\mathbf{z}^{i})bold_v [ start_ARG italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_ARG ] ( bold_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) =J⁢(𝐳i−T−1⁢(𝐳i)),absent𝐽superscript𝐳𝑖superscript𝑇1superscript𝐳𝑖\displaystyle=J(\mathbf{z}^{i}-T^{-1}(\mathbf{z}^{i})),= italic_J ( bold_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT - italic_T start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) ) , (1.4) where T:𝒳→𝒴:𝑇→𝒳𝒴T:\mathcal{X}\to\mathcal{Y}italic_T : caligraphic_X → caligraphic_Y is now the optimal transport map from 𝟙Xsubscript1𝑋\mathbbm{1}_{X}blackboard_1 start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT to the discrete measure αtNsuperscriptsubscript𝛼𝑡𝑁\alpha_{t}^{N}italic_α start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT. Note that because the target measure is discrete, the problem is a semi-discrete optimal transport problem, with respect to the quadratic cost. It is well known that its solution must be of the form T=1N⁢∑i=1N𝟙Li𝑇1𝑁superscriptsubscript𝑖1𝑁subscript1superscript𝐿𝑖T=\frac{1}{N}\sum_{i=1}^{N}{\mathbbm{1}}_{L^{i}}italic_T = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT blackboard_1 start_POSTSUBSCRIPT italic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, where Lisuperscript𝐿𝑖L^{i}italic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT are cells forming a covering of the space 𝒳𝒳\mathcal{X}caligraphic_X. Rigorously, the i𝑖iitalic_ith Laguerre cell is defined, for i∈{1,…,N}𝑖1…𝑁i\in\quantity{1,\ldots,N}italic_i ∈ { start_ARG 1 , … , italic_N end_ARG }, by Li={𝐱∈𝒳:‖𝐱−𝐳i‖2−wi≤‖𝐱−𝐳j‖2−wj∀j∈{1,…,N}},superscript𝐿𝑖:𝐱𝒳formulae-sequencesuperscriptnorm𝐱superscript𝐳𝑖2superscript𝑤𝑖superscriptnorm𝐱superscript𝐳𝑗2superscript𝑤𝑗for-all𝑗1…𝑁\displaystyle L^{i}=\quantity{\mathbf{x}\in\mathcal{X}:\|\mathbf{x}-\mathbf{z}% ^{i}\|^{2}-w^{i}\leq\|\mathbf{x}-\mathbf{z}^{j}\|^{2}-w^{j}\quad\forall\,j\in% \quantity{1,\ldots,N}},italic_L start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = { start_ARG bold_x ∈ caligraphic_X : ∥ bold_x - bold_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ≤ ∥ bold_x - bold_z start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ∥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_w start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ∀ italic_j ∈ { start_ARG 1 , … , italic_N end_ARG } end_ARG } ,"
https://arxiv.org/html/2411.00487v1,Comparative Analysis of Polynomials with Their Computational Costs,"In this article, we explore the effectiveness of two polynomial methods in solving non-linear time and space fractional partial differential equations. We first outline the general methodology and then apply it to five distinct experiments. The proposed method, noted for its simplicity, demonstrates a high degree of accuracy. Comparative analysis with existing techniques reveals that our approach yields more precise solutions. The results, presented through graphs and tables, indicate that He’s and Daftardar-Jafari polynomials significantly enhance accuracy. Additionally, we provide an in-depth discussion on the computational costs associated with these polynomials. Due to its straightforward implementation, proposed method can be extended for application to a broader range of problems.","Fractional Calculus (FC) is a mathematical field that extends the traditional concepts of integrals and derivatives to non-integer orders. This area of study has gained significant attention due to its wide-ranging applications in modeling various real-world phenomena. For instance, FC has been effectively applied to model epidemic spreading, providing insights into how diseases propagate through populations (see [1]). Additionally, it has been utilized in the analysis of earthquakes, offering a more nuanced understanding of seismic activities. In the biomedical and biological fields, fractional calculus has facilitated the development of models that capture complex biological processes and interactions. The growing interest in FC has led researchers to formulate sophisticated mathematical models that incorporate fractional integrals and derivatives. These models often address complex systems where traditional integer-order derivatives fall short. To analyze and solve these fractional models, researchers have employed innovative techniques from various disciplines, including robotic technology, genetic algorithms, and applications in physics, economics, and finance (see [2, 3, 4]). As the field has evolved, a notable advancement is the development of fractional partial differential equations (FPDEs). These equations enhance modeling precision in several domains, including mechanics, plasma physics, finance, biomathematics, and fluid mechanics. The versatility of FPDEs has made them invaluable in applied sciences, allowing for more accurate representations of dynamic systems (see [5]). Fractional differential equations (FDEs) have emerged as powerful tools for analyzing and modeling the behavior of various physical phenomena. Their capability to capture complex dynamics makes them particularly valuable in contexts where traditional integer-order differential equations may fall short. This increased accuracy is well-documented in the literature, where FDEs are frequently employed to provide more precise models of diverse phenomena (see Section 2, Reference [6]). One notable application of FDEs is in the study of nonlinear oscillations. Researchers have found that incorporating fractional derivatives leads to a better understanding of oscillatory behavior in systems that exhibit memory or hereditary properties (see [7], [5]). Similarly, the non-linear Korteweg-De Vries (KdV) equation, which models wave propagation in shallow water, has been effectively adapted to fractional forms, enhancing its predictive capabilities (see [8]). FDEs have also been applied to model advection-diffusion processes in two-dimensional semiconductor systems, where the inclusion of fractional derivatives allows for a more nuanced representation of charge carrier dynamics (see [9]). Additionally, these equations have been utilized to analyze the dynamics of national soccer leagues, where they help in understanding complex interactions and patterns within the league’s performance metrics. Moreover, space-fractional diffusion processes have been extensively studied using FDEs, providing insights into phenomena such as anomalous diffusion, where particles spread in a non-standard manner across a medium (see [10, 11, 12, 13, 14]). The ability of FDEs to model these intricate behaviors highlights their significance in both theoretical and applied research across various scientific fields. Recently, mathematicians have shown a growing interest in developing and applying a variety of efficient numerical and analytical techniques to solve fractional partial differential equations (FPDEs) and their systems. This surge in interest reflects the complexity and significance of FPDEs in modeling real-world phenomena. Several well-established techniques have been created or implemented for this purpose. Among them, the Laplace Adomian Decomposition Method (LADM) [15] and the Adomian Decomposition Method (ADM) are notable for their effectiveness in breaking down complex equations into simpler components. The Finite Difference Method [16] is widely used for its straightforward approach to discretizing differential equations, making it suitable for numerical solutions. Other innovative methods include Feng’s First Integral Method [17], which offers a unique perspective in solving FPDEs, and the Homotopy Analysis Method (HAM) [18], which provides a systematic way to construct solutions. The Homotopy Perturbation Transform Method (HPTM) [19] and Meshless Method (MM) [20] are also significant for their flexibility in handling various types of equations. Moreover, the Aboodh Transform Iterative Method [21] and the Modified Homotopy Perturbation Method (MHPM) [22] have been developed to enhance the accuracy of solutions. Techniques such as the Multiple Exponential Function Algorithms [23], Shifted Chebyshev-Gauss-Lobatto Collocation [24], and the Operational Matrix Method [25] further exemplify the breadth of methods available for tackling FPDEs. The Variational Iteration Method (VIM) [26] also plays a crucial role in providing approximate solutions to these complex equations. Overall, while obtaining analytical and approximate solutions for non-linear FPDEs and their systems can be a challenging endeavor, it is a fascinating area of research that attracts mathematicians. Researchers are particularly keen to discover accurate and effective techniques for solving FPDEs, as the analysis of fractional solutions significantly contributes to understanding the actual dynamics of various physical phenomena. This growing interest in FPDEs has led to increased popularity and numerous studies within the mathematical community [27, 28, 29, 30, 31, 32, 33, 34]. Controlling the non-linear components of differential equations remains a significant challenge for researchers. Non-linearities often introduce complex behaviors that can complicate both analysis and solution processes. These challenges stem from the intricate interactions within non-linear systems, which can lead to phenomena such as chaos, bifurcations, and multiple equilibria. As a result, developing effective methods to manage and control these non-linear parts is crucial for advancing our understanding and application of differential equations in various fields. Researchers continue to explore innovative approaches and techniques to tackle these complexities, striving for more robust solutions in both theoretical and practical contexts. In [35], one said that most of the non-linear systems are impossible to solve analytically. In solving non-linear differential equations, the decomposition and iterative handling of non-linear terms are crucial for achieving accurate and efficient solutions. Various polynomial methods have been developed to address this challenge, including Daftardar-Jafari polynomials, Adomian polynomials, and He’s polynomials. Each of these methods offers a unique approach to decomposing non-linear terms, facilitating the iterative process required for solving complex differential equations. Here, we provide an overview of these polynomial techniques and their applications. Daftardar-Jafari polynomials are employed in iterative methods to break down non-linear terms systematically. This approach enhances the efficiency of solving non-linear functional equations. The polynomials are defined recursively, allowing for the iterative approximation of solutions (see ref [36, 37]). Adomian polynomials are a cornerstone of the Adomian Decomposition Method (ADM), which is extensively used for solving non-linear differential equations. These polynomials decompose the non-linear operator into a series, simplifying the solution process (see ref. [38, 39]). He’s polynomials are integral to variational iteration methods and other iterative techniques. They facilitate the management of non-linear terms, improving the convergence and accuracy of the solutions for non-linear differential equations (see ref. [40, 11, 13, 7]). The polynomial methods of Daftardar-Jafari, Adomian, and He’s provide robust frameworks for handling non-linear terms in differential equations. Their recursive and systematic approaches enable iterative solutions that are both accurate and computationally efficient. These methods are foundational tools in the mathematical toolbox for solving a wide range of non-linear fractional order integro-differential equations and non-linear fractional order partial differential equations, with applications spanning various scientific and engineering disciplines (see, recent results in [41, 42, 43, 44, 45]). This research article tackles non-linear fractional-order PDEs using the Iterative Laplace Transform Method (ILTM) along with He’s and D-J polynomials. This approach is notable for its higher accuracy and robustness when applied to fractional differential equations. The study carefully examines the effects of these polynomials and presents the results using graphs and tables. ILTM is an advanced technique that iteratively applies the Laplace transform to solve differential equations. The iterative nature of this method allows for improved convergence and accuracy, making it suitable for non-linear and fractional-order equations. By transforming the problem into the Laplace domain, the differential equation is converted into an algebraic equation, which is often simpler to solve. He’s polynomials and D-J polynomials are utilized to approximate the solutions of the transformed equations. These polynomials are instrumental in breaking down complex non-linear terms into manageable forms, facilitating the iterative process of ILTM. The choice of polynomials can significantly influence the convergence and accuracy of the solution, which is why their effects are closely examined in the study. The solutions obtained through ILTM are analyzed using graphs and tables. Graphical analysis helps visually assess the solution’s behavior over time and space, while tabular data provides quantitative insights into the accuracy and convergence rates. This dual approach ensures a comprehensive evaluation of the method’s performance. The success of ILTM in solving the fractional-order Advection equation paves the way for its application to other fractional problems. For instance, fractional diffusion equations, fractional wave equations, and fractional Schrödinger equations could benefit from this approach. The methodology can be adapted by applying similar polynomial approximations and iterative procedures under various integral transforms. This research shows a strong and accurate method for solving non-linear fractional-order PDEs using ILTM and polynomial approximations. The method’s potential to be extended to other fractional problems highlights its importance and opens up many opportunities for future research and applications. The rest of the paper is organised as follows. Section 2 presents an overview of fundamental definitions and the methodology utilized in the study. In Section 3, numerical problems are addressed to illustrate the method’s applications. Section 4 discusses the obtained results, providing detailed analysis and discussion. Finally, Section 5 concludes the paper by summarizing the main findings and proposing directions for future research."
https://arxiv.org/html/2411.00463v1,The learned range test method for the inverse inclusion problem,"We consider the inverse problem consisting of the reconstruction of an inclusion B𝐵Bitalic_B contained in a bounded domain Ω⊂ℝdΩsuperscriptℝ𝑑\Omega\subset\mathbb{R}^{d}roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT from a single pair of Cauchy data (u|∂Ω,∂νu|∂Ω)evaluated-at𝑢Ωevaluated-atsubscript𝜈𝑢Ω(u|_{\partial\Omega},\partial_{\nu}u|_{\partial\Omega})( italic_u | start_POSTSUBSCRIPT ∂ roman_Ω end_POSTSUBSCRIPT , ∂ start_POSTSUBSCRIPT italic_ν end_POSTSUBSCRIPT italic_u | start_POSTSUBSCRIPT ∂ roman_Ω end_POSTSUBSCRIPT ), where Δ⁢u=0Δ𝑢0\Delta u=0roman_Δ italic_u = 0 in Ω∖B¯Ω¯𝐵\Omega\setminus\overline{B}roman_Ω ∖ over¯ start_ARG italic_B end_ARG and u=0𝑢0u=0italic_u = 0 on ∂B𝐵\partial B∂ italic_B. We show that the reconstruction algorithm based on the range test, a domain sampling method, can be written as a neural network with a specific architecture. We propose to learn the weights of this network in the framework of supervised learning, and to combine it with a pre-trained classifier, with the purpose of distinguishing the inclusions based on their distance from the boundary. The numerical simulations show that this learned range test method provides accurate and stable reconstructions of polygonal inclusions. Furthermore, the results are superior to those obtained with the standard range test method (without learning) and with an end-to-end fully connected deep neural network, a purely data-driven method.","Electrostatic and thermal imaging are important techniques with various applications in scientific and industrial disciplines, including non-destructive testing and evaluation, remote sensing, ultrasound imaging and so on; see, for instance, [1, 33, 41, 8] and the references therein. Roughly speaking, these techniques aim to recover the unknown boundary impedance or an abnormal inclusion (for example, cavity or crack) inside the conducting medium, from the knowledge of voltage and current measurements or of the temperature on the boundary of the medium. In mathematical terms, it can be modeled as an inverse boundary value problem for the Laplace equation. In this work, we focus on the reconstruction of a perfectly insulated inclusion inside a homogeneous medium. Specifically, let Ω⊂ℝdΩsuperscriptℝ𝑑\Omega\subset\mathbb{R}^{d}roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT be a bounded domain. Assume that there is a perfectly insulated inclusion B⊆Ω𝐵ΩB\subseteq\Omegaitalic_B ⊆ roman_Ω. The electric potential u∈H1⁢(Ω∖B¯)𝑢superscript𝐻1Ω¯𝐵u\in H^{1}(\Omega\setminus\overline{B})italic_u ∈ italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( roman_Ω ∖ over¯ start_ARG italic_B end_ARG ) satisfies {Δ⁢u=0in⁢Ω∖B¯,u=gon⁢∂Ω,u=0on⁢∂B,casesΔ𝑢0inΩ¯𝐵𝑢𝑔onΩ𝑢0on𝐵\begin{cases}\Delta u=0&\mathrm{in}\ \Omega\setminus\overline{B},\\ u=g&\mathrm{on}\ \partial\Omega,\\ u=0&\mathrm{on}\ \partial B,\end{cases}{ start_ROW start_CELL roman_Δ italic_u = 0 end_CELL start_CELL roman_in roman_Ω ∖ over¯ start_ARG italic_B end_ARG , end_CELL end_ROW start_ROW start_CELL italic_u = italic_g end_CELL start_CELL roman_on ∂ roman_Ω , end_CELL end_ROW start_ROW start_CELL italic_u = 0 end_CELL start_CELL roman_on ∂ italic_B , end_CELL end_ROW with a prescribed boundary condition g∈H12⁢(∂Ω)𝑔superscript𝐻12Ωg\in H^{\frac{1}{2}}(\partial\Omega)italic_g ∈ italic_H start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT ( ∂ roman_Ω ). Our goal is to recover the inclusion B𝐵Bitalic_B from a single measurement {g,∂νu|∂Ω}𝑔evaluated-atsubscript𝜈𝑢Ω\{g,\partial_{\nu}u|_{\partial\Omega}\}{ italic_g , ∂ start_POSTSUBSCRIPT italic_ν end_POSTSUBSCRIPT italic_u | start_POSTSUBSCRIPT ∂ roman_Ω end_POSTSUBSCRIPT }, which is a nonlinear and ill-posed inverse problem. This is a variation of Calderón’s inverse problem [13, 60], modeling electrical impedance tomography, in which the unknown is not a simple inclusion but the full conductivity density, and the data consists of all Cauchy pairs on ∂ΩΩ\partial\Omega∂ roman_Ω. In recent years, several theoretical and numerical works have provided many insights into this inverse problem; see §§\S§2.2 below for a short literature review. In this work, we are particularly interested in a non-iterative method, the range test (RT). This is a qualitative method, or more precisely, a domain sampling method [20, 19]. This method was initially introduced for the inverse acoustic scattering problem [53], and was later extended to the Oseen problem [62], for which the convergence of the RT is studied in detail. Recently, the RT was employed to solve the inverse boundary value problem for the heat equation [57]. For additional applications of the RT we refer the readers to [37, 51, 6]. In essence, the RT tackles the corresponding inverse problem by detecting the range of a specific operator. In other words, the philosophy behind the RT is that the presence of an anomaly within the conductor will affect the regularity of the solution and therefore the range of an integral operator. The RT has been recently extended to the inverse boundary value problem for the Laplace equation discussed above [46, 56]. A sophisticated indicator function based on the range of a boundary integral operator relative to a prescribed test domain allows us to establish whether the test domain contains the inclusion. Hence, one can detect the inclusion by taking the intersection of all the test domains containing the inclusion. The RT method inherits the advantages of qualitative approaches: it can perform the reconstruction with just one boundary measurement and requires minimal a priori information about the inclusion. However, it has three notable limitations. First, a careful selection of parameters, which often vary with the inclusion, is needed for the numerical implementation of the RT method, making it impractical. Second, the reconstruction it provides is relatively rough. Lastly, the method becomes computationally intensive when applied to large sampling domains. Over the past decade, deep learning methods have attracted extensive attention because of their salient success in solving inverse problems; see, for example, [23, 61, 59, 54, 15]. Directly solving inverse problems by constructing an end-to-end neural network and ignoring the physical model loses interpretability and often leads to less effective reconstruction methods [9, 47]. As a result, the integration of conventional methods with deep learning methods has become a popular direction in recent years, also in the context of qualitative methods. For example, in [30], the authors propose a deep learning method for electrical impedance tomography based on the conventional direct sampling method. They employ fully connected neural networks and convolutional neural networks to approximate the indicator function of the inclusion. The study [43] integrates the orthogonality sampling method with deep neural networks to reconstruct the geometry of a penetrable object from scattering measurements generated by a single incident wave. In [50], the authors propose a sampling-based deep learning method to tackle the inverse medium scattering problem. Specifically, they integrate the results generated by the direct sampling method with a U-Net to learn the relationship between the indicator function and the true resolution of the target. In [45], the authors consider the inverse inhomogeneous medium problem using a learning-based iterative algorithm. Briefly speaking, they make use of a deep neural network to learn the a priori information on the shape of the unknown scatterer, and then refine the reconstruction by using the iteratively regularized Gauss-Newton method. In this paper, to overcome the weaknesses of the RT, we propose an RT-based deep learning method, denoted as the learned range test (LRT), to realize the reconstruction of the inclusion with high accuracy and efficiency. The main contributions of our study are the following. • We show that the reconstruction algorithm based on the RT described in [56] can be expressed as a neural network with a specific architecture. The weights of this NN can then be learned with a classical supervised learning approach. • We illustrate that it is impractical to expect that one universal network with this architecture can provide accurate reconstructions for all inclusions. As a remedy, we propose to combine several neural networks with this architecture with a pre-trained classifier, in order to treat in a different way the inclusions that have a different distance from the boundary. • Various numerical experiments are conducted to verify the effectiveness of the LRT. The numerical results suggest that the LRT has the capacity to reconstruct inclusions with high accuracy and efficiency from one measurement, especially if compared with the standard RT method or with a fully connected end-to-end deep neural network. The remainder of this paper is structured as follows. The mathematical formulation of our inverse problem is stated in Section 2, where a literature review concerning this problem is also included. In Section 3, we briefly revisit the RT from the theoretical and numerical perspectives. Section 4 is devoted to the LRT method, where the architecture of the neural network and the three-step strategy are described. Various numerical experiments are carried out in Section 5. In Section 6, we provide some concluding remarks. Finally, some details on the computational aspects, with a particular focus on the training of the neural networks of this work, are discussed in Appendix A."
https://arxiv.org/html/2411.00442v1,FPRev: Revealing the Order of Floating-Point Summation by Numerical Testing,"The order of floating-point summation is a key factor in numerical reproducibility. However, this critical information is generally unspecified and unknown for most summation-based functions in numerical libraries, making it challenging to migrate them to new environments reproducibly. This paper presents novel, non-intrusive, testing-based algorithms that can reveal the order of floating-point summation by treating functions as callable black boxes. By constructing well-designed input that can cause the swamping phenomenon of floating-point addition, we can infer the order of summation from the output. We introduce FPRev, a tool that implements these algorithms, and validate its efficiency through extensive experiments with popular numerical libraries on various CPUs and GPUs (including those with Tensor Cores). FPRev reveals the varying summation orders across different libraries and devices, and outperforms other methods in terms of time complexity. The source code of FPRev is at https://github.com/microsoft/RepDL/tree/main/tools/FPRev.","With the rapid evolution of heterogeneous hardware and diverse software stacks, the lack of reproducibility in numerical computing has become a recognized problem (he_using_2001, ; villa_effects_2009, ; taufer_improving_2010, ; bailey_facilitating_2016, ; chen_towards_2022, ; liu_reproducibility_2022, ). The same numeric function can produce varying results when software is migrated to new hardware or when numerical libraries are updated. Non-reproducible results pose significant challenges in scientific research, software engineering, deep learning, and applications that rely on numerical models for decision making. These challenges undermine the credibility of findings, hinder progress by obscuring errors in programs, and can lead to incorrect conclusions or suboptimal decisions, ultimately affecting the reliability and trustworthiness. A primary cause of numerical non-reproducibility is discrepancies in the order of floating-point summation (robey_search_2011, ; demmel_fast_2013, ; arteaga_designing_2014, ; demmel_parallel_2015, ; collange_numerical_2015, ). The result of floating-point summation depends on the order of computation due to the non-associative nature of floating-point addition. For example, as shown in Table 1, the sum of 0.1, 0.2 and 0.3 is order-dependent, because (0.1+0.2)+0.3≠0.1+(0.2+0.3)0.10.20.30.10.20.3(0.1+0.2)+0.3\neq 0.1+(0.2+0.3)( 0.1 + 0.2 ) + 0.3 ≠ 0.1 + ( 0.2 + 0.3 ) in IEEE-754 (noauthor_ieee_2019, ) binary64 (also known as float64). There is no general specification that stipulates the order of floating-point summation. Consequently, without well-defined specifications, numerical libraries usually compute floating-point summation in various orders in different environments, leading to inconsistent numerical output. Table 1. Examples of the non-associative nature of float64 addition. (0.1+0.2)+0.30.10.20.3(0.1+0.2)+0.3( 0.1 + 0.2 ) + 0.3 0.1+(0.2+0.3)0.10.20.30.1+(0.2+0.3)0.1 + ( 0.2 + 0.3 ) (−260+260)+1superscript260superscript2601(-2^{60}+2^{60})+1( - 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT ) + 1 −260+(260+1)superscript260superscript2601-2^{60}+(2^{60}+1)- 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + ( 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + 1 ) Decimal 0.60000000000000008882 0.59999999999999997780 1 0 Hexadecimal 0x1.3333333333334p-1 0x1.3333333333333p-1 0x1p0 0x0p0 Knowing the order of summation is critical for reproducibility. Consider a function based on floating-point summation (e.g., matrix multiplication) that produces inconsistent output in new environments, which is undesirable. To fix the issue, the order of summation must be known. This information can serve as a valuable guide and constraint in determining the appropriate order of summation when migrating the function to the new environments. However, the information is virtually unknown. Existing numerical libraries, such as Intel MKL (mkl, ), OpenBLAS (OpenBLAS, ), and NVIDIA cuBLAS (cublas, ), do not specify this information in their documentation, and there is no specialized tool to reveal the information. Revealing the order of summation is a challenging task. For example, people can manually determine the order by analyzing the static source code, but many libraries or hardware implementations are black-box, which limits the static approach. Even if the function’s trace is obtained and analyzed, no tool can automatically generate the computational graph of the summation. We build a non-intrusive, testing-based tool called FPRev to reveal the order of summation. FPRev treats the summation-based function as a callable black box, generates specialized test cases, and infers the order of summation from the function’s output. FPRev provides two versions of algorithms: FPRev-basic and FPRev-advanced. Both leverage the swamping phenomenon in floating-point addition to generate well-designed numerical input. When two floating-point numbers differing by many orders of magnitude are added, the smaller number is swamped and does not contribute to the sum. For example, 260+1superscript26012^{60}+12 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + 1 equals 260superscript2602^{60}2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT when using float64. Based on the phenomenon, we can utilize large numbers as masks to hide certain summands from the sum. In FPRev-basic, we first generate several “masked all-one arrays”. Each array is predominantly composed of the floating-point number 1.0, with exactly two non-one elements: M𝑀Mitalic_M and −M𝑀-M- italic_M. Here, M𝑀Mitalic_M represents a large positive number that can cause the swamping phenomenon in floating-point arithmetic. Specifically, let n𝑛nitalic_n denote the number of summands. M𝑀Mitalic_M satisfies ±M+μ=±Mplus-or-minus𝑀𝜇plus-or-minus𝑀\pm M+\mu=\pm M± italic_M + italic_μ = ± italic_M for all non-negative number μ<n𝜇𝑛\mu<nitalic_μ < italic_n. Next, we call the tested function multiple times with different masked all-one arrays. Each output reveals how many summands are swamped by ±Mplus-or-minus𝑀\pm M± italic_M during summation and how many are not. This information relates to the structure of the computational graph. The graph is a full binary tree that accurately depicts the order of operations. Each output equals the number of leaf nodes out of the subtree rooted at the lowest common ancestor of the nodes corresponding to the indexes of M𝑀Mitalic_M and −M𝑀-M- italic_M. For example, Table 2 demonstrate the information for Algorithm 1, whose computational graph is Figure 1. Finally, we use the output information to construct the summation tree. This involves a tree-algorithm problem: how to construct a full binary tree given {(i,j,li,j):0≤i<j<n}conditional-set𝑖𝑗superscript𝑙𝑖𝑗0𝑖𝑗𝑛\{(i,j,l^{i,j}):0\leq i<j<n\}{ ( italic_i , italic_j , italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT ) : 0 ≤ italic_i < italic_j < italic_n }, where li,jsuperscript𝑙𝑖𝑗l^{i,j}italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT denotes how many leaf nodes are in the subtree rooted at the lowest common ancestor of the i𝑖iitalic_i-th and j𝑗jitalic_j-th leaf nodes. We construct the tree in a bottom-up (leaf-to-root) way. We begin by constructing subtrees with two leaf nodes (corresponding to li,j=2superscript𝑙𝑖𝑗2l^{i,j}=2italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT = 2). Subsequently, larger subtrees (corresponding to next larger li,jsuperscript𝑙𝑖𝑗l^{i,j}italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT) are built from existing subtrees or isolate nodes, and the process is repeated until the entire tree is generated. Algorithm 1 An example of summation (n=8𝑛8n=8italic_n = 8). {minted} C++ float sum = 0; for (int i=0; i¡8; i+=2) sum += a[i] + a[i+1]; Figure 1. The summation tree of Algorithm 1. The numbers in the leaf nodes denote the indexes. Table 2. The outputs and order-related information for Algorithm 1 with different masked all-one arrays. i𝑖iitalic_i j𝑗jitalic_j Ai,jsuperscript𝐴𝑖𝑗A^{i,j}italic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT sum(Ai,jsuperscript𝐴𝑖𝑗A^{i,j}italic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT) li,jsuperscript𝑙𝑖𝑗l^{i,j}italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT 0 1 (M,−M,1,1,1,1,1,1)𝑀𝑀111111(M,-M,1,1,1,1,1,1)( italic_M , - italic_M , 1 , 1 , 1 , 1 , 1 , 1 ) 6 2 0 2 (M,1,−M,1,1,1,1,1)𝑀1𝑀11111(M,1,-M,1,1,1,1,1)( italic_M , 1 , - italic_M , 1 , 1 , 1 , 1 , 1 ) 4 4 0 3 (M,1,1,−M,1,1,1,1)𝑀11𝑀1111(M,1,1,-M,1,1,1,1)( italic_M , 1 , 1 , - italic_M , 1 , 1 , 1 , 1 ) 4 4 0 4 (M,1,1,1,−M,1,1,1)𝑀111𝑀111(M,1,1,1,-M,1,1,1)( italic_M , 1 , 1 , 1 , - italic_M , 1 , 1 , 1 ) 2 6 0 5 (M,1,1,1,1,−M,1,1)𝑀1111𝑀11(M,1,1,1,1,-M,1,1)( italic_M , 1 , 1 , 1 , 1 , - italic_M , 1 , 1 ) 2 6 0 6 (M,1,1,1,1,1,−M,1)𝑀11111𝑀1(M,1,1,1,1,1,-M,1)( italic_M , 1 , 1 , 1 , 1 , 1 , - italic_M , 1 ) 0 8 0 7 (M,1,1,1,1,1,1,−M)𝑀111111𝑀(M,1,1,1,1,1,1,-M)( italic_M , 1 , 1 , 1 , 1 , 1 , 1 , - italic_M ) 0 8 ………… 2 3 (1,1,M,−M,1,1,1,1)11𝑀𝑀1111(1,1,M,-M,1,1,1,1)( 1 , 1 , italic_M , - italic_M , 1 , 1 , 1 , 1 ) 6 2 2 4 (1,1,M,1,−M,1,1,1)11𝑀1𝑀111(1,1,M,1,-M,1,1,1)( 1 , 1 , italic_M , 1 , - italic_M , 1 , 1 , 1 ) 2 6 ………… FPRev-basic has a time complexity of Θ⁢(n2⁢t⁢(n))Θsuperscript𝑛2𝑡𝑛\Theta(n^{2}t(n))roman_Θ ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_t ( italic_n ) ), where t⁢(n)𝑡𝑛t(n)italic_t ( italic_n ) is the time complexity of the tested function. As a contrast, the naive brute-force method has a time complexity of O⁢(4n/n3/2⋅t⁢(n))𝑂⋅superscript4𝑛superscript𝑛32𝑡𝑛O(4^{n}/n^{3/2}\cdot t(n))italic_O ( 4 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT / italic_n start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT ⋅ italic_t ( italic_n ) ). Building on FPRev-basic, we propose FPRev-advanced, which has a time complexity of Ω⁢(n⁢t⁢(n))Ω𝑛𝑡𝑛\Omega(nt(n))roman_Ω ( italic_n italic_t ( italic_n ) ) and O⁢(n2⁢t⁢(n))𝑂superscript𝑛2𝑡𝑛O(n^{2}t(n))italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_t ( italic_n ) ) and supports multi-term fused summation (fasi_numerical_2021, ) used by matrix accelerators like NVIDIA GPU’s Tensor Cores (markidis_nvidia_2018, ). We evaluate efficiency of FPRev by comprehensive experiments. We test FPRev with three popular numerical libraries across six different CPUs and GPUs. Experimental results show that FPRev-advanced is significantly faster than FPRev-basic, demonstrating its lower time complexity. We also showcase the discrepancies in the revealed orders across different libraries and devices. In summary, the contributions of this paper include the following: (1) We propose novel testing-based algorithms to reveal the order of summation for functions based on floating-point summation. The time complexity of the algorithms is polynomial, in contrast to the exponential time complexity of the naive approach. (2) We develop FPRev, a tool that enables automatic revelation of the order of floating-point summation. This tool is significantly helpful in debugging non-reproducible programs, and provides useful information for reproducing the program. (3) We demonstrate the practical efficiency of FPRev with extensive experiments. (4) We reveal the order of summation for common numerical libraries like cuBLAS for the first time."
https://arxiv.org/html/2411.00363v1,Localized Orthogonal Decomposition Method withH1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPTInterpolation for Multiscale Elliptic Problem,This paper employs a localized orthogonal decomposition (LOD) method with H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT interpolation for solving the multiscale elliptic problem. This method does not need any assumptions on scale separation. We give a priori error estimate for the proposed method. The theoretical results are conformed by various numerical experiments.,"Consider the multiscale elliptic problem {−∇⋅(Aε⁢(x)⁢∇uε⁢(x))=f⁢(x),x∈Ω,uε⁢(x)=0,x∈∂Ω,cases⋅∇superscript𝐴𝜀𝑥∇superscript𝑢𝜀𝑥𝑓𝑥𝑥Ωsuperscript𝑢𝜀𝑥0𝑥Ω\displaystyle\left\{\begin{array}[]{ll}\displaystyle-\nabla\cdot(A^{% \varepsilon}(x)\nabla u^{\varepsilon}(x))=f(x),&x\in\Omega,\\ \displaystyle u^{\varepsilon}(x)=0,&x\in\partial\Omega,\end{array}\right.{ start_ARRAY start_ROW start_CELL - ∇ ⋅ ( italic_A start_POSTSUPERSCRIPT italic_ε end_POSTSUPERSCRIPT ( italic_x ) ∇ italic_u start_POSTSUPERSCRIPT italic_ε end_POSTSUPERSCRIPT ( italic_x ) ) = italic_f ( italic_x ) , end_CELL start_CELL italic_x ∈ roman_Ω , end_CELL end_ROW start_ROW start_CELL italic_u start_POSTSUPERSCRIPT italic_ε end_POSTSUPERSCRIPT ( italic_x ) = 0 , end_CELL start_CELL italic_x ∈ ∂ roman_Ω , end_CELL end_ROW end_ARRAY (3) where Ω⊂ℝ2Ωsuperscriptℝ2\Omega\subset\mathbb{R}^{2}roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (or ℝ3superscriptℝ3\mathbb{R}^{3}blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT) is a bounded convex polygonal domain with Lipschitz boundary ∂ΩΩ\partial\Omega∂ roman_Ω, ε≪1much-less-than𝜀1\varepsilon\ll 1italic_ε ≪ 1 is a positive parameter which signifies the multiscale nature of (3). For simplicity, we only consider d=2𝑑2d=2italic_d = 2. All the results are still valid for high dimensional problems. Direct numerical simulations such as finite element method (FEM) in order to capture fast oscillations on microscopic scale lead to problems of very large size which are prohibitively expensive. From the engineering point of view, the macroscopic properties and macroscopic behaviors of the materials are more important. Homogenization and numerical homogenization [4, 26] have been used to simulate multiscale materials. These methods can calculate effective material properties to solve the macro scale problems successfully. However the limitation is based on the assumption of material, such as periodic micro-structures. To overcome these difficulties, several multiscale computing strategies have been developed to solve the multiscale elliptic problem in heterogeneous structures on grids that are coarser than the scale of oscillations. Babuška and Osborn developed the so-called generalized multiscale finite element methods (GMSFEM) [5, 6, 7] for problems with rough coefficients, including discontinuous coefficients as well as coefficients with multiple scales. Its main idea is to modify the finite element space in the framework of the finite element method. Hou et al. developed a multiscale finite element method (MsFEM) [16, 22] for the study of multi-dimensional problems. This is achieved by constructing multiscale finite element basis functions which adapt to the local property of the differential operator. E and Engquist developed the heterogeneous multiscale method (HMM) [1, 12, 13], which is a generally efficient methodology for problems with multiple scales. The method avoids computing the coefficients in the fine scale mesh as well as the coefficients of the homogenized equations. However, it depends on some powerful assumptions, such as periodicity and scale separation. There are some other methods to solve multiscale problems with strongly varying coefficients. For example, the variational multiscale method (VMS) [23, 24] and residual-free bubbles method [18] in which the test function space was decomposed into the sum of coarse and fine scale components. In this paper, we study the Localized Orthogonal Decomposition (LOD) method with H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT interpolation for elliptic problems with highly varying coefficients. The LOD method was first introduced for elliptic multiscale problems [28], which is suitable for the general framework of the Variational Multiscale Method (VMS)[23, 24]. The method constructs local generalized finite element basis which is exponentially decayed by using the modified Clément interpolation [10]. The analysis does not depend on the regularity of the solution or the scale separation in the coefficient. The modified Clément interpolation is essentially a L2superscript𝐿2L^{2}italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT projection, but H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT interpolation is more natural for elliptic problems. The method is also applied to parabolic problem [29], semi-linear problem [20], wave propagation [2], elliptic optimal control problem [8], and so on. Recently, Peterseim reviewed the VMS methods of Linear multiscale partial differential equations [27] and gave a new simple proof for the exponential decay property. We will use H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT interpolation to apply the technique. By using the classical finite element methods, we prove convergence of optimal order in H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm. Numerical examples are given to complete the analysis, which supports our theoretical findings. The outline of this paper is as follows. In Section 2, we describe the model problem and present the localized orthogonal decomposition method with H1superscript𝐻1H^{1}italic_H start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT interpolation. The localization technique is proposed in Section 3. In section 4, some numerical experiments are given."
https://arxiv.org/html/2411.00362v1,Heterogeneous Multiscale Method for elliptic problem without scale separation,This paper shows that the Heterogeneous Multiscale Method can be applied to elliptic problem without scale separation. The Localized Orthogonal Method is a special case of the Heterogeneous Multiscale Method. Keywords: Heterogeneous Multiscale Method; Localized Orthogonal Method; Elliptic Problems; Scale Separation.,"Consider the classical multiscale elliptic problem {−∇⋅(aε⁢(x)⁢∇u⁢(x))=f⁢(x),x∈Ω⊂ℝd,u=0,x∈∂Ω,cases⋅∇superscript𝑎𝜀𝑥∇𝑢𝑥𝑓𝑥𝑥Ωsuperscriptℝ𝑑𝑢0𝑥Ω\displaystyle\left\{\begin{array}[]{ll}\displaystyle-\nabla\cdot(a^{% \varepsilon}(x)\nabla u(x))=f(x),&x\in\Omega\subset\mathbb{R}^{d},\\ \displaystyle u=0,&x\in\partial\Omega,\end{array}\right.{ start_ARRAY start_ROW start_CELL - ∇ ⋅ ( italic_a start_POSTSUPERSCRIPT italic_ε end_POSTSUPERSCRIPT ( italic_x ) ∇ italic_u ( italic_x ) ) = italic_f ( italic_x ) , end_CELL start_CELL italic_x ∈ roman_Ω ⊂ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , end_CELL end_ROW start_ROW start_CELL italic_u = 0 , end_CELL start_CELL italic_x ∈ ∂ roman_Ω , end_CELL end_ROW end_ARRAY (1.3) where ε≪1much-less-than𝜀1\varepsilon\ll 1italic_ε ≪ 1 is a small parameter that represents the multiscale nature in the problem. Several multiscale methods have been developed for the numerical solution of this multiscale elliptic problem. The most well known method is the Heterogeneous Multiscale Method (HMM), developed in [3]. The method is applied into elliptic homogenization problem in [4] and parabolic homogenization problem in [9]. HMM is a general framework for designing multiscale algorithms. It consists of two components: selection of a macroscopic solver and estimating the missing macroscale data by solving locally the fine scale problem. The convergence analysis usually assumes certain periodicity and scale separation. In this paper, we deduce that HMM can be applied to multiscale elliptic problem without scale separation, and the optimal error estimate can also be obtained for this case. Recently, a Localized Orthogonal Decomposition (LOD) method was introduced for elliptic multiscale problems in [7], fitting into the general framework of the Variational Multiscale Method (VMS)[5]. The method constructs local generalized finite element basis which are exponential decayed. The analysis does not rely on high regularity of the solution or scale separation in the coefficient. The method is also applied to parabolic problems [8]. Peterseim review the VMS methods for Linear multiscale PDEs in [6] and give a new simply proof for the exponential decay property. In this paper, we show that the LOD method is a special case of HMM. Thus, the error estimate for HMM can be obtained from the LOD method directly. The outline of the paper is as follows. In Section 2, we review the original definition for the Heterogeneous Multiscale Method. The localization technology is developed in Section 3."
https://arxiv.org/html/2411.00060v1,Asymptotic expansions for approximate solutions of boundary integral equations,"This paper uses the Modified Projection Method to examine the errors in solving the boundary integral equation from Laplace’s equation. The analysis uses weighted norms, and parallel algorithms help solve the independent linear systems. By applying the method developed by Kulkarni, the study shows how the approximate solution behaves in polygonal domains. It also explores computational techniques using the double-layer potential kernel to solve Laplace’s equation in these domains. The iterated Galerkin method provides an approximation of order 2⁢r+22𝑟22r+22 italic_r + 2 in smooth domains. However, the corners in polygonal domains cause singularities that reduce the accuracy. By adjusting the mesh near these corners, the accuracy can almost be restored when the error is measured using the uniform norm. This paper builds on the work of Rude et al. By using Kulkarni’s modified operator and observes superconvergence in iterated solutions. This leads to an asymptotic error expansion, with the leading term being O⁢(h4)𝑂superscriptℎ4O(h^{4})italic_O ( italic_h start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) and the remaining error term O⁢(h6)𝑂superscriptℎ6O(h^{6})italic_O ( italic_h start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ), resulting in a method with similar accuracy.","The boundary integral equation (BIE) method has emerged as a powerful approach for solving potential problems, especially those governed by Laplace’s equation. One of its main advantages is that it reduces the dimensionality of the problem by focusing on the boundary, resulting in fewer unknowns and often higher computational efficiency compared to traditional finite element or finite difference methods. This benefit is particularly pronounced when dealing with complex geometries, such as polygonal domains, which introduce additional challenges like boundary singularities at corners. While the BIE method offers improved efficiency with fewer unknowns, handling these singularities is crucial for maintaining convergence rates and accuracy. Existing approaches, such as the Galerkin method with piecewise polynomials, provide effective solutions for smooth domains. However, in polygonal domains, the presence of corners degrades the convergence rate due to singularities in the solution and kernel. Mesh grading techniques have been introduced by Rice [8] to mitigate this issue, restoring much of the convergence when measured in the uniform norm. For comprehensive information on the numerical treatment of boundary integral equations, one can refer to [1] and Hackbusch [6]. Chandler [2] demonstrated that the iterated Galerkin solution can achieve superconvergence, with accuracy improvements up to order 2 in the uniform norm. Furthermore, Richardson extrapolation has been shown to increase the convergence order to 4, as detailed by Rude et al., who also introduced multi-parameter extrapolation for handling boundary integral equations in polygonal domains. Lin and Xie [7] demonstrated the existence of asymptotic error expansions under suitable conditions. This paper builds on the work of Rude et al.[9] by extending their multi-parameter asymptotic expansion technique using a modified operator proposed by Kulkarni [5]. This finite rank approximating operator enhances the accuracy of solving second-kind Fredholm integral equations, allowing for superconvergence effects in iterated solutions. The resulting asymptotic error expansion achieves a leading term of O⁢(h4)𝑂superscriptℎ4O(h^{4})italic_O ( italic_h start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ), where hℎhitalic_h is the mesh characteristic parameter, with the remaining error term being O⁢(h6)𝑂superscriptℎ6O(h^{6})italic_O ( italic_h start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT ). By using linear combinations of solutions for different mesh parameters, we can eliminate the leading error term, achieving a highly accurate solution with a significantly reduced computational cost. In addition, we explore multi-parameter asymptotic expansions where the boundary is partitioned into multiple segments, each with an independently chosen mesh width. This technique results in a discretization scheme with several independent parameters, offering enhanced flexibility in mesh refinement and accuracy. Our work extends previous analyses, providing new insights and computational strategies for improving the accuracy and efficiency of boundary integral equation solutions, particularly for complex, polygonal geometries. In multi-parameter asymptotic expansions, the boundary is partitioned into r𝑟ritalic_r parts. The mesh width hisubscriptℎ𝑖h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for each segment (1≤i≤r)1𝑖𝑟(1\leq i\leq r)( 1 ≤ italic_i ≤ italic_r ) can be chosen independently of the others. This results in a discretization that has r𝑟ritalic_r independent parameters (h1,…,hr)subscriptℎ1…subscriptℎ𝑟(h_{1},...,h_{r})( italic_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_h start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ). We will extend the analysis of [7] to show that the resulting discrete approximation permits a multivariate asymptotic error expansion in these p parameters. In section 2, we state the precise definition of our problem set notations, and recall some preliminaries, In section 3 we describe the modified projection method. Asymptotic series expansion for the proposed solution is obtained in Section 4. Section 5 summarizes our results. It also contains implementation details and a comparison with the method suggested by Rude et al [9]."
https://arxiv.org/html/2411.00043v1,Conditional quasi-optimal error estimate for a finite element discretization of thep𝑝pitalic_p-Navier–Stokes equations: The casep>2𝑝2p>2italic_p > 2,"In this paper, we derive quasi-optimal a priori error estimates for the kinematic pressure for a Finite Element (FE) approximation of steady systems of p𝑝pitalic_p-Navier–Stokes type in the case of shear-thickening, i.e., in the case p>2𝑝2p>2italic_p > 2, imposing a new mild Muckenhoupt regularity condition.","††* funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 525389262. In the present paper, we examine a Finite Element (FE) approximation of steady systems of p𝑝pitalic_p-Navier–Stokes type, i.e., −div⁢𝐒⁢(𝐃𝐯)+[∇𝐯]⁢𝐯+∇qdiv𝐒𝐃𝐯delimited-[]∇𝐯𝐯∇𝑞\displaystyle-\mathrm{div}\,\mathbf{S}({\bf D}{\bf v})+[\nabla{\bf v}]{\bf v}+\nabla q- roman_div bold_S ( bold_Dv ) + [ ∇ bold_v ] bold_v + ∇ italic_q =𝐟absent𝐟\displaystyle={\bf f}\qquad= bold_f in ⁢Ω,in Ω\displaystyle\text{in }\Omega\,,in roman_Ω , (1.1) div⁢𝐯div𝐯\displaystyle\mathrm{div}\,{\bf v}roman_div bold_v =0absent0\displaystyle=0\qquad= 0 in ⁢Ω,in Ω\displaystyle\text{in }\Omega\,,in roman_Ω , 𝐯𝐯\displaystyle{\bf v}bold_v =𝟎absent0\displaystyle=\mathbf{0}= bold_0 on ⁢∂Ω,on Ω\displaystyle\text{on }\partial\Omega\,{\color[rgb]{0,0,0}\definecolor[named]{% pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill% {0},}on ∂ roman_Ω , for quasi-optimal a priori error estimates for the kinematic pressure in the case of shear-thickening fluids, i.e., p>2𝑝2p>2italic_p > 2. The system (1.1) describes the steady motion of a homogeneous, incompressible fluid with shear-dependent viscosity. More precisely, for a given vector field 𝐟:Ω→ℝd:𝐟→Ωsuperscriptℝ𝑑{\bf f}\colon\Omega\to\mathbb{R}^{d}bold_f : roman_Ω → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT describing external forces, an incompressibility constraint (1.1)2, and a non-slip boundary condition (1.1)3, the system (1.1) seeks for a velocity vector field 𝐯=(v1,…,vd)⊤:Ω→ℝd:𝐯superscriptsubscript𝑣1…subscript𝑣𝑑top→Ωsuperscriptℝ𝑑{\bf v}=(v_{1},\ldots,v_{d})^{\top}\colon\Omega\to\mathbb{R}^{d}bold_v = ( italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT : roman_Ω → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and a kinematic pressure q:Ω→ℝ:𝑞→Ωℝq\colon\Omega\to\mathbb{R}italic_q : roman_Ω → blackboard_R solving (1.1). Here, Ω⊆ℝdΩsuperscriptℝ𝑑\Omega\subseteq\mathbb{R}^{d}roman_Ω ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT, d∈{2,3}𝑑23d\in{\{{2,3}\}}italic_d ∈ { 2 , 3 }, is a bounded, polygonal (if d=2𝑑2d=2italic_d = 2) or polyhedral (if d=3𝑑3{d=3}italic_d = 3) Lipschitz domain. The extra stress tensor 𝐒⁢(𝐃𝐯):Ω→ℝsymd×d:𝐒𝐃𝐯→Ωsubscriptsuperscriptℝ𝑑𝑑sym\mathbf{S}({\bf D}{\bf v})\colon\Omega\to\smash{\mathbb{R}^{d\times d}_{% \textup{sym}}}bold_S ( bold_Dv ) : roman_Ω → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT sym end_POSTSUBSCRIPT depends on the strain rate tensor 𝐃𝐯≔12⁢(∇𝐯+∇𝐯⊤):Ω→ℝsymd×d:≔𝐃𝐯12∇𝐯∇superscript𝐯top→Ωsubscriptsuperscriptℝ𝑑𝑑sym{\bf D}{\bf v}\coloneqq\frac{1}{2}(\nabla{\bf v}+\nabla{\bf v}^{\top})\colon% \Omega\to\smash{\mathbb{R}^{d\times d}_{\textup{sym}}}bold_Dv ≔ divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( ∇ bold_v + ∇ bold_v start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT ) : roman_Ω → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT sym end_POSTSUBSCRIPT, i.e., the symmetric part of the velocity gradient ∇𝐯≔(∂jvi)i,j=1,…,d:Ω→ℝd×d:≔∇𝐯subscriptsubscript𝑗subscript𝑣𝑖formulae-sequence𝑖𝑗1…𝑑→Ωsuperscriptℝ𝑑𝑑\nabla{\bf v}\coloneqq(\partial_{j}v_{i})_{i,j=1,\ldots,d}\colon\Omega\to% \mathbb{R}^{d\times d}∇ bold_v ≔ ( ∂ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i , italic_j = 1 , … , italic_d end_POSTSUBSCRIPT : roman_Ω → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT. The convective term [∇𝐯]⁢𝐯:Ω→ℝd:delimited-[]∇𝐯𝐯→Ωsuperscriptℝ𝑑\smash{[\nabla{\bf v}]{\bf v}\colon\Omega\to\mathbb{R}^{d}}[ ∇ bold_v ] bold_v : roman_Ω → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is defined via ([∇𝐯]⁢𝐯)i≔∑j=1dvj⁢∂jvi≔subscriptdelimited-[]∇𝐯𝐯𝑖superscriptsubscript𝑗1𝑑subscript𝑣𝑗subscript𝑗subscript𝑣𝑖\smash{([\nabla{\bf v}]{\bf v})_{i}\coloneqq\sum_{j=1}^{d}{v_{j}\partial_{j}v_% {i}}}( [ ∇ bold_v ] bold_v ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ≔ ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ∂ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for all i=1,…,d𝑖1…𝑑i=1,\ldots,ditalic_i = 1 , … , italic_d. Throughout the paper, we assume that the extra stress tensor 𝐒𝐒\mathbf{S}bold_S has (p,δ)𝑝𝛿(p,\delta)( italic_p , italic_δ )-structure (cf. Assumption 2.4). The relevant example falling into this class is 𝐒:ℝd×d→ℝsymd×d:𝐒→superscriptℝ𝑑𝑑subscriptsuperscriptℝ𝑑𝑑sym\mathbf{S}\colon\mathbb{R}^{d\times d}\to\mathbb{R}^{d\times d}_{\textup{sym}}bold_S : blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT sym end_POSTSUBSCRIPT, for every 𝐀∈ℝd×d𝐀superscriptℝ𝑑𝑑{\bf A}\in\mathbb{R}^{d\times d}bold_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT defined via 𝐒⁢(𝐀)≔μ0⁢(δ+|𝐀|)p−2⁢𝐀,≔𝐒𝐀subscript𝜇0superscript𝛿𝐀𝑝2𝐀\displaystyle\mathbf{S}({\bf A})\coloneqq\mu_{0}\,(\delta+|{\bf A}|)^{p-2}{\bf A% }\,,bold_S ( bold_A ) ≔ italic_μ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_δ + | bold_A | ) start_POSTSUPERSCRIPT italic_p - 2 end_POSTSUPERSCRIPT bold_A , (1.2) where p∈(1,∞)𝑝1p\in(1,\infty)italic_p ∈ ( 1 , ∞ ), δ≥0𝛿0\delta\geq 0italic_δ ≥ 0, and μ0>0subscript𝜇00\mu_{0}>0italic_μ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 0. The a priori error analysis of the steady p𝑝pitalic_p-Navier–Stokes problem (1.1) using FE approximations is by now well-understood: recently, in [20], a priori error estimates in the case of shear-thickening, i.e., in the case p>2𝑝2p>2italic_p > 2, were derived, which are optimal for the velocity vector field, but sub-optimal for the kinematic pressure. More precisely, this lacuna is mainly due to the following technical hurdle, here, exemplified by the error analysis of the FE approximations of the steady p𝑝pitalic_p-Navier–Stokes problem (1.1) used in [20]: in it, it turns out that the error of the kinematic pressure measured in the squared Lp′⁢(Ω)superscript𝐿superscript𝑝′ΩL^{p^{\prime}}(\Omega)italic_L start_POSTSUPERSCRIPT italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( roman_Ω )-norm is bounded by the squared L(φ|𝐃𝐯|)∗⁢(Ω)superscript𝐿superscriptsubscript𝜑𝐃𝐯ΩL^{(\varphi_{\smash{|{\bf D}{\bf v}|}})^{*}}(\Omega)italic_L start_POSTSUPERSCRIPT ( italic_φ start_POSTSUBSCRIPT | bold_Dv | end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( roman_Ω )-norm of the extra-stress errors, i.e., it holds that ‖qh−q‖p′,Ω2≤‖𝐒⁢(𝐃𝐯h)−𝐒⁢(𝐃𝐯)‖(φ|𝐃𝐯|)∗,Ω2+(h.o.t.).superscriptsubscriptnormsubscript𝑞ℎ𝑞superscript𝑝′Ω2absentsuperscriptsubscriptnorm𝐒subscript𝐃𝐯ℎ𝐒𝐃𝐯superscriptsubscript𝜑𝐃𝐯Ω2(h.o.t.)\displaystyle\begin{aligned} \|q_{h}-q\|_{p^{\prime},\Omega}^{2}&\leq\|{\bf S}% ({\bf D}{\bf v}_{h})-{\bf S}({\bf D}{\bf v})\|_{(\varphi_{\smash{|{\bf D}{\bf v% }|}})^{*},\Omega}^{2}+\textup{(h.o.t.)}\,.\end{aligned}start_ROW start_CELL ∥ italic_q start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT - italic_q ∥ start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_CELL start_CELL ≤ ∥ bold_S ( bold_Dv start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) - bold_S ( bold_Dv ) ∥ start_POSTSUBSCRIPT ( italic_φ start_POSTSUBSCRIPT | bold_Dv | end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + (h.o.t.) . end_CELL end_ROW (1.3) The relation (1.3) is a consequence of the squared discrete inf-sup stability result (cf. [9, Lem. 6.10]) c⁢‖zh‖p′,Ω2≤sup𝐳h∈V˚h:‖∇𝐳h‖p,Ω≤1((zh,div⁢𝐳h)Ω)2,𝑐superscriptsubscriptnormsubscript𝑧ℎsuperscript𝑝′Ω2subscriptsupremum:subscript𝐳ℎsubscript˚𝑉ℎsubscriptnorm∇subscript𝐳ℎ𝑝Ω1superscriptsubscriptsubscript𝑧ℎdivsubscript𝐳ℎΩ2\displaystyle c\,\|z_{h}\|_{p^{\prime},\Omega}^{2}\leq\sup_{{\bf z}_{h}\in{% \mathaccent 23{V}}_{h}\;:\;\|\nabla{\bf z}_{h}\|_{p,\Omega}\leq 1}{\big{(}(z_{% h},\mathrm{div}\,{\bf z}_{h})_{\Omega}\big{)}^{2}}\,,italic_c ∥ italic_z start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ roman_sup start_POSTSUBSCRIPT bold_z start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∈ over˚ start_ARG italic_V end_ARG start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT : ∥ ∇ bold_z start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∥ start_POSTSUBSCRIPT italic_p , roman_Ω end_POSTSUBSCRIPT ≤ 1 end_POSTSUBSCRIPT ( ( italic_z start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT , roman_div bold_z start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT roman_Ω end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (1.4) valid for each zh∈Q˚hsubscript𝑧ℎsubscript˚𝑄ℎz_{h}\in{\mathaccent 23{Q}}_{h}italic_z start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ∈ over˚ start_ARG italic_Q end_ARG start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT, where V˚h⊆(W01,p⁢(Ω))dsubscript˚𝑉ℎsuperscriptsubscriptsuperscript𝑊1𝑝0Ω𝑑{\mathaccent 23{V}}_{h}\subseteq(W^{1,p}_{0}(\Omega))^{d}over˚ start_ARG italic_V end_ARG start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ⊆ ( italic_W start_POSTSUPERSCRIPT 1 , italic_p end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT denotes a discrete velocity vector space and and Q˚h⊆L0p′⁢(Ω)subscript˚𝑄ℎsubscriptsuperscript𝐿superscript𝑝′0Ω{\mathaccent 23{Q}}_{h}\subseteq L^{p^{\prime}}_{0}(\Omega)over˚ start_ARG italic_Q end_ARG start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ⊆ italic_L start_POSTSUPERSCRIPT italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( roman_Ω ) a discrete pressure space jointly forming a discretely inf-sup-stable finite element couple. Then, using the estimates (cf. [22, Lem. 4.4(ii)]) ‖𝐒⁢(𝐃𝐯h)−𝐒⁢(𝐃𝐯)‖(φ|𝐃𝐯|)∗,Ω2≤c⁢‖𝐅⁢(𝐃𝐯h)−𝐅⁢(𝐃𝐯)‖2,Ω2,superscriptsubscriptnorm𝐒subscript𝐃𝐯ℎ𝐒𝐃𝐯superscriptsubscript𝜑𝐃𝐯Ω2𝑐superscriptsubscriptnorm𝐅subscript𝐃𝐯ℎ𝐅𝐃𝐯2Ω2\displaystyle\|{\bf S}({\bf D}{\bf v}_{h})-{\bf S}({\bf D}{\bf v})\|_{(\varphi% _{\smash{|{\bf D}{\bf v}|}})^{*},\Omega}^{2}\leq c\,\|{\bf F}({\bf D}{\bf v}_{% h})-{\bf F}({\bf D}{\bf v})\|_{2,\Omega}^{2}\,,∥ bold_S ( bold_Dv start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) - bold_S ( bold_Dv ) ∥ start_POSTSUBSCRIPT ( italic_φ start_POSTSUBSCRIPT | bold_Dv | end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_c ∥ bold_F ( bold_Dv start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) - bold_F ( bold_Dv ) ∥ start_POSTSUBSCRIPT 2 , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , from the relation (1.3), it follows that ‖qh−q‖p′,Ω2≤c⁢‖𝐅⁢(𝐃𝐯h)−𝐅⁢(𝐃𝐯)‖2,Ω2+(h.o.t.),superscriptsubscriptnormsubscript𝑞ℎ𝑞superscript𝑝′Ω2𝑐superscriptsubscriptnorm𝐅subscript𝐃𝐯ℎ𝐅𝐃𝐯2Ω2(h.o.t.)\displaystyle\|q_{h}-q\|_{p^{\prime},\Omega}^{2}\leq c\,\|{\bf F}({\bf D}{\bf v% }_{h})-{\bf F}({\bf D}{\bf v})\|_{2,\Omega}^{2}+\textup{(h.o.t.)}\,,∥ italic_q start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT - italic_q ∥ start_POSTSUBSCRIPT italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ≤ italic_c ∥ bold_F ( bold_Dv start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) - bold_F ( bold_Dv ) ∥ start_POSTSUBSCRIPT 2 , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + (h.o.t.) , (1.5) i.e., the kinematic pressure error measured in the squared Lp′⁢(Ω)superscript𝐿superscript𝑝′ΩL^{p^{\prime}}(\Omega)italic_L start_POSTSUPERSCRIPT italic_p start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ( roman_Ω )-norm is bounded by the squared velocity vector field error. However, numerical experiments (cf. [22]) indicate that the relation (1.5) is potentially sub-optimal and suggest instead the relation ρ(φ|𝐃𝐯|)∗,Ω⁢(qh−q)≤c⁢‖𝐅⁢(𝐃𝐯h)−𝐅⁢(𝐃𝐯)‖2,Ω2+(h.o.t.).subscript𝜌superscriptsubscript𝜑𝐃𝐯Ωsubscript𝑞ℎ𝑞𝑐superscriptsubscriptnorm𝐅subscript𝐃𝐯ℎ𝐅𝐃𝐯2Ω2(h.o.t.)\displaystyle\rho_{(\varphi_{|{\bf D}{\bf v}|})^{*},\Omega}(q_{h}-q)\leq c\,\|% {\bf F}({\bf D}{\bf v}_{h})-{\bf F}({\bf D}{\bf v})\|_{2,\Omega}^{2}+\textup{(% h.o.t.)}\,.italic_ρ start_POSTSUBSCRIPT ( italic_φ start_POSTSUBSCRIPT | bold_Dv | end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT , roman_Ω end_POSTSUBSCRIPT ( italic_q start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT - italic_q ) ≤ italic_c ∥ bold_F ( bold_Dv start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ) - bold_F ( bold_Dv ) ∥ start_POSTSUBSCRIPT 2 , roman_Ω end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + (h.o.t.) . (1.6) In [23], in the case of a Local Discontinuous Galerkin (LDG) approximation, such a discrete analogue has been established under the additional assumption that the viscosity of the fluid is a Muckenhoupt weight of class 2222, i.e., if we have that μ𝐃𝐯≔(δ+|𝐃𝐯¯|)p−2∈A2⁢(ℝd),≔subscript𝜇𝐃𝐯superscript𝛿¯𝐃𝐯𝑝2subscript𝐴2superscriptℝ𝑑\displaystyle\mu_{{\bf D}{\bf v}}\coloneqq(\delta+|\overline{{\bf D}{\bf v}}|)% ^{p-2}\in A_{2}(\mathbb{R}^{d})\,,italic_μ start_POSTSUBSCRIPT bold_Dv end_POSTSUBSCRIPT ≔ ( italic_δ + | over¯ start_ARG bold_Dv end_ARG | ) start_POSTSUPERSCRIPT italic_p - 2 end_POSTSUPERSCRIPT ∈ italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) , (1.7) where 𝐃𝐯¯≔𝐃𝐯≔¯𝐃𝐯𝐃𝐯\overline{{\bf D}{\bf v}}\coloneqq{\bf D}{\bf v}over¯ start_ARG bold_Dv end_ARG ≔ bold_Dv a.e. in ΩΩ\Omegaroman_Ω and 𝐃𝐯¯≔𝟎≔¯𝐃𝐯0\overline{{\bf D}{\bf v}}\coloneqq{\mathbf{0}}over¯ start_ARG bold_Dv end_ARG ≔ bold_0 a.e. in ℝd∖Ωsuperscriptℝ𝑑Ω\mathbb{R}^{d}\setminus\Omegablackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ∖ roman_Ω. In [27, 28], it turned out that the Muckenhoupt regularity assumption (1.7) can not be expected, in general, in the three-dimensional case. However, in the two-dimensional case, regularity results (cf. [24]) suggest that (1.7) is satisfied under mild assumptions. Finally, if the Muckenhoupt regularity condition (1.7) is satisfied, from the relation (1.6), we are able to derive an alternative a priori error estimate for the pressure which turns out to be quasi-optimal and, thus, is the first quasi-optimal a priori error estimate for the pressure in the case of shear-thickening, i.e., p>2𝑝2p>2italic_p > 2. This paper is organized as follows: In Section 2, we introduce the employed notation, define relevant function spaces, basic assumptions on the extra stress tensor 𝐒𝐒\mathbf{S}bold_S and its consequences, the weak formulations Problem (Q) and Problem (P) of the system (1.1), and the discrete operators. In Section 3, we introduce the discrete weak formulations Problem (Qh) and Problem (Ph), recall known a priori error estimates and prove the main result of the paper, i.e., a quasi-optimal (with respect to the Muckenhoupt regularity condition (1.7)) a priori error estimate for the kinematic pressure in the case p>2𝑝2p>2italic_p > 2 (cf. Theorem 3.15, Corollary 3.16). In Section 4, we review the theoretical findings via numerical experiments."
https://arxiv.org/html/2411.00040v1,P2C2Net: Pde-Preserved Coarse Correction Network for Efficient Prediction of Spatiotemporal Dynamics,"When solving partial differential equations (PDEs), classical numerical methods often require fine mesh grids and small time stepping to meet stability, consistency, and convergence conditions, leading to high computational cost. Recently, machine learning has been increasingly utilized to solve PDE problems, but they often encounter challenges related to interpretability, generalizability, and strong dependency on rich labeled data. Hence, we introduce a new Pde-Preserved Coarse Correction Network (P2C2Net) to efficiently solve spatiotemporal PDE problems on coarse mesh grids in small data regimes. The model consists of two synergistic modules: (1) a trainable PDE block that learns to update the coarse solution (i.e., the system state), based on a high-order numerical scheme with boundary condition encoding, and (2) a neural network block that consistently corrects the solution on the fly. In particular, we propose a learnable symmetric Conv filter, with weights shared over the entire model, to accurately estimate the spatial derivatives of PDE based on the neural-corrected system state. The resulting physics-encoded model is capable of handling limited training data (e.g., 3–5 trajectories) and accelerates the prediction of PDE solutions on coarse spatiotemporal grids while maintaining a high accuracy. P2C2Net achieves consistent state-of-the-art performance with over 50% gain (e.g., in terms of relative prediction error) across four datasets covering complex reaction-diffusion processes and turbulent flows.","Complex spatiotemporal dynamical systems are pivotal and commonly seen in numerous fields such biology, meteorology, fluid mechanics, etc. The behaviors of these systems are primarily governed by partial differential equations (PDEs), conventionally solved by numerical methods [1, 2, 3, 4]. However, direct numerical simulation (DNS) demands in-depth knowledge of the underlying physics, and the efficacy of numerical solutions is intricately linked to the resolution of mesh grids and time steps. High-resolution spatiotemporal grids are essential for accurate and convergent calculations, yet leading to substantial computational costs. For instance, simulating the flow field around a large aircraft [5, 6] involves creating over millions of mesh nodes and may consume vast simulation time even on high performance computing. Additionally, any changes in initial and boundary conditions (I/BCs) or design parameters necessitate recalculations, further compounding the complexity. Recently, tremendous efforts have been placed on machine learning for data-driven simulation of these systems [7, 8, 9], demonstrating promising potential. These methods do not require the a priori knowledge of physics and, meanwhile, help bypass some traditional constraints, e.g., the smallest size of mesh grid and time step to guarantee solution accuracy, stability and convergence [10]. However, they typically face issues of poor interpretability, weak generalizability and strong dependency of rich labeled data. Their performance deteriorate significantly particularly in small data regimes. Embedding prior physics knowledge into the learning process has demonstrated effective to overcome the aforementioned issues. A brute-force way lies in creating regularizers (e.g., the residual form of PDEs and I/BCs) as “soft” penalty in the loss function, e.g., the family of physics-informed neural works (PINNs) [11, 12, 13, 14, 15, 16, 17]. However, such a strategy has limited scalability and generalizability, and the solution accuracy relies largely on a proper selection of loss weight hyperparameters. Embedding physics explicitly into the network architecture, which imposes “hard” constraints such as physics-encoded recurrent convolutional neural network (PeRCNN) [18, 19], possesses better generalizability as well as offers better convergence and flexibility for model training without the need of fine-tuning hyperparameters. Nevertheless, existing methods fail to handle coarse mesh grids and suffer from instability issues especially for long-range prediction of dynamics. Hybridizing classical numerical schemes and neural networks, e.g., the learned interpolation (LI) model [20], can enable accelerated simulation on coarse mesh grids with satisfied accuracy. Yet, since the numerical part is non-trainable, such models still require rich labeled data to retain accuracy. To tackle these critical challenges, we introduce the P2C2Net model for efficient prediction of spatiotemporal dynamics on coarse mesh grids in small training data regimes. Specifically, a trainable PDE block (a white box) is designed to learn the coarse solution at low resolution, where the temporal marching of system states is handled by a fourth-order Runge-Kutta (RK4) scheme. We also propose a learnable symmetric Conv filter for more accurate estimation of spatial derivatives on coarse grids, as required in PDE block. A neural network (NN) block, which serves as a correction module, is further introduced to correct the coarse solution, restoring information lost due to reduced resolution. We also encode BC into the solution via a padding strategy. Our primary contributions are threefold: • We propose a new physics-encoded correction learning model (P2C2Net) to efficiently predict complex spatiotemporal dynamics on coarse mesh grids. The model requires only a small set of training data and possesses plausible generalizability. • We introduce a structured Conv filter that preserves symmetry to improve the estimation accuracy of coarse spatial derivatives required in the solution updating process, which makes the PDE block trainable with flexibility of handling coarse grids. • P2C2Net achieves consistent state-of-the-art performance with at least 50% gain (e.g., in terms of relative prediction error) across four datasets covering complex reaction-diffusion (RD) processes and turbulent flows, simultaneously retaining accuracy and efficiency."
https://arxiv.org/html/2411.00033v1,An Example Article††thanks:Submitted to the editors DATE.\fundingThis work was funded by the Fog Research Institute under contract no. FRI-454.,"This paper describes a fast algorithm for transforming Legendre coefficients into Chebyshev coefficients, and vice versa. The algorithm is based on the fast multipole method and is similar to the approach described by Alpert and Rokhlin [SIAM J. Sci. Comput., 12 (1991)]. The main difference is that we utilise a modal Galerkin approach with Chebyshev basis functions instead of a nodal approach with a Lagrange basis. Part of the algorithm is a novel method that facilitates faster spreading of intermediate results through neighbouring levels of hierarchical matrices. This enhancement leads to a method that is approximately 20%percent2020~{}\%20 % faster to execute, due to less floating point operations. We also describe an efficient initialization algorithm that for the Lagrange basis is roughly 5 times faster than the original method for large input arrays. The described method has both a planning and execution stage that asymptotically require 𝒪⁢(N)𝒪𝑁\mathcal{O}(N)caligraphic_O ( italic_N ) flops. The algorithm is simple enough that it can be implemented in 100 lines of vectorized Python code. Moreover, its efficiency is such that a single-threaded C implementation can transform 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT coefficients in approximately 20 milliseconds on a new MacBook Pro M3, representing about 3 times the execution time of a well-planned (single-threaded) type 2 discrete cosine transform from FFTW (www.fftw.org). Planning for the 106superscript10610^{6}10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT coefficients requires approximately 50 milliseconds.","Let ℙNsubscriptℙ𝑁\mathbb{P}_{N}blackboard_P start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT be the set of polynomials of degree less than or equal to N𝑁Nitalic_N. Any function f⁢(x)∈ℙN−1𝑓𝑥subscriptℙ𝑁1f(x)\in\mathbb{P}_{N-1}italic_f ( italic_x ) ∈ blackboard_P start_POSTSUBSCRIPT italic_N - 1 end_POSTSUBSCRIPT defined on x∈[−1,1]𝑥11x\in[-1,1]italic_x ∈ [ - 1 , 1 ] can then be expanded as (1)"
https://arxiv.org/html/2411.00002v1,Noise Guided Structural Learning from Observing Stochastic Dynamics,"We develop an innovative learning framework that incorporate the noise structure to infer the governing equations from observation of trajectory data generated by stochastic dynamics. Our approach can proficiently captures both the noise and the drift terms. Furthermore, it can also accommodate a wide range of noise types, including correlated and state-dependent variations. Moreover, our method demonstrates scalability to high-dimensional systems. Through extensive numerical experiments, we showcase the exceptional performance of our learning algorithm in accurately reconstructing the underlying stochastic dynamics.","Stochastic Differential Equations (SDEs) provides an accessible and flexible framework for fundamental modeling of stochastic phenomena arising in science and engineering applications [10, 28]. Compared to traditional deterministic differential equations (ODEs) models, SDEs can capture the underlying randomness of the systems, thus leading to more accurate descriptions of the complex behaviors. By incorporating a random component, typically through a Brownian motion, SDE provides a more realistic and flexible framework for simulating and predicting the behavior of these complex and dynamic systems. The SDE model considered here takes on the following form d⁢𝐱t=𝒇⁢(𝐱t)⁢d⁡t+σ⁢(𝐱t)⁢d⁡𝐰t,𝐱t,𝐰t∈ℝd,formulae-sequence𝑑subscript𝐱𝑡𝒇subscript𝐱𝑡d𝑡𝜎subscript𝐱𝑡dsubscript𝐰𝑡subscript𝐱𝑡subscript𝐰𝑡superscriptℝ𝑑d{\mathbf{x}}_{t}={\bm{f}}({\mathbf{x}}_{t})\operatorname{d\!}t+\sigma({% \mathbf{x}}_{t})\operatorname{d\!}{\mathbf{w}}_{t},\quad{\mathbf{x}}_{t},{% \mathbf{w}}_{t}\in\mathbb{R}^{d},italic_d bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_f ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_OPFUNCTION roman_d end_OPFUNCTION italic_t + italic_σ ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_OPFUNCTION roman_d end_OPFUNCTION bold_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , where the the drift term 𝒇:ℝd→ℝd:𝒇→superscriptℝ𝑑superscriptℝ𝑑{\bm{f}}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}bold_italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and diffusion coefficient σ:ℝd→ℝd×d:𝜎→superscriptℝ𝑑superscriptℝ𝑑𝑑\sigma:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d\times d}italic_σ : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT (σ𝜎\sigmaitalic_σ is symmetric positive definite) can be both unknown, and the stochastic noise 𝐰tsubscript𝐰𝑡{\mathbf{w}}_{t}bold_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is a vector of independent standard Brownian motions. The noise structure of the SDE system is described by a state dependent covariance matrix Σ:ℝd→ℝd×d:Σ→superscriptℝ𝑑superscriptℝ𝑑𝑑\Sigma:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d\times d}roman_Σ : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d end_POSTSUPERSCRIPT where Σ=σ⁢σ⊺Σ𝜎superscript𝜎⊺\Sigma=\sigma\sigma^{\intercal}roman_Σ = italic_σ italic_σ start_POSTSUPERSCRIPT ⊺ end_POSTSUPERSCRIPT. These SDE models are ubiquitous in physics, biology, finance, chemistry, and many other applications where they provide a robust framework for integrating noise directly into the evolution of system states. In physics, the Langevin equation [27, 6, 9, 32] models the behavior of particles under the influence of both systematic forces and random thermal fluctuations. The model offers insights into particle dynamics at microscopic scales where random forces dominate. Biology benefits from SDEs through models like the stochastic Lotka-Volterra equations, which describe the interactions between predator and prey populations under environmental uncertainty [31]. These models are vital for studying population dynamics where random events can significantly impact species survival and interaction.Additionally, SDEs are also applied in modeling biological systems [30] and cell dynamics [8]. In chemistry, SDEs model the kinetics of chemical reactions involving small numbers of molecules, where traditional deterministic models fail to capture the randomness of molecular collisions [36]. The Chemical Langevin Equation, for instance, is used to simulate reaction pathways in fluctuating environments. SDE models are at the core of mathematical finance, underpinning key areas such as option pricing, risk management, and modeling of interest rates, among which we mention classical Black-Scholes Model [2, 17], Vasicek Model [33] for analyzing interest rate dynamic, and Heston Model [14] for modeling stochastic volatility. Finally, we mention Diffusion Model [16] that currently got traction thanks to its formulation using SDE [29] that makes the analysis and improvement more flexible. The accurate application of SDEs critically depends on the proper calibration, or estimation of the drift and noise structure. Proper parameter estimation ensures that the SDEs not only reflect the theoretical properties of the systems but also closely align with observed phenomena. This alignment is crucial for the models to be truly predictive and reliable in practical applications. This requires the use of diverse statistical and mathematical techniques to ensure the models’ outputs align with empirical data, thereby enhancing their predictive and explanatory power. Since usually SDE models in each field of study have explicit function form of both drift and diffusion terms, one common method to calibrate or estimate the parameters is done by minimizing the least square error between the observation and model prediction [25, 1]. Statistical inference for SDEs has a long history, and we refer to [18] for more details. A canonical approach for estimating the drift is to derive a maximum-likelihood estimator by maximizing the likelihood function or the Radon–Nikodym derivative [20, Chapter 7], assuming that the entire trajectory {𝐱t}t∈[0,T]subscriptsubscript𝐱𝑡𝑡0𝑇\{{\mathbf{x}}_{t}\}_{t\in[0,T]}{ bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t ∈ [ 0 , italic_T ] end_POSTSUBSCRIPT is observed. This approach is employed in recent work of [13]. Following similar arguments, in this work we allowing state-dependent correlated noise, and using the likelihood function we are able to capture the essential structure of 𝒇𝒇{\bm{f}}bold_italic_f from data with complex noise structure. 1.1 Related Works System identification of the drift term from deterministic dynamics has been studied in many different scenarios, e.g. identification by enforcing sparsity such as SINDy [3], neural network based methods such as NeuralODE [4], PINN [26] and autoencoder [37], regression based [7], and high-dimensional reduction variational framework [23]. There are statistical methods which can be used to estimate the drift and noise terms using point-wise statistics. SINDy for SDEs was also developed in [34]. The observation data generated by SDEs can be treated as a time-series data with a mild assumption on the relationship between 𝐱tsubscript𝐱𝑡{\mathbf{x}}_{t}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and 𝐱t+Δ⁢tsubscript𝐱𝑡Δ𝑡{\mathbf{x}}_{t+\Delta t}bold_x start_POSTSUBSCRIPT italic_t + roman_Δ italic_t end_POSTSUBSCRIPT. Various deep neural network architectures can be used to learn the drift term as well as predicting the trajectory data, using RNN, LSTM, and Transformers, see [19, 38, 35] for detailed discussion. Furthermore, when the noise level becomes a constant, i.e. σ⁢(𝐱)=σ>0𝜎𝐱𝜎0\sigma({\mathbf{x}})=\sigma>0italic_σ ( bold_x ) = italic_σ > 0, we arrive at a much simpler loss ℰℋSimpler⁢(𝒇~)=𝔼⁢[12⁢T⁢σ2⁢(∫t=0T‖𝒇~⁢(𝐱t)‖2⁢d⁡t−2⁢⟨𝒇~⁢(𝐱t),d⁡𝐱t⟩)],superscriptsubscriptℰℋSimpler~𝒇𝔼delimited-[]12𝑇superscript𝜎2superscriptsubscript𝑡0𝑇superscriptnorm~𝒇subscript𝐱𝑡2d𝑡2~𝒇subscript𝐱𝑡dsubscript𝐱𝑡\mathcal{E}_{{\mathcal{H}}}^{\text{Simpler}}(\tilde{\bm{f}})=\mathbb{E}\Big{[}% \frac{1}{2T\sigma^{2}}\big{(}\int_{t=0}^{T}||\tilde{\bm{f}}({\mathbf{x}}_{t})|% |^{2}\,\operatorname{d\!}t-2\langle\tilde{\bm{f}}({\mathbf{x}}_{t}),% \operatorname{d\!}{\mathbf{x}}_{t}\rangle\big{)}\Big{]},caligraphic_E start_POSTSUBSCRIPT caligraphic_H end_POSTSUBSCRIPT start_POSTSUPERSCRIPT Simpler end_POSTSUPERSCRIPT ( over~ start_ARG bold_italic_f end_ARG ) = blackboard_E [ divide start_ARG 1 end_ARG start_ARG 2 italic_T italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ( ∫ start_POSTSUBSCRIPT italic_t = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT | | over~ start_ARG bold_italic_f end_ARG ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_OPFUNCTION roman_d end_OPFUNCTION italic_t - 2 ⟨ over~ start_ARG bold_italic_f end_ARG ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , start_OPFUNCTION roman_d end_OPFUNCTION bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ⟩ ) ] , which has been investigated in [22] in combination of high-dimensional 𝐱𝐱{\mathbf{x}}bold_x with a special structure in 𝒇𝒇{\bm{f}}bold_italic_f, the drift term. The uniqueness of our method is that we incorporate the covariance matrix into the learning and hence improving the estimation especially when the noise is correlated. 1.2 Contributions of this Work In this paper, we develop a novel noise guided trajectory based learning approach to infer the governing structures of SDEs from observation data. Our method has contributed to the following aspects • We develop a novel noise guided trajectory based learning method that can discover the governing structures of SDEs from data, including both the drift and the noise terms. Our method takes the noise into the consideration of the learning procedure and focuses on the overall evolution of the trajectory instead of focusing on one particular time point. • We investigate the stability, accuracy and efficiency of our learning method over various kinds of SDEs with different noise structures. We showcase the superior performance of our algorithm using these examples. • We allow the noise to have different structures. 1.3 Structure The remainder of the paper is structured as follows. Section 2 outlines the framework we use to learn the drift term and the noise structure. We demonstrates the effectiveness of our learning by testing it on various cases summarized in section 3. We conclude our paper in section 4 with a few pointers for ongoing and future developments."
https://arxiv.org/html/2411.00657v1,Fast Spectrum Estimation of Some Kernel Matrices††thanks:Preprint. The research of Mikhail Lepilov was supported in part by NSF grant DMS-2038118.,"In data science, individual observations are often assumed to come independently from an underlying probability space. Kernel matrices formed from large sets of such observations arise frequently, for example during classification tasks. It is desirable to know the eigenvalue decay properties of these matrices without explicitly forming them, such as when determining if a low-rank approximation is feasible. In this work, we introduce a new eigenvalue quantile estimation framework for some kernel matrices. This framework gives meaningful bounds for all the eigenvalues of a kernel matrix while avoiding the cost of constructing the full matrix. The kernel matrices under consideration come from a kernel with quick decay away from the diagonal applied to uniformly-distributed sets of points in Euclidean space of any dimension. We prove the efficacy of this framework given certain bounds on the kernel function, and we provide empirical evidence for its accuracy. In the process, we also prove a very general interlacing-type theorem for finite sets of numbers. Additionally, we indicate an application of this framework to the study of the intrinsic dimension of data, as well as several other directions in which to generalize this work.","Background Kernel matrices that result from applying a positive-definite function pairwise to a finite set of points X⊆ℝd𝑋superscriptℝ𝑑X\subseteq\mathbb{R}^{d}italic_X ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT arise in several areas of computational mathematics such as image processing and machine learning. In the latter field especially, common methods involve performing expensive computations with a kernel matrix, such as inverting it or finding its eigenvalues [18, 19]. The kernel matrix involved, however, may be of a prohibitively large size to even form, let alone to do computations with. On the other hand, if the matrix has quick eigenvalue decay relative to its norm, then we may be able to efficiently carry out computations on its low-rank approximation instead. A good overview of such computations and their complexity is found in [6]. Hence, it is useful to study a priori the eigenvalue decay of a kernel matrix. Given the n𝑛nitalic_n data points with which the kernel matrix is formed, we would like to find ways to estimate all of its eigenvalues faster than by having to form the matrix first. That is, we would like to do so in a sub-quadratic number of operations relative to n𝑛nitalic_n. We consider a setting common in data science, which is when the points in X𝑋Xitalic_X are assumed to be independent and identically-distributed, coming from some latent distribution. In the past, the study of eigenvalue decay of such kernel matrices often focused on asymptotic eigenvalue behavior as the number of distribution samples in X𝑋Xitalic_X was taken to infinity, after making some appropriate assumptions on the distribution and kernel function involved [15, 4]. However, as the examples in [4] suggest, these bounds rely on the kernel function having its truncated eigendecomposition (in some appropriate function space) readily available. Furthermore, it is unclear exactly how many terms to keep when computing and truncating such an eigendecomposition in order to obtain an eigenvalue decay bound within some tolerance. Thus, it is impractical to use such ideas for our purposes of estimating eigenvalue decay of a given kernel matrix. These difficulties are sidestepped when empirical methods are used to obtain bounds on eigenvalues, such as matrix sketching. However, most sketching techniques typically require not only forming the kernel matrix but also finding matrix-vector products with sets of specially-crafted vectors. For some examples and an overview, see [22, 20]. Such techniques applied to an n×n𝑛𝑛n\times nitalic_n × italic_n matrix, therefore, would require a number of operations that scales at least quadratically in n𝑛nitalic_n, so most sketching approaches do not provide a way to achieve our goal. One exception is the class of techniques known as Nyström methods, which can be thought of as matrix sketching methods that do not require forming the entire kernel matrix. In Nyström methods, a random subsample of the points in X𝑋Xitalic_X, and hence of the kernel matrix, is used to obtain a low-rank decomposition of the full matrix. The spectrum of this randomly-subsampled matrix is shown to be correlated pointwise with the first few eigenvalues of the full matrix [21]. Various strategies for sampling the matrix and obtaining theoretical pointwise accuracy guarantees for this correlation have been implemented over the years. Such guarantees depend on performing additional computations with the data points informing the choice of samples; see, for example, [8]. An in-depth empirical exploration of such guarantees, and especially their limitations, is given in [12]. However, since the goal of such methods is to find the best low-rank approximation, and not to find whether or not a good such approximation exists, these accuracy guarantees only apply to give eigenvalue estimates for the first few eigenvalues. Furthermore, in practice, the low-cost “naive” Nyström method of [21] actually does not work to give a subsampled matrix with similar eigenvalues if the matrix has high numerical rank; see Figure 1.1 for an illustration of this phenomenon. Magnitude Magnitude Index of eigenvalueMagnitude Figure 1.1: The first 100 eigenvalues of the kernel matrix (blue) formed when X𝑋Xitalic_X consists of 512 points taken from the standard uniform distribution in one dimension, as well as those of its “naive” Nyström approximation (red) with 32 points. Here, the kernel used is κ⁢(x,y)=exp⁢(−10⁢(x−y)2)𝜅𝑥𝑦exp10superscript𝑥𝑦2\kappa(x,y)=\mathrm{exp}(-10(x-y)^{2})italic_κ ( italic_x , italic_y ) = roman_exp ( - 10 ( italic_x - italic_y ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) (top figure), κ⁢(x,y)=e⁢x⁢p⁢(−100⁢(x−y)2)𝜅𝑥𝑦𝑒𝑥𝑝100superscript𝑥𝑦2\kappa(x,y)=exp(-100(x-y)^{2})italic_κ ( italic_x , italic_y ) = italic_e italic_x italic_p ( - 100 ( italic_x - italic_y ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) (middle figure), and κ⁢(x,y)=e⁢x⁢p⁢(−10000⁢(x−y)2)𝜅𝑥𝑦𝑒𝑥𝑝10000superscript𝑥𝑦2\kappa(x,y)=exp(-10000(x-y)^{2})italic_κ ( italic_x , italic_y ) = italic_e italic_x italic_p ( - 10000 ( italic_x - italic_y ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) (bottom figure). It is evident that, in the top figure, the eigenvalue decay of the subsampled matrix corresponds well with the eigenvalue decay of the full matrix, but in the center and especially bottom figures, this is no longer the case. This indicates that the Nyström method only works to give an estimate of numerical rank if we know a priori that it is low for our given kernel matrix, as in the top figure. Even more recently, related work comes from approximating graph spectra in subquadratic time, such as in [2, 5]. In this approach, the kernel matrix can be regarded as the Laplacian of a particular weighted complete graph. Specifically, each vertex corresponds to one point, and each edge has weight equal to the kernel evaluated at the points corresponding to the vertices that the edge connects. Methods based on this are different from Nyström methods and instead give bounds in the Wasserstein-1 metric, often referred to as the “earth-mover distance.” From this, however, it is difficult to obtain pointwise estimates of the matrix spectrum. The reference [2] does contain such estimates for the first few eigenvalues but not for the later eigenvalues. Finally, something close to being fit for our purpose may be found in [3]. This is a result for general symmetric matrices that, in its basic form, gives additive bounds unrelated to the magnitude of each eigenvalue for the later eigenvalues. This makes controlling errors difficult for the later eigenvalues, and it prevents us from using the approach if the numerical rank of the matrix is not already low. Hence, to obtain accurate pointwise estimates for all the eigenvalues of a given kernel matrix in subquadratic time, we must find a new empirical approach that avoids the issues of the methods above. To do so, we first note that all of the methods we mention so far use no more information than just the fact that the matrix is symmetric. Thus, using more information about the distribution underlying X𝑋Xitalic_X, as well as the kernel involved in forming the matrix, may enable us to find a better approximation for its spectrum. Our contribution In this work, we use this information to design a fundamentally new eigenvalue estimation technique based on finding bounds for the expected k𝑘kitalic_k quantiles of the eigenvalue distribution of a kernel matrix, for the case that k≪nmuch-less-than𝑘𝑛k\ll nitalic_k ≪ italic_n. This is done, in turn, by matching the moments of this eigenvalue distribution with that of a smaller, k×k𝑘𝑘k\times kitalic_k × italic_k matrix formed specifically for the purpose. Empirically, it turns out that this technique works precisely when the kernel in question has quick decay away from the diagonal, which corresponds to the case that the matrix is of high numerical rank. This complements the existing methods mentioned above, which do not give good accuracy guarantees in such cases (again, see Figure 1.1). Although it is true that in the case of a one-dimensional kernel, such matrices may be approximated by banded matrices, this is no longer the case when X𝑋Xitalic_X is in Euclidean space of moderate or high dimension. Our framework, on the other hand, still applies even in the moderate- or high-dimensional setting. This new framework requires O⁢(m⁢k2)𝑂𝑚superscript𝑘2O(mk^{2})italic_O ( italic_m italic_k start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) computations, where m𝑚mitalic_m is a constant that depends on the desired approximation accuracy. Thus, for certain distributions giving rise to X𝑋Xitalic_X and kernels used to compute A𝐴Aitalic_A, our new framework allows for the only subquadratic method to find bounds on the later eigenvalues of the resulting kernel matrix, after a preprocessing step that does not depend on the matrix or kernel. In addition, since this is an entirely new approach, it provides a natural set of questions for further study that could allow subquadratic eigenvalue estimates for wider classes of kernel matrices. Along the way, we also show a very general result concerning the interlacing of sets of real numbers which, to our knowledge, has never been shown before. Finally, we propose an application of this work to the problem of finding the so-called intrinsic dimension of a dataset. The rest of the paper is structured as follows: in Section 2, we detail our approach. In the process, we prove several new results that show its efficacy in kernel matrix eigenvalue quantile estimation. Among these results is the aforementioned new, general interlacing result about finite sets of real numbers. In Section 3, we give some numerical experiments showing the strengths and limitations of our new framework. Finally, in Section 4, we pose a number of questions for further study that could improve the framework. We also suggest an application to the problem of dimension reduction in data science. Throughout the paper, we use the following notation. Let d,n∈ℕ𝑑𝑛ℕd,n\in\mathbb{N}italic_d , italic_n ∈ blackboard_N, and let X⊆ℝd𝑋superscriptℝ𝑑X\subseteq\mathbb{R}^{d}italic_X ⊆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT with |X|=n𝑋𝑛|X|=n| italic_X | = italic_n. Let κ:ℝd×ℝd→ℝ:𝜅→superscriptℝ𝑑superscriptℝ𝑑ℝ\kappa:\mathbb{R}^{d}\times\mathbb{R}^{d}\to\mathbb{R}italic_κ : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT × blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT → blackboard_R be a symmetric, positive-definite function. Fix an indexing X={x1,…,xn}𝑋subscript𝑥1…subscript𝑥𝑛X=\{x_{1},\ldots,x_{n}\}italic_X = { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }. By κ⁢(X,X)𝜅𝑋𝑋\kappa(X,X)italic_κ ( italic_X , italic_X ), we mean the kernel matrix A∈ℝn×n𝐴superscriptℝ𝑛𝑛A\in\mathbb{R}^{n\times n}italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT with entries Ai⁢j=κ⁢(xi,xj)subscript𝐴𝑖𝑗𝜅subscript𝑥𝑖subscript𝑥𝑗A_{ij}=\kappa(x_{i},x_{j})italic_A start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_κ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ). For a symmetrix matrix A∈ℝn×n𝐴superscriptℝ𝑛𝑛A\in\mathbb{R}^{n\times n}italic_A ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT and some 1≤j≤n1𝑗𝑛1\leq j\leq n1 ≤ italic_j ≤ italic_n, we denote by σj⁢(A)subscript𝜎𝑗𝐴\sigma_{j}(A)italic_σ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ( italic_A ) the j𝑗jitalic_jth largest eigenvalue of A𝐴Aitalic_A. Finally, for a,b∈ℝ𝑎𝑏ℝa,b\in\mathbb{R}italic_a , italic_b ∈ blackboard_R with a≤b𝑎𝑏a\leq bitalic_a ≤ italic_b, we denote by U⁢[a,b]𝑈𝑎𝑏U[a,b]italic_U [ italic_a , italic_b ] the uniform distribution on the interval [a,b]𝑎𝑏[a,b][ italic_a , italic_b ]."
https://arxiv.org/html/2411.00482v1,On the required number of electrodes for uniqueness and convex reformulation in an inverse coefficient problem,"We introduce a computer-assisted proof for uniqueness and global reconstruction for the inverse Robin transmission problem, where the corrosion function on the boundary of an interior object is to be determined from current-voltage measurements on the boundary of an outer domain. We consider the shunt electrode model where, in contrast to the standard Neumann boundary condition, the applied electrical current is only partially known. The aim is to determine the corrosion coefficient with a finite number of measurements.In this paper, we present a numerically verifiable criterion that ensures unique solvability of the inverse problem, given a desired resolution. This allows us to explicitly determine the required number and position of the required electrodes. Furthermore, we will present an error estimate for noisy data. By rewriting the problem as a convex optimisation problem, our aim is to develop a globally convergent reconstruction algorithm. The results extend the recent research in [1] to the shunt electrode model and establish a more precise criterion for unique solvability.","In our study, we focus on non-destructive impedance-based corrosion detection that holds the challenge of reconstructing an unknown Robin transmission coefficient on a known interior boundary from current-voltage measurements taken at electrodes attached to the outer boundary. In this work, we aim to prove uniqueness and global reconstruction in a non-linear inverse elliptic coefficient problem. To the best knowledge of the authors, this work presents the first method for explicitly calculating the required number of electrodes for a resolution guarantee in a non-linear spatially dependent coefficient reconstruction problem. Practitioners usually attempt to solve such reconstruction problems with a regularized data fitting approach. The non-convexity of the residual function presents a significant challenge for nonlinear inverse problems, which is why, in general, only a few global reconstruction algorithms are known. In [1, 2], a novel approach to overcome non-convexity and achieve global convergence is presented for the idealized model, where standard Neumann boundary conditions are considered. Our study translates these results to the shunt electrode model, which extends the standard Neumann boundary problem to a realistic electrode model, where the applied electrical current is only partially known. We provide a criterion to check whether the number of electrodes is sufficient for unique solvability of the inverse problem and provide an equivalent reformulation by rewriting the problem as a convex semidefinite program. We are able to explicitly determine the required number of electrodes for the reconstruction guarantee of the unknown Robin parameter. By evaluating a finite number of forward solutions, and their derivatives, the criterion can be explicitly and numerically verified for a given desired resolution, so the proof for uniqueness and global reconstruction is computer-assisted. Additionally, the criterion provides explicit error estimates for noisy data. Finally, we genrealize the criterion for unique solvability in [1] to sharper assumptions. The Calderón problem cf. [3], is the problem of reconstructing information about the interior of a domain from measurements at its boudary. Traditionally, a prominent application of the Calderón problem is electrical impedance tomography (EIT), see eg. [4] for an overview. In EIT, one seeks to reconstruct information from a finite number of current-voltage measurements. There are recent theoretical achievements in global reconstruction of the unknown conductivity parameter with infinitely many measurements cf. [5, 6]. However, there are only very few achievements in global reconstruction approaches with finitely many measurements cf. [7, 8, 9, 10] and [11] for the fractional case. The Robin problem, that is considered in [1], is a special case where one seeks to reconstruct a parameter on a small subdomain. For the Robin case theoretical results, such as Runge approximation and the technique of localized potentials, hold in general eg. cf. [12]. Theoretical investigations of uniqueness for the Robin case have mainly focused on the idealized infinite-dimensional setting, where the unknown coefficient function is to be determined with infinite resolution from infinitely many measurements as in [12]. The works [1, 2] address the practically relevant scenario of a finite resolution and finitely many measurements, where a measurement corresponds to measuring the exact Dirichlet data from applied Neumann boundary values. In [12], the authors have established Lipschitz stability results for cases involving finitely many unknowns and infinitely many measurements. In this work, we consider the Robin problem with a realistic electrode model, where electrodes are attached to the outer boundary of the domain (see [13]). There are different types of electrode models, most prominently the complete electrode model. We consider a slightly simplified model, the shunt model, where the impedance between the electrodes and the body is neglected. In [14, 15, 16, 17], some theoretical fundations have been layed. In [9, 18, 19, 20] the authors aim to approximate the idealized model with electrode models and hereby translate the theoretical results to the electrode model case. However, in EIT little is known about characterizing the required number of electrodes to achieve a desired resolution. Figure 1: The domain Ω=Ω1∪Ω2ΩsubscriptΩ1subscriptΩ2\Omega=\Omega_{1}\cup\Omega_{2}roman_Ω = roman_Ω start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∪ roman_Ω start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT."
https://arxiv.org/html/2411.00017v1,Applying Data Driven Decision Making to rank Vocational and Educational Training Programs with TOPSIS,In this paper we present a multi-criteria classification of Vocational and Educational Programs in Extremadura (Spain) during the period 2009-2016. This ranking has been carried out through the integration into a complete database of the detailed information of individuals finishing such studies together with their labor data. The multicriteria method used is TOPSIS together with a new decision support method for assessing the influence of each criterion and its dependence on the weights assigned to them. This new method is based on a worst-best case scenario analysis and it is compared to a well known global sensitivity analysis technique based on the Pearson’s correlation ratio.,"The 2008 financial crisis that hit the worldﬂs economies has had a particularly acute impact in Spain (Guardiola and Guillen-Royo, 2015). It is only since 2014 that Spain seemed to begin its recovery (Martí and Pérez, 2015). However, this recuperation is still far to be acceptable with regard to the labor landscape (Casares and Vázquez, 2018). One of the main Spanish weaknesses that the crisis exposed was the so-called duality of the labor market. Thus, Spain is characterized by the existence of two very different types of workers. On one hand, long term workers on indefinite contracts, having both a very high job security and a very high cost for companies (especially in terms of dismissals) and usually with university studies even for jobs that do not require them. On the other hand, short term workers on temporary contracts or seasonal contracts with low wages and, in most cases, with very little training. Another structural weakness of the Spanish economy unveiled during the years of the crisis was the fact that it had been relied heavily on two pillars: construction and tourism (and their associated services). This productive model had its main Achilles heel in the low level studies required in many of the jobs created in both sectors. Moreover, the relatively high wages that a worker could earn before the crisis, mainly in construction, led many young people to abandon their studies to work in these industries, without prior quality training. When the crisis arose and the destruction of employment reached unprecedented levels, Spain discovered that had to deal with a mass of unemployed, mostly young, people who, having no adequate training, had very difficult or even impossible reinstatement into the labor market. This problem has been most pronounced in some regions of Spain as Extremadura. Extremadura is a European Union Objective 1 region located in western Spain that according to the Eurostat Regional Yearbook 2018111https://cutt.ly/as19ww2, its GDP per inhabitant in relation to the EU-28 average is 61.47%, it has 23.7% of unemployment rate and, even worse, its early leavers from education and training of young people rate is 20.9% and its young people neither in employment nor in education or training rate is 20%. In other countries where the economic model was more diversified, with large sectors of skilled employment and better trained workers, the crisis was less intense, the employment destruction less acute, and the recovery was faster. One of the differences between Spain and, particularly, Extremadura with respect to those countries is the importance they give to Vocational Eucation and Training (hereinafter VET). For the European Union, VET should “prepare young people for entering and successfully and sustainably participating in the labor markets as well as to enable high potentials (e.g. migrants, refugees, low-skilled and unemployed, inactive groups, including women) to stay and/or (re-)enter the labor market” (Advisory Committee on Vocational Training, 2018). In Germany, for example, VET studies are closely linked to the labor market, so that the majority of VET students are also trained in companies where, in many cases, they end up working. This means that there is a quarry of workers with a specific qualification for the needs of the labor market. However, in the collective imagination of Spanish families, VET has been considered for decades a second-rate training and has not been much appreciated. This vision, together with the high drop-out rates, has caused a very marked duality in training: on the one hand people who either did not finish more than compulsory education or, at best, have VET studies (which in this last case are seen wrongly as a low level education), or people with university education who, due to the high unemployment rates suffered in Spain during the crisis (still persisting), are hired in positions for which such studies are not really required, causing another of the many big problems of the Spanish labor market, which is the overqualification of workers (Flisi et al., 2017). In order to try to alleviate the mentioned problems, the Government of Extremadura, providing its historical data, asked for a scientific analysis about the real impact of VET studies on accessing to the labor market with a two-fold goal: increase the resources of those VET studies with higher employment rate and promote such studies among their unemployed citizens enhancing the image of the VET studies that really help to get a job. To this end, the aim of this work is to determine the efficiency of VET studies and the evaluation of the performance of VET graduates from Extremadura in the different degrees of VET programs in the labor market. Thus, we illustrate a data-driven multi-criteria decision-making methodology with the aim of classifying the different degrees of VET programs according to some criteria related to labor insertion. Concretely, we have applied TOPSIS (Technique for Order Performance by Similarity to Ideal Solution) (Hwang and Yoon, 1981) to this problem. TOPSIS is a well-known classical MCDM method, widely used by researchers and practitioners, that supports decision makers in performing analysis, comparisons and rankings to select the best alternative using a finite number of criteria. Moreover, since all criteria are weighted, their importance may be modified providing, thus, the flexibility for creating different rankings prioritizing different aspects. In our case, we have considered 8 different criteria whose values have been computed from a real dataset with more than 28000 VET student records containing both educational and labor information. This is a key contribution for this work since the information obtained is enriched by real data instead of being based on questionnaires as similar TOPSIS approaches (Rad et al., 2011) or the REFLEX project in the field of higher education (Allen and Van Der Velden, 2006). Furthermore, to the best of our knowledge, the relationship between VET programs and labor market has not been explored by previous works with such level of detail. The rest of the article is organized as follows. Section 2 reviews some works related with predicting different outcomes using academic data, and some other works using TOPSIS as Multi-Criteria Decision Analysis method together with a weight assignment analysis, on different scientific domains. Section 3 describes the datasets used and the process applied to them for computing the different data used in our study. Section 4 describes the methodology followed to apply TOPSIS. Section 5 explains the influence of the criteria applied during the process. The results obtained and further considerations are detailed in Section 6. Finally, Section 7 concludes the paper."
https://arxiv.org/html/2411.00011v1,Solving the 2D Advection-Diffusion Equation using Fixed-Depth Symbolic Regression and Symbolic Differentiation without Expression Trees,"This paper presents a novel method for solving the 2D advection-diffusion equation using fixed-depth symbolic regression and symbolic differentiation without expression trees. The method is applied to two cases with distinct initial and boundary conditions, demonstrating its accuracy and ability to find approximate solutions efficiently. This framework offers a promising, scalable solution for finding approximate solutions to differential equations, with the potential for future improvements in computational performance and applicability to more complex systems involving vector-valued objectives.","1.1 Symbolic Regression Symbolic regression (SR) is a method that seeks a symbolic model f⁢(x→)𝑓→𝑥f\left(\vec{x}\right)italic_f ( over→ start_ARG italic_x end_ARG ) to predict a (typically scalar) label y𝑦yitalic_y based on an N-dimensional feature vector x→→𝑥\vec{x}over→ start_ARG italic_x end_ARG which minimizes a loss metric ℒ⁢(f⁢(x→),y)ℒ𝑓→𝑥𝑦\mathcal{L}\left(f\left(\vec{x}\right),y\right)caligraphic_L ( italic_f ( over→ start_ARG italic_x end_ARG ) , italic_y ) [22]. The nodes of the expression f𝑓fitalic_f usually fall into one of the following three categories [6]: • Unary operators: Any operator of arity 1, such as cos\cosroman_cos, sin\sinroman_sin, exp\exproman_exp, ln\lnroman_ln, tanh\tanhroman_tanh, etc. • Binary operators: Any operator of arity 2, such as +++, −--, ∗*∗, ÷\div÷, etc. • Leaf Nodes: Any of the individual features x→={x1,x2,…,xN}→𝑥subscript𝑥1subscript𝑥2…subscript𝑥𝑁\vec{x}=\{x_{1},x_{2},\ldots,x_{N}\}over→ start_ARG italic_x end_ARG = { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } and constant tokens that can potentially be optimized with non-linear optimization routines such as L-BFGS [5] or Levenberg-Marquadt [13] [15]. The models distilled by symbolic regression are inherently interpretable compared to other machine learning methods, which is advantageous in many disciplines [6] such as health, law, the natural sciences, etc. In the context of differential equations, the optimization problem can be framed as finding a model f^^𝑓\hat{f}over^ start_ARG italic_f end_ARG such that f^=argminf^∈ℱ⁢𝒟⁢(f^)⁢ subject to ⁢ℐ,ℬ→^𝑓argminfragments^𝑓F𝒟^𝑓 subject to ℐ→ℬ\hat{f}=\begin{tabular}[t]{@{}c@{}}\text{argmin}\\[-3.00003pt] \scriptsize$\hat{f}\in\mathcal{F}$\end{tabular}\;\mathcal{D}(\hat{f})\;\text{ % subject to }\;\mathcal{I},\vec{\mathcal{B}}over^ start_ARG italic_f end_ARG = start_ROW start_CELL roman_argmin end_CELL end_ROW start_ROW start_CELL over^ start_ARG italic_f end_ARG ∈ caligraphic_F end_CELL end_ROW caligraphic_D ( over^ start_ARG italic_f end_ARG ) subject to caligraphic_I , over→ start_ARG caligraphic_B end_ARG (1) where ℱℱ\mathcal{F}caligraphic_F is the space of all possible symbolic models under consideration, 𝒟𝒟\mathcal{D}caligraphic_D is a differential operator representing the differential equation 𝒟⁢(f)=0𝒟𝑓0\mathcal{D}(f)=0caligraphic_D ( italic_f ) = 0 (f𝑓fitalic_f being the true model), ℐℐ\,\mathcal{I}caligraphic_I is the initial condition, and ℬ→→ℬ\vec{\mathcal{B}}over→ start_ARG caligraphic_B end_ARG are the boundary conditions. 1.2 Symbolic Differentiation Symbolic differentiation is a computer algebra approach to solving derivatives analytically [8] through techniques such as the chain, product, and quotient rules. Symbolic differentiation falls under the umbrella of “computer algebra” technology [26], which modern-day software such as Mathematica [11], SymPy [17], Maxima [16] and many others have made generally available. However, as remarked in [21], many symbolic differentiation software rely on creating tree data structures for the input formulae and the corresponding derivatives. While the tree data structure enables versatility, it is not ideal for symbolic regression, which requires minimal memory allocations and maximal speed for optimal performance [27]. Thus, in this paper, the symbolic differentiation method detailed in [21], as well as the in-situ simplification routines detailed therein, are implemented. Furthermore, the analogous differentiation techniques of [21] using Polish notation (prefix) expressions (as opposed to Reverse Polish notation (postfix)) are also implemented herein. The array-based method of [21] reduces memory allocation cost as no reallocations are needed for creating expression trees, and, since expressions are represented in this paper using prefix/postfix notation (using the grammars and algorithms developed in [7]), the conversion from infix to prefix/postfix is also eliminated, making the method developed in this paper highly efficient. 1.3 2D Advection Diffusion Equation The two-dimensional advection-diffusion equation: ∂T∂t+∇⋅(u→⁢T)=κ⁢∇⋅(∇T)𝑇𝑡⋅∇→𝑢𝑇⋅𝜅∇∇𝑇\displaystyle\frac{\partial T}{\partial t}+\nabla\cdot\left(\vec{u}T\right)=% \kappa\nabla\cdot\left(\nabla T\right)divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_t end_ARG + ∇ ⋅ ( over→ start_ARG italic_u end_ARG italic_T ) = italic_κ ∇ ⋅ ( ∇ italic_T ) ∂T∂t+(∂∂x⁢x^+∂∂y⁢y^)⋅(T⁢ux⁢x^+T⁢uy⁢y^)=κ⁢(∂∂x⁢x^+∂∂y⁢y^)⋅(∂T∂x⁢x^+∂T∂y⁢y^)𝑇𝑡⋅𝑥^𝑥𝑦^𝑦𝑇subscript𝑢𝑥^𝑥𝑇subscript𝑢𝑦^𝑦⋅𝜅𝑥^𝑥𝑦^𝑦𝑇𝑥^𝑥𝑇𝑦^𝑦\displaystyle\frac{\partial T}{\partial t}+\left(\frac{\partial}{\partial x}% \hat{x}+\frac{\partial}{\partial y}\hat{y}\right)\cdot\left(Tu_{x}\hat{x}+Tu_{% y}\hat{y}\right)=\kappa\left(\frac{\partial}{\partial x}\hat{x}+\frac{\partial% }{\partial y}\hat{y}\right)\cdot\left(\frac{\partial T}{\partial x}\hat{x}+% \frac{\partial T}{\partial y}\hat{y}\right)divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_t end_ARG + ( divide start_ARG ∂ end_ARG start_ARG ∂ italic_x end_ARG over^ start_ARG italic_x end_ARG + divide start_ARG ∂ end_ARG start_ARG ∂ italic_y end_ARG over^ start_ARG italic_y end_ARG ) ⋅ ( italic_T italic_u start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT over^ start_ARG italic_x end_ARG + italic_T italic_u start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT over^ start_ARG italic_y end_ARG ) = italic_κ ( divide start_ARG ∂ end_ARG start_ARG ∂ italic_x end_ARG over^ start_ARG italic_x end_ARG + divide start_ARG ∂ end_ARG start_ARG ∂ italic_y end_ARG over^ start_ARG italic_y end_ARG ) ⋅ ( divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_x end_ARG over^ start_ARG italic_x end_ARG + divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_y end_ARG over^ start_ARG italic_y end_ARG ) ∂T∂t+∂∂x⁢(T⁢ux)+∂∂y⁢(T⁢uy)=κ⁢(∂2T∂x2+∂2T∂y2)𝑇𝑡𝑥𝑇subscript𝑢𝑥𝑦𝑇subscript𝑢𝑦𝜅superscript2𝑇superscript𝑥2superscript2𝑇superscript𝑦2\displaystyle\frac{\partial T}{\partial t}+\frac{\partial}{\partial x}\left(Tu% _{x}\right)+\frac{\partial}{\partial y}\left(Tu_{y}\right)=\kappa\left(\frac{% \partial^{2}T}{\partial x^{2}}+\frac{\partial^{2}T}{\partial y^{2}}\right)divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_t end_ARG + divide start_ARG ∂ end_ARG start_ARG ∂ italic_x end_ARG ( italic_T italic_u start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) + divide start_ARG ∂ end_ARG start_ARG ∂ italic_y end_ARG ( italic_T italic_u start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) = italic_κ ( divide start_ARG ∂ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_T end_ARG start_ARG ∂ italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG ∂ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_T end_ARG start_ARG ∂ italic_y start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) ∂T∂t+∂∂x⁢(T⁢ux)+∂∂y⁢(T⁢uy)−κ⁢(∂2T∂x2+∂2T∂y2)=0,𝑇𝑡𝑥𝑇subscript𝑢𝑥𝑦𝑇subscript𝑢𝑦𝜅superscript2𝑇superscript𝑥2superscript2𝑇superscript𝑦20\displaystyle\frac{\partial T}{\partial t}+\frac{\partial}{\partial x}\left(Tu% _{x}\right)+\frac{\partial}{\partial y}\left(Tu_{y}\right)-\kappa\left(\frac{% \partial^{2}T}{\partial x^{2}}+\frac{\partial^{2}T}{\partial y^{2}}\right)=0,divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_t end_ARG + divide start_ARG ∂ end_ARG start_ARG ∂ italic_x end_ARG ( italic_T italic_u start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT ) + divide start_ARG ∂ end_ARG start_ARG ∂ italic_y end_ARG ( italic_T italic_u start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) - italic_κ ( divide start_ARG ∂ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_T end_ARG start_ARG ∂ italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG ∂ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_T end_ARG start_ARG ∂ italic_y start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) = 0 , (2) is frequently used to model heat transfer and diffusion, as well as the movement of gases and fluids through various media [4]. In the case of constant u→→𝑢\vec{u}over→ start_ARG italic_u end_ARG and κ𝜅\kappaitalic_κ, analytical solutions are known [4] [28]. In this work, two cases of constant κ𝜅\kappaitalic_κ and uxsubscript𝑢𝑥u_{x}italic_u start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT, uysubscript𝑢𝑦u_{y}italic_u start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT constant in x𝑥xitalic_x, y𝑦yitalic_y, respectively, are considered, which means that equation 2 can be rewritten as ∂T∂t+ux⁢∂T∂x+uy⁢∂T∂y−κ⁢(∂2T∂x2+∂2T∂y2)=0𝑇𝑡subscript𝑢𝑥𝑇𝑥subscript𝑢𝑦𝑇𝑦𝜅superscript2𝑇superscript𝑥2superscript2𝑇superscript𝑦20\frac{\partial T}{\partial t}+u_{x}\frac{\partial T}{\partial x}+u_{y}\frac{% \partial T}{\partial y}-\kappa\left(\frac{\partial^{2}T}{\partial x^{2}}+\frac% {\partial^{2}T}{\partial y^{2}}\right)=0divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_t end_ARG + italic_u start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_x end_ARG + italic_u start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT divide start_ARG ∂ italic_T end_ARG start_ARG ∂ italic_y end_ARG - italic_κ ( divide start_ARG ∂ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_T end_ARG start_ARG ∂ italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG + divide start_ARG ∂ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_T end_ARG start_ARG ∂ italic_y start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) = 0 (3) In this work, an approximate analytical equation for T𝑇Titalic_T is sought such that 3, subject to two different ℐℐ\mathcal{I}caligraphic_I and ℬ→→ℬ\vec{\mathcal{B}}over→ start_ARG caligraphic_B end_ARG is approximately satisfied."
