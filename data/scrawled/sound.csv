URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04366v1,The Concatenator: A Bayesian Approach To Real Time Concatenative Musaicing,"We present ‚ÄúThe Concatenator,‚Äù a real time system for audio-guided concatenative synthesis. Similarly to Driedger et al.‚Äôs ‚Äúmusaicing‚Äù (or ‚Äúaudio mosaicing‚Äù) technique, we concatenate a set number of windows within a corpus of audio to re-create the harmonic and percussive aspects of a target audio stream. Unlike Driedger‚Äôs NMF-based technique, however, we instead use an explicitly Bayesian point of view, where corpus window indices are hidden states and the target audio stream is an observation. We use a particle filter to infer the best hidden corpus states in real-time. Our transition model includes a tunable parameter to control the time-continuity of corpus grains, and our observation model allows users to prioritize how quickly windows change to match the target. Because the computational complexity of the system is independent of the corpus size, our system scales to corpora that are hours long, which is an important feature in the age of vast audio data collections. Within The Concatenator module itself, composers can vary grain length, fit to target, and pitch shift in real time while reacting to the sounds they hear, enabling them to rapidly iterate ideas. To conclude our work, we evaluate our system with extensive quantitative tests of the effects of parameters, as well as a qualitative evaluation with artistic insights. Based on the quality of the results, we believe the real-time capability unlocks new avenues for musical expression and control, suitable for live performance and modular synthesis integration, which furthermore represents an essential breakthrough in concatenative synthesis technology.","Concatenative synthesis, or audio mosaicing, is a data-driven approach to arrange granular fragments of audio samples, particularly using data sourced from the spectral-temporal features of a target sound. While granular synthesis systems typically rely on combinations of aleatoric parameterization, deterministic automation, and traditional synthesis modulation to achieve complex and evolving textures from sound fragments [28], concatenative synthesis algorithms utilize Music Information Retrieval technology to decide parameters such as the index, amplitude, and pitch of each sound fragment. Modern music producers are inundated by audio data. Services like Splice offer hundreds of thousands of samples readily available on the cloud, and Kontakt multi-sample libraries can often take up over 10gb of disk space to capture a single instrument. Music Producers generate plenty of their own audio data as well: stems, multi-tracks, long-form recordings, and mix variations account for a large portion of many a music producer‚Äôs audio collection. Recent software such as XO by XLN Audio, Sononym, and Ableton Live 12 offer automatic organization of audio files based on various tags and descriptors, but these implementations of MIR technology are more utilitarian than creative in their design and application. Meanwhile, concatenative synthesis options remain sparse since its conceptual inception [31]: Reformer by Krotos is designed to create foley designs, apps like Samplebrain and CataRT [32, 33] are lacking in critical musical areas such as pitch tracking, with the more advanced options having limited accessibility for artists, requiring prior knowledge of Max (FluCoMa, MuBu) or Python (Audioguide). The Concatenator advances concatenative synthesis in 3 major ways: 1) it is capable of accurately reproducing harmonic and percussive sounds using arbitrary corpora 2) in real-time at scale, 3) affording new levels of control and accessibility. Furthermore, unlike neural audio systems [5], it requires no training and can adapt to arbitrary corpora at runtime. The speed, ease, and scope of The Concatenator offers a fresh paradigm for music producers to interact creatively with their ever-expanding excess of audio data, leading to what we believe is a breakthrough in the field."
https://arxiv.org/html/2411.04337v1,Model and Deep learning based Dynamic Range Compression Inversion,"Dynamic Range Compression (DRC) is a popular audio effect used to control the dynamic range of a signal. Inverting DRC can also help to restore the original dynamics to produce new mixes and/or to improve the overall quality of the audio signal. Since, state-of-the-art DRC inversion techniques either ignore parameters or require precise parameters that are difficult to estimate, we fill the gap by combining a model-based approach with neural networks for DRC inversion. To this end, depending on the scenario, we use different neural networks to estimate DRC parameters. Then, a model-based inversion is completed to restore the original audio signal. Our experimental results show the effectiveness and robustness of the proposed method in comparison to several state-of-the-art methods, when applied on two music datasets.","Dynamic Range Compression (DRC) is a fundamental process in audio signal processing which aims at changing the dynamic range of a signal. This technique is widely used in various stages of audio production, such as recording, mixing, and mastering, to control the loudness of an audio signal and prevent clipping or distortion [1]. However, the application of DRC often leads to changes in the audio‚Äôs timbre and perceived quality, making its inversion a challenging task. Thus, inverting DRC is full of interest in the context of audio reverse engineering [2] since it aims at recovering the original dynamic range and audio quality of a signal. This task could find many applications such as signal restoration, remixing, and enhancing creative control. Inverting DRC is a challenging problem which often requires side information with an explicit DRC model and prior knowledge about the DRC parameters to be efficiently processed. There only exist a few studies which directly address the problem of DRC inversion. In [3], the authors consider DRC inversion as a rate-distortion optimization problem using a coder-decoder framework which minimizes both the side-information and the reconstruction error when combined with a specific estimator applied to the compressed signal. In [4], the authors propose a specific DRC model which provides promising reconstruction approximation but require to know exactly the DRC parameters of the compressed signal. Other work such as [5] only attempts to cancel or to reduce the effects of DRC without directly addressing the challenging problem of inversion. More recent inversion methods based on deep learning only consider a specific type of DRC (eg. limiter, distortion, clip) and require for each of them a specific training dataset [6, 7]. In the present work, we propose to revisit the DRC inversion problem by resorting to deep neural network through a new approach which first identifies the mixture configuration to obtain the DRC parameters, and then applies the adequate model-based inversion to restore the original uncompressed signal. Our contributions are manifold. First, we propose a novel technique based on deep learning models which blindly predicts the DRC profile or parameters applied to an audio signal by analyzing the waveform of the compressed signal yùë¶yitalic_y. Second, we propose to use the estimated parameters corresponding to the predicted profile with a modified existing model-based DRC inversion technique first proposed by [4] to restore the original signal xùë•xitalic_x. Finally we compare our proposal with several state-of-the-art methods applied on two public datasets in terms of parameter estimation accuracy, and signal reconstruction quality. Our paper is organized as follows. Section 2 introduces the DRC inversion problem and describes our new approach, including signal processing and model design. Section 3 presents the experimental setup. Section 4 presents the numerical results of these experiments. Finally, Section 5 contains our conclusions and future work."
https://arxiv.org/html/2411.04573v1,Multistage Fine-tuning Strategies for Automatic Speech Recognition in Low-resource Languages,"This paper presents a novel multistage fine-tuning strategy designed to enhance automatic speech recognition (ASR) performance in low-resource languages using OpenAI‚Äôs Whisper model. In this approach we aim to build ASR model for languages with limited digital resources by sequentially adapting the model across linguistically similar languages. We experimented this on the Malasar language, a Dravidian language spoken by approximately ten thousand people in the Western Ghats of South India. Malasar language faces critical challenges for technological intervention due to its lack of a native script and absence of digital or spoken data resources. Working in collaboration with Wycliffe India and Malasar community members, we created a spoken Malasar corpus paired with transcription in Tamil script, a closely related major language. In our approach to build ASR model for Malasar, we first build an intermediate Tamil ASR, leveraging higher data availability for Tamil annotated speech. This intermediate model is subsequently fine-tuned on Malasar data, allowing for more effective ASR adaptation despite limited resources. The multistage fine-tuning strategy demonstrated significant improvements over direct fine-tuning on Malasar data alone, achieving a word error rate (WER) of 51.9%, which is 4.5% absolute reduction when compared to the direct fine-tuning method. Further a WER reduction to 47.3% was achieved through punctuation removal in post-processing, which addresses formatting inconsistencies that impact evaluation. Our results underscore the effectiveness of sequential multistage fine-tuning combined with targeted post-processing as a scalable strategy for ASR system development in low-resource languages, especially where linguistic similarities can be leveraged to bridge gaps in training data.","The Malasar language, spoken by the indigenous Malasar community in the Western Ghats region of southern India, remains stable despite its relatively small speaker base111https://www.ethnologue.com/language/ymr/. According to the 2011 census of India, there are only 9626 Malasar community members, out of which 6431 are in the state of Tamil Nadu and remaining are in the state of Kerala (Chandramouli, 2013). Although the language is not sustained by formal institutions, it continues to thrive in homes and communities where it is passed down to children as the norm. As an oral language without a native script, Malasar is vulnerable to loss as communities diminish or integrate with larger linguistic groups. Documenting and archiving these languages become more complicated without a standardized script. In terms of technology integration, creating systems like automatic speech recognition (ASR) is significantly more difficult. ASR systems typically depend on large, annotated corpora for training, which are hard to generate without a written form of the language. This also affects the development of natural language processing (NLP) tools, such as machine translation and text-to-speech systems, which rely on a text corpus for training and evaluation. In general, the lack of a script poses a challenge for linguistic research. To address the challenges of documenting unwritten languages, UNESCO published a set of guidelines in 2003 (Robinson, 2003). These guidelines explore the processes involved in writing unwritten languages, thereby offering new opportunities for expression and learning to the world‚Äôs linguistic minorities and indigenous people. Malasar shares lexical similarities with Tamil, Malayalam, Muduga, Eravallan, and certain dialects of Irula (Varghese, 2015). The lexical similarity of Malasar with other languages is illustrated in Fig. 1. According to the guidelines (Robinson, 2003), Tamil has been selected as the script for transcribing Malasar speech, facilitating a more consistent and comprehensible representation of the language. Figure 1. Lexical Similarity Of Malasar with other neighbouring languages The development of an ASR system for the Malasar language serves multiple critical purposes in language preservation and documentation efforts. By enabling accurate transcription of spoken Malasar into Tamil script, the system facilitates the creation of valuable linguistic records (Prud‚Äôhommeaux et al., 2021) while supporting detailed analysis of the language‚Äôs phonetic and grammatical features. Beyond documentation, an ASR system drives digital accessibility, fostering language revitalization through modern educational tools and enhanced community engagement. It also empowers governing bodies and strengthens the Malasar community‚Äôs societal integration. In the context of low-resourced languages, such ASR development represents a crucial step toward preserving indigenous knowledge and cultural heritage. To address these specific linguistic and technical hurdles unique to building an ASR system for Malasar, our study adopts a multi-faceted approach: (1) To build an initial corpus from scratch, we collaborated with Wycliff India, resulting in approximately four hours of transcribed Malasar audio data (2) The lack of a native writing system for this oral-only language led us to adopt Tamil script for transcription due to its high lexical similarity, ensuring more accurate representation of Malasar speech (3) To address data scarcity in training an ASR model, we implemented a novel sequential multistage fine-tuning strategy with Whisper, leveraging Tamil as an intermediate fine-tuning stage before adapting the model to Malasar (4) To ensure accurate evaluation, we also introduced a punctuation filter, which significantly improved the reported WER by removing non-linguistic elements that impact the metric (Manohar et al., 2024). In conlusion, the work described in this paper sets a foundation for further NLP applications in this under-resourced language."
https://arxiv.org/html/2411.04158v1,Analyzing Multimodal Features of Spontaneous Voice Assistant Commands for Mild Cognitive Impairment Detection,"Mild cognitive impairment (MCI) is a major public health concern due to its high risk of progressing to dementia. This study investigates the potential of detecting MCI with spontaneous voice assistant (VA) commands from 35 older adults in a controlled setting. Specifically, a command-generation task is designed with pre-defined intents for participants to freely generate commands that are more associated with cognitive ability than read commands. We develop MCI classification and regression models with audio, textual, intent, and multimodal fusion features. We find the command-generation task outperforms the command-reading task with an average classification accuracy of 82%, achieved by leveraging multimodal fusion features. In addition, generated commands correlate more strongly with memory and attention subdomains than read commands. Our results confirm the effectiveness of the command-generation task and imply the promise of using longitudinal in-home commands for MCI detection.","Mild cognitive impairment (MCI) presents a significant public health concern because of its substantial risk of progressing to dementia [1]. Around half of MCI patients develop dementia within 3 years and an annual progression rate of 6%percent66\%6 % to 15%percent1515\%15 % [2]. Early detection of MCI is crucial for preventing or delaying the emergence of dementia [3]. Speech-based dementia detection is promising as a low-cost early-stage screening method [4, 5, 6, 7]. Researchers have explored various approaches for detecting MCI and other cognitive impairments from speech, such as automated phone task [8], picture description task [9, 10], and tablet-based automatic assessment [11]. Previous works mainly focused on the common dataset, including Pitt Corpus and the Cookie Theft Picture description task [12, 13, 9]. Despite three decades of research, this task faces data limitations due to the expensive data collection process [10]. In recent years, an AI-driven Voice Assistant (VA) can be integrated into a wide range of Internet of Things (IoT) devices, such as smart speakers and smartphones. They can engage older adults in conversations, which makes it a practical and convenient tool for collecting speech data and enabling health-related speech-based research [14, 15, 16, 6, 17]. Researchers [18] evaluated VA capabilities in detecting MCI through a command-reading task with a focus on analyzing audio data embeddings. Both MCI and Healthy Controls (HC) participants demonstrated highly similar command outputs when reading from identical instruction sheets of VA commands. We consider the speech dataset collected from such a command-reading task does not require the participant to use much of their memory and attention ability, known as cognitive load [19]. Cognitive load has been proven as an effective metric to be used to assess a patient‚Äôs mental condition [11]. We envision that a task that gives older adults more freedom in the command generation process represents a more realistic VA scenario in their homes and can enhance MCI detection. In this paper, we design a command-generation task through the VA system to enable participants to freely generate commands according to an intent, which may result in subtle changes between MCI and HC. Specifically, the command-generation task provides participants with a set of intent keywords to assist them in generating voice commands. The commands can be formed differently but meant to accomplish the same intent. We propose new intent features and aim to study and compare the effectiveness of the intent features, and fusion of multimodal features extracted from the VA commands and investigate whether the spontaneous VA commands from the command-generation task can achieve enhanced performance in MCI detection. The contributions of our paper are as follows: First, we propose a command-generation task through VA, which is associated with a higher cognitive load, resulting in a better performance in MCI detection than the command-reading task. The average classification accuracy of all features in multi-modality achieved higher results, increased from 73% to 77% for all classifiers, and increased from 78% to 82% using the Random Forest (RF) classifier. Second, we study the correlation between the VA commands and six subdomains of the Montreal Cognitive Assessment (MoCA) [20]. VA commands from the command-generation task exhibit stronger correlations across all subdomains compared to the command-reading task, particularly within memory and attention subdomains. The results show that the command-generation task demands increased attention and short-term memory retention. Third, we compare the performance of intent, audio, textual, and multimodal fusion features on the command-generation task and find that audio and fusion (intent + audio) features achieved better classification accuracy (mean: 75% and 74%, best: 88% and 91%). The results confirm that the intent and audio features are more sensitive to the MCI detection than the textual features. Figure 1: MCI detection model using multimodal features extracted from voice assistant commands (L: Language, O: Orientation, V: Visuospatial, E: Executive function, M: Memory, A: Attention)."
https://arxiv.org/html/2411.03948v1,Long-Form Text-to-Music Generation with Adaptive Prompts: A Case of Study in Tabletop Role-Playing Games Soundtracks,"This paper investigates the capabilities of text-to-audio music generation models in producing long-form music with prompts that change over time, focusing on soundtrack generation for Tabletop Role-Playing Games (TRPGs). We introduce Babel Bardo, a system that uses Large Language Models (LLMs) to transform speech transcriptions into music descriptions for controlling a text-to-music model. Four versions of Babel Bardo were compared in two TRPG campaigns: a baseline using direct speech transcriptions, and three LLM-based versions with varying approaches to music description generation. Evaluations considered audio quality, story alignment, and transition smoothness. Results indicate that detailed music descriptions improve audio quality while maintaining consistency across consecutive descriptions enhances story alignment and transition smoothness.","Recent text-to-audio music generation models such as MusicLM [1] and MusicGen [2] are capable of producing high-quality music in the audio domain that aligns with a given textual description. These models typically generate music autoregressively by predicting the next token from a context window, which limits the size of the signal they can model. While the context size is limited, these models can generate longer signals by sliding a context window through time. Regardless of this capability, they have mainly been evaluated with a fixed prompt and for relatively short music durations. For instance, MusicGen [2] was evaluated considering 30-second music pieces, each generated from a single music description. In this paper, we are interested in evaluating whether text-to-music models can maintain music quality while generating long music pieces, where music descriptions change over time. It is important to evaluate text-to-music models considering long music pieces (greater than 30 seconds, for example) because many music production scenarios involve music durations longer than one can generate with a single short audio context window (e.g., pop music composition, jazz improvisation, soundtrack generation). One key problem of generating long sequences from a small context is that a model has to split the generation into multiple parts, ensuring that the independent parts are smoothly connected in the final composition. Moreover, one might change the initial prompt at any time step, steering the composition in a different direction, and the model must consider both the previous audio context and the new prompt. Figure 1: At every 30 seconds of gameplay, Babel Bardo transcribes the players‚Äô speeches into a text sisubscriptùë†ùëñs_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT using a Speech Recognition system and uses a Large Langue Model (LLM) to map sisubscriptùë†ùëñs_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into a music description disubscriptùëëùëñd_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that matches the scene described by the players. This music description is given to a Text-to-Music system that generates a 30-second piece aisubscriptùëéùëña_{i}italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT directly in the audio domain. In this paper, we investigate long generation with text-to-audio models in the context of Tabletop Role-Playing Games (TRPGs). In this scenario, a music generator takes speech as input and must generate music that matches the story being told by the players. We chose this problem because it inherently poses the challenge of long music generation, where prompts have to change over time to adjust for different story scenes. We also use TRPGs as a research object because TRPG players often enhance their gaming experience by manually selecting songs to play as background music [3], which allows us to compare the results of a generator against a human baseline. To investigate the capabilities of current text-to-music models in generating background music for TRPG stories, we‚Äôve built a system called Babel Bardo, which is inspired by Bardo Composer[4], a system that generates symbolic music by transcribing players‚Äô speeches into text and conditioning an autoregressive model with the emotional tone of this text, as given by an emotion classifier. Different than Bardo Composer, Babel Bardo composes music directly in the audio domain by leveraging a Large Language Model (LLM) to transform the speech transcriptions into music descriptions every 30 seconds of gameplay. These descriptions are then given to a text-to-music model to generate a piece of music for that current moment of the story. Figure 1 shows an overview of our system. Babel Bardo is inspired by Hermann1[5], which uses LLMs and text-to-music models to generate soundtracks for films. We compared four different versions of Babel Bardo in two TRPG campaigns played on YouTube: Call of the Wild (in English) and O Segredo na Ilha (in Brazilian Portuguese). The first version is our baseline and uses the speech transcriptions directly as prompts for a text-to-music model. All other versions use an LLM to transform the transcriptions into music descriptions. The second one follows the Bardo Composer approach and applies an LLM as an emotion classifier. The music description follows a template that is adjusted based on the emotion given by the LLM. The remaining two versions use the LLM to produce a complete music description; however, one generates a new description for every transcript, while the other can just continue the previously generated segment if the scene hasn‚Äôt changed. We evaluated our models according to audio quality, alignment with the story, and transition smoothness between transcriptions. Results suggest that while detailed music descriptions contribute to improved audio quality, maintaining consistency across consecutive descriptions helps achieve smoother transitions between musical segments. Furthermore, our findings indicate that emotion serves as an effective signal for aligning generated music with TRPG narratives."
https://arxiv.org/html/2411.03715v1,MOS-Bench: Benchmarking Generalization Abilities of Subjective Speech Quality Assessment Models,"Subjective speech quality assessment (SSQA) is critical for evaluating speech samples as perceived by human listeners. While model-based SSQA has enjoyed great success thanks to the development of deep neural networks (DNNs), generalization remains a key challenge, especially for unseen, out-of-domain data. To benchmark the generalization abilities of SSQA models, we present MOS-Bench, a diverse collection of datasets. In addition, we also introduce SHEET, an open-source toolkit containing complete recipes to conduct SSQA experiments. We provided benchmark results for MOS-Bench, and we also explored multi-dataset training to enhance generalization. Additionally, we proposed a new performance metric, best score difference/ratio, and used latent space visualizations to explain model behavior, offering valuable insights for future research.","Speech quality assessment (SQA) refers to evaluating the quality of a speech sample. In this work, we are particularly interested in subjective speech quality assessment (SSQA). Since most ‚Äúend-listeners‚Äù of speech are human, the most accurate way to assess perceived quality is through subjective listening tests [1]. A common listening test protocol is the mean opinion score (MOS) test, where the quality of a query speech sample is represented as the average of ratings from multiple listeners, usually on a 5-point scale. However, conducting listening tests is inarguably very costly and time-consuming. Therefore, researchers have dedicated efforts to developing automated objective evaluation methods, which can ease the evaluation process and accelerate the development iteration. Objective measures can be categorized into two types [2, 3]. A signal-based metric usually measures the distance or similarity between the query and a reference. However, researchers found that most signal-based methods do not correlate well with human perception [4], which is essential if the goal of the objective metrics is to replace listening tests. Another shortcoming is that signal-based metrics are usually intrusive (or reference-based, double-ended), which means that a reference sample is needed. The choice of the reference is essential to the correctness of the metric, which can be a difficult task. For synthetic speech samples generated by text-to-speech (TTS) or voice conversion (VC) systems, it is often impossible to find the proper reference sample. Therefore A non-intrusive (or reference-free, single-ended) metric would be more convenient to use, thus more attractive. Another line of work focusing on model-based metrics is therefore attractive. These methods directly learn from a subjective speech rating dataset that consists of ‚ü®‚ü®\langle‚ü®speech, human rating‚ü©‚ü©\rangle‚ü© pairs, thus they correlate better with human perception compared to signal-based methods. Also, most of the model-based metrics are non-intrusive (with exceptions like SpeechBERTScore [5]), making them easy to use. The development of model-based approaches has been greatly accelerated by deep neural networks (DNNs) in the past decade. In the VoiceMOS Challenge (VMC) 2022 [6], a scientific competition for promoting SSQA, it was shown that the best-performing system achieved a high correlation (0.959) with human ratings. However, SSQA models suffer from generalization ability issues, as shown in [7]. In machine learning, generalization ability refers to the model‚Äôs capacity to make accurate predictions across a wide variety of conditions, beyond the dataset it was trained on. In the context of SSQA, we identify generalization ability in terms of two aspects. ‚Ä¢ Ranking correctness: the ability to correctly rank samples, either at the utterance-level or system-level. ‚Ä¢ Faithfulness: the ability to make predictions that are faithful to humans. Generalization is especially difficult in SSQA due to the nature of how listening tests are conducted. Each listening test represents a unique context, with different contents (text, speakers, etc.), recruited listeners, ranges of systems being evaluated, and even instructions. A test set could be either in-domain or out-of-domain111Making predictions for out-of-domain test sets is also referred to as zero-shot prediction [8]. w.r.t. an SSQA model. The former means the test samples and the ratings come from the same listening test as that of the training set, and the latter means they come from different listening tests. In VMC‚Äô23 where participants were asked to make zero-shot predictions on three out-of-domain test sets [8], most teams struggled to excel in all test sets, even if their model did well in one of the sets. This result motivated us to study the generalization ability in a more standard, large-scale setting. In this work, we present MOS-Bench, a collection of datasets for training and benchmarking subjective speech quality predictors. We also introduce SHEET, which stands for the Speech Human Evaluation Estimation Toolkit222Available at https://github.com/unilight/sheet.. SHEET is an all-in-one toolkit for SSQA research, with support of several representative DNN-based SSQA models. In our experiments, we provide benchmark results of the twelve test sets in MOS-Bench. We also explored training SSQA models by pooling multiple training datasets, with the motivation to improve the generalization ability. The main contributions of this work are listed as follows. ‚Ä¢ MOS-Bench was presented, which contains seven training sets and twelve test sets, covering different sampling frequencies, languages, and speech types, from synthetic speech generated by TTS, VC, speech enhancement (SE) systems, to non-synthetic speech such as transmitted, noisy and reverberant speech. ‚Ä¢ SHEET was developed for research purposes, providing complete recipes to conduct SSQA experiments, from data processing, training to benchmarking. ‚Ä¢ We proposed a new metric called best score difference/ratio, with the motivation to assess the overall performance of the SSQA models. ‚Ä¢ We investigated training SSQA models with multiple datasets, and found it could improve the generalization ability, without sacrificing much the performance of in-domain test sets. ‚Ä¢ We confirmed that the visualization of the training and test set samples in the latent space of the SSQA models could be used to explain the generalization ability."
https://arxiv.org/html/2411.03668v1,Mobile Recording Device Recognition Based Cross-Scale and Multi-Level Representation Learning,"The issue of identifying the source of mobile recording devices is a crucial focus in the realm of consumer electronics applications. It significantly aids in evidence collection for judicial authorities and the protection of individual intellectual property. Existing studies predominantly utilize a single network module to deeply analyze input features at the frame-level, neglecting the representation of audio at the sample-level and the integration of global temporal information. To address these gaps, this paper introduces a modeling approach that employs multi-level global processing, encompassing both short-term frame-level and long-term sample-level feature scales. In the initial stage of shallow feature extraction, various scales are employed to extract multi-level features, including Mel-Frequency Cepstral Coefficients (MFCC) and pre-Fbank log energy spectrum. The construction of the identification network model involves considering the input two-dimensional temporal features from both frame and sample levels. Specifically, the model initially employs one-dimensional convolution-based Convolutional Long Short-Term Memory (ConvLSTM) to fuse spatiotemporal information and extract short-term frame-level features. Subsequently, bidirectional long Short-Term Memory (BiLSTM) is utilized to learn long-term sample-level sequential representations. The transformer encoder then performs cross-scale, multi-level processing on global frame-level and sample-level features, facilitating deep feature representation and fusion at both levels. Finally, recognition results are obtained through Softmax. Our method achieves an impressive 99.6% recognition accuracy on the CCNU_Mobile dataset, exhibiting a notable improvement of 2% to 12% compared to the baseline system. Additionally, we thoroughly investigate the transferability of our model, achieving an 87.9% accuracy in a classification task on a new dataset.","The pervasive use of digital media, including audio [1] and video [2, 3], in fundamental social contexts such as communication, socialization, and media has significantly simplified information dissemination in consumer electronics applications [4, 5, 6]. However, the ease of editing and modifying digital media files, facilitated by various software applications [7, 8], has also made it remarkably convenient for malicious entities to tamper with and forge audio and video files [9], raising concerns about information security and escalating associated risks [10, 11, 12, 13, 14]. To counter these challenges, the emergence of mobile recording device identification technology for digital media audio files has proven vital [15]. Prior research confirms that each digital audio recording device introduces a unique ‚Äùmachine fingerprint‚Äù during the recording process, stemming from subtle differences in hardware and software configurations [16, 17]. The key to solving the mobile recording device identification problem lies in extracting representative feature information from recorded audio to ensure heightened sensitivity in recognizing these machine fingerprints [18]. This paper explores strategies for extracting finer and more discernible features from audio data to enhance machine fingerprint identification accuracy. Research on mobile recording device source identification comprises two main components: frontend feature extraction and backend recognition model construction. For the frontend, two primary methods of feature extraction exist. The first method focuses on spectral features of the original audio signal, such as MFCC [19, 20, 21] and Band Energy Difference (BED) [22]. These methods utilize acoustically relevant prior knowledge to extract spectral features or other distinguishing features and employ techniques like Voice Activity Detection (VAD) [23] and spectral subtraction [24] to enhance recognition accuracy. The second method is based on the representation of key information in the probability model of the original audio. Researchers use MFCC to construct Gaussian Mixture Models (GMM) and extract Gaussian Supervector Vectors (GSV) [25] to represent audio features, often combined with backend work in classical machine learning. Backend recognition models fall into two categories: traditional machine learning methods, such as Support Vector Machine (SVM) [19, 26] and GMM[27, 28, 29], and representation learning based on neural network models, incorporating deep learning technologies like Bidirectional Encoder Representations from Transformers (BERT) and Transformer from other domains [30]. Despite the application of deep neural networks, current research predominantly focuses on frame-level features of audio data, with limited consideration for sample-level features on a global scale. This paper emphasizes the extraction of audio features at multiple levels and explores temporal and spatial features locally and globally in the backend recognition model to improve recognition performance. The main contributions of this paper can be summarized as follows: ‚Ä¢ In the frontend feature extraction, this paper focuses on the frequency domain features of original audio, conducting multiple shallow feature extractions and fusions. The extracted MFCC features and pre-Fbank features are fused based on time and space representations. ‚Ä¢ In the backend recognition network, this paper conducts temporal and spatial representation learning of frame-level and sample-level features, thoroughly mining device source information embedded in shallow features from multiple scales. Through ConvLSTM and BiLSTM networks, information mining is performed on frame-level and sample-level scales, obtaining embeddings with a larger receptive field. ‚Ä¢ In the backend recognition network, this paper also conducts representation learning on global features, interacting information from various scales (frame-level, sample-level, and cross-scale level). Through Transformer-encoder, deeply processed depth information at multiple levels is globally processed, providing better representation for embeddings and benefiting the final recognition performance. ‚Ä¢ In experimental results, the proposed method in this paper exhibits precise recognition effects on the CCNU_Mobile dataset, achieving numerical values of 99.6% in recognition accuracy, recall, precision, and F1-Score. This proves the effectiveness of the method proposed in this paper. Furthermore, through fine-tuning pre-trained models and utilizing small batch sample transfer training, the method achieves a recognition accuracy of 87.9% on a new database. The structure of the remaining sections of this paper is as follows: Section II extensively reviews existing methods and related work in the literature. Section III initiates a preliminary discussion, defines the problems to be addressed, and provides theoretical support, including a list of relevant symbols, for reader reference. Section IV provides a detailed introduction to the principles and theoretical support of the proposed method. Section V introduces experiments designed to prove the effectiveness and superiority of the method through analysis of experimental data. Finally, Section VI summarizes this study and conducts a comprehensive discussion of identified shortcomings."
https://arxiv.org/html/2411.03866v1,"Performance evaluation of SLAM-ASR: The Good, the Bad, the Ugly, and the Way Forward","Recent research has demonstrated that training a linear connector between speech foundation encoders and large language models (LLMs) enables this architecture to achieve strong ASR capabilities. Despite the impressive results, it remains unclear whether these simple approaches are robust enough across different scenarios and speech conditions, such as domain shifts and different speech perturbations. In this paper, we address these questions by conducting various ablation experiments using a recent and widely adopted approach called SLAM-ASR. We present novel empirical findings that offer insights on how to effectively utilize the SLAM-ASR architecture across a wide range of settings. Our main findings indicate that the SLAM-ASR exhibits poor performance in cross-domain evaluation settings. Additionally, speech perturbations within in-domain data, such as changes in speed or the presence of additive noise, can significantly impact performance. Our findings offer critical insights for fine-tuning and configuring robust LLM-based ASR models, tailored to different data characteristics and computational resources.","‚Ä†‚Ä†footnotetext: ‚ô£ Corresponding authors: {shashi.kumar, iuliia.nigmatulina, sergio.burdisso, esau.villatoro}@idiap.ch Enabling large language models (LLMs) to ‚Äúcomprehend‚Äù non-textual modalities has received substantial attention recently. For instance, in [1] the authors trained a projection layer to align the outputs of a visual encoder with an LLM. In the context of automatic speech recognition (ASR), some early methods utilize a cascaded approach, where speech is first transcribed using an automated ASR system, followed by processing the resulting text with an LLM to enhance the transcription accuracy [2, 3, 4, 5]. However, cascaded approaches have several limitations, including error propagation and a lack of access to valuable paralinguistic acoustic information, such as prosody, speaker characteristics, and emotional valence. Recently, systems that integrate robust speech encoders with instruction-tuned LLMs through a connector/projector layer have been proposed as end-to-end ASR solutions [6, 7, 8, 9, 10]; henceforth referred as LLM-based ASR systems. Intuitively, the main task of the connector/projector is to learn how to transform acoustic embeddings from speech encoders into speech representations (tokens) that are meaningful within the LLM‚Äôs embedding space. These representations are then combined with text instructions (i.e., prompts) and fed into an LLM to generate various predictions, such as transcription, emotion classification, language identification, and named entity recognition. Three immediate advantages of such architectures are: (a) high compute efficiency, as the entire system can be adapted to new tasks by adopting parameter-efficient approaches, and/or by only training the projector layer; (b) data efficiency due to the vast corpora used in the pre-training of foundational models; and (c) enhanced generalization capability, as LLMs can leverage prompts for zero-shot or in-context learning to handle unseen tasks effectively [7]. In [6], the authors proposed attaching an audio encoder (comprising 36 Conformer [11] layer) to an LLM (Llama-7B [12]) to perform the ASR task. Embeddings generated by the audio encoder are stacked and projected onto the LLM‚Äôs input embedding space which is further trained using the Low-Rank Adaptation (LoRA) approach [13]. Similarly, in SpeechVerse [7] and SpeechLLM [14], the authors describe robust multitask training and curriculum learning frameworks that combine pre-trained speech and text foundation models via a small set of learnable parameters. Contrary to [6], these approaches applied a 1-D convolution module that operates over the audio feature sequence to ensure compatibility with the LLM. In the end, only the convolutional downsampling module and the LoRA adapter are trained. Figure 1: SLAM-ASR pipeline. The selected models and the number of parameters for the performed experiments appear between brackets. Recently, the SLAM-ASR architecture was introduced as ‚Äúan embarrassingly simple approach for large language models with robust ASR capabilities‚Äù [10]. Authors argue that elaborate neural architecture designs are unnecessary, showing that a simple composition of off-the-shelf speech encoders and LLMs‚Äîusing only a simple trainable linear projector to connect them‚Äîis sufficient for the ASR task (Figure 1). Despite the impressive results reported by SLAM-ASR [10], there remains uncertainty about whether this novel, yet simple solution, is ‚Äúthe way to go‚Äù for LLM-based ASR systems. This paper aims to address this question and performs three different ablation experiments on the SLAM-ASR architecture to evaluate how strong its claimed ASR capacity really is. We study and analyze the performance of SLAM-ASR on three well-known benchmark datasets and one private dataset, as well as its performance under extreme, yet plausible conditions normally addressed by traditional ASR models. This allows insights into the structure and organization of the knowledge represented within the SLAM-ASR architecture, providing transparency and interpretability of the network‚Äôs behavior. Overall, our work makes three main contributions that we hope will support the methodological decisions of future researchers working on LLM-based ASR under SLAM-ASR paradigm: (i) we empirically show that SLAM-ASR has a big dependence on the data used for training the linear projector (i.e., over-fitting), resulting in a model that lacks robustness when used in a cross-dataset scenario; (ii) we show how sensitive the SLAM-ASR architecture is to temporal and noise perturbations, unlike its ASR counterpart, and (iii) we performed an exhaustive analysis, both quantitative and qualitative, of what type of alignments the projector layer is learning and, we show how such an alignment can be improved resulting in a better ASR performance."
https://arxiv.org/html/2411.03109v1,pTSE-T: Presentation Target Speaker Extraction using Unaligned Text Cues,"Target Speaker Extraction (TSE) aims to extract the clean speech of the target speaker in an audio mixture, thus eliminating irrelevant background noise and speech. While prior work has explored various auxiliary cues including pre-recorded speech, visual information (e.g., lip motions and gestures), and spatial information, the acquisition and selection of such strong cues are infeasible in many practical scenarios. Unlike all existing work, in this paper, we condition the TSE algorithm on semantic cues extracted from limited and unaligned text content, such as condensed points from a presentation slide. This method is particularly useful in scenarios like meetings, poster sessions, or lecture presentations, where acquiring other cues in real-time is challenging. To this end, we design two different networks. Specifically, our proposed Text Prompt Extractor Network (TPE) fuses audio features with content-based semantic cues to facilitate time-frequency mask generation to filter out extraneous noise, while another proposal, namely Text-Speech Recognition Network (TSR), employs the contrastive learning technique to associate blindly separated speech signals with semantic cues. The experimental results show the efficacy in accurately identifying the target speaker by utilizing semantic cues derived from limited and unaligned text, resulting in SI-SDRi of 12.16 dB, SDRi of 12.66 dB, PESQi of 0.830 and STOIi of 0.150, respectively. Dataset and source code will be publicly available. Project demo page: https://slideTSE.github.io/.","Speech communication is inherently the most natural and predominant mode of interaction among humans. However, the efficacy of computational models in processing speech-related tasks, including but not limited to Automatic Speech Recognition (ASR) (Yue et al. 2019), speaker localization (Qian et al. 2021), Active Speaker Detection (ASD) (Tao et al. 2021), and Speech Emotion Recognition (SER) (Pan et al. 2020), is significantly compromised in scenarios with overlapping speakers and severe background noise. This motivates researchers to investigate different ways to extract high-quality target speaker‚Äôs speech. The ‚Äòcocktail party problem‚Äô (Cherry 1953) is defined as a common social situation in which multiple conversations occur simultaneously under ambient noise. Such an environment presents a challenge to interpersonal communication due to the potential interference from other speakers‚Äô voices. In this case, the human auditory system has an extraordinary ability to selectively attend to the speech of a target speaker while effectively suppressing irrelevant acoustic interference, i.e., selective auditory attention. Figure 1: Illustration of our proposed pTSE-T task which extracts the presenter‚Äôs speech from the audio mixture (including interfering speaker‚Äôs speech and background noise) using unaligned text from the visual presentation slide. The cocktail party problem can be approached by extracting individual sound sources from mixed audio signals. In this case, speech separation (Hershey et al. 2016; Liu and Wang 2019; Luo and Mesgarani 2019; Kolb√¶k et al. 2017; Zeghidour and Grangier 2021) serves as a solution that separates a mixed signal into individual speech streams, given prior knowledge of the total speaker number. However, separated speech signals do not have an association with speaker identities, which often suffers from the ambigous ID permutation problem. However, with an unknown speaker number, the speech separation performance degrades. Differently, by mimicking humans‚Äô intrinsic attentive listening ability, TSE does not require prior knowledge of the exact speaker number. Instead, it uses speaker-specific reference cues to accurately isolate the desired voice. For example, a pre-registered speech signal (Wang et al. 2019; Ge et al. 2020a; Xu et al. 2020) is used primarily as a cue to extract ID-consistent speech. Moreover, co-speech gestures (Pan, Qian, and Li 2022) and lip movements (Pan et al. 2020) from synchronized video recordings, or even live location (Ge et al. 2022) can also be used as the auxiliary visual cues. However, acquiring such strong cues either requires a pre-registration process or high-quality visual conditions, e.g., the speaker mostly moves inside the camera‚Äôs field of view with visible frontal face, continuous camera tracking, which may not always be available. In this work, we target the meeting or presentation scenario where multiple individuals speak simultaneously and strong cues such as live recording or voice pre-registration are not available. In these typical scenarios, speakers use visual support tools, such as PowerPoint slides or posters, to ensure that the information conveyed is easily understood with enhanced clarity and impact. The text contents, i.e. the key points or summaries on the presentation slides, offer limited but concise semantic cues which are intrinsically related to the speaker‚Äôs spoken content. To the best of our knowledge, no previous work has used such a text as the condition for TSE. Although challenging, it is useful in many scenarios. Therefore, in this paper, we propose a new task, namely Presentation Target Speaker Extraction with unaligned Text cues (pTSE-T), that uses the text on the slide as a cue for speaker extraction. Our key contributions can be summarized as follows. 1. We are the first proposal of the pTSE-T task, as illustrated in Fig. 1, which extracts the presenter‚Äôs clean speech from the audio mixture given the unaligned semantic text cues obtained from the presentation slide. 2. To facilitate this new pTSE-T task, we design and annotate an innovative data corpus, which includes temporal-synchronized speech and presentation slides. In addition, we also provide the ASR transcripts and the text descriptions extracted from slides using an open-source Optical Character Recognition (OCR) toolkit. 3. We propose two novel networks i.e. TPE and DPRNN-TSR. Specifically, TPE fuses the text prompts with audio features for TSE, achieving correct extraction rates of 96.46% and 95.26% on the MMSpeech-2mix and -3mix datasets, respectively. DPRNN-TSR employs contrastive learning to explore the intra-modal correlation between paired text prompts and speech data, achieving correct matching rates of 86.28% and 76.70%, respectively."
https://arxiv.org/html/2411.03085v1,Speech Separation with Pretrained Frontend to Minimize Domain Mismatch,"Speech separation seeks to separate individual speech signals from a speech mixture. Typically, most separation models are trained on synthetic data due to the unavailability of target reference in real-world cocktail party scenarios. As a result, there exists a domain gap between real and synthetic data when deploying speech separation models in real-world applications. In this paper, we propose a self-supervised domain-invariant pretrained (DIP) frontend that is exposed to mixture data without the need for target reference speech. The DIP frontend utilizes a Siamese network with two innovative pretext tasks, mixture predictive coding (MPC) and mixture invariant coding (MIC), to capture shared contextual cues between real and synthetic unlabeled mixtures. Subsequently, we freeze the DIP frontend as a feature extractor when training the downstream speech separation models on synthetic data. By pretraining the DIP frontend with the contextual cues, we expect that the speech separation skills learned from synthetic data can be effectively transferred to real data. To benefit from the DIP frontend, we introduce a novel separation pipeline to align the feature resolution of the separation models. We evaluate the speech separation quality on standard benchmarks and real-world datasets. The results confirm the superiority of our DIP frontend over existing speech separation models. This study underscores the potential of large-scale pretraining to enhance the quality and intelligibility of speech separation in real-world applications.","Speech separation seeks to separate individual speech signals in a complex, multi-speaker acoustic environments, commonly known as the ‚Äòcocktail party problem‚Äô [1]. Speech separation has been an active research, not only because it is a human cognitive ability, but also it serves as a frontend in many practical speech processing tasks, such as speaker recognition [2, 3, 4], automatic speech recognition [5, 6, 7], and voice conversion [8, 9]. Traditional speech separation technique includes independent component analysis (ICA) [10, 11, 12], non-negative matrix factorization (NMF) [13, 14, 15] or the theory of computational auditory scene analysis (CASA) [16, 17, 18]. With the advent of deep learning, supervised speech separation models, including both time-domain [19, 20, 21] and frequency-domain models [22, 23, 24], have achieved remarkable advancements. Despite impressive results, these models don‚Äôt generalize well to real-world scenarios. This is primarily due to the fact that most speech separation models are trained on synthetic anechoic data [25], that are very different from real-world speech data with unpredictable attenuation and reverberation [26, 27, 28]. In this paper, such a problem is referred to as domain mismatch. To mitigate this performance degradation due to domain mismatch, there were attempts to incorporate real-world speech mixtures during training. Unsupervised speech separation is one example. In [29], the authors utilize the mixture waveform to generate the mixture of the mixture (MOM) and train the separation model with the mixture invariant training method. Tzinis et al. [30] expand this architecture by incorporating various remixing methods. Karamatli et al. [31] further promote the model through cyclic learning for continuous improvement. Semi-supervised speech separation is another example. Zhang et al. [32] introduce a framework where the student model learns to predict pseudo-labels from the teacher model. In [33], Han et al. extend this approach by introducing an ensemble structure combined with separation consistency training methods. Furthermore, Sivaraman et al. [34] propose a combination of semi-supervised and unsupervised methods to improve the model performance in meeting scenarios. The above techniques incorporate real speech mixtures during model training. However, the quality improvement over conventional supervised methods [29] is limited, probably because they introduce another mismatch, i.e. target mismatch. The target labels in those techniques are either the mixture itself or fake labels rather than the clean reference speech. In a different attempt, Ben et al. [35] demonstrate that the domain-invariant knowledge is crucial for models to overcome domain mismatch. The effectiveness of domain-invariant knowledge has also been validated in various tasks [36, 37, 38]. Motivated by these findings, we seek to utilize the domain-invariant knowledge across real and synthetic data, to assist the downstream models in overcoming the domain discrepancy. Unlike the individual speech frames that are sensitive to channel and noise variation, contextual speech flow is a prominent auditory cue for speech perception and cognition across diverse scenarios [39]. According to the studies by Bregman et al. [40, 41, 42, 43], the cognition of specific speech signals in a complex environment is guided by the contextual knowledge. For example, in the study of double synthetic vowel experiment [44, 45, 46], psychoacousticians reveal that listeners unconsciously infer the most likely sound through the contextual cues, including phoneme and word information, during the perceptual process, which is referred to as the auditory induction [47]. In this paper, we are inspired by the prior findings to study a domain-invariant pretrained (DIP) frontend that captures contextual cues from unlabeled, real speech mixtures. With such a DIP frontend, we anticipate that the downstream speech separation models will acquire transferable knowledge for real-world scenarios. In practice, the proposed DIP frontend is a learnable contextual cue extractor trained on some pretext tasks with large-scale wild unlabeled or weakly labeled data. As demonstrated in [48], there are two typical types of pretext tasks for pretrained frontend, namely generative pretext task and discriminative pretext task. The generative pretext task [49, 50, 51, 52, 53, 54, 55] treats the speech itself as a sequential structure and reconstruct the detailed information based on the prior context. On the other hand, the discriminative pretext task [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 48, 67, 68, 69, 70] focuses on capturing the high-level ‚Äúslow features‚Äù [39] that span many time steps, through predicting the masked frames in a contrastive manner or the index of masked clustering labels. We advocate that contrastive predictive coding (CPC) [39, 56, 57, 58], a discriminative pretext task, is suitable for reducing the domain mismatch between real and synthetic data because it learns contextual cues by encoding multi-scale information [71] with mutual information maximization. In this paper, we adopt a Siamese network as the domain-invariant pretrained frontend, and devise the pretraining strategy. The contributions of our paper are summarized as follows, ‚Ä¢ We introduce the concept of using mixture waveform for self-supervised frontend pretraining and design pretext task to capture the contextual cues with both real and synthetic inputs. ‚Ä¢ We derive the domain discrepancy distance and formulate a Siamese network with a novel domain loss to further reduce domain mismatch. ‚Ä¢ We propose a general pipeline to incorporate popular frontends into various separation models. ‚Ä¢ We conduct comprehensive experiments to validate the superior performance of DIP frontend across diverse real and synthetic datasets, with various downstream models, and different evaluation metrics. The rest of the paper is organized as follows. Section II discusses the prior studies on pretrained frontend. Section III formulates the pretraining method and introduces the DIP model architecture. In Section IV, we describe the model configuration and experimental setup. In Section V, we report the results. Finally, Section VI concludes the paper."
https://arxiv.org/html/2411.02964v2,Speaker Emotion Recognition: Leveraging Self-Supervised Models for Feature Extraction Using Wav2Vec2 and HuBERT,"Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of self-supervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.","Speech is one of the primary ways of expressing emotions. Therefore, a system that can recognize, interpret, and respond to the emotions expressed in speech is highly valuable. Emotions influence both the vocal characteristics and the linguistic content of speech. In recent years, enormous efforts have been devoted to developing methods for automatically identifying human emotions from speech signals, a field known as speaker emotion recognition (SER). SER systems have many applications including human-machine interactions [11, 41], medicine [32], and psychology [60]. Moreover, evaluation of the conversations in a call centre is one of the major applications of SER, the manager of a call centre can analyse the performance of the operators. The operators are able to consider the feeling of the customers during conversation to handle necessary cases. In all of the classification problems one of the main step is feature extraction. Before using deep learning most of the conventional machine learning methods used hand-crafted features speech recognition including Mel-frequency cepstral coefficients (MFCCs, pitch, zero-crossing, Fourier transform, and energy of signals [51, 43, 1, 44, 40, 46]. After growing deep learning usage in speech processing tasks, the models extracted features automatically by deep-based models. They used convolutional neural networks (CNNs) to extract spatial features from input, the input was mostly spectrograms. Also, they used recurrent neural networks (RNNs) to evaluate temporal features. The models extracted local information and long-term contextual dependencies [3, 33, 57, 2, 58, 30, 56, 10, 36, 53]. Additionally, the attention mechanism is another popular method that has been used in different fields such as speech recognition [10] , visual object classification [36] and document classification [53]. It is a powerful concept that helps models focus on specific parts of the input sequence when generating each part of the output sequence. In the SER problem, models try to ignore silence frames and other parts of the utterance that do not carry emotional content [9, 35, 21, 22]. Our contribution in this study lies in solving speaker emotion recognition by using transformer-based models, HuBERT [19] and Wav2vVec [4], which trained in a self-supervised method on the huge amount of data. These models are very powerful for feature extraction, so we used the models to extract features from raw audio data. We will delve deeper into the models in the next sections."
https://arxiv.org/html/2411.02848v1,Adversarial multi-task underwater acoustic target recognition: towards robustness against various influential factors,"Underwater acoustic target recognition based on passive sonar faces numerous challenges in practical maritime applications. One of the main challenges lies in the susceptibility of signal characteristics to diverse environmental conditions and data acquisition configurations, which can lead to instability in recognition systems. While significant efforts have been dedicated to addressing these influential factors in other domains of underwater acoustics, they are often neglected in the field of underwater acoustic target recognition. To overcome this limitation, this study designs auxiliary tasks that model influential factors (e.g., source range, water column depth, or wind speed) based on available annotations and adopts a multi-task framework to connect these factors to the recognition task. Furthermore, we integrate an adversarial learning mechanism into the multi-task framework to prompt the model to extract representations that are robust against influential factors. Through extensive experiments and analyses on the ShipsEar dataset, our proposed adversarial multi-task model demonstrates its capacity to effectively model the influential factors and achieve state-of-the-art performance on the 12-class recognition task.","Underwater acoustic target recognition based on passive sonar plays a vital role in the field of marine acoustics Rajagopal et al. (1990). The primary objective of this task is to automatically classify the category of targets by analyzing the radiated noise signals received by passive sonar. The long detection range, reassuring concealment, and low deployment cost of passive underwater acoustic target recognition make it indispensable in practical applications Heupel et al. (2006). This technology finds extensive applications in various areas, including underwater surveillance Sutin et al. (2010), marine resources development and protection Vaccaro (1998), and security defense Fillinger et al. (2010). In recent years, data-driven machine learning techniques, particularly deep neural networks, have emerged as the dominant approaches for underwater acoustic recognition systems Xie et al. (2022b); Sun and Wang (2022); Xu et al. (2023). These techniques rely less on prior knowledge and manual parameter setting Ren et al. (2022) and possess powerful modeling capabilities for diverse distributions of underwater acoustic data Irfan et al. (2021). However, in real-world application scenarios, machine learning-based recognition models often struggle to capture robust target-relevant characteristics due to the inherent instability and susceptibility of underwater acoustic signals to various environmental conditions and data acquisition configurations Santos-Dom√≠nguez et al. (2016); Chen et al. (2021); Xie et al. (2022b). Environmental conditions, such as ambient noise, wind speed, water column depth, etc., can influence the amplitude and frequency components of signals through the generation of interference waves or by affecting the propagation of sound waves. Additionally, data acquisition configurations, such as the sound source range, the frequency response characteristics of receivers, etc., can also impact the power level and frequency distribution of signals. Fig. 1 compares the waveforms and spectrograms of signals belonging to the same target (a passenger ship named ‚ÄúMinho uno‚Äù) under different conditions of source range, water column depth, and wind speed. As shown in Fig. 1 (b) and (c), when the wind speed reaches 10.5 or 13 km/h, strong winds can generate surface waves that propagate into the water column. These wind-induced surface waves have the potential to cause noticeable interference to signals, which can be observed as vertical stripes in the waveform and spectrogram. Furthermore, when the source range and water column depth increase, there is a noticeable attenuation of the high-frequency components of the signal (indicated by lighter colors in the spectrogram, representing lower energy). This propagation-induced attenuation can also affect the frequency distribution. Figure 1: A comparison of signal waveforms and spectrograms belonging to the same target (a passenger ship ‚ÄúMinho Uno‚Äù) under different influential factors. The samples are drawn from the ShipsEar dataset. According to our research, modeling environmental conditions and data acquisition configurations (referred to as ‚Äúinfluential factors‚Äù in subsequent paragraphs) has been widely applied across various domains in underwater acoustics, including underwater acoustic networks Jiang (2008); Domingo (2008), underwater acoustic communications Yang (2012); Khan et al. (2020); Chen et al. (2021), underwater source localization Hassab (1984); Worthmann et al. (2017), and others. However, in the domain of underwater acoustic target recognition, which primarily relies on data-driven machine learning, the limited availability of data and related annotations concerning influential factors hinders the research development. While partial researchers have acknowledged the influence of these factors in signal characteristics Xie et al. (2022b); Li et al. (2023a), there remains a gap in establishing a clear association between these factors and the recognition task. To address the issue, previous recognition-related studies have primarily focused on mitigating marine environment noise Zhou and Yang (2020); Zhou et al. (2023) or generating synthetic signals to simulate data under different influential factors for data augmentation Li et al. (2023a). However, these approaches may yield limited improvements or even lead to a decline in performance due to the discrepancies between manually estimated conditions and the actual underwater environment Xu et al. (2023). To address this research gap, this study adopts a multi-task framework to leverage influential factors in a direct manner. We selected several influential factors (source range, water column depth, wind speed) with relatively complete annotations from the ShipsEar dataset Santos-Dom√≠nguez et al. (2016), and designed relevant auxiliary task to model specific influential factors. The auxiliary task can facilitate the transfer of knowledge about influential factors to the recognition task through shared network layers and parameters. Furthermore, we integrate an extra adversarial learning mechanism into the multi-task framework. This mechanism can discourage the model from focusing on discriminative patterns related to the influential factors, thereby prompting the front-end shared layer of the model to extract robust representations that are insensitive to influential factors. In addition, we also introduce two customized optimization techniques, which serve to address potential issues of our proposed adversarial multi-task model. To evaluate the efficacy of our proposed model, we conducted extensive experiments on the ShipsEar dataset, demonstrating substantial performance enhancements compared to baselines and current advanced methods. Our AMTNet can achieve state-of-the-art results on the 12-class recognition task, with an average accuracy of 80.95%. The key contributions of this study can be summarized as follows: ‚Ä¢ This study identifies the limitations of current recognition models, which are susceptible to environmental conditions and data acquisition configurations, and pioneers the modeling of these influential factors in the underwater acoustic recognition domain; ‚Ä¢ We design auxiliary tasks that model influential factors based on available annotations and adopt a multi-task framework as a bridge to connect these factors with the recognition task; ‚Ä¢ We incorporate an adversarial learning mechanism into the multi-task framework, aiming to enhance the model‚Äôs robustness against influential factors."
https://arxiv.org/html/2411.02787v1,Advancing Robust Underwater Acoustic Target Recognition through Multi-task Learning and Multi-Gate Mixture-of-Experts,"Underwater acoustic target recognition has emerged as a prominent research area within the field of underwater acoustics. However, the current availability of authentic underwater acoustic signal recordings remains limited, which hinders data-driven acoustic recognition models from learning robust patterns of targets from a limited set of intricate underwater signals, thereby compromising their stability in practical applications. To overcome these limitations, this study proposes a recognition framework called M3 (Multi-task, Multi-gate, Multi-expert) to enhance the model‚Äôs ability to capture robust patterns by making it aware of the inherent properties of targets. In this framework, an auxiliary task that focuses on target properties, such as estimating target size, is designed. The auxiliary task then shares parameters with the recognition task to realize multi-task learning. This paradigm allows the model to concentrate on shared information across tasks and identify robust patterns of targets in a regularized manner, thereby enhancing the model‚Äôs generalization ability. Moreover, M3 incorporates multi-expert and multi-gate mechanisms, allowing for the allocation of distinct parameter spaces to various underwater signals. This enables the model to process intricate signal patterns in a fine-grained and differentiated manner. To evaluate the effectiveness of M3, extensive experiments were implemented on the ShipsEar underwater ship-radiated noise dataset. The results substantiate that M3 has the ability to outperform the most advanced single-task recognition models, thereby achieving the state-of-the-art performance.","Underwater acoustic target recognition is a crucial component of marine acoustics Rajagopal et al. (1990). Its purpose is to automatically recognize different types of underwater targets by analyzing their radiating sound. This technology finds extensive applications in underwater surveillance Sutin et al. (2010), marine resources development and protection Vaccaro (1998), and security defense Fillinger et al. (2010). In recent years, deep learning has emerged as the dominant technique for underwater acoustic recognition systems Niu et al. (2023); Hummel et al. (2024). Data-driven acoustic recognition models based on deep learning have shown promising performance on publicly available datasets Santos-Dom√≠nguez et al. (2016); Irfan et al. (2021). These data-driven recognition models may struggle to capture underlying robust patterns from limited but complicated signal recordings, leading to issues such as overfitting and limited generalization capabilities in real-world ocean scenarios Xie et al. (2023a); Irfan et al. (2021); Xie et al. (2022b). Furthermore, the difficulties and costs associated with underwater signal acquisition, as well as restrictions imposed by security defense and military applications Santos-Dom√≠nguez et al. (2016); Xie et al. (2024), may perpetuate the scarcity of publicly accessible underwater acoustic signal recordings, hindering the development of large-scale robust models over an extended period Xu et al. (2023). To make further progress under such conditions, a promising idea is to enhance the model‚Äôs capacity to capture robust patterns by making it aware of targets‚Äô inherent properties (e.g., target size). According to our research, the majority of existing literature in underwater acoustic recognition Xie et al. (2022a); Erkmen and Yƒ±ldƒ±rƒ±m (2008); Simonoviƒá et al. (2021); Jia et al. (2022) relies solely on category labels as supervisory information, which provides limited insights into the intrinsic patterns of targets when data is insufficient. In response to this aspect, recent advancements in machine learning techniques have offered opportunities to exploit additional information beyond category labels. Among them, multi-task learning Caruana (1997); Ma et al. (2018); Misra et al. (2016); Tang et al. (2020) can serve as an attractive solution. For acoustic signal recognition, the multi-task model can consider exploiting intrinsic properties from signals as the auxiliary task, which shares parameters with the recognition task. The interaction between tasks facilitates mutual information transfer and initiates regularization constraints. This can prompt the model to focus on common robust features of target signals across different perspectives, while disregarding task-irrelevant noise. Such a paradigm exhibits enhanced data utilization efficiency and can assist the model in acquiring greater robustness. In this study, we employ a multi-task learning framework to enable the recognition task to perceive robust patterns related to the inherent properties of targets in underwater signals, with ‚Äútarget size estimation‚Äù serving as the auxiliary task. This auxiliary task does not require additional annotations since the target size can be easily inferred based on the category labels. In addition, it can also encourage the model to learn about size-related acoustic characteristics such as resonant frequencies, hull noise Song et al. (2014), etc., thus improving the model‚Äôs insights into signals. Moreover, inspired by recent work on the Mixture of Experts (MoE) Jacobs et al. (1991); Riquelme et al. (2021); Xie et al. (2024) and the multi-gate mechanism Ma et al. (2018); Tang et al. (2020), we adopt an improved multi-task framework called M3 (Multi-task, Multi-gate, Multi-expert) to fully exploit the potential of multi-task learning. Specifically, M3 employs multiple independent network layers, referred to as expert layers, to replace the traditional shared layer to facilitate information sharing between tasks. Expert layers have identical architectures but distinct parameters, allowing them to specialize in different aspects and provide fine-grained knowledge with separate parameter spaces. Additionally, multiple gating layers Jacobs et al. (1991); Ma et al. (2018) are employed to dynamically learn task-specific weights, allowing each task to linearly add the outputs of expert layers with unique weights and acquire task-specific representations (refer to Sections III.B and III.C for detailed introductions to the expert layer and the gating layer). M3 also incorporates one-dimensional frequency domain features as the input to the gating layers, replacing conventional inputs, to provide low-dimensional and non-redundant information. Furthermore, this study presents an optional strategy to alleviate interference among tasks by designating certain experts exclusively for specific tasks. To validate the superior performance of the multi-task framework and M3, a series of experiments were implemented on the ShipsEar dataset Santos-Dom√≠nguez et al. (2016). The results demonstrate that the multi-task models can achieve superior recognition accuracy, with M3 exhibiting a notable performance enhancement. On the ShipsEar dataset, M3 can achieve a remarkable accuracy of 87.07¬±plus-or-minus\pm¬±2.43% on the 9-class recognition task, reaching state-of-the-art performance. The main contributions of this study can be summarized as follows: ‚Ä¢ Identifying the limitations of current acoustic recognition methods under existing data conditions and employing multi-task learning to enhance the model‚Äôs ability to capture robust patterns by leveraging inherent properties of targets. ‚Ä¢ Introducing multi-expert and multi-gate mechanisms, which facilitate the processing of complex underwater acoustic signals by utilizing specialized and separate parameter spaces. ‚Ä¢ Proposing two optimization strategies: incorporating low dimensional features as input to the gating layer, and converting certain expert layers into task-specific ones. ‚Ä¢ Conducting abundant experiments to validate the superiority of the M3 model, which achieves state-of-the-art performance on the ShipsEar dataset."
https://arxiv.org/html/2411.02758v1,DEMONet: Underwater Acoustic Target Recognition based on Multi-Expert Network and Cross-Temporal Variational Autoencoder,"Building a robust underwater acoustic recognition system in real-world scenarios is challenging due to the complex underwater environment and the dynamic motion states of targets. A promising optimization approach is to leverage the intrinsic physical characteristics of targets, which remain invariable regardless of environmental conditions, to provide robust insights. However, our study reveals that while physical characteristics exhibit robust properties, they may lack class-specific discriminative patterns. Consequently, directly incorporating physical characteristics into model training can potentially introduce unintended inductive biases, leading to performance degradation. To utilize the benefits of physical characteristics while mitigating possible detrimental effects, we propose DEMONet in this study, which utilizes the detection of envelope modulation on noise (DEMON) to provide robust insights into the shaft frequency or blade counts of targets. DEMONet is a multi-expert network that allocates various underwater signals to their best-matched expert layer based on DEMON spectra for fine-grained signal processing. Thereinto, DEMON spectra are solely responsible for providing implicit physical characteristics without establishing a mapping relationship with the target category. Furthermore, to mitigate noise and spurious modulation spectra in DEMON features, we introduce a cross-temporal alignment strategy and employ a variational autoencoder (VAE) to reconstruct noise-resistant DEMON spectra to replace the raw DEMON features. The effectiveness of the proposed DEMONet with cross-temporal VAE was primarily evaluated on the DeepShip dataset and our proprietary datasets. Experimental results demonstrated that our approach could achieve state-of-the-art performance on both datasets.","Underwater acoustic target recognition is a crucial component of marine acoustics Rajagopal et al. (1990); Tian et al. (2023); Jin et al. (2023), aimed at recognizing underwater targets such as ships by analyzing their radiating sound. This technology has extensive applications in underwater surveillance and navigation systems Sutin et al. (2010); Jia et al. (2022), security defense systems, and in the development and protection of marine resources Etter (2012); Fillinger et al. (2010). As a vital task in underwater acoustics, underwater acoustic target recognition is usually accompanied by many challenges in practical scenarios Xie et al. (2022b), such as complex underwater environments, unpredictable transmission channels, and dynamic motion states of targets Erbe et al. (2019); Xu et al. (2023). In the past, commonly used underwater acoustic features focused on well-defined physical characteristics Rajagopal et al. (1990); Vaccaro (1998). For instance, peaks and harmonics in DEMON (detection of envelope modulation on noise) spectra, which provide information about propeller shaft frequency and blade counts, have been utilized for underwater target recognition Wang et al. (2021); Liu et al. (2020); Hashmi and Raza (2023), etc. These characteristics are derived from the inherent properties of targets and are less influenced by complex marine environments Gao et al. (2021), making them interpretable and robust features. However, these features tend to emphasize local physical characteristics, making it challenging to establish a mapping relationship with diverse target categories. In other words, they may not offer sufficient class-specific discriminative patterns to support comprehensive modeling. For example, while DEMON spectra can differentiate between targets with three-blade and five-blade propeller configurations, they may struggle to distinguish between targets that have the same propeller blade counts but belong to different categories, such as passenger ships and cargo ships. Related studies have demonstrated that physical characteristics exhibit limitations in recognition tasks involving diverse categories Xie et al. (2022a) and diverse feature space Irfan et al. (2021). Additionally, considering the limited capability to handle non-stationary signals, physical characteristics have gradually been overlooked in recognition tasks within this data-driven era. With the widespread adoption of deep neural networks, time-frequency features, such as spectrograms, have become the prevailing choice for recognition tasks Yang et al. (2019); Li et al. (2011). These features can provide comprehensive information that includes frequency components and time-varying characteristics, thus supporting complex modeling. Previous studies Xie et al. (2022b, a) have reported that recognition systems based on time-frequency features can achieve leading performance on public underwater acoustic datasets Irfan et al. (2021); Santos-Dom√≠nguez et al. (2016). However, time-frequency features are susceptible to interferences caused by environmental and motion variations Xie et al. (2022b); Xu et al. (2023), which may cause recognition models to overfit on target-irrelevant information, such as background noise, channel noise, spurious line spectra, and Doppler shift Liu et al. (2020), etc. This vulnerability undermines the generalization and robustness of spectrogram-based recognition systems, thereby limiting their performance in practical application scenarios. In general, physical characteristics exhibit greater resistance to interferences but lack sufficient class-specific discriminative patterns, whereas spectrograms contain ample class-specific information but are less robust. A question naturally arises: ‚ÄúCan we integrate physical characteristics into mainstream spectrogram-based recognition systems to compensate for the lack of robustness?‚Äù In our preliminary experiments (see Section 5.1 and Table LABEL:tab_demon), we attempted to integrate physical characteristics using strategies such as feature fusion Xie et al. (2023b) or model ensemble Yang et al. (2016). However, the experimental results revealed that directly integrating physical characteristics could not yield satisfactory results and, in some cases, even had negative effects. According to the analyses, we attribute the detrimental effect to the lack of class-specific discriminative patterns. Imposing a mapping between physical characteristics and target types can introduce inappropriate inductive biases that hurt the model‚Äôs performance. In this study, we propose DEMONet, which leverages DEMON spectra to provide robust insights into shaft frequency or blade counts of targets. To benefit from physical characteristics while avoiding potential detrimental effects, DEMONet incorporates multiple separate expert layers and a routing layer, wherein the routing layer assigns inputs to their best-matched expert layer based on the physical characteristics provided by DEMON spectra. This approach treats physical characteristics as the basis for routing, thus enabling each expert layer to learn from data with common physical characteristics to acquire specialized knowledge. In essence, our design resembles ‚Äúclustering before classification‚Äù Mathivanan et al. (2018). It facilitates the model‚Äôs ability to process signals with diverse physical characteristics in a differentiated and fine-grained manner, and prevents the DEMON spectra from directly mapping to target categories. In addition, we leverage the time-invariant nature of physical characteristics to design the cross-temporal variational autoencoder (VAE), which takes the reconstruction of DEMON spectra belonging to the same signal across different periods as the training objective. The reconstructed DEMON spectra are less susceptible to noise and spurious modulation spectra, which are better suited to serve as the input DEMON feature. Additionally, we incorporate the load balancing loss Fedus et al. (2022) to address the issue of expert layer underfitting caused by potential load imbalance. To validate the effectiveness of our proposed methods, we conducted extensive comparative experiments and ablation experiments on multiple underwater acoustics datasets. The experimental results demonstrated that DEMONet with cross-temporal VAE can outperform current advanced models on the DeepShip Irfan et al. (2021) and our proprietary dataset Ren et al. (2019). Besides, we investigated the performance of DEMONet on the data-scarce ShipsEar dataset, further clarifying its practical value and application scenarios. The main contributions of this study are summarized as follows: ‚Ä¢ Identification of limitations of physical characteristics and mainstream time-frequency features in underwater acoustic recognition. ‚Ä¢ Proposal of DEMONet, a multi-expert structure that integrates physical characteristics into recognition systems to enhance robustness without the risk of performance degradation. ‚Ä¢ Design of cross-temporal variational autoencoder to mitigate noise and interference in DEMON spectra, along with the utilization of load balancing loss to address the load imbalance issue."
https://arxiv.org/html/2411.02711v1,Self-supervised Multi-view Learning for Disentangled Music Audio Representations,"Self-supervised learning (SSL) offers a powerful way to learn robust, generalizable representations without labeled data. In music, where labeled data is scarce, existing SSL methods typically use generated supervision and multi-view redundancy to create pretext tasks. However, these approaches often produce entangled representations and lose view-specific information. We propose a novel self-supervised multi-view learning framework for audio designed to incentivize separation between private and shared representation spaces. A case study on audio disentanglement in a controlled setting demonstrates the effectiveness of our method.","SSL uses pretext tasks to uncover patterns from unlabeled data. In single-view SSL, a model learns from one perspective of the input via information restoration [1]. Multi-view SSL, however, utilizes distinct views to generate supervision, assuming shared information across views suffices for downstream tasks [2]. These methods align and contrast information across views for learning. Recent multi-view SSL studies use contrastive learning to treat audio segments or augmentations as transformed views [3, 4]. However, these approaches neglect the intrinsic structure of music audio, entangling representations with attributes like timbre, frequency, and tude. They also focus on shared information across views, missing task-relevant, view-specific details. Previous research has primarily focused on separating pitch and timbre to encode disentangled music representations, often designing dedicated encoders for each attribute. Some approaches train generative models with explicit supervision on pitch and timbre latents [5], while others reduce supervision by applying auxiliary metric-based regularization in the latent spaces [6, 7, 8, 9]. We propose a novel self-supervised multi-view learning framework for music audio, inspired by multi-view/multimodal disentanglement [10, 11], which explicitly separates shared and private representations. This approach preserves the uniqueness of each view while capturing common latent factors. As a case study, we tackle music audio disentanglement from a multi-view learning perspective with self-supervision. We validate our method using Syntone [12], a dataset with controlled variations in music attributes such as timbre and frequency."
https://arxiv.org/html/2411.02625v1,EmoSphere++: Emotion-Controllable Zero-Shot Text-to-Speech via Emotion-Adaptive Spherical Vector,"Emotional text-to-speech (TTS) technology has achieved significant progress in recent years; however, challenges remain owing to the inherent complexity of emotions and limitations of the available emotional speech datasets and models. Previous studies typically relied on limited emotional speech datasets or required extensive manual annotations, restricting their ability to generalize across different speakers and emotional styles. In this paper, we present EmoSphere++, an emotion-controllable zero-shot TTS model that can control emotional style and intensity to resemble natural human speech. We introduce a novel emotion-adaptive spherical vector that models emotional style and intensity without human annotation. Moreover, we propose a multi-level style encoder that can ensure effective generalization for both seen and unseen speakers. We also introduce additional loss functions to enhance the emotion transfer performance for zero-shot scenarios. We employ a conditional flow matching-based decoder to achieve high-quality and expressive emotional TTS in a few sampling steps. Experimental results demonstrate the effectiveness of the proposed framework.","Emotions are interrelated in a highly systematic fashion [1]. For example, the emotion of sadness can be expressed with derivative states of primary emotions such as feeling hurt or lonely, depending on the style and intensity. Recently, emotional text-to-speech (TTS) technology has experienced rapid developments, increasing the interest in high-level interpretable emotion control [2, 3, 4, 5]. Controllable emotional TTS represents a breakthrough in reproducing human-like emotions in speech synthesis, thus enabling more emotionally intelligent interactions between humans and computers. Although researchers have made significant progress in controlling emotional intensity, the ability to precisely control emotional style must be further improved. The modeling of diverse emotional styles and intensities represents a significant challenge in controllable emotion TTS. A reason for this challenge is that emotion style and intensity are even more subjective and complex than discrete emotion categories, making them difficult to model. Two general approaches for achieving controllable emotional TTS involve controlling conditioning features or manipulating internal emotion representations. That is, one approach uses conditioning features of emotion intensity, such as relative ranking matrices [6, 7, 8, 9, 10, 11], distance-based quantization [12], or voiced, unvoiced, and silence (VUS) states [13]. The alternative approach involves the manipulation of internal emotion representations through the application of scaling factors [14, 15] or interpolation of the embedding space [16]. However, despite these methods, the explicit control of emotion style and intensity remains a largely unexplored topic in emotional speech synthesis. Figure 1: (a) Three-dimensional valence-arousal-dominance (VAD) cubes of emotions, where all emotional styles occur as derivative states of primary emotions. Emotional intensity control method is used for (b) conventional models and (c) the proposed model with consideration for emotional style. Another approach to controlling emotional expression involves utilizing emotional dimensions. Compared to the discrete emotion approach, the dimensional approach, such as Russell‚Äôs circumplex model, provides a more precise method for capturing the nuances between different emotional states [1]. Recently, studies on TTS systems have attempted to control emotional attributes through the emotion dimension [17, 18]. In one of these studies, a prosody control block is extended by incorporating the continuous space of arousal and valence to allow interpretable emotional prosody control [18]. Another study proposes an expressive TTS model with a semi-supervised latent variable to control emotions in six discrete emotional states of arousal-valence combinations [17]. However, this setup requires labor-intensive annotations, which are more expensive to obtain than categorical labels and more susceptible to annotator bias. The emotional dimension model also exhibits limitations when explicitly controlling emotion style and intensity. Most emotional TTS systems utilize Sequence-to-sequence (Seq2Seq) models, which not only predict the duration of speech automatically but also learn feature mapping and alignment simultaneously [19, 20]. The attention mechanism in these models allows them to focus on the emotionally emphasized parts of an utterance [21]. However, Seq2Seq models face the typical challenges of auto-regressive models, such as long-term dependence and repetition problems. Furthermore, most emotional speech syntheses adopt fine-tuning to control emotion intensity on a single speaker dataset; however, some of these methods exhibit noticeably degraded speech quality [7, 10]. Researchers have explored acoustic models for emotion transfer to ensure the capture of expressiveness when synthesizing acoustic features. Existing methods focus on emotion transfer using discrete emotion labels, which overlook the complexity of emotions conveyed in human speech [15, 22]. To address this issue, a TTS system with high generalization capability in zero-shot emotion transfer scenarios must be implemented [23, 24, 25, 26, 2]. Figure 2: Illustration of coordinate transformations: (a) Cartesian-to-spherical coordinate transformation and (b) Emotion-adaptive coordinate transformation. In psychology, several studies [27, 28] have been conducted to explore the frameworks and methods for measuring complex emotions that arise from more primary emotional states. Moreover, TTS models are being challenged by the increasing demand for personalized speech generation, focusing on achieving high generalization capability and producing high-quality speech. However, the following primary research problems exist: (1) how can emotional style and intensity be defined and modeled as derivatives of primary emotions, (2) how can high-quality emotional style be controlled and transferred in zero-shot scenarios, and (3) how can the synthesized emotional speech be evaluated. In this article, we address these challenges, with our primary contributions listed as follows: ‚Ä¢ We introduce an emotion-adaptive coordinate transformation that models an emotion-adaptive spherical vector (EASV), allowing for subtle control over emotion style and intensity. ‚Ä¢ We introduce a multi-level style encoder for high- and low-level style adaptations of custom utterances. Particularly, we introduce additional loss functions to enhance the emotion transfer performance for zero-shot scenarios. ‚Ä¢ We propose a novel objective evaluation of spherical vector angle similarity (SVAS) to assess the emotional expressiveness of the speech. ‚Ä¢ We carefully designed objective and subjective evaluations to demonstrate the effectiveness and contributions of the proposed model from multiple perspectives. Audio samples and source code are available at https://github.com/Choddeok/EmoSpherepp."
https://arxiv.org/html/2411.02609v1,"Estimating the Number and Locations of Boundaries in Reverberant Environments with Deep Learning‚Ä†‚Ä†thanks:This work was supported by NSF grants CCF-1911094 and IIS-1730574; ONR grants N00014-18-1-2571, N00014-20-1-2534, N00014-23-1-2714, N00014-24-1-2225, N000014-23-1-2803, and MURI N00014-20-1-2787; AFOSR grant FA9550-22-1-0060; DOE grant DE-SC0020345; DOI grant 140D0423C0076; and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047.","Underwater acoustic environment estimation is a challenging but important task for remote sensing scenarios. Current estimation methods require high signal strength and a solution to the fragile echo labeling problem to be effective. In previous publications, we proposed a general deep learning-based method for two-dimensional environment estimation which outperformed the state-of-the-art, both in simulation and in real-life experimental settings. A limitation of this method was that some prior information had to be provided by the user on the number and locations of the reflective boundaries, and that its neural networks had to be re-trained accordingly for different environments. Utilizing more advanced neural network and time delay estimation techniques, the proposed improved method no longer requires prior knowledge the number of boundaries or their locations, and is able to estimate two-dimensional environments with one or two boundaries. Future work will extend the proposed method to more boundaries and larger-scale environments.","Estimation of the reflective boundaries in reverberant environments is an important yet difficult task for underwater and indoor acoustics [1, 2, 3], that allows the use of non-line of sight (NLOS) arrivals to enhance localization performance [4]. Over short ranges, these boundaries can be approximated as planar, which yield mirror images of the true emitter as ‚Äòvirtual‚Äô emitters. Thus, given a known or estimated emitter location, and known receiver locations, a variety of methods can be used for boundary estimation through virtual emitter localization [5], [6], [7]. However, low signal-to-noise ratios (SNRs) [8] are not addressed by these existing methods. They also generally require the solution of a difficult combinatorial echo labeling problem to differentiate between the boundaries [9], complicated by missing or spurious echoes. Figure 1: General simulation setting: each NLOS arrival yields an ellipse whose common tangents are reflective boundaries. In previous work [10], [11], we introduced the convolutional neural network-based (CNN) Neuro-COTANS method for boundary estimation, to overcome these challenges. Neuro-COTANS leveraged the fact that in 2D, a NLOS arrival corresponds to an ellipse with a path distance of dNLOSsubscriptùëëNLOSd_{\textrm{NLOS}}italic_d start_POSTSUBSCRIPT NLOS end_POSTSUBSCRIPT, whose foci are the emitter and receiver locations. Multiple receivers define multiple ellipses whose common tangents are the boundaries, as in Fig. 1. We parametrized the tangents to ellipses by their range œÅùúå\rhoitalic_œÅ and azimuth Œ∏ùúÉ\thetaitalic_Œ∏ [4], calling this œÅùúå\rhoitalic_œÅ- and Œ∏ùúÉ\thetaitalic_Œ∏-space the common tangents to spheroids (COTANS) domain. Creating COTANS-domain images where each ellipse corresponded to a curve as in Fig. 2, we trained an AlexNet [12] neural network (NN) to estimate boundaries from the unlabeled NLOS distances. Neuro-COTANS proved more robust than methods which applied heuristic smoothing filters to COTANS images [13], [14], [15]. It also outperformed the state-of-the-art least-squares (LS) [5] and Euclidean distance matrices (EDM) [16], [6] methods by up to 6 dB SNR, even when they were advantageously initialized with the correct echoes. While Neuro-COTANS achieved groundbreaking performance, it had several shortcomings that motivated future work. One of them was that the number of boundaries had to be assumed known. Another was that the boundaries were assumed to be known to within 10‚àòsuperscript1010^{\circ}10 start_POSTSUPERSCRIPT ‚àò end_POSTSUPERSCRIPT in azimuth, informed by prior knowledge. These constraints meant that the NN had to be retrained for different environments. In this paper, we introduce U-COTANS, a U-Net [17] method which introduces the critical operational capabilities of estimating the number of boundaries and covering the entire COTANS image of a given resolution and scale. We retain the multi-scale, multi-stage framework of Neuro-COTANS [11], while exploiting the proven capabilities of U-Nets to solve difficult 2D estimation problems through image segmentation [18]. Through a careful choice of training and test models, we maintain our groundbreaking performance while incorporating new abilities into our overall approach. The paper is organized as follows. In Section II, we briefly review the COTANS image generation framework introduced in our past work. The U-COTANS method is detailed in Section III. Simulation results are presented in Section IV, and final remarks are given in Section V. Our code is publicly available at https://github.com/torosarikan/U-COTANS."
https://arxiv.org/html/2411.02551v2,"PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text","While piano music has become a significant area of study in Music Information Retrieval (MIR), there is a notable lack of datasets for piano solo music with text labels. To address this gap, we present PIAST (PIano dataset with Audio, Symbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy of semantic tags, we collected 9,673 tracks from YouTube and added human annotations for 2,023 tracks by music experts, resulting in two subsets: PIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and transcribed MIDI utilizing state-of-the-art piano transcription and beat tracking models. Among many possible tasks with the multimodal dataset, we conduct music tagging and retrieval using both audio and MIDI data and report baseline performances to demonstrate its potential as a valuable resource for MIR research.","Piano music presents unique opportunities for music research due to its ability to express diverse styles using a single instrument and its superior transcription performance. Given these characteristics, it has become a significant area of study in Music Information Retrieval (MIR), encompassing tasks such as classification Hung et al. (2021); Chou et al. (2021), and music generation with various conditions Wu and Yang (2023); Choi and Lee (2023). While these tasks require datasets that combine piano audio with various modalities such as MIDI, sheet music, or text, there is a notable scarcity of such comprehensive multimodal piano datasets. However, existing multimodal music datasets, particularly music-text datasets, rarely focus exclusively on piano music, and piano solo pieces comprise only a small portion of general music-text datasets. For instance, in the ECALS Dataset Doh et al. (2023), a subset of the Million Song Dataset Bertin-Mahieux et al. (2011), the number of piano solo tracks is very limited. We observed that excluding tracks tagged with instruments other than the piano or genres that could not be solely represented by the piano, only approximately 0.46% of the entire dataset can be identified as piano solo music. Several piano datasets, such as MAESTRO Hawthorne et al. (2019), have been developed in recent years, which provide classical piano performances primarily used for piano transcription. Another classical piano dataset, GiantMIDI Kong et al. (2022), is also commonly used in transcription tasks. Other datasets like Pop1K7 Hsiao et al. (2021) focus on the performance generation of pop piano music, while PiJAMA Edwards et al. (2023) is employed for performer identification tasks with their jazz piano data. However, these datasets are confined to a single genre and lack text labels. This absence of genre diversity within a single dataset and the lack of textual information underscores the need for a piano dataset with text information. Some piano datasets contain emotion labels, such as EMOPIA Hung et al. (2021) and VGMIDI Ferreira and Whitehead (2019). However, these datasets are annotated only with emotion information based on either Russell‚Äôs four quadrants Hung et al. (2021) or the valence-arousal model Ferreira and Whitehead (2019). This limited annotation approach lacks the rich textual descriptions needed for text-based MIR tasks. To address the limitations, we present multimodal piano music data with rich text annotations and transcribed MIDI. To build the dataset, we first created a piano-specific taxonomy with 31 tags that include genre, emotion, mood, and style information to encompass the broad and diverse musical range that the piano can express. Based on this taxonomy, we collected data from YouTube, transcribed it to MIDI format, and conducted an annotation process. The PIAST dataset consists of two subsets: PIAST-YT, 9,673 tracks collected from YouTube, providing audio and text information (titles, tags, and descriptions), and PIAST-AT, 2,023 tracks with annotations by music experts. This dual approach ensures both breadth and accuracy in the dataset. Additionally, PIAST includes transcribed performance MIDI data alongside audio and text, enhancing its capabilities beyond existing methods Hsiao et al. (2021). This paper details the dataset collection process and analyzes the data. We present baseline results for piano music annotation and retrieval tasks, utilizing the PIAST-YT and the PIAST-AT datasets across audio and MIDI domains. The PIAST dataset is available in our online repository111https://hayeonbang.github.io/PIAST_dataset/, and the source code for the experiments can be found on GitHub222https://github.com/Hayeonbang/PIAST."
https://arxiv.org/html/2411.02402v1,Optimal Transport Maps are Good Voice Converters,"Recently, neural network-based methods for computing optimal transport maps have been effectively applied to style transfer problems. However, the application of these methods to voice conversion is underexplored. In our paper, we fill this gap by investigating optimal transport as a framework for voice conversion. We present a variety of optimal transport algorithms designed for different data representations, such as mel-spectrograms and latent representation of self-supervised speech models. For the mel-spectogram data representation, we achieve strong results in terms of Fr√©chet Audio Distance (FAD). This performance is consistent with our theoretical analysis, which suggests that our method provides an upper bound on the FAD between the target and generated distributions. Within the latent space of the WavLM encoder, we achived state-of-the-art results and outperformed existing methods even with limited reference speaker data.","The goal of VC is to generate a modified voice that maintains the linguistic content of the source speaker while adopting the prosody and vocal characteristics of a target speaker [1]. It has various applications including voice modification [2], singing voice conversion [3], and privacy [4]. The most common case is non-parallel VC, i.e. when the speech content is different for the source and target speaker datasets. Existing VC methods such as StarGAN-VC [5, 6], Diff-VC [7] and kNN-VC [8] deliver impressive results. But these models have an drawbacks, such as complex training procedures, significant computational resources required for inference or large quantity of target speaker data (\wasyparagraph2.2). There has been recent interest in the use of neural network-based optimal transport (OT) maps for generative modelling in high-dimensions [9, 10, 11]. However, the potential of optimal transport maps in voice conversion has not been comprehensively explored. In this paper, we fill this gap by proposing variants of OT methods for voice conversion. ‚Ä¢ For mel-spectogram representation we present a neural network-based OT approach called NOT-VC (\wasyparagraph3). This method proves to be resource-efficient in terms of inference and simpler in terms of training compared to its predecessors as DiffVC and StarGAN-VC respectively. We justify our approach theoretically by analyzing the recovered maps and show that the learned map upper bounds the FAD [12] between real and generated data (\wasyparagraph3.1). Furthermore, we extend this method to create an Extremal NOT [13] named XNOT-VC (\wasyparagraph3.2) and demonstrate state-of-the-art (SOTA) performance according to FAD. ‚Ä¢ For voice conversion within the latent space of the WavLM [14] model (\wasyparagraph3.3), we propose an OT-based Flow-Matching [15, 16] approach named FMVC. This method is lightweight and effectively circumvents the limitations of the current best-performing any-to-any VC method named kNN-VC [8], providing SOTA results even with limited target data (\wasyparagraph5). In our paper, we explore the applicability of optimal transport to non-parallel VC. The main contribution is the development and theoretical justification of several OT methods that achieve impressive results and avoid the limitations of existing VC approaches."
https://arxiv.org/html/2411.03172v1,"Blind Estimation of Sub-band Acoustic Parameters from Ambisonics Recordings using Spectro-Spatial Covariance Features‚Ä†‚Ä†thanks:The work for this paper was conducted as a Research
Internship at Dolby Laboratories Australia.","Estimating frequency-varying acoustic parameters is essential for enhancing immersive perception in realistic spatial audio creation. In this paper, we propose a unified framework that blindly estimates reverberation time (T60), direct-to-reverberant ratio (DRR), and clarity (C50) across 10 frequency bands using first-order Ambisonics (FOA) speech recordings as inputs. The proposed framework utilizes a novel feature named Spectro-Spatial Covariance Vector (SSCV), efficiently representing temporal, spectral as well as spatial information of the FOA signal. Our models significantly outperform existing single-channel methods with only spectral information, reducing estimation errors by more than half for all three acoustic parameters. Additionally, we introduce FOA-Conv3D, a novel back-end network for effectively utilising the SSCV feature with a 3D convolutional encoder. FOA-Conv3D outperforms the convolutional neural network (CNN) and recurrent convolutional neural network (CRNN) backends, achieving lower estimation errors and accounting for a higher proportion of variance (PoV) for all 3 acoustic parameters.","In recent years, dynamic parameterization of acoustic environments has gained significant attention. Estimating acoustic parameters across different frequencies is crucial for simulating virtual sounds, ensuring Virtual Reality (VR) and Augmented Reality (AR) applications provide a congruent and plausible acoustic simulation [1]. Instead of measuring acoustic attributes from microphone captures using controlled signals such as sine sweeps [2], earlier studies focused on determining acoustic parameters from microphone captures of a prior unknown speech signal, known as blind acoustic parameters estimation [3, 4, 5, 6, 7, 8]. However, these studies typically predict average acoustic parameters across all frequencies, failing to capture the natural frequency-dependent variations in acoustic parameters that happen across various spaces. On the other hand, research into the estimation of acoustic contexts, such as Direction-of-Arrival (DoA) and reverberation time (T60), are increasingly relying on deep networks [8, 9, 10, 11, 12, 13, 14, 15]. This shift is driven not only by their superior performance [9, 8], but also by the limitation of signal processing methods, which depend on specific environmental assumptions and struggle to generalize across different tasks [16, 14]. Existing deep learning based approaches are mainly developed to estimate acoustic parameters from a single-channel audio signal, which contains limited spatial information [17, 6, 8, 7]. Recent research has shifted toward multi-channel signals. In [17], the author proposed a Convolution Neural Network (CNN) based framework that extracted binaural features from dual-channel signals for T60 and room volume estimation. With multi-channel microphone rigs becoming more available, efficiently utilizing spatial information to enhance deep network estimation performance has emerged as a new challenge. First-order Ambisonics (FOA) is a prevalent audio recording format in AR and VR [18], with its four channels (W, X, Y, Z) capturing a soundfield in all directions. The integration of FOA and deep networks has been well-established for DoA estimation [10, 19, 20]. However, its potential for estimating other acoustic parameters remains largely unexplored. Inspired by [21], we propose a unified framework that derives the Spectro-Spatial Covariance Vector (SSCV) from FOA recordings to estimate frequency-varying DRR, T60, and C50, as illustrated in Figure 1. To the best of our knowledge, this is the first method to utilize FOA inputs for blind estimation of frequency-dependent acoustic parameters. Figure 1: The problem context of this paper"
https://arxiv.org/html/2411.02860v1,Continual Audio-Visual Sound Separation,"In this paper, we introduce a novel continual audio-visual sound separation task, aiming to continuously separate sound sources for new classes while preserving performance on previously learned classes, with the aid of visual guidance. This problem is crucial for practical visually guided auditory perception as it can significantly enhance the adaptability and robustness of audio-visual sound separation models, making them more applicable for real-world scenarios where encountering new sound sources is commonplace. The task is inherently challenging as our models must not only effectively utilize information from both modalities in current tasks but also preserve their cross-modal association in old tasks to mitigate catastrophic forgetting during audio-visual continual learning. To address these challenges, we propose a novel approach named ContAV-Sep (Continual Audio-Visual Sound Separation). ContAV-Sep presents a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to uphold the cross-modal semantic similarity through incremental tasks and retain previously acquired knowledge of semantic similarity in old models, mitigating the risk of catastrophic forgetting. The CrossSDC can seamlessly integrate into the training process of different audio-visual sound separation frameworks. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance compared to other continual learning baselines for audio-visual sound separation. Code is available at: https://github.com/weiguoPian/ContAV-Sep_NeurIPS2024.","Humans can effortlessly separate and identify individual sound sources in daily experience [25, 7, 64, 33]. This skill plays a crucial role in our ability to understand and interact with the complex auditory environments that surround us [34]. However, replicating this capability in machines remains a significant challenge due to the inherent complexity of real-world auditory scenes [7, 77]. Inspired by the multisensory perception of humans [62, 60], audio-visual sound separation tackles this challenge by utilizing visual information to guide the separation of individual sound sources in an audio mixture. Recent advances in deep learning have led to significant progress in audio-visual sound separation [84, 23, 21, 67, 14, 65, 81, 63, 11, 71]. Benefiting from more advanced architectures (e.g., U-Net [84, 23], Transformer [14], and diffusion models [27]) and discriminative visual cues (e.g., grounded visual objects [67], motion [83], and dynamic gestures [21]), audio-visual separation models are able to separate sounds ranging from domain-specific speech, musical instrument sounds to open-domain general sounds within training sound categories. However, a limitation of these studies is their focus on scenarios where all sound source classes are presently known, overlooking the potential inclusion of unknown sound source classes during inference in real-world applications. This oversight leads to the catastrophic forgetting issue [32, 3], where the fine-tuning of models on new classes detrimentally impacts their performance on previously learned classes. Despite Chen et al. [14] demonstrating that their iQuery model can generalize to new classes well through simple fine-tuning, it still suffers from the catastrophic forgetting problem on old classes. This prevents the trained models from continuously updating in real-world scenarios, impeding their adaptability to dynamic environments. The question how to effectively leverage visual guidance to continuously separate sounds from new categories while preserving separation ability for old sound categories remains open. Figure 1: Top: Illustration of the continual audio-visual sound separation task, where the model (separator) learns from sequential audio-visual sound separation tasks. Bottom: Illustration of the catastrophic forgetting problem in continual audio-visual sound separation and its mitigation by our proposed method. Fine-tuning: Directly fine-tune the separation model on new sound source classes; Upper bound: Train the model using all training data from seen sound source classes. To bridge this gap, we introduce a novel continual audio-visual sound separation task by integrating audio-visual sound separation with continual learning principles. The goal of this task is to develop an audio-visual model that can continuously separate sound sources in new classes while maintaining performance on previously learned classes. The key challenge we need to address is catastrophic forgetting during continual audio-visual learning, which occurs when the model is updated solely with data from new classes or tasks, resulting in a significant performance drop on old ones. We illustrate our new task and the catastrophic forgetting issue in Fig. 1. Unlike typical continual learning problems such as task-, domain-, or class-incremental classification in visual domains [2, 57, 38, 53, 85], which result in progressively increasing logits (or probability distribution) across all observed classes at each incremental step, our task uniquely produces fixed-size separation masks throughout all incremental steps. In this context, each entry in the mask does not directly correspond to any specific classes. Additionally, the new task involves both audio and visual modalities. Therefore, simply applying existing visual-only methods cannot fully exploit and preserve the inherent cross-modal semantic correlations. Very recently, Pian et al. [53] and Mo et al. [44] extended continual learning to the audio-visual domain, but both focused on classification tasks. To address these challenges, in this paper, we propose a novel approach named ContAV-Sep (Continual Audio-Visual Sound Separation). Upon the framework, we introduce a novel Cross-modal Similarity Distillation Constraint (CrossSDC) to not only maintain the cross-modal semantic similarity through incremental tasks but also preserve previously learned knowledge of semantic similarity in old models to counter catastrophic forgetting. The CrossSDC is a generic constraint that can be seamlessly integrated into the training process of different audio-visual sound separators. To evaluate the effectiveness of our proposed ContAV-Sep, we conducted experiments on the MUSIC-21 dataset within the framework of continual learning, using the state-of-the-art audio-visual sound separation model iQuery [14] and a representative audio-visual sound separation model Co-Separation [23], as our separation base models. Experiments demonstrate that ContAV-Sep can effectively mitigate catastrophic forgetting and achieve significantly better performance than other continual learning baselines. In summary, this paper contributes follows: (i) To explore more practical audio-visual sound separation, in which the separation model should be generalized to new sound source classes continually, we pose a Continual Audio-Visual Sound Separation task that trains the separation model under the setting of continual learning. To the best of our knowledge, this is the first work on continual learning for audio-visual sound separation. (ii) We propose ContAV-Sep for the new task. It uses a novel cross-modal similarity distillation constraint to preserve cross-modal semantic similarity knowledge from previously learned models. (iii) Experiments on the MUSIC-21 dataset can validate the effectiveness of our ContAV-Sep, demonstrating promising performance gain over baselines."
https://arxiv.org/html/2411.02026v1,CTEFM-VC: Zero-Shot Voice Conversion Based on Content-Aware Timbre Ensemble Modeling and Flow Matching,"Zero-shot voice conversion (VC) aims to transform the timbre of a source speaker into any previously unseen target speaker, while preserving the original linguistic content. Despite notable progress, attaining a degree of speaker similarity and naturalness on par with ground truth recordings continues to pose great challenge. In this paper, we propose CTEFM-VC, a zero-shot VC framework that leverages Content-aware Timbre Ensemble modeling and Flow Matching. Specifically, CTEFM-VC disentangles utterances into linguistic content and timbre representations, subsequently utilizing a conditional flow matching model and a vocoder to reconstruct the mel-spectrogram and waveform. To enhance its timbre modeling capability and the naturalness of generated speech, we propose a context-aware timbre ensemble modeling approach that adaptively integrates diverse speaker verification embeddings and enables the joint utilization of linguistic and timbre features through a cross-attention module. Experiments show that our CTEFM-VC system surpasses state-of-the-art VC methods in both speaker similarity and naturalness by at least 18.5% and 7.0%.","As a pivotal task within the field of speech signal processing, zero-shot voice conversion (VC) seeks to transfer the timbre of a source utterance to an unseen target speaker while maintaining the original phonetic content, with applications spanning various practical domains such as voice anonymization [1] and audiobook production [2]. ‚Ä†‚Ä†‚Ä†‚Ä†\dagger‚Ä† denotes the corresponding author. Overall, the core difficulties in zero-shot VC lies in effectively modeling, decoupling, and utilizing various speech attributes, including content, timbre, and so forth. Previous zero-shot VC approaches [3, 4, 5] often use pre-trained automatic speech recognition (ASR) [6, 7] and speaker verification (SV) models [8, 9] to extract content and timbre information from the source and target speech, respectively. Nevertheless, due to factors such as the inherent complexity of speech signals [10, 11] and the limitations in timbre and content modeling [12, 13], these methods present significant opportunities for performance enhancement. With recent progressions of self-supervised learning (SSL) based speech models [14, 15], many works [16, 12] have sought to use them to extract semantic features from utterances. However, due to limitations of the SSL mechanism, the extracted features inevitably contain certain source timbre characteristics, ultimately affecting their VC quality. To this end, [17, 13, 18] incorporated the model quantization technique to minimize non-content elements. Nonetheless, this operation incurs additional training overhead, and variations in training objectives may lead to token format inconsistencies across different models, thereby limiting broader applicability. Moreover, existing VC methods [3, 5, 12, 18] normally employ a single pre-trained SV model to capture target timbre embeddings. Although speaker embedding techniques have advanced significantly [9, 19, 20], relying exclusively on a single model is insufficient to deliver high-performance VC, resulting in subpar speaker similarity compared to authentic recordings. Inspired by the powerful zero-shot capabilities of large-scale language models (LLMs) [21], recent studies [22, 23] attempted to discretize utterances with neural speech codecs [24, 25], so as to leverage LLMs to generate target waveform in an auto-regressive manner. For instance, [22] proposed a two-stage language model that first generates coarse acoustic tokens to capture source linguistic content and target timbre, then refines acoustic details for VC. Afterwards, [23] implemented a single-stage VC framework based on context-aware language model and acoustic predictor, facilitating zero-shot voice conversion in a streamable way. Despite these advancements, such methods commonly encounter stability problems due to their auto-regressive fashion and may experience error accumulation, leading to a gradual decline in VC performance. Figure 1: Overall training architecture of the proposed CTEFM-VC framework. To address aforementioned problems, we propose CTEFM-VC, an effective zero-shot VC framework based on content-aware timbre ensemble modeling and flow matching. To be specific, we first use a pretrained ASR model HybridFormer [7] to capture the precise linguistic content of the source speech. Subsequently, to improve timbre modeling capabilities, we introduce an ensemble strategy that leverages multiple pre-trained SV models to extract timbre embeddings from the target speech, concatenating these embeddings as conditional inputs for a conditional flow matching (CFM) module. Additionally, to further enhance the speaker similarity and naturalness of the entire system, we propose an effective and easily scalable content-aware timbre ensemble modeling (CTE) approach that incorporates learnable hyperparameters for each SV embedding and utilizes a cross-attention module to achieve adaptive fusion between timbre embeddings and linguistic content features, thereby generating higher-quality representations to serve as inputs for the CFM module. Finally, we incorporate the CFM and pre-trained vocoder [26] to reconstruct the mel-spectrogram and generate the converted speech. Experimental results demonstrate that the proposed CTEFM-VC approach outperforms several state-of-the-art (SOTA) zero-shot VC systems in terms of both speaker similarity and naturalness, achieving relative improvements of at least 18.5% and 7.0%, respectively."
https://arxiv.org/html/2411.01805v1,MoMu-Diffusion: On Learning Long-Term Motion-Music Synchronization and Correspondence,"Motion-to-music and music-to-motion have been studied separately, each attracting substantial research interest within their respective domains. The interaction between human motion and music is a reflection of advanced human intelligence, and establishing a unified relationship between them is particularly important. However, to date, there has been no work that considers them jointly to explore the modality alignment within. To bridge this gap, we propose a novel framework, termed MoMu-Diffusion, for long-term and synchronous motion-music generation. Firstly, to mitigate the huge computational costs raised by long sequences, we propose a novel Bidirectional Contrastive Rhythmic Variational Auto-Encoder (BiCoR-VAE) that extracts the modality-aligned latent representations for both motion and music inputs. Subsequently, leveraging the aligned latent spaces, we introduce a multi-modal Transformer-based diffusion model and a cross-guidance sampling strategy to enable various generation tasks, including cross-modal, multi-modal, and variable-length generation. Extensive experiments demonstrate that MoMu-Diffusion surpasses recent state-of-the-art methods both qualitatively and quantitatively, and can synthesize realistic, diverse, long-term, and beat-matched music or motion sequences. The generated samples and codes are available at https://momu-diffusion.github.io/.","Dancing to the musical beats or creating a variety of rhythmically synchronized music for a given motion is a fundamental aspect of human creativity. Music and human motions serve as universal languages that are shared by all civilizations, transcending cultural and geographical boundaries around the world [25]. For computational methodologies, the motion-music generation poses several challenges: 1) maintaining long-term coherence in typically lengthy motion-music sequences 2) ensuring temporal synchronization and rhythmic alignment between motion and music sequences, and 3) generating realistic, diverse, and variable-length human motions or music. Existing works usually divide the motion-music generation into two distinct tasks: motion-to-music and music-to-motion. For motion-to-music, some methods compress the conditional video frames into a single image, in which the temporal information is lost [52, 53]. The state-of-the-art work, LORIS [49], employs a hierarchical conditional diffusion model to generate long-term musical waveforms. However, LORIS introduces huge computational costs and training difficulties since it generates long-term musical waveforms directly. For music-to-motion, the Dancing2Music (D2M) [26] framework divides the generation process into two stages: decomposing the dance into basic dancing movements with a VAE and compositing the basic movements into dance with a GAN. Nonetheless, D2M‚Äôs approach of segmenting long-term music into short clips (approximately 1-2 seconds) diminishes the coherence of the synthesized motion sequences. Table 1: Comparison with the state-of-the-art audio-visual generation works, including but not limited to motion-music generation. Method Pub. Joint Generation Pretrain Long-Term Synthesis Latent Space Diff-Foley NeurIPS‚Äô23 ‚úó ‚úì ‚úó ‚úì MM-Diffusion CVPR‚Äô23 ‚úì ‚úó ‚úó ‚úó LORIS ICML‚Äô23 ‚úó ‚úó ‚úì ‚úó D2M NeurIPS‚Äô19 ‚úó ‚úì ‚úó ‚úì CDCD ICLR‚Äô23 ‚úó ‚úó ‚úì ‚úì MoMu-Diffusion ‚úì ‚úì ‚úì ‚úì Motivated by the fact that human motions are highly associated with music yet existing computational methods often study them in isolation, we propose a novel multi-modal framework, termed MoMu-Diffusion, to address the aforementioned challenges jointly. Firstly, to mitigate the computational costs and optimization complexities raised by long sequences, we employ a VAE to encode both motion and music sequences into latent spaces. Subsequently, to investigate the relationship between human movements and musical beats, we propose rhythmic contrastive learning. This approach involves constructing contrast pairs with a kinematic amplitude indicator, which quantifies the temporal variation in motion and is derived from the spatial motion directrogram differences as detailed in [4]. Given that the motion and music sequences are interactively aligned in the latent space to discern the correlation between kinematic shifts and musical rhythmic beats, we call our model as the Bidirectional Contrastive Rhythmic VAE (BiCoR-VAE). With the aligned latent space, we introduce a Transformer-based diffusion model that captures long-term dependencies and facilitates sequence generation across variable lengths. Additionally, we introduce a simple cross-guidance sampling strategy that integrates different cross-modal generation models, enabling multi-modal joint generation without extra training. By incorporating the BiCoR-VAE and the diffusion Transformer model, our MoMu-Diffusion framework effectively models the long-term motion-music synchronization and correspondence, enabling motion-to-music, music-to-motion, and joint motion-music generation. Moreover, MoMu-Diffusion supports generating motion-music samples in variable lengths. The pipeline of MoMu-Diffusion is illustrated in Figure 1. We have conducted extensive experiments on three motion-to-music and two music-to-motion datasets, including scenarios such as dancing and competitive sports. The experimental results demonstrate that MoMu-Diffusion attains state-of-the-art performance across both objective and subjective metrics, significantly enhancing music/motion quality and cross-modal rhythmic/kinematic alignment. Furthermore, we have carried out abundant ablation studies to validate the efficacy of the BiCoR-VAE and the DiT architecture. A comparative analysis with state-of-the-art motion-to-music methods CDCD [53] and LORIS [49], 2D music-to-motion method D2M [26], and general video-to-audio methods Diff-Foley [33] and MM-Diffusion [41], is presented in Table 1."
https://arxiv.org/html/2411.01661v1,Sing-On-Your-Beat: Simple Text-Controllable Accompaniment Generations,"Singing is one of the most cherished forms of human entertainment. However, creating a beautiful song requires an accompaniment that complements the vocals and aligns well with the song‚Äôs instruments and genre. With advancements in deep learning, previous research has focused on generating suitable accompaniments but often lacks precise alignment with the desired instrumentation and genre. To address this, we propose a straightforward method that enables control over the accompaniment through text prompts, allowing the generation of music that not only complements the vocals but also aligns with the song‚Äôs instrumental and genre requirements. Through extensive experiments, we successfully generate 10-second accompaniments using vocal input and text control. Additionally, our method demonstrates robust control over the generated accompaniment based on input prompts, improving alignment with the song‚Äôs instrumental and genre needs. A link to our work is available at https://songgen-ai.github.io/llambada-demo/.","Music is a fundamental aspect of human culture. In recent years, with the rapid advancement of deep learning, several methods for non-vocal music generation have been proposed, such as AudioLM Borsos et al. (2023), MusicGen Copet et al. (2024), and Stable Audio Evans et al. (2024). These methods have shown promising results in addressing music generation challenges. In addition to non-vocal music, vocal music generation has become a significant task today, with various real-world applications. However, generating vocal music requires both the singing voice and instrumental accompaniment, as they play crucial roles in the process. Naturally, in singing, the vocals and melody align with the beat, essential to forming a complete song. For this reason, SingSong Donahue et al. (2023) and Fast-SAG Chen et al. (2024) are two of the first systems designed to generate instrumental audio that accompanies vocal recordings. Regarding SingSong, they propose a system that adapts AudioLM Borsos et al. (2023) with the Transformer model to train an ""audio-to-audio"" model for the accompaniment generation via the vocal input. With FastSAG, they propose a Non-Autoregressive diffusion-based framework that is conditioned based on the vocal signal to directly generate the Mel-Spectrogram of the accompaniment. From their published results, the quality of the output audio is superior and their results are promising and have the potential to enhance music production. Since the beat and accompaniment reflect the creativity of the producer, users often want control over these elements, including the specific instruments, beat type, rhythm, and tempo. Despite the promising audio quality of previous works, they have struggled to generate accompaniment that meets users‚Äô creative demands. To address this challenge, we propose Llambada, a simple, text-controllable system for accompaniment generation. Llambada allows users to define their desired accompaniment by inputting their requirements through a text prompt, enabling the input vocal sings along to their custom beats. This model incorporates AudioLM Borsos et al. (2023), the Audio-Language Alignment Model (CLAP) Elizalde et al. (2023), Encodec D√©fossez et al. (2022), and Mert Li et al. (2023). These components process text prompt tokens alongside vocal semantic tokens‚Äîrepresenting the general rhythm and harmony of the accompaniment‚Äîto generate accompaniment that is both in sync with the vocal input and text-controllable. To our knowledge, our system is one of the first systems that support the accompaniment generation with text control. Moreover, the challenge of this task is the lack of a dataset. Previous works focus on the dataset with text for non-vocal music, and they do not concentrate on the prompt for the full song or the accompaniment. Due to the challenge of the dataset, we also propose the pipeline to generate the vocal and accompaniment instrumental with text prompt control to solve the challenge of text-controllable music generation, which can facilitate the dataset creation for the training of the model, thus encouraging the further research to explore and improve the quality of the accompaniment generation tasks. In our work, through extensive experiments, we successfully generate the 10-second accompaniment from vocal input with text control, which illustrates the promise of our method. ‚Ä¢ We propose a system that generates the accompaniment from vocal input with the control from the text prompt that can allow the user to control the accompaniment generation via prompt input. ‚Ä¢ We propose a dataset pipeline to support the accompaniment via text control tasks, which can be further used for other tasks of music generation. ‚Ä¢ We conduct extensive experiments to evaluate the effectiveness of the Llambada models. Experimental results show that our Llambada model can generate high-quality music, with text effectively controlling the instrumentation and song type. Additionally, we successfully achieve 10-second accompaniment generation using vocal input and text control. ‚Ä¢ We open-source the implementation and the pre-trained (if possible) of the Llambada for further research in the accompaniment generation. This paper is organized as follows: in Section 2 we briefly review existing methods related to this research. Then we propose our methods in Section 3. Experiments setup are in Section 4. Finally, we present the conclusion in Section 5."
https://arxiv.org/html/2411.01156v1,Fish-Speech: Leveraging Large Language Models for Advanced Multilingual Text-to-Speech Synthesis,"Text-to-Speech (TTS) systems face ongoing challenges in processing complex linguistic features, handling polyphonic expressions, and producing natural-sounding multilingual speech - capabilities that are crucial for future AI applications. In this paper, we present Fish-Speech, a novel framework that implements a serial fast-slow Dual Autoregressive (Dual-AR) architecture to enhance the stability of Grouped Finite Scalar Vector Quantization (GFSQ) in sequence generation tasks. This architecture improves codebook processing efficiency while maintaining high-fidelity outputs, making it particularly effective for AI interactions and voice cloning.Fish-Speech leverages Large Language Models (LLMs) for linguistic feature extraction, eliminating the need for traditional grapheme-to-phoneme (G2P) conversion and thereby streamlining the synthesis pipeline and enhancing multilingual support. Additionally, we developed FF-GAN through GFSQ to achieve superior compression ratios and near 100% codebook utilization.Our approach addresses key limitations of current TTS systems while providing a foundation for more sophisticated, context-aware speech synthesis. Experimental results show that Fish-Speech significantly outperforms baseline models in handling complex linguistic scenarios and voice cloning tasks, demonstrating its potential to advance TTS technology in AI applications. The implementation is open source at https://github.com/fishaudio/fish-speech.","The past decade has seen remarkable progress in Text-to-Speech (TTS) systems, transforming applications from virtual assistants to educational tools. Current TTS architectures, such as VALL-E [Wang et al. (2023)], VITS [Kim et al. (2021)], Fastspeech [Ren et al. (2020)] typically rely on grapheme-to-phoneme (G2P) conversion [Klatt (1987)] to convert text into phonetic representations before synthesis. While effective, this approach struggles with context-dependent polyphonic words and cross-lingual generalization due to complex phonetic rules. Recent advances in zero-shot voice conversion such as YourTTS [Casanova et al. (2022)] and unified speech generation model UniAudio [Yang et al. (2023)] have shown the potential of neural architectures in handling various speech tasks. Additionally, flow-based models like CosyVoice[Du et al. (2024), MatchaTTSMehta et al. (2024)] have demonstrated promising results in natural speech synthesis. However, most solutions disentangled semantic and accoustic feature as a trade-of to improve stability, and reduce the voice cloning / bg understanding ability. As demand grows for multilingual TTS systems, the limitations of G2P-based approaches become more apparent. The need for language-specific phonetic rules and lexicons hinders scalability and complicates system maintenance. Recent research has explored the use of Large Language Models (LLMs) for direct linguistic feature extraction, eliminating the need for explicit G2P conversion. [Betker (2023)]. We introduce Fish-Speech, a novel TTS framework featuring a serial fast-slow dual autoregressive (Dual-AR) architecture. This design improves the stability of grouped finite scalar vector quantization (GFSQ) in sequence generation while maintaining high-quality output. By incorporating LLMs into the TTS pipeline, Fish-Speech simplifies the synthesis process and better handles polyphonic characters and multilingual text. The model trains on 720,000 hours of multilingual audio data, enabling it to learn diverse linguistic patterns and pronunciation variations. To improve synthesis quality, we develop Firefly-GAN (FFGAN), a new vocoder architecture based on Grouped Finite Scalar Vector Quantization(GFSQ). FFGAN combines Finite Scalar Quantization (FSQ) [Mentzer et al. (2023)], and Group Vector Quantization (GVQ) to optimize compression ratios and codebook usage. Our evaluations show 100% codebook utilization, representing state-of-the-art performance in this field. The primary contributions of this work are as follows: ‚Ä¢ We introduce Fish-Speech, a novel TTS framework that leverages LLMs and a Dual-AR architecture to replace traditional G2P conversion, providing robust and scalable multilingual speech synthesis. ‚Ä¢ We present FFGAN, an advanced vocoder that integrates multiple vector quantization techniques to achieve high-fidelity speech synthesis with optimized compression ratios and codebook utilization. ‚Ä¢ We develop fish-tech acceleration methodologies, the system achieves real-time factors of approximately 1:5 on consumer-grade NVIDIA RTX 4060 mobile platforms and 1:15 on high-performance NVIDIA RTX 4090 configurations. And a latency of 150ms which is far less than other TTS system using DiT and Flow structure. We encourage readers to listen to our samples on fish speech 1.4 sample. We also highly recommend that you go to our online synthesis site fish.audio to try out the different speakers of audio synthesized by the community."
https://arxiv.org/html/2411.01135v2,Music Foundation Model as Generic Booster for Music Downstream Tasks,"We demonstrate the efficacy of using intermediate representations from a single foundation model to enhance various music downstream tasks. We introduce SoniDo, a music foundation model (MFM) designed to extract hierarchical features from target music samples. By leveraging hierarchical intermediate features, SoniDo constrains the information granularity, leading to improved performance across various downstream tasks including both understanding and generative tasks. We specifically evaluated this approach on representative tasks such as music tagging, music transcription, music source separation, and music mixing. Our results reveal that the features extracted from foundation models provide valuable enhancements in training downstream task models. This highlights the capability of using features extracted from music foundation models as a booster for downstream tasks. Our approach not only benefits existing task-specific models but also supports music downstream tasks constrained by data scarcity. This paves the way for more effective and accessible music processing solutions.","A foundation model is a pre-trained model developed on a large-scale dataset that can be adapted for a variety of downstream tasks (Bommasani et al., 2021). Several language processing models (Radford et al., 2021; Brown et al., 2020; Devlin et al., 2018; Team et al., 2024) are considered foundation models due to their ability to unify all language tasks as sequence prediction tasks, effectively addressing multiple tasks with a single model. These foundation models have gained significant attraction and are widely used in everyday applications. In contrast, a powerful music foundation model capable of handling various music downstream tasks for music production is lacking. We categorize the tasks that a music foundation model primarily addresses into two types: understanding tasks, such as tagging and transcription, and generative tasks, such as mixing and mastering. Several multi-task models have been proposed as potential music foundation models (Li et al., 2023a; Yang et al., 2023; Copet et al., 2023; Agostinelli et al., 2023). However, this approach necessitates the inclusion of the desired tasks during the training phase. A notable strategy to overcome this limitation is to inject features extracted from a pre-trained large-scale model into smaller back-end models for downstream tasks that were not seen during training. This ensemble approach, which combines a large-scale model with various smaller models, can effectively function as a music foundation model. The codified audio language modeling (CALM) framework proposed by Castellon et al. (2021) is the first work in this direction, utilizing the intermediate representations from Jukebox (Dhariwal et al., 2020) to tackle music information retrieval (MIR) tasks, covering most music understanding tasks. Beyond MIR,Donahue et al. (2022) leveraged representations from Jukebox for melody transcription. Other studies have followed this approach to address time-invariant MIR tasks using the latest generative models based on residual quantized variational Autoencoders (RQ-VAEs) (Zeghidour et al., 2022; D√©fossez et al., 2023), enhancing the state-of-the-art (SOTA). However, these applications remain limited to music understanding tasks. Li et al. (2023b) expanded the focus to include music source separation, a generative task, but encountered instability issues during training. The performance of this extension does not yet match that of the baselines mentioned by Mitsufuji et al. (2022). An extensive overview of related work can be found in Appendix A. We extended the methodology from MIR tasks to generic music downstream tasks and hypothesize that the representation structure of foundation models is crucial in this case. Specifically, we propose that hierarchical representations, which divide information of varying granularity into different levels of embedding, are expected to provide efficient information hierarchy for all downstream tasks including both understanding and generative tasks. We empirically verify this hypothesis in Section 4. In contrast, music foundation models that have been applied to boost music downstream tasks do not have such a hierarchical structure. For example, Jukebox (Castellon et al., 2021; Donahue et al., 2022) is trained to have multi-level representation inspired from hierarchical latent representation (Razavi et al., 2019); however, each level is independently trained. RQ-VAEs (Yang et al., 2023; Li et al., 2023b) learn factorized representation that has a self-organized coarse-to-fine structure, however, they are not hierarchical. In accordance with the aforementioned hypothesis, we outline this study as follows. We propose and train our music foundation model, SoniDo (meaning sound in Spanish), on a high-fidelity internal dataset***The rights of this internal dataset are trained on licensed content only. Except for as specifically authorized by the rights owner, the rights owner expressly prohibits and has opted out of any text or data mining, web scraping or similar, reproductions, extractions or uses, of its content for any purposes, including in relation to training, developing, or commercializing any Al System. to establish a task-agnostic feature extraction pipeline. SoniDo is a generative model consisting of a multi-level transformer with a multi-level hierarchical encoder. With proper pre-processing, we infuse its intermediate representation as features to task-specific models on various music downstream tasks with data augmentation. Moreover, for understanding tasks, we proposed an on-the-fly data augmentation method called token-out to avoid the overfitting problem. We conducted a performance evaluation by benchmarking with representative tasks from understanding to generative tasks: music tagging, music transcription, music source separation, and music mixing. Table 1: Performance overview of applying extracted features to various music downstream tasks. Bold: best, underline: second best. Results obtained with different evaluation protocols are marked with asterisks. Downstream Task Dataset Metric SoniDo MusicGen Small MusicGen Large CALM w/ Jukebox-5B MERT Task-Specific SOTA Multi-task Music Tagging MusicTagATune ROC-AUC 91.7 90.4 90.5 91.5 91.3 92.0 (Huang et al., 2022a) mAP 41.5 38.8 39.0 41.4 40.2 38.4 Pitch Estimation Nsynth Acc. 93.8 93.3 92.8 91.6 94.4 89.2 (McCallum et al., 2022) Instrument Classification Acc. 78.0 71.9 74.2 70.4 72.6 78.2 (Wang et al., 2022) Emotion Regression EmoMusic Averaged R2superscriptùëÖ2R^{2}italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT 64.7 45.6 46.2 66.9 68.0 63.0* (Castellon et al., 2021) Key Detection GiantSteps Weighted Acc. 63.5 65.2 62.4 66.7 65.6 79.6 (Castellon et al., 2021) Genre Classification GTZAN Acc. 80.7 75.2 70.3 79.7 79.3 83.5 (McCallum et al., 2022) Singer Identification VocalSet Acc. 87.0 82.3 83.3 82.6 87.1 80.3 (Modrzejewski et al., 2023) Technique Identification Acc. 74.4 66.1 63.9 76.7 76.9 65.6 (Yamamoto et al., 2022) Music Transcription MAPS Frame F1 83.92 82.94 81.53 - - 82.89 (Toyama et al., 2023) Note F1 86.45 85.97 85.14 - - 85.14 Note w/ Offset F1 68.27 68.27 66.28 - - 66.34 Note w/ Offset & Velocity F1 51.34 50.42 48.69 - - 48.20 Source Separation MUSDB18 SDR (bass) 9.50 8.86 8.17 4.9 5.6 11.31 (Lu et al., 2024) SDR (drums) 8.65 8.03 7.50 4.1 3.6 9.49 SDR (other) 5.91 5.59 5.54 2.7 3.0 7.73 SDR (vocals) 8.07 7.57 7.66 5.1 5.3 10.66 MDXDB21 hidden SDR (bass) 8.14 7.44 7.40 - - 7.86 (Rouard et al., 2023) SDR (drums) 8.16 8.31 7.37 - - 7.89 SDR (other) 5.21 5.26 4.93 - - 5.09 SDR (vocals) 8.04 7.81 7.73 - - 7.70 Music Mixing MDXDB21-dry hidden Stereo-Invariant 79.86 87.27 87.32 - - 82.09 (Mart√≠nez-Ram√≠rez et al., 2022) Spectralmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.221 0.229 0.228 - - 0.193 Panningmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.175 0.244 0.219 - - 0.179 Dynamicmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.064 0.072 0.073 - - 0.070 Loudnessmapemape{}_{\textrm{mape}}start_FLOATSUBSCRIPT mape end_FLOATSUBSCRIPT 0.171 0.148 0.132 - - 0.152 The encoder design of SoniDo is inspired by Jukebox but makes the representation hierarchical by enforcing the fine level to be conditioned by the coarse levels using a hierarchical autoencoder framework called hierarchically quantized VAE (HQ-VAE) (Takida et al., 2024). We then use a transformer-based multi-level auto-regressive model to characterize the probability mass of learnt HQ-VAE embeddings. We extract features from the intermediate representation of SoniDo by first converting input audio with the encoder into tokens, feeding them into the transformers, and extracting the intermediate output from the midst layer. We refer to these extracted features as SoniDo features. As shown in Table 1, we test our SoniDo‚Äôs feature injection for several music downstream tasks. To the best of our knowledge, this is the first study on enhancing both understanding and generative tasks with the intermediate representation from a single model. We briefly list the major findings of this study: 1. We empirically show that, with a generative model that is established on hierarchical representation, its intermediate representation can serve as generic booster of various music downstream tasks. 2. We verify that the extracted intermediate representation is beneficial for music understanding tasks even with only an extra shallow back-end network. The extension of the shallow network with attention layers leads to further improvement. 3. We show that the extracted intermediate representation is beneficial for enhancing task-specific models, through the applications to both understanding and generative tasks. 4. Several of the above improvements in each task category result in new SOTA scores. The summary of our results is shown in Table 1."
https://arxiv.org/html/2411.02236v1,3D Audio-Visual Segmentation,"Recognizing the sounding objects in scenes is a longstanding objective in embodied AI, with diverse applications in robotics and AR/VR/MR. To that end, Audio-Visual Segmentation (AVS), taking as condition an audio signal to identify the masks of the target sounding objects in an input image with synchronous camera and microphone sensors, has been recently advanced. However, this paradigm is still insufficient for real-world operation, as the mapping from 2D images to 3D scenes is missing. To address this fundamental limitation, we introduce a novel research problem, 3D Audio-Visual Segmentation, extending the existing AVS to the 3D output space. This problem poses more challenges due to variations in camera extrinsics, audio scattering, occlusions, and diverse acoustics across sounding object categories. To facilitate this research, we create the very first simulation based benchmark, 3DAVS-S34-O7, providing photorealistic 3D scene environments with grounded spatial audio under single-instance and multi-instance settings, across 34 scenes and 7 object categories. This is made possible by re-purposing the Habitat simulator [32] to generate comprehensive annotations of sounding object locations and corresponding 3D masks. Subsequently, we propose a new approach, EchoSegnet, characterized by integrating the ready-to-use knowledge from pretrained 2D audio-visual foundation models synergistically with 3D visual scene representation through spatial audio-aware mask alignment and refinement. Extensive experiments demonstrate that EchoSegnet can effectively segment sounding objects in 3D space on our new benchmark, representing a significant advancement in the field of embodied AI. Project page: https://surrey-uplab.github.io/research/3d-audio-visual-segmentation/","Human perception of the real world, both visual and acoustic, predominantly occurs in three dimensions. Prior psychology literature [37] has highlighted humans‚Äô remarkable ability to correspond across multiple modalities, often involving the association of events across these modalities. For instance, we can effortlessly ground emergent surround sound with its potential source in 3D visuals [26]. Inspired by this capability, a crucial aspect in the development of embodied AI systems is their ability to integrate cues from synchronous multimodal input streams and establish targets corresponding to their goals. In this work, we aim to build a machine model to achieve this multimodal correspondence, particularly targeted towards the task of audio-visual segmentation (AVS) in 3D. Albeit AVS has been widely explored within audio-visual scene analysis and correspondence learning, prominent research in this field has focused on 2D environments involving mono (single channel) sound sources, thus devoid of spatial presence entirely. In this paper, we take the first step towards exploring 3D AVS and introduce a large benchmark, 3DAVS-S34-O7. Our exploration is rooted in a fundamental grounding problem: given an embodied agent equipped with a camera and a binaural microphone, can we teach the agent to obtain fine-grained localization of potential sounding objects (generally by predicting a segment-level mask of the object in 3D) while also utilizing spatial audio cues? (see Fig. 1) Furthermore, we extend our benchmark to include a more competitive multi-instance setup where, although multiple instances of the same object might be present in the scene, the goal is to segment only the sounding instance. This setup helps us testify to the efficacy of spatial presence harnessed from the input binaural audio samples. Recently, 3D Gaussian Splatting (3D-GS) [15] has emerged as a prospective method for modeling static 3D scenes directly from input RGB frames. Owing to its explicit Gaussian based representation, it has paved a natural pathway for 3D visual segmentation [13, 41, 33]. Deriving inspiration from human spatial memory in indoor environments, we design EchoSegnet, a purely training-free pipeline for 3D AVS within a 3D-GS representation. EchoSegnet leverages 2D foundation models (namely SAM [17] and ImageBind [9]) to first obtain 2D AVS masks on the input RGB frames. These 2D AVS masks are further used to segment the Gaussians in the learned 3D-GS representation to obtain multi-view masks to achieve a consistent 3D segmentation. Figure 1: Comparison of the existing 2D AVS task with our proposed 3D AVS. Former task utilizes single channel audio to generate pixel-level masks of the potential sounding object in the input RGB frame. 3D AVS on the other hand is aimed at generating 3D masks (from which multi-view consistent 2D masks can be rendered) while utilizing multichannel (spatial) audio. To summarize, we make the following contributions: (1) the first 3D audio-visual segmentation benchmark composing of fairly complex indoor room scenes with integrated spatial sound cues; (2) a training-free AVS framework, EchoSegnet, capable of syncing across sequential frames from 3D environments; (3) a novel Audio-Informed Spatial Refinement Module AISRM, designed to enhance 3D segmentation and resolve ambiguities in complex, multi-instance environments by leveraging spatial audio intensity maps. We perform a comprehensive evaluation of EchoSegnet on the proposed 3DAVS-S34-O7 for both single-instance and multi-instance scenarios, along with an ablative comparison with existing 2D AVS models, highlighting their shortcomings in aligning audio-visual cues within 3D scenes -establishing their adaptation to 3DAVS-S34-O7 as non-trivial."
https://arxiv.org/html/2411.02165v1,"Joint Training of Speaker Embedding Extractor, Speech and Overlap Detection for Diarization‚Ä†‚Ä†thanks:Computing on IT4I supercomputer was supported by the Czech Ministry of Education, Youth and Sports through the e-INFRA CZ (ID:90254).","In spite of the popularity of end-to-end diarization systems nowadays, modular systems comprised of voice activity detection (VAD), speaker embedding extraction plus clustering, and overlapped speech detection (OSD) plus handling still attain competitive performance in many conditions. However, one of the main drawbacks of modular systems is the need to run (and train) different modules independently. In this work, we propose an approach to jointly train a model to produce speaker embeddings, VAD and OSD simultaneously and reach competitive performance at a fraction of the inference time of a standard approach. Furthermore, the joint inference leads to a simplified overall pipeline which brings us one step closer to a unified clustering-based method that can be trained end-to-end towards a diarization-specific objective.","Until a few years ago, competitive speaker diarization systems were mostly modular [1, 2, 3], i.e., consisting of different modules to handle voice/speech activity detection (VAD/SAD), embedding extraction over uniform segmentation, clustering, optional resegmentation, and overlapped speech detection (OSD) and handling. However, end-to-end models such as end-to-end neural diarization (EEND) [4, 5], and two-stage systems such as target-speaker voice activity detection (TS-VAD) [6] or end-to-end with vector clustering (E2E-VC) [7, 8, 9, 10] have recently gained more and more attention. The reasons for this are mainly their inherent ability to handle overlapped speech (where modular systems underperform) and fewer steps at inference time. Nevertheless, in contrast with modular systems, single-stage end-to-end systems do not handle well scenarios with many speakers [11] and they are very data-hungry, requiring high volumes of training data with diarization annotations. While two-stage systems produce per-frame speaker labels directly with a neural network (NN), they still build on clustering of embeddings: TS-VAD normally uses a clustering-based approach for initialization and recent competitive approaches based on E2E-VC [12] make use of the best speaker embedding extractors available together with clustering to reconcile short-segment decisions. Besides, modular systems can still attain competitive performance in certain scenarios [13], so speaker embedding extraction and clustering are still very relevant for diarization even today. Speaker embedding extraction and clustering have been the main components of modular systems for more than a decade. Since the development of x-vectors [14], the embeddings have been NN-based with new versions such as ResNet [15, 16], ECAPA-TDNN [17] or ECAPA2 [18] providing better and better results for speaker recognition and verification. These models are trained and utilized on at least a few seconds-long recordings for these tasks, but for diarization, embeddings are extracted on shorter segments since speakers can have short turns. However, the models are not designed for such usage, and tailoring them could lead to better performance [19, 20, 21, 22, 23]. In the context of clustering-based diarization, VAD (and optionally OSD) is needed. In this work, we modify the embedding extractor to produce per-frame embeddings for the whole recording at once, naturally avoiding multiple calls to the embedding extraction routine and speeding up the process, while producing VAD and OSD labels for each embedding. This is done by removing the pooling mechanism and introducing linear layers to produce VAD and OSD decisions from the embeddings. Moreover, we train the model for VAD+OSD and for embedding extraction on different data, thus taking advantage of different types of supervision that different corpora might offer, without generating synthetic training data, unlike end-to-end systems [4, 24, 25, 26]. In related works like [27], before x-vectors became popular, a single NN was used to extract per-frame speaker embeddings and produce VAD and OSD labels. However, the quality of the embeddings was restricted by the contrastive loss used to train the model and the limited speaker set contained in diarization-annotated datasets. In [28], per-frame embeddings were produced in a teacher-student framework where the teacher model produced per-segment embeddings. More recently, in [29, 30], the embedding extractor was used to provide VAD labels as a by-product by utilizing information encoded by intermediate representations in a weakly supervised VAD framework. The results obtained with our proposed approach show that it is possible to train a single model for the three tasks (VAD, embedding extraction, OSD) and produce high-quality embeddings at a higher frequency. This opens up the space for building speaker verification systems that can discard silence- and overlap-related frames before producing per-utterance embeddings. Moreover, the results are encouraging in our plan to combine this model with discriminative VBx (DVBx) [31], which will enable training of the whole modular pipeline in an end-to-end fashion towards a diarization-specific objective."
https://arxiv.org/html/2411.02038v1,Addressing Representation Collapse in Vector Quantized Models with One Linear Layer,"Vector Quantization (VQ) is a widely used method for converting continuous representations into discrete codes, which has become fundamental in unsupervised representation learning and latent generative models. However, VQ models are often hindered by the problem of representation collapse in the latent space, which leads to low codebook utilization and limits the scalability of the codebook for large-scale training. Existing methods designed to mitigate representation collapse typically reduce the dimensionality of latent space at the expense of model capacity, which do not fully resolve the core issue. In this study, we conduct a theoretical analysis of representation collapse in VQ models and identify its primary cause as the disjoint optimization of the codebook, where only a small subset of code vectors are updated through gradient descent. To address this issue, we propose SimVQ, a novel method which reparameterizes the code vectors through a linear transformation layer based on a learnable latent basis. This transformation optimizes the entire linear space spanned by the codebook, rather than merely updating the code vector selected by the nearest-neighbor search in vanilla VQ models. Although it is commonly understood that the multiplication of two linear matrices is equivalent to applying a single linear layer, our approach works surprisingly well in resolving the collapse issue in VQ models with just one linear layer. We validate the efficacy of SimVQ through extensive experiments across various modalities, including image and audio data with different model architectures. The results show that SimVQ not only effectively addresses the problem of representation collapse but also proves highly adaptable and easy to implement, suggesting its broad applicability in diverse machine learning contexts. Our code is available at https://github.com/youngsheen/SimVQ.","In recent years, vector quantization (VQ) (van den Oord et al., 2017; Razavi et al., 2019) has emerged as a foundational technique in unsupervised representation learning (Baevski et al., 2020; Bruce et al., 2024) and latent generative models (Rombach et al., 2022; Yu et al., 2022a; b; Borsos et al., 2023; Wang et al., 2023; Zhu et al., 2024b). By converting continuous representations into discrete codes, VQ models can effectively identify the inherent structure of data and enable various discrete modeling methods on continuous data, from high-quality image generation (Esser et al., 2021) to audio synthesis (D√©fossez et al., 2023). The recent success of Large Language Models (LLMs) (Achiam et al., 2023) has highlighted the effectiveness of next-token prediction as a powerful and versatile training objective. Consequently, VQ models are taken as the direct method to transform data from various modalities (Zhang et al., 2023a; Sun et al., 2024; Team, 2024) or scientific domains (Gao et al., 2024) to discrete sequences for next token prediction training. However, attempts to integrate VQ models as multimodal tokenizers to leverage the scaling laws of LLMs face significant challenges because of the difficulty of expanding the codebook. For example, the Chameleon model (Team, 2024) constrains its codebook size to 8‚Å¢k8ùëò8k8 italic_k, which is significantly trailing behind the vocabulary size of LLMs (e.g., LLaMA3‚Äôs vocabulary size is 128‚Å¢k128ùëò128k128 italic_k (Dubey et al., 2024)). Figure 1: Comparison of Vanilla VQ and SimVQ. (a): (left) Disjoint optimization in Vanilla VQ. Only the nearest codes are updated, resulting in a high percentage of ‚Äúdead‚Äù codes that are not updated. (b): (right) Joint optimization in SimVQ. The entire codebook is updated with a latent basis, ensuring all codes remain active. There is a broad agreement that increasing vocabulary size can consistently improve the performance of LLMs (Tao et al., 2024). However, recent studies (Zhu et al., 2024a) indicate that traditional VQ models often fail to utilize the additional parameters introduced by codebook expansion, leaving most codes inactive during training. The contradiction between codebook expansion and low codebook utilization in VQ models is known as the representation collapse problem (Roy et al., 2018), where increasing the codebook size fails to improve the performance. To address these discrepancies, we conduct a theoretical analysis of the optimization procedure of VQ models and identify that the disjoint optimization of the codebook is the root cause of representation collapse. As illustrated in Fig. 1(a), the core mechanism of VQ models involves a nearest-neighbor replacement strategy, where the encoder‚Äôs output features are replaced by the nearest vector in the codebook to serve as input to the decoder. The indices of the nearest vector are taken as the discrete representation of the data. This nearest-selection operator results in only a subset of codes being updated through gradient descent, while the remaining codes remain unchanged. Recently, some approaches have been proposed to mitigate representation collapse. FSQ (Mentzer et al., 2024), LFQ (Yu et al., 2024) and ViTVQGAN (Yu et al., 2022a) reduce the dimension of the latent space to a very small scale (e.g., 8 vs. 128) to alleviate the curse of dimensionality, thereby improving the overlap between the encoder‚Äôs features and the codebook. However, while these methods enhance codebook utilization, they do so at the cost of model capacity, leading to worse performance compared to vanilla VQ models when the codebook size is small and representation collapse is not severe. Another approach, VQGAN-LC (Zhu et al., 2024a), initializes the codebook with features extracted from the pre-trained CLIP model (Radford et al., 2021) to create a well-structured latent space that better matches the distribution of the encoder output. Nevertheless, the latent space defined by an external pre-trained model limits the model‚Äôs ability to generalize to diverse datasets and reaches a performance plateau as the codebook size increases. These limitations highlight the need for a more effective method to improve codebook utilization without compromising model capacity or relying on external models. We critically assess prevalent methodologies and reveal that optimizing the latent space rather than individual code vectors is key to preventing representation collapse. Building on this insight, we introduce a simple yet effective method, termed SimVQ, to directly update the latent space spanned by the codebook by linear transforming the code vectors via a learnable latent basis. Specifically, the vectors in the codebook are reparameterized as a linear combination of the basis in the learnable linear layer ùëæùëæ\bm{W}bold_italic_W: ùë™‚àà‚ÑùK√ód‚áíùë™‚Å¢ùëæ‚Å¢with‚Å¢ùëæ‚àà‚Ñùd√ód,ùë™superscript‚Ñùùêæùëë‚áíùë™ùëæwithùëæsuperscript‚Ñùùëëùëë\bm{C}\in\mathbb{R}^{K\times d}\Rightarrow\bm{CW}~{}\text{with}~{}\bm{W}\in% \mathbb{R}^{d\times d},bold_italic_C ‚àà blackboard_R start_POSTSUPERSCRIPT italic_K √ó italic_d end_POSTSUPERSCRIPT ‚áí bold_italic_C bold_italic_W with bold_italic_W ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT , (1) where KùêæKitalic_K denotes the codebook size and dùëëditalic_d represents the dimension of latent space. This reparameterization with linear transformation disentangles the optimization of the codebook into two components: the coefficient matrix ùë™ùë™\bm{C}bold_italic_C and the basis of linear space ùëæùëæ\bm{W}bold_italic_W respectively. As illustrated in Fig. 1(b), by optimizing the basis matrix ùëæùëæ\bm{W}bold_italic_W, the latent space spanned by ùë™‚Å¢ùëæùë™ùëæ\bm{CW}bold_italic_C bold_italic_W is rotated and stretched to match encoder‚Äôs output feature. The entire codebook is updated jointly to prevent the representation collapse problem. The simplicity of the proposed method makes it highly portable and easily adaptable for improving VQ-based models across a wide range of domains, requiring only one linear layer. In summary, our contributions to vector quantized models are as follows: ‚Ä¢ We theoretically analyze the representation collapse problem of VQ models and reveal that optimizing the latent space spanned by the codebook, rather than focusing on the individual code vectors, is crucial to addressing this issue. ‚Ä¢ We propose a novel method, SimVQ, which reparameterizes the codebook vectors in VQ models via a linear transformation with a learnable latent basis. This simple yet effective approach is highly adaptable and easy to implement, making it broadly applicable across various machine learning contexts. ‚Ä¢ We conduct an extensive evaluation of SimVQ across diverse modalities, including image and audio with different model architectures. The results show that SimVQ not only effectively addresses the representation collapse problem by achieving near-complete codebook utilization regardless of the codebook size, but also establishes new state-of-the-art performance. Furthermore, when scaling up the codebook size, SimVQ consistently delivers improved results."
https://arxiv.org/html/2411.02019v1,Modulating State Space Model withSlowFastFramework for Compute-Efficient Ultra Low-Latency Speech Enhancement‚Ä†‚Ä†thanks:This work was supported by a research contract from Meta Reality Labs Research.,"Deep learning-based speech enhancement (SE) methods often face significant computational challenges when needing to meet low-latency requirements because of the increased number of frames to be processed. This paper introduces the SlowFast framework which aims to reduce computation costs specifically when low-latency enhancement is needed. The framework consists of a slow branch that analyzes the acoustic environment at a low frame rate, and a fast branch that performs SE in the time domain at the needed higher frame rate to match the required latency. Specifically, the fast branch employs a state space model where its state transition process is dynamically modulated by the slow branch. Experiments on a SE task with a 2 ms algorithmic latency requirement using the Voice Bank + Demand dataset show that our approach reduces computation cost by 70% compared to a baseline single-branch network with equivalent parameters, without compromising enhancement performance. Furthermore, by leveraging the SlowFast framework, we implemented a network that achieves an algorithmic latency of just 60 Œºùúá\muitalic_Œºs (one sample point at 16 kHz sample rate) with a computation cost of 100 M MACs/s, while scoring a PESQ-NB of 3.12 and SISNR of 16.62.","Low-latency speech signal processing is crucial for devices and applications such as hearing aids, communication devices, and immersive virtual reality, where minimizing delays in the output of processed speech signal is essential to ensure optimal user experience [1, 2]. Recent deep learning-based Speech Enhancement (SE) algorithms have focused extensively on solutions to meet these low-latency requirements. For instance, causal networks have been utilized to reduce the number of lookahead samples needed [3, 4, 5, 6, 7, 8]. Additionally, various methods have been developed to reduce the algorithmic latency, achieving as low as 2 ms. Several studies [9, 10, 11, 12] employ symmetric analysis-synthesis windows, where both the analytical window used to frame the input signal and the synthesis window used in the Overlap-and-Add (OLA) process have the same length to match the algorithmic latency requirements. In contrast, other studies [13, 14, 15] propose the use of asymmetric windows [16, 17], where a larger window size is used for analysis to capture more extensive acoustic information, while a smaller window is used during synthesis to reduce algorithmic latency. As latency requirements tighten, more frames must be processed per unit of time due to the shorter hop size needed for framing inputs. For example, reducing algorithmic latency from 20 ms to 2 ms requires processing 10√ó\times√ó more frames, which causes a tenfold increase in computation cost for existing methods. This poses significant challenges for deploying current low algorithmic latency networks on edge devices. Meanwhile, reducing the hop size also leads to a decreased differences between consecutive frames, thus increasing information redundancy between adjacent inputs. However, despite this redundancy, current methods still run the same network for every input frame, resulting in a substantial amount of redundant computations. Figure 1: Illustration of proposed SlowFast framework for compute-efficient low-latency speech enhancement. (A) SlowFast Processing when Œ¥=3ùõø3\delta=3italic_Œ¥ = 3. : The slow branch (orange, bottom) operates at a lower frame rate, the fast branch (blue, top) operates at a higher rate. (B) Framing and OLA Process: The slow branch processes longer segments with a larger hop size, while the fast branch processes shorter segments with a smaller hop size. The enhanced speech is obtained by doing the \pdftooltipOLAOverlap-and-Add on the fast branch outputs. (C) SSM Modulation: The slow branch modulates the state transition process in the fast branch. In this paper, we propose the SlowFast framework designed to reduce the computation cost of \pdftooltipSESpeech Enhancement networks under low-latency requirement while maintaining enhancement performance. To the best of our knowledge, this is the first framework enabling single sample-level algorithmic latency speech enhancement at an extremely low computation cost. The key contributions of this work are as follows: ‚Ä¢ We propose a dual-branch \pdftooltipSESpeech Enhancement framework in which the slow branch, being relatively computation-heavy, operates at a lower frame rate, while only the fast branch runs at a higher rate, thereby reducing overall computational costs. ‚Ä¢ We introduce a lightweight State Space Model (SSM)-based fast branch. In the \pdftooltipSSMState Space Model, the state transitions are dynamically modulated according to the speech and noise characteristics modeled by the slow branch. ‚Ä¢ Evaluation results on a \pdftooltipSESpeech Enhancement task demonstrate that networks designed with the SlowFast framework can achieve single sample-level algorithmic latency with a computation cost of approximately 100 M Multiply-Accumulates (MAC) per second. Furthermore, for a 2 ms algorithmic latency requirement scenario, evaluations on the Voice Bank + DEMAND dataset show a 70% reduction in network computation cost, while preserving the quality and intelligibility of the enhanced speech."
https://arxiv.org/html/2411.01710v1,SPES: Spectrogram Perturbationfor Explainable Speech-to-Text Generation,"Spurred by the demand for interpretable models, research on eXplainable AI for language technologies has experienced significant growth, with feature attribution methods emerging as a cornerstone of this progress. While prior work in NLP explored such methods for classification tasks and textual applications, explainability intersecting generation and speech is lagging, with existing techniques failing to account for the autoregressive nature of state-of-the-art models and to provide fine-grained, phonetically meaningful explanations. We address this gap by introducing Spectrogram Perturbation for Explainable Speech-to-text Generation (SPES), a feature attribution technique applicable to sequence generation tasks with autoregressive models. SPES provides explanations for each predicted token based on both the input spectrogram and the previously generated tokens. Extensive evaluation on speech recognition and translation demonstrates that SPES generates explanations that are faithful and plausible to humans.","The recent advances of Artificial Intelligence (AI) and the emergence of foundation models (Bommasani et al., 2021) have come together with concerns about their risks and impact (Weidinger et al., 2021), as well as calls for a better understanding of their inner workings (Barredo Arrieta et al., 2020; Eiras et al., 2024). This need has been reinforced by the demands for transparency by legal frameworks like the EU AI Act and the US National AI Initiative Act (Panigutti et al., 2023; Roberts et al., 2024). In response, the field of eXplainable AI (XAI) has emerged prominently, with the goal of elucidating the reasoning behind system predictions (Doshi-Velez and Kim, 2017; Carvalho et al., 2019; Vilone and Longo, 2021; Pradhan et al., 2022). Among the various active XAI approaches (Ferrando et al., 2024), this paper centers on feature attribution methods, which aim to identify and quantify the importance of each input feature in determining a model‚Äôs final output. Originally developed for image and text classification (Danilevsky et al., 2020; Kamakshi and Krishnan, 2023), feature attribution methods have been extended to other modalities‚Äîlike speech (Becker et al., 2024; Pastor et al., 2024)‚Äîand to text generation (Sarti et al., 2023; Zhao et al., 2024, et al.). Still, XAI in the domain of speech-to-text (S2T) generative tasks is limited, with only a few preliminary works focusing on automatic speech recognition (ASR) (Mandel, 2016; Trinh et al., 2018; Trinh and Mandel, 2021; Kavaki and Mandel, 2020; Markert et al., 2021; Wu et al., 2023, 2024). This contrasts with the pivotal role of spoken language‚Äîarguably the most natural form of human interaction (Munteanu et al., 2013)‚Äîand consequently with the importance of S2T technologies, which are now fostered by foundation models that transcribe and translate at an unprecedented scale (Latif et al., 2023). Moreveor, most existing feature attribution methods in S2T Mandel (2016); Trinh and Mandel (2021); Markert et al. (2021); Wu et al. (2023, 2024) are not applied to autoregressive models, which predict each token iteratively, relying on both speech input and previous output tokens. Even the only method applied to autoregressive S2T models Kavaki and Mandel (2020) assumes conditional independence between prediction steps, disregarding the influence of previously generated text on each new prediction. Finally, these methods are either incompatible with the spectrogram input format (Markert et al., 2021; Wu et al., 2023, 2024), which is typically used in modern S2T models, or produce explanations that fail to highlight fine-grained patterns related to acoustic characteristics of speech, such as the fundamental frequency and formants (Mandel, 2016; Trinh et al., 2018; Kavaki and Mandel, 2020; Trinh and Mandel, 2021). To overcome the above limitations, we propose Spectrogram Perturbation for Explainable Speech-to-text Generation (SPES), the first feature attribution technique that provides token-level explanations for autoregressive S2T models by considering both the speech input and previously generated tokens.111Code released under Apache 2.0 upon paper publication. SPES employs a perturbation-based approach that adapts image segmentation to spectrograms, enabling the identification of fine-grained, meaningful patterns. Through quantitative assessments and in-depth analyses across two S2T tasks‚ÄîASR and, for the first time, speech translation (ST)‚Äîwe show that SPES produces accurate, phonetically interpretable explanations aligned with human understanding."
https://arxiv.org/html/2411.01174v1,Leveraging LLM and Text-Queried Separation for Noise-Robust Sound Event Detection,"Sound Event Detection (SED) is challenging in noisy environments where overlapping sounds obscure target events. Language-queried audio source separation (LASS) aims to isolate the target sound events from a noisy clip. However, this approach can fail when the exact target sound is unknown, particularly in noisy test sets, leading to reduced performance. To address this issue, we leverage the capabilities of large language models (LLMs) to analyze and summarize acoustic data. By using LLMs to identify and select specific noise types, we implement a noise augmentation method for noise-robust fine-tuning. The fine-tuned model is applied to predict clip-wise event predictions as text queries for the LASS model. Our studies demonstrate that the proposed method improves SED performance in noisy environments. This work represents an early application of LLMs in noise-robust SED and suggests a promising direction for handling overlapping events in SED. Codes and pretrained models are available at https://github.com/apple-yinhan/Noise-robust-SED.","Sound event detection (SED) [1] involves identifying sound event classes and corresponding timestamps, which has been widely applied in various applications such as smart city [2, 3] and medical monitoring [4]. Recent deep learning advancements have significantly enhanced SED, supported by diverse datasets designed for specific scenarios. One such widely used dataset is domestic environment sound event detection (DESED) [5], which is also considered for Task 4 of the detection and classification of acoustic scenes and events (DCASE)111https://dcase.community/challenge2024/ challenge series. Convolutional recurrent neural network (CRNN)-based models [6, 7, 8] have performed well on the DESED dataset in recent years. However, these studies typically consider testing data that closely matches the training data conditions, limiting the models‚Äô ability to generalize to real-world conditions [9]. In noisy environments, audio may include sound events that were not be present during training, which can act as background noise, overlap with target sounds, and make detection more difficult, leading to reduced performance. As a result, developing noise-robust SED system remains a challenging problem. The pursuit of developing noise-robust SED has led to different methods for improving the performance in noisy conditions. One common approach is to fine-tune models using noisy data, which involves selecting noise types that represent real-world environments while being distinct from target sounds [10, 11, 12, 13, 14]. However, artificial selection can be difficult due to bias and unnatural correlations. Recently, large language models (LLMs) like GPT-4 [15], GPT-3 [16], and Llama [17] have shown great capabilities in analyzing the semantic relationships between different sound types. An LLM-powered dataset referred to as WildDESED was proposed in [18], where SED models are trained with a noisy training set that included the same noise events as in the test sets. However, training of SED models in this way, may not be very ideal for real-world applications, as predicting the types of noise events during testing is often not feasible. To avoid the issue of predicting noise types during testing, we propose to use audio source separation to extract target sounds from noisy audio during testing. This method does not rely on knowing the specific noise events beforehand. Instead, it isolates the target sound directly from the audio mixture, making it adaptable to various noisy conditions. Language-queried audio source separation (LASS) models allow them to isolate audio signals that match the text query by pre-training on large audio-text paired datasets [19]. However, the text query is crucial, as it directly impacts the separation result. These models can only isolate sound events present in the audio. For example, if ‚Äúdog barking‚Äù is requested in a clip containing only speech, the output may be unreliable. In noisy environments, the actual sound events are unknown, and therefore it is difficult to generate accurate text queries for separation. To address this, we propose using LLMs to develop a noise-robust model that can generate reliable event predictions for text queries. In this study, to address the challenge of noisy sound events in SED, we propose an LLM-based framework that integrates noise augmentation with the text-queried audio separation method for noise-robust SED. Our proposed approach first leverages LLMs to select appropriate noise types and fine-tune the SED model. The fine-tuned model then generates clip-wise event predictions, which serve as text queries for the LASS model during testing. Experimental studies in this work are conducted on DESED and WildDESED datasets to demonstrate the effectiveness of the proposed method to develop noise-robust SED system in challenging acoustic conditions. We introduce four different baseline methods for generating text queries to validate the effectiveness of our LLM-based approach. To the best of our knowledge it is the first study to apply LASS models for noise-robust SED. We have released the code to support future research. Figure 1: The overview of LLM-based noise augmentation and text-queried separation for noise-robust sound event detection."
https://arxiv.org/html/2411.00980v1,Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO,"Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) frequently face challenges with articulation, leading to dysarthria and resulting in atypical speech patterns. In healthcare settings, coomunication breakdowns reduce the quality of care. While building an augmentative and alternative communication (AAC) tool to enable fluid communication we found that state-of-the-art (SOTA) automatic speech recognition (ASR) technology like Whisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack of training data. Our work looks to leverage SOTA ASR followed by domain specific error-correction. English dysarthric ASR performance is often evaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this dataset where phrases overlap between training and test speakers. Our work proposes an algorithm to break this prompt-overlap. After reducing prompt-overlap, results with SOTA ASR models produce extremely high word error rates for speakers with mild and severe dysarthria. Furthermore, to improve ASR, our work looks at the impact of n-gram language models and large-language model (LLM) based multi-modal generative error-correction algorithms like Whispering-LLaMA for a second pass ASR. Our work highlights how much more needs to be done to improve ASR for atypical speakers to enable equitable healthcare access both in-person and in e-health settings.","Healthcare professionals rely on augmentative and alternative communication (AAC) software to support telehealth and in-person appointments for patients with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) [1]. Damage to the nervous system can result in paralysis or weakness of the muscles responsible for speech, leading to dysarthria and atypical speech patterns in individuals with ALS or cerebral palsy. Atypical speakers who are verbal, often prefer to use their own voice to communicate their needs. Modern AAC applications like VoiceItt111www.voiceitt.com or our own AAC application SpeakEase [2] allow for audio input from the speaker with the intention to provide a faithful transcription. Mulfari et al. [3] propose a low-power, on-device, deep-learning based isolated word ASR system to work in an ‚Äúalways-on‚Äù mode for dysarthric speakers with reduced mobility. Such a system has promise to enable communication in healthcare and home settings. Figure 1: Dysarthric automatic speech recognition followed by error correction There is little or no data available on the open domain for atypical speakers. On the other hand web-scale speech datasets like Mozilla Common Voice [4] and GigaSpeech [5] allow for state-of-the-art speech recognition for typical speakers. Dysarthric speech recognition is a low-resource out-of-domain problem [6]. To leverage well-developed typical speaker ASR systems, our work looks for a first pass transcription from such a system followed by error-correction (EC) as show in Figure 1. The figure shows disordered input speech with an imperfect noisy transcription after ASR followed by error-correction. ASR systems are trained with audio and correct transcription pairs. EC systems are trained with inputs consisting of multiple hypotheses of transcribed text (referred to as n-best lists), possibly with errors, with outputs mapping to the correct target text. 1 Ref: he slowly takes a short walk in the open air each day ASR: he shlly takes a wall in the week a eh day EC: he slowly takes a short walk in the open air each day 2 Ref: usually minus several buttons ASR: usually min sell fold buttons EC: usually sell fold buttons 3 Ref: you wished to know all about my grandfather ASR: u‚Äô wal awarke youar gread fap EC: you wished to know all about my grandfather Figure 2: Inference samples for error-correction (EC) for speaker M05. Ref shows the reference transcription, ASR shows the transcription output which serves as input to the EC model. Notice how the EC system has memorized transcripts due to prompt overlap in TORGO. To evaluate English ASR for dysarthric speakers, a well-known dataset called the TORGO dataset [7] is widely used. The TORGO dataset for dysarthric speech has data from speakers with either ALS or CP. Other dysarthric ASR databases such as the Nemours corpus222The authors were unable to obtain a recent copy of this database due to a lack of information on the internet [8], UASpeech [9] and the HomeService corpus [10] are either hard to obtain or largely consist of isolated word utterances. The TORGO dataset is one of the few containing both isloated word and a few sentence level utterances. Figure 2 shows inference examples from our initial experiments of error-correction(EC) following ASR. The EC model memorizes the target transcription without doing any error-correction. This issue stems from the dataset design, which features a significant amount of prompt overlap among the speakers. The research community acknowledges that the TORGO dataset has a very high degree of prompt overlap between speakers [11, 12]. This data leakage prevents the dataset from being used to evaluate ASR and EC algorithms for real-world applications like telehealth and e-health. Our work in this paper makes the following contributions: ‚Ä¢ Develop an algorithm based on mixed-integer linear programming to partition the TORGO dataset with no prompt overlap with the constraint to minimize data loss. This dataset is called no-prompt overlap TORGO or NP-TORGO. ‚Ä¢ Understand the impact of removing prompt overlap on dysarthric ASR performance using SOTA baseline ASR models. ‚Ä¢ Understand the impact of out-of-domain language modelling using text data from the training utterances from NP-TORGO, and Librispeech [13] . ‚Ä¢ Understand the impact of error-correction (EC) without ASR system fine-tuning, with a state-of-the-art cross-modal error-correction system such as Whispering-LLAMA [14]. This paper is organized as follows. Section II puts our current work in the context of prior work. Section III introduces the TORGO dataset, and Section IV introduces our approach to remove prompt overlap. Section V presents our experimental setup, and experimental results are presented in Section VI. Section VII provides a discussion of our work, and Section VIII concludes the paper."
https://arxiv.org/html/2411.00774v1,Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM,"The rapid development of large language models has brought many new smart applications, especially the excellent multimodal human-computer interaction in GPT-4o has brought impressive experience to users. In this background, researchers have proposed many multimodal LLMs that can achieve speech-to-speech dialogue recently. In this paper, we propose a speech-text multimodal LLM architecture called Freeze-Omni. Our main contribution is the speech input and output modalities can connected to the LLM while keeping the LLM frozen throughout the training process. We designed 3-stage training strategies both for the modeling of speech input and output, enabling Freeze-Omni to obtain speech-to-speech dialogue ability using text-speech paired data (such as ASR and TTS data) and only 60,000 multi-round text Q&A data on 8 GPUs. Moreover, we can effectively ensure that the intelligence of the Freeze-Omni in the speech modality is at the same level compared with that in the text modality of its backbone LLM, while the end-to-end latency of the spoken response achieves a low level. In addition, we also designed a method to achieve duplex dialogue ability through multi-task training, making Freeze-Omni have a more natural style of dialogue ability between the users. Freeze-Omni mainly provides a possibility for researchers to conduct multimodal LLM under the condition of a frozen LLM, avoiding various impacts caused by the catastrophic forgetting of LLM caused by fewer data and training resources.","In recent years, the development of large language models has been extremely rapid. A series of large language models represented by the GPT series [10, 1] of OpenAI has demonstrated extraordinary capabilities. As speech interaction is one of the most natural forms of human-computer interaction, combining speech input and output with an LLM can bring an extraordinary experience to users. The traditional method is to use a cascaded approach of ASR + LLM + TTS to achieve the interaction with LLM in speech modality. However, this approach often leads to a relatively high engineering complexity and a considerable interaction latency. Nevertheless, GPT-4o [18] has changed this situation, it provides an end-to-end speech interaction mode which has significantly improved the user experience, triggering a research boom among researchers regarding multimodal LLMs for speech-to-speech interaction. In the field of general LLMs, many public models such as Llama 3.2 [8], Qwen2.5 [21], Mixtral [14], etc. have provided very good opportunities for researchers to develop downstream tasks on them. Therefore, in the research field of multimodal LLMs for speech-to-speech, works such as Mini-Omni2 [24], LLaMA-Omni [9], and Moshi [7] have provided excellent references for researchers. These works adopt different strategies to align the speech modality with the LLM and design some methods to achieve a duplex dialogue mode, demonstrating excellent performance. In this research context, we found that in the process of aligning the LLM with the speech modality in existing public speech-text multimodal LLMs [6, 7, 9, 11, 27, 23], the parameters of the LLM are more or less fine-tuned. However, in most cases, it is very difficult for researchers to easily collect spoken Q&A data at the million-hour level (the corresponding text content can be comparable to the amount of data for training text-modal LLMs). This inevitably brings about the forgetting problem to the LLM, resulting in a negative impact on its intelligence. In addition, only a few works have evaluated the accuracy of spoken question-answering tasks for speech-to-speech multimodal LLMs, and show an obvious gap in performance between spoken question-answering and text-modality question-answering. Therefore, in this paper, we propose a speech-to-speech dialogue LLM called Freeze-Omni, achieving speech modality alignment while the LLM is frozen throughout the training process, and obtaining low latency speech dialogue capabilities while keeping the intelligence of the backbone LLM. Freeze-Omni is mainly implemented in the following steps: Modeling of speech input We first use a large amount of ASR data to align the speech encoder and the LLM, enabling the LLM to understand the semantic information from the speech. Then, with the LLM frozen, a training strategy of prompt embedding is used to let the model have the ability to possess speech input to text output, training on only a small amount of Q&A data. Modeling of speech output Second, we use a mount of text-speech paired data to train the AR-based speech decoder which can generate speech tokens from text and a single-codebook based codec model is used to decode the speech token into waveform. Then, we design a prefix kv-cache fine-tune strategy, using the hidden state vector output by the LLM to transfer the speech decoder into the output text space of LLM, achieving the ability of text input to speech output while keeping the LLM frozen. Design for duplex dialogue Finally, we simultaneously connect the speech encoder and speech decoder from the above parts to the backbone LLM. Then, a task of chunk-wise state prediction is used to enable the LLM to interrupt or reject the user‚Äôs input, achieving the duplex speech-to-speech dialogue ability. In conclusion, the main contributions of the proposed Freeze-Omni are as follows: ‚Ä¢ The parameters of the LLM are completely frozen throughout the training process, ensuring that the intelligence of the LLM will be kept. At the same time, the ability of low latency speech-to-speech dialogue is still obtained. ‚Ä¢ The data scale relied on during the training process is small and consumes fewer computing resources. It requires text-speech paired data (such as ASR and TTS training data) and only a small amount of Q&A data in text modality. ‚Ä¢ Freeze-Omni can support any (multimodal) LLM that has a text modality and retains the abilities of the LLM such as prompt following and role-playing. Moreover, if it is necessary to change the style of the LLM‚Äôs response, it is only necessary to fine-tune the LLM with text data in the corresponding style."
https://arxiv.org/html/2411.00469v1,MIRFLEX: Music Information Retrieval Feature Library for Extraction,"This paper introduces an extendable modular system that compiles a range of music feature extraction models to aid music information retrieval research. The features include musical elements like key, downbeats, and genre, as well as audio characteristics like instrument recognition, vocals/instrumental classification, and vocals gender detection. The integrated models are state-of-the-art or latest open-source. The features can be extracted as latent or post-processed labels, enabling integration into music applications such as generative music, recommendation, and playlist generation. The modular design allows easy integration of newly developed systems, making it a good benchmarking and comparison tool. This versatile toolkit supports the research community in developing innovative solutions by providing concrete musical features.","Music Information Retrieval (MIR) is a complex field focused on computational analysis and processing of musical data, with tasks like similarity estimation, genre classification, and recommendation. While recent advances in machine learning have led to powerful feature extraction methods, the fragmented nature of these tools poses challenges for researchers who must integrate multiple disparate systems. To address this, we present MIRFLEX, a unified feature extraction library designed for MIR research. MIRFLEX offers a diverse set of extractors covering key musical aspects such as key, beats, and genre, using both signal processing and machine learning techniques to generate comprehensive audio representations. The primary objectives of this work are threefold: 1. To offer a centralized and easily accessible collection of feature extraction tools, reducing the burden on researchers to implement and integrate disparate feature extraction techniques. 2. To provide a comprehensive feature set that captures the multifaceted nature of musical data, enabling researchers to explore a wide range of music-related applications and queries. 3. To contribute to the advancement of music information retrieval research by facilitating the rapid prototyping and development of new applications that leverage easily accessible and readily available musical features. The proposed feature extraction library is available at 111https://github.com/AMAAI-Lab/megamusicaps"
https://arxiv.org/html/2411.00464v1,AUTHOR GUIDELINES FOR BLIND SLT 2024 MANUSCRIPTS,"The abstract should appear at the top of the left-hand column of text, about 0.5 inch (12 mm) below the title area and no more than 3.125 inches (80 mm) in length. Leave a 0.5 inch (12 mm) space between the end of the abstract and the beginning of the main text. The abstract should contain about 100 to 150 words, and should be identical to the abstract text submitted electronically along with the paper cover sheet. All manuscripts must be in English, printed in black ink.","These guidelines include complete descriptions of the fonts, spacing, and related information for producing your proceedings manuscripts. Please follow them and if you have any questions, please use the Contact Chairs page at the paper submission portal CMT to contact the committee."
https://arxiv.org/html/2411.00321v2,MACE: Leveraging Audio for Evaluating Audio Captioning Systems,"The Automated Audio Captioning (AAC) task aims to describe an audio signal using natural language. To evaluate machine-generated captions, the metrics should take into account audio events, acoustic scenes, paralinguistics, signal characteristics, and other audio information. Traditional AAC evaluation relies on natural language generation metrics like ROUGE and BLEU, image captioning metrics such as SPICE and CIDEr, or Sentence-BERT embedding similarity. However, these metrics only compare generated captions to human references, overlooking the audio signal itself. In this work, we propose MACE (Multimodal Audio-Caption Evaluation), a novel metric that integrates both audio and reference captions for comprehensive audio caption evaluation. MACE incorporates audio information from audio as well as predicted and reference captions and weights it with a fluency penalty. Our experiments demonstrate MACE‚Äôs superior performance in predicting human quality judgments compared to traditional metrics. Specifically, MACE achieves a 3.28% and 4.36% relative accuracy improvement over the FENSE metric on the AudioCaps-Eval and Clotho-Eval datasets respectively. Moreover, it significantly outperforms all the previous metrics on the audio captioning evaluation task. The metric is opensourced at https://github.com/satvik-dixit/mace.","Automated Audio Captioning (AAC) task [1] is centered on producing natural language descriptions for audio content. This process involves identifying audio events [2], acoustic scenes [3], temporal relationships [4], etc., within the audio stream. Once trained, an AAC system has numerous applications [1], such as assisting individuals with hearing impairments, enhancing security and surveillance systems, supporting multimedia retrieval, and more. Building a robust AAC system requires evaluating outputs on three main dimensions: accuracy (covering all audio events, scenes, actions), linguistic quality (grammar, coherence), and readability (clarity and logical flow). Traditional AAC metrics, such as BLEU [5], ROUGE [6], and METEOR [7], emphasize linguistic variation [8] through n-gram overlap between candidate and reference sentences. Metrics like SPICE [9] incorporate relational information by parsing captions into a graph containing semantic elements, their attributes, and relations to one another, and evaluates the candidate graph via synonym lemma matching. Recently, FENSE [10], developed specifically for audio captioning, leverages sentence-BERT [11] embeddings to capture semantic similarity between generated and reference captions. Subsequent methods [12, 13] have built upon this approach, predominantly employing sentence-BERT embeddings. However, a key limitation of existing metrics is their exclusion of audio information and needing reference captions to perform evaluation. We hypothesize that incorporating audio information into AAC metrics will enhance semantic accuracy and better align with human judgment. Audio information can be integrated either through direct audio-caption comparison or by grounding text embeddings in audio. For example, a generated caption like ‚ÄúThe crowd is applauding in a stadium‚Äù versus the reference ‚ÄúThe crowd is silent in a stadium‚Äù would be scored similarly by current metrics like FENSE, despite opposite meanings. Grounding embeddings in audio, however, could yield lower similarity scores for such differences. CLAP [14, 15, 16, 17] offers one approach, aligning audio and text in a multimodal space to capture shared audio events and scenes. Yet, solely using CLAP may miss essential linguistic and readability factors [18], underscoring the need for metrics that integrate both linguistic quality and audio context for comprehensive evaluation. In this paper, we propose MACE, a novel and comprehensive metric to evaluate audio captioning system. MACE addresses a fundamental limitation in existing evaluation approaches by incorporating both audio and linguistic information. The metric comprises three components: first, it leverages CLAP audio and text embeddings to assess the relevance of the generated caption with respect to the audio content; second, it employs text embeddings to measure acoustic similarity between the generated and reference captions; third, it applies a fluency error penalty to weight the similarity scores, ensuring grammatical accuracy in the generated captions. We evaluate MACE on two commonly used benchmarks for evaluating AAC metrics - AudioCaps-Eval [10] and Clotho-Eval [10] benchmarks. MACE produces SoTA results and outperforms all prior metrics. Specially, MACE achieves a 3.28% and 4.36% relative accuracy improvement over the FENSE metric on the AudioCaps-Eval and Clotho-Eval datasets respectively. Moreover, as MACE contains three components, one can evaluate caption quality without requiring reference caption by using two of the three components."
https://arxiv.org/html/2411.00195v1,"Machine Learning Framework for Audio-Based Content Evaluation using MFCC, Chroma, Spectral Contrast, and Temporal Feature Engineering","This study presents a machine learning framework for assessing similarity between audio content and predicting sentiment score. We construct a dataset containing audio samples from music covers on YouTube along with the audio of the original song, and sentiment scores derived from user comments, serving as proxy labels for content quality. Our approach involves extensive pre-processing, segmenting audio signals into 30-second windows, and extracting high-dimensional feature representations through Mel-Frequency Cepstral Coefficients (MFCC), Chroma, Spectral Contrast, and Temporal characteristics. Leveraging these features, we train regression models to predict sentiment scores on a 0-100 scale, achieving root mean square error (RMSE) values of 3.420, 5.482, 2.783, and 4.212, respectively. Improvements over a baseline model based on absolute difference metrics are observed. These results demonstrate the potential of machine learning to capture sentiment and similarity in audio, offering an adaptable framework for AI applications in media analysis.","In today‚Äôs digital landscape, social media platforms like TikTok, Instagram, and YouTube have become essential for artists and musicians to share their performances and gain audience feedback. When creators upload content, they are unable to revise it post-publication without losing valuable metrics such as likes and comments. This limitation is especially challenging for musicians who upload song covers, as it restricts their ability to make quality improvements based on initial audience reactions. Traditional quality checks, such as previewing content for test audiences, are costly and time-intensive, making them unfeasible for many creators. This study proposes a machine learning model to perform this quality check in a way that is automated, repeatable, and does not require any external human input (like a test audience). The model works by providing creators with a predictive sentiment score. We train our model using audio samples from covers and original songs along with sentiment scores aggregated from social media comments. By analyzing these inputs, the model generates a predicted sentiment score on a 0-100 scale, representing anticipated audience reception if the cover were published. The innovation in this study lies in the tailored model architecture and feature engineering, which goes beyond simple audio comparisons. Our baseline approach‚Äîcalculating absolute differences between cover and original‚Äîfails to capture the nuanced changes in tone, style, and instrumentation that often resonate positively with audiences. Instead, we use advanced feature representations like Mel-frequency Cepstral coefficients (MFCC) for timbral nuances, along with Chroma, Spectral Contrast, and Temporal features, to train regression models that more accurately predict audience sentiment."
https://arxiv.org/html/2411.00153v1,Angular Distance Distribution Loss for Audio Classification,"Classification is a pivotal task in deep learning not only because of its intrinsic importance, but also for providing embeddings with desirable properties in other tasks. To optimize these properties, a wide variety of loss functions have been proposed that attempt to minimize the intra-class distance and maximize the inter-class distance in the embeddings space. In this paper we argue that, in addition to these two, eliminating hierarchies within and among classes are two other desirable properties for classification embeddings. Furthermore, we propose the Angular Distance Distribution (ADD) Loss, which aims to enhance the four previous properties jointly. For this purpose, it imposes conditions on the first and second order statistical moments of the angular distance between embeddings. Finally, we perform experiments showing that our loss function improves all four properties and, consequently, performs better than other loss functions in audio classification tasks.","Classification is one of the main tasks to be solved with machine learning. In this task, there are typically high-dimensional elements and the goal is to decide to which class of a finite set each of these elements belongs. For this purpose, most of the solutions, particularly those based on deep learning, involve obtaining intermediate representations of reduced dimension of the elements to be classified. These representations are called embeddings and they can be considered as a summary of these elements containing the information that is relevant for classification. This problem is very popular not only because of its intrinsic importance, but also because it provides a simple way to obtain embeddings compared to other methods. Embeddings are useful for a multitude of tasks such as anomaly detection [1, 2], biometric recognition [3, 4], etc. The standard loss function to solve the classification task is the cross-entropy. As a secondary result of using this loss function, the embeddings of the different classes usually end up being somewhat separated. However, it is common to impose certain conditions directly on them due to two reasons: (i) this tends to improve the performance in the classification problem by guiding more the optimization [5, 6]; and (ii) it may be desirable for embeddings to have certain properties when used for a specific task other than classification [7, 8]. These conditions on embeddings are usually imposed through the loss function. Typically, a term is added to the cross-entropy or a modification is made to it. In this paper we propose a loss function that is added to cross-entropy and we call it Angular Distance Distribution Loss because it imposes conditions on the first and second order statistical moments of the angular distances between embeddings in order to organize the embeddings in the space. Specifically, this organization consists of: (i) bringing embeddings of the same class closer, (ii) moving embeddings of different class away, (iii) minimizing the variation of the distances of the embeddings of the same class, and (iv) making the embeddings of a class equal in distance to the embeddings of any class. Traditionally, only the first two have been considered in the literature. However, in section 3 we formalize all four, arguing why they are all important. In addition, we reason how they relate to the statistical moments of the distances between embeddings. Furthermore, we propose an experimental framework with different Audio Classification datasets. In these experiments, on the one hand, we verify that our embeddings satisfy the properties described in the previous paragraph, so we verify that our loss function encourages the properties to be satisfied. On the other hand, we obtain a better accuracy than other loss functions that aim to establish conditions on the embeddings. Thus, we verify that the described properties translate into better classification performance. The details of these experiments are presented in the section 4 and can be replicated using the code in https://github.com/antonioalmudevar/distance_distribution_loss"
https://arxiv.org/html/2411.00121v1,I Can Hear You: Selective Robust Training for Deepfake Audio Detection,"Recent advances in AI-generated voices have intensified the challenge of detecting deepfake audio, posing further risks for the spread of scams and disinformation. To tackle this issue, we establish the largest public voice dataset to date, named DeepFakeVox-HQ, comprising 1.3 million samples, including 270,000 high-quality deepfake samples from 14 diverse sources. Despite previously reported high accuracy, existing deepfake voice detectors struggle with our diversely collected dataset, and their detection success rates drop even further under realistic corruptions and adversarial attacks. We conduct a holistic investigation into factors that enhance model robustness and show that incorporating a diversified set of voice augmentations is beneficial. Moreover, we find that the best detection models often rely on high-frequency features, which are imperceptible to humans and can be easily manipulated by an attacker. To address this, we propose the F-SAT: Frequency-Selective Adversarial Training method focusing on high-frequency components. Empirical results demonstrate that our training dataset boosts baseline model performance (without robust training) by 33%, and our robust training further improves accuracy by 7.7% on clean samples and by 29.3% on corrupted and attacked samples, over the state-of-the-art RawNet3 model.","AI-generated voices have become increasingly realistic due to larger datasets and enhanced model capacities (Ju et al., 2024; Neekhara et al., 2024), and they have been used in many important applications (Calahorra-Candao & Mart√≠n-de Hoyos, 2024). However, the success of AI-synthesized human voices poses significant security risks, including deepfake voice fraud and scams (Tak et al., 2021; Sun et al., 2023; Yang et al., 2024). A recent CNN report reveals a fraud in Hong Kong where a finance worker sent $25 million to scammers after a video call with a deepfake ‚Äòchief financial officer‚Äô. The voice was created by an AI model, highlighting the risk of such technology. Figure 1: The distribution of deepfake samples over predicted scores using the state-of-the-art detector (Jung et al., 2022) trained on the In-the-Wild dataset (M√ºller et al., 2022), with a decision boundary at 0.5. Tests on original, corrupted, attacked, and real-world deepfake audio reveal significant shifts in prediction scores, highlighting that training solely on current public datasets without robust training methods leads to poor performance. Figure 2: We apply a high-pass filter to audio samples to remove low-frequency components. The x-axis represents the center frequency of the filter applied. Notably, there is a marked decline in detection performance for real audio starting at 4000 Hz and for fake audio at 6000 Hz. Due to the importance of this problem, a number of work has investigated detecting AI-generated audio. Despite previously reported high detection accuracy on public datasets (Todisco et al., 2019; Frank & Sch√∂nherr, 2021), existing deepfake voice detectors perform poorly under real-world conditions (Xu et al., 2020; M√ºller et al., 2022; Radford et al., 2023). This is because the established benchmarks are often trivial, small, outdated, and homogeneous. Consequently, models trained and validated solely on these datasets fail to generalize to more diverse and challenging real-world deepfake samples. Moreover, deep learning models for audio are particularly vulnerable to adversarial attacks (Szegedy et al., 2013) (Zhang et al., 2019), where an attacker can subtly alter audio inputs in ways that are imperceptible to humans but mislead models into incorrect classifications. Figure 1 illustrates a dramatic shift in the models‚Äô prediction scores when exposed to these factors, underscoring the need for more robust training methodologies. To address the above limitations, we first created the largest deepfake audio dataset to date, DeepFakeVox-HQ, including 270,000 high-quality deepfake samples from 14 diverse and distinct origins. We show that simply training on our collected dataset can produce new state-of-the-art models. Moreover, we find that even the state-of-the-art AI-voice detection models often depend on high-frequency features to make decisions (see Figure 2), which are imperceptible to humans. On the other hand, the low frequency signals can be heard by humans but are not relied on by the model to make predictions. As a result, natural corruptions in high frequency or attackers can easily manipulate the model by changing the high frequency signals, reducing the detection‚Äôs robustness. In an initial study, we observed that standard adversarial training on raw waveforms not only fails to bolster robustness but also diminishes performance on unattacked data. To address these shortcomings, we propose Frequency-Selective Adversarial Training (F-SAT), which focuses on high-frequency components. Since our adversarial training is targeted, we can mitigate specific vulnerabilities without touching the true features at lower frequencies, thus enhancing the model‚Äôs resilience to corruptions and attacks while maintaining high accuracy on clean data. Visualizations and empirical experiments demonstrate that using only our training dataset, we can produce state-of-the-art models, achieving a 33% improvement on the out-of-distribution portion of our test set, which includes 1,000 deepfake samples from the top five AI voice synthesis companies and 600 samples from social media. Additionally, by incorporating random audio augmentations, our model achieves the highest accuracy across 24 different types of corruptions. Furthermore, after applying F-SAT, our model further achieves a 30.4% improvement against adversarial attacks in the frequency domain and an 18.3% improvement against unseen attacks targeting raw waveform data in the time domain."
https://arxiv.org/html/2411.00064v1,"The ISCSLP 2024 Conversational Voice Clone (CoVoC) Challenge: Tasks, Results and Findings","The ISCSLP 2024 Conversational Voice Clone (CoVoC) Challenge aims to benchmark and advance zero-shot spontaneous style voice cloning, particularly focusing on generating spontaneous behaviors in conversational speech. The challenge comprises two tracks: an unconstrained track without limitation on data and model usage, and a constrained track only allowing the use of constrained open-source datasets. A 100-hour high-quality conversational speech dataset is also made available with the challenge. This paper details the data, tracks, submitted systems, evaluation results, and findings. The challenge‚Äôs official website is https://www.magicdatatech.com/iscslp-2024.","Text-to-speech (TTS) aims to generate speech that sounds as natural and human-like as possible. Recent advancements in neural speech synthesis have significantly enhanced the quality and naturalness of generated speech [1, 2, 3], leading to widespread applications of TTS systems in real-world scenarios. A notable breakthrough in the field is witnessed in zero-shot TTS, driven by expanded datasets [4] and new approaches [5] (e.g., decoder-only paradigms), attracting extensive attention from academia and industry. However, these advancements haven‚Äôt been sufficiently investigated to address challenges in spontaneous [6, 7] and conversational [8] contexts. Specifically, the primary challenge lies in effectively managing prosody details in the generated speech, which is attributed to the diverse and intricate spontaneous behaviors that differentiate spontaneous speech from read speech. Large-scale TTS systems yield promising outcomes in zero-shot generation due to in-context learning ability. However, a prevalent challenge in the field of large-scale zero-shot TTS is the lack of consistency in training and testing datasets, along with a standardized evaluation benchmark. This issue hinders direct comparisons and makes it challenging to accurately assess various systems‚Äô performance. We launch the Conversational Voice Clone Challenge (CoVoC) to promote the development of expressive spontaneous-style speech synthesis in the zero-shot scenario. Besides the existing 10,000-hour WenetSpeech4TTS [9] dataset and 180 hours of Mandarin conversational speech data 111https://www.openslr.org/123/, we also release a new 100-hour high-quality conversational dataset. Furthermore, we also conduct a standardized testing dataset accompanied by carefully designed text which aims to establish a comprehensive benchmark. This paper presents the data details, track design, submitted systems, evaluation results, and key findings."
https://arxiv.org/html/2411.00023v2,Device-Directed Speech Detection for Follow-up Conversations Using Large Language Models,"Follow-up conversations with virtual assistants (VAs) enable a user to seamlessly interact with a VA without the need to repeatedly invoke it using a keyword (after the first query). Therefore, accurate Device-directed Speech Detection (DDSD) from the follow-up queries is critical for enabling naturalistic user experience. To this end, we explore the notion of Large Language Models (LLMs) and model the first query when making inference about the follow-ups (based on the ASR-decoded text), via prompting of a pretrained LLM, or by adapting a binary classifier on top of the LLM. In doing so, we also exploit the ASR uncertainty when designing the LLM prompts. We show on the real-world dataset of follow-up conversations that this approach yields large gains (20-40% reduction in false alarms at 10% fixed false rejects) due to the joint modeling of the previous speech context and ASR uncertainty, compared to when follow-ups are modeled alone.","Virtual assistants (VAs) are at the core of smart devices (e.g., mobile phones, smart speakers, wearables, etc.) as they aim to enable a naturalistic voice-based interaction between a user and a device. For VAs to respond to the user requests reliably, they need to infer whether the user is talking to the device or not. For instance, the user could be talking to someone else, and/or there could be side-speech conversations, background noise, etc. Therefore, classifying accurately if the user‚Äôs speech is device-directed is critical for providing relevant responses, and to avoid interfering with the user‚Äôs interactions which are not device-directed, i.e., intended for the VA. This task is often referred to as the device-directed speech detection (DDSD) [1; 2; 3; 4]. Most existing works on DDSD focus on detection from single queries of the user, often beginning with a wakeword (e.g., ‚ÄúHey Google‚Äù, ‚ÄúHey Siri‚Äù, ‚ÄúAlexa‚Äù, and so on). Such isolated utterances are usually a complete question or a task request from a user to the VA, and often do not require additional context to determine if the speech is the VA directed. In this work, we address the task of DDSD in follow-up conversations, where the user‚Äôs first query starts with a wakeword (that is easier to detect with high accuracy by existing systems for the wakeword detection [5]), potentially followed by another query (termed as the ‚Äúfollow-up‚Äù), as a continuation of the conversation with the VA. The follow-ups by design do not require the wakeword, and, therefore, classifying them correctly is far more challenging (see Fig. 1). Figure 1: Follow-up conversations: the pair of user queries are first processed by an ASR system, which outputs the text transcriptions of the user‚Äôs speech. The joint ASR transcription of the initial and follow-up queries are input to the LLM that detects if the latter is directed to the Virtual Assistant (VA). Previous approaches to DDSD process isolated utterances (i.e., no previous context is considered) either directly from audio [2], text [6], or from intermediate Automatic Speech Recognition (ASR) lattice-based features [7]. A few recent approaches attempted classification of device-directed speech in the context of natural turn-taking [3; 4; 8] by exploring various acoustic and lexical features; however, these works do not account for joint modeling of the ASR uncertainty and multi-turn user queries. The approach proposed here is built upon the recent work in [1] that focuses on the DDSD task from a single query using a combination of off-the-shelf ASR model (nùëõnitalic_n-best ASR hypotheses) and LLM. We focus on a more challenging task ‚Äì DDSD for follow-ups, where, given the previous query of the user, the goal is to determine if the follow-up query is directed to the device. For this, we propose two LLM-based approaches: (i) prompting-based, where we experiment with direct text-prompting of a pretrained, and a finetuned LLM (Sec. 2.1). (ii) classification-based approach, where, instead of parsing the LLM decisions from output text, as in (i), we introduce a classification head on top of the LLM, so the approach can make probabilistic binary decisions for the DDSD (Sec. 2.2). Specifically, we simultaneously address two key limitations of the existing systems for DDSD: (i) the lack of context ‚Äì where only the single query is modeled; instead, we account for both the initial and follow-up queries, thus, providing valuable information to the LLM when making decisions, and (ii) ASR uncertainty ‚Äì while ASR models aim to transcribe speech to text accurately, speech recognition from the real-world speech is often inaccurate and the 1111-best ASR hypothesis acts as an information bottleneck between the ASR and the LLM component. We expand this information bottleneck by exposing the LLM to an nùëõnitalic_n-best list of ASR hypotheses, and we do so only for the follow-up utterance to prevent confusing the model, i.e., we focus on the uncertainty in the follow-up only. Apart from direct prompting, we also explore finetuning of Low-Rank Adaptation (LoRA) [9] adapters using training data of the nùëõnitalic_n-best list prompts. We use an in-house general-purpose English ASR system and Vicuna [10], an instruction-tuned LLaMA LLM [11]. Prompt: In this task, we provide a pair of queries made by human in the following format: ‚ÄòQuery 1: <<<text>>> | Query 2: <<<text>>>‚Äô. Query 1 is directed toward the voice assistant. Query 2 is the follow-up query made by human. {For Query 2, we provided an n-best list of ASR hypotheses for the spoken utterance. Each of the hypothesis is separated by a newline character. The cost of each hypothesis is at the end in the format ‚Äò[cost]‚Äô where a low cost indicates that we are more confident about that ASR hypothesis.} Determine whether Query 2 is directed towards a voice assistant or a human being. Typical spoken utterances directed towards the voice assistant are commands to fulfill a task or queries to get some information. Answer only from the following categories [‚Äò1‚Äô, ‚Äò0‚Äô] where ‚Äò1‚Äô indicates that the utterance is directed towards the voice assistant and ‚Äò0‚Äô indicates that the utterance is directed towards a human being. In your answer the last line should contain nothing else but the number ‚Äò0‚Äô or ‚Äò1‚Äô. Figure 2: Task-prompt used for Device-directed Speech Detection. Note that for the follow-up query, the text in italics is additionally added when including the n-best hypothesis by the ASR system. Our experiments show noticeable gains in terms of the DDSD accuracy over the simple approach where only the follow-up query is used as input to LLM. While this is intuitive, we empirically show on the challenging real-world dataset of follow-up conversations that jointly modeling the context (the previous and follow-up query), together with ASR uncertainty, helps to improve the LLM detection accuracy in the range of ‚àºsimilar-to\sim‚àº20-40%, when the classifier is trained on top of the LLM. It also largely outperforms the traditional prompting-based approach without the LLM tuning on the target task."
