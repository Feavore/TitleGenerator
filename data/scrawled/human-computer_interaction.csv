URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04691v1,AWARE Narrator and the Utilization of Large Language Models to Extract Behavioral Insights from Smartphone Sensing Data,"Smartphones, equipped with an array of sensors, have become valuable tools for personal sensing. Particularly in digital health, smartphones facilitate the tracking of health-related behaviors and contexts, contributing significantly to digital phenotyping, a process where data from digital interactions is analyzed to infer behaviors and assess mental health. Traditional methods process raw sensor data into information features for statistical and machine learning analyses. In this paper, we introduce a novel approach that systematically converts smartphone-collected data into structured, chronological narratives. The AWARE Narrator translates quantitative smartphone sensing data into English language descriptions, forming comprehensive narratives of an individual’s activities. We apply the framework to the data collected from university students over a week, demonstrating the potential of utilizing the narratives to summarize individual behavior, and analyzing psychological states by leveraging large language models.","The modern smartphone contains an array of sensors that enable the sensing and tracking of various phone states, uses and properties. These sensors include accelerometer, GPS/geolocation, Bluetooth, communication logs (phone and SMS), application usage and keyboard activity. Given their various sensors and the opportunities to utilise them, smartphones, the Swiss army knives of digital technology, have proven to be valuable personal sensing devices, with applications in domains such as health, education and leisure. Given their potential to track various health-related behaviours and user contexts, as well as the emergence of health apps, smartphone sensing has become a pivotal topic in digital health. This is particularly the case in digital mental health, where the concept of digital phenotyping has emerged in recent years. In short, digital phenotyping espouses the idea that the data created from our use of and interaction with digital technologies, such as smartphones, can be mined or analysed to infer behaviours and, ultimately assess mental health (Baumeister and Montag, 2019; Bufano et al., 2023). The focus of our work in this paper is on leveraging smartphone sensing as a tool in psychology and mental health. Once raw sensor data is collected, it is typically processed into information features that can be used in statistical analyses and machine learning model construction. For instance, from raw geolocation data one, features such as total distance travelled or time spent at the most visited location can be derived. In this paper, however, we propose a novel approach to analyze smartphone sensing data. The core idea is to translate quantitative smartphone sensing data records into corresponding descriptions in English (or other natural language), which could ultimately be used to construct a narrative summary that describes an individual’s day (or other specified period). The generation of sensing statements based on data from digital sensors, let alone modern smartphone sensors, has received limited attention in the literature (Reddington and Tintarev, 2011). Beyond general interest, from our perspective the idea of translating smartphone data records into English descriptions has a further motivation in recent times given the contemporary influence of large language models (LLM) and the availability of systems such as OpenAI’s GPT models and Google’s Gemini. Whilst one approach involves applying LLMs to tabular data for tasks such as description, prediction and general quantitative reasoning (Fang et al., 2024), another promising avenue is to convert tabular smartphone data records into sets of English statements. These statements could then be fed into LLMs to extract descriptive summaries, generate pattern insights, or even explore the potential for LLMs to make inferences about mental health. AWARE Narrator systematically and chronologically organizes smartphone-collected data into structured narratives. In this study, we demonstrate the potential of consolidating multi-sensor smartphone data into sensing statements that can be effectively utilized with LLMs. This approach shifts the behavioral analysis problem into the realm of natural language processing. Compared to traditional analysis methodologies (e.g. basic data feature calculations), this approach offers several benefits. First, by integrating data from a wide array of sensors, the sensing statements reveal more information that might not be readily apparent through conventional analysis. Compared to raw quantitative and categorical data, the sensing statements offer finer granularity and richer, human-readable information. Second, this approach provides multi-dimensional information. Each entry point of the sensing statement encapsulates various dimensions such as timeline, sensor type, and detailed target information collected (e.g. message/call contact names or Bluetooth device names). Third, the sensing statements are designed to be interpretable by both humans and machines, thereby enhancing transparency in data analysis. This dual interpretability may better facilitate communication and collaboration between human analysts and automated systems."
https://arxiv.org/html/2411.04671v1,CUIfy the XR: An Open-Source Package to Embed LLM-powered Conversational Agents in XR,"Recent developments in computer graphics, machine learning, and sensor technologies enable numerous opportunities for extended reality (XR) setups for everyday life, from skills training to entertainment. With large corporations offering consumer-grade head-mounted displays (HMDs) in an affordable way, it is likely that XR will become pervasive, and HMDs will develop as personal devices like smartphones and tablets. However, having intelligent spaces and naturalistic interactions in XR is as important as technological advances so that users grow their engagement in virtual and augmented spaces. To this end, large language model (LLM)–powered non-player characters (NPCs) with speech-to-text (STT) and text-to-speech (TTS) models bring significant advantages over conventional or pre-scripted NPCs for facilitating more natural conversational user interfaces (CUIs) in XR. In this paper, we provide the community with an open-source, customizable, extensible, and privacy-aware Unity package, CUIfy, that facilitates speech-based NPC-user interaction with various LLMs, STT, and TTS models. Our package also supports multiple LLM-powered NPCs per environment and minimizes the latency between different computational models through streaming to achieve usable interactions between users and NPCs. We publish our source code in the following repository: https://gitlab.lrz.de/hctl/cuify","Recent advances in computer graphics, hardware, and artificial intelligence have led virtual and augmented reality (VR/AR) systems to become ubiquitous and head-mounted displays (HMDs) to be used more regularly. VR and AR have different use contexts and configurations, and each of them provides different advantages for users. For instance, VR is especially useful for generating simulations to train users in a fully immersive setting [1, 2, 3, 4]; AR is more convenient when context-aware visual support is overlaid on real-world content [5, 6, 7]. While both VR and AR, which are part of the broader field of extended reality (XR), can be useful for everyday life, making such virtual and augmented spaces intelligent often requires significant engineering effort, especially for environmental and social interactions. Considering intelligent XR spaces, practitioners often utilize non-player characters (NPCs), and these characters interact with users for different purposes [8, 9]. However, single-purpose NPCs may cause users to lose interest after a few interactions, as these characters tend to repeat the same or very similar content, eventually leading users to stop using the XR application. To this end, generative artificial intelligence (AI) and particularly large language models (LLMs) can provide numerous opportunities for XR due to their versatile computational capabilities, as they are trained with a significant portion of the Internet and can generate highly realistic synthetic data. By default, LLMs are utilized for the next-word prediction task; however, they can also be aligned for conversational purposes [10], and one of the most prevalent examples is ChatGPT, which became publicly available in 2022 [11]. Since then, the general public has been more heavily exposed to generative AI systems and models. At the same time, the use of LLMs has accelerated in various domains, including medicine [12], law [13], and education [14]. While LLMs and generative AI models can be embedded into XR for different purposes, such as for creating 3D virtual content based on user preferences or for modifying interactive experiences, one of the most straightforward schemes is to embed LLMs into NPCs for speech-based interaction so that they can execute as conversational user interfaces (CUIs). In fact, prior research has utilized LLMs for speech-based NPC-user interactions with speech-to-text (STT) and text-to-speech (TTS) models [15, 16, 17]. However, to the best of our knowledge, there is no open-source software that implements the pipeline consisting of STT, LLM, and TTS models for XR in a generic and extendable way. This means that for every XR application that includes LLM-based speech interaction, especially with NPCs, practitioners either implement the aforementioned pipeline from scratch or replicate it from previous projects, likely by also carrying out some modifications. Considering the aforementioned issue, in this paper, we provide the community with an open-source Unity111Unity is a widely used, cross-platform game engine that has been developed by Unity Software Inc. package that combines LLMs, STT, and TTS models into a pipeline to enable speech-based interaction. Our package minimizes the latency between the models by utilizing streaming, supports plugging different models and pipelines into multiple NPCs in a single environment, and can prompt the LLMs. Furthermore, it supports both accessing LLMs via application programming interfaces (APIs) and handling open-source LLMs either on local devices or on a separate server. The source code is available on the following repository: https://gitlab.lrz.de/hctl/cuify."
https://arxiv.org/html/2411.04568v1,Dynamic-Attention-based EEG State Transition Modeling for Emotion Recognition,"Electroencephalogram (EEG)-based emotion decoding can objectively quantify people’s emotional state and has broad application prospects in human-computer interaction and early detection of emotional disorders. Recently emerging deep learning architectures have significantly improved the performance of EEG emotion decoding. However, existing methods still fall short of fully capturing the complex spatiotemporal dynamics of neural signals, which are crucial for representing emotion processing. This study proposes a Dynamic-Attention-based EEG State Transition (DAEST) modeling method to characterize EEG spatiotemporal dynamics. The model extracts spatiotemporal components of EEG that represent multiple parallel neural processes and estimates dynamic attention weights on these components to capture transitions in brain states. The model is optimized within a contrastive learning framework for cross-subject emotion recognition. The proposed method achieved state-of-the-art performance on three publicly available datasets: FACED, SEED, and SEED-V. It achieved 75.4±5.5%plus-or-minus75.4percent5.575.4\pm 5.5\%75.4 ± 5.5 % accuracy in the binary classification of positive and negative emotions and 59.3±7.7%plus-or-minus59.3percent7.759.3\pm 7.7\%59.3 ± 7.7 % in nine-class discrete emotion classification on the FACED dataset, 88.1±3.6%plus-or-minus88.1percent3.688.1\pm 3.6\%88.1 ± 3.6 % in the three-class classification of positive, negative, and neutral emotions on the SEED dataset, and 73.6±12.7%plus-or-minus73.6percent12.773.6\pm 12.7\%73.6 ± 12.7 % in five-class discrete emotion classification on the SEED-V dataset. The learned EEG spatiotemporal patterns and dynamic transition properties offer valuable insights into neural dynamics underlying emotion processing.","Emotions play a crucial role in human social life. Quantifying human emotions and enabling intelligent devices to interpret them can significantly enhance human-centric intelligent interaction systems. This also facilitates real-time monitoring and early detection of emotional disorders[1]. Electroencephalography (EEG) measures multi-channel electrical activities of the neural system, providing rich information and high objectivity of emotion recognition, which has increasingly attracted researchers’ attention[2]. Many studies have explored effective EEG features and deep learning architectures for emotion decoding. Commonly used features include power spectral density (PSD)[3] and differential entropy (DE, equivalent to the logarithmic energy spectrum)[4], both of which capture the power or energy of local electrode signals in specific frequency bands. Various deep learning methods, such as recurrent neural networks (RNNs)[5], convolutional neural networks (CNNs)[6], and graph neural networks (GNNs)[7], have been applied to learn emotion-related representations from DE features. Attention mechanisms have also been adopted recently to emphasize critical feature dimensions for emotion recognition[8]. Many of these methods focus on identifying interaction between features from different brain regions. However, the rapid dynamics of EEG signals on finer time scales have been largely ignored. Neural activities exhibit complex and intriguing spatiotemporal dynamics [9]. From the neuron population to the whole-brain level, the spatial and temporal dynamic analyses are key to understanding the brain’s organization and functions [10], [11]. EEG signals encompass orchestration dynamics of neuronal activities at the macro- and millisecond-level [12]. State transition modeling is a major approach to studying EEG spatiotemporal dynamics, which reveals meta-stable states of spatial, spectral, or network activities in EEG [13], [14], [15]. Temporal dynamics of these transient states serve as functional or behavioral signatures and changes in neurological or mental disorders [16], [17], [18], [19], [20]. Different emotion states modulate temporal features and transition properties of these EEG states [21], [22], [23], [24], [25]. It is promising to investigate EEG spatiotemporal dynamics to understand how the brain produces different emotion states and develop biophysically interpretable models for emotion recognition. Previous EEG state transition modeling mainly relies on simple clustering methods like microstate [13] or statistical methods like hidden Markov models [14]. These methods suffer from weak representative power and do not account for the inter-subject differences of EEG dynamics. Deep learning provides flexible model architectural design and self-supervised learning strategies to extract target-related and generalizable representations [26]. However, current deep-learning-based emotion recognition methods largely overlook EEG state transition properties. A deep learning architecture designed for EEG state transition modeling can potentially boost emotion recognition performance. In this paper, we propose a Dynamic-Attention-based EEG State Transition (DAEST) model for emotion recognition. With the assumption that dominant EEG spatiotemporal components vary in different brain states, we estimate dynamic attention weights on these spatiotemporal components at each time point. The spatiotemporal components are extracted using a combination of temporal convolution for spectral decomposition and spatial transition convolution for spatial variation pattern estimation. A depthwise convolution with temporal pooling is employed to estimate the dynamic attention weights on these components. We utilize a contrastive learning method for inter-subject alignment in cross-subject emotion recognition. It identifies interpretable emotion-related EEG spatiotemporal dynamics shared across subjects and facilitates the generalization of emotion recognition across different individuals. The code is made publicly available at https://github.com/RunminGan1218/DAEST."
https://arxiv.org/html/2411.04542v1,Automatic Identification of Political Hate Articles from Social Media using Recurrent Neural Networks,"The increasing growth of social media provides us with an instant opportunity to be informed of the opinions of a large number of politically active individuals in real-time. We can get an overall idea of the ideologies of these individuals on governmental issues by analyzing the social media texts. Nowadays, different kinds of news websites and popular social media such as Facebook, YouTube, Instagram, etc. are the most popular means of communication for the mass population. So the political perception of the users toward different parties in the country is reflected in the data collected from these social sites. In this work, we have extracted three types of features, such as the stylometric feature, the word-embedding feature, and the TF-IDF feature. Traditional machine learning classifiers and deep learning models are employed to identify political ideology from the text. We have compared our methodology with the research work in different languages. Among them, the word embedding feature with LSTM outperforms all other models with 88.28% accuracy.","Text is the most important means of communication in today’s world. Popular online social networking sites such as Facebook, X, LinkedIn, etc. are mainly text-based. The rapid growth of social media has created enough opportunities to share information across time and space. Users are now more comfortable contributing to the content of social media websites and posting their own material. With the constant flow of information on the internet, individuals are inclined to consume a greater amount of content from various social media posts and the accompanying comments. Nowadays, the people of Bangladesh heavily rely on social media [tasnim2021political]. They have a huge amount of political information at their disposal. Individuals have the ability to share and articulate their opinions or critique political news from their own perspective using a global platform. They express their critical opinions by commenting on the news articles. An individual’s political ideology may often be discerned by examining the comments in the comment sections of political news articles, as an individual’s words often reveal their political ideology [iyyer2014political]. The classical approach to feature extraction for opinion mining from textual data is to identify unique stylometric features of written texts. The underlying assumption here is that each author has unique writing styles that are relatively fixed and barely change with time. So we can use stylometric features to uniquely identify the writing style of the author [shalabi_kanaanbt]. Along with the stylometric feature, we will use the TF-IDF vectorizer and Word Embedding approach to identify political ideology from textual data. In this work, we are interested in implementing an intelligent system that can analyze and predict political ideology on the basis of the political debate online on social media websites. The system can take any political comment and predict if it is a positive or negative comment. Based on the prediction, the political ideology of that user can be detected easily. We are interested in addressing the political ideology problem from social media Bangla text. To the best of our knowledge, only one paper has previously addressed this problem. Figure 1: Problem Statement Although political ideology(PI) identification has been widely studied in different languages, it is still understudied in the Bangla language. Bangla language is one of the most widely spoken and culturally rich languages. This language is the 7th most spoken language [spoken_language] of the world and the native language of Bangladesh. However, this is not the only reason to study PI problems in the Bangla language. The problems associated with the Bangla language and the relatively under-developed field of Bangla Natural Language Processing (NLP) makes it more challenging to study such problems for Bangla. In this work, we will follow the following steps to study the political ideology identification problem. We will create a dataset from Facebook containing political posts and neutral posts. We will then pre-process the data and extract features from the data in 3 ways. One is called the Bag-of-words technique. Another one is computing stylometric features to capture the writing style of the author. The third one is using a word embedding approach to convert text into a feature vector. In recent years, deep learning-based recurrent neural models have been used to automate political ideology extraction due to their performance in building models. These models do not require to be provided with pre-defined handpicked features. Instead, they can learn useful features from the data by themselves [Bsir_zrigui]. In this work, we will use deep learning recurrent models to automate political ideology determination from Facebook textual data. Specifically, we will use LSTM and GRU models from deep learning recurrent neural network models. From the traditional model, we will use SVM and NB models. Then the performance of the deep learning model is compared to traditional machine learning models. The possible contributions to this work are as follows: • We will create a dataset that contains user political text or neutral text in the Bangla language. • We will design a system that aims to identify political ideology from social media comments or text in the Bangla language. • We will propose several types of stylometric features as political post indicators for the Bangla language. We will design a set of measures to infer political ideology from short writings through extensive experiments. • We will apply machine learning classifiers and deep learning models to identify political bias in short writing. The rest of this paper is organized as follows: Section II overviews the related works of the political ideology determination problem. Section III discusses the mathematical background of the problem. In section IV, we have proposed our detailed solution. Section V presents the experimental results. Finally, in Section VI, we provide project planning and schedule."
https://arxiv.org/html/2411.04524v1,"Emotion Analysis of Social Media Bangla Text and Its Impact on Identifying the Author’s
Gender","The Gender Identification (GI) problem is concerned with determining the gender of the author from a given text. It has numerous applications in different fields like forensics, literature, security, marketing, trade, etc. Due to its importance, researchers have put extensive efforts into identifying gender from the text for different languages. Unfortunately, the same statement is not true for the Bangla language despite its being the 7th most spoken language in the world [spoken_language]. In this work, we explore Gender Identification from Social media Bangla Text. Specially, we consider two approaches for feature extraction. The first one is Bag-Of-Words(BOW) approach and another one is based on computing features from sentiment and emotions. There is a common stereotype that female authors write in a more emotional way than male authors. One goal of this work is to validate this stereotype for the Bangla language.","Text is the most important means of communication in today’s world. Popular online social networking sites such as Facebook, Twitter, and MySpace etc. are mainly text based. The rapid growth of the Social Media has created enough opportunities to share information across time and space. Users are now comfortable to contribute more to contents of the social media websites and post their own material. The underlying anonymity and the lack of accountability of social media websites make it difficult to determine the real author of any text posted online. Any user can share anything on social media websites and can claim to be another person or falsely claim to have certain characteristics (such as gender, age, etc.). To detect this false claim accurately and efficiently, we need an automated tool so that user anonymity is no longer preserved. The authorship analysis(AA) problem is concerned with analyzing a text to determine the identity or the characteristics of the author based on content of the text. Determining identity from text is known as authorship authentication problem. Again, determining characteristics of author from text is known as authorship profiling problem. Authorship analysis is achieved based on the context and writing style of the text [abbasi_chen, altheneyan_bachirMenai, ranjel_rosso, tetreault_cahill]. Each type of AA problem has its own applications. One example is from security applications where deception and fraud can be prevented using authorship profiling technique. We can apply authorship profiling techniques to targeted advertisement which is one of the key revenue generator for many Internet based companies. Authorship authentication deals with doubtful attribution to well-known authors such as Shakespeare. The classical approach to feature extraction for authorship analysis problem is to identify unique stylometric features of written texts. The underlying assumption here is that each author has unique writing styles that are relatively fixed and barely changes with time. So we can use stylometric features to uniquely identify the author (or his/her characteristics) [shalabi_kanaanbt]. In this work, we are interested in determining gender of author given a text which is basically an authorship profiling problem. The Gender Identification(GI) problem has numerous applications including from marketing to security. We are interested in addressing the GI problem for social media Bangla text. To the best of our knowledge, only one paper has previously addressed this problem. Although gender identification has been widely studied in different languages, it is still under studied in Bangla language. Bangla language is the one of the most widely spoken and culturally rich language. This language is the 7th most spoken language [spoken_language] of the world and native language of Bangladesh. However, this is the only reason to study GI problem in Bangla language. The problems associated with the Bangla language and the relatively under-developed field of Bangla Natural Language Processing (NLP) makes it more challenging to study such problems for Bangla. In this work, we will follow the following steps to study the GI problem. We will use one the dataset collected from one of the previous work. The dataset consists of Facebook text written by various authors of both genders. We will then pre-process the data and extract feature from the data in 2 ways. One is called Bag-of-words technique and another one is based on computing features related to sentiments and emotions. There is a common stereotype that female authors write in a more emotional way than male authors [jaffe_lee, Cheng_Na]. One goal of this work is to validate this stereotype. We perform various experiments and do a t-test on the returned result in order to claim or disclaim the statement. The rest of this paper is organized as follows: Section II overviews the related works of gender identification problem. In section III, we have proposed our detailed solution. Section IV presents the experimental results. Finally, in section V, we conclude our findings with a discussion of the obtained observations and the future directions of this work."
https://arxiv.org/html/2411.04499v1,Memory Remedy: An AI-Enhanced Interactive Story Exploring Human-Robot Interaction and Companionship,"We present our approach to using AI-generated content (AIGC) and multiple media to develop an immersive, game-based, interactive story experience. The narrative of the story, ”Memory Remedy”, unfolds through flashbacks, allowing the audience to gradually uncover the story and the complex relationship between the robot protagonist and the older adults. This exploration explores important themes such as the journey of life, the profound influence of memories, and the concept of post-human emotional care. By engaging with this AIGC-based interactive story, audiences are encouraged to reflect on the potential role of robotic companionship in the lives of older adults in the future, and to encourage deeper reflection on the complex relationship between artificial intelligence and humanity.","1. INTRODUCTION Memory Remedy is an interactive game-based interactive storytelling that explores the themes of aging, connection, and companionship through the lens of artificial intelligence robot. This captivating narrative takes the audiences on a journey of self-discovery, emotional exploration, and profound human connections in a world where AI facilitates memories and companionship. Drawing from the context of global aging and the devastating impact of Alzheimer’s disease (AD). Memory Remedy delves into a thought-provoking exploration of how this debilitating condition affects personal and social relationships. The storytelling experience weaves together inter-temporal themes and explores the intricate relationship between humans and AI robot. Through active participation in the first-person perspective, the audience engages with the protagonist’s experiences and unusual choices. Through this immersive perspective, viewers gain a deeper understanding of the challenges, emotions, and difficult decisions faced by older adults with Alzheimer’s disease. Memory Remedy encourages reflection on the profound impact of the disease on personal identity, memory, and the fundamental connections that shape our lives. AI has proven effective in enhancing interactive storytelling and visual scene generation (Ai et al., 2024; Wang et al., 2023). We were inspired to investigate if incorporating an innovative AI tool (Labs, 2024), capable of generating immersive 3D panoramic visual scenes from text, alongside 3D computer graphics using Unreal Engine (UE) (unr, 2023), could enhance the expression of our work. We continuously refine the hypertext descriptions to craft richer visual scenes and construct a wider variety of narrative paths, to offer audiences a more immersive and dynamic experience. Combining immersive panoramic scenes with a diverse narrative, we intricately designed each choice to facilitate the transition from passive viewing to active decision-making. We found the integration of the AI tool which is effective to create a game-based interactive story that allows for personalized exploration and engagement. We provide an overview of the background and motivation behind the creation of “Memory Remedy”, discussing the artistic prototype. We proceed to describe the design of the narrative story and the generative pipeline, emphasizing the incorporation of AI technologies at each stage of the production process. Moreover, we delve into the intricate aspects of hypertext novel story and AI-enhanced scene generation, offering valuable insights into the creative choices and utilization of AI tools throughout the production process. We believe that the important contributions of humanities and art and design researchers must be better integrated into human interaction and media discussions of social robots and geriatric care."
https://arxiv.org/html/2411.04489v1,An Equitable Experience? How HCI Research Conceptualizes Accessibility of Virtual Reality in the Context of Disability,"Creating accessible Virtual Reality (VR) is an ongoing concern in the Human-Computer Interaction (HCI) research community. However, there is little reflection on how accessibility should be conceptualized in the context of an experiential technology. We address this gap in our work: We first explore how accessibility is currently defined, highlighting a growing recognition of the importance of equitable and enriching experiences. We then carry out a literature study (N=28) to examine how accessibility and its relationship with experience is currently conceptualized in VR research. Our results show that existing work seldom defines accessibility in the context of VR, and that barrier-centric research is prevalent. Likewise, we show that experience – e.g., that of presence or immersion – is rarely designed for or evaluated, while participant feedback suggests that it is relevant for disabled users of VR. On this basis, we contribute a working definition of VR accessibility that considers experience a necessary condition for equitable access, and discuss the need for future work to focus on experience in the same way as VR research addressing non-disabled persons does.","Making Virtual Reality (VR) accessible for disabled people is of ongoing concern within the Human-Computer Interaction (HCI) and accessibility research communities. For example, there have been a number of empirical investigations addressing access barriers, e.g., Mott et al. (2020) explore whether and how people with limited mobility can engage with VR, showing that the technology is associated with numerous access barriers. This is echoed by Creed et al. (2023, 2024), who carried out multidisciplinary sandpits with expert stakeholders including disabled people, and identified detailed research opportunities pertaining to VR hardware and software to remove access barriers for disabled people. Likewise, Gerling and Spiel (Gerling and Spiel, 2021) engaged in a theoretical examination of VR from the perspective of disability studies, highlighting that VR is a technology that places high demands on human bodies, which aligns with previous work reflecting on VR accessibility for different groups of disabled people (Mott et al., 2019). Here, Dudley et al. (2023) highlight the need for inclusive immersion in their recent literature review that surveyed VR and augmented reality research, suggesting that we need to move toward ”maximising the inclusiveness of VR and AR technologies”. However, while their work provides an extensive overview of existing systems, it does not explore the experiential domain of VR in the context of disability. This raises the question of what experiences disabled people are currently afforded by VR systems, and how experience is addressed in the context of accessibility research: The vision behind VR is one that deeply prioritizes the experiential qualities of the technology (Steuer et al., 1992), for example emphasizing the relevance of presence or the sense of actually being in the virtual environment (Slater and Wilbur, 1997) as one of the pillars of VR, and details of the human experience of VR are extensively studied in the context of the medium for non-disabled users (also see section 2.2.2). Yet, it remains unclear how the experiential domain of VR is approached in HCI research addressing disabled people, and whether experience plays a role in how accessibility is conceptualized. To address this gap, we raise the following two research questions: RQ1: How does the HCI and accessibility research community currently conceptualize accessibility of VR for disabled users? RQ2: What role does experiential accessibility or the opportunity for disabled people to have equitable experiences in VR play? We address these questions through a two-step research process: We first explore how accessibility is currently defined in HCI research and beyond, highlighting a growing recognition of the importance of equitable and enriching experiences. We then carry out a literature study (N=28) and engage in Qualitative Content Analysis (Zhang and Wildemuth, 2005) to examine how accessibility and its relationship with experience is currently conceptualized in research that addresses VR for disabled persons. Our results show that existing work seldom defines accessibility in the context of VR. Overall, research examining the barriers associated with VR is prevalent addressing concerns around safety and human factors, while there is a lesser focus on potential facilitators that could support accessible and meaningful VR experiences for disabled people. Likewise, we show that experience – e.g., that of presence or immersion (cf. section 2.2.2) – is rarely designed for or evaluated, which is a notable deviation from VR research addressing non-disabled persons in which experience is routinely considered. However, we observed numerous instances of disabled participants discussing the importance of experience without being prompted by researchers, underscoring its relevance for all user groups. On the basis of these results, our work makes the following three core contributions: (1) We provide a working definition of VR accessibility that accounts for safety, but considers experience a necessary condition for equitable access. (2) We discuss the experiential domain of VR, critically appraising core assumptions underpinning the technology in the context of disability to arrive at an inclusive perspective on the medium. (3) We present opportunities for future work to focus on experience in the same way as VR research addressing non-disabled persons does, outlining how our community can address previous calls for accessibility research to embrace third-wave HCI."
https://arxiv.org/html/2411.04340v1,Survival of the Notable: Gender Asymmetry in Wikipedia Collective Deliberations,"Communities on the web rely on open conversation forums for a number of tasks, including governance, information sharing, and decision making. However these forms of collective deliberation can often result in biased outcomes. A prime example are Articles for Deletion (AfD) discussions on Wikipedia, which allow editors to gauge the notability of existing articles, and that, as prior work has suggested, may play a role in perpetuating the notorious gender gap of Wikipedia. Prior attempts to address this question have been hampered by access to narrow observation windows, reliance on limited subsets of both biographies and editorial outcomes, and by potential confounding factors. To address these limitations, here we adopt a competing risk survival framework to fully situate biographical AfD discussions within the full editorial cycle of Wikipedia content. We find that biographies of women are nominated for deletion faster than those of men, despite editors taking longer to reach a consensus for deletion of women, even after controlling for the size of the discussion. Furthermore, we find that AfDs about historical figures show a strong tendency to result into the redirecting or merging of the biography under discussion into other encyclopedic entries, and that there is a striking gender asymmetry: biographies of women are redirected or merged into biographies of men more often than the other way round. Our study provides a more complete picture of the role of AfD in the gender gap of Wikipedia, with implications for the governance of the open knowledge infrastructure of the web.","In 2018, the physicists Donna Strickland, Gérard Mourou, and Arthur Ashkin received the Nobel Prize for “groundbreaking inventions in the field of laser physics”111Morou and Strickland shared half of the price for their work on chirped pulse amplification; the other half went to Ashkin for separate work.. The Nobel prize in Physics recognizes a lifetime of accomplishments of a scientific nature, but this award carried additional significance since it was the first to go to a woman in 55 years. Nonetheless, in the immediate aftermath of the announcement, internet users looking to know more about Donna Strickland could not find a Wikipedia entry about her — as one would normally expect when looking up an accomplished scientist — even though her two co-awardees had been listed in the encyclopedia since 2005. Later, it was discovered that there had indeed been an attempt to create an entry for her in Wikipedia. Some editors had drafted her biography, but this draft had never been published, and thus had never been indexed by search engines, since — unlike her scientific work — she had not been considered to be ‘notable’ enough to warrant the creation of a separate, individual entry about her (Bazely, 2018). On February 11, 2019, the biography of nuclear scientist Clarice Phelps, who is the first African American woman to help discover a new chemical element, was deleted from Wikipedia (Jarvis, 2019). The debate over whether Phelps met the internal notability standards of Wikipedia was contentious. Editors initially deleted her entry, then reinstated it, only to delete it again later. In total, her entry was deleted (and later reinstated) three times — each deletion sparking increasingly heated discussions. Last but not least, in 2017 a Wikipedia editor raised concerns about the notability of Margaret D. Foster (1895–1970), an American chemist known for being the first female chemist at the United States Geological Survey and for her involvement on the Manhattan Project (Harrison, 2019). Despite this record, her Wikipedia page was flagged for potential deletion due to concerns about her notability. These three scenarios — which are drawn from STEM areas but are not necessarily limited to those fields — exemplify how barrier-breaking women get undermined and undervalued and face challenges to be fully included in Wikipedia — the largest and most influential online encyclopedia. Though this issue reflects broader challenges women encounter across the web, the central role of Wikipedia in shaping public knowledge makes it a critical site where women struggle for the recognition they deserve. As a free encyclopedia, Wikipedia aspires to encapsulate the entirety of human knowledge. It stands as a significant source of encyclopedic information encompassing notable individuals from diverse countries, historical periods, and fields of knowledge on a global scale. Within its extensive body of knowledge, about 1.9 million entries cover the biographies of notable individuals like scientists, artists, politicians, etc., yet very few of these biographies are about women (approximately 19% in the English version, the largest of all Wikipedia versions (Konieczny and Klein, 2018)), highlighting a substantial gender gap in encyclopedic coverage between men and women. This is concerning, since the recognition of the merits of an individual, particularly in a societal context, requires fair representation, but also because the presence of this kind of gap in one of the pillars of the digital knowledge infrastructure poses the risk of propagating preexisting social biases in other digital platforms, and thus of further strengthening the underlying gender inequality in society (Conway, 2018). For example, Wikipedia is often used to train Machine Learning models for a variety of tasks, representing one of the highest quality corpora openly available. As a result, several AI models are at risk of being biased against women in various applications (Stanovsky et al., 2019; Gor et al., 2021; Kotek et al., 2023). Figure 1. Multi-state model depicting competing risks of nomination and each outcome of the deliberation \Description Multi-state model depicting competing risks of nomination and each outcome of the deliberation Prior work on the gender gap in Wikipedia has found that articles about women are brought up for deletion more frequently than those about men (Tripodi, 2023; Martini, 2023; Lemieux et al., 2023), yet it is not clear how and when this takes place. Like many other editorial actions, the deletion of an article is subject to the regular collaborative norms of Wikipedia (Reagle, 2007), and thus articles are susceptible of being flagged for deletion at any point during their editorial cycle. Our objective is thus to investigate the following question: how quickly do Wikipedia biographies interact with the Articles for Deletion (AfD) process? Specifically, we aim to determine if there is any gender-based discrepancy in the various stages of the process, from the initial nomination for deletion to the consensus outcome taken by the discussants (if any). Our analysis covers the full history of the AfD process, from January 15, 2001 to November 3, 2023. Definition of gender For our analysis, we rely on human annotations provided by the Wikidata project to identify the gender of biographical subjects. Even though the vital records information from Wikidata provided us with rich gender information about the subjects of biographies, in this study, we chose to restrict ourselves to a gender binary (i.e., man and woman). This choice was dictated by the limited frequency of other, non-binary gender labels in the dataset (0.09%), which would have severely limited our ability to draw reliable statistical inferences about them. Thus, it is important to acknowledge that our analysis is restricted in its scope, as it does not encompass all existing notions of gender. The rest of the article proceeds as follows. First, to define the scope of the problem, we review how prior work has documented the gender gap on Wikipedia and its community. We then provide a brief description of how the Wikipedia deletion process works and discuss prior literature on gender and deletion discussions. From this discussion we derive the hypotheses that guide our main research question, and highlight how a host of contingent factors, like the historical evolution of Wikipedia as a mass collaboration project, are relevant when measuring the gender disparity in deletion discussions. Since Wikipedia is a repository of both historical and contemporary knowledge, we also considered two more factors: the additional considerations that Wikipedians typically take into account when debating about the merits of living people to be featured on Wikipedia, and the challenges in finding information about historical figures. In the Discussion section, we go back to the motivating research question, consider the main limitations of our analyses, and point to ways in which future work can build on our approach."
https://arxiv.org/html/2411.04263v1,Object Recognition in Human Computer Interaction:- A Comparative Analysis,"Human-computer interaction (HCI) has been a widely researched area for many years, with continuous advancements in technology leading to the development of new techniques that change the way we interact with computers. With the recent advent of powerful computers, we recognize human actions and interact accordingly, thus revolutionizing the way we interact with computers. The purpose of this paper is to provide a comparative analysis of various algorithms used for recognizing user faces and gestures in the context of computer vision and HCI. This study aims to explore and evaluate the performance of different algorithms in terms of accuracy, robustness, and efficiency. This study aims to provide a comprehensive analysis of algorithms for face and gesture recognition in the context of computer vision and HCI, with the goal of improving the design and development of interactive systems that are more intuitive, efficient, and user-friendly.","The field of human-computer interaction (HCI) has undergone significant transformation in recent years with advancements in computing power, computer vision, and machine learning techniques. The demand for more interactive and natural user interfaces has led to the development of systems that can recognize human actions and gestures [1], [2], [3]. These smart-systems enable users to interact with computers more effectively and intuitively, making HCI a topic of extensive research in academia and industry. One critical aspect of HCI is the recognition of users’ faces and gestures. Recognizing faces and gestures accurately and in real-time is an essential component of many applications, including gaming, security, and human-robot interaction. Several approaches have been proposed in the literature for face and gesture recognition, including traditional computer vision-based methods [4], [5], and more recent deep learning-based techniques [6]. Face recognition verifies identity via facial features, with practical applications in security, biometrics, and HCI. It has been an area of extensive research for decades, with a wide range of practical applications, including security and surveillance systems, biometric identification, and access control. Similar to face recognition, gesture recognition is an important area of research in human-computer interaction, allowing machines to recognize and understand human actions and gestures. It has numerous practical applications, including gaming, virtual reality, and human-robot interaction. In recent years, there has been significant progress in the development of face and gesture recognition techniques, driven largely by advances in machine learning and computer vision algorithms. These techniques include traditional computer vision-based methods and more recent deep learning-based approaches. In this paper, we will look at them in detail."
https://arxiv.org/html/2411.04994v1,"Public Procurement for Responsible AI?Understanding U.S. Cities’ Practices, Challenges, and Needs","Most AI tools adopted by governments are not developed internally, but instead are acquired from third-party vendors in a process called public procurement. While scholars and regulatory proposals have recently turned towards procurement as a site of intervention to encourage responsible AI governance practices, little is known about the practices and needs of city employees in charge of AI procurement. In this paper, we present findings from semi-structured interviews with 18 city employees across 7 US cities. We find that AI acquired by cities often does not go through a conventional public procurement process, posing challenges to oversight and governance. We identify five key types of challenges to leveraging procurement for responsible AI that city employees face when interacting with colleagues, AI vendors, and members of the public. We conclude by discussing recommendations and implications for governments, researchers, and policymakers.","Artificial intelligence is increasingly utilized in the public sector to automate bureaucratic process and workflows, and assist critical decision-making processes that impact residents (Reisman et al., 2018; Kim et al., 2024; Chouldechova et al., 2018; Eubanks, 2018; Whitney et al., 2021; Kawakami et al., 2024a; Levy et al., 2021). Often, such public-sector AI applications are not developed in-house, but are purchased from external third-party vendors through a process called “public procurement” (Sloane et al., 2021; Prier and McCue, 2009; Lloyd and McCue, 2004). In fact, in a 2023 opening statement for the full committee hearing on AI and procurement, U.S. Senator Gary Peters stated that “over half of the AI tools used by federal agencies have been purchased from commercial vendors” (Peters, 2023). Experts estimate that this number is even higher at lower levels of government, such as state and local governments that are even less likely to have internal expertise to develop AI (Rubenstein, 2020, 2021; Mulligan and Bamberger, 2019). Thus, most public-sector AI systems used today are developed by and acquired from private vendors. A growing number of academic and advocacy efforts have pointed out how AI systems procured in the public sector have predominantly targeted narrowly defined notions of efficiency and performance enhancements, resulting in adverse effects that disparately impact marginalized communities (Eubanks, 2018; Roberts, 2022; Stapleton et al., 2022; Harcourt, 2006; in Pittsburgh, 2020; Bhuiyan, 2021). While such incidents have exposed flaws in individual AI systems, they highlight deeper issues in how AI is acquired, used, and governed in the public sector. The AI procurement process encompasses decisions of which AI tools to ask for, adopt or reject, and the manner in which they are developed and deployed: decisions of critical importance for communities who may be harmed by AI. Such decisions not only influence the performance and risks posed by AI systems, but also play a significant role in shaping broader governance practices and ethical standards by which AI operates in the public sector. Interestingly, there is a long history of governments adapting their public procurement practices to enact social change, e.g., by creating processes that prioritize minority-owned businesses (McCrudden, 2007), enable public oversight over government surveillance (Young et al., 2019), or incentivize other ethical behaviors, such as sustainability (Lăzăroiu et al., 2020; Varga, 2021). Drawing from this tradition, several scholars point out how existing public procurement processes such as competitive solicitations (e.g., RFPs), vendor selection practices, and contract negotiations pose several opportunities to encourage more responsible practices surrounding the adoption and use of AI. A shared recognition of AI procurement as a gateway to more responsible adoption and use of the technology in public sector has sparked a recent explosion of attention and action taken by governments, academics, and think-tanks to develop AI procurement guidelines and resources to be used by government employees (Shapanka et al., 2024; Richardson, 2021; Hasselbach et al., 2020; Project, 2024; of Canada, 2023). While a handful of these resources have been empirically investigated (Richardson et al., 2019), we still lack a broader understanding of if, and how, such resources align with governments’ actual contexts and needs. Our Contributions. In a time when AI procurement has become a pressing matter of policy attention, we believe that empirical research—to understand the challenges government employees face when attempting to incorporate responsible AI considerations into their procurement practices—can help inform policy development and implementation. To date, there is a dearth of empirical research focused on understanding governments’ AI procurement practices. To address this gap, this work builds on the burgeoning efforts across the United States to assist governments in procuring AI and investigates how city employees are approaching the procurement of AI systems. In this paper, we present findings from semi-structured interviews with 18 city employees across 7 cities who are responsible for AI procurement in their city."
https://arxiv.org/html/2411.04890v1,GUI Agents with Foundation Models:A Comprehensive Survey,"Recent advances in foundation models, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), facilitate intelligent agents being capable of performing complex tasks. By leveraging the ability of (M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents can autonomously execute user instructions by simulating human-like interactions such as clicking and typing. This survey consolidates recent research on (M)LLM-based GUI agents, highlighting key innovations in data, frameworks, and applications. We begin by discussing representative datasets and benchmarks. Next, we summarize a unified framework that captures the essential components used in prior research, accompanied by a taxonomy. Additionally, we explore commercial applications of (M)LLM-based GUI agents. Drawing from existing work, we identify several key challenges and propose future research directions. We hope this paper will inspire further developments in the field of (M)LLM-based GUI agents.","Graphical User Interfaces (GUIs) serve as the primary interaction points between humans and digital devices. People interact with GUIs daily on mobile phones and websites, and a well-designed GUI agent can significantly enhance the user experience. Consequently, research on GUI agents has been extensive. However, traditional rule-based and reinforcement learning-based methods struggle with tasks that require human-like interactions Gur et al. (2018); Liu et al. (2018), limiting their practical application. Figure 1: Illustration of the growth trend in the field of GUI Agent with Foundation Models. In recent years, advancements in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have elevated their abilities in language understanding and cognitive processing to unprecedented levels OpenAI et al. (2024); Touvron et al. (2023); Yang et al. (2024). With improved natural language comprehension and enhanced reasoning capabilities, (M)LLM-based agents can effectively interpret and utilize human language, develop detailed plans, and execute complex tasks. These breakthroughs offer new opportunities for AI researchers to tackle challenges that were once deemed highly difficult, such as automating tasks within GUIs. As a result, numerous studies have been published focusing on (M)LLM-based GUI agents, as shown in Figure 1, especially over the last two years. However, few efforts have been made to comprehensively summarize and compare the research in this emerging field of GUI agents. A systematic review is urgently needed to provide a holistic understanding and inspire future developments. This paper presents a comprehensive survey of GUI agents with foundation models. We organize the survey around three key areas: data, framework, and application. First, we investigate the available datasets and benchmarks for GUI agents, and list them as a resource for researchers in this area. Second, we review the recent works on (M)LLM-based GUI agents, which are classified by their input modalities and learning modes. Finally, we summarize the latest industrial applications with (M)LLM-based GUI agents, which hold significant commercial potential."
https://arxiv.org/html/2411.04578v1,Multi-Agents are Social Groups: Investigating Social Influence of Multiple Agents in Human-Agent Interactions,"Multi-agent systems - systems with multiple independent AI agents working together to achieve a common goal - are becoming increasingly prevalent in daily life. Drawing inspiration from the phenomenon of human group social influence, we investigate whether a group of AI agents can create social pressure on users to agree with them, potentially changing their stance on a topic. We conducted a study in which participants discussed social issues with either a single or multiple AI agents, and where the agents either agreed or disagreed with the user’s stance on the topic. We found that conversing with multiple agents (holding conversation content constant) increased the social pressure felt by participants, and caused a greater shift in opinion towards the agents’ stances on each topic. Our study shows the potential advantages of multi-agent systems over single-agent platforms in causing opinion change. We discuss design implications for possible multi-agent systems that promote social good, as well as the potential for malicious actors to use these systems to manipulate public opinion.","AI agents are advanced systems capable of interacting with users, holding conversations, and expressing their own thoughts (Wang et al., 2024b). These agents leverage the conversational and reasoning capabilities of large language models (LLMs) like Gemini, GPT-4, and LLaMA, and have become commonplace in daily life (Reuters, 2024). Significant research has also explored using these agents across a broad range of scenarios including education (Lieb and Goel, 2024), search interfaces (Ma et al., 2024), and trading platforms (Yu et al., 2024). The prevalence and ubiquity of LLM-powered agents has given rise to the concept of multi-agent systems - systems with two or more independent agents working together to fulfill a common goal and improve task performance (Guo et al., 2024; Chen et al., 2023b; Clarke et al., 2022). Major AI companies such as Anthropic (Claude, 2024) and Google (Google, 2024) have recently introduced multi-agent frameworks, making it easier for developers to create such interfaces. With both the rise of multi-agent systems and the increasing prevalence of virtual agents in our daily lives, an interesting question emerges: Can a group of AI agents collectively exert social influence on users? This arises from a well-documented observation in human-human interaction: individuals are more likely to be influenced by the opinions of larger groups of people, a phenomenon known as social influence. Various forms of social influence, including conformance to the majority (Cialdini and Goldstein, 2004), peer pressure (Shepherd et al., 2011; Hui and Buchegger, 2009), desire for social acceptance (Richters and Waters, 1991; Oostveen et al., 1996), and adherence to social norms (McDonald and Crandall, 2015; Goldstein and Cialdini, 2011), are pervasive in everyday life. It is well-known that human-computer interaction dynamics tend to mimic human-human dynamics (Nass et al., 1994), with humans perceiving and reacting to computer systems similar to how they would behave towards other humans. This leads to the possibility that a group of AI agents holding the same opinion may exert social influence on users, similar to how a group of humans would. The importance of investigating this issue is two-fold. Firstly, multi-agent interactions have become increasingly common. Most notably, on social media platforms like Twitter, human-like social bots have been effectively used to disseminate information and increase user engagement (Wischnewski et al., 2024). Multi-agent systems have also been designed to support online discussions (Zhang et al., 2024), public information elicitation (Jiang et al., 2023), and health coaching (Beinema et al., 2021), exposing users to opinions from multiple agents in a range of settings. Secondly, the potential social influence of multi-agent systems on users raises potential concerns. Evidence from psychology studies suggests that interactions with groups sharing similar views can significantly impact user opinions (Myers and Lamm, 1976; Isenberg, 1986), potentially leading to opinion polarization and manipulation. As digital agents increasingly function as social actors, the influence they exert on users is a critical yet underexplored issue. Beyond potential negative effects, the social influence of multi-agent systems can also be harnessed in a positive way. In the traditional human-human domain, majority opinion has long been used to encourage changes in attitudes and behaviors, such as in health interventions (Skalski and Tamborini, 2007; Zhang et al., 2015) and sustainability efforts (Athanasiadis and Mitkas, 2005; Vossen et al., 2009). In human-computer interaction, persuasive systems have been designed across numerous domains like health and wellness (Balloccu et al., 2021; Oyebode et al., 2021), education (Ahtinen and Kaipainen, 2020; Tian et al., 2021), and social welfare (Shi et al., 2020), aiming to improve user behavior or change their attitudes through persuasion and informational education. Yet, these implementations are limited to single-agent interactions. The potential of multi-agent systems that collectively effect changes in user behavior or opinion remains unexplored. The potential of AI agents in enacting persuasive effects is further supported by research in the HCI domain, which has highlighted the significant social influence individual AI agents exert, shaping users’ attitudes (Jakesch et al., 2023) and behaviors (Shi et al., 2020; Zhang et al., 2020). However, such research has focused on the design characteristics of individual agents: for instance, on how the perceived identity of an agent as a chatbot or a human impacts outcomes (Shi et al., 2020). Little attention has been given to how the number of agents affects the effectiveness of these systems. Existing research on multi-agent system has largely revolved around enhancing task performance (Guo et al., 2024; Chen et al., 2023b), either by helping users complete tasks more effectively or improving the technical performance of the system itself (Du et al., 2023; Chen et al., 2023a). In contrast, the indirect effects of social influence caused by these multi-agent systems has been largely overlooked. The impact of social influence is less relevant in these systems, where the primary goal is often to improve the clarity of information conveyed to the user. In scenarios where the system needs to both convey information and persuade a user of a certain viewpoint, the effect of multiple vs. single agents is critically underexplored. Therefore, given 1) the increasing prevalence of multi-agent systems and interactions with multiple social bots online, 2) the known social influence that AI agents can exert, and 3) the established impacts of group social influence among humans that may be mirrored in human-AI interactions, there is 4) significant potential for both positive and negative effects caused by the collective social influence created by a group of AI agents. In this study, we address this issue by asking: Can interacting with multiple virtual agents create social influence that shifts our opinions? Specifically, we explore whether interactions with multiple agents can induce greater levels of social influence and lead to stronger opinion changes, and identify demographics factors that increase susceptibility to AI influence, and aim to form guidelines for the effective and ethical implementation of multi-agent systems. To investigate these questions, we conducted an experiment (n = 94) where participants discussed their stances on two social topics with one, three, or five agents. Each participant was randomly assigned to a condition, with discussion content kept consistent across groups, differing only in the number of agents presenting it. Agents presented arguments that aligned with the participant’s stance on one topic and opposed it on the other. Survey data and open-ended responses were collected before and after each conversation round. Through quantitative and qualitative analysis, we found that the multi-agent setup significantly influenced opinion shifts. Participants showed stronger opinion changes toward the agents when they disagreed and greater polarization when agents agreed with them. However, the shift did not intensify as the number of agents increased from 3 to 5. Instead, the 5-agent group led to a sense of polarization, with more participants rejecting the agents’ arguments and moving further from the agents’ positions. Additionally, multi-agent groups heightened perceptions of social influence, particularly normative influence to align with the majority. Open-ended responses indicated that participants expressed a desire for affiliation within the group, which contributed to social influence and opinion shifts in the 3- and 5-agent conditions. Demographic analysis further revealed that younger participants were more susceptible to multi-agent influence. Our study makes the following contributions: • We show that having multiple agents in a discussion is more likely to create social influence and drive opinion change. This addresses a gap in understanding how multi-agent systems influence people, revealing their potential to drive stronger shifts in attitudes compared to single-agent setups. • We provide a foundational understanding of the differences in social influence between single-agent and multi-agent systems. By observing participants’ feelings of social exclusion and their desire to engage, we identify an implicit sense of affiliation with a group of agents, expanding existing social influence theories to include human-agent interactions. • We provide design suggestions for using multi-agent setups to more effectively convey persuasive strategies. By leveraging the normative influence exerted by multiple agents, future systems could foster emotional engagement in participants, promoting more lasting and internalized changes in attitudes and behaviors."
https://arxiv.org/html/2411.04273v1,Understanding Generative AI in Robot Logic Parametrization,"Leveraging generative AI (e.g., Large Language Models) for language understanding within robotics opens up possibilities for LLM-driven robot end-user development (EUD). Despite the numerous design opportunities it provides, little is understood about how this technology can be utilized when constructing robot program logic. In this paper, we outline the background in capturing natural language end-user intent and summarize previous use cases of LLMs within EUD. Taking the context of filmmaking as an example, we explore how a cinematography practitioner’s intent to film a certain scene (1) can be articulated using natural language, (2) can be captured by an LLM, and (3) further be parametrized as low-level robot arm movement using an LLM. We explore the capabilities of an LLM interpreting end-user intent and mapping natural language to pre-defined, cross-modal data in the process of iterative program development. We conclude by suggesting future opportunities for domain exploration beyond cinematography to support language-driven robotic camera navigation.","Within robot program design, end users have a crucial role in specifying robot behaviors that match unique needs within a domain (Barricelli et al., 2019; Coronado et al., 2019). Although end users may have ideas on what they want to program within a domain, they may not know how to program the robot (Chung and Cakmak, 2020; Racca et al., 2020). Here, we motivate this problem by introducing a real-life scenario in which a domain expert uses a robot to perform a specific task. Imagine a filmmaker using a robotic camera to capture a specific scene. The robotic camera could be used as a cobot, or a collaborative robot (Colgate et al., 1996), to aid an expert in capturing scenes through teleoperation (Praveena et al., 2023). Cinematography practitioners may have ideas and intuition on how the scene should be filmed. For example, in a thriller movie, if the expert were to capture suspense from a scene, they could have recommendations on what types of shots to use (e.g., extreme close-up (Heiderich, 2024)) to add suspense to the scene. The expert would then express their intent of filming the scene using specific techniques. However, the question remains: how would they convey their intent if they are not familiar with robot operation methods? Given the aforementioned example, we identify two problems associated with expert end-users giving instructions to a robot. First, end users may not be equipped with the specific semantic rules associated with robot operation. Put differently, end users may not know what a well-informed intent (Liu et al., 2023) should look like, making it challenging for users to formulate their intent for effective communication. For example, experts may understand how to frame and control the camera movement to match their cinematic vision; however, they may not be as well-trained to verbalize these into commands. Experts may face difficulties in going through trial-and-error, framing their intent in different formats of statements such as controlling verbosity level and repeating certain phrases or keywords to emphasize their intent. Second, end users’ well-informed intent may not suffice to generate complete, low-level behaviors of a robot (Liu et al., 2023). As with all robotics, factors such as how the robotic parameters (e.g., what signals are sent to the servo motors) should change are essential to the movement of robots. However, understanding and adopting the syntactic-level operation rules of a robot is a challenging task for the user (Chung and Cakmak, 2020), considering that learning how to program a robot requires much training/experience even for experienced programmers (Racca et al., 2020; Ritschel et al., 2023). While experts can familiarize themselves with how to “talk” to the robot, this does not mean they can directly write programs to operate the robot. Thus, it is necessary to explore how a system can infer how these robotic parameters should change to match the expert’s vision. To address the first problem, we see potential in LLMS as they afford an intuitive way for users to utilize natural language in expressing intent. Although user utterances provided (i.e., prompts) for the LLM may not be used verbatim in capturing user intent and implementing programs, this way of communication through natural language allows users to express their programming intent intuitively. To address the second problem, we plan to fetch and translate user intent from natural language into parameters that may be used to define a preliminary set of motions for the robotic camera. LLMs could be used to refine user intent to generate robot-understandable, well-formed intent by generalizing user utterances and facilitating the modification of low-level behavior parameters by suggesting feasible changes and ranges for low-level behavior parameters using a probabilistic approach. Our work is heavily inspired by Praveena et al.’s work (Praveena et al., 2023) that explored the potential use of robotic cameras in the cinematography domain. Within this domain, we (1) explore how LLMs can be utilized as a robot EUD tool in capturing and parametrizing user intent, and (2) how the LLM-driven technique can work towards closing the abstraction gap (Liu and Zhang, 2019; Sarkar et al., 2022; Liu et al., 2023) (i.e., the gap between user-formulated intent and the solution space that is specific to the system). Figure 2. (Left) Formative study: We plan to conduct a formative study where a cinematography expert (denoted as the director) uses natural language and movement to express filming intent. The experimenter will act as the camera operator, closely following the expert’s instruction and moving the camera. (Middle) Input for Module: Different types of input modality will be used to construct separate robot program modules, namely the action module and the intent module. (Right) LLM Interaction: We envision the interaction scenario with the robotic arm mediated by LLM, where it interprets keywords within the end-user prompt and maps those keywords to pre-defined modules and respective parameter values. A robotic camera can initiate a movement given those parameter values."
https://arxiv.org/html/2411.03827v1,DesignMinds: Enhancing Video-Based Design Ideation with Vision-Language Model and Context-Injected Large Language Model,"Ideation is a critical component of video-based design (VBD), where videos serve as the primary medium for design exploration and inspiration. The emergence of generative AI offers considerable potential to enhance this process by streamlining video analysis and facilitating idea generation. In this paper, we present DesignMinds, a prototype that integrates a state-of-the-art Vision-Language Model (VLM) with a context-enhanced Large Language Model (LLM) to support ideation in VBD. To evaluate DesignMinds, we conducted a between-subject study with 35 design practitioners, comparing its performance to a baseline condition. Our results demonstrate that DesignMinds significantly enhances the flexibility and originality of ideation, while also increasing task engagement. Importantly, the introduction of this technology did not negatively impact user experience, technology acceptance, or usability.","Idea generation is the cornerstone of innovation and serves as the foundation for new designs (López-Mesa et al., 2011; Chulvi et al., 2012). Video-Based Design (VBD) enables designers to utilize video content as a key tool for generating knowledge, inspiring new ideas, and identifying potential challenges (Ylirisku and Buur, 2007b; Tatar, 1989; Vertelney, 1989; Fucs et al., 2020). The ideation of VBD plays a crucial role in brainstorming to produce a wide range of ideas, which are then filtered and refined to develop optimal solutions (Mackay and Fayard, 1999; Mackay et al., 2000; Dam and Siang, 2017). However, generating novel design ideas from videos is challenging for a large group of practitioners. It requires not only a significant investment of time and effort but also extensive design experience to generate a substantial number of related ideas for practice (Dam and Siang, 2017). Consolidating design problems and generating feasible solutions from videos using traditional VBD methods typically requires extensive video review and the application of professional divergent thinking (Ylirisku and Buur, 2007b). This process is often labor-intensive and heavily dependent on the practitioner’s design experience and knowledge, which can be particularly challenging for novice designers with limited expertise and resources (Zimmerman, 2005). Additionally, previous research indicated that advanced video tools can potentially enhance the design work with videos to improve the quality outcomes, and to facilitate interactions (Zahn et al., 2010). With the recent surge in Generative AI (GenAI), technologies such as the Large Language Model (LLM) GPT-4 (OpenAI, 2023) demonstrate significant potential to enhance creative tasks across various design domains. A base LLM model can generate ideas across diverse scopes. Its capabilities can be further refined by incorporating contextual material through a process known as Retrieval-Augmented Generation (RAG) to make it adaptable in current circumstances (Lewis et al., 2020). Additionally, Vision-Language Models (VLMs) possess the ability to interpret videos with high detail, reducing the need for extensive human effort (Bordes et al., 5 27). These advancements have the potential to assist designers in overcoming challenges associated with generating efficient and effective ideas, particularly when faced with prolonged video viewing and limited design experience (Rezwana and Maher, 2023; Kim et al., 2021). As such, this paper explores an approach that combines a customized VLM and LLM (DesignMinds) to enhance the ”watch-summarize-ideate” process in VBD tasks through designer-AI co-ideation. We then present our benchmarks and evaluate the quality of the generated ideas, cognitive processes, user experience (UX) and technology acceptance and use from VBD ideation. Our work makes the following contributions: • We introduce a novel GenAI-powered chatbot that features video understanding and design-context-based idea recommendations to enhance the ideation capabilities of new VBD practitioners. • We investigate the impact of our prototype in terms of ideation quality, cognitive processing during ideation, and subsequent UX and technology acceptance. • Ultimately, we propose a potential tool (DesignMinds) involving the use of a customized VLM and LLM to scale up the VBD ideation process for new designers. Finally, our findings indicate that DesignMinds improves the flexibility and originality of design ideas and boost design task engagement. The adoption of this technology also did not adversely affect the established patterns of UX, technology acceptance and usability."
https://arxiv.org/html/2411.03709v1,AutoGameUI: Constructing High-Fidelity Game UIs via Multimodal Learning and Interactive Web-Based Tool,"We introduce an innovative system, AutoGameUI, for efficiently constructing cohesive user interfaces in game development. Our system is the first to address the coherence issue arising from integrating inconsistent UI and UX designs, typically leading to mismatches and inefficiencies. We propose a two-stage multimodal learning pipeline to obtain comprehensive representations of both UI and UX designs, and to establish their correspondences. Through the correspondences, a cohesive user interface is automatically constructed from pairwise designs. To achieve high-fidelity effects, we introduce a universal data protocol for precise design descriptions and cross-platform applications. We also develop an interactive web-based tool for game developers to facilitate the use of our system. We create a game UI dataset from actual game projects and combine it with a public dataset for training and evaluation. Our experimental results demonstrate the effectiveness of our system in maintaining coherence between the constructed interfaces and the original designs.","With the widespread adoption of personal computers and smart mobile devices, the gaming industry has generated considerable economic value and cultural influence around the world. However, as a critical step in game development, game UI development still faces various challenges that need to be addressed. Compared to the numerous literature (Deka et al., 2017; Wu et al., 2021; Li et al., 2022; Warner et al., 2023; Jiang et al., 2024) in the computational UI domain for mobile apps and web pages, few academic research and industrial applications discovered and truly resolved the issues encountered in game UI development until now. In conventional game UI development, two specialized teams collaborate in parallel to create a cohesive game user interface (GameUI) that can be played in the game engine. The first team, made up of UI designers or artists, focuses on creating appealing and expressive visual elements to achieve diverse game styles. The second team, composed of UX designers or game engineers, pays more attention to emphasizing usability and functionality by designing effective logical structures. As illustrated in Fig .1, the distinct focus areas naturally lead to conflicting design elements, varying style patterns, and inconsistent layouts, making it difficult to construct the GameUI that requires a seamless integration of the aesthetic UI design and the functional UX design. Game project teams often employ a great deal of manpower to manually bridge the gap and construct the GameUI in the game engine, while the final quality cannot be guaranteed owing to the lack of industry-wide construction standards. Moreover, both UI and UX designs need to be frequently updated according to the latest game design script, which means that the previously completed GameUI must be revised. Regardless of developers’ experience, manually handling such cases is extremely time-consuming and usually causes delays in the development timeline. Figure 1. Example of the coherence issue in conventional game UI development. \Description Figure 1 illustrates the inefficient manual construction in conventional game user interface development. There are three participants: a UI designer, a UX designer, and a downstream game developer. The UI designer focuses on crafting visually appealing and expressive elements for diverse game styles, while the UX designer focuses on crafting effective logical structures. The distinct focus areas of these teams naturally lead to various mismatches, making it challenging for downstream developers to create cohesive user interfaces. Figure 2. System framework. \Description Figure 2 illustrates the overall framework of the AutoGameUI system, which consists of three components: automatic correspondence matching, universal data protocol, and an interactive web-based tool. Initially, UI and UX designers utilize individual plug-ins to export a Photoshop file and a Unity prefab as universal protocols. These universal protocols are then input into an interactive web-based tool. The tool operates on a client-server architecture, remotely executing the automatic correspondence matching algorithm and displaying the matching results on the user’s desktop PC. Following several interactive processes, the updated protocols are re-imported into the Unity engine, ultimately constructing the cohesive game user interface. To alleviate the bottleneck posed above, this paper introduces a novel automated system for GameUI construction, which is named as AutoGameUI. As depicted in Fig .2, our system consists of three components: automatic correspondence matching, universal data protocol, and interactive web-based tool. The automatic correspondence matching is composed of two multimodal models and a flexible integer programming algorithm. The multimodal models are based on the transformer model (Vaswani, 2017) and are learned in two separate stages, with the first stage focusing on representing UI and UX designs in a latent space, and the second stage aiming to estimate the matching probability between them. Previous studies (Li et al., 2017; Yamaguchi, 2021; Bai et al., 2023; Jiang et al., 2024) typically examined the representation of graphical UIs from a limited number of perspectives and failed to integrate more comprehensive attributes. In contrast, our method inclusively considers various aspects such as spatial layout, functional semantics, textual content, hierarchy, and even rendering order, except image texture, which is useless in our task. Besides, our models adopt relative positional encoding to accommodate graphical UIs with varying lengths, which is different from some relevant attempts (Xu et al., 2020a, b; Huang et al., 2022; Gupta et al., 2021; Inoue et al., 2023). These works employed absolute positional encoding in the transformer model which can struggle to adjust the performance of original training data in extended test data. Furthermore, some works (Patil et al., 2020; Xu et al., 2022; Wu et al., 2023; Otani et al., 2024) tried to estimate the correspondences between pairs of graphical UIs, relying on one-to-one element similarity or Hungarian algorithm (Kuhn, 1955) for bipartite matching. However, there exist complex correspondence rules present in real-world UI and UX designs, including but not limited to one-to-one, and many-to-one mapping, which conflicts with the existing methods. In our method, we first identify and summarize all correspondence rules between pairwise UI and UX designs. We then exploit grouped cross-attention modules to calculate one-to-group similarity with reduced dimensions. By integrating the similarity with the hierarchical and rendering constraints, we formulate an optimization problem that is effectively solved using integer programming to achieve optimal performance. Following prior works (Baciková et al., 2013; Ribeiro and da Silva, 2014; Panayiotou et al., 2024), we design a universal protocol to describe the UI and UX designs, enabling the data stream across different platforms. By utilizing the universal data protocol and the optimal correspondences, we can easily achieve high-fidelity effects in the GameUI construction. To streamline the usage of our system and provide users with intuitive feedback and guidance, we develop an interactive web-based tool to give a smooth experience during the construction process. The web-based tool also offers the ability of data annotation, enabling us to collect and build a specialized dataset from real-world game UIs, which aids in training and evaluating our approach. In summary, this paper makes the following contributions: • We present a complete system named AutoGameUI, which innovatively utilizes artificial intelligence techniques for high-quality GameUI construction. This system breaks through the bottleneck limited by traditional manual construction and significantly accelerates the game UI development workflow. • We propose an efficient multimodal learning pipeline that acquires the comprehensive representations of UI and UX designs and estimates their optimal correspondences. To enhance efficiency and accuracy, we incorporate grouped cross-attention modules with hierarchical and rendering constraints into the estimation process. • We introduce a universal data protocol and an accompanying interactive web-based tool for our system. This not only supports cross-platform applications but also offers interactive features to assist users in rapid construction, thereby improving the overall user experience. • We build a game UI dataset with annotated UI and UX designs for model training and evaluation."
https://arxiv.org/html/2411.03708v1,Evaluating Eye Tracking Signal Quality with Real-time Gaze Interaction Simulation,"We present a real-time gaze-based interaction simulation methodology using an offline dataset to evaluate the eye-tracking signal quality. This study employs three fundamental eye-movement classification algorithms to identify physiological fixations from the eye-tracking data. We introduce the Rank-1 fixation selection approach to identify the most stable fixation period nearest to a target, referred to as the trigger-event. Our evaluation explores how varying constraints impact the definition of trigger-events and evaluates the eye-tracking signal quality of defined trigger-events. Results show that while the dispersion threshold-based algorithm identifies trigger-events more accurately, the Kalman filter-based classification algorithm performs better in eye-tracking signal quality, as demonstrated through a user-centric quality assessment using user- and error-percentile tiers. Despite median user-level performance showing minor differences across algorithms, significant variability in signal quality across participants highlights the importance of algorithm selection to ensure system reliability.","1 INTRODUCTION Recent innovations in consumer electronics, such as Meta’s Orion [1] and Apple’s Vision Pro [2], indicate a shift towards the mainstream adoption of eye-tracking in consumer electronics. These devices use eye-tracking to enable intuitive gaze-based controls, optimize rendering with foveated techniques, and create more immersive, efficient experiences. For instance, Apple introduced eye-tracking as an accessibility feature in iOS 18 [3], enabling users to interact with their devices through gaze, i.e., gaze-based interaction, thus making gaze interaction available to billions of iOS users, something that was never done before. Gaze-based interaction utilizes eye movements to control devices and systems, transforming the user’s gaze into an intuitive input modality [4]. Prior research has highlighted the potential of gaze-based interaction to enhance the efficiency of human-computer interaction (HCI), particularly as a medium for target selection, due to its natural and direct nature [5, 6, 7, 8]. Recent research validates this claim as gaze-based interaction is employed across various devices, including computers [9], smartphones [10], virtual reality (VR) headsets [11, 12], and augmented reality (AR) systems [13, 14]. As an input method, gaze interaction offers both benefits and limitations. Since gaze inherently gathers visual information, gaze location can effectively indicate the user’s focus of attention [15]. One of its primary advantages is its naturalness. Users can easily focus on the target simply by looking at them, faster and less physically demanding, making gaze an intuitive means of interaction [16, 4]. Despite the many benefits of gaze-based interaction, several challenges remain, particularly regarding the accuracy of gaze detection. Accurate gaze detection refers to the system’s ability to pinpoint the exact location on the screen where the user is gazing, such as a specific object or target. The better the accuracy, the more effectively the system can respond to the user’s gaze. Conversely, poor accuracy can lead to incorrect target selection, disrupting the user experience by causing slower, less efficient interactions and increasing user frustration. Furthermore, analyzing eye movements during real-time interactions with dynamic stimuli presents additional challenges such as the noisier nature of the real-time gaze data, and synchronization issues between real-time eye movements and events [17]. To address these challenges, we simulate real-time gaze interaction using the offline GazeBase dataset. This dataset provides standardized eye movement data with high spatial accuracy, collected in a controlled environment. The part of the dataset employed in our study includes both gaze data and relative target data, allowing us to precisely control and align gaze-target interactions, mitigating synchronization issues and enabling more consistent and reliable measurements. This approach ensures consistency, replicability, and valid comparisons across studies eliminating the inherent variability and delays encountered in real-time scenarios. Most importantly, the GazeBase dataset allows us to analyze data from 322 subjects, a scale that is often difficult to achieve with interaction-based datasets, which typically involve a much smaller number of participants. Motivated by these considerations, we develop a novel methodology for simulating real-time gaze data from an offline dataset to simulate real-time gaze interaction. This approach introduces the Rank-1 Fixation Selection method, which interprets fixations as a user’s intent to interact with the system through gaze. We adapted real-time versions of three foundational eye movement classification algorithms to reliably identify physiological fixations from eye-tracking data as these fixations are the core unit around which the interactions are scheduled. Our methodology further evaluates the effectiveness of these simulated interactions. Additionally, we assess the eye-tracking signal quality for interaction using a user-centric quality evaluation, employing user- and error-percentile tiers to evaluate performance in the gaze-based system. Finally, our evaluations incorporate constraints to ensure robustness and applicability across different settings and use cases in gaze-based systems."
https://arxiv.org/html/2411.03575v1,Semantic Navigation for AI-assisted Ideation,"We present a novel AI-based ideation assistant and evaluate it in a user study with a group of innovators. The key contribution of our work is twofold: we propose a method of idea exploration in a constrained domain by means of LLM-supported semantic navigation of problem and solution spaces, and employ novel automated data input filtering to improve generations. We found that semantic exploration is preferred to the traditional prompt-output interactions, measured both in explicit survey rankings, and in terms of innovation assistant engagement, where 2.12.12.12.1x more generations were performed using semantic exploration. We also show that filtering input data with metrics such as relevancy, coherence and human alignment leads to improved generations in the same metrics as well as enhanced quality of experience among innovators.","The process of innovation in an organization usually begins with brainstorming, either individually or in a group. Group brainstorming has been shown to suffer from productivity loss compared to working individually [5] but also has benefit of engaging the organzational memory [17]. One approach to boost overall innovation by enabling individual creativity while benefiting from organization (group) memory is to use an AI assistant as a tool allowing the individual to brainstorm by interacting with a Large Language Model (LLM) [18] without social anxiety [7]. Note that given the nature of the creative process, the ideation input to the LLM tends to be noisy, and poor input gives poor output. A pre-trained general-purpose LLM asked to assist in ideation favors the most widely-circulated public-domain ideas, which hinders exploration and can lead to design fixation [19]. Adjusting the “temperature” parameter of the LLM allows for more surprising or creative output, but outside of a narrow range, yields more irrelevant and incoherent output. Constraining innovation to a particular domain to get more relevant output presents the challenge of re-training or fine-tuning the LLM with potentially proprietary inputs, and repeating the process frequently, as innovations are constantly evolving and based on the latest findings and extensive organizational memory of past innovations. In this work, our main contributions are: (a) describing and providing a method to navigate and explore problem and solution spaces in a proprietary constrained domain during ideation assisted by an LLM (Section 3), (b) using efficient fine tuning of small models (Section 4) with automated filtering of high quality inputs to improve generations (Section 5), and (c) a user study evaluating our ideation assistant with innovators (Section 6). The semantic navigation in our model is achieved by allowing reverse solution-to-problem mappings in addition to the more typical problem-to-solution mapping during ideation. We can traverse back to a problem statement from the generated solution as well as input pre-existing semantically related problem statements to generate additional solutions. This not only helps with exploration but also allows for refining problem statements, a fundamental stage of early ideation. The use of smaller models that are feasible to host locally on affordable GPUs allows us to more (cost and compute) efficiently re-tune with new inputs while keeping proprietary data local. Our automated input filtering method relies on measuring relevancy between prompt and output, as well as coherence of the generated text. Moreover, we apply concepts from the RLHF methodology [13] to train a reward model that can be reapplied to filter new, unseen inputs. We implemented and deployed our semantic navigation model in an ideation assistant accessed through a Slack bot (Section 7). The user study allowed us to evaluate the experience of innovators implicitly by monitoring their usage in logs as well as explicitly in surveys. This allowed users to evaluate the tool as an ideation assistant and integrate the benefits of group brainstorming (organizational memory) into the higher performing individual brainstorming process. The surveys also provided open-ended feedback that we could analyze qualitatively (Section 8)."
https://arxiv.org/html/2411.03477v1,CrowdGenUI: Enhancing LLM-Based UI Widget Generation with a Crowdsourced Preference Library,"Large Language Models (LLMs) have demonstrated remarkable skills across various design domains, including UI generation. However, current LLMs for UI generation tend to offer generic solutions that lack a deep understanding of task context and user preferences in specific scenarios. We present CrowdGenUI, a framework that enhances LLM-driven UI generation with a crowdsourced user preference library. This approach addresses the limitations of existing methods by guiding LLM reasoning with user preferences, enabling the generation of UI widgets that align more closely with user needs and task-specific requirements. Using image editing as a test domain, we built this library from 50 users, capturing 720 user preferences, which include the predictability, efficiency, and explorability of multiple UI widgets. In a user study with 72 additional participants, our framework outperformed standard LLM-generated widgets in meeting user preferences and task requirements. We discuss these findings to inform future opportunities for designing user-centered and customizable UIs by comprehensively analyzing the extendability of the proposed framework and crowdsourced library.","Graphical user interfaces (GUIs) play a fundamental role in how most people operate computers and other computing devices. Carefully designed by expert interface designers, these GUIs aim to provide users with learnable ways to interact with software (Adobe, 2024c, b). Although effective, such interfaces can be complicated and sometimes overwhelming for users. In recent years, the emergence of large language models (LLMs) has led to increased interest in natural language interfaces. These language-based interfaces process human language input, analyze high-level intentions, and turn the interpreted intents into operations, providing a simplified way for users to interact with technology (OpenAI, 2024a; Descript, 2024; Maddigan and Susnjak, 2023). However, while language interfaces are promising, they are typically suited for high-level task execution, such as removing an object from an image or adding a filtering effect to videos. They often lack the precision required when users need to fine-tune specific parameters, such as adjusting the color in an image or customizing detailed settings in a video editing project. To address the complexity of professional GUIs and the inaccuracy of natural language interfaces, researchers have begun to explore the integration of LLMs with UI generation to combine the high-level task understanding enabled by LLMs with the precise, granular control offered by GUIs (Vaithilingam et al., 2024; Cheng et al., 2024). By turning natural language inputs into interactable UI widgets through UI generation, these systems provide an exciting hybrid interaction model that supports both user interaction with widgets for precise control and language as input for high-level task interpretation. Nevertheless, the existing UI generation systems also come with challenges. These LLM-generated UIs tend to be generic and may not fully account for the nuances of domain-specific tasks or the diverse preferences of individual users. Without specialized knowledge of interface design or a deeper understanding of user preferences, these UIs may fail to provide the optimal interaction experience. To overcome these challenges, we propose a novel framework, CrowdGenUI, that enhances LLM-driven UI generation by incorporating a crowdsourced user preference library, as illustrated in Figure 1. This framework leverages the collective insights of users to guide the LLM in generating tailored UI widgets for different user preferences. While the framework is broadly applicable across various domains, we demonstrate its effectiveness in the context of image editing within a Python programming environment. To build the preference library, we collected data from 50 users, with no requirement on UI design or programming experience, capturing their preferences across three key dimensions of interface design, i.e., predictability, efficiency, and explorability. By integrating this library into the LLM’s UI widget generation process, our system generates widgets that are not only more precise but also better aligned with user preferences and task-specific requirements. In our user study with a different set of 72 participants, we found that UI widgets generated based on our proposed framework were more aligned with user preferences and task requirements than those generic widgets generated by the LLM alone. This demonstrates the potential of our framework to improve LLM-based UI generation and suggests that crowdsourced user preference data can play a crucial role in creating user-centered user interfaces. To summarize, this paper makes the following key contributions: • We propose a general framework to enhance LLM-driven UI generation by incorporating a crowdsourced user preference library for task-aware and preference-aligned widget generation. • As a concrete implementation of this framework, we develop a system for the domain of image editing within a Python programming environment. This system includes crowdsourced data from 50 users, capturing user preferences for UI widget configurations in image editing tasks. • We conduct a user study with an additional 72 participants to evaluate the effectiveness of the implemented system. The results of this study are extensively discussed to inform future research on the development of customizable and user-centered UIs."
https://arxiv.org/html/2411.03395v1,Exploring Large Language Modelsfor Specialist-level Oncology Care,"Large language models (LLMs) have shown remarkable progress in encoding clinical knowledge and responding to complex medical queries with appropriate clinical reasoning. However, their applicability in subspecialist or complex medical settings remains underexplored. In this work, we probe the performance of AMIE, a research conversational diagnostic AI system, in the subspecialist domain of breast oncology care without specific fine-tuning to this challenging domain. To perform this evaluation, we curated a set of 50 synthetic breast cancer vignettes representing a range of treatment-naive and treatment-refractory cases and mirroring the key information available to a multidisciplinary tumor board for decision-making (openly released with this work). We developed a detailed clinical rubric for evaluating management plans, including axes such as the quality of case summarization, safety of the proposed care plan, and recommendations for chemotherapy, radiotherapy, surgery and hormonal therapy. To improve performance, we enhanced AMIE with the inference-time ability to perform web search retrieval to gather relevant and up-to-date clinical knowledge and refine its responses with a multi-stage self-critique pipeline. We compare response quality of AMIE with internal medicine trainees, oncology fellows, and general oncology attendings under both automated and specialist clinician evaluations. In our evaluations, AMIE outperformed trainees and fellows demonstrating the potential of the system in this challenging and important domain. We further demonstrate through qualitative examples, how systems such as AMIE might facilitate conversational interactions to assist clinicians in their decision making. However, AMIE’s performance was overall inferior to attending oncologists suggesting that further research is needed prior to consideration of prospective uses.","A significant and growing challenge facing healthcare systems globally is the shortage of specialty medical expertise [WHO2016, aamc_physician_supply_2024, charlton2015challenges]. This challenge is particularly acute in subspecialist fields like breast oncology, where projected staffing shortages are compounded by the need for patients to navigate time-consuming sequences of referrals and investigations [asco2023oncology]. This process exacerbates the already limited availability and accessibility of medical expertise, leading to delays in timely and effective care and ultimately increasing the risk of morbidity and mortality due to disease progression [hanna2020mortality]. Large Language Models (LLMs) such as Google’s Gemini [team2023gemini] and OpenAI’s GPT4 [achiam2023gpt] are notable for their ability to understand, generate, and interact with human language. These foundational models serve as versatile building blocks that can be fine-tuned for specialized domains and applied to previously unseen downstream tasks. LLMs display great potential in encoding general medical knowledge [singhal2022large, singhal2023towards], and have demonstrated expert-level performance in a wide variety of tasks including question-answering for medical licensing-style examinations, open-ended consumer question-answering, visual question-answering, medical text summarization [tu2024towards, saab2024capabilities], differential diagnosis in complex cases [mcduff2023towards], and more. This, in turn, has meant LLMs finding applications in non-specialized healthcare tasks such as knowledge retrieval, summarization and administrative assistance [turner2023epic, van2024adapted, Vimalananda2020-aq]; however, their performance in complex subspecialty settings such as oncology care remains poorly examined [sorin2023large]. This study probes the potential of specialized medical LLMs to capture the diagnostic and therapeutic nuances of decision-making for breast oncology care. Breast cancer is the most prevalent malignancy in women, and artificial intelligence (AI) tools have already demonstrated great promise at earlier stages of healthcare delivery for this disease, most notably in imaging for disease screening [mckinney2020international, laang2023artificial]. Beyond initial diagnostics, breast cancer has well-studied management pathways, representing a promising domain for LLMs to serve as clinical decision support tools, and providing a well-structured but challenging setting in which to test the subspecialist medical knowledge of LLMs [gradishar2024breast]. Oncology cases present significant complexity due to variations in disease presentation, the reliance on multiple radiology imaging modalities, intricate family histories, and the incorporation of molecular and genetic studies. Moreover, treatment strategies, including neoadjuvant and adjuvant therapies [shien2020adjuvant], introduce further complexity [wang2023breast]. We define neoadjuvant therapy as a pre-surgical treatment to downstage tumors, while adjuvant therapy is used in the post-surgical setting to eliminate residual disease, tailored according to molecular and histopathological features [kerr2022adjuvant]. Available adjuvant therapies include broad chemotherapy, radiotherapy, targeted molecular therapies, immune checkpoint inhibitors, and endocrine therapies [carlson2006nccn]. While well-resourced cancer centers have access to a range of experienced breast oncologists with whom a case can be referred or discussed, smaller regional providers can face resource constraints or lack access to the same breadth of expertise [bleicher2016time, losk2016factors, bleicher2019treatment, jaiswal2018delays]. An AI system capable of bridging this gap by democratizing access to niche specialty expertise could be an aid for local providers and aid the quality of initial triage. Such a system might, for example, empower oncologists in under-resourced regional or rural settings to make more informed treatment decisions and confident referrals, ultimately contributing to more equitable cancer care [rizvi2024genomic]. In this study, we investigate the performance of Articulate Medical Intelligence Explorer (AMIE) [tu2024towards], a recent research, conversational diagnostic medical AI system, in this subspecialist domain of breast oncology. Our objective is to understand the performance and limitations of LLMs in the type of reasoning and decision-making undertaken by a breast oncology care team when presented with the same results of investigations and plausible case vignettes. This builds on recent work exploring the potential of AMIE in subspecialty cardiology settings [o2024towards]. Our key contributions are as follows: • Open-source dataset of representative breast cancer scenarios: Collaborating with three breast cancer specialists, we develop and open-source a set of 50 synthetic breast cancer scenarios. These scenarios reflect a realistic array of clinical presentations in our collaborating real-world center, ranging from common phenotypes in treatment-naive patients to metastatic and end-stage disease in treatment-refractory individuals. • Comprehensive evaluation rubric for breast oncology assessments: We design a detailed 19-question rubric to rigorously evaluate the quality of breast oncology assessments, focusing on aspects like management reasoning, safety considerations, and summarization. • Novel inference strategy for enhanced LLM performance in breast oncology: We employ a lightweight inference strategy that combines web search and self-critique to enhance the performance of AMIE, a research conversational diagnostic medical AI system, in the subspecialty of breast oncology. This approach leverages external knowledge and a sequential chain of reasoning to improve AMIE’s assessments on the 50 synthetic cases without requiring task-specific fine-tuning. • Benchmarking LLM performance against human clinicians: Our evaluation reveals that AMIE’s performance surpasses that of internal medicine trainees and early oncology fellows along majority of criteria considered. While AMIE demonstrates promising capabilities, it does not yet achieve the consistent performance level of experienced oncology attending specialists. • Illustrating clinical applications: We present qualitative examples of beneficial revisions to clinician assessments and realistic hypothetical dialogue scenarios to illustrate the potential clinical utility of systems like AMIE in democratizing breast oncology expertise. Figure 1: Overview of study design and results. (a) Study design. Breast Oncologists evaluate responses from AMIE and six clinicians for the 30 treatment-naive and 20 treatment-refractory cases using the rubric in Figures 3 and 4. (b) Proportion of favorable responses for each group. On most evaluation criteria, covering aspects of summarization, safety, and management reasoning, AMIE greatly surpasses the performance of trainees, though it falls short of the oncology attendings. See Figures A.1, A.2, A.3 and A.4 for more detailed breakdowns of each group’s performance on the evaluation criteria."
https://arxiv.org/html/2411.04090v2,A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement,"Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content—an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model’s inherent uncertainty in predicting toxicity and disagreement. The framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.","Content moderation (CM) has been an important pillar in maintaining ethical online interactions. Given the large amount of user-generated text, CM systems often employ moderation algorithms [1] to combat the spread of online toxicity. Over the past decade, much research on toxicity detection in text data has leveraged the use of machine learning (ML) models, which have been shown to suffer from reliability and robustness issues, wrong predictions due to spurious lexical features [2] and biases [3]. Robustness and reliability are cornerstones, especially in sensitive areas such as CM, where subjectivity and context significantly influence the results [1]. As observed in a recent survey on toxicity detection [4], most research contributions reported in this area to date have largely overlooked a key element in decision-making: the estimation of the model’s confidence in its prediction. In the few cases when uncertainty quantification (UQ) has been considered [5], models have shown remarkable improvements in robustness. UQ not only optimizes the accuracy of predictions, but also facilitates human moderation. By identifying the least reliable predictions, UQ techniques allow moderators to focus their attention on cases where the model exhibits the highest uncertainty, thus optimizing the use of time and resources devoted to human review. Although incorporating uncertainty has advanced ML models in different disciplines, such as traffic modelling [6] or medical imaging [7], it is crucial to recognize that tasks like toxic language detection, which are inherently subjective, require a different perspective due to their complexity. Disagreements among annotators during labelling processes often occur in content moderation reflecting this subjectivity, which can lead to bias and, in some cases, to the censorship of minority opinions. The quantification of inter-annotator disagreement is thus a valuable source of information about the complexity inherent in comments, and is directly related to the aleatoric uncertainty present in the large databases whose inputs require moderation. Annotation disagreement quantification, together with the epistemic uncertainty of the modelling process, allow us to identify the cases when the model cannot provide a robust prediction, as well as those when, due to their complexity and subjectivity, the intervention of a human moderator is necessary. Accounting for both cases is critical, as ML models trained with a majority vote disregard the inherent ambiguity of comments. Unfortunately, considering different annotation perspectives alongside the data modelling pipeline has been observed not to scale well in diverse annotation processes [8]. Additionally, as toxic language evolves over time, it is important to endow content moderators with the tools to make final decisions. This manuscript builds upon the above observations. Specifically, we propose a novel modelling framework that unifies the primary task of toxic language classification with the auxiliary task of annotation disagreement quantification through a multitask framework architecture. The proposed approach is augmented with UQ for both tasks, using Inductive Conformal Prediction (CP) for this purpose. This proposal is grounded on a previous study by Fornaciari et al. [9], which observed that unifying annotation disagreement and classification can be beneficial for the performance of the primary task. Finally, it aims to address a need highlighted by Gillespie [10], who stated: “Perhaps, automated tools are best used to identify the bulk of the cases, leaving the less obvious or more controversial identifications to human reviewers, as societies hold together not by reaching perfect consensus, but by keeping their values under constant and legitimate reconsideration”. The proposed framework is evaluated over an experimental setup designed to answer with evidence three research questions (RQ): • RQ1: How does the integration of annotation disagreement prediction as an auxiliary task influence the performance and calibration of toxicity detection as the primary task? Conversely, what is the impact of the primary task on the auxiliary task? • RQ2: In what ways does predicting annotation disagreement affect the quantification and interpretation of uncertainty in the toxicity detection task? How does the integration of the primary task influence uncertainty quantification and interpretation in the auxiliary task? • RQ3: What are the benefits, if any, of incorporating an auxiliary task into the toxicity identification process compared to treating them as separate tasks, in terms of robustness and the need for human moderation or revision? These three research questions aim to analyze the improvements the multitask approach yields compared to simple single-task models, or, in the case of the full framework, compared to performing quantification separately using a composite of single-task models (CoM). The rest of the manuscript is organized as follows: Section 2 provides an overview of content moderation and CP on text data, and highlights our main contributions. Section 3 describes the materials and methods employed in our experiments. In Section 4, we outline the experimental setup used to address the RQs. Section 5 presents the experimental results and includes a discussion of key findings. Finally, Section 6 summarizes the contributions, key insights and limitations, and suggests avenues for future research."
https://arxiv.org/html/2411.04037v2,Taming Toxicity or Fueling It? The Great Ban’s Role in Shifting Toxic User Behavior and Engagement,"In today’s online environments users experience harm and abuse on a daily basis. Therefore, content moderation is crucial to ensure their safety and well-being. However, the effectiveness of many moderation interventions is still uncertain. We evaluate the effectiveness of The Great Ban, one of the largest deplatforming interventions carried out by Reddit that affected almost 2,000 communities. We analyze 53M comments shared by nearly 34K users, providing in-depth results on both the intended and unintended consequences of this ban. We found that 15.6% of the moderated users abandoned the platform while the remaining ones decreased their overall toxicity by 4.1%. Nonetheless, a subset of those users increased their toxicity by 70% after the intervention. In any case, increases in toxicity did not lead to marked increases in activity or engagement, meaning that the most toxic users had overall a limited impact. Our findings bring to light new insights on the effectiveness of deplatforming. Furthermore, they also contribute to informing future content moderation strategies.","Content moderation is essential for online platforms, as it prevents the spread of harmful content and hateful behavior such as the perpetuation of hate speech [1]. It is also crucial to promote fairness and safety among users by enforcing ethical standards and supporting the health of online communities [2]. In fact, platforms use content moderation as a means to enforce their policies [3]. In particular, administrators apply different types of interventions, which can be more or less severe. For example, they can send short warning messages and use informative labels [4], or even take the drastic decision to remove content and/or users [5, 6]. However, despite the increasing reliance on content moderation, there is still little understanding of the effects of most moderation interventions, which threatens their efficacy. Recent studies have demonstrated that while some interventions had heterogeneous [7, 8] or even ineffectual outcomes [9], others led to undesirable consequences [10, 11]. This is why it is crucial to assess the effects of recent moderation interventions as a preliminary step in planning and developing new ones. The most popular and widely used intervention is known as deplatforming, which involves the removal of content, users, or even entire communities [12]. Notorious examples are the ban that Donald Trump received in 2021 from Facebook and X (formerly Twitter) [13] and the deplatforming of three particularly toxic influencers from X [6]. Additionally, X removed accounts involved in coordinated inauthentic behavior [14] and Reddit permanently shut down different communities because of racism, sexism and hatefulness [15, 16]. In June 2020, Reddit itself hosted one of the biggest deplatforming campaigns in the history of social media –The Great Ban– which resulted in around 2,000 subreddits being banned due to ongoing spread of toxicity and hate speech.111https://www.reddit.com/r/announcements/comments/hi3oht/update_to_our_content_policy/ (accessed: 10/15/2024) Among these are popular communities such as r/The_Donald and r/ChapoTrapHouse. Despite its impact on several communities and users within and outside Reddit, its effects are still little-explored. For example, current research on The Great Ban has mainly investigated the changes in the writing style of the users [17] without assessing the ban’s effectiveness. Few studies analyzed the changes in toxicity, and those who did focused only on a small set of subreddits [5]. Moreover, the majority of the existing works on The Great Ban are focused on community-level effects, neglecting the individual user-level responses that are instrumental for understanding how effective the ban was in mitigating problematic behaviors [18, 8]. In a previous work, we provided preliminary results on the effectiveness and unintended consequences of The Great Ban [19]. Here, we extend our previous analysis by adopting a robust causal method based on Difference-in-Differences (DiD) to estimate the effects of the intervention. Furthermore, we address an additional research question investigating the impact that toxic users had after the intervention, as explained in the following. Research focus. We address the outstanding knowledge gaps by conducting a comprehensive quantitative causal analysis of the changes in toxicity among users active in the 15 most popular subreddits involved in The Great Ban. We analyze 53M comments posted by nearly 34k users over a period of 14 months guided by the following research questions. ∙∙\mathbin{\vbox{\hbox{\scalebox{0.75}{$\bullet$}}}}\;∙RQ1: Did The Great Ban effectively reduce toxicity? Studies have shown that some interventions led to an increase rather than a decrease in toxic behavior. Here we assess the effectiveness of The Great Ban in reducing toxicity, as hate and toxic speech were the main reasons behind the ban. ∙∙\mathbin{\vbox{\hbox{\scalebox{0.75}{$\bullet$}}}}\;∙RQ2: Did The Great Ban lead to any unintended side effects for certain users? That is, were there users who became significantly more toxic after the intervention? The evaluation of the outcomes of a moderation intervention has to take into consideration the possible presence of users who grew resentful of the platforms and increased –rather than decreased– their toxicity. Such extreme reactions can arise even amid an overall reduction in toxicity at platform- or community- level, requiring further analyses at user level. In this study, we evaluate and estimate the extent of these reactions to The Great Ban, considering them as potential side effects of the intervention. ∙∙\mathbin{\vbox{\hbox{\scalebox{0.75}{$\bullet$}}}}\;∙RQ3: What were the behavioral dynamics of toxic users, in terms of their activity and generated engagement? To deepen our analysis, we examine further behavioral dimensions of those users who became much more toxic after The Great Ban. We specifically focus on their degree of activity on the platform and the engagement they receive from other users. Conjointly analyzing the dynamics of toxicity and activity is important as it can allow to identify users who are both very active and very toxic. Additionally, investigating the social feedback they receive may reveal the degree to which highly toxic behaviors are tolerated, and possibly even encouraged, by other users [20]. Main findings. Based on the insights gained from answering the previous research questions, our study produces the following main findings: • The Great Ban led 15.6% of the moderated users to abandon the platform, while those who stayed reduced their toxicity by an average of 4.1%. • While the reduction in toxicity was limited, a significant fraction of users became much more toxic. In particular, 5% of users increased their toxicity by over 70% compared to their pre-ban levels. • Resentful users who escalated their toxicity were found across each of the analyzed subreddits. However, their impact was limited probably due to their limited activity and the lack of positive feedback from other users. • Major changes in a dimension of user behavior do not necessarily lead to major changes in other dimensions. For example, the majority of users who drastically increased their toxicity did not experience significant changes in activity or engagement. Our work provides an in-depth analysis of the effects of The Great Ban. It points out the shortcomings by highlighting the complex challenges of moderating different communities. Our findings can guide the development of future moderation interventions aimed at increasing their effectiveness."
https://arxiv.org/html/2411.03740v1,Human-in-the-Loop Feature Selection Using Interpretable Kolmogorov-Arnold Network-based Double Deep Q-Network,"Feature selection is critical for improving the performance and interpretability of machine learning models, particularly in high-dimensional spaces where complex feature interactions can reduce accuracy and increase computational demands. Existing approaches often rely on static feature subsets or manual intervention, limiting adaptability and scalability. However, dynamic, per-instance feature selection methods and model-specific interpretability in reinforcement learning remain underexplored. This study proposes a human-in-the-loop (HITL) feature selection framework integrated into a Double Deep Q-Network (DDQN) using a Kolmogorov-Arnold Network (KAN). Our novel approach leverages simulated human feedback and stochastic distribution-based sampling, specifically Beta, to iteratively refine feature subsets per data instance, improving flexibility in feature selection. The KAN-DDQN achieved notable test accuracies of 93% on MNIST and 83% on FashionMNIST, outperforming conventional MLP-DDQN models by up to 9%. The KAN-based model provided high interpretability via symbolic representation while using 4 times fewer neurons in the hidden layer than MLPs did. Comparatively, the models without feature selection achieved test accuracies of only 58% on MNIST and 64% on FashionMNIST, highlighting significant gains with our framework. Pruning and visualization further enhanced model transparency by elucidating decision pathways. These findings present a scalable, interpretable solution for feature selection that is suitable for applications requiring real-time, adaptive decision-making with minimal human oversight.","Feature selection is essential for building efficient, accurate, and robust machine learning models [1, 2]. While models ideally should automatically identify the most predictive features, a high-dimensional input space can significantly hinder performance, often requiring large volumes of data to learn from the complex relationships between features effectively. This phenomenon, known as the “curse of dimensionality,” increases computation time and resource use. Consequently, selecting a smaller subset of relevant features improves performance and makes the model more cost-effective. One common solution is to incorporate expert knowledge to determine the most useful features; however, this process is costly, time-consuming, and highly manual. Additionally, experts with deep domain knowledge are often not involved in the actual design and development of the model. Automatic feature selection methods offer an alternative, ranking features by their relevance or importance; however, both manual and automatic approaches typically yield a single subset of features for the entire dataset, which may not capture variability across individual observations. When training data are sparse relative to the feature space, the use of a single subset can limit the model’s ability to generalize effectively across all instances [3]. To address the challenge of per-example feature selection, we propose a reinforcement learning (RL) framework that leverages simulated feedback to replicate human feature selection. Our approach employs a Double Deep Q-Network (DDQN) setup with a Kolmogorov-Arnold Network (KAN) as both the Q and target networks. This model-specific interpretable KAN-based RL structure aims to refine feature selection on a per-example basis iteratively. In this setup, simulated feedback, rather than direct human input, is a proxy for expert annotation. The feedback signal highlights the most relevant features for each data example, creating a pathway for the model to prioritize these features during training. Unlike prior methods, our model explores distributions beyond Bernoulli and organizes convolutional and pooling layers to ensure feature maps match the input and simulated feedback shapes. RL then optimizes a policy to select a unique subset of features per observation. By minimizing the classifier’s prediction loss and the discrepancy between simulated feedback and the model-selected features, this policy yields feature subsets that improve the interpretability and performance of the final predictions. Since predictions are based only on the selected feature subsets, this method offers interpretable, case-specific insights into the model’s output. Using simulated feedback further enables the model to reflect causal structures likely to be relevant in practical applications. We validate our methodology through rigorous experimentation on benchmark datasets, showing the efficiency of our approach in improving model accuracy while maintaining computational feasibility. We aim to establish best practices for integrating human feedback into the feature selection process by investigating the influence of various hyperparameters, stochastic distributions, and the absence of feature selection. The key components of our contribution include the following: 1. We introduce a novel approach incorporating simulated feedback via Gaussian heatmaps and stochastic, distribution-based sampling to refine feature subsets on a per-example basis, thereby enhancing model interpretability and performance. 2. By incorporating KAN into the DDQN architecture for both the Q-network and target network, we achieve significantly better performance than traditional MLP-based DDQN across all test cases. This approach uses a hidden layer with four times fewer neurons than MLP while offering model-specific interpretability. 3. Our research presents a simulated feedback mechanism that generates feature relevance feedback without human annotators, facilitating a scalable training process that reflects the causal relationships typically identified by human experts. The following sections provide an overview of our work: Section II reviews the relevant background; Section III outlines the complete methodology of our proposed framework; Section IV details the experimental design; Section V discusses the results and their interpretations; and finally, Section VI concludes the research while outlining potential future directions."
https://arxiv.org/html/2411.03654v1,PyroGuardian: An IoT-Enabled System for Health and Location Monitoring in High-Risk Firefighting Environments,"First responders risk their lives to reduce property damage and prevent injuries during disasters. Among first responders, firefighters work with fires in residential properties, forests, or other locations where fire occurs. We built the PyroGuardian system that uses wearable modules to transmit unit information over Long Range (LoRa) to an Android tablet. The tablet runs our application, PyroPortal, to assign each firefighter’s stats, such as body temperature, heart rate, and GPS location. PyroPortal displays this information on unit dashboards, and markers on Google Maps represent the firefighter’s location and the direction they are facing. These dashboards can help the incident commander (IC) make more informed decisions on mission control operations and remove specific units whose health stats, such as oximeter and pulse, passed certain thresholds. PyroGuardian completes all these tasks at an affordable cost and in an impressive maximum range between the units and IC. In addition, PyroGuardian has various application scenarios, such as law enforcement and military operations, besides firefighting. We also conducted a sample mission inside a burning building while real firefighters watched. After the demonstration, they completed a survey on system usability and PyroGuardian’s potential to meet their requirements.","First responders operate immediately after a disaster, such as earthquakes, floods, nuclear leakages, fires, and explosions (Girma et al., 2020). Their goal is quickly reaching the disaster point to save lives and reduce property damage (Girma et al., 2020). Significant incidents, such as the terrorist attacks of September 11, 2001, the anthrax attacks of 2001, and the response and recovery efforts of the 2004 Southeast Asia tsunami, have emphasized the role of first responders (Benedek et al., 2007). However, they are not always safe and can suffer severe duty-related consequences. Among first responders, firefighters frequently suffer injuries, as the NFPA 2015 National Fire Experience Survey from fire departments indicated that 68,085 firefighter injuries occurred in the line of duty in 2015 in the US (Haynes and Molis, 2015). Another study found that in 2019, 48 firefighters died while on duty in the US (Fahy et al., 2020). The US Fire Department stated there is a fire in a residential area every 85 seconds (Shokouhi et al., 2019). These facts make firefighters’ efficiency and safety a crucial matter for the public as firefighting is one of the most life-threatening, emotionally traumatic, and stressful occupations (Meina et al., 2020). There are several factors in fire scenes, such as smoke and noise. A firefighter is deafened when they walk inside a building. They cannot see well, and radio communications are challenging (mic, 2021). Thus, an IC will not know their units’ well-being. This lack of communication makes the job more difficult and dangerous. A system is needed to preserve communication between the IC and units. ICs should monitor their critical information and location to interfere in time before any injury. Relying on technological equipment can solve this problem (Yizhe, 2021; Technology in the Fire Service, 2020). Existing solutions tackle this field with techniques like monitoring units’ vitals or tracking their outdoor location. Some issues with existing solutions include the lack of GPS, preventing outdoor localization for wildfires, or vital health information monitoring. We have not found a solution that combines all these tasks into a unified dashboard for the IC. In addition, we have not found a solution that aims to keep the IC behind the safety line while allowing real-time data streaming. To fill this gap and develop a solution while considering pricing and user convenience, we created PyroGuardian: an IoT-powered framework for firefighter mission control. PyroGuardian will warn the IC when firefighters’ vitals or environment variables are dangerous. It can also stream real-time unit location and health data to the IC with a range of 610 meters. The PyroGuardian consists of a tablet, an Android application named PyroPortal, an external USB LoRa adapter for PyroPortal, PyroStrap, and PyroHelm. The IC will use PyroPortal, connected to the LoRa adapter. Each firefighter will equip one PyroHelm and PyroStrap. The PyroHelm’s casing is attachable to the exterior of the standard firefighter gas masks and helmets. The PyroHelm broadcasts sensor data, such as GPS, temperature, and 3-D inertial information, to the PyroPortal via the wireless protocol, LoRa. We used an external LoRa adapter as tablets do not have built-in LoRa. PyroPortal will receive additional sensor data from the PyroStrap via LoRa broadcasting. The tablet has a transceiver LoRa adapter connected to its USB-C port. Once the LoRa adapter receives the sensor data, it inputs the data into the PyroPortal, displaying each unit’s location and vital health information on its map. The IC can then interpret this data to navigate his officers and remove endangered units from the scene. Another contribution of this paper includes creating a real-world fire scenario in the Illinois Fire Service Institute and observing PyroGuardian’s efficiency during the mission. We invited 34 firefighters to watch this mission and provide feedback on our surveys. We evaluate PyroGuardian’s system usability factor to ensure it is convenient for tradition-focused fire departments. Furthermore, the survey also asked them if PyroGuardian meets their needs. Finally, we will analyze the feasibility of our solution in other first-responder fields. Our contributions to this field include: • A novel IoT-powered solution for firefighting mission control that is cheap and easy to use. • A unified dashboard that monitors the unit location and health together • A user study from fire department personnel. • An analysis of extending our framework to other first responder departments, such as law enforcement. Section two gives background regarding some wireless tools and sensors integrated into PyroGuardian. The following section describes a high-level overview of PyroGuardian’s model, workflow, and configuration. The “Implementation” section elaborates on PyroGuardian’s sensors and breakout boards. It will explain in detail the communication between each component. In addition, “Evaluation” presents our results, a comprehensive user study, and PyroGuardian’s performance based on our metrics, such as cost. Section six, Discussion, interprets our results and extends PyroGuardian to a broader scope. Finally, the conclusion states the future trends and ends the paper with our final remarks."
https://arxiv.org/html/2411.03417v1,Usefulness of LLMs as an Author Checklist Assistantfor Scientific Papers: NeurIPS’24 Experiment,"Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peer review. This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting paper submissions against submission standards. We conduct an experiment at the 2024 Neural Information Processing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an “LLM-based Checklist Assistant.” This assistant validates whether papers adhere to the author checklist used by NeurIPS, which includes questions to ensure compliance with research and manuscript preparation standards. Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistant was generally helpful in verifying checklist completion. In post-usage surveys, over 70% of authors found the assistant useful, and 70% indicate that they would revise their papers or checklist responses based on its feedback. While causal attribution to the assistant is not definitive, qualitative evidence suggests that the LLM contributed to improving some submissions. Survey responses and analysis of re-submissions indicate that authors made substantive revisions to their submissions in response to specific feedback from the LLM. The experiment also highlights common issues with LLMs—inaccuracy (20/52) and excessive strictness (14/52) were the most frequent issues flagged by authors. We also conduct experiments to understand potential gaming of the system, which reveal that the assistant could be manipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities of automated review tools.","Recent advancements in large language models (LLMs) have significantly enhanced their capabilities in areas such as question answering and text generation. One promising application of LLMs is in aiding the scientific peer-review process [Sha22, KAD+24]. However, the idea of using LLMs in peer review is contentious and fraught with potential issues [LS23]. LLMs can hallucinate, exhibit biases, and may compromise the fairness of the peer-review process. Despite these potential issues, LLMs may serve as useful analytical tools to scrutinize manuscripts and identify possible weaknesses or inaccuracies that need addressing. In this study, we take the first steps towards harnessing the power of LLMs in the application of conference peer review. We conduct an experiment the the Neural Information Processing Systems (NeurIPS) 2024 conference, a premier conference in the field of machine learning.111In computer science, unlike most other fields, conferences are a primary venue for publication, with the peer-review process evaluating entire manuscripts rather than just abstracts. While the wider ethical implications and appropriate use cases of LLMs remain unclear and must be a larger community discussion, here, we evaluate a relatively clear-cut and low-risk use case: vetting paper submissions against submission standards, with results shown only to the authors. Specifically, the NeurIPS peer-review process requires authors to submit a checklist appended to their manuscripts. Such author checklists, utilized in NeurIPS as well as in other peer-review venues [MSA01, VEA+07, MLT+09], contain a set of questions designed to ensure that authors follow appropriate research and manuscript preparation practices. The NeurIPS Paper Checklist is a series of yes/no questions that help authors check if their work meets reproducibility, transparency, and ethical research standards expected for papers at NeurIPS. The checklist is a critical component in maintaining standards of research presented at the conference. Adhering to the guidelines outlined by these checklists helps authors avoid mistakes that could lead to rejection during peer review. We deploy and evaluate a NeurIPS 2024 Checklist Assistant powered by LLMs. This assistant scrutinizes authors’ responses to the NeurIPS checklist, proposing enhancements for submissions to meet the conference’s requirements. To prevent any potential bias in the review process, we confine its usage exclusively to the authors of papers, so the checklist assistant is not accessible to reviewers. We then systematically evaluate the benefits and risks of LLMs by conducting a structured study to understand if LLMs can enhance research quality and improve efficiency by helping authors understand if their work meets research standards. Specifically, we administered surveys both before and after use of the Checklist Assistant asking authors about their expectations for and perceptions of the tool. We received 539 responses to the pre-usage survey, 234 submissions the the Checklist Assistant and 78 responses to the post-usage survey. Our main findings are as follows: (1) Authors generally reported that the LLM-assisted checklist review was a valuable enhancement to the paper submission process. • The majority of surveyed authors reported a positive experience using the LLM assistant. After using the assistant, over 70% of authors reported that they found the assistant useful and over 70% reported that they would modify their paper and/or checklist responses based on the feedback given (Section 4.1.3). • Authors’ expectations of the assistant’s effectiveness were even more positive before using it than their assessments after actually using it (Section 4.1.3). • Among the main issues reported by authors in qualitative feedback, the most frequently cited were inaccuracy (20/52 respondents) and that the LLM was too strict in its requirements (14/52 respondents) (Section 4.1.4). (2) While changes in NeurIPS paper submissions cannot be causally attributed to use of the checklist verification assistant, we find qualitative evidence that the checklist review meaningfully helped some authors to improve their submissions. • Analysis of the content of LLM feedback to authors indicates that the LLM provided granular feedback to authors, generally giving 4-6 distinct and specific points of feedback per question across the 15 questions (Section 4.2.1). • Survey responses reflect that some authors made meaningful changes to their submissions—35 survey respondents described specific modifications they would make to their submissions in response to the Checklist Assistant (Section 4.2.2). • In 40 instances, authors submitted their paper twice to the checklist verifier (accounting for 80 total paper submissions.) Between these two submissions, authors tended to increase the length of their checklist justifications significantly, suggesting that they may have added content in response to LLM feedback (Section 4.2.3). Finally, we investigate how LLM-based tools can be easily manipulated – specifically, we find that with AI-assisted re-writing of the justifications, an adversarial author can make the Checklist Assistant significantly more lenient (Section 5.1). In summary, the majority of authors found LLM assistance to be beneficial, highlighting the significant potential of LLMs to enhance scientific workflows—whether by serving as direct assistants to authors or helping journals and conferences verify guideline compliance. However, our findings also underscore that LLMs cannot fully replace human expertise in these contexts. A notable portion of users encountered inaccuracies, and the models were also vulnerable to adversarial manipulation. Our code, LLM prompts, and sample papers used for testing are available at: https://github.com/ihsaan-ullah/neurips-checklist-assistant"
https://arxiv.org/html/2411.03397v1,SAUCE\scalerel*X: Synchronous and Asynchronous User-Customizable Environment for Multi-Agent LLM Interaction,"Many human interactions, such as political debates, are carried out in group settings, where there are arbitrarily many participants, each with different views and agendas. To explore such complex social settings, we present SAUCE\scalerel*X: a customizable Python platform, allowing researchers to plug-and-play various LLMs participating in discussions on any topic chosen by the user. Our platform takes care of instantiating the models, scheduling their responses, managing the discussion history, and producing a comprehensive output log, all customizable through configuration files, requiring little to no coding skills. A novel feature of SAUCE\scalerel*X is our asynchronous communication feature, where models decide when to speak in addition to what to say, thus modeling an important facet of human communication. We show SAUCE\scalerel*X’s attractiveness in two initial experiments, and invite the community to use it in simulating various group simulations.111SAUCE\scalerel*X is publicly available on github.com/Deep-Cognition-Lab/SAUCE. A short demo video is available on this YouTube link.","Recent years have seen the rise of large language models (LLMs) with improved chat abilities Köpf et al. (2024). Such models are largely trained and evaluated under two basic assumptions. First, the interaction with an LLM is usually done assuming binary interaction. I.e., there is a single human user issuing natural language instructions which a single LLM then tries to follow. Furthermore, the interaction is synchronous, namely the LLM answers every request by the human user with a single response of its own, to which the user can then respond with a single follow-up request based on the model’s response, etc., where there is no notion of an outside time passing. This framing is expressive enough to handle a wide variety of tasks in various domains. Figure 1: Illustration of a discussion between different agents, run on SAUCE\scalerel*X. Our framework allows setting up a discussion topic, and then manages the group discussion by instantiating models and scheduling their responses. However, many real-world human interactions do not adhere to these shared basic assumptions and therefore cannot be captured in standard LLM applications. First, human interactions are often carried out between arbitrarily many participants, each with potentially differing points of view, and varying objectives for the outcome of the interaction. Often the goal of such multi-party discussion is to find some common ground through agreement or compromise between participants. For example, this is ideally the case in political debates. Second, in many real-world scenarios human interaction is asynchronous, where there is significant challenge in deciding when to speak in addition to deciding what to say. For example, in many strategic bargaining scenarios, such as financial discussions or more structured social games, choosing to remain silent can often convey significant information. In this work, we introduce SAUCE\scalerel*X, a modular and user-friendly Python platform for multi-agent, asynchronous LLM experiments. SAUCE\scalerel*X sets up a discussion room where different models can be instantiated to interact with each other around a shared discussion topic (see Figures 1 and 4), providing both synchronous scheduling, where the LLMs are prompted in a predefined manner, as well as asynchronous scheduling, where SAUCE\scalerel*X keeps track of a simulated outside clock, allowing models to “skip” their turn, based on the outside time and the discussion history. We present experiments showing that SAUCE\scalerel*X effectively facilitates the study of multi-agent LLM interactions in synchronous and asynchronous environments. Taubenfeld et al. (2024) have recently used our platform to simulate political debates between agents representing different political ideologies, uncovering a tendency for LLM agents to conform to the model’s inherent social biases, even when instructed to debate from specific political perspectives. This behavior notably diverges from well-established social dynamics observed in humans. In another experiment, we simulate an asynchronous philosophical debate on the trolley problem Thomson (1984), illustrating how agents adjust their participation based on context and time constraints. This setup showcases the flexibility of asynchronous communication, revealing diverse speaking strategies such as speaking frequently, choosing to wait and listen, and adapting the participation according to the evolving context. SAUCE\scalerel*X can spur research in two complementing directions. First, model developers interested in realistic scenarios with multiple participants or in an asynchronous environment, can readily plug-and play their models to evaluate how they interact with one another. Second, SAUCE\scalerel*X enables user studies incorporating human subjects interacting with LLMs in such settings."
https://arxiv.org/html/2411.03295v1,Examining Human-AI Collaboration for Co-Writing Constructive Comments Online,"This paper examines how large language models (LLMs) can help people write constructive comments in online debates on divisive social issues and whether the notions of constructiveness vary across cultures. Through controlled experiments with 600 participants from India and the US, who reviewed and wrote constructive comments on online threads on Islamophobia and homophobia, we found potential misalignment in how LLMs and humans perceive constructiveness in online comments. While the LLM was more likely to view dialectical comments as more constructive, participants favored comments that emphasized logic and facts more than the LLM did. Despite these differences, participants rated LLM-generated and human-AI co-written comments as significantly more constructive than those written independently by humans. Our analysis also revealed that LLM-generated and human-AI co-written comments exhibited more linguistic features associated with constructiveness compared to human-written comments on divisive topics. When participants used LLMs to refine their comments, the resulting comments were longer, more polite, positive, less toxic, and more readable, with added argumentative features that retained the original intent but occasionally lost nuances. Based on these findings, we discuss ethical and design considerations in using LLMs to facilitate constructive discourse online.","Most people use social media in good faith (Rajadesingan et al., 2021), but often struggle to reach common ground during online debates (Baughan et al., 2021). Online disagreements often lead to toxicity and personal attacks, and the lack of support for constructive dialogue discourages people from challenging problematic content (Gurgun et al., 2023; Shahid et al., 2024a). This results in downstream harms, such as the disappearance of minority viewpoints (Collier and Bear, 2012; Grevet et al., 2014) and increased propagation of harmful content (Shahid et al., 2024b). To suppress online conflict and encourage civil dialogue, most platforms rely on reactive measures such as content removal and banning offenders, which are not always effective in encouraging prosocial behaviors (Seering et al., 2017; Srinivasan et al., 2019). In response, HCI researchers have explored proactive strategies to promote healthy discourse, such as using social cues to highlight positive behavior (Jhaver et al., 2017; Rajadesingan et al., 2021), and introducing frictions (Seering et al., 2019; Katsaros et al., 2022; Masrani et al., 2023) and nudges (Esau et al., 2017; Taylor et al., 2019; Bossens et al., 2021; Park et al., 2023) to discourage people from using offensive language. However, these interventions put the onus on users to write their opinions constructively, which is already difficult in non-conflicting situations (Cutler et al., 2022), and more so during disputes on socially divisive issues. Researchers have defined constructive comments as those that balance argumentation with politeness (Kolhatkar and Taboada, 2017a, b; Kolhatkar et al., 2020). Given the challenges people face in engaging constructively in online arguments (Gurgun et al., 2023), some researchers have trained large language models (LLMs) to help people in argumentative writing (Lee et al., 2022a; Zhang et al., 2023; Dang et al., 2023). While early evaluations show that these models improve people’s writing (Zhang et al., 2023) and help them find common ground on divisive issues (Argyle et al., 2023), much of this work focuses on argumentative essay writing and one-on-one private debates. Moreover, these studies do not take into account how people from different cultures approach argumentation. Prior research shows that people from individualistic cultures prefer logical arguments that follow formal rules to substantiate points (Nisbett et al., 2001; Peng and Nisbett, 1999; Choi, 1988). In contrast, those from collectivist cultures favor dialectical arguments during social conflicts, emphasizing a holistic consideration of different viewpoints and finding middle ground (Ting-Toomey et al., 1991; Ting-Toomey and Kurogi, 1998; Peng and Nisbett, 1999). These cultural differences might prove critical while designing interventions to help people engage in online debates on divisive social issues, especially since LLMs tend to homogenize writing towards Western norms and diminish cultural nuances (Agarwal et al., 2024). We extend this growing body of work in two key ways. First, we examine how people perceive and write constructive comments on socially divisive issues within online threads, where meaningful discussions often get overshadowed by thoughtless and negative comments (Napoles et al., 2017). Second, we investigate if notions of constructiveness in online debates vary across cultures. Specifically, we address the following research questions: RQ1: Do perceptions of constructive comments differ between humans and LLMs? RQ2: Can LLMs help people write constructive comments on divisive social issues? RQ3: Do notions of constructiveness in online comments vary across cultures? To answer these questions, we conducted a two-phase study with participants from India and the United States of America (US), who reviewed online threads containing homophobic and Islamophobic content relevant to their cultural contexts. In Phase 1, we used GPT-4 to generate constructive comments on these threads, following either a logical or dialectical argumentation style, and adopting different stances (either in favor or against the issue). We verified, both quantitatively and qualitatively, that the LLM-generated comments differed only in argumentation style and not in linguistic features of constructiveness. We then conducted a forced-choice experiment with 103 Indian and American crowd workers on Prolific. Each participant reviewed either a homophobic or an Islamophobic thread along with randomly selected pairs of logical vs. dialectical comments, written for the same thread from the same stance. For each pair, participants indicated which comment they perceived as more constructive and why. To compare human and LLM’s perceptions of constructiveness, we also assigned the same task to GPT-4. In response to RQ1, we found that GPT-4 was 2.46 times more likely than humans to select dialectical comments as more constructive than logical ones. GPT-4 tended to prioritize comments that were polite and balanced different viewpoints (dialectical arguments). In contrast, our participants favored comments that emphasized logic and facts (logical arguments) more than GPT-4 while picking constructive comments. Given these differences, we next examined how people engage with LLMs when writing constructive comments. In Phase 2, we conducted a between-subject experiment with 103 Indian and American crowd workers recruited through Prolific. Participants were asked to write constructive comments on homophobic and Islamophobic threads. They were randomly assigned to one of two groups: a control group where they wrote comments independently, and a test group where they could select prompts to request an LLM to rewrite their comments constructively. In the test group, participants had the flexibility to accept, edit, reject, or regenerate the suggestions from LLM. To address RQ2, we compared the comments from Phase 2— the ones participants wrote independently in control group (human-written) or with LLM’s assistance in test group (human-AI written)— alongside the comments generated solely by LLM (AI) in Phase 1 to assess which type of comment was more constructive. Crowd evaluation showed that, when presented with (Human vs. AI) comment pairs, participants were 8.51 times more likely to select LLM-generated comments as more constructive than the human-written comments. Similarly, participants were 3.19 times more likely to choose HAI-written comments as more constructive over human-written comments. Quantitative analyses of these comments revealed that both LLM-generated and HAI-written comments contained significantly more constructive features, such as greater length, more discourse connectives, and stance adverbials, than human-written comments. These suggest that LLMs have the potential to assist people in writing more constructive comments on divisive social issues. We noted that when participants requested suggestions from the LLM, it made their comments significantly more positive, more constructive, and less toxic. In most cases, participants accepted the LLM’s suggestions because it conveyed their points better without homogenizing their writing. However, some participants felt that the LLM misrepresented their views and edited the suggestions in ways that made their comments more negative and toxic. Overall, participants were satisfied with the comments they wrote with the assistance of LLMs and found the process easier than writing constructive comments independently. Finally, we compared the responses of Indian and American participants in Phase 1 and Phase 2 to address RQ3. We found that participants from both countries considered dialectical comments as more constructive than logical ones and provided similar reasoning for their preferences. They showed comparable skill in writing constructive comments, both independently and with LLM assistance. In the test group, participants from both countries used similar prompts to make their comments constructive. Additionally, they also rated LLM-generated and HAI-written comments as significantly more constructive than the human-written comments. These findings suggest a shared understanding of constructiveness across cultures in the case of online comments on divisive social issues. Taken together, our work makes the following contributions: • We provide both quantitative and qualitative evidence that LLMs can help people from different cultures write constructive comments on divisive issues. • We uncover potential misalignment between human’s and LLM’s preferences for different argumentation styles while assessing constructiveness. • We reveal the potential risk of LLMs misrepresenting people’s views on divisive issues by applying positive sentiment to their comments. • We discuss both ethical and design consideration for developing socio-technical systems that promote constructive discourse on divisive issues across different cultures."
https://arxiv.org/html/2411.03213v1,What Makes an Educational Robot Game Fun? Framework Analysis of Children’s Design Ideas,"Fun acts as a catalyst for learning by enhancing motivation, active engagement and knowledge retention. As social robots gain traction as educational tools, understanding how their unique affordances can be leveraged to cultivate fun becomes crucial. This research investigates the concept of fun in educational games involving social robots to support the design of REMind: 111REMind is short for Robots Empowering Minds a robot-mediated role-play game aimed at encouraging bystander intervention against peer bullying among children. To incorporate fun elements into design of REMind, we conducted a user-centered Research through Design (RtD) study with focus groups of children to gain a deeper understanding of their perceptions of fun. We analyzed children’s ideas by using Framework Analysis and leveraging LeBlanc’s Taxonomy of Game Pleasures and identified 28 elements of fun that can be incorporated into robot-mediated games. We present our observations, discuss their impact on REMind’s design, and offer recommendations for designing fun educational games using social robots.","Social robots are increasingly popular in education, offering unique opportunities to enhance learning. Their physical embodiment and expressive capabilities make them highly engaging, allowing for creating interactive and emotionally resonant experiences. By integrating social robots with educational games, we can create playful scenarios that offer personalized and situated experiences, increasing learner engagement and potentially enhancing learning outcomes [32]. Games, as structured play, are excellent facilitators of learning because they induce Flow state [31, 5]. Flow states, characterized by deep involvement with an activity and a distorted sense of time, are important in educational settings as they reduce self-consciousness and enhance learning [30]. Furthermore, games create a safe space for learning by eliminating real-life costs of failure, encouraging persistence and self-regulated learning through multiple attempts and immediate feedback [38]. Additionally, games engage learners as active producers of knowledge, unlike the passive consumption often found in traditional education [13]. Thus, we argue that integrating Game-Based Learning (GBL) into social robot educational experiences could increase their effectiveness. Broadly defined, GBL involves the use of games and game-like elements to support the acquisition of knowledge, skills, and attitudes. It differs from gamification, which typically involves adding incentives like stars, points, or rankings to motivate learners to engage with otherwise tedious tasks [31]. Superficially incorporating fun elements into educational components (e.g., by merely transforming a quiz into a digital format with a point system) often falls flat and leads to disengagement if the core task is not fun. Game scholars use the metaphor of ‘chocolate-covered broccoli’ to illustrate this point: adding fun to a game as an afterthought is like coating vegetables in chocolate; it does not make them candy [42]. In contrast, GBL involves redesigning learning tasks to be inherently interesting and meaningful, and aiming to create experiences that are both effective educational tools and fun games [31, 23]. While educational video games are a modern example, GBL has a long history. Developmental psychology has long recognized play as a natural form of learning, with significant research on playful learning predating the digital era [30]. A key feature of play is that it is intrinsically motivating– we play because it is fun. Games are engaging and enjoyable by design, which can increase learners’ motivation and interest in the learning content, leading to higher levels of effort, persistence, and achievement [24, 28]. Although games that have a purpose beyond entertainment are commonly labeled ‘Serious Games,’ game design scholars argue that this label is misleading, as it implies that such games need not prioritize fun; instead they advocate for the term ‘Transformational Games’ [38]. Regardless of terminology, to be effective, such games must indeed be fun [31]. Hence, it is crucial to explore how we might objectively integrate fun into the designs of educational experiences. Game designers highlight several key factors: goals, feedback, interactivity, storytelling, aesthetics, challenges, and social interaction. While the fundamentals of play remain consistent, we argue that novel technologies like social robots introduce new opportunities for play through their unique affordances. This raises the question: how can social robots make transformational games more fun? We explored this question in the context of developing REMind, an educational game using social robots to teach anti-bullying intervention skills to children. The subjective and context-dependant nature of fun poses a design challenge. Recognizing users as “experts of their own experiences” [33], we took a user-centered Research through Design (RtD) approach. We engaged children, the game’s primary stakeholders, in focus groups to brainstorm ways to make REMind fun. We analyzed the qualitative data gathered from this study by applying Framework Analysis [14] and using LeBlanc’s Taxonomy of Game Pleasures [15], which identifies 8 categories of enjoyment in games. This method helped us identify 28 elements of fun in robot games, which informed our game design. In this paper, we present our findings, discuss how they shaped the design of REMind, and offer design recommendations based on these insights."
https://arxiv.org/html/2411.03137v1,From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice,"Creative writers have a love for their craft, yet AI systems using large language models (LLMs) offer the automation of significant parts of the writing process. So why do some creative writers choose to integrate AI into their workflows? To explore this, we interview and observe a writing session with 18 creative writers who already use AI regularly in their writing practice. Our findings reveal that creative writers are intentional about how they incorporate AI, making many deliberate decisions about when and how to engage AI based on the core values they hold about writing. These values, such as authenticity and craftsmanship, alongside writers’ relationships with and use of AI influence the parts of writing over which they wish to maintain control. Through our analysis, we contribute a taxonomy of writer values, writer relationships with AI, and integration strategies, and discuss how these three elements interrelate.","“…[A]rt is something that results from making a lot of choices… When you are writing fiction, you are—consciously or unconsciously—making a choice about almost every word you type; to oversimplify, we can imagine that a ten-thousand-word short story requires something on the order of ten thousand choices. When you give a generative-A.I. program a prompt, you are making very few choices; if you supply a hundred-word prompt, you have made on the order of a hundred choices.” — Ted Chiang, “Why A.I. Isn’t Going to Make Art” (Chiang, 2024) As LLMs become more sophisticated and accessible, researchers and developers have explored ways to leverage AI for supporting different parts of the writing process in a variety of mediums, from summarizing papers for academic writing (Jin et al., 2024), to generating copy for marketing (Zhang et al., 2021), writing emails in business settings (Jovic and Mnasri, 2024), and fact-checking in journalism (Zhou et al., 2024). While LLMs have been found to be useful for many of these tasks, they have also sparked many concerns among writers, such as how to approach attribution and authorship (Huang et al., 2024), how to manage hallucinations (Wei et al., 2024), ethical considerations (McKee and Porter, 2020), and worries about job displacement (Wang, 2023). Creative writing, whether in fiction, poetry, screenwriting, or experimental forms, is a particular form of writing grounded in artistic expression and is often seen as a deeply personal and human craft. For creative writing specifically, LLMs raise even deeper questions about the relationship between AI and human creativity, urging creative writers to reflect on the nature of their practice and motivations for why they write. Some perceive that using AI for writing means doing little more than pressing a button (Chiang, 2024)—producing content with almost no effort or creativity involved on the part of the human (Acovino, 2023). If this is indeed the case, why would writers choose to incorporate AI into their creative process? What is actually valuable for writers when they are writing, and how are they able to use AI in ways that preserve that? In this work, we investigate the perspectives and strategies of creative writers who have already integrated the use of language-based generative AI into their writing process and consider the following questions: • RQ1: When and why do creative writers use AI? • RQ2: How do creative writers negotiate AI usage with their values as writers? • RQ3: How are creative writers incorporating AI into their writing process? To gain insight into how creative writers view AI and navigate these challenges over the long term, we conducted a contextual inquiry study with 18 creative writers who have been using AI in their practice ranging from months to years. In addition to interviewing writers about their views and uses of AI, we also observed each writer’s practice through a live-writing session while they thought aloud during the process. The writers we interviewed and aim to understand represent a distinct group: those who self-identify as creative writers who have chosen to use AI. In this moment in time where the role of AI in creative work is still evolving, these creative writers are uniquely positioned to reflect on their experiences both before and after writing with AI. Given that these writers have also used AI for a long enough period to develop their own practices, we observed the strategies they take to navigate tensions around their use of AI. We find that creative writers who use AI are highly intentional about when to use AI or not, stemming from strongly held values about writing, including desiring to preserve their authentic voice or maintain ownership over the output. Writers carefully negotiate their relationship with AI—dynamically delegating more or less and different work to AI depending on their values and their goals in the moment. For instance, writers describe roles for AI that range from a muse that is in charge of inspiration and new ideas but no writing support, to a ghostwriter that is in charge of writing in the author’s voice but not contributing creatively. But ultimately, all writers maintain a strong sense of control and ownership over their work. Even when AI generates text or sparks ideas, writers feel it’s their creative vision and decisions that shape the final result. We also find that creative writers use AI in an effortful way, involving many iterations and decision points. Writers both love and view the process of writing as a craft—not something to automate away. Instead, writers often make many repeated calls to the AI that are deeply embedded in different parts of their existing workflow and also make many decisions surrounding a single call. For instance, when overcoming writer’s block, successive calls to the AI for variations of text can help a writer get unstuck by allowing them to clarify what they don’t like, even if they end up not using any of the generated text. We discuss how writer values, writer relationships with AI, and integration strategies are all intertwined and impact each other, as well as the long-term implications of the use of AI for the craft of writing. Our findings provide insight for writing communities into how writers with similar values to them are engaging with AI, and show that using AI does not mean just one thing, but can encompass many different workflows. We also offer considerations for those designing AI tools to guide development that better aligns with the needs of writers, and discuss the evolving roles that AI might play in creative writing practices for the future."
https://arxiv.org/html/2411.03108v1,“Create a Fear of Missing Out” — ChatGPT Implements Unsolicited Deceptive Designs in Generated Websites Without Warning (draft),"With the recent advancements in Large Language Models (LLMs), web developers increasingly apply their code-generation capabilities to website design. However, since these models are trained on existing designerly knowledge, they may inadvertently replicate bad or even illegal practices, especially deceptive designs (DD). This paper examines whether users can accidentally create DD for a fictitious webshop using GPT-4. We recruited 20 participants, asking them to use ChatGPT to generate functionalities (product overview or checkout) and then modify these using neutral prompts to meet a business goal (e.g., “increase the likelihood of us selling our product”). We found that all 20 generated websites contained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no warnings. When reflecting on the designs, only 4 participants expressed concerns, while most considered the outcomes satisfactory and not morally problematic, despite the potential ethical and legal implications for end-users and those adopting ChatGPT’s recommendations.","Large Language Models (LLMs), such as ChatGPT, Gemini, Llama, or Claude, are powerful tools for solving complex and creative tasks, answering rich-layered questions, and supporting software development through natural text input (i.e., prompts) (Khojah et al., 2024). LLMs are trained on existing data, with their generated output essentially being a reconfiguration of pre-existing artifacts and concepts. Much like the concept of precedent-based design – a widely recognized approach in design theory that involves reusing previous design solutions for similar or identical challenges (Eilouti, 2009) – LLMs have the potential to disseminate design knowledge by recreating and reproducing established ideas. However, this becomes particularly challenging when design knowledge is derived from examples incorporating deceptive design (DD) practices or patterns. DD patterns describe “design practices that materially distort or impair […] the ability of recipients of [a] service to make autonomous and informed choices or decisions” (European Union, 2022). Such practices can be implemented knowingly or unknowingly (e.g., when existing designs are replicated or adapted) (European Union, 2022) and are frequently found on e-commerce websites (Mathur et al., 2019). As a result, LLMs trained on web pages and online knowledge about DD practices and patterns including their underlying psychological levers may reproduce, remix, and even propagate respective DD practices. Accordingly, we need to assess the extent to which ChatGPT may push for the adoption of DDs without disclosure to the LLM user. We base our research on a fictitious shoe-selling company that wants to increase its sales. We recruited 20 participants to generate interactive product overviews or check-out webpages incorporating HTML, CSS, and JavaScript with ChatGPT as the market leading chatbot based on a LLM111In December 2023, OpenAI held 39% of the generative AI and models market share according to https://iot-analytics.com/leading-generative-ai-companies/. To not lead the model to incorporate deceptive designs, our participants used neutral language in their prompts, such as “increase the likelihood of people signing up to our newsletter” or “we need to get more people to buy our product.” Throughout our study, each participant generated three single-page HTML files with the help of ChatGPT and shared the complete chat history of the overall interaction. We further assessed our participants’ satisfaction, perceived ownership of the resulting design, perceived responsibility, and the morality of the resulting user interfaces. In an additional preliminary cross-validation study in which we prompted the LLMs Gemini 1.5 Flash and Claude 3.5 Sonnet, we investigated the potential generalizability of our findings. When analyzing the prompts, the respective responses, and the corresponding HTML files in a co-coding process with four authors, we found that each of the generated, functional, and interactive interfaces contained at least one DD pattern as defined by Gray et al.’s ontology (Gray et al., 2024), with a mean of 5 DD and a maximum of 9 DD patterns per generated HTML file. The most dominant category were patterns utilizing the strategies Interface Interference and Social Engineering (Gray et al., 2024). When we assessed the prompts and respective responses, we found that ChatGPT grounds its proposals in mechanisms based on pressure (e.g., urgency, scarcity), psychosocial factors (e.g., social proof, persuasion), perception (e.g., attention, prominence), and economic incentives. Alarmingly, ChatGPT only voiced a single disclaimer in one instance of our datasets but never pointed towards warnings or concerns regarding the proposed functional websites and the included deceptive design practices that resulted from neutral prompts. Further, when asked about satisfaction, our participants were satisfied with the ready-to-use interfaces or surprised by how well ChatGPT handled the task of generating interactive HTML files. Only 4 participants pointed toward the potential issues of fake customer reviews or the leading of users when asked directly to assess social implications. With this paper, we contribute to the ongoing debates about ethical and legal responsibility and negative side-effects of LLM generated results and their unsupervised application. We specifically focus on DD practices and patterns and how they might be distributed based on LLM-generated output even though the prompts did not contain pertinent keywords or engaged in prompt-jailbreaking. Our contribution is fourfold: 1) We demonstrate that ChatGPT proposes and generates DP based on neutral prompts, 2) we map the DDs ChatGPT ultimately proposes to Gray et al.’s ontology (Gray et al., 2024), and 3) show that ChatGPT does not disclose incorporating deceptive designs in a meaningful manner, which could lead to the knowing and willing reapplication of such designs through designers and developers. Furthermore, web designers and developers may face not just ethical, but also potential legal consequences if they reapply ChatGPT’s design proposals without scrutiny. 4) Finally, our findings of a preliminary cross-validation study outline that this issue also applies to other competing LLMs and, therefore, requires immediate attention."
https://arxiv.org/html/2411.02831v1,Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults in an Emotion-Aware Chatbot,"ABSTRACT The research community has traditionally shown a keen interest in emotion modeling, with a notable emphasis on the detection aspect. In contrast, the exploration of emotion generation has received less attention.This study delves into an existing state-of-the-art emotional chatbot, EmoBot, designed for generating emotions in general-purpose conversations. This research involves a comprehensive examination, including a survey to evaluate EmoBot’s proficiency in key dimensions like usability, accuracy, and overall user satisfaction, with a specific focus on fault tolerance. By closely examining the chatbot’s operations, we identified some noteworthy shortcomings in the existing model. We propose some solutions designed to address and overcome the identified issues.","In recent years, there has been a growing emphasis on the study of social chatbots. Unlike older rule-based counterparts, modern chatbots, driven by deep learning, have shown significant improvements (Sutskever et al., 2014). In order to design a chatbot that provides a meaningful experience, we must first understand what expectations people have for this technology, and what opportunities are there for chatbots based on user needs (Zamora, 2017). As chatbots become more prevalent in areas like entertainment and customer service (Io and Lee, 2017), the focus has shifted to making them more emotionally responsive and human-like. The goal is to enable chatbots to engage in empathetic conversations, often assuming the role of a social companion, thereby positively impacting the well-being of individuals (Skjuve et al., 2021). Therefore, adaptability to various scenarios and meeting user needs are crucial for a social chatbot’s success.This study delves into Emobot, an emotional chatbot using the cognitive appraisal theory to generate emotions based on user responses (Ehtesham-Ul-Haque et al., 2024). Our analysis involves an exploration of user experiences with this chatbot. We examine user experiences with Emobot, identifying areas for improvement in its emotion generation. Our goal is to detect and address these issues, proposing practical solutions to enhance the chatbot’s performance. Our contribution in this light are as follows : • Conducted a survey to assess user experience and subsequently performed a qualitative analysis on the gathered data. • By examining the codebase and analysing user experince we have found some faults in the existing Emobot. • Proposed different solution approaches according to the shortcomings."
https://arxiv.org/html/2411.02725v1,Leveraging LLM Tutoring Systems for Non-Native English Speakers in Introductory CS Courses,"Computer science has historically presented barriers for non-native English speaking (NNES) students, often due to language and terminology challenges. With the rise of large language models (LLMs), there is potential to leverage this technology to support NNES students more effectively. Recent implementations of LLMs as tutors in classrooms have shown promising results. In this study, we deployed an LLM tutor in an accelerated introductory computing course to evaluate its effectiveness specifically for NNES students. Key insights for LLM tutor use are as follows: NNES students signed up for the LLM tutor at a similar rate to native English speakers (NES); NNES students used the system at a lower rate than NES students—to a small effect; NNES students asked significantly more questions in languages other than English compared to NES students, with many of the questions being multilingual by incorporating English programming keywords. Results for views of the LLM tutor are as follows: both NNES and NES students appreciated the LLM tutor for its accessibility, conversational style, and the guardrails put in place to guide users to answers rather than directly providing solutions; NNES students highlighted its approachability as they did not need to communicate in perfect English; NNES students rated help-seeking preferences of online resources higher than NES students; Many NNES students were unfamiliar with computing terminology in their native languages. These results suggest that LLM tutors can be a valuable resource for NNES students in computing, providing tailored support that enhances their learning experience and overcomes language barriers.","In computing education, being a non-native English speaker (NNES) can present learning challenges, especially for those outside anglophone countries or with limited English proficiency, due to the predominance of English-based materials (Becker, 2019). The most popular programming languages (e.g., Java, Python, C) use English for their keywords and API naming conventions (Hanselman, 2008; Chistyakov, 2017; Igawa et al., [n.d.]). English computer terminology used in classroom settings is one of the main factors negatively affecting NNES student performance in programming classes (Keen and Etzkorn, 2009), predicting approximately 50% of final exam performance when controlling for English proficiency (Alaofi and Russell, 2022). Moreover, English proficiency is strongly correlated with programming performance (Qian and Lehman, 2016; Rauchas et al., 2006; Wibowo and Tandiono, 2022; Pudyastuti et al., 2014), further widening the gap for NNES students. In addition to programming languages being primarily based in English, most online resources for learning programming (e.g., documentation, online Q&A forums, tutorials) are also predominantly in English. These skewed linguistic accommodations make a certain level of English proficiency increasingly necessary to learn computer programming (Hanselman, 2008). This de facto prerequisite creates a significant hurdle for NNES individuals who may have limited English proficiency (Guo, 2018). While tailoring the delivery of materials for NNES students has been suggested (Becker et al., 2022), the practical implementation of this strategy has traditionally been a challenge. However, large language models (LLMs) may now provide an opportunity to better serving NNES computing students as the models having been trained on 100+ natural languages (Xue et al., 2020). Indeed, with the multilingual capabilities of LLMs and their past implementation in computing classrooms (Denny et al., 2024c; Prather et al., 2023; Liu and M’Hiri, 2024; Liu et al., 2024; Woodrow et al., 2024), this technology might help alleviate the de facto primarily English setting by allowing NNES students to easily obtain assistance in their native language. With the rapid rise of LLMs, computing educators face the challenge of integrating this technology into the classroom (Becker et al., 2023). The study by Denny et al. suggests that LLMs can enhance the accessibility and scalability of high-quality educational content while reducing educators’ workloads (Denny et al., 2023a). One implementation of this technology is through tutoring systems that provide guidance, rather than direct answers to programming questions, helping users to reach solutions independently (Kazemitabaar et al., 2024; Liffiton et al., 2023). LLM tutoring systems have shown positive effects for students in the classroom (Kazemitabaar et al., 2024; Liffiton et al., 2023; Zapata et al., 2023). Coupled with this, state-of-the-art LLMs that have been trained on 100+ languages (Xue et al., 2020) can perform extremely well when translating between languages (Jordan et al., 2024). Due to this, we hypothesize that NNES students may especially benefit from the use of LLM tutoring systems to ask for help—including in their native language. To understand this, we asked the following research questions: (1) How do NNES students interact with a LLM tutor, and how does this differ from native English speaker (NES) students? (2) How do NNES students’ views differ from NES students in regards to LLM tutor use?"
https://arxiv.org/html/2411.02684v1,"Towards Intelligent Augmented Reality (iAR): A Taxonomy of Context, an Architecture for iAR, and an Empirical Study","Recent advancements in Augmented Reality (AR) research have highlighted the critical role of context awareness in enhancing interface effectiveness and user experience. This underscores the need for intelligent AR (iAR) interfaces that dynamically adapt across various contexts to provide optimal experiences. In this paper, we (a) propose a comprehensive framework for context-aware inference and adaptation in iAR, (b) introduce a taxonomy that describes context through quantifiable input data, and (c) present an architecture that outlines the implementation of our proposed framework and taxonomy within iAR. Additionally, we present an empirical AR experiment to observe user behavior and record user performance, context, and user-specified adaptations to the AR interfaces within a context-switching scenario. We (d) explore the nuanced relationships between context and user adaptations in this scenario and discuss the significance of our framework in identifying these patterns. This experiment emphasizes the significance of context-awareness in iAR and provides a preliminary training dataset for this specific Scenario.","1 Related Work In today’s life, the growing dependence on personal computing devices such as mobile phones for decision-making and task performance has led to challenges, such as disruptions to social interactions [65, 20, 14, 1, 39]. This has driven Ubiquitous computing’s goal of seamlessly embedding near-constant, yet unobtrusive, digital information into daily life [70, 69]. Research indicates that by integrating the information into the real world (RW), AR can enhance efficiency and memory retention and reduce such challenges [67, 18]. However, AR effectiveness depends on presenting the right information at the right time and form [9, 28, 36]. This work focuses on developing iAR systems that enhance user experience and efficiency by ensuring timely and appropriate information delivery in any context. Various studies, such as ARWin and ARBrowse, have demonstrated the benefits of AR for providing on-demand information, assistance, and entertainment, spanning from domestic use to workplace collaboration and productivity [43, 22, 45]. However, the effectiveness of AR interfaces is influenced by factors such as information overload, visual clutter, obtrusiveness, and distractions, all of which can negatively affect situational awareness, cognitive load, and performance [44, 7, 25, 35, 66]. For instance, in social contexts, poorly designed AR interfaces can obstruct communication by occluding facial expressions, leading to social isolation and privacy concerns [38, 37, 3, 33]. This has led to research on AR design principles for non-invasive AR and view management that provide socially relevant information while maintaining visual focus on interlocutors [41, 57, 19]. The RW Spatial setting and occlusion also significantly impact AR effectiveness [50, 48, 15, 24]. Extensive research has proposed AR design principles to address occlusion between RW and virtual objects [47, 63, 72, 8, 55]. These principles often emphasize view management techniques that ensure non-occlusive yet visible placement of virtual content [6, 4, 34, 54], as well as peripheral placement approaches [11, 53, 51]. Other strategies prioritize the real world, activating virtual content only when necessary through user-triggered adaptations, such as transparency, layout, or level of detail (LoD) [27, 26]. This work examines the underexplored impact of RW spatial settings and occlusions on AR effectiveness, focusing on user-specified AR adaptations in context-switching scenarios. An effective AR interface must include considerations such as positioning, transparency, and LoD for timely and appropriate information presentation in a given context. Pervasive AR envisions “continuous, universal, and omnipresent"" integration of AR content into various daily tasks [36]. Non-adaptive AR interfaces, which fail to adjust to user context, risk presenting information inappropriately. Achieving Pervasive AR requires a deep understanding of the user’s changing context, task, and environment to inform interface design. AR devices, equipped with sensors, offer unique opportunities for context detection, as seen in research assisting visually impaired users [56], highlighting AR’s potential to meet everyday information needs. Context-aware interfaces, such as ARWin and HoloDoc, propose design principles to optimize content presentation and enhance user experiences in specialized contexts using contextual data like user attention, spatial settings, lighting, and environmental cues [46, 68, 71]. However, an interface suitable in one scenario may be ineffective in another, as user priorities shift across contexts, requiring the interface to adapt accordingly [40, 23]. For instance, while providing relevant information to the user’s conversation can enhance user experience [65, 19], the user’s cognitive load may require low levels of information if they are engaged in a high-level collaborative task [59, 10]. User-triggered, context-aware interfaces allow manual selection of contextually-customized AR content presentation, offering predictability and control but increasing user effort [61, 60]. Recent research has focused on automatic adaptations to AR content without manual intervention, demonstrating the integration of unobtrusive context-aware AR into daily life [49, 45, 13]. To enable Pervasive AR, the interface must automatically detect and respond to contextual changes. However, current context-aware interfaces are limited in their adaptation scope, detect only a narrow range of contextual components, such as fatigue or social interactions, and use predefined design principles tailored to specific scenarios [12, 19]. This work investigates iAR interfaces capable of optimal adaptations in unfamiliar contexts without prior knowledge. Enabling iAR requires a framework that supports all potential AR adaptations, the detection and representation of the context, and its delivery to the decision-making process [58, 21]. Previous work identifies a design space of the potential adaptations to an AR interface [17]. Various approaches have been proposed for representing context, such as the 5 Ws (Who, What, Where, When, Why), or using categories like location, identity, time, and activity [58, 2, 62]. Others study specific sub-contexts such as physical environment [34, 13], objects [71, 42], background [52], and depth perception [30]. However, these taxonomies often overlook components essential to AR interface effectiveness, incorporate implicit elements that cannot be automatically detected or quantified by an iAR system, or lack clear frameworks for identifying implicit information essential for AR design. In this work, we reviewed existing research on context characterization and taxonomies and, through iterative refinement, identified key contextual information that influences AR performance. We propose a comprehensive taxonomy of quantifiable contextual components and a framework for iAR systems that use these components to infer the impact of various AR adaptations and make optimal adjustments in real-time."
https://arxiv.org/html/2411.02662v1,Interaction Design with Generative AI: An Empirical Study of Emerging Strategies Across the Four Phases of Design,"Generative Artificial Intelligence (Generative AI) holds significant promise in reshaping interactive systems design, yet its potential across the four key phases of human-centered design remains underexplored. This article addresses this gap by investigating how Generative AI contributes to requirements elicitation, conceptual design, physical design, and evaluation. Based on empirical findings from a comprehensive eight-week study, we provide detailed empirical accounts and comparisons of successful strategies for diverse design activities across all key phases, along with recurring prompting patterns and challenges faced. Our results demonstrate that Generative AI can successfully support the designer in all key phases, but the generated outcomes require manual quality assessments. Further, our analysis revealed that the successful prompting patterns used to create or evaluate outcomes of design activities require different structures depending on the phase of the design and the specific design activity. We derive implications for designers and future tools that support interaction design with Generative AI.","The user-centered design cycle is commonly modeled in four key phases, comprising requirement elicitation, conceptual and physical design, and evaluation (ISO 9241-210:2019). Thus far, human-centered design in large parts has been an activity performed by humans with humans, requiring considerable manual effort, time, resources, and methodological expertise. Recent advances in Generative Artificial Intelligence (Generative AI) hold promise to support the design process in ways that were solely attributed to humans so far. Of note, now widely available and easily accessible Large Language Models (LLMs) like ChatGPT (Brown et al., 2020), Bard111Google Bard: https://bard.google.com/chat (accessed 02/05/2024), and Copilot222Microsoft Copilot: https://copilot.microsoft.com/ (accessed 02/05/2024), encode both general and domain-specific knowledge that can be relevant for design processes, while powerful image-generation models like DALL-E (Ramesh et al., 2021) and Midjourney333Midjourney: https://www.midjourney.com/ (accessed 02/05/2024) hold promise for visual design tasks. Recent work highlights the potential of Generative AI to reshape the traditional human-centered design process (Schmidt, 2023; Schmidt et al., 2024). It was suggested that Generative AI might complement the design process, extend the range of existing methods for human-centered design, and in parts might ultimately even replace costly human work (Schmidt et al., 2024). Initial work started exploring Generative AI’s potential for specific design activities, such as for persona creation (Goel et al., 2023), or generating synthetic user data (Hämäläinen et al., 2023). However, only very little work has investigated how Generative AI tools can be used for interaction design from a holistic and more practical perspective. This aligns with prior work that highlights the difficulties of prompt design to steer the generated outputs and points to a lack of systematic research from an HCI perspective (Dang et al., 2022). Specifically, we currently lack a comprehensive understanding of Generative AI’s potential across the key phases of the design process, alongside a systematic investigation of successful usage strategies and comparison of how these strategies might be different or similar in different design phases. This is crucial to guide more targeted use of Generative AI in interaction design. This article presents empirical insights from a systematic exploration of Generative AI’s potential in the user-centered design cycle. These allow us to systematically compare different strategies and prompting patterns of Generative AI across the different phases of design. We conducted an eight-week study (n=10) within a graduate-level human-computer interaction course, in which participants used GPT-4 and DALL-E3 to explore how Generative AI can support interaction design for a given design task. The participants engaged in diverse design activities in a systematic procedure, covering the four design phases, and documented successful and failed approaches of how they used Generative AI. We evaluate selected AI-generated outcomes or artifacts through another user study (n=7), in which a new pool of participants was tasked to rate the quality, adequacy of provided detail, and relevance to the given design task. A qualitative analysis revealed that Generative AI offered helpful support to designers across all phases of user-centered design, spanning such versatile activities as analyzing usage contexts, creating design solutions and performing critical assessments not only of design solutions but also of evaluation methods. For each of them, we report on successful strategies identified by our participants. These strategies cover versatile design activities across different design phases, for instance, demonstrating how Generative AI was used to create a set of elaborated personas or to evaluate an experimental design. We further extend beyond prior work by providing in-depth empirical accounts of successful prompting patterns, including their key elements, example inputs and outputs, and by discussing the quality of the provided results. We conclude by discussing remaining challenges and implications for designers, for future tools and for the role of Generative AI in interaction design. The key findings of this article are: • Generative AI can effectively support the designer across various design activities and phases to create artifacts for design activities from scratch, to iteratively refine them, or to gather feedback on existing artifacts. Yet, the generated artifacts require manual refinement. Based on these insights, we propose a novel meta-design cycle that interleaves rapid AI-driven iterations and human guidance (see Figure 7). • We provide detailed empirical accounts and practical insights of successful examples for the requirements analysis, design and evaluation phases of user-centered design. We further discuss the AI-generated artifacts by evaluating their quality, the adequacy of the level of detail provided, and their relevance to the given design task. • We identify recurring prompting patterns of successful strategies. Our results reveal that design activities in earlier design phases benefited from single and oftentimes highly structured prompts, whereas iterative prompting proved helpful for advanced design phases. To gather feedback on existing artifacts, participants deployed single prompts that provided minimal additional information. Persona patterns (White et al., 2023) were deployed throughout various design phases, but with different purposes depending on the respective phase. • We identify challenges that our participants faced across the key phases of design, comprising stereotypical representation of users and scenarios in the early design phases, difficulties in generating functional designs and conveying technical or subjective information relevant to later design phases, and ethical concerns in using commercial Generative AI tools in interaction design. This research offers practical insights into the use of Generative AI in user-centered design. We envision our work to enrich our understanding of the potential of Generative AI and promote more effective and innovative design practices in interactive systems–a relevant consideration, given the rapid advancement of Generative AI capabilities and the need for a timely understanding of its potential impacts and risks. Thereby, this paper is targeted at designers, developers of AI tools, and researchers interested in practical applications of Generative AI."
https://arxiv.org/html/2411.02650v1,A Scoping Review of Functional Near-Infrared Spectroscopy (fNIRS) Applications in Game-Based Learning Environments,"This scoping review analyzes the use of Functional Near-Infrared Spectroscopy (fNIRS) in game-based learning (GBL) settings, providing a thorough examination of contemporary trends and approaches. This review aims to clarify the evolving role of fNIRS in GBL research, highlighting key trends and gaps in the literature to guide future studies in educational neuroscience by offering practical suggestions on device selection, study design, and data analysis. Employing the PRISMA framework, an initial collection of 956 articles was methodically screened, resulting in 18 research papers that satisfied the inclusion criteria. Each chosen study was assessed based on many criteria, including measurable outcomes, equipment characteristics, and study design. The review categorizes fNIRS-based GBL research into two primary types: cognitive response studies, which analyze how the brain function during tasks and comparative studies, which evaluate finding across different study materials or methods based on neural activities. The analysis includes learning platforms, gaming devices, and various fNIRS devices that has been used. Additionally, study designs and data collection methodologies were reviewed to evaluate their impact on research results. A comprehensive analysis outlines the specifications of fNIRS devices used in diverse studies, including yearly publication trends categorized by learning type, gaming equipment, fNIRS study classification, and outcome measures such as learning improvements and cerebral pattern analysis. Furthermore, the study design and analysis techniques are detailed alongside the number of studies in each category, emphasizing methodological trends and analytical strategies. The studies presented in this paper shows that GBL is as effective as traditional methods and it reduce cognitive load, enhance learning outcomes and cognitive engagement with fNIRS providing objective, real-time insights beyond self-reported questionnaires.","Learning is the process of constructing and modifying our knowledge, skills, strategies, beliefs, attitudes, and behaviors (Schunk, 2012), resulting in lasting improvements in our behavior or abilities (Gagné, 1985) through practice and experience (Shuell, 1986). These enduring changes exceed natural progression and emerge from intentional educational endeavors (Shuell, 1986; Gagné, 1985). Both implicit and explicit learning are essential for acquiring knowledge; explicit learning facilitates theoretical understanding, whilst implicit learning fosters the development of practical abilities (Schunk, 2012; Sun et al., 2005). To enhance student’s learning experience, educators are adapting and modifying their teaching methods and emphasizing on the importance of teacher-student relationship (Schunk, 2012). Learning in a game environment is popular in education because it includes both implicit and explicit learning (Pinheiro et al., 2024; Dewey, 1986; Papert, 2020) that improves engagement, motivation, and interactivity among learners (Miura et al., 2020; Holly et al., 2024; Gordillo et al., 2022; Krath et al., 2021; Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). Integrating gaming elements into educational activities enhances student engagement, resulting in increased information retention (Holly et al., 2024) and enhanced learning outcomes (Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). However, understanding how cognitive processes occur while learning in real-time remains a challenge. This is where Functional Near-Infrared Spectroscopy (fNIRS) can play a critical role to evaluate neural activity during learning (Lloyd-Fox et al., 2010; Skau et al., 2021). fNIRS is a non-invasive method of imaging the brain that detects hemodynamic response by calculating alterations in blood oxygenation levels (Kalanadhabhatta et al., 2022; Lloyd-Fox et al., 2010; Ning et al., 2024; Gao et al., 2024; Chance et al., 1993; Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). It allows researchers to monitor how the brain responds to educational stimuli during gameplay. Game-based learning (GBL), educational games, serious games, and gamification are terms frequently used interchangeably in studies to represent various approaches to learning within a game environment (Becker, 2021; Krath et al., 2021). In this paper we are going to use the term GBL to interpret the learning in the game environment. These types of learning environment facilitates active learning, offering instant feedback, and fostering a more fun and exciting learning atmosphere (Hosseini et al., 2019; Bonwell and Eison, 1991; Gordillo et al., 2022). The capacity for learning through a game both digital (e.g. video games: quiz-like games) or non-digital (board game) (Hwang and Chen, 2022) lies on the customization of educational experiences and the integration of adaptive feedback mechanisms to meet distinct learning needs. Nevertheless, it is important to acknowledge several challenges, like the possibility of being easily diverted, the danger of excessively depending on amusement, and the restricted application of acquired abilities in practical situations (Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). Traditionally, to assess the efficiency of learning methods researchers were dependent on tools like test score, self -reported measures, surveys, interviews, and observation (Davidesco et al., 2021; Varma et al., 2008; Samuels, 2009; Dingler et al., 2017). Solely depending on subjective metrics, such as self-reports, might introduce biases like social desirability and require thorough self-assessment from participants, leaving the data more complicated. On the other hand, objective measure like analyzing neural activity during learning can mitigate biases and provide more precise insights into individuals’ interactions with digital learning aids (Pinheiro et al., 2024; Varma et al., 2008; Samuels, 2009; Dingler et al., 2017). Besides fNIRS other brain imaging techniques such as electroencephalography (EEG) (Davidesco et al., 2023, 2021), and functional magnetic resonance imaging (fMRI) (Greipl et al., 2021), Near-Infrared Spectroscopy (NIRS) (Kober et al., 2020; Watson et al., 2023), have also been used to understand neural activities during learning. We chose fNIRS because it is more comfortable (Kalanadhabhatta et al., 2022), less affected by motion artifacts—allowing more movement than EEG and fMRI (Ninaus et al., 2014a; Lamb et al., 2018)—and suitable for all ages, including children and people with attention deficient hyperactivity disorder (ADHD), Developmental coordination disorder (DCD), or autism spectrum disorder (ASD) (Ayaz et al., 2022). fNIRS use technology of NIRS (Ayaz et al., 2022; Watson et al., 2023) to measure oxygen level by focusing only on brain activity and function where NIRS can be used on any tissues (Ayaz et al., 2022; Kober et al., 2020). This method operates by transmitting near-infrared light onto the scalp and quantifying the absorption of this light by oxygenated (Δ⁢H⁢b⁢OΔ𝐻𝑏𝑂\Delta HbOroman_Δ italic_H italic_b italic_O) and deoxygenated (Δ⁢H⁢b⁢RΔ𝐻𝑏𝑅\Delta HbRroman_Δ italic_H italic_b italic_R) hemoglobin (von Lühmann et al., 2024; Lamb et al., 2018; Lloyd-Fox et al., 2010; Ayaz et al., 2022). fNIRS has several advantages, such as its portability, relatively high temporal resolution, and low susceptibility to motion artifacts (Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). Neural activity observation while using learning materials such as game is becoming prominent to understand student’s engagement in study (Davidesco et al., 2023). This allows researchers to investigate the cognitive process behind learning and enhance our understanding of how the brain responds to educational stimuli (Lloyd-Fox et al., 2010; Skau et al., 2022). The fNIRS contributes significantly in the field of neural activity research in education sector because it supports longitudinal studies to enhance customized learning experience (Lloyd-Fox et al., 2010; Skau et al., 2022; Lamb et al., 2018) in different environments including classrooms and laboratories (Gao et al., 2024) that has. Evaluating neural activity yields significant insights into the brain’s information processing and the regions activated during learning tasks; however, the domain of brain signal analysis remains still developing, as cognitive processes are inferred indirectly from variables such as task performance and hemodynamic responses. Significant advancements have been made in research concerning the prefrontal cortex (PFC), which is crucial for higher cognitive functions such as decision-making and attention (Ninaus et al., 2014b; Kober et al., 2020; Doherty et al., 2023; Dingler et al., 2017). fNIRS can be used to understand how various game components influence cognitive processes, helping students become more engaged, make decisions, and enhance attention through real-time neural activity data. This strategy precisely measures the impact of various educational tools on cognitive development by assessing neural activity (Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). Thus, this strategy is now used to evaluate the efficacy of different educational tools including GBL. fNIRS generally can accommodate minor movements, so participants do not need to be strictly immobilized (Lloyd-Fox et al., 2010; Ning et al., 2024; Herold et al., 2018; Agbangla et al., 2017; Rudroff et al., 2018). fNIRS also allows longitudinal investigations of cognitive development, revealing how GBL affects long-term educational outcomes. Personalizing learning experiences based on cognitive responses can also improve student learning outcomes by making educational interventions more effective and engaging (Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). However, fNIRS has limited accuracy in identifying exact brain locations and only covers a small part of the brain’s surface. (Tenório et al., 2022; Kotsia et al., 2013; Wu et al., 2023; Tinga et al., 2020). Also, fNIRS is sensitive to abrupt or sudden head movements, resulting in sensor displacement that can create motion artifacts (Lloyd-Fox et al., 2010; Ning et al., 2024). In dynamic and interactive games, this is tough. Standardized protocols and methodologies are needed to achieve uniform and accurate outcomes across investigations. Collectively, GBL has demonstrated encouraging findings, its efficacy and effects on cognitive processes need for further investigation to understand the neural mechanisms behind how people interact with and gain from these digital learning assistance. Recent advancements in brain-imaging techniques have opened new avenues for understanding cognitive development in educational contexts. The fNIRS and its application in GBL have gained significant attention in the literature. The paper is structured as follows: in section 2, we review prior research on fNIRS and education. Methodology section (section 3) covers our motivation, search methods, and selection criteria in detail. In section 4, we outline our data collection and classification approach, including learning platforms, game devices, fNIRS devices, outcome measurements, and study types. section 5 examines trends and correlations across studies, while section 6 discusses methodologies and findings from selected papers, focusing on comparative and cognitive response studies in details. Finally, LABEL:sec_direction explores future directions, and we conclude with an overall summary (section 8)."
https://arxiv.org/html/2411.02607v1,Towards Context-Aware Adaptation in Extended Reality: A Design Space for XR Interfaces and an Adaptive Placement Strategy,"By converting the entire 3D space around the user into a screen, Extended Reality (XR) can ameliorate traditional displays’ space limitations and facilitate the consumption of multiple pieces of information at a time. However, if designed inappropriately, these XR interfaces can overwhelm the user and complicate information access. In this work, we explored the design dimensions that can be adapted to enable suitable presentation and interaction within an XR interface. To investigate a specific use case of context-aware adaptations within our proposed design space, we concentrated on the spatial layout of the XR content and investigated non-adaptive and adaptive placement strategies. In this paper, we (1) present a comprehensive design space for XR interfaces, (2) propose Environment-referenced, an adaptive placement strategy that uses a relevant intermediary from the environment within a Hybrid Frame of Reference (FoR) for each XR object, and (3) evaluate the effectiveness of this adaptive placement strategy and a non-adaptive Body-Fixed placement strategy in four contextual scenarios varying in terms of social setting and user mobility in the environment. The performance of these placement strategies from our within-subjects user study emphasized the importance of intermediaries’ relevance to the user’s focus. These findings underscore the importance of context-aware interfaces, indicating that the appropriate use of an adaptive content placement strategy in a context can significantly improve task efficiency, accuracy, and usability.","1 Related Work Previous work explored various innovative applications and design dimensions of XR to spatially place 2D & 3D objects and transition between them, visualize hierarchies, and provide persistent and portable presentation of the personal information [14, 20, 38, 16, 34]. Morrison et al. highlighted unique design elements within AR for enhancing accessibility for visually impaired children [45]. These studies underscore the broad design space of XR interfaces and the versatile and transformative applications that XR enables across various contexts. This work, investigates previous work and the design elements they utilized, providing a comprehensive XR design space. XR offers the potential to enable efficient information access, reduce cognitive load, and enhance user convenience compared to traditional methods such as mobile phones [55, 12, 43, 11, 40]. However, intrusive XR interfaces may result in challenges such as information overload and occlusion of important cues within the environment [31, 4], increase cognitive load and discomfort, and reduce the user’s situational awareness and performance [17, 25, 54]. Various approaches for intuitive and seamless integration of XR content into the environment have been extensively explored. For instance, to enhance efficiency and minimize intrusiveness, numerous designs adapt the XR content’s availability, transparency, placement, and Level of Detail (LoD) [15, 3, 46], as well as spatial layout and size [19, 9]. Lages & Bowman highlighted the significance of adapting the AR content placement strategy to avoid occlusions and accommodate activities like walking [33]. For adaptations to the XR content placement, the concept of the frame of reference, also referred to as fixation was introduced [20]. User-triggered adaptation through gaze, hand, and head-based inputs such as finger taps and handheld controllers are extensively explored for adjustments to the transparency, LoD, and spatial layout of XR content [19, 32, 50, 39, 48, 47]. In AR, for instance, many applications were designed to prioritize the real-world [11] by initially keeping the XR content hidden, in the peripheral, or at a lower LoD, and granting access to them through explicit interactions [49, 42, 46]. However, AR’s definition emphasizes the integration of the digital content into the real world [1], underscoring the significance of context awareness. While user-triggered adaptations offer control and predictability, they increase the user’s physical and mental workload of deciding when, what, and how to apply the adaptations [51]. Automatic XR adaptation can enhance efficiency and reduce workload compared to the user-triggered ones [11]. Numerous studies suggest rule-based approaches for XR adaptations. The significance of such rule-based adaptations in meeting the XR task requirements within various applications such as driving and conversation have been highlighted [4, 12]. To prevent occlusion issues, Ens et al. suggested a rule-based adaptive design to exclusively place the XR objects on empty surfaces [18]. Constraints, explicitly imposed by the users, were utilized as guidelines to group related XR objects together and prevent their occlusion within a rule-based view management [3]. Such rule-based adaptive approaches are highly tailored to specific use cases and applications.Even within the same application or use case, slight contextual deviations can cause a rule to fail, making it suitable only within unchanging contexts. This work proposes an adaptive placement strategy, applicable within changing contexts, to extract and utilize contextual information from the environment and user state to spatially place the XR content. Context refers to the external components that influence or relate to the user’s interactions with the interface [13]. In recent years, context-aware XR has become a focal point of research, promising the potential for “ubiquitous"" and “pervasive"" computing through AR [56, 26]. Contextual aspects such as user preferences, cognitive load, device profiles, task environment, semantic changes, and task-specific security parameters have been utilized for adaptations to the XR content’s appearance, LoD, frame of reference, and spatial layout [37, 9, 36]."
https://arxiv.org/html/2411.02601v1,Understanding Young People’s Creative Goals with Augmented Reality,"Young people are major consumers of Augmented Reality (AR) tools like Pokémon GO, but they rarely engage in creating these experiences. \chaddedCreating with technology gives young people a platform for expressing themselves and making social connections. However, we do not know what young people want to create with AR, as existing AR authoring tools are largely designed for adults. \chdeletedThe ability to create with today’s technology can enable people to more actively participate in society, yet we do not know what young people would like to create to express themselves. To investigate \chreplacedthe requirements for an AR authoring tool how to support creative character-based self-expression in AR, we ran eight design workshops with 17 young people in Argentina and the United States that centered on young people’s perspectives and experiences. We identified four ways in which young people want to \chaddedcreate with AR, \chaddedand contribute the following design implications for designers of AR authoring tools for young people: (1) \chreplacedBlending imagination into AR scenarios to preserve narrativesleveraging location, time to design AR interactions with the physical world, (2) \chreplacedMaking traces of actions visible to foster social presencemaking asynchronous multiplayer games that preserve contribution through visible digital traces (3) \chreplacedExploring how AR artifacts can serve as invitations to connect with othersrespecting autonomy when sharing creations through mimicking of social conventions in AR, and (4) \chreplacedLeveraging information asymmetry to encourage learning about the physical worldleveraging information asymmetry as learning opportunities. \chdeletedFrom these, we contribute four implications for designers to support young people in becoming creators of AR technology.","Young people (preteens and teenagers) today are frequently engaged in Augmented Reality (AR) activities (Radu and MacIntyre, 2009; Das et al., 2017; Paavilainen et al., 2017a; Hnatyuk, 2023). Many young people play AR games with location-based digital characters on Pokémon GO (Niantic, 2016; Paavilainen et al., 2017a), they adorn their videos with AR effects on TikTok (Tik, 2016), or they converse using face-altering AR filters on Snapchat (Sna, 2011). However, young people are merely given those AR experiences specifically for them to consume. They rarely engage in creating those AR experiences, leaving young people without a voice in shaping the future of AR experiences despite being major users (Hnatyuk, 2023).\chadded Though young people can articulate complex ideas on what they think about popular technologies, they are often not included as stakeholders when developing technology (Solyst et al., 2023). Motivated by this oversight, we were interested in involving young people as stakeholders and hearing their ideas on what they would do with an AR authoring tool designed for them. Although HCI researchers have tried to address this by developing tools for novices to more easily create AR experiences (Guo et al., 2019; Lunding et al., 2022), there is little research aimed at understanding what young people would like to create with AR\chadded, and how they would like to connect with other younger people in the process of creation. \chdeleted For people - including the younger members of our communities - to be active participants in society, they should be able to create with today’s digital technology (Resnick et al., [n. d.]; resnick2008sowing). Since the early days of computing, young people have enjoyed playing with digital characters, such as the turtle from Logo (Solomon, 1978), various animals in Moose Crossing (Bruckman, 1997), and sprites from Scratch (Resnick et al., 2009). More recently, the success of Pokémon GO reaffirmed young people’s interest in interacting with and controlling digital characters. Additionally, studies of desktop-based tools show that young people enjoy the freedom to express themselves in digital settings through creating scenarios such as animating characters, creating stories, and interacting with other users (Resnick et al., [n. d.]; Bruckman, 1997; Scratch Foundation, 2019). Expanding on this, we investigate how to leverage AR’s close ties between the digital and the physical to identify how young people can express themselves through character-based immersive experiences that connect with the world and people around them. We focus our investigation on character-based scenarios as it is an established medium through which young people enjoy interacting in digital spaces (Solomon, 1978; Bruckman, 1997). More specifically, we aimed to answer the following guiding question: • RQ: What types of playful, character-based experiences do young people envision they could craft with \chaddedan AR authoring tool? To answer our guiding question, we ran eight one-hour workshops with 17 children split into groups of 2 to 3 participants. We recruited participants aged 11–16 in the United States and Argentina. Participants completed a physical and digital design activity where they manipulated characters to brainstorm scenarios they would like to create. We recorded these sessions and analyzed them through thematic analysis. From our analysis, we derived the following findings, discovered \chaddedunderlying goals and values, and identified design opportunities for designers of AR \chreplacedauthoring toolstechnology. We found that young people: 1. \chreplaced Explored different ways to immerse their AR characters in the physical world using technical features and their imaginations.Want to realistically ground AR characters in the physical world, but they can use their imaginations to craft narratives that overcome the limitations of AR technology. 2. Created asynchronous scenarios anchored by location, but it is important to them to leave traces that make their contributions visible. 3. \chreplaced Fostered a sense of togetherness and friendship between AR characters through meetings in physical space.Are curious about other young peoples’ creations, but this curiosity must be balanced with respecting autonomy. This balance can be achieved by leveraging AR’s spatial ”discovery” to mimic social conventions. 4. \chreplaced Wanted information asymmetry between characters and users to create learning opportunities.Are interested in leveraging information asymmetry to craft social interactions, so young people should be able to use AR artifacts to discover and gather information about the world that is interesting to them. Hence, the contributions of this work to the CSCW community are two-fold. First, through running design workshops, we provide empirical insights into young people’s creative and \chaddedsocial goals \chaddedwhen creating with AR. Second, we propose design opportunities for \chreplacedfor build AR authoring tools that support the goals of young people."
https://arxiv.org/html/2411.02594v1,"“It’s a conversation, not a quiz”: A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health","This is an unpublished pre-print. Please check the authors’ websites for updated versions and publication information.Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as accessible information sources or communication tools across different domains. In public health—where stakes are high and impacts extend across populations—adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with health professionals and health issue experiencers to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: vaccines, opioid use disorder, and intimate partner violence. We synthesize participants’ perspectives into a risk taxonomy, distinguishing and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk in individual behaviors, human-centered care, information ecosystem, and technology accountability. For each dimension, we discuss specific risks and example reflection questions to help practitioners adopt a risk-reflexive approach. This work offers a shared vocabulary and reflection tool for experts in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm when they are used.","“I love new technologies but this one has me scared. I myself wouldn’t feel like I know enough to ask the right questions and to evaluate the impact of it.” — P5 (public health director) Recent breakthroughs in large language models (LLMs) have spurred widespread attention and rapid adoption across different fields. With their abilities to generate convincing human-like language on the basis of large sets of human-written content (Brown et al., 2020; Vaswani et al., 2017), LLMs hold the potential to influence how we interact with information. LLMs have quickly gathered hundreds of millions of active users (Reuters, 2024) and made their way into everyday products, sometimes even without users’ full awareness. Accordingly, public discourse has shown excitement about LLM assistance in various information-seeking and support tasks and even hype about the framing of LLMs as a “next-generation search engine.” 111https://ico.org.uk/about-the-ico/research-reports-impact-and-evaluation/research-and-reports/technology-and-innovation/tech-horizons-report/next-generation-search/ This technology enthusiasm extends into the health sector, where studies have presented evidence of LLMs’ competence in completing tasks such as clinical documentation (Thirunavukarasu et al., 2023; Zhang et al., 2024b), decision support (Thirunavukarasu et al., 2023; Tu et al., 2024), therapeutic conversations (Wang et al., 2021; Sharma et al., 2024b; Kim et al., 2024), and public health intervention (Jo et al., 2023, 2024; Karinshak et al., 2023). As a matter of fact, LLMs have already been incorporated into large-scale health systems. For instance, Epic has partnered with Microsoft to start integrating LLM tools into its electronic health record (EHR) software, which owns the largest market share of acute care hospitals in the U.S., to automatically draft clinical notes (Landi, 2023) and patient message responses (Landi, 2024). While this proliferation seems promising, experts and scholars caution against exaggerating LLM capabilities (Drogt et al., 2024; Wornow et al., 2023). Specifically, uncertainty about information quality is problematic, particularly regarding inaccurate information, biases, and harmful content (Zhou et al., 2023; Liang et al., 2022; Gehman et al., 2020). Given the high-stakes nature and data sensitivity of population-level health, it warrants careful deliberations of when it is appropriate to employ LLM capacities and how we can mitigate potential harm when we do. However, in practice, evaluating the risks of potential LLM adoption in public health remains challenging. The first gap lies in the lack of domain understanding, as the majority of risk taxonomies lack granularity to be applied for specific uses (Liao and Vaughan, 2023) or are created within the computer science community and tend to leave out domain experts and real users (Shelby et al., 2023; Weidinger et al., 2022; Bender et al., 2021). Second, when considering population-level impact, potential risks come in ecological “layers” from individuals to society, and conventional categorizations based on content types (e.g., misinformation, hate speech, and biases) are not sufficient to evaluate real-world cases (Scheuerman et al., 2021) while different types often intertwine with each other (Kim and Kesari, 2021; Gover et al., 2020). Identifying the presence of generated harmful content is only the first step; we must also contextualize low-quality information within specific health issues and relevant populations in order to evaluate the consequences and severity and mitigate potential harm. In response to these gaps, this paper is situated in three distinct and critical public health issues and grounded in the perspectives of both health professionals and members of the general public who might use such tools to seek health information. Specifically, we ask: what negative consequences might arise in adopting LLMs in public health for informational needs and support? We conducted focus group sessions with ten health professionals and ten health issue experiencers to uncover potential negative influences of using LLMs as a communication tool or information source in public health. We selected vaccines, opioid use disorder (OUD), and intimate partner violence (IPV) as topics for different sessions based on their significance across different dimensions of public health—infectious disease prevention, chronic and well-being care, and community health and safety—all demanding high-quality information with existing prevalent issues such as misinformation, biases, and sensitivity. The result is a risk taxonomy of potential adoption of LLM for public health. Our taxonomy consists of four dimensions of harm: individual behaviors, human-centered care, information ecosystem, and technology accountability (Fig 2). Within each area, we list specific risks and associated example reflection questions to help assist practitioners in both computing and health fields in becoming reflexive and risk-aware. This risk taxonomy makes several contributions. First, it gives a comprehensive and grounded list of possible risks in implementing LLMs for public health, and differs from pre-existing generic taxonomies by being grounded in public health issues and learning from domain experts and real users. To our knowledge, this is the first work to comprehensively explore potential risks of LLMs for public health. Second, by offering a shared vocabulary, it paves the path for future collaborations between experts in computing and health fields. As public interest starts to gather in LLMs as an emerging technology, it allows for careful and thorough reflections on potential negative consequences and prevents reckless adoption that could result in unbearable disruptions and real harms to individuals and communities and further erode trust in public health responses. Content Warning: We caution the readers that this paper discusses sensitive topics, including intimate partner violence and opioid use disorder. Some readers may find certain quotes and descriptions to be emotionally triggering."
https://arxiv.org/html/2411.02576v1,Designing and Evaluating Sampling Strategies for Multiple-Forecast Visualization (MFV),"With the growing availability of quantitative forecasts from various sources, effectively communicating these multiple forecasts has become increasingly crucial. Recent advances have explored using Multiple-Forecast Visualizations (MFVs) to display multiple time-series forecasts. However, how to systematically sample from a pool of disparate forecasts to create MFVs that effectively facilitate decision-making requires further investigation. To address this challenge, we examine two cluster-based sampling strategies for creating MFVs and three designs for visualizing them to assist people in decision-making with forecasts. Through two online studies (Experiment 1 n = 711 and Experiment 2 n = 400) and over 15 decision-making-related metrics, we evaluated participants’ perceptions of eight visualization designs using historical COVID-19 forecasts as a test bed. Our findings revealed that one sampling method significantly enhanced participants’ ability to predict future outcomes, thereby reducing their surprise when confronted with the actual outcomes. Importantly, since no approach excels in all metrics, we advise choosing different visualization designs based on communication goals. Furthermore, qualitative response data demonstrate a correlation between response consistency and people’s inclination to extrapolate from the forecast segment of the visualization. This research offers insights into how to improve visualizations of multiple forecasts using an automated and empirically validated technique for selecting forecasts that outperform common techniques on several key metrics and reduce overplotting.","Decision-making about future events is notoriously challenging, partly because humans struggle with understanding uncertainty [1]111In this paper, uncertainty refers to quantified uncertainty, which can be modeled and represented probabilistically.. As an example of the difficulty people have understanding uncertainty, several studies have found that the general public often misinterprets standard methods of visualizing uncertainty in weather forecasts [2, 3, 4, 5]. This difficulty underscores the need to design effective communication strategies in conveying uncertainty, particularly under high-stake decision-making scenarios. Fortunately, many research fields aim to support decision-making in high-risk scenarios and have developed various methods to facilitate uncertainty communication, such as utilizing multiple forecasts. The multiple forecasts approach involves collecting predictions from multiple independent groups to provide a comprehensive overview of potential future events [6], often communicated through visualizations (exemplified by the COVID-19 dashboard [7]). As more uncertainty communication shifts to using the multiple forecast strategy, it is crucial to rigorously test and refine the communication of multiple forecasts, ensuring that people understand the uncertainty during the complicated decision-making process. Recent research has explored a technique called Multiple-Forecast Visualization (MFV) [8] to communicate multiple forecast information. MFV presents multiple forecasts within the same space, giving readers insights into the range, shape, and concentration of the multiple forecasts. Prior works on MFVs of time-series forecasts have provided initial evidence that they can help users successfully predict future trends in alignment with actual outcomes [8, 9]. Additionally, the studies have proposed basic guidance on the optimal number of forecasts to display simultaneously to balance user prediction efficacy and trust in the forecasts. These findings are exciting, given the mixed results of prior attempts to improve decision-making using uncertainty visualizations [2, 3, 10, 11, 5]. Nevertheless, MFVs are an emerging approach to uncertainty communication, and there are numerous open research challenges concerning implementation and visualization design choices that best support decision-making. Broadly, the open challenges regarding MFVs fall into two interrelated categories. The first is establishing principled methods for selecting or aggregating forecasts, as prior work manually selected individual forecasts [8, 9], which are not generalizable to real-world contexts. New methods are needed to curate heterogeneous forecasts of varying qualities to meet specific design goals, such as helping users identify extreme events, understand the distribution of possible outcomes, and predict future events. The second challenge is visualizing these forecasts in a way that adheres to good visualization practices and meets the design goals. Given the expansive range of open questions regarding the design and implementation of MFVs, this work takes the first step by developing several methods for selecting forecasts to support users in predicting the actual outcome of an event, a crucial communication goal for any forecast visualization. Moreover, predicting future events involves more than just correctly extrapolating from a visualization; it also encompasses psychological aspects, such as the degree of trust a user has in the visualization. To advance the development of MFVs, this work aims to: 1) establish a principled method for selecting forecasts and 2) identify a visualization method that facilitates users’ ability to predict future outcomes. To achieve these goals, we developed two approaches inspired by the Gestalt cognition theory [12] for sampling forecasts. Then, we created three visualization designs to reflect the relative distribution of multiple forecasts (Sec. 3). Each design has different attributes and ideal use cases, which we discuss at length. We conducted two rounds of large-scale comprehensive evaluations where we compared the three candidate visualization designs with current state-of-the-art methods, as well as several control conditions (Experiment 1, n = 711 and Experiment 2, n = 400, Sec. 4). We rigorously tested responses across five unique forecast cases over 14 metrics that reflect key decision-making-related processes. Specifically, the metrics include accuracy in predicting future COVID-19 mortality rates [9], trust [13], surprise in response to the actual outcome [14], and task load [15, 16]. Through our analysis (Sec. 5), we found that no single approach outperforms others in all metrics we tested. Different communication goals could lead to different visualization design choices. Nevertheless, we identified a particularly effective technique that involves generating a cluster of forecasts and then randomly selecting a representative forecast from each cluster. This method performs well, eliciting the best judgment performance in predicting future outcomes on average while incurring minimal surprise. The primary contributions of this work include: • We developed two sampling strategies for MFVs to provide a principled methodology for filtering and presenting essential information from multiple forecasts. These methods reduce the overplotting that can occur with MFVs while supporting several performance goals. • We present comprehensive empirical evidence for the correlation between different design choices of multiple forecasts and human decision-making from four facets. We advise designers to choose multiple forecast designs according to their communication goal."
https://arxiv.org/html/2411.02408v1,AI on My Shoulder: Supporting Emotional Labor in Front-Office Roles with an LLM-based Empathetic Coworker,"Client-Service Representatives (CSRs) are vital to organizations. Frequent interactions with disgruntled clients, however, disrupt their mental well-being. To help CSRs regulate their emotions while interacting with uncivil clients, we designed Pro-Pilot, an LLM-powered assistant, and evaluated its efficacy, perception, and use. Our comparative analyses between 665 human and Pro-Pilot-generated support messages demonstrate Pro-Pilot’s ability to adapt to and demonstrate empathy in various incivility incidents. Additionally, 143 CSRs assessed Pro-Pilot’s empathy as more sincere and actionable than human messages. Finally, we interviewed 20 CSRs who interacted with Pro-Pilot in a simulation exercise. They reported that Pro-Pilot helped them avoid negative thinking, recenter thoughts, and humanize clients; showing potential for bridging gaps in coworker support. Yet, they also noted deployment challenges and emphasized the irreplaceability of shared experiences. We discuss future designs and societal implications of AI-mediated emotional labor, underscoring empathy as a critical function for AI assistants in front-office roles.","When we engage an organization for their product or service, our initial contact is with staff known as front-office workers or Client Service Representatives (CSRs). They are the first line of response for the organization. Unlike other roles within an organization, a CSR’s task involves frequent interactions with clients and individuals outside an organization (Hochschild, 1983). These interactions require CSRs to manage their emotions constantly to complete tasks. Essentially they exert emotional labor to appear professional (Hochschild, 1983). The crucial challenge for a CSR is engaging with a client who starts behaving uncivilly by communicating in a rude, aggressive, and emotionally charged way (Grandey et al., 2007). No matter the type of request, a CSR’s role is to resolve a client’s concern and comply with the adage, “the customer is always right.” Unfortunately, this leads to an emotional dissonance between what a CSR expresses and what they actually feel (Grandey, 2000). Ultimately, such workers are vulnerable to excessive stress and eventual burnout. Not only do CSRs report being emotionally depleted and detached, but they also report a lack of accomplishment (Brotheridge and Grandey, 2002). Clearly, CSRs play a critical role within the organization, but, we have witnessed little innovation in alleviating their emotional toll. Our paper investigates how AI-coworkers help CSRs regulate their emotions in the face of client incivility. The brunt of client incivility in front-office work makes it infamous for low satisfaction and high-turnover (Pienaar and Willemse, 2008). A fundamental solution to the emotional distress of this role is Emotional Regulation (ER) (Yang and Lau, 2019). Basically, ER is the process through which one rethinks a negative situation (Grandey, 2000). While a worker may be able to do this on their own, research shows that coworkers play an important role in supporting ER (Yang and Lau, 2019). A good coworker can read the emotional cues of CSR’s work tasks and provide suggestions to help minimize the brunt of an aggressive client. However, CSRs are increasingly adopting remote work setups (Haan and Holznienkemper, 2023), which dampens social support (Vacchiano et al., 2024). Meanwhile, organizational scientists are calling for digital interventions to support worker wellbeing at scale (Black et al., 2019). We answer this call by designing and evaluating, Pro-Pilot— a Large Language Model (LLM)–based AI assistant for on-task Emotional Regulation in front-office work. While generative AI is emerging as a potent tool to complement the informational load of many different roles, the HCI community lacks research to investigate their use in emotional labor. Our research demonstrates the efficacy of LLM-generated empathetic support and evaluates how such a tool can be situated in CSR interactions with uncivil clients to answer the following research questions: RQ I:: How appropriate are LLM–based empathetic support messages for CSRs in response to client incivility? RQ II:: What is the role of embedding LLM–based empathetic support into CSR’s emotional labor? We developed Pro-Pilot by leveraging domain knowledge on client incivility (Andersson and Pearson, 1999; Frey-Cordes et al., 2020; Cho et al., 2002), real-world complaint data (Axelbrooke, 2017), and recent advancements in LLM-powered cognitive change (Burger et al., 2021; Sharma et al., 2023) and simulation (Shaikh et al., 2023). To answer the first research question, we conducted data-generation and evaluation tasks with 259 CSRs to comparatively analyze Pro-Pilot and human-coworker support for a variety of client incivility situations (1(a)). To answer the second question, we used Pro-Pilot as a technology probe and conducted a mixed-methods simulation exercise to understand how real CSRs could include it in their client interactions by juxtaposing Pro-Pilot usage with CSR’s socio-organizational norms (1(b)). Consequently, we contribute: • Pro-Pilot: an interactive technological artifact to expose CSRs to client incivility and learn healthy, long-term emotional labor practices to improve their own health and support their coworkers. • Empirical evidence that LLM–based empathetic support can be engineered to adapt to— and express empathy in— various client-incivility scenarios (RQI). Our results demonstrate that Pro-Pilot’s messages are linguistically distinct from both zero-shot approaches and human–coworkers, and moreover, Pro-Pilot’s messages were perceived to be more empathetic on several dimensions including sincerity, actionability, and relatability. • End-user insight on the function of LLM–based empathetic support to scaffold them through emotional labor during uncivil interactions (RQ2). Our findings showcase Pro-Pilot’s process of redirecting negativity and making CSRs more self-efficacious. While CSRs envision Pro-Pilot addresses important opportunities in workplace social support, they also surfaced the challenges of Pro-Pilot emulating holistic human support. This paper has implications for re-imagining how AI-assistants for workers should be designed and also re-imagining the social norms and policies to accommodate these advancements. Reflexive Considerations. Front-office work has many stakeholders, including the employer and the clients. However, it is the CSR who bears the burden of repeated emotional labor (Grandey, 2000). The relationship between these stakeholders is asymmetric, as employers can replace personnel and clients can switch services, but the CSR does not possess the same mobility (Fisk and Neville, 2011). Following from recent works that take a worker-centered perspective (Das Swain et al., 2023a, 2024; Kawakami et al., 2023), our research aims to illuminate the worker who is disadvantaged. This paper focuses on the needs of CSRs and centers their perspective throughout the evaluation. Two authors have past experience in front-office roles and direct end-user servicing. Four authors are researchers in an organization that employs its own CSRs. They helped us access real CSRs (and their resources) to provide feedback on our study design. We recruited participants for this study outside their organization to capture perspectives from different organizational sectors. All evaluations described in the paper were approved by the IRB of the first author’s institute. (a) Technical evaluation: Compared Pro-Pilot’s support messages with those produced by human-coworkers in CSR roles. \Description Study overview for technical evaluation. (b) User evaluation: Studied participant experiences with Pro-Pilot’s emotional support while interacting with uncivil clients. \Description Study overview for user evaluation. Figure 1. Schematic figures showing an overview of our study design."
https://arxiv.org/html/2411.02405v1,Accuracy nudges are not effective against non-harmful deepfakes,"I conducted a preregistered survey experiment (n=525) to assess the effectiveness of “accuracy nudges” against deepfakes (osf.io/69x17). The results, based on a sample of Colombian participants, replicated previous findings showing that prompting participants to assess the accuracy of a headline at the beginning of the survey significantly decreased their intention to share fake news. However, this effect was not significant when applied to a non-harmful AI-generated video. Keywords: Misinformation, Deepfakes, Artificial Intelligence, Nudges.","Shifting attention to accuracy through nudges has been proposed as a scalable intervention to reduce the spread of misinformation online. These interventions have proven effective in reducing the spread of false information related to health and political issues (Pennycook et al., 2021b, ; Pennycook and Rand,, 2019; Offer-Westort et al.,, 2023). However, recent advances in generative artificial intelligence have made these systems capable of creating increasingly credible messages, known as deepfakes, raising concerns about their potential use for influence operations (Goldstein et al.,, 2023; Dufour et al.,, 2023). How well do accuracy nudge interventions work on deepfake sharing intentions? This study aims to answer this question using a simplified version of the experimental method proposed by Pennycook et al., 2021a ."
https://arxiv.org/html/2411.03292v1,Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?,"Converting webpage design into functional UI code is a critical step for building websites, which can be labor-intensive and time-consuming. To automate this design-to-code transformation process, various automated methods using learning-based networks and multi-modal large language models (MLLMs) have been proposed. However, these studies were merely evaluated on a narrow range of static web pages and ignored dynamic interaction elements, making them less practical for real-world website deployment.To fill in the blank, we present the first systematic investigation of MLLMs in generating interactive webpages. Specifically, we first formulate the Interaction-to-Code task and build the Interaction2Code benchmark that contains 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories. We then conduct comprehensive experiments on three state-of-the-art (SOTA) MLLMs using both automatic metrics and human evaluations, thereby summarizing six findings accordingly. Our experimental results highlight the limitations of MLLMs in generating fine-grained interactive features and managing interactions with complex transformations and subtle visual modifications. We further analyze failure cases and their underlying causes, identifying 10 common failure types and assessing their severity. Additionally, our findings reveal three critical influencing factors, i.e., prompts, visual saliency, and textual descriptions, that can enhance the interaction generation performance of MLLMs. Based on these findings, we elicit implications for researchers and developers, providing a foundation for future advancements in this field. Datasets and source code are available at https://github.com/WebPAI/Interaction2Code.","As the Internet continues to evolve and expand, more and more websites emerge, contributing to the diverse and ever-growing online world. As of 2024, the digital landscape comprises approximately 1.09 billion websites (sta, 2024), supporting a variety of applications in people’s daily lives. The design and development of Graphical User Interfaces (GUIs) are vital for creating a website. A well-designed GUI not only enhances the website’s visual attractiveness but also improves usability and user satisfaction. In such a process, GUI design involves shaping the website’s aesthetics(UIt, 2024), such as layout, colors, and typography (Chen et al., 2018a; Kuusinen and Mikkonen, 2013). In contrast, GUI development is about implementing that aesthetic through programming languages. Nevertheless, such conversion is a complex and time-consuming task. Developers must manually map visual elements to their corresponding implementation details, which can lead to errors and discrepancies between the original design and the final looks (Chen et al., 2018a; Nguyen and Csallner, 2015a; Lelli et al., 2015; Moran et al., 2018b; Zeidler et al., 2013). To allow developers to transform design diagrams into functional GUI code more easily, several automated GUI code generation methods have been proposed, which can be further categorized into two types: learning-based and LLM-based approaches. The learning-based methods, such as Pix2code (Beltramelli, 2018), design a novel method based on CNN and LSTM to generate user interface code by reverse-engineering a single GUI image input. Chen et al.(Chen et al., 2018b) present a neural machine translator to extract visual features in UI images, encode these features’ spatial layouts, and generate GUI skeletons in a unified neural network framework. However, these deep learning-based methods exhibit compromised performance and fail in generalizing to diverse web page elements due to their limited knowledge learning from training samples. Recently, incorporating visual information into Large Language Models (LLMs) has led to the development of Multimodal Large Language Models (MLLMs) (Yin et al., 2023; Chen et al., 2022b; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Li et al., 2023b). Leading models in this domain, such as GPT-4o (OpenAI, 2024a), Claude-3.5 (Anthropic, 2024), and Gemini-1.5 (Google, 2024), have achieved excellent performance in visual understanding tasks (Yang et al., 2023; Dai et al., 2023). Furthermore, research has shown that LLMs have remarkable performance on various code intelligence tasks (Hou et al., 2023), including code generation (Yu et al., 2023; Li et al., 2023a; Du et al., 2023; Dong et al., 2023; Jiang et al., 2023; Gilbert et al., 2023), code completion (Nijkamp et al., 2022; Ding et al., 2023; Dibia et al., 2022; Li et al., 2023c, 2022; Chen et al., 2021), and code summarization (Mastropaolo et al., 2021, 2022; Gu et al., 2022; Chen et al., 2022a; Gao et al., 2023; Arakelyan et al., 2023). These advances create new opportunities for the Design-to-Code task, i.e., generating code from screenshots to replicate web page elements, layout, text, and colors. For example, Design2Code (Si et al., 2024) designs three types of prompts to stimulate MLLMs’ web content understanding and self-refined capabilities for GUI code generation. DCGen (Wan et al., 2024) proposes a divide-and-conquer-based approach to prompt MLLMs to generate webpage elements more accurately. Regardless of the continuous investigation on promoting the models’ capability, their evaluation scope is restricted to static pages. More specifically, existing research (Si et al., 2024; Yun et al., 2024; Gui et al., 2024) only focuses on the static appearance of the webpage (e.g., color, layouts), ignoring the dynamic interactive properties and functionality of elements, such as size selection list, quantity adjustment button shown in Fig. 1(a), and other designs for user engagements. Additionally, we observe that such interactive elements account for a large proportion of the webpage in real-world software practices. We randomly select 10 real-world webpages with different topics to analyze the ratio of interactive elements, the results in Fig. 1(b) indicate that interactive elements take up more than 50% cases. Then we utilize GPT-4o (OpenAI, 2024a) to generate the GUI code containing interactive elements. As shown in Fig. 1(c), fewer than 15% of interactive elements are correctly implemented, highlighting the current limitations in handling webpage interactive design. (a) Example of interactive elements. (b) Ratio of interactive and static elements. (c) Implemented vs. unimplemented interactive elements ratio of GPT-4o. Figure 1. Interaction example and interactive elements ratio of different types of webpages. Static webpages inherently limit user interaction with web elements, hindering access to new content (such as browsing images via carousel buttons) or impeding task completion (like selecting clothing sizes from drop-down menus), thereby impairing user experience. In this context, evaluations of static pages become inadequate for real-world webpage deployments, where dynamic elements are prevalent. Therefore, We argue that a benchmark for webpages that includes interactive elements is essential to enhance the practicality, usability, and user engagement of studies on auto-generated GUI code. In this paper, we emphasize the importance of webpage interactions by investigating the following question: to what extent MLLMs can produce interaction code based on the visual design? To this end, we provide a systematic analysis of MLLMs’ capability in reproducing dynamic interactions on webpages. Specifically, we first define the Interaction-to-Code task, i.e., generating code from a series of screenshots representing webpage interactions to replicate interactive elements. Then we build the Interaction2Code benchmark that encompasses a diverse array of webpages and interactions. It comprises 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories. By curating a wide range of interaction types, we offer a representative and diverse evaluation dataset for assessing the capabilities of MLLMs producing dynamic webpages in a more realistic scenario. We mainly investigate the following six research questions (RQs): Table 1. Summarization of key findings. Aspects Findings Limitations MLLMs exhibit limited performance in reproducing fine-grained interaction features, such as structure, text, and position (Finding 1). Performance based on the type of interaction: MLLMs excel at handling interaction with fixed pattern (e.g., selection list) and clear changes (e.g., new window creation), while struggle with interactions that involve complex changes (e.g., iframe, progress) and subtle visual modifications (Finding 3). Failure Types The predominant failures are “No interaction”, “Partial implementation”, “Interactive element missing”, and “Wrong position after interaction”. The most critical failures include “Interactive element Missing”, “Effect on wrong element”, “Wrong Function” and “No interaction” (Finding 4). Key Factors ★Well-designed prompts are effective: Chain-of-Thought enables step-by-step interaction analysis, while marking interaction areas provides essential visual signals. Both approaches improve the quality of generated interactions (Finding 2). ★ Enhanced visual saliency significantly improves interaction generation, particularly in complicated cases (Finding 5) ★Supplementary textual descriptions substantially boost MLLMs’ interaction generation capabilities (Finding 6). • RQ1: How do different MLLMs perform in Interaction-to-Code task under different prompts? • RQ2: How do humans evaluate the usability of interactions generated by MLLMs? • RQ3: How do MLLMs perform in code generation across different interaction scenarios? • RQ4: What types of mistakes do MLLMs make in generating interactions? • RQ5: How does visual saliency influence the quality of generated interactions? • RQ6: Which representation modality – visual signals or textual description, enhances MLLMs to generate interaction code? To address RQ1, we design three distinct prompt types: direct prompts, Chain-of-Thought prompts, and Mark prompts (which mark the interaction areas) to evaluate the performance of three state-of-the-art MLLMs under varying prompt conditions. For RQ2, we conduct user studies where participants interact with the generated webpages to assess the usability. In RQ3, we analyze MLLMs’ performance across different interaction scenarios by calculating usability rates for various interaction types. To answer RQ4, we invite human annotators to categorize and discuss webpage generation failures, followed by data analysis to reveal the most prevalent error patterns and their severity. For RQ5, we evaluate the generated interactions across varying saliency levels to investigate their impact on interaction generation performance. Finally, for RQ6, we examine the influence of interaction representation modality by comparing three input configurations: visual-only, textual description-only, and combined visual-textual. Based on our experimental results, we present six key findings, shown in Table 1, including the limitations of MLLMs, failure types and key factors for enhancing interaction generation performance. Our contributions are summarized as follows: • Task formulation. To the best of our knowledge, this is the first study to formulate the Interaction-to-Code task and present a systematic study on the code generation capabilities of MLLMs for dynamic interaction of webpages. • Benchmark. We build the first real-world webpage interaction datasets Interaction2Code containing 97 webpages and 213 interactions, spanning 15 webpage topics and 30 interaction categories. • Key Findings. Our in-depth analysis reveals the limitations of MLLMs, identifies 10 representative failure types and their underline cause, and provides key factors for enhancing performance on the Interaction-to-Code task. This key findings offer valuable implications for researchers and developers engaged in automated front-end development."
https://arxiv.org/html/2411.02851v1,"Learning to Unify Audio, Visual and Text for Audio-Enhanced Multilingual Visual Answer Localization","The goal of Multilingual Visual Answer Localization (MVAL) is to locate a video segment that answers a given multilingual question. Existing methods either focus solely on visual modality or integrate visual and subtitle modalities. However, these methods neglect the audio modality in videos, consequently leading to incomplete input information and poor performance in the MVAL task. In this paper, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method that incorporates audio modality to augment both visual and textual representations for the MVAL task. Specifically, we integrate features from three modalities and develop three predictors, each tailored to the unique contributions of the fused modalities: an audio-visual predictor, a visual predictor, and a textual predictor. Each predictor generates predictions based on its respective modality. To maintain consistency across the predicted results, we introduce an Audio-Visual-Textual Consistency module. This module utilizes a Dynamic Triangular Loss (DTL) function, allowing each modality’s predictor to dynamically learn from the others. This collaborative learning ensures that the model generates consistent and comprehensive answers. Extensive experiments show that our proposed method outperforms several state-of-the-art (SOTA) methods, which demonstrates the effectiveness of the audio modality.","With the rapid expansion of the internet, an increasing number of users are turning to online platforms to seek medical advice by posing natural language questions (O’Donnell et al., 2023; Lim et al., 2022). Current online platforms typically fall into two categories: those that provide textual answers, which may be difficult for users to interpret, and those that offer visual answers, which are generally more intuitive and easier to follow (Tang et al., 2021b). However, the retrieved videos often contain substantial amounts of information irrelevant to the user’s query (Moon et al., 2023), which significantly hinders the efficiency of information retrieval (Zhang et al., 2023). In response to this challenge, the task of Visual Answer Localization (VAL) has been introduced (Weng and Li, 2023). Figure 1: (a) Overview of the audio-enhanced multilingual video answer localization task. (b) Difference between existing methods and our method. (c) Performance comparison diagram of visual-based, textual-based, and our method. Existing VAL approaches can be broadly categorized into visual-based (Tang et al., 2021a; Chen et al., 2020a) and textual-based methods (Li et al., 2023a; Weng and Li, 2023; Li et al., 2024b). Visual-based methods are effective in scenarios where subtitle text is sparse, but their performance tends to degrade significantly in other contexts. In contrast, textual-based methods excel when abundant subtitle text is available, as the semantic similarity between the question and subtitle is typically greater than between the question and the video (Li et al., 2024b). However, these methods often overlook audio, which plays a crucial role in complementing both visual and textual modalities. There is inherent consistency and complementarity among these modalities (Chen et al., 2023a), and harnessing this synergy can enhance both visual and textual modalities by integrating information from the audio. Incorporating audio thus addresses the performance limitations in VAL, particularly in video segments lacking subtitles (Liu et al., 2022; Chen et al., 2020b; Sun et al., 2024). To this end, we study the Audio-enhanced Multilingual Visual Answer Localization (AMVAL) which aims to locate video segments that answer a user’s natural language question, in either Chinese or English. By providing video segments with verbal explanations for medical guidance, this approach not only facilitates the learning of specific actions but also helps bridge language barriers, making the content accessible to people who speak different languages (Macedonia and Knösche, 2011; Diamond et al., 2020). However, a significant challenge lies in effectively integrating the three modalities and fully utilizing their individual strengths to tackle the AMVAL task. To address this challenge, we propose a unified Audio-Visual-Textual Span Localization (AVTSL) method for AMVAL, aimed at reducing cross-modal discrepancies and improving the accuracy of span localization by integrating audio modality. We designed a network architecture with three modality channels (audio, visual, and textual) to fully leverage the semantic information from each modality in the video, addressing the limitations of single-modality methods. Each channel is equipped with a corresponding predictor: an audio-visual predictor, a visual predictor, and a textual predictor. During joint training, distinct objectives are assigned to each predictor, enabling them to leverage the unique strengths of their respective modalities. To improve modality integration, we introduce an Audio-Visual-Textual consistency module, which employs a Dynamic Triangular Loss (DTL) function based on Intersection over Union (IoU). This loss function aligns the modalities by minimizing the discrepancies between each predictor’s output and the target answer, as well as between the outputs of the other two predictors. Our approach promotes mutual learning among the predictors to achieve consistent and cohesive multimodal representations. Our contributions are as follows: (1) We study the AMVAL and propose the AVTSL method, which is the first to introduce the audio modality for the AMVAL; (2) We designed an Audio-Visual-Text Consistency module, which leverages the consistency and complementarity between different modalities using the DTL loss function; (3) We conducted extensive experiments to demonstrate the effectiveness of the AVTSL method, where our method outperformed other state-of-the-art (SOTA) methods by incorporating the audio modality."
https://arxiv.org/html/2411.02812v1,NEOviz: Uncertainty-Driven Visual Analysis of Asteroid Trajectories,"We introduce NEOviz, an interactive visualization system designed to assist planetary defense experts in the visual analysis of the movements of near-Earth objects in the Solar System that might prove hazardous to Earth. Asteroids are often discovered using optical telescopes and their trajectories are calculated from images, resulting in an inherent asymmetric uncertainty in their position and velocity. Consequently, we typically cannot determine the exact trajectory of an asteroid, and an ensemble of trajectories must be generated to estimate an asteroid’s movement over time. When propagating these ensembles over decades, it is challenging to visualize the varying paths and determine their potential impact on Earth, which could cause catastrophic damage. NEOviz equips experts with the necessary tools to effectively analyze the existing catalog of asteroid observations. In particular, we present a novel approach for visualizing the 3D uncertainty region through which an asteroid travels, while providing accurate spatial context in relation to system-critical infrastructure such as Earth, the Moon, and artificial satellites. Furthermore, we use NEOviz to visualize the divergence of asteroid trajectories, capturing high-variance events in an asteroid’s orbital properties. For potential impactors, we combine the 3D visualization with an uncertainty-aware impact map to illustrate the potential risks to human populations. NEOviz was developed with continuous input from members of the planetary defense community through a participatory design process. It is exemplified in three real-world use cases and evaluated via expert feedback interviews.","Our Solar System is filled with asteroids and comets, making it inevitable that a large object will impact Earth. An estimated 1 trillion objects orbit the Sun, out of which around 1.3 million asteroids and 4,000 comets have been discovered so far. The number of newly identified small bodies has been growing exponentially in recent years [11]. Of particular interest are the near-Earth objects (NEOs). A NEO is a comet or an asteroid whose trajectory brings it close to Earth’s orbit around the Sun. One of the most significant NEOs in history is the Chicxulub impactor, an asteroid with an approximate diameter of 10 km. It struck Earth about 66 million years ago, ending the age of the dinosaurs, marking one of the most devastating events in the history of life on Earth [33]. Currently, we know of roughly 75 asteroids of comparable or greater size that would have civilization-ending consequences if they were to collide with Earth. More recently, the Tunguska event in 1908 flattened around 2,000 km2superscriptkm2\textrm{km}^{2}km start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT of forest and was caused by an asteroid estimated to be 50-60 m [10]. In 2013, the 20 m Chelyabinsk meteor exploded over Chelyabinsk, injuring over 1,600 people. Planetary defense is concerned with detecting potential impactors and devising strategies for their mitigation. The risks posed by these potential impactors are particularly prevalent if the asteroids airburst in or near densely populated areas, which can lead to catastrophic loss of life. We develop NEOviz as a visualization system that enables uncertainty-driven analysis of asteroid trajectories using a participatory design process that involves collaboration with planetary defense experts from the B612 Foundation. The foundation’s mission is to discover and track potentially hazardous NEOs, raise awareness of their risks, and devise strategies to mitigate their impact. When observing an asteroid, it is much easier to infer its (x,y)𝑥𝑦(x,y)( italic_x , italic_y ) position from a telescope image than the depth in the line-of-sight direction z𝑧zitalic_z. This introduces inherent asymmetric uncertainty into the orbit fitting and propagation process, which is challenging to visualize. The limited number of observations, combined with the fact that most observations can only occur at night and can take place over several months, leads to a scarcity of data when accurately reconstructing the trajectory of an asteroid, introducing another source of uncertainty. While the quantification of such uncertainty using, for example, covariance matrices, at specific time points and corresponding mitigation strategies has been well-researched [39, 30], the visualization of its progression over time and its impact on Earth remains largely unexplored. An effective and accessible tool for communicating the complex, time-varying 3D uncertainty could be invaluable not only for scientific research but also for public outreach and education. Designing an informative uncertainty representation for asteroid orbit trajectories presents several challenges, including visual clutter, time-varying uncertainty, and significant variability in both time and space. Since the impact probabilities for any given asteroid are fairly low (for example 2.7% for (99942) Apophis), we can only obtain a representative ensemble of an asteroid’s potential trajectories by drawing a large number of samples from its uncertainty distribution. This process creates inevitable visual clutter and heavy computational cost, as many thousands of orbits must be displayed, with only a few trajectories impacting Earth. Additionally, as the uncertainty evolves over time, it is crucial to visualize its progression instead of a collection of static snapshots, which requires a smooth transition between consecutive time steps. Finally, there is large variability in both the magnitude of the uncertainty and the corresponding time period as some uncertainty propagation could span several decades, while others only involve a few hours of data. For some asteroids, their orbit uncertainty could exceed the diameter of the Earth, while for others, their impact point can be predicted accurately. Therefore, we strive to design our visualization system to accommodate and adapt to these wide ranges of scenarios. In response to these challenges, we developed NEOviz, a visualization system that supports the analysis of individual asteroid trajectory uncertainties. NEOviz is integrated into the astrovisualization engine OpenSpace [6], which provides the necessary spatial context for asteroid observations. The resulting visualization system offers capabilities surpassing currently available methods, enabling experts to gain important scientific and operational insights into an asteroid’s potential movements. The major components of NEOviz include: • Given an ensemble of an asteroid’s trajectories, we present the Uncertainty Tube, a spatiotemporal representation of the volume encompassing all sampled asteroid trajectories. It provides a time-varying visualization of the physical variability of the ensemble. • At the cross-sections of the Uncertainty Tube, Interactive cut-planes shows the sample distribution at particular time steps. For each time step, the user can browse the cut-planes and examine each distribution in detail along the Uncertainty Tube. • The Impact Map visualizes the likelihood of impact locations on Earth together with an estimated risk analysis for each location. The rest of the paper is structured as follows. Section 2 discusses prior work on ensemble and uncertainty visualization focusing on curve-based and 3D uncertainties, and then surveys NEO-related visualization tools. Section 3 provides technical knowledge about the unique characteristics of this uncertain data. We describe our system components in detail in Section 4 and three case studies in Section 5. Finally, we summarize the results of four qualitative interviews with planetary defense experts in Section 6 and discuss limitations and future work in Section 7."
https://arxiv.org/html/2411.02353v1,Social-RAG: Retrieving from Group Interactions to Socially Ground Proactive AI Generation to Group Preferences,"AI agents are increasingly tasked with making proactive suggestions in online spaces where groups collaborate, but can be unhelpful or even annoying, due to not fitting the group’s preferences or behaving in socially inappropriate ways. Fortunately, group spaces have a rich history of prior social interactions and affordances for social feedback to support creating agents that align to a group’s interests and norms. We present Social-RAG, a workflow for grounding agents to social information about a group, which retrieves from prior group interactions, selects relevant social signals, and then feeds the context into a large language model to generate messages to the group. We implement this into PaperPing, our system that posts academic paper recommendations in group chat, leveraging social signals determined from formative studies with 39 researchers. From a three-month deployment in 18 channels, we observed PaperPing posted relevant messages in groups without disrupting their existing social practices, fostering group common ground.","AI-enabled agents are increasingly deployed into online social environments such as group conversational and collaborative spaces as assistants, facilitators, or collaborators to support varied tasks such as conversation summarization (Zhang and Cranshaw, 2018a), brainstorming (Rayan et al., 2024), and conflict resolution (Govers et al., 2024; Tessler et al., 2024). Integrated into part of the group context, these agents insert automated messages into group spaces where human-human conversation and interaction naturally occur. While many are reactive agents, which only act upon explicit human instructions, recent research and commercial tools have also started exploring proactive agents that can act without explicit requests. Compared to reactive interactions, proactive agents open up the unique opportunities of providing useful information beyond those explicitly requested by users (Avula et al., 2022) and encouraging more active and balanced user participation (Do et al., 2022; Dennis et al., 2023). For instance, CoExplorer, which preemptively adjusts the window layout of online meetings based on predicted meeting phases and goals, shows potential to help groups stay on track and reduce workload (Park et al., 2024). Another study found evidence that an AI agent that proactively posts text-based cues in brainstorming meetings could inspire new directions of the group discussions (Rayan et al., 2024). Despite their promises, proactive AI interjections could easily veer into being unhelpful or even annoying to the group members who are using the same space to communicate with each other (Cai et al., 2024; Avula et al., 2022). Indeed, the lack of explicit user instruction to proactive agents could also lead to ambiguity in users’ preferences, resulting in irrelevant interjections (Zhang et al., 2024a). Proactive agent interjection at inappropriate times during group communication could also cause distraction (Peters et al., 2017). Rayan et al. observed that even when human facilitators play the role of AI in a wizard-of-oz study, almost half of the AI interjections were dismissed or ignored by group members since they were irrelevant or not timely (Rayan et al., 2024). As a result, without thoughtful integration into a group’s social environment, AI-generated messages may be intrusive or ignored and eventually lead to system abandonment (Avula et al., 2022). While posing challenges for agents’ appropriate integration, the rich human-human interactions that already exist and the subsequent human-agent interactions within social spaces could also be a unique asset for AI agents, providing exciting opportunities for systems to learn and adapt themselves to users’ preferences. Success in leveraging these opportunities can enable us to build socially-grounded AI agents that are able to proactively initiate relevant interaction in groups without disrupting existing group dynamics. For example, the past and ongoing interactions in the social space could inform agents to post messages with relevant information to both individual members and the group as a whole, to interact with members in the style and tone that adhere to the social norm of the group, and to time the interaction in appropriate time and cadence. The emerging capabilities for AI models (e.g., Large Language Models) to understand and generate human-like text also afford agents to post messages with flexible styles beyond scripted responses (Yang et al., 2024), though it’s still unclear how to scaffold the AI models to generate messages that are aligned with the preferences and norms of a group without frequent explicit instruction (e.g., users’ prompting in each session). In this work, we consider how AI agents that proactively post messages into a group’s social space can take advantage of the existing social contexts (e.g., previous Slack conversations between group members) to better align with the group’s interests and social norms. We propose Social-RAG, a workflow for AI agents using Large Language Models (LLMs) that takes social signals gleaned from prior interactions between group members and between members and the AI agent as context for aligning the agent’s generation to better match the group. Instead of requiring groups to painstakingly input their explicit preferences at the onset of their experience or before each suggestion, we can bootstrap the agent by taking advantage of the fact that the spaces in which groups interact typically store a rich history of social interactions between group members and also contain a multitude of affordances for members to give each other social feedback. This allows for repurposing the existing social affordances for group interaction (e.g., reactions and replies) to enable group members to give additional and ongoing feedback to the agent. Much like how knowledge bases in traditional retrieval-augmented generation (RAG) pipelines must curate their database of facts to be useful as context, we describe strategies for parsing and retrieving from messy social interaction data to glean “social facts” about a group. Unlike RAG, which retrieves from knowledge bases as context to improve an LLM’s factual grounding (BehnamGhader et al., 2022), Social-RAG retrieves from data sources containing social information about a group (e.g., the group’s chat history) to improve an LLM’s social grounding. To test our Social-RAG workflow in a real-world online social environment, we built PaperPing, an AI agent that proactively posts messages containing scholarly paper recommendations and LLM-generated explanations into group chat spaces dedicated to research labs or projects. We chose this setting as a test bed due to the collaborative nature of scientific research (Kraut et al., 1986; Jirotka et al., 2013), where exchanging and discussing scholarly articles is both common and a prerequisite of conducting high-quality research (Jirotka et al., 2013). The rapidly growing volumes of academic research publications also lead to researchers’ increasing need to identify relevant research papers. Indeed, in two formative studies with 39 researchers, we found that researchers frequently recommend papers in group chats by targeting a specific member or relating them to prior conversations. They also leverage affordances such as emoji reactions and threaded replies to provide feedback on each other’s recommendations. Besides creating a rich repository from which to learn people’s interests, this existing social behavior also informed the design of PaperPing. Instantiating steps described in the Social-RAG workflow, PaperPing collects and analyzes prior communication history (both between group members and between members and the agent) to learn group preferences, enabling the selection of relevant papers and the tailoring of its LLM-generated explanations to emphasize aspects of recommended papers that are relevant to the group. PaperPing was deployed for three months in 18 pre-existing Slack channels with various types of prior dynamics (e.g., types and frequency of messages) and exposure to over 500 users in total, allowing us to evaluate its performance in a real-world setting over time. We find that PaperPing effectively learns researchers individual and group preferences with their minimum upfront effort, leveraging existing feedback signals in groups such as link sharing and emoji reactions. PaperPing’s LLM-synthesized messages were considered to be contextually relevant, outperforming paper content summaries in helping participants understand the relevance of the paper recommendation with their groups. Moreover, PaperPing fits into existing practices within groups without disruption to group dynamics and fosters common ground within groups. However, the real-world deployment also surfaced PaperPing’s limitations, such as in balancing group and individual preferences and reflecting the shift of group interests in a timely manner, which point to ways to further augment Social-RAG. We conclude by discussing the broader implications of Social-RAG for designing more adaptive and socially aware AI systems in diverse collaborative settings. We also present future work on applying our strategies for social grounding in additional contexts and discuss the limitations of our approach and workflow. In summary, our work provides the following contributions: • A novel workflow Social-RAG to design AI agents that are grounded in social information within a group space, enabling the alignment of the agent’s generation to groups’ interests and norms. • An AI agent PaperPing, which instantiates the Social-RAG workflow to proactively post relevant scholarly paper recommendations with LLM-generated explanations into group chat spaces. • Empirical observation showing how researchers interact and perceive proactive paper recommendation group messages from PaperPing, based on interviews, surveys, and log data collected during a 3-month deployment of PaperPing in 18 slack channels with exposure to over 500 researchers."
https://arxiv.org/html/2411.02332v2,SplatOverflow: Asynchronous Hardware Troubleshooting,"As tools for designing and manufacturing hardware become more accessible, smaller producers can develop and distribute novel hardware. However, there aren’t established tools to support end-user hardware troubleshooting or routine maintenance. As a result, technical support for hardware remains ad-hoc and challenging to scale. Inspired by software troubleshooting workflows like StackOverflow, we propose a workflow for asynchronous hardware troubleshooting: SplatOverflow. SplatOverflow creates a novel boundary object, the SplatOverflow scene, that users reference to communicate about hardware. The scene comprises a 3D Gaussian Splat of the user’s hardware registered onto the hardware’s CAD model. The splat captures the current state of the hardware, and the registered CAD model acts as a referential anchor for troubleshooting instructions. With SplatOverflow, maintainers can directly address issues and author instructions in the user’s workspace. The instructions define workflows that can easily be shared between users and recontextualized in new environments. In this paper, we describe the design of SplatOverflow, detail the workflows it enables, and illustrate its utility to different kinds of users. We also validate that non-experts can use SplatOverflow to troubleshoot common problems with a 3D printer in a user study.","Hardware design and manufacturing tools have enabled small teams to develop and distribute novel machines and devices for niche applications. Examples of such hardware range from automation equipment for mid-scale manufacturing, such as the Lumen pick-and-place, to personal fabrication machines, such as the Prusa MK3S, to do-it-yourself gadgets like the Open Book e-reader. We refer to such products generally as hardware. For producers of this kind of hardware, turning a prototype into a viable product and supporting a growing user base with varying technical expertise remains challenging (Hodges and Chen, 2019). This paper focuses on one aspect of producing hardware: supporting end-user troubleshooting and maintenance. Detailed and thorough documentation is an essential part of developing a hardware product(Bonvoisin et al., 2017; Ackermann, 2008; Hodges and Chen, 2019). However, documentation alone is insufficient to support the long tail of niche hardware issues users may encounter (ORR, 1996a). Instead, Subbaraman and Peek (2023a) argue that maintenance should be considered a core part of owning such hardware, and systems should be designed to support end-user troubleshooting and maintenance. We take inspiration from the infrastructure that supports software maintenance and troubleshooting workflows to examine how to create similar systems for hardware. Platforms such as GitHub and StackOverflow have supported communities of software users by allowing them to help one another troubleshoot issues and by cataloging a history of past issues for anyone to reference. Crucially, these platforms rely on asynchronous communication between users. This allows users to seek out help or provide suggestions without coordinating availability or scheduling meetings with one another. Eliminating this barrier has allowed distributed communities of users to flourish and made asynchronous modes the norm in software development (Yamauchi et al., 2000; Ackerman et al., 2013). This contrasts the workflows HCI researchers have proposed for remote expert guidance and hardware troubleshooting, which are primarily designed for synchronous modes of communication (Oda et al., 2015; Gauglitz et al., 2012, 2014). The utility of asynchronous workflows for troubleshooting is due, in part, to how they facilitate communication between users: via references to shared digital artifacts. These artifacts resemble boundary objects (Star and Griesemer, [n. d.]; Ackerman et al., 2013; Lutters and Ackerman, 2007) that serve as mechanisms for communicating context and ideas online. In StackOverflow, for example, issues are accompanied by segments of code that the user is writing. Suggestions are then made as references to lines of code the user shared. The boundary object (in this case, lines of code) captures the user’s context and scaffolds the asynchronous communication with others. Importantly, this context is identical for those contributing to troubleshooting the issue and future users referencing the issue. As a result, a larger group of current and future users benefit from the knowledge gleaned in the interaction (Yamauchi et al., 2000). Hardware needs a robust boundary object to support the asynchronous modes of communication necessary to scale maintenance and troubleshooting infrastructure. We present SplatOverflow, a system that enables asynchronous hardware troubleshooting through a novel boundary object: a SplatOverflow scene. A SplatOverflow scene captures the as-built hardware through a scan (a 3D Gaussian Splat (Kerbl et al., 2023)) and aligns that scan to a shared CAD model of the hardware. The scan lets a remote user independently navigate a local user’s environment and inspect issues with the as-built hardware. The CAD model lets users interact with individual parts of the hardware and manipulate them to make actions. Interactions with the SplatOverflow scene are captured in a timeline that captures the history of instructions and discussions relating to a hardware issue. The construction of a SplatOverflow scene presents distinct advantages to local users (who have the hardware in front of them), remote assistants, and future users who may encounter similar hardware issues. Local users benefit from access to technical documentation linked to the CAD model and the ability to describe issues on their hardware by pointing and clicking on parts rather than knowing hardware-specific vocabulary. Remote assistants benefit from seeing the local user’s issue registered onto the familiar CAD model and being able to freely move the view-port to inspect the hardware in the local user’s environment. Finally, future users benefit from being able to retrieve and replay solutions to past issues without seeking out synchronous support. Instead, SplatOverflow can re-contextualize instructions in a past issue by overlaying them onto the current user’s environment. This ability to accommodate multiple users and assistants is essential to how SplatOverflow can help support scaling the maintenance effort for hardware. In this paper, we present the design of SplatOverflow, demonstrate how it can be used to troubleshoot issues on different kinds of hardware, and validate that non-expert users can use SplatOverflow to troubleshoot hardware issues."
https://arxiv.org/html/2411.02263v1,"AI Should Challenge, Not Obey",Let’s transform our robot secretaries into Socratic gadflies.,"AI as Provocateur In between the two extreme visions of AI as a servant and AI as a sentient fighter-lover, resides an important and practical alternative: AI as a provocateur. Conceiving of AI as a provocateur requires us to move away from the legacy of AI as deriving an objectivist statistical truth, to producing fallible provocations representing the stochastic replay of subjective human judgements. It enables us to broaden the role of AI from the workflow completion and progress orientation of assistance, to counter-argumentation, criticism, and questioning. A provocateur does not complete your report. It does not draft your email. It does not write your code. It does not generate slides. Rather, it critiques your work. Where are your arguments thin? What are your assumptions and biases? What are the alternative perspectives? Is what you’re doing worth doing in the first place? Rather than optimise speed and efficiency, a provocateur engages in discussions, offers counterarguments, and asks questions [8] to stimulate our thinking. The idea of AI as provocateur complements, yet challenges, current frameworks of “human-AI collaboration” (notwithstanding objections to the term [5]), which situate AI within knowledge workflows. Human-AI “collaborations” can be categorised by how often the human (versus the AI system) initiates an action [9], or whether human or AI takes on a supervisory role [10]. AI can play roles such as “coordinator”, “creator”, “perfectionist”, “doer”, [11] and “friend”, “collaborator”, “student”, “manager” [12]. Researchers have called for metacognitive support in AI tools [13], and to “educate people to be critical information seekers and users” [14]. Yet the role of AI as provocateur, which improves the critical thinking of the human in the loop, has not been explicitly identified. The “collaboration” metaphor easily accommodates the role of provocateur; challenging collaborators and presenting alternative perspectives are features of successful collaborations. How else might AI help? Edward De Bono’s influential Six Thinking Hats [15] framework distinguishes roles for critical thinking conversations, such as information gathering (white hat), evaluation and caution (black hat), and so forth. “Black hat” conversational agents, for example, lead to higher quality ideas in design thinking [16]. Even within the remit of “provocateur”, there are many possibilities not well-distinguished by existing theories of human-AI collaboration. A constant barrage of criticism would frustrate users. This presents a design challenge, and a reason to look beyond the predominant interaction metaphor of “chat”. The AI provocateur is not primarily a tool of work, but a tool of thought. As Iverson notes, notations function as tools of thought by compressing complex ideas and offloading cognitive burdens [17]. Earlier generations of knowledge tools, like maps, grids, writing, lists, place value numerals, and algebraic notation, each amplified how we naturally perceive and process information. How should we build AI as provocateur, with interfaces less like chat and more like notations? For nearly a century, educators have been preoccupied with a strikingly similar question: how do we teach critical thinking?"
https://arxiv.org/html/2411.02227v1,Optimization Models to Meet the Conditions of Order Preservation in the Analytic Hierarchy Process,"Deriving a priority vector from a pairwise comparison matrix (PCM) is a crucial step in the Analytical Hierarchy Process (AHP). Although there exists a priority vector that satisfies the conditions of order preservation (COP), the priority vectors obtained through existing prioritization methods frequently violate these conditions, resulting in numerous COP violations. To address this issue, this paper introduces a novel procedure to manage COP violations in AHP. Firstly, we prove that the index-exchangeability condition is both a necessary and sufficient condition for determining whether a priority vector satisfies COP. This enables the direct detection of COP violations, relying solely on the pairwise comparison preferences of decision-makers, rather than the prioritization methods utilized. Subsequently, we propose the Minimal Number of Violations and Deviations Method (MNVDM) model, which aims to derive a priority vector with the minimal number of COP violations. In particular, the MNVDM can obtain a violation-free priority vector when the PCM meets the index exchangeability conditions. Furthermore, an optimization model based on minimizing information loss is designed to ensure the COP by revising the preferences when the index-exchangeability conditions are violated. Finally, the feasibility and efficiency of the proposed models are validated through numerical examples and Monte Carlo simulation experiments. Our implementation is available at: https://github.com/Tommytutu/COP.","The Analytical Hierarchy Process (AHP), proposed by Saaty Saaty (1980), is one of the most widely utilized multi-criteria decision-making methods. AHP encompasses five primary steps: structuring the hierarchy, constructing pairwise comparison matrices (PCMs), checking consistency, deriving the priority vector, and synthesizing the results. This paper focuses specifically on the procedures of checking consistency and deriving the priority vector. AHP distinguishes itself from other methods through its use of pairwise comparison techniques to establish the weights of criteria at the same level Brunelli (2015). This method’s effectiveness lies in its ability to break down complex decision-making problems into simpler, more manageable sub-problems, thereby allowing decision-makers (DMs) to systematically compare the relative importance of each criterion. However, the subjective nature of judgments can introduce inconsistencies Brunelli and Cavallo (2020a). Scholars have thus proposed diverse measures to assess the rationality of DMs’ judgments, ranging from consistency indices Brunelli (2018); Grzybowski (2016); P. T. Kazibudzki (2023); Brunelli and Fedrizzi (2024), transitivity (also known as ordinal consistency) (Wu and Tu, 2021; Brunelli and Cavallo, 2020b; Xu et al., 2021; Liu et al., 2021), to conditions of order preservation (COP) Bana e Costa and Vansnick (2008); Kułakowski (2015); Kułakowski et al. (2019); Cavallo and D’Apuzzo (2020); Mazurek (2022). The consistency ratio (CR) Saaty (1980) is the most widely used consistency index. Saaty Saaty (1980) suggested that the inconsistency level of the PCM is acceptable when its CR value is below 0.1. In cases where the CR exceeds this threshold, the DM is advised to revisit their preferences to ensure greater consistency. However, Saaty’s concept of consistency measurement has been seriously questioned (Genest and Rivest, 1994; Karapetrovic and Rosenbloom, 1999; Koczkodaj and Urban, 2018). As a result, numerous other inconsistency indices have been developed from various perspectives. These include the minimal error index (MEI) (Kułakowski, 2015; Kułakowski et al., 2019), the geometric consistency index (GCI) Aguarón and Moreno-Jiménez (2003), the index of logarithmic squared deviations method (ILSDM) (Kazibudzki, 2022, 2021). For further exploration of inconsistency indices, interested readers can refer to recent surveys on the topic Brunelli (2018); Pant et al. (2022). Deriving the priority vector from the PCM is a crucial step in AHP after conducting pairwise comparisons. The eigenvalue method (EM), as proposed by Saaty Saaty (1980), is the most popular prioritization method. However, EM has faced criticism from academics due to fundamental weaknesses when dealing with inconsistent PCMs. For example, it has been noted that the weight is not invariant under transpositions(Johnson et al., 1979), is inefficient (Bozóki and Fülöp, 2018), and violates the monotonicity axioms (Csató and Petróczy, 2021). Therefore, various alternative prioritization methods have been proposed (Choo and Wedley, 2004). Many of these aim to minimize the deviations between the estimated consistent PCM and the original PCM, sharing a common framework(Choo and Wedley, 2004; Kazibudzki, 2019b): min ⁢𝒟⁢(𝐀,[wi/wj]) min 𝒟𝐀delimited-[]subscript𝑤𝑖subscript𝑤𝑗\text{ min }{\cal{D}}\left(\mathbf{A},\left[w_{i}/w_{j}\right]\right)min caligraphic_D ( bold_A , [ italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] ) (1) with some accompanying constraints such as positive coefficients and the normalization conditions. When 𝐀𝐀\mathbf{A}bold_A is inconsistent, different definitions of function 𝒟⁢(𝐀,[wi/wj])𝒟𝐀delimited-[]subscript𝑤𝑖subscript𝑤𝑗{\cal{D}}\left(\mathbf{A},\left[w_{i}/w_{j}\right]\right)caligraphic_D ( bold_A , [ italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ] ) can yield varying results. Besides, the objective of model (1) can be interpreted as the inconsistency index. Specially, the logarithmic least squares method (LLSM) put forth by Crawford and Williams (Crawford and Williams, 1985) has gained increasing prominence owing to its sound axiomatic justifications (Csató, 2019; Csató and Petróczy, 2021; Bozóki and Fülöp, 2018). Undoubtedly, several important work on prioritization methods has been reported in recent years Kułakowski (2015); Kazibudzki (2022, 2021); Zhang et al. (2021a); Faramondi et al. (2020); Yuen (2024). As per Saaty Saaty (2003), the priority vector of a set of alternatives possesses two distinct interpretations, stating that “The first is a numerical ranking of the alternatives that indicates an order of preference among them. The other is that the orders should also reflect intensity or cardinal preference as indicated by the ratios of the numerical values and is thus unique to within a positive multiplicative constant (a similarity transformation)."" In other words, the priority vector should contain information regarding the order and intensity of the preferences. Bana e Costa and Vansnick Bana e Costa and Vansnick (2008) concurred with this and articulated these two meanings of priority vectors as the COP, which encompass the preservation of order preference (POP) conditions and the preservation of order of intensity preference (POIP) conditions. They emphasized that when the PCM is inconsistent, the priority vector obtained by the EM and LLSM sometimes cannot appropriately reflect the DM’s preferences. That is, the EM and LLSM may fail to generate a priority vector from the acceptable PCM (CR≤0.1CR0.1\text{CR}\leq 0.1CR ≤ 0.1) meeting the COP, and the consistency measures (such as CR and GCI) may not be capable of detecting or warning of this ‘violation’ phenomenon (see Example 3). The ranking outcomes are deemed indisputable when the priority vector adheres to the COP. Conversely, if the priority vector violates COP, the DM may resist accepting ranking results that appear unreasonable. Complying with COP can thus reduce the costs associated with implementing the evaluation system, offering advantages such as providing the ranking participants with a sense of fairness and transparency. Therefore, this paper develops optimization models to ensure adherence to the COP within the AHP. The subsequent sections of this paper are structured as follows: Section 2 provides a review of relevant literature concerning COP. Section 3 introduces some fundamental concepts. Section 4 outlines a methodology for identifying COP violations. Section 5 introduces the minimal number of violations and deviations method (MNVDM) for deriving priority vectors from inconsistent PCMs. Section 6 designs an optimization model to revise the preferences when the violations are inevitable. The conclusions are given in Section 7."
https://arxiv.org/html/2411.02118v1,Grounding Emotional Descriptions to Electrovibration Haptic Signals††thanks:This work was supported by research grants from VILLUM FONDEN (VIL50296) and the National Science Foundation (#2339707).,"Designing and displaying haptic signals with sensory and emotional attributes can improve the user experience in various applications. Free-form user language provides rich sensory and emotional information for haptic design (e.g., “This signal feels smooth and exciting”), but little work exists on linking user descriptions to haptic signals (i.e., language grounding). To address this gap, we conducted a study where 12 users described the feel of 32 signals perceived on a surface haptics (i.e., electrovibration) display. We developed a computational pipeline using natural language processing (NLP) techniques, such as GPT-3.5 Turbo and word embedding methods, to extract sensory and emotional keywords and group them into semantic clusters (i.e., concepts). We linked the keyword clusters to haptic signal features (e.g., pulse count) using correlation analysis. The proposed pipeline demonstrates the viability of a computational approach to analyzing haptic experiences. We discuss our future plans for creating a predictive model of haptic experience.","Programmable touch signals (i.e., haptics) with sensory and emotional attributes can provide immediate feedback, enhancing user experience in various applications like gaming, virtual reality environments, and mobile applications. For example, a lively haptic signal can increase user enjoyment during game play [1] or encourage the user to continue an action, while an alarming signal can warn the user about potential errors such as deleting a file [2]. Children can feel the emotional sentiment of words in a storybook with haptic effects. Also, haptics offers an alternative means of receiving emotional and contextual information for users with visual or auditory impairments. Haptics researchers have proposed various methods to assess the emotional content of haptic signals [3, 4]. Some works collected free-form user descriptions to showcase the rich range of sensations evoked by haptic signals [5]. User descriptions provide insights into the sensory and emotional attributes of haptic signals, but computational approaches for analyzing user language are missing in haptics. For example, a user may describe an electrovibration signal as “this signal feels smooth and exciting” or “it’s not super strong. It’s really quite pleasing.” Haptics researchers manually analyze these descriptions to extract relevant keywords and identify trends or themes across various users [6]. Yet, the manual data analysis is time-consuming and difficult to scale. This difficulty is exacerbated since haptic designers often need to test various combinations of signal parameters in user studies to verify the sensory and emotional content of the signals [3, 7]. To accelerate haptic signal design, we need efficient methods for analyzing user descriptions and linking them to haptic signal features such as pulse count (i.e., language grounding). Figure 1: The electrovibration haptic device in our data collection user study and an example of free-form user description. In this paper, we leveraged natural language processing (NLP) techniques to analyze free-form user descriptions for surface haptic signals [8, 9]. First, we collected user descriptions for 32 surface electrovibration signals in a lab study with 12 users (16 signals per participant). During the study, the participant(s) listened to white noise on a headphone and felt each haptic signal by sliding their index finger over a 3M MicroTouch electrostatic screen without seeing any visual representation of the signal (Figure 1). They verbally described the sensory, emotional, and associative attributes of the tactile sensation. All sessions were audio-recorded and transcribed. Next, we developed an NLP pipeline to analyze the descriptions in three steps. First, we used NLP techniques to extract sensory and emotion keywords from free-form user descriptions. Second, we divided the keywords into positive and negative sets to form semantic concepts (i.e., keyword clusters). Finally, we linked the keyword clusters to haptic signal features (e.g., pulse count) using correlation analysis. We contribute: • The first computational pipeline for analyzing free-form haptic descriptions • Clusters of positive and negative keywords used for describing haptic experience • Preliminary results on linking the concept clusters and electrovibration signal features using correlation analysis Our late-breaking results suggest a promising direction for haptic researchers is to further explore computational approaches for haptic signal design through NLP techniques. Figure 2: The overview of our computational pipeline with three main phases: (1) Keyword extraction identifies sensory and emotional keywords from the user descriptions and groups the keywords into positive and negative sentiments, (2) Keyword clustering clusters the keywords using four word embedding methods, and (3) Correlation analysis extracts statistical signal features and calculates the correlations between signal features and concept clusters."
https://arxiv.org/html/2411.02102v1,Toward Realistic Cinema: The State of the Art in Mechatronics for Modern Animatronics,"The pursuit of realism in cinema has driven significant advancements in animatronics, where the integration of mechatronics, a multidisciplinary field that combines mechanical engineering, electronics, and computer science, plays a pivotal role in enhancing the functionality and realism of animatronics. This interdisciplinary approach facilitates smoother characters movements and enhances the sophistication of behaviors in animatronic creatures, thereby increasing their realism. This article examines the most recent developments in mechatronic technology and their significant impact on the art and engineering of animatronics in the filmmaking. It explores the sophisticated integration of system components and analyzes how these enhancements foster complexity and integration, crucial for achieving unprecedented levels of realism in modern cinema. Further, the article delves into in-depth case studies of well-known movie characters, demonstrating the practical applicability of these state-of-the-art mechatronic solutions in creating compelling, lifelike cinematic experiences. This paper aims to bridge the gap between the technical aspects of mechatronics and the creative demands of the film industry, ultimately contributing to the ongoing evolution of cinematic realism.","Prior to the widespread adoption of mechatronic technology, the film industry faced numerous mechanical constraints that significantly affected production efficiency and creative possibilities. The integration of mechatronics—an interdisciplinary field combining mechanical engineering, electronics, computer science, and control engineering—has transformed these challenges into opportunities for innovation. Historically, the film industry relied on traditional mechanical systems for camera operation, lighting, and set design. These systems often posed limitations such as rigidity and lack of flexibility [1], labor-intensive processes [2], precision limitations [1] and the lack of integration between different mechanical systems made it difficult to synchronize various elements of production, such as camera movements with lighting changes or special effects. These challenges restricted both the art framework for creativity and the technical capabilities of cinema production, as well as limited the feasible speeds of old film copying methods. These earlier mechanisms, which mainly relied on mechanical gears and sprockets, frequently experienced problems that weakened the quality of the final film production [3]. Nowadays, mechatronics plays a crucial role in scenes that combine practical effects with computer-generated imagery (CGI). Animatronics provide a physical presence that can be enhanced with CGI, giving film-makers more flexibility to achieve visual effects that were previously impossible [3]. For instance, the subtle movements of facial features or the fluid motion of limbs are much more refined with mechatronic technology. Traditional animatronics relied heavily on simple mechanics like pneumatics and hydraulics, which limited their range of motion. Mechatronics allows for the incorporation of advanced robotics into creature design, which can simulate muscle movement and facial expressions more naturally and with greater control. With advancements in mechatronics, designers can program creatures to perform complex sequences of actions that can be adjusted and repeated with high precision. This programmability also enhances the safety and repeatability of complex scenes, reducing the need for retakes and decreasing production costs. Mechatronic creatures can interact more effectively with human actors and environments due to sensors and real-time processing capabilities. These features allow the creatures to respond to external stimuli, adding an unscripted element to interactions that can enhance the believability of scenes. Overall, mechatronics has enabled film-makers to push the boundaries of what is visually possible, creating more engaging and believable non-human characters that captivate audiences. In general, scholarly publications about animatronics in motion pictures enhance our comprehension of the convergence of technology, art, and narrative in the motion picture business. This article explores novel ways to use animatronics to animate movie characters through innovative processes. It examines the most recent developments in mechatronic technology and their significant impact on the art and engineering of animatronics in film-making. The paper aims to explore the sophisticated integration of system components and analyze how these enhancements foster complexity and integration, crucial for achieving unprecedented levels of realism in modern cinema. Additionally, the article delves into in-depth case studies of well-known movie characters, demonstrating the practical applicability of these state-of-the-art mechatronic solutions in creating compelling, lifelike cinematic experiences. This paper is organized as follows: Section 2 provides the theoretical and technical aspects of mechatronics with practical applications in the film industry, offering a comprehensive overview of how these systems are integral to the evolution of cinematic technology and artistry. Section 3 showcases notable examples of animatronics in films, demonstrating the diverse applications of this technology across different models such as humans, sharks, bees, bats, monsters, and vampires. Section 4 reviews the latest technological advancements in mechatronics relevant to film-making and analyzes how these advancements have facilitated smoother and more sophisticated movements in animatronic creatures. Section 5 explores the significant impact of mechatronics on the design and realism of cinematic creatures, highlighting the technological advancements that have enabled more dynamic and interactive performances. Section 6 discusses the future of creature animation, speculating on future trends and potential innovations in mechatronics that could further transform film-making. Section 7 provides in-depth case studies of notable movie characters enhanced through mechatronics, demonstrating the practical application and specific mechatronic solutions used to achieve lifelike effects. Finally, Section 8 summarizes the key points discussed, reflecting on the transformational role of mechatronics in animatronics and its broader implications for the film industry. Figure 1: Integrative Framework of Mechatronics Figure 2: Animatronic Face Mechanism, Huber et al. [4]"
https://arxiv.org/html/2411.02094v1,Alignment-Based Adversarial Training (ABAT)for Improving the Robustness and Accuracyof EEG-Based BCIs,"Machine learning has achieved great success in electroencephalogram (EEG) based brain-computer interfaces (BCIs). Most existing BCI studies focused on improving the decoding accuracy, with only a few considering the adversarial security. Although many adversarial defense approaches have been proposed in other application domains such as computer vision, previous research showed that their direct extensions to BCIs degrade the classification accuracy on benign samples. This phenomenon greatly affects the applicability of adversarial defense approaches to EEG-based BCIs. To mitigate this problem, we propose alignment-based adversarial training (ABAT), which performs EEG data alignment before adversarial training. Data alignment aligns EEG trials from different domains to reduce their distribution discrepancies, and adversarial training further robustifies the classification boundary. The integration of data alignment and adversarial training can make the trained EEG classifiers simultaneously more accurate and more robust. Experiments on five EEG datasets from two different BCI paradigms (motor imagery classification, and event related potential recognition), three convolutional neural network classifiers (EEGNet, ShallowCNN and DeepCNN) and three different experimental settings (offline within-subject cross-block/-session classification, online cross-session classification, and pre-trained classifiers) demonstrated its effectiveness. It is very intriguing that adversarial attacks, which are usually used to damage BCI systems, can be used in ABAT to simultaneously improve the model accuracy and robustness.","Abrain-computer interface (BCI) establishes a direct communication channel connecting the human brain and a computer [1]. Electroencephalogram (EEG), which records the brain’s electrical activities from the scalp, is the most commonly utilized input signal in non-invasive BCIs, due to its affordability and ease of use [2]. An EEG-based BCI system typically includes four components: signal acquisition, signal processing, machine learning, and controller, as illustrated in Fig. 1. Figure 1: Flowchart of a closed-loop BCI system. Most prior research on EEG decoding primarily focused on the accuracy and efficiency of machine learning algorithms [3]. Nonetheless, a critical discovery by Zhang and Wu [4] revealed that adversarial examples, generated using unsupervised fast gradient sign method (FGSM) [5], can significantly degrade the performance of deep learning classifiers in EEG-based BCIs. They introduced an attack framework that transforms a benign EEG epoch into an adversarial one by injecting a jamming module before machine learning to add adversarial perturbations, as depicted in Fig. 2. Furthermore, Zhang et al. [6] demonstrated that adversarial examples can also fool traditional machine learning classifiers in BCI spellers, misleading them to output an arbitrary (incorrect) character specified by the attacker. Liu et al. [7] and Jung et al. [8] developed approaches to generate universal adversarial perturbations for EEG-based BCIs, making adversarial attacks much easier to implement. Bian et al. [9] employed simple square wave signals to generate adversarial examples, for attacking steady-state visual evoked potential based BCIs. Wang et al. [10] investigated physically constrained adversarial attacks to BCIs. Meng et al. [11] also performed adversarial attacks in EEG-based BCI regression problems. Figure 2: The attack framework proposed in [4], which injects a jamming module between signal processing and machine learning to generate adversarial examples. Adversarial attacks to EEG-based BCIs could have various consequences, from mere user frustration to life-threatening accidents. As pointed out in [12], “In BCI spellers for Amyotrophic Lateral Sclerosis patients, adversarial attacks may hijack the user’s true input and output wrong letters. The user’s intention may be manipulated, or the user may feel too frustrated to use the BCI speller, losing his/her only way to communicate with others. In BCI-based driver drowsiness estimation [13], adversarial attacks may manipulate the output of the BCI system and increase the risk of accidents. In EEG-based awareness evaluation/detection for disorder of consciousness patients [14], adversarial attacks may disturb the true responses of the patients and lead to misdiagnosis."" In military applications, adversarial attacks to BCIs may generate false commands, potentially causing friendly fire [15]. Consequently, it is very important to develop BCI machine learning models that are robust against adversarial attacks. Many adversarial defense approaches have been proposed in the literature [16, 17, 18], among which robust training [19] may be the most classical and effective strategy. Adversarial training (AT) [16] is a representative robust training approach, and many other approaches [17, 18] can be regarded as its variants. AT solves a minimax problem (the saddle point problem). During training, AT generates adversarial examples along gradients that increase the model’s loss to the input, and then minimizes the model’s loss on these adversarial examples repeatedly [16]. This process aims to minimize the model’s loss on adversarial examples, but does not explicitly optimize the performance on benign examples. Many studies [16, 20, 21, 22] have shown that robust training may result in a significant decrease of the accuracy on benign samples, which is undesirable. Few studies have explored the possibility of improving the machine learning performance using adversarial examples. For image classification, Xie et al. [23] employed a separate auxiliary batch normalization for adversarial examples to prevent model overfitting. For EEG classification, Ni et al. [24] used a loss on adversarial examples to improve the cross-subject and cross-state transfer learning performance. However, Li et al. [21] and Meng et al. [22] have shown that conventional robust training approaches usually lead to an evident reduction in BCI model accuracy on benign samples, i.e., it is difficult to achieve both high accuracy and good robustness through robust training. Robust models aim to maintain good classification performance under adversarial attacks, which is important in safety-critical applications. However, the accuracy degradation of robust models on benign samples seriously affects their adaption. To mitigate this problem, we propose alignment-based adversarial training (ABAT) to align EEG data for each session before performing robust training on them. This simple approach can be readily used in deep model training. After ABAT, the model’s classification accuracy on benign samples and robustness on adversarial samples can simultaneously be improved. Experiments on five datasets using two different BCI paradigms, three classifiers and three different experimental settings demonstrated its effectiveness. To our knowledge, this is the first work on simultaneously improving the accuracy and robustness of the classifiers in EEG-based BCIs, and also the first time that EEG data alignment has been used in BCI adversarial defense. We hope that our findings can inspire more future research on robust EEG classifiers. The remainder of this paper is organized as follows: Section II introduces related works. Section III proposes ABAT. Section IV describes the experimental settings. Section V presents the experimental results. Finally, Section VI draws conclusions."
https://arxiv.org/html/2411.02090v1,Game Engines for Immersive Visualization:Using Unreal Engine Beyond Entertainment,"One core aspect of immersive visualization labs is to develop and provide powerful tools and applications that allow for efficient analysis and exploration of scientific data. As the requirements for such applications are often diverse and complex, the same applies to the development process. This has led to a myriad of different tools, frameworks, and approaches that grew and developed over time. The steady advance of commercial off-the-shelf game engines such as Unreal Engine has made them a valuable option for development in immersive visualization labs. In this work, we share our experience of migrating to Unreal Engine as a primary developing environment for immersive visualization applications. We share our considerations on requirements, present use cases developed in our lab to communicate advantages and challenges experienced, discuss implications on our research and development environments, and aim to provide guidance for others within our community facing similar challenges.","Developing immersive visualization applications imposes specific demands on the software used to create them. Choosing a software stack greatly impacts the application’s interaction, fidelity, and performance and affects the development process. Historically, there have been tendencies toward creating custom software to achieve immersive visualizations. These can broadly be divided into two categories: bespoke software specific to the applications and frameworks that can be used to build applications. While bespoke software gives much flexibility, it is often connected to a high (re-)implementation effort. On the other hand, custom frameworks develop centralized functionality into a common code base, which can then be (re-)used to create immersive visualization applications. Both, however, face a common challenge today: Maintenance efforts have increased drastically to meet the growing complexity and demands in various settings. Coincidentally, game engines — commercial-off-the-shelf (COTS) solutions — became more powerful, more open, and easier to use. This is particularly interesting to immersive visualization labs, as the development of such engines shares several key characteristics with immersive visualization applications. Both require low-latency computations, interactive rendering, ergonomic interactions, and visual fidelity. The rise of COTS virtual reality (VR) hardware in the early 2010s especially positively impacted the VR-readiness of COTS game engines. Besides the similar requirements for the end product, game engines have a large focus on tooling and developer efficiency. To enable low-friction game development that focuses on gameplay rather than technology, these engines are often highly optimized, bring ready-to-use techniques, and are easy to learn. Therefore, the obvious question is whether game engines could also be utilized to create immersive visualization applications. In this work, we present our considerations concerning the use of game engines as a key component of the workflow in immersive visualization labs. We reflect on historic decisions, development choices, and current workflows within our research lab at RWTH Aachen University, which led from custom software to the use of Unreal Engine (UE) (Epic Games, \APACyear\bibnodate\APACexlab\BCntND4). Based on these experiences, we suggest and motivate a list of requirements that we deem important when considering if a game engine is suitable for use in immersive visualization labs. We then present multiple use cases that highlight challenges within our lab and our solutions and discuss how the engine influenced the development in these particular cases. Finally, reiterating the introduced requirements, we evaluate and discuss Unreal Engine as a tool for immersive visualization applications and summarize our findings."
https://arxiv.org/html/2411.02088v1,Affordances and Design Principles of The Political Left and Right,"Like any form of technology, social media services embed values. To examine how societal values may be present in these systems, we focus on exploring political ideology as a value system. We organised four co-design workshops with political representatives from five major parties in Finland to investigate what values they would incorporate into social media services. The participants were divided into one right-leaning group, two left-leaning groups, and one mixed group. This approach allows us to examine how their political ideologies, i.e., value systems, influenced the design of social media. We analysed produced artefacts (early-stage paper mockups) to identify different features and affordances for each group and then contrasted the ideological compositions. Our results revealed a clear distinction between groups: right-leaning groups favoured market-based visibility, while left-leaning groups rejected such design principles in favour of open profile work. Additionally, we found tentative differences in design outcomes along the liberal–conservative dimension. These findings underscore the importance of acknowledging existing political value systems in the design of social computing systems. They also highlight the need for further research to map out political ideologies in technology design.","Technology design always encompasses values, a well-known fact among human-computer interaction researchers and practitioners. To account for various values, value-sensitive design approaches were developed to help align the system’s values with its user’s values (Friedman et al., 2009; Borning and Muller, 2012). Extensive work has been done to understand the values of children and parents in the context of parental software (Van Mechelen et al., 2014; Nouwen et al., 2015). Alternatively, some design work incorporates specific values as a political statement, highlighting a social issue or envisioning alternative futures. For example, Fox et al. (2019) focused on the distribution of menstrual products in public toilets, a feminist issue. Researchers have often focused on end-users and less on decision-makers, such as political elites, when considering different values and their influence on design. At the same time, the government is a major provider of services, facilitated increasingly through digital means. Even more importantly, in democratic societies, the elected representatives have the legislative power – they establish laws increasingly related to technology, as shown in the EU AI Act or the California Consumer Privacy Act. Both involve value-based decisions about the rights and responsibilities of citizens and their expected roles. In the political system, there are reasons to suggest that such decisions mirror a wider set of beliefs and assumptions (or political ideologies), as well as political realities based on different balances of power among representatives. Members of Congress, Parliament or city councils, and other representatives rarely have been studied as stakeholders in the design of digital systems. To this end, we organised co-design workshops with political representatives to elicit how their ideologies would be embedded in technology design. In four co-design workshops, 13 participants across the political spectrum were tasked to design a social media platform. We arranged participants into one right-leaning group, two left-leaning groups, and one mixed group for the workshops to compare their design and value decisions. We examined the outcomes of the workshops as the participants’ suggestions and analysed those towards the specific features and implicit affordances. Based on the literature on social media platforms (Boyd, 2010; Bucher and Helmond, 2017; DeVito et al., 2017; Van Raemdonck and Pierson, 2021), we categorised affordances and connected them to underlying values to uncover ideological tensions between the different design solutions. We next discuss relevant literature to present political ideologies as values and discuss social media design further. Following this, we describe the organisation of the co-design workshops. In this paper, we focus only on the final artefacts of the co-design activity, which exemplify the value-based decisions the groups made during the design workshop. We present a method to analyse these artefacts and summarise observations from the artefacts. We observe that right-leaning groups supported market-based dynamics while left-leaning groups supported diverse self-presentation. Mixed groups seemed to have both kinds of features. Before concluding our work, we discuss the importance of ideological diversity in design activities. Our work makes the following contributions focused on the design of information technology: • We introduce political ideologies as a set of values and connect the debate on values to national politics. • We introduce a co-design approach which allows one to examine political ideologies in technology design. • We show how political ideologies (right-left) shape social media design and identify design principles of flexible identity and competitive logic to be specific to the left- and right-leaning parties."
https://arxiv.org/html/2411.02067v1,Imagining Better AI-Enabled Healthcare Futures: The Case for Care By Design,"We find ourselves on the ever-shifting cusp of an AI revolution — with potentially metamorphic implications for the future practice of healthcare. For many, such innovations cannot come quickly enough; as healthcare systems worldwide struggle to keep up with the ever-changing needs of our populations. And yet, the potential of AI tools and systems to shape healthcare is as often approached with great trepidation as celebrated by health professionals and patients alike. These fears alight not only in the form of privacy and security concerns but for the potential of AI tools to reduce patients to datapoints and professionals to aggregators — to make healthcare, in short, less caring. This infixated concern, we - as designers, developers and researchers of AI systems - believe it essential we tackle head on; if we are not only to overcome the AI implementation gap, but realise the potential of AI systems to truly augment human-centred practices of care. This, we argue we might yet achieve by realising newly-accessible practices of AI healthcare innovation, engaging providers, recipients and affected communities of care in the inclusive design of AI tools we may yet enthusiastically embrace to materialise new opportunities for increasingly meaningful and effective care.","1. The Case for AI HealthCare Healthcare systems worldwide are under ever-growing pressures, often described a result of ageing populations and growing rates of chronic, co-morbid medical conditions as much as the under-funding of services (Andersen et al., 2019; Organization et al., 2005). Innovative solutions to this long-standing and ever anew challenging situation are required, we are told by policy-makers, health system managers, researchers and developers alike. For many years, digital technologies from electronic patient records to shared-decision making tools have been promoted as the answer — a discourse only further accented today by the recent arrival of AI technologies we are told will “help save lives by transforming healthcare” through “more personalized, accessible and effective solutions” (Google, 2024). Indeed, AI systems offer great potential to address many urgent healthcare challenges, improve patient outcomes, and advance the adoption of digital tools to support healthcare (Matheny et al., 2020). And yet, despite such promise, many doctors speak to us still today of the degree to which they “hate their computers” (Gawande, 2018). This claim, as made by surgeon Atul Gawande in an influential New Yorker Annals of Medicine article (ibid.), is equally a refrain heard often by those of us engaged in the frontline practice of the design, development and evaluation of novel digital health solutions. Despite billions spent and decades of effort, still today the vast majority of digital clinical innovations either fail outright or are deployed only to be met with practices of passive hindrance and resistive compliance if not active resistance by health professionals (Ziebland et al., 2021; Zaj\kac et al., 2023) — each often visions of AI in-action which yet fail to function in-practice (Aristidou et al., 2022; Coiera, 2019). And the primary cause of this state of affairs, the weight of the human-computer interaction (HCI) and computer-supported cooperative work (CSCW) literature suggests, remains a lack of consideration for human factors; including clinicians’ needs, values, and ways of working (Yang et al., 2019; Zaj\kac et al., 2023; Shaw et al., 2019; Osman Andersen et al., 2021; Jacobs et al., 2021). This lack of consideration, we see increasingly realised in the form of practices of resistance to the perceived imposition of AI systems, even those with seemingly-impressive evidence for their capacity to improve care (Yang et al., 2019). At the root of these fears, we often hear, lie concerns about datafication and inaccuracies (Schüll, 2016), exacerbated healthcare disparities (Matheny et al., 2020; Obermeyer et al., 2019), confused lines of responsibility (Doherty et al., 2020) and increased workloads (Yang et al., 2019). Such responses might then yet give us pause to ask; is this opposition to the technology, or to the assumptions about care we promote, embed and occasionally even enforce therein? In other words, are we tackling the right problems? If not approached with care, the decisions we make in the design of systems developed to address challenges in the practice of care, may instead worsen the underlying causes of crisis. 1.1. Future Possible AI-Enabled Healthcare Crises Indeed, should we in the development of AI-enabled systems of care inadvertently reify existing problematic assumptions, we may come to create a healthcare future in which clinicians find themselves employing systems they loathe (Ziebland et al., 2021) for purposes they feel reduce their patients to potentially misleading data points (Schüll, 2016) and their professional roles to automatons (Gawande, 2018) — such that AI serves less as a meaningful lever for care than a “crowbar to pry dignity out of our collective hands” (Geuter, 2024). Much work has yet been conducted to date to avoid just such scenarios — researchers’ own efforts towards not only resistance but awareness-raising. This includes calls for consideration of bias, trust and transparency in the research and development of AI systems (Waefler and Schmid, 2020; Kim et al., 2023), as encapsulated in the transparency requirements of the European Union’s (EU) AI Act, and new EU Ethics Guidelines for Trustworthy AI requiring developers to support “diversity, non-discrimination and fairness” for the benefit of “all human beings” (AI, 2019; van Kolfschooten and van Oirschot, 2024). These commendable efforts yet surface both the desire for action and the immense challenges of bridging AI theory, policy and the realities of AI systems development — MIT’s recently-developed AI-Risk Repository highlighting more than 700 AI risks categorized by cause and risk domain (Slattery et al., 2024). HCI and CSCW researchers and practitioners have at the same time long advocated, as one path towards the design of more desirable AI-enabled futures of care, for the involvement of users in the design of digital tools and systems (Jacobs et al., 2021; Osman Andersen et al., 2021; Karusala et al., 2021). Drawing on human factors ergonomics, design-thinking, user-centred and service-design traditions, we as a collective community have developed and employed numerous methods to promote and enable greater participation in design and research activities spanning the product-development lifecycle by a wide range of stakeholders; from clinicians to patients and diverse other caregivers. HCI and Design as academic and professional practices have therefore today granted us a diverse portfolio of tools and methodologies for the user-centred design of digital tools and systems — from ideation, brainstorming and user-centred evaluation techniques to creative methods of low- and high-fidelity prototyping (Chivukula et al., 2024; Hsieh et al., 2023; Carayon et al., 2020). And yet, we at the same time recognise the design of Trustworthy AI (TAI) tools as presenting new and significant challenges to the adoption and adaptation of these practices (Sadek et al., 2024b). Indeed, a number of researchers have turned their focus in recent years towards figuring out just how such methods may be adapted to permit stakeholder-engagement in the design of AI systems in particular — producing notable conceptual tools from Carnegie Mellon’s AI Brainstorming (Yildirim et al., 2023) to Microsoft’s Human-AI eXperience (HAX) toolkits (Vorvoreanu, 2023) (for more see (Data and Society, [n. d.])). While other researchers have even begun to adopt insider-ethnographer and meta-research perspectives, seeking to uncover problems and processes for the more effective development of tools - conceptual and tangible - to realise more sustainable and responsible AI futures (Kawakami et al., 2024; Wong et al., 2023; Sadek et al., 2024a). Healthcare is yet a high-stakes context imposing specific demands on AI systems. In this domain, it is essential that AI systems not only prove accurate, reliable and avoid exacerbating inequalities, but refrain from dehumanising users, detracting from the meaningfulness of clinical work, nor devaluing the very heart of this most human of endeavours — care itself (Waefler and Schmid, 2020; Doherty et al., 2022; Kurniawan et al., 2024). These are fears motivated in many respects by shared concern for the forces of rationalisation and determinism — whether wrought by perverse commercial incentives, the reductionist tendencies of technology or a neo-liberal politics of austerity. And the complex, hyped and often opaque nature of AI systems does yet increasingly risk excluding key stakeholders from essential conversations about the present and future direction of these technologies. It is, in this socio-technical context, unsurprising to see HCI and CSCW’s advocacy for human-centred design practices increasingly mirrored by ever-more ardent calls among medical practitioners and researchers alike for a renewed focus on patient-centred care. Connecting these threads, strikes us, authors and professionals spanning diverse disciplines, as increasingly critical given it is then not only technology but healthcare itself which many have argued requires greater care (Montori, 2020). It is in the care of healthcare that patients and professionals alike find meaning. And care then challenges us to think more deeply about what we perceive technology as ‘for’. We are yet however to quite articulate in theory nor practice, just how AI systems might be designed to support care as a fundamental component of the practice of healthcare."
https://arxiv.org/html/2411.01844v1,DeMod: A Holistic Tool with Explainable Detection and Personalized Modification for Toxicity Censorship,"Although there have been automated approaches and tools supporting toxicity censorship for social posts, most of them focus on detection. Toxicity censorship is a complex process, wherein detection is just an initial task and a user can have further needs such as rationale understanding and content modification. For this problem, we conduct a needfinding study to investigate people’s diverse needs in toxicity censorship and then build a ChatGPT-based censorship tool named DeMod accordingly. DeMod is equipped with the features of explainable Detection and personalized Modification, providing fine-grained detection results, detailed explanations, and personalized modification suggestions. We also implemented the tool and recruited 35 Weibo users for evaluation. The results suggest DeMod’s multiple strengths like the richness of functionality, the accuracy of censorship, and ease of use. Based on the findings, we further propose several insights into the design of content censorship systems.","Nowadays, social media sites have been popular mediums for self-disclosure. For example, hundreds of millions of people utilize Twitter (Jhaver et al., 2018), Facebook (Seering, 2020; Pan et al., 2022; Saha et al., 2021), and Weibo (Zhao, 2017) to record life events, express personal thoughts and opinions, and interact with friends every day. The openness of social media provides a spacious environment for content sharing while resulting in the disclosure of toxic content (toxicity), defined as ”a rude, disrespectful, or unreasonable comment that is likely to make someone leave a discussion” (Per, 2023), including hate speech (Kang et al., 2022), harassment (Jhaver et al., 2018; Cai and Wohn, 2019), insults and abuse (Beres et al., 2021), and offensive language (Deng et al., 2022), etc. Since the severe problem of context collapse (Misra and Such, 2017), social media users are usually unaware of the disclosure of toxic content. For example, the prior studies (Michael Wiegand, 2021; Maarten et al., 2020) found that about two-thirds of toxic content was implicit toxicity in online communities and the corresponding users were usually unaware of the content and the harm to others. Research revealed that 23.00% of users regret when they re-examine their shared content due to several reasons (Wang et al., 2011), such as lack of the consequence consideration of posts, culture misjudgment, unintended audience, misunderstanding of platform norms. To avoid toxic content disclosure, social media users generally conduct content censorship before publishing a post. The censorship procedure can be implemented by users themselves or by leveraging some automated tools. For example, several studies have found that individuals usually censored their content by checking, adjusting, or even deleting part of the content to make the content suitable to be published on social media (Wisniewski et al., 2012). Although there have been various censorship approaches, most of them focus on toxic content detection, e.g., toxicity score evaluation with Perspective API (Per, 2023) and toxic keywords identification (Wright et al., 2021). Toxic content censorship is a complex process, wherein detection is just an initial task, and a user can have diverse needs such as detection result understanding and content modification. For example, a user can identify toxic words in the content with the RECAST tool (Wright et al., 2021) while not knowing how to reduce its toxicity limited by her/his knowledge or experience. Therefore, there needs a holistic automated tool that can help social media users conduct multiple censorship tasks including toxic content detection, content modification, etc. Building a holistic tool for toxicity censorship faces several challenges. First, social media users’ diverse needs for toxic content censorship remain unknown. As mentioned, social media users may have different function demands like enriching explanations and giving modifications. Therefore, a systematic investigation of toxic content censorship demands is needed when conducting research on a holistic censorship tool, aggravating the complexity of this study. Second, designing and implementing a toxicity censorship tool that meets the diverse needs of users is non-trivial. Such a tool should be characterized by multiple objectives like accurate detection, fine-grained results, and appropriate revisions. How to achieve different functions and integrate them efficiently is a challenging task. Third, extensive evaluations in practice are difficult to conduct. To demonstrate the tool’s performance in helping users censor toxic content, it needs to conduct long-term evaluations in real social media scenarios by using various measurements, while some of them like the modification effects, are difficult to measure. For the above problem and challenges, we explore to design a holistic automated tool for helping users conduct toxic content censorship on social media. First, we conduct a needfinding study on a popular Chinese social platform - Weibo to systematically understand users’ current toxicity censorship practice, the problems encountered, and their corresponding expectations for system design. By combining a questionnaire survey and interviews, we uncover users’ diverse demands for the design of toxicity censorship tools and propose five goals to guide our system design, including providing holistic censorship, offering fine-grained detection results, strengthening interpretability, giving personalized revising suggestions, and ensuring user-control. Second, according to these goals, we design and implement a holistic automated toxicity censorship tool named DeMod. It is essentially a ChatGPT-enhanced tool equipped with the modules of explainable Detector and personalized Modifier. The explainable Detector can detect toxic content by giving fine-grained results like keywords and providing immediate and dynamic explanations. The immediate explanation clarifies why the content is toxic, and the dynamic explanation simulates audiences’ attitudes to the forthcoming post, helping a user know the content’s potential effects. Both explanations aim to enhance the user’s understanding of toxic content and encourage behavior regulation. After that, the modifier gives suggestions on how to revise the toxic content by considering multiple requirements, including detoxifying, reserving the original semantics, and revealing a user’s personalized language style. By taking advantage of these modules, social media users can conduct content censorship more efficiently and flexibly. Third, we implement DeMod as a third-party tool by setting Weibo as a research site and recruit 35 participants to conduct extensive evaluations. We adopt several metrics regarding our design goals. The evaluation results suggest DeMod’s capability in toxicity censorship and high acceptance among participants. Based on the above work and results, we also propose several insights into the design of content censorship tools, including enhancing censorship tools from the holistic perspective, emphasizing the interpretability of the process and results, and providing improvement measures to assist users in posting better. To conclude, our contributions can be summarized as: • We conduct a needfinding study to investigate social media users’ current toxicity censorship practice, the problems encountered, and their corresponding expectations for system design, based on which five design goals are proposed to guide the improvement of toxicity censorship tools. • We propose a holistic automated tool based on ChatGPT for helping users conduct toxicity censorship. To the best of our knowledge, this is the first work that supports users’ demands in multiple stages of toxicity censorship beyond detection. • We conduct extensive evaluations in real social media scenarios and validate DeMod’s strengths in toxicity censorship. • Several insights are proposed for the further improvement of content censorship system design. The rest of this paper is organized as follows. In Section 2, we review related research on content censorship and large language models. In Section 3, we introduce the procedure and results of our empirical study. The framework of DeMod and its implementation are given in Section 4. Section 5 exhibits our evaluation settings and results. Section 6 discusses our findings, and the limitations and future work are clarified in Section 7. Finally, conclusions are given in Section 8."
https://arxiv.org/html/2411.01675v1,Perceiving and Countering Hate: The Role of Identity in Online Responses,"This study investigates how online counterspeech, defined as direct responses to harmful online content with the intention of dissuading the perpetrator from further engaging in such behavior, is influenced by the match between a target of the hate speech and a counterspeech writer’s identity. Using a sample of 458 English-speaking adults who responded to online hate speech posts covering race, gender, religion, sexual orientation, and disability status, our research reveals that the match between a hate post’s topic and a counter-speaker’s identity (topic-identity match, or TIM) shapes perceptions of hatefulness and experiences with counterspeech writing. Specifically, TIM significantly increases the perceived hatefulness of posts related to race and sexual orientation. TIM generally boosts counter-speakers’ satisfaction and perceived effectiveness of their responses, and reduces the difficulty of crafting them, with an exception of gender-focused hate speech. In addition, counterspeech that displayed more empathy, was longer, had a more positive tone, and was associated with higher ratings of effectiveness and perceptions of hatefulness. Prior experience with, and openness to AI writing assistance tools like ChatGPT, correlate negatively with perceived difficulty in writing online counterspeech. Overall, this study contributes insights into linguistic and identity-related factors shaping counterspeech on social media. The findings inform the development of supportive technologies and moderation strategies for promoting effective responses to online hate.","The escalation of online hate speech presents a significant threat to individuals and society (Chandrasekharan et al., 2017; Mathew et al., 2019). With the proliferation of social media, people now have access to a vast audience to disseminate harmful content that attacks individuals or groups based on their race (Matamoros-Fernández and Farkas, 2021; Costello et al., 2019a; Munger, 2017), gender (Henry and Powell, 2018; Cowan and Khatchadourian, 2003; Stroud and Cox, 2018), religion (Obermaier et al., 2023; Bonotti, 2017; Castaño-Pulgarín et al., 2021), sexual orientation (Costello et al., 2019b; Cowan et al., 2005; Goyal et al., 2022), or disability status (Sherry et al., 2019; Sherry, 2019; Vedeler et al., 2019). These topics represent some of the most common targets of online hate speech (Paz et al., 2020). The United Nations characterizes hate speech as any communication that vilifies individuals or groups based on aspects such as religion, ethnicity, nationality, race, color, descent, gender, or other identity factors (Nations, 2023). Unlike generally offensive language, hate speech specifically targets core aspects of an individual’s or a group’s inherent identity - in essence, who they are. For this reason, hate speech is particularly insidious as it targets fundamental aspects of a person’s or group’s identity, exacerbating social divisions and often prompting discrimination (Benesch, 2014). The harm inflicted by such speech can have profound impacts on the targeted individuals and groups. For instance, Schmid et al. (2024) found in their qualitative study that being confronted with hate speech can have similar consequences to traumatic events, causing frustration, fear, and anger, and inducing psychological stress or even depression, particularly for targeted groups such as women who tend to perceive such incivility as more severe (Schmid et al., 2024). Exposure to online hate speech has been linked to experieincing mood swings, fear, and anger (Costello and Hawdon, 2020; Keipi et al., 2017). Exposure is also related to diminshed levels of trust (Näsi et al., 2015) and adopting discriminatory attitudes (Foxman and Wolf, 2013). The gravest concern regarding encounters with hate material on the Internet is its potential to radicalize. Indeed, there are numerous instances linking exposure to online hate to violence, including mass violence and even terrorism (Holt et al., 2019; Frissen, 2021; Hollewell and Longpré, 2022). Evidence suggests that exposure to online hate is widespread and frequent (Reichelmann et al., 2021). Given the dangers associated with exposure, it is critically important that we find effective ways to combat it and reduce its impact. One possible solution to online hate speech is online counterspeech, which is the act of responding to hateful content with the intention of stopping it, reducing its impact, or supporting the target (Garland et al., 2022; Hangartner et al., 2021; Rieger et al., 2018; Ruths et al., 2016a). Online counterspeech can take various forms (e.g., memes or pictures, written text, etc.(Fátima et al., 2023)) and use different strategies, such as using humor (MacAvaney et al., 2019; Mathew et al., 2019), showing empathy (Hangartner et al., 2021), or warning the perpetrators (MacAvaney et al., 2019; Mathew et al., 2019). Research has shown that counterspeech can be effective in challenging online hate and promoting civility in online communities (Hangartner et al., 2021; Mathew et al., 2019; Ziegele et al., 2018). Nonetheless, crafting effective online counterspeech is complex (Buerger, 2021a) and often demands specific skills (e.g., linguistic fluency, motivation, and confidence (Garland et al., 2022; Salmivalli et al., 2011)). Another critical factor that may influence how and whether people engage in online counterspeech may be what we call in this paper, Topic-Identity Match (TIM), or the alignment between the topical focus of the hate speech and the demographic identity of the individuals responding to hate speech. For instance, hate speech directed at Asians might resonate differently with an Asian individual compared to others. Similarly, a woman countering a hateful online post against women might draw from her own personal experiences to make her response more authentic and impactful (Stroud and Cox, 2018). Studies have shown that the extent to which individuals perceive online hate speech as offensive significantly affects their likelihood of and approach to responding to it (Buerger, 2021b). Thus, TIM may not only influence the intensity with which individuals perceive hate speech as offensive, as a direct match between their identity and the hate speech’s target can heighten the perceived hatefulness, but also influence how someone engages in counterspeech. Hence, understanding the role of TIM is essential in evaluating the perception of hatefulness by individuals who respond to hate speech and how they write online counterspeech, as these perceptions shape their engagement strategies that indirectly contribute to the effectiveness and overall discourse quality of counterspeech. However, most prior research has primarily focused on the impact of online counterspeech on hate speakers (Munger, 2017; Buerger, 2021b; Garland et al., 2022) or its overall effectiveness in reducing hate (Hangartner et al., 2021; Bilewicz et al., 2021; Saltman et al., 2023). Limited attention has been given to how the alignment between the hate speech’s topic and the identity of individuals responding to hate speech influences perceptions and responses to hate speech. In this paper, we address this gap by examining how TIM influences how users perceive and respond to online hate speech. In summary, we ask the following research questions in this paper: RQ1: How does the alignment between an individual’s identity and the target of hate speech, known as Topic-Identity Match (TIM), shape the individual’s perceived hatefulness of online hate speech? RQ2: How does TIM influence users’ subjective experience of writing a counterspeech - namely, their perceived satisfaction with their counterspeech, their perceptions of its effectiveness in responding to hate speech, and their perceived difficulty in crafting online counterspeech? RQ3: Given the influence of TIM, how do specific linguistic features of participant-written counterspeech, including strategy, length, and sentiment polarity, correlate with (a) the participants’ perceived hatefulness of the online hate speech they are responding to and (b) their subjective experience of writing counterspeech, measured in terms of satisfaction, perceived effectiveness, and difficulty? Meanwhile, social media companies are beginning to take advantage of artificial intelligence (AI) with the intent to foster more positive online interactions while preventing harmful discourse on their platforms. For instance, Quora, a question-and-answer platform, uses AI to help users write clearer questions, in addition to providing AI-generated responses to users’ questions (Pierce, 2023). Instagram utilizes AI to offer suggested replies for creators in direct messages (Wha, 2023). Nextdoor, a neighborhood-based social network, has integrated OpenAI’s language models to recommend modifications for user posts that could potentially incite hostility (Nex, 2023). Amidst this technological shift, several researchers advocate the use of AI to help generate online counterspeech as a strategy against online hate (Chung et al., 2021; Saha, 2023; Zhu and Bhat, 2021; Mun et al., 2024). In particular, Mun et al. (2024) (Mun et al., 2024) discussed the potential benefits and concerns of AI involvement in the counterspeech process, such as providing guidance on formulating effective responses and helping with emotion regulation and clear communication. In the process of our analysis, we noticed a pattern where individuals who perceived AI writing assistants as more useful also found writing counterspeech to be less challenging. Given the potential implications of AI, we found it relevant to present this observation beyond our primary three research questions. Therefore, we included an exploratory analysis to briefly investigate the relationship between the perceived usefulness of AI writing assistants like ChatGPT and the challenges people face when writing online counterspeech. Our study uses a survey with 458 participants who wrote counterspeech in response to three online hate posts randomly selected from a pool of 900 hate posts covering topics such as race, gender, religion, sexual orientation, or disability status. We then asked them follow-up questions to understand their perceptions of online hate speech and experience of writing counterspeech, such as their satisfaction, self-perceived effectiveness, the difficulty of their counterspeech, and their attitudes toward using AI to assist them in writing counterspeech. We used mixed-effects models to analyze the hierarchical data and capture individual and contextual effects. We investigated how the TIM between the hate post and the user’s identity affected the user’s perceived hatefulness of the hate post and their experience of responding to it with a counterspeech. We also examined how various linguistic characteristics of the user-written counterspeech were associated with their counterspeech writing experience, and their perceived hatefulness of the online hate post they were responding to. Finally, we investigated how the user’s prior use of, and attitudes towards AI writing-assistant tools were associated with their difficulty in writing online counterspeech. Our results reveal that the TIM between the hate post and the identity of counterspeech writers (or counter-speakers) influences their perception of online hate speech and their counterspeech writing experience. We found that TIM and prior exposure to hate posts (seeing more hate posts online) increased the perceived hatefulness of the hate posts, especially for race and sexual orientation topics (RQ1). Second, TIM positively influenced the satisfaction and self-perceived effectiveness of counterspeech and negatively influenced the difficulty of writing counterspeech for most topics, except for gender. We also found that counterspeech perceptions were affected by counter-speaker characteristics and behavior, such as, more frequent exposure to online hate speech, using their real name online, and higher commenting frequency. All of these factors were related to higher satisfaction and self-perceived effectiveness (RQ2). Third, linguistic characteristics of counterspeech were associated with counter-speakers’ writing experience and perceptions of hate speech. We found that the use of empathy in counterspeech was related to higher difficulty, satisfaction, and self-perceived effectiveness; longer counterspeech was related to higher satisfaction, self-perceived effectiveness, and hatefulness ratings; and more sentimentally positive counterspeech was linked to higher satisfaction and hatefulness ratings (RQ3). Finally, in an exploratory analysis, we found that prior use of ChatGPT and perceived usefulness of ChatGPT were negatively correlated with the difficulty of writing counterspeech, especially for those who found ChatGPT more useful. Contributions: We contribute to CSCW research by offering a comprehensive understanding of the various factors that shape the counter-speakers’ perception and writing of counterspeech on social media. First, we offer a theoretical lens to understand how Topic-Identity Match (TIM), counter-speakers’ characteristics, and linguistic features shape the counter-speakers’ writing experience. The theoretical explanation allows our work to extend to counter-extremist efforts more generally, informing the broader literature on ways to potentially thwart radicalization in online environments (Hawdon and Costello, 2022). Second, we extend existing research by considering a comprehensive set of counter-speakers’ characteristics, including demographic factors, political views, hate speech exposure, and social media behavior. By simultaneously examining these identity factors, we address limitations in previous studies that often focus on fewer factors in isolation. Third, we contribute to the literature investigating the linguistic characteristics of online counterspeech and how this influences perceptions of writing counterspeech narratives. We offer empirical evidence on the relationship between these characteristics and perceived effectiveness, satisfaction, and difficulty in counterspeech writing. Understanding these factors can guide the development of improved moderation tools, user interfaces promoting constructive dialogue, and AI-assisted writing systems for counterspeech on social media (Meske and Bunde, 2023). Fourth, we provide a large-scale quantitative analysis of how TIM influences perception and writing of counterspeech. While prior scholarship has mostly focused on the impact of counterspeech on the hate speakers or its effectiveness (Blackwell et al., 2017; Costello and Hawdon, 2018; Wachs and Wright, 2018), our work adds a new layer of depth by empirically validating TIM. Finally, our exploratory analysis contributes to the ongoing discourse on the role of AI in crafting online counterspeech by offering an empirical, quantitative perspective that complements the existing qualitative insights into countering hate speech on social media. Understanding how the identity of counter-speakers and the use of AI influence an individual’s willingness and ability to intervene upon encountering hate speech provides valuable insights for designing effective counter-extremism strategies."
https://arxiv.org/html/2411.01426v1,"AURA: Amplifying Understanding, Resilience, and Awareness for Responsible AI Content Work","Behind the scenes of maintaining the safety of technology products from harmful and illegal digital content lies unrecognized human labor. The recent rise in the use of generative AI technologies and the accelerating demands to meet responsible AI (RAI) aims necessitates an increased focus on the labor behind such efforts in the age of AI. This study investigates the nature and challenges of content work that supports RAI efforts, or “RAI content work,” that span content moderation, data labeling, and red teaming – through the lived experiences of content workers. We conduct a formative survey and semi-structured interview studies to develop a conceptualization of RAI content work and a subsequent framework of recommendations for providing holistic support for content workers. We validate our recommendations through a series of workshops with content workers and derive considerations for and examples of implementing such recommendations. We discuss how our framework may guide future innovation to support the well-being and professional development of the RAI content workforce.","On July 21, 2023, the United States White House released a statement detailing the voluntary commitments of companies leading in developing artificial intelligence (AI) (House, 2023). These commitments include promises to ensure AI systems are safe through “internal and external testing” before their introduction to the public. Such promises subsequently raise concerns about how human expertise is being recruited and supported in this type of testing. Thus in this paper, we explore how to best support people engaging in work practices that ensure ethical and safe AI products. We define those practices as Responsible AI (RAI) content work, which involves generating, reviewing, or reasoning about digital content with the goal of ensuring safety and ethical standards in AI systems (OpenAI, 2023c). In this paper, we focus on three key aspects of RAI content work to scope our study: content moderation, data labeling, and the emerging practice of red teaming. These areas are critical to ensuring the ethical and responsible development of contemporary AI systems. It is important to note, however, that individual RAI content workers may engage in a multitude of these activities, reflecting the multifaceted nature of their role in supporting responsible AI development. Regardless of the specific activities workers engage with, the support for human efforts behind these initiatives is often overlooked despite the importance of the work conducted (Wohn, 2019; Dosono and Semaan, 2019; Pinchevski, 2023; Schöpke-Gonzalez et al., [n. d.]; Steiger et al., [n. d.]; Gray and Suri, 2019). Without a comprehensive understanding of these efforts, we may see history repeat itself with content work facing challenges of invisibility of the workforce and a lack of well-being support crucial to workers. Prior human-computer interaction (HCI) literature on harmful content exposure within content moderation has surfaced key challenges of developing psychological symptoms such as anxiety, depression, and burnout within populations (Wohn, 2019; Dosono and Semaan, 2019; Pinchevski, 2023; Schöpke-Gonzalez et al., [n. d.]; Steiger et al., [n. d.]). However, empirical data on how these challenges manifest in other types of content work and factors unique to RAI (e.g., sudden increases in content volume due to interest in AI integration) remains limited. Studies have also explored using technologies to mitigate harmful content exposure and treat symptoms (Holman et al., 2014; Das et al., 2020; Karunakaran and Ramakrishan, 2019; Steiger et al., 2022), but were limited to primarily image and video-based content that does not cover the full spectrum of types of exposure in all types of content work. Recent calls within Computer-Supported Cooperative Work (CSCW) advocate examining the transformation of human labor within AI systems (Sheehan and Le Dantec, 2023; Cheon, 2023). In this context, we investigate the emergence of RAI content work as a new form of digital labor and the potential disruptions generative AI may bring to the digital content ecosystem, raising uncertainty about the impact on those maintaining AI system safety. Previous studies within CSCW have examined content moderation challenges specific to end-user communities (Zhang et al., 2023; Chancellor et al., 2016; Rubya and Yarosh, 2017) and platforms (Reddy and Chandrasekharan, 2023; Han et al., 2023). However, the challenges related to the well-being and work quality of content workers employed and working with AI systems have yet to be explored in depth. To address this gap, we ground our study in the lived experiences of self-identifying content workers engaged in various activities with and around content. We aim to highlight the need to evaluate challenges content workers face amidst growing AI-related content demands and to inform future practices of content work in the age of AI. Figure 1. Flow of our two-phase study. In the first phase, we conducted a survey study (N=67) and an interview study (N=22) to understand the nature of content work. From these insights, we developed a set of recommendations to improve content worker well-being. In the second phase, we validated the challenges we discovered and our recommendations to address those challenges, within the AURA framework that organize those recommendations, through interactive workshops (N=14). We take a comprehensive approach, examining all types of content work from content moderation to red teaming through a two-phase study (see Figure 1 for the study flow). In the first phase, we provide empirical insights from surveys and interviews on the nature of content work (RQ1) and the challenges content workers face (RQ2). We illustrate the multi-faceted nature of content work, detailing findings from the main factors that constitute it: workers’ roles, types of content that workers are exposed to, protective tools they use, impacts of engaging with content, and practices for collaboration. Building off of these insights, we surface challenges about misconceptions about the realities of content work, shortcomings of tools and metrics, failures of workplace support, and barriers to career growth. These challenges informed our proposal of a framework for amplifying understanding, resilience, and awareness (AURA) for RAI Content Workers comprised of four categories: recruitment, tooling, adaptive wellness, and retention. In the second phase, we further revise our recommendations through validation workshops that surface challenges and considerations for the applications of these recommendations in our framework. Overall, our study informs future improvements in the design of content work, developments that can support the well-being of workers, and progress in defining the professional identity and growth of the RAI workforce."
https://arxiv.org/html/2411.01344v1,"Can Humans Oversee Agents to Prevent Privacy Leakage? A Study on Privacy Awareness, Preferences, and Trust in Language Model Agents","Language model (LM) agents that act on users’ behalf for personal tasks can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people’s capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey (N=300𝑁300N=300italic_N = 300), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further uncovered distinct patterns of privacy behaviors, attitudes, and preferences, and the nuanced interactions between privacy considerations and other factors. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.","Recent advances in language models have led to new applications of Language Model agents (LM agents) such as AutoGPT (Richards, 2023), AgentGPT (age, [n. d.]) and personal.ai (Inc, 2024). Unlike basic language models or non-agentic AI systems, LM agents are inherently endowed with agency, allowing them to (semi-)automatically handle complex real-world tasks, such as accessing and retrieving information from connected databases (e.g., a user’s calendar) to generate and reply to emails (Muthusamy et al., 2023; Talebirad and Nadiri, 2023). These LM agents free users from having to instruct the LM step by step, potentially increasing productivity. However, this increased agency also means that LM agents can make decisions with limited human supervision, which raises new privacy challenges, especially in interpersonal communication where the agents act on behalf of the user to share information with other people. What if LM agents share information the users did not intend to disclose? Prior studies found that even without malicious attackers, LM agents can have unintended privacy leakage in their actions (Mireshghallah et al., 2023; Shao et al., 2024). For example, Shao et al. (2024) demonstrated a case where an LM agent accesses the user John’s calendar data to generate an email reply, which shares the information that John is “talking to a few companies about switching jobs” in an email to John’s manager without John’s explicit consent. This issue not only risks violating one’s own privacy but can also impact bystanders, as LM agents might inadvertently share information about other people in the users’ connected database. Such unintentional privacy leaks occur because LMs lack the ability to understand and operate under contextual privacy norms, even when privacy-enhanced prompts are used (Shao et al., 2024). Current studies mainly focus on measuring these privacy leakage (Mireshghallah et al., 2023; Shao et al., 2024) and addressing them with model alignment (Staab et al., 2023; Irving et al., 2018; Gabriel, 2020), which aims to align AI models with human value (e.g., people’s privacy preferences and norms). However, preserving people’s privacy in the use of LM agents also requires a better understanding of human users, particularly their privacy awareness, preferences, and trust, which are key factors shaping people’s privacy-preserving behaviors (Olivero and Lunt, 2004; Paramarta et al., 2018; Paspatis et al., 2023; Augustin et al., 2022). Privacy awareness is an important prerequisite for users to make informed decisions about disclosing and sharing data (Pötzsch, 2009; Malandrino et al., 2013). A lack of awareness of privacy risks can lead to unexpected consequences, resulting in uncomfortable, creepy experiences or even physical or financial harm (Pötzsch, 2009). Moreover, studying people’s privacy awareness also sheds light on the impact of using different definitions of user preferences in the model alignment efforts. Gabriel (2020) introduced multiple ways to measure user preference for model alignment, such as revealed preferences, focusing on model alignment with preferences as they are revealed through a person’s behaviour, and informed preferences, focusing on the preferences that a person would have if they were fully informed and rational. However, in light of “privacy paradox” (Kokolakis, 2017; Solove, 2021), the revealed preferences (their actual behavior) and informed preferences might be different. A key factor contributing to the privacy paradox is the lack of privacy awareness (Pötzsch, 2009; Deuker, 2010), where users are not fully aware of how their data is being used or what risks they are exposed to, leading to inaction despite a desire to protect their privacy (Pötzsch, 2009; Deuker, 2010; Dienlin and Trepte, 2015). Considering that the revealed preferences provide a practical measure for optimizing the LM agents’ actions by observing human behavior, while informed preferences are closer to individuals’ authentic preferences but are not directly observable and more costly to collect (Hadfield-Menell et al., 2016), studying users’ privacy awareness helps bridge the gap between these two types of preferences and guide model alignments closer to users’ true preferences in practice. Trust is a more direct factor that influence people’s willingness to disclose information and engage with technologies (Shin et al., 2022; Dimodugno et al., 2021). Higher levels of trust can lead to more willingness to disclose information (Joinson et al., 2010; Zimmer et al., 2010), while overtrust, often associated with “misunderstanding the risks associated with an action” (Wagner et al., 2018), can result in people overlooking privacy risks and becoming vulnerable to privacy violations (Zimmer et al., 2010; Wang and Lin, 2017). Conversely, a lack of trust can hinder the adoption of technology and limit its benefits in people’s daily lives. Research found that people are less likely to continue using AI systems if these systems make mistakes (Alexander et al., 2018). This highlights the importance of developing LM agents that can help users establish and calibrate their trust in delegating different levels of tasks to the LM agents, ensuring that the agentic systems are used effectively in people’s everyday life in a privacy-friendly way. Although privacy awareness, preference and trust are key elements in privacy research and have been studied across various domains such as online browsing (Schaub et al., 2016; Wills and Zeljkovic, 2011), e-commerce (Windl et al., 2022; Bergmann, 2008; Spiekermann et al., 2001), IoT and mobile sensing (Thakkar et al., 2022; Wang and Lin, 2017; Prange et al., 2021) and AR/VR (Cummings and Shore, 2024; Do et al., 2023), the nature of LM agents brings unique challenges. The lack of transparency due to the automation of complex tasks makes it difficult for people to know what happens behind the scenes. Moreover, people could hold flawed mental models of the LM systems (Zhang et al., 2024), which leads to incorrect expectations of the systems’ behavior. These challenges could hinder people from understanding or anticipating unintended privacy leakage in the LM agent’s actions (Zhang et al., 2024; Li et al., 2024a). Having in mind both the importance of user privacy awareness, preferences, and trust, and the challenges raised by LM agents, this work aims to study these factors in LM agents, focusing on the unintended privacy leakage risk in asynchronous interpersonal communication tasks (e.g., writing emails and creating social media posts). We are interested in examining the following research questions: RQ1: What are people’s primary concerns with LM agents, and what is the role of privacy? RQ2: How well do people oversee privacy leakage in LM agents’ actions, and how does it relate to their privacy awareness and trust in AI? RQ3: How do people’s behaviors (revealed preferences) differ from their informed preferences? How different are these preferences from person to person? To answer these questions, we conducted a task-based online survey with 300 participants. Each participant was asked to complete an asynchronous interpersonal communication task, such as drafting an email response. They were then introduced to an LM agent and asked to choose between their own responses and a response generated by an LM agent, which included sensitive items that violate contextual privacy norms. They were then asked to explain their reasoning. Afterward, participants were informed of the privacy norm tuples in the scenario and asked to rate the perceived harmfulness of the sensitive items, which we used as the ground truth to evaluate each individual’s privacy leakage. Our results show that participants often included fewer privacy leaks in their own draft, while many selected the response written by LM agents. 48.0% of participants preferred the LM agent’s response or considered both responses equally good, despite the agent’s response containing more privacy leaks, leading to an increase in privacy leakage from 15.7% to a range between 38.4% (only considering the strict preference of the agent’s response) and 55.0% (also considering the “both are good” selections). Most appeared to be unaware of the privacy leaks in the agent’s actions according to their selection rationales, exhibiting an overtrust toward the LM agent. These findings suggest that putting users in control of the final action of an LM agent is not adequate to prevent privacy risks, alluding to broader challenges in human oversight of AI systems. In addition to the known issues related to humans supervising a system handling complex, domain-specific tasks (Burns et al., 2023), there are also risks associated with the lack of capacity to oversee everyday tasks involving nuanced social norms. To better understand the reasoning behind people’s disclosure behaviors and explore the issues of overtrust and a lack of privacy awareness in agentic systems, we conducted a clustering analysis to categorize participants into four privacy profiles, based on their behaviors, attitudes, preferences, and decision-making rationales. We found that only the group Privacy Advocate (28.3%) was able to recognize the privacy leaks in the LM agent’s actions and successfully avoided more leaks when using the agent. On the other hand, participants in the Humanity Proponent (25.0%) and AI Optimist (37.0%) groups were unaware of these leaks and primarily considered the AI’s capabilities in their decision-making. Although Humanity Proponent did not leak more information, their rationales were more rooted in their fondness of the “humanness” of their own response and relatively low trust in AI, which overshadows the privacy issues and suggests that they still remain vulnerable to privacy risks, especially when AI capabilities (e.g., personalized expression) improve. Finally, we noticed a smaller group, “Privacy Paradox” (9.7%). Similar to the typical privacy paradox phenomenon, these people perceived some information items as harmful to leak when prompted about them while including the exact items in their own draft. We further studied users’ trust in the agent and comfortableness of delegating different levels of tasks to it among the four groups, and found that both the Privacy Advocate and Humanity Proponent experienced a significant decrease of trust in the agent and the comfortableness of delegating AI to draft the response and automatically send the message after the task. Finally, we found frequent disclosure of information in the users’ draft (i.e., revealed preferences), which people rated as harmful (i.e., informed preferences) afterward. This phenomenon, named subjective privacy leakage, indicates a discrepancy between users’ revealed preferences and informed preferences. This discrepancy raises discussions about how alignment goals should be operationalized to respect users’ preferences while still allowing for practical solutions. Furthermore, we also uncovered varied levels of consistency between the subjective perceptions of harmfulness and the general privacy norms, as well as the inter-participant variance of preferences, across different scenarios. These findings highlight the complex and individualized nature of privacy preferences, raising important questions about how to design LM agents that respect diverse user preferences and build calibrated trust with various users, contributing empirical insights that can guide further efforts of personalization and bidirectional alignment (Shen et al., 2024) in agentic systems. In summary, our key contributions include: 1 A task-based survey study (N=300𝑁300N=300italic_N = 300) on humans’ privacy awareness, preferences, and trust in LM agents, focusing on the unintended privacy leakage in asynchronous interpersonal communication tasks. This is the first study examining humans’ capacity to supervise LM agents’ privacy practices. 2 Four privacy profiles that reflect distinct patterns of users based on privacy behaviors, awareness, and preferences, as well as the interaction between privacy considerations and other factors, which can inform the design of tailored controls and education for different user groups of LM agents. 3 An analysis of the relationships among different ways to define and measure privacy preferences and discuss the implications on bidirectional alignment for privacy in LM agents."
https://arxiv.org/html/2411.01228v1,The Interaction Layer: An Exploration for Co-Designing User-LLM Interactions in Parental Wellbeing Support Systems,"Parenting brings emotional and physical challenges, from balancing work, childcare, and finances to coping with exhaustion and limited personal time. Yet, one in three parents never seek support. AI systems potentially offer stigma-free, accessible, and affordable solutions. Yet, user adoption often fails due to issues with explainability and reliability. To see if these issues could be solved using a co-design approach, we developed and tested NurtureBot, a wellbeing support assistant for new parents. 32 parents co-designed the system through Asynchronous Remote Communities method, identifying the key challenge as achieving a “successful chat.” As part of co-design, parents role-played as NurturBot, rewriting its dialogues to improve user understanding, control, and outcomes. The refined prototype evaluated by 32 initial and 46 new parents, showed improved user experience and usability, with final CUQ score of 91.3/100, demonstrating successful interaction patterns. Our process revealed useful interaction design lessons for effective AI parenting support.","Parenting, often hailed as the most important job in our world, exacts a significant toll on both mind and body. The perinatal period represents a critical developmental stage for the child, the parent(s), and the formation of attachment relationships (Li et al., 2017; Allan et al., 2013). This period, spanning from the onset of pregnancy to the first year postpartum, is marked by profound physical, emotional, and psychological changes, which significantly impact parental wellbeing. In particular, many expectant and new mothers encounter substantial challenges, in mental health, emotional adjustment, and caregiving (Widarsson et al., 2012), in addition to the transformation their body goes through. During this period, parents experience heightened stress, anxiety, and feelings of isolation, increasing the need for support in these areas. Yet, we know that one in three parents do not seek help (UNICEF UK, 2022). The literature on perinatal care identifies several key issues, including inadequate access to mental health resources, availability of healthcare professionals, cost to the government, stigma surrounding parental mental health, and challenges in identifying and supporting at-risk mothers (Rallis et al., 2014; Davis et al., 2022; Howard and Khalifeh, 2020). These challenges have been further aggravated by the COVID-19 pandemic (Bridle et al., 2022; Kinser et al., 2022). As a result, we see a growing body of research exploring how digital technologies can enhance perinatal care and provide support during this critical period (van den Heuvel et al., 2018; Ginja et al., 2018). Notable Human-Computer Interaction (HCI) research has laid the groundwork for exploring how digital interventions increase wellbeing across aspects such as physical and mental health of a family (Pina et al., 2017), a child’s growth (Kientz et al., 2009), sleep tracking of parents and children (Pina et al., 2020), and managing family screen-time behaviours (Hiniker et al., 2016). Today, with the availability of Artificial Intelligence (AI) to everyday users, use of general generative applications such a ChatGPT for parenting is on the rise (Quan et al., 2024). With responsible design, Large Language Models (LLMs) have the potential to facilitate a range of services, from offering self-guided mental health support (Sharma et al., 2024), real-time emotional support (Kang et al., 2024) to providing evidence-based information at scale (Lai et al., 2023). The current use of LLMs to support parental and child wellbeing is reviewed with high potential and ethical cautions (Ashraf, 2024; Stade et al., 2024). Their application within this context is fraught with limitations and uncertainties, especially of accuracy, empathy, and usefulness (Kaneda et al., 2023). Given these challenges, our research adopts an exploratory approach, probing into a small subset of these broader issues. We focus on understanding how interactions between LLMs and parents might be designed to be perceived as helpful and engaging. Our goal is to explore what constitutes a usable and useful interaction in these contexts and how LLMs can be integrated into wellbeing support in ways that are practically engaging and thus useful for parents. To this end, we conducted a six-part online study involving 78 parents, who trialled iterative improvements of NurtureBot, a technology probe, designed to support parental wellbeing. This study, while not aimed at delivering a finalised solution, served as an exploratory step toward understanding how parents interact with LLM-based agents and identifying necessary features to make these tools genuinely supportive. The following Research Questions (RQs) guide our investigation into the user interaction aspects of engaging with an LLM-based parental wellbeing assistant: • RQ1: What are the main problems parents face when interacting with an LLM-based parental wellbeing assistant in support-seeking scenarios? • RQ2: What key user needs must a parental wellbeing assistant address? How do parents describe their ideal vision of how the assistant should function, particularly regarding their understanding of its workings, control over it, and opportunities to improve the outcomes? • RQ3: How can we employ co-design methods to craft interactions around AI, enhancing usability and the overall user experience? Study Flow Overview This six-part user-centred design process, as illustrated in Figure 1, guided the iterative development of NurtureBot. To mitigate emotional risks, we engaged a ‘slightly-out-of-target’ group of parents, each with a child aged five or six. These parents simulated stages of their perinatal period and reflected on past challenges with NurtureBot. Using a mixed-methods approach, we collected both qualitative and quantitative feedback, with NurtureBot v1, v2, and v3 tested over a five-day period.On the final day, participants completed the Chatbot Usability Questionnaire (CUQ) (Holmes et al., 2019), assessing usability and engagement. (see Figure2). • Part I: The Initial Prototype: NurtureBot v1 The development of NurtureBot v1 leveraged LLM technology, informed by a community wellbeing initiative supporting new parents. Zero-shot prompting was used to integrate supportive conversation starters and key features, including empathetic chatting, wellbeing exercises, and parenting information (see Section 3). • Part II: NurtureBot v1 Testing and Analysis A cohort of 32 parents engaged with NurtureBot v1, generating 144 conversations. Thematic analysis identified several issues, including lack of conversational flow, robotic tone, transactional attitude, limited personalisation, and localisation. Despite these challenges, NurtureBot v1 achieved a CUQ score of 85.4, surpassing the benchmark of 68 (Holmes et al., 2019), indicating good usability (see Section 4). • Part III: ARC Co-Design Sessions and Analysis The Asynchronous Remote Community (ARC) (MacLeod et al., 2016; Prabhakar et al., 2017) co-design sessions engaged the same cohort of parents with Miro board activities (Miro, 2011). – Parents first prioritised key issues, with “successful chat” emerging as the top priority. – Then they reimagined and rewrote NurtureBot’s dialogues, aiming for better user understanding, user control, and improved outcomes, a structure akin to a cybernetic loop (Patten and Odum, 1981). – Thematic analysis of 189 revised dialogues led to requests for essential features such as metaphorical explanations, empathetic engagement, feature introductions, personalising exchanges, and step-by-step guidance (see Section 5). • Part IV: The Interaction Layer: NurtureBot v2 NurtureBot v2 introduced an interaction layer with three essential components (See Figure 7): (i) A core interaction principle that enabled users to ”Understand, Control, and Improve” exchanges, (ii) Interaction levels based on few-shot prompting, incorporating co-designed dialogues from ARC sessions, and (iii) Interaction elements, tailored to address user needs identified during ARC. • Part V: NurtureBot v2 Testing and Analysis NurtureBot v2 underwent testing with the same initial cohort, yielding 104 conversations. Our analysis revealed that two minor issues persisted: repetitive conversational hints and the delivery of parenting information in long, overwhelming paragraphs. Additionally, themes such as localisation and conversational memory, which we did not prioritise addressing in this study, continued to emerge. NurtureBot v2 achieved a CUQ score of 87.4, indicating further improvement, though the potential bias from participants’ involvement in co-design was considered (see Section 7). • Part VI: Prototype Improvement: NurtureBot v3 and Validation To ensure an unbiased evaluation, we refined NurtureBot further and tested v3 with a new cohort of 46 parents. Feedback analysis identified two new problems, the need for more detailed responses at times, difficulty following step-by-step exercises via text, and repeated requests for additional features such as videos and memory retention for personalised interactions. NurtureBot v3 received a CUQ score of 91.3, marking a robust improvement in usability (see Section 8). Our contributions are: • Empirical Findings and Implications for co-designing ‘usable and useful’ LLM-based parental wellbeing support systems, from a longitudinal four-week iterative study, gathered from 78 parents, eliciting the interactional challenges they encountered and the solutions proposed to address these issues. • The Interaction Layer a prompt architecture that affords in-situ user understanding, user control, and conversational improvements by directly incorporating end-user inputs as prompt instructions usable by LLMs, addressing usability challenges to enhance user experience with AI-driven support tools for parenting and beyond. This study initiates our exploration into co-designing agents for parental wellbeing, setting the foundation for future research to refine and expand AI tools. The diverse parenting experiences and emotional complexities highlight the need for continued development and testing."
https://arxiv.org/html/2411.01049v1,Exploratory Models of Human-AI Teams: Leveraging Human Digital Twins to Investigate Trust Development,"As human-agent teaming (HAT) research continues to grow, computational methods for modeling HAT behaviors and measuring HAT effectiveness also continue to develop. One rising method involves the use of human digital twins (HDT) to approximate human behaviors and socio-emotional-cognitive reactions to AI-driven agent team members (Barricelli & Fogli, 2024). In this paper, we address three research questions relating to the use of digital twins for modeling trust in HATs. First, to address the question of how we can appropriately model and operationalize HAT trust through HDT HAT experiments, we conducted causal analytics of team communication data to understand the impact of empathy, socio-cognitive, and emotional constructs on trust formation. Additionally, we reflect on the current state of the HAT trust science to discuss characteristics of HAT trust that must be replicable by a HDT such as individual differences in trust tendencies (e.g., propensity to trust, Jessup et al., 2019), emergent trust patterns (e.g., trust violation and repair, Wildman et al., 2024), and appropriate measurement of these characteristics (e.g., growth modeling, Abramov et al., 2020). Second, to address the question of how valid measures of HDT trust are for approximating human trust in HATs, we discuss the properties of HDT trust: self-report measures, interaction-based measures, and compliance type behavioral measures. Additionally, we share results of preliminary simulations comparing different LLM models for generating HDT communications and analyze their ability to replicate human-like trust dynamics. Third, to address how HAT experimental manipulations will extend to human digital twin studies, we share experimental design focusing on propensity to trust for HDTs vs. transparency and competency-based trust for AI agents.","The integration of artificial intelligence (AI) into operational environments has become increasingly vital across diverse domains, fundamentally transforming how humans and machines collaborate to achieve shared objectives. Over the past decade, Human-AI Teaming (HAT) research has emerged as a critical field, with scholars applying cognitive science principles to understand the complexities of these novel partnerships. This growing body of research reflects the urgency of understanding how humans and AI can work together effectively, safely, and productively. As the HAT literature expands and AI capabilities advance, innovative methodologies for studying these interactions have emerged. One promising approach involves the use of human digital twins (HDTs) - computational models designed to replicate human responses and behaviors within HAT contexts. These HDTs can be configured to simulate both state-based responses (such as transient cognitive and affective reactions) and trait-based characteristics (including dispositional individual differences), offering a versatile alternative to traditional human-subjects research [1, 2]. The advantages of HDTs in HAT research are significant. Beyond addressing the practical constraints of human-subjects studies, such as cost and recruitment challenges, HDTs provide unprecedented control over experimental variables and the ability to rapidly test multiple scenarios. This capability is particularly valuable for investigating complex team phenomena, with trust emerging as a critical area of focus. Trust - the willingness to be vulnerable to another agent’s actions - serves as a fundamental determinant of HAT effectiveness and success. However, the validity of using HDTs to study trust dynamics in HATs requires careful examination. This paper addresses three crucial questions regarding the implementation of HDTs in trust research: (1) How can we effectively model and measure HAT trust using HDT-based approaches? (2) What are the essential characteristics of HAT trust that must be operationalized in HDT trust models? (3) How do experimental manipulations from traditional HAT studies translate to HDT-based research? By examining these questions, we aim to establish a framework for validating and implementing HDTs in HAT trust research, ultimately advancing our understanding of human-AI collaboration and trust development."
https://arxiv.org/html/2411.01007v1,”When Two Wrongs Don’t Make a Right” - Examining Confirmation Bias and the Role of Time Pressure During Human-AI Collaboration in Computational Pathology,"Artificial intelligence (AI)-based decision support systems hold promise for enhancing diagnostic accuracy and efficiency in computational pathology. However, human-AI collaboration can introduce and amplify cognitive biases, such as confirmation bias caused by false confirmation when erroneous human opinions are reinforced by inaccurate AI output. This bias may worsen when time pressure, ubiquitously present in routine pathology, strains practitioners’ cognitive resources. We quantified confirmation bias triggered by AI-induced false confirmation and examined the role of time constraints in a web-based experiment, where trained pathology experts (n=28) estimated tumor cell percentages. Our results suggest that AI integration may fuel confirmation bias, evidenced by a statistically significant positive linear-mixed-effects model coefficient linking AI recommendations mirroring flawed human judgment and alignment with system advice. Conversely, time pressure appeared to weaken this relationship. These findings highlight potential risks of AI use in healthcare and aim to support the safe integration of clinical decision support systems.","Recent exponential progress in artificial intelligence (AI) research paves the way for its employment in high-stakes fields such as healthcare. With rising staff shortages leading to increased workloads and poorer treatment outcomes (Metter et al., 2019), AI technologies are being increasingly deployed to support medical professionals. These AI systems hold promise to augment practitioners’ capabilities by enhancing diagnostic accuracy, streamlining workflows and boosting efficiency (Pantanowitz, 2010). In particular, specialized deep learning (DL)-based approaches in the medical imaging domain have achieved accuracy on par with or even surpassing that of human experts (Aubreville et al., 2020, 2023; Bertram et al., 2022). This success enables emerging fields like computational pathology to provide new tools augmenting traditional microscopy. However, due to the safety-critical nature of the medical sector a fully autonomous AI may not be desirable nor legally feasible (Schemmer et al., 2022). With human lives at stake and given the complexity of legal liability for machine misjudgments, AI-powered decision support systems, where the final judgment for diagnoses and treatment choices remains with the medical expert, present a more appropriate solution. DSSs embody the idea of hybrid intelligence, combining the distinct but complementary competencies of humans and machines, which can achieve outcomes that surpass the individual capabilities of each party alone (Dellermann et al., 2019). Nonetheless, the need for practitioner oversight risks creating an entirely new set of challenges, as the mere presence of a second opinion in form of AI advice can influence the medical decision-making process and potentially evoke or amplify cognitive biases, i.e., systematic patterns of deviation from rationality in judgment or decision-making (Caverni et al., 1990). For instance, quantitative methods in pathology, such as the manual scoring of biomarkers, are susceptible to high interobserver variability and bias due to the challenges of visual quantification (Aeffner et al., 2017). Therefore, second opinion reviews are common practice for mitigating the subjectivity inherent in visual assessments (Farooq et al., 2021). A study on the identification of mitotic figures explored the application of a deep learning algorithm as a second rater, allowing participating pathologists to decide whether to accept AI inferences. The results showcase increased interobserver consistency and improved performance with AI assistance compared to unaided evaluation (Bertram et al., 2022). However, insights from related human-computer interaction (HCI) research suggest that individuals are more inclined to accept AI advice when it aligns with their own initial assessments (Nazaretsky et al., 2021; Bashkirova and Krpan, 2024; Wysocki et al., 2023). This can result in false confirmation when erroneous human judgments are reinforced by inaccurate AI recommendations, causing one error to compound another. The human tendency to selectively seek out and accept information, that aligns with pre-existing beliefs while disregarding contradictory evidence (Pines, 2005) is commonly referred to as confirmation bias. This cognitive bias poses a significant challenge to the effectiveness of human-AI collaboration, as AI predictions cease to be independent from the medical expert due to a greater likelihood of accepting congruent recommendations. Consequently, practitioners may not fully leverage the benefits of AI integration if they disregard accurate but conflicting model outputs. Of greater concern, both human and AI-generated errors may go unnoticed when coinciding error patterns lead to false confirmation, potentially impacting the quality of patient care. Moreover, the manifestation of confirmation bias could be further amplified by the time pressure practitioners face in everyday pathology. As time becomes scarce, the prioritization of the decisions and tasks perceived as most relevant leads to selective information processing (Edland and Svenson, 1993). Empirical studies involving experts quantifying confirmation bias in medical decision-making during human-AI collaboration are still scarce, with none conducted in the field of pathology despite the anticipated use of AI in this area (Berbís et al., 2023). Most existing research has focused on discrete decisions (Bashkirova and Krpan, 2024; Wysocki et al., 2023; Rosenbacke, 2024), typically employing vignette-based study designs, resulting in a substantial gap in the exploration of continuous decisions, which, however, constitute a substantial portion of routine medical tasks. Furthermore, there has been no empirical investigation into the interplay between time pressure and AI-induced confirmation bias in healthcare. To address this gap, we conducted a within-subject, four-condition online experiment with trained pathology experts to examine whether and how AI integration might lead to the manifestation of confirmation bias in computational pathology and to evaluate the impact of time pressure on this dynamic. As part of this investigation, participating pathologists estimated the tumor cell percentage (TCP) on 20 image patches from different hematoxylin and eosin (H&E)-stained slides first independently, building a baseline, and second, after a two-week wash-out period, with the aid of a TCP prediction provided by an AI algorithm (see Fig. 1). To introduce an additional layer of complexity, half of the slides in each round were rated under the constraint of a timer. Our findings reveal that AI suggestions can indeed trigger confirmation bias, particularly when system output mirrors the medical experts’ independent and potentially flawed judgments. This is evidenced by: • A statistically significant, positive linear-mixed-effects model coefficient linking close alignment of AI recommendations with erroneous independent assessments and participants’ adoption of the system’s advice. • A different weighting of congruent versus incongruent system suggestions in the decision-making process. • A general increase in participants’ confidence in their assessments and greater alignment with system predictions when the AI output is consistent with their prior judgments. Contrary to our expectations, while time pressure led to heightened reliance on AI advice, it appeared to reduce confirmation bias. This is indicated by a weakened relationship between the congruence of AI output with initial assessments and the subsequent alignment with AI recommendations. In summary, the contributions of this research are as follows: (1) We demonstrate the presence of confirmation bias during medical-decision-making, for the first time in the domain of computational pathology, induced by AI integration due to false confirmation of erroneous human judgments by AI predictions. This finding underscores the potential pitfalls that must be addressed before widespread deployment of AI-based clinical decision support systems in clinical practice. Furthermore, by conducting our experiment directly with medical professionals on a routine task, we enhance the external validity of our findings. (2) We center our study design and data analysis on continuous decisions, integral to many routine medical tasks, thereby addressing a gap in confirmation bias research within HCI in healthcare. (3) For the first time, we examine how time stress affects confirmation bias in AI-supported medical decision-making. Our findings showcase that while time pressure increases overall reliance on AI advice, it does not seem to exacerbate confirmation bias. These insights contribute to a more comprehensive understanding of cognitive biases and their influencing factors in AI-assisted medical decision-making."
https://arxiv.org/html/2411.00998v1,Automation Bias in AI-Assisted Medical Decision-Making under Time Pressure in Computational Pathology,"Artificial intelligence (AI)-based clinical decision support systems (CDSS) promise to enhance diagnostic accuracy and efficiency in computational pathology. However, human-AI collaboration might introduce automation bias, where users uncritically follow automated cues. This bias may worsen when time pressure strains practitioners’ cognitive resources. We quantified automation bias by measuring the adoption of negative system consultations and examined the role of time pressure in a web-based experiment, where trained pathology experts (n=28) estimated tumor cell percentages. Our results indicate that while AI integration led to a statistically significant increase in overall performance, it also resulted in a 7% automation bias rate, where initially correct evaluations were overturned by erroneous AI advice. Conversely, time pressure did not exacerbate automation bias occurrence, but appeared to increase its severity, evidenced by heightened reliance on the system’s negative consultations and subsequent performance decline. These findings highlight potential risks of AI use in healthcare.","1 Problem Statement and Related Work Driven by the success of deep learning algorithms in medical imaging, computational pathology seeks to augment practitioner capabilities in areas traditionally challenging for humans like quantitative image analysis tasks e.g., manual biomarker scoring. Given the safety-critical nature of healthcare and the complexity of legal liability for machine misjudgments, clinical decision support systems (CDSS) allow the full benefits of artificial intelligence (AI) integration, including improved diagnostic accuracy and increased efficiency, while keeping the responsibility for the final diagnosis with the medical expert. However, the necessity for practitioner oversight harbors the risk of introducing a new set of challenges, as the mere presence of AI advice could trigger or amplify cognitive biases, which are systematic patterns of deviation from rationality in judgment, such as automation bias (AB). AB refers to the tendency to treat automated cues as infallible, following them unquestioningly instead of vigilant information seeking. This leads to errors when issues go unnoticed because the system fails to detect them (omission errors) or when incorrect automation output is uncritically adopted (commission errors) [Goddard2012]. Environmental factors like time pressure, ubiquitously present in routine pathology, can place strain on cognitive resources, resulting in heuristic-based usage of decision support systems (DSS) or even automation overdependence [Goddard2012]. In essence, time stress may amplify both the frequency and magnitude of AB. While most studies on CDSS highlight the overall performance gains from AI integration, there have been few studies on the effect size of AB in medical decision-making [Goddard2012], with none in the field of pathology. While existing research typically evaluates overall acceptance of false AI advice, following Goddard et al. [Goddard2012] we will measure AB through commission errors, where a previously correct independent evaluation is overturned by incorrect AI guidance (negative consultation), allowing us to isolate AB from other cognitive biases for more precise quantification. Research on the impact of time pressure on AB presents conflicting results, suggesting its effects may be context-dependent: some work demonstrates increased automation dependence under time constraints [Rice2008], while others report reduced system reliance in time critical situations [Rieger2022]."
https://arxiv.org/html/2411.00949v1,From Fake Perfects to Conversational Imperfects: Exploring Image-Generative AI as a Boundary Object for Participatory Design of Public Spaces,"Designing public spaces requires balancing the interests of diverse stakeholders within a constrained physical and institutional space. Designers usually approach these problems through participatory methods but struggle to incorporate diverse perspectives into design outputs. The growing capabilities of image-generative artificial intelligence (IGAI) could support participatory design. Prior work in leveraging IGAI’s capabilities in design has focused on augmenting the experience and performance of individual creators. We study how IGAI could facilitate participatory processes when designing public spaces, a complex collaborative task. We conducted workshops and IGAI-mediated interviews in a real-world participatory process to upgrade a park in Los Angeles. We found (1) a shift from focusing on accuracy to fostering richer conversations as the desirable outcome of adopting IGAI in participatory design, (2) that IGAI promoted more space-aware conversations, and (3) that IGAI-mediated conversations are subject to the abilities of the facilitators in managing the interaction between themselves, the AI, and stakeholders. We contribute by discussing practical implications for using IGAI in participatory design, including success metrics, relevant skills, and asymmetries between designers and stakeholders. We finish by proposing a series of open research questions.","Designing public spaces is a complex process that requires balancing the interests of diverse stakeholders and conquering constraints from physical, cultural, and institutional spaces. Designers generally rely on participatory methods to identify, comprehend, and critically assess diverse needs and preferences from multi-stakeholders (e.g., Hou and Rios, 2003; Shaftoe, 2012; Raaphorst et al., 2019; Gaete Cruz et al., 2023). However, designers often struggle to successfully elicit how to represent different cultures and identities in the built environment due to existing asymmetries between themselves and different stakeholders, such as power dynamics, communication barriers, cultural sensibility, and access to resources and information (Hou, 2013; Delgado et al., 2023). To facilitate conversations with stakeholders, designers often rely on tools and methods such as reference images, generative urban design models, and different technological platforms (e.g., Boudjelida et al., 2016; Quan, 2022; Ardhianto et al., 2023; Eanes et al., 2018; Jiang et al., 2023; Raaphorst et al., 2019). Despite such efforts, research has found that perspectives from various communities are still not equally incorporated into design outputs, often leaving out the needs of underrepresented groups and their cultural heritage (Jiang et al., 2023; Hou, 2013). With the rapidly growing capabilities of image-generative artificial intelligence (IGAI), recent work advocates the potential of leveraging IGAI tools to support communication and collaboration between designers and their public stakeholders (Guridi et al., 2024; Jiang et al., 2023). We examine whether IGAI tools deliver their promise and identify issues that remain as obstacles to effective co-design processes (Guridi et al., 2024; Zhang et al., 2023; Struppek et al., 2024; Schramowski et al., 2023). Applying IGAI to support public engagement in designing public spaces opens up a new avenue for human-AI co-design. While leveraging generative AI’s capabilities in design tasks has attained extensive research interests, the majority of such work focuses on how AI-powered tools augment the experience and performance of individual creators (e.g., Chiou et al., 2023; Koch et al., 2019; Chung and Adar, 2023; Hwang, 2022; Lawton et al., 2023; Wang et al., 2023). Commonly, individual creators have established preferences, targeted audiences, and clear expectations for how they might respond to new technologies and tools. By contrast, designing public spaces demands more extensive, multi-stakeholder involvement, while each participant lacks awareness of the other’s needs and wants. Moreover, practitioners often encounter constraints from physical spaces and institutional hurdles. These challenges are less apparent to those without rich domain knowledge, making it more challenging to avoid design obstacles proactively. To explore how IGAI could facilitate public space design, we conducted a case study using IGAI as a boundary object in interviews with young legal immigrants to elicit information for designing a public park in Los Angeles. Our study focuses on how designers and stakeholders interact and how IGAI shaped their discussion of experiences, needs, and desired qualities to design a more welcoming public space. We investigate the following question: How can IGAI support participatory processes when designers and stakeholders co-design public spaces? This study is part of a larger project that analyzes how technology can help spatialize translocality by identifying immigrants’ desires, visions, and tensions in cultural and spatial negotiation. We conducted a three-stage study to explore, examine, and reflect on the use of IGAI tools as boundary objects to mediate conversations between designers and stakeholders in public space design. We collected data and extracted insights from multiple interviews and workshops. We organize our findings into three themes as follows: (1) how multi-stakeholders jointly redefine successful generative output in IGAI-mediated participatory sessions, (2) how IGAI promoted space-aware conversations, and (3) how IGAI-mediated conversations are subject to the abilities of the facilitators in managing the interaction between themselves, the AI, and stakeholders. All these findings shift the focal point from ”Fake Perfects” to ”Conversational Imperfects,” suggesting that producing ’imperfect’ results that prompt richer conversations should be prioritized over attaining accurate (’perfect’) images in participatory processes. We discuss three core contributions: First, we provide practical implications for designing technologies to seek public engagement in public space design. We discuss the need to shift from object- to process-oriented participatory methods; this implies future participatory tool design should focus on interfaces and features that enable moderators of a participatory session to probe richer conversations with their participants. Second, we discuss the importance of AI literacy, technical expertise, and soft skills to navigate conversations when adopting novel technologies to facilitate participatory processes. Third, we propose a research agenda for technology-mediated engagement in the design of public spaces. We encourage future work to investigate these open research questions."
https://arxiv.org/html/2411.00820v1,AutoGLM: Autonomous Foundation Agents for GUIs,"We present AutoGLM, a new series in the ChatGLM family [11], designed to serve as foundation agents for autonomous control of digital devices through Graphical User Interfaces (GUIs). While foundation models excel at acquiring human knowledge, they often struggle with decision-making in dynamic real-world environments, limiting their progress toward artificial general intelligence. This limitation underscores the importance of developing foundation agents capable of learning through autonomous environmental interactions by reinforcing existing models. Focusing on Web Browser and Phone as representative GUI scenarios, we have developed AutoGLM as a practical foundation agent system for real-world GUI interactions. Our approach integrates a comprehensive suite of techniques and infrastructures to create deployable agent systems suitable for user delivery. Through this development, we have derived two key insights: First, the design of an appropriate ""intermediate interface"" for GUI control is crucial, enabling the separation of planning and grounding behaviors, which require distinct optimization for flexibility and accuracy respectively. Second, we have developed a novel progressive training framework that enables self-evolving online curriculum reinforcement learning for AutoGLM. Our evaluations demonstrate AutoGLM’s effectiveness across multiple domains. For web browsing, AutoGLM achieves a 55.2% success rate on VAB-WebArena-Lite (improving to 59.1% with a second attempt) and 96.2% on OpenTable evaluation tasks. In Android device control, AutoGLM attains a 36.2% success rate on AndroidLab (VAB-Mobile) and 89.7% on common tasks in popular Chinese APPs. Select AutoGLM capabilities are now available through the Qingyan Browser Plugin for web applications and via Form Applications for invited Android testing. Additional results and materials will be released at https://github.com/THUDM/AutoGLM.","Foundation models, including Large Language Models (LLMs) [5; 27; 7; 2; 42; 11] and Large Multimodal Models (LMMs) [20; 25; 26; 1], have captured widespread attention for their remarkable language understanding and generation capabilities. Through extensive self-supervised [22] pre-training on internet-scale corpora, these models have acquired not only knowledge and language abilities but also human-like reasoning and planning capabilities, giving rise to LLMs as Agents [21; 28]. These agents have demonstrated their utility across diverse domains, including coding [35; 16; 44], data analysis [14; 21], and gaming [34; 18], charting a promising course toward Artificial General Intelligence (AGI) through the development of multimodal Foundation Agents [23] that serve as generalists across multiple tasks and environments. Figure 1: AutoGLM real phone use demonstration for instruction “Order a hot coconut latte from Luckin Coffee with half sugar” for Chinese Android APP Meituan. See more videos. Figure 2: AutoGLM real web-browser use demonstration for instruction “Could you help me book a reservation for my parents and I at Megan’s Kitchen on October 23, 2024 at 7:30 PM through OpenTable? You can reach me at 146xxxxxxxx, China.” on the website OpenTable. See more videos. The ubiquity of digital devices presents a unique opportunity for GUI-capable agents [13; 46; 43; 17]. This domain offers several advantages: GUI simulators can be readily deployed in parallel for data annotation and online reinforcement learning (RL); GUI environments provide rich textual and visual inputs essential for foundation model agents, but in a safer and controllable environments compared to embodied environments; and GUI agents hold broad practical appeal given their extensive potential user base. Their successful development could fundamentally transform human-device interaction. However, the development of foundation agents for GUI faces a critical challenge: the scarcity of decision-making data in existing pre-training sets. While the internet contains vast human knowledge, it primarily consists of static information that inadequately captures human decision-making and environmental interaction. Building capable foundation agents requires enriching them with dynamic knowledge, either through direct interaction with real-world environments or through learning from synthesized trajectories. Such foundation agents can then self-evolve in the digital world, iteratively improving to achieve genuine general intelligence. Crucially, these systems must be developed with progressive user deployment in mind. Autonomous agents are designed to augment, not replace, human capabilities. User deployment serves the dual purpose of teaching agents effective human assistance while allowing humans to adapt to intelligent assistants. This approach also enables researchers to systematically understand, discover, and examine both the potential benefits and risks of autonomous foundation agents during development. In response to these opportunities and challenges, we introduce AutoGLM, a series of foundation agents built upon the ChatGLM [11] model family. AutoGLM represents a pioneering attempt to develop foundation agent prototypes for two fundamental GUI scenarios: Web Browser and Android. To address the data scarcity challenge, we employ a comprehensive suite of training techniques and develop key infrastructures for user deployment. This process has yielded two crucial insights: • Intermediate Interface Design: We find it essential to design an intermediate interface that disentangles planning and grounding behaviors in foundation GUI agents. They present distinct requirements – planning demands flexibility and error recovery, while grounding emphasizes action accuracy. Their separation enables more agile development and enhanced performance. • Self-Evolving Online Curriculum RL [30]: We recognize that error recovery [23] is crucial for robust and deployable agent applications, yet it remains difficult to acquire through offline training alone. Additionally, the shortage of instructions and trajectories impedes training progress. We address this challenge through self-evolving RL, implemented according to a progressive weak-to-strong curriculum schedule in an online manner. Building on these insights, AutoGLM demonstrates exceptional capabilities across various benchmarks and real-world tests. In Web Browsing, AutoGLM achieves a task success rate (SR) of 55.2% on the challenging VAB-WebArena-Lite [47; 23], substantially surpassing GPT-4o’s 18.2%. With a second attempt opportunity, this improves to 59.1%. On OpenTable real-world booking tasks, AutoGLM achieves 96.2% SR, outperforming both GPT-4o (62.6% SR) and Agent Q [29] (81.7%). Select AutoGLM web capabilities are publicly available via the Qingyan Browser Plugin on both Chrome and Edge Plugin Store. See the real example in Figure 2. For Android control, AutoGLM achieves 36.2% SR on AndroidLab [37] (previously known as VAB-Mobile [23]), a comprehensive interactive Android evaluation framework. This performance exceeds both GPT-4o (31.2% SR) and Claude-3.5-Sonnet (29.0% SR). We have also implemented a practical Android application via AccessibilityService for autonomous device control. In human evaluation, AutoGLM achieves an impressive 89.7% SR on common tasks (e.g., “Please Order a large iced Americano with half sugar from the nearest coffee shop for delivery to my company”) across popular Chinese APPs. The Android client is currently available for invited internal testing through Form Applications. See the real example in Figure 2."
https://arxiv.org/html/2411.02348v1,Can large language models generalize analogy solving like children can?,"When we solve an analogy we transfer information from a known context to a new one through abstract rules and relational similarity. In people, the ability to solve analogies such as “body : feet :: table : ?” emerges in childhood, and appears to transfer easily to other domains, such as the visual domain “(((( : )))) :: <<< : ?”. Recent research shows that large language models (LLMs) can solve various forms of analogies. However, can LLMs generalize analogy solving to new domains like people can? To investigate this, we had children, adults, and LLMs solve a series of letter-string analogies (e.g., a b : a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek alphabet), and a far transfer domain (list of symbols). As expected, children and adults easily generalized their knowledge to unfamiliar domains, whereas LLMs did not. This key difference between human and AI performance is evidence that these LLMs still struggle with robust human-like analogical transfer.","You may be familiar with the analogy “consciousness is like an iceberg”. Here, people can intuitively infer the below-the-surface depth and complexity of consciousness by relating it to an iceberg, whose mass is mostly found under water, just as our subconscious dwells under our conscious minds. This intuitive ability emerges in childhood Goddu et al., (2020); Gentner, (1988); Stevenson and Hickendorff, (2018). However, it is a subject of debate whether analogical reasoning has emerged in Large Language Models (LLMs) Webb et al., (2023); Lewis and Mitchell, (2024); Hodel and West, (2023); Webb et al., (2024). More importantly, are LLMs able to solve analogies at this level of conceptual abstraction and generalize to novel domains Mitchell, (2021); Shiffrin and Mitchell, (2023)? In this study, we investigate analogical transfer at two levels of abstraction (near and far), and compare LLM performance not only to adults, but also to children, who are still developing analogical reasoning abilities. We ask the question: Can LLMs can generalize analogy solving like children can? Analogical reasoning, the process of applying a known concept to understand something new through relational similarity, is fundamental to the way people think and learn Holyoak, (2012); Gentner and Hoyos, (2017). This is because we humans can easily generalize – that is, transfer principles discovered in one domain to new domains that share varying degrees of similarity with the original (Doumas et al.,, 2022). This can be principles in near contexts that are similar in terms of concrete attributes (e.g., shape, “a pyramid is like an iceberg”) or in farther contexts that are only similar in terms of abstract relations (e.g., abstraction of depth, “consciousness is like an iceberg”) Barnett and Ceci, (2002). Near analogies tend to be easier for both adults and children to solve than far analogies Stevenson et al., (2023); Jones et al., (2022); Thibaut and French, (2016). And, in general, adults are better at solving analogies than children. But, when the required domain knowledge and a causal framing are present then children can solve analogies such as “body is to feet as table is to ?” as early as the 3-4 years-old (e.g., Goddu et al.,, 2020; Goswami,, 1991). And when analogies are presented in a more challenging or far context young children tend to revert to associative strategies, e.g., replying ’egg’ to ’dog is to doghouse as chicken is to ?’ instead of ’chicken coop’ Stevenson and Hickendorff, (2018); Gentner, (1988); Thibaut and French, (2016). There are many tasks used to study analogical reasoning and transfer in people, from verbal to geometric to scene analogy problems (e.g., Ichien et al.,, 2020; Richland et al.,, 2006; Mulholland et al.,, 1980). However, many of these tasks are either not suitable for children (e.g., verbal analogies may contain unfamiliar words or relations for children) or to LLMs (e.g., visual analogies designed for children are still difficult for today’s multimodal models Yiu et al., (2024)). Therefore, we need a domain that is text-based, but doesn’t require domain knowledge beyond what a typical child or LLM would know. Letter-string analogies fit the bill as they require very little domain knowledge and offer an idealized scenario to examine analogical reasoning in a “pure, uncontaminated way” (Hofstadter,, 1984, p. 3). In these puzzles, a string of letters is transformed according to one or more rules, and the task is to use analogy and apply the same transformations to a new string. For example, “If abc changes to abd, what should pqr change to?” (Mitchell,, 2021). Letter-string analogy solving has been studied in human adults and LLMs. For example, (Webb et al.,, 2023) showed that GPT-3 is able to solve letter-string analogies better than college students. (Lewis and Mitchell,, 2024) showed that GPT-models solved letter-string analogies at about 60% accuracy in the Latin alphabet domain, somewhat below the level of adults they tested. Interestingly, (Lewis and Mitchell,, 2024) and (Hodel and West,, 2023) found that GPT-3’s performance degraded when presented with these same analogies using an alphabet of shuffled letters. Moreover, (Lewis and Mitchell,, 2024) showed that GPT-models had great difficulty solving letter-string analogies in an unfamiliar alphabet of symbols, whereas people did not. As such, there is conflicting evidence of whether LLMs can generalize analogy solving to novel domains (Lewis and Mitchell,, 2024; Webb et al.,, 2024; Hodel and West,, 2023), something that comes easily to adults (e.g., Thibaut et al.,, 2022; Doumas et al.,, 2022), and that even children appear capable of when domains share structural similarities Chen, (1996); Gentner and Toupin, (1986); Bobrowicz et al., (2020); Holyoak et al., (1984). Thus, while there is some evidence to suggest that LLMs can solve letter-string analogies at around the same level as people, it is unclear whether these models understand the problem and are actually using analogical reasoning Opiełka et al., (2024); Stevenson et al., (2023); Moskvichev et al., (2023). In this study, we investigate whether LLMs can generalize analogy solving to new domains like adults and 8-year-old children can at two levels of abstraction. To this end, we compare how adults, children, and LLMs generalize analogy solving on the letter-string task to both near (Greek alphabet) and far (Symbol list) domains."
https://arxiv.org/html/2411.02179v1,CleAR: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality,"High-quality environment lighting is the foundation of creating immersive user experiences in mobile augmented reality (AR) applications. However, achieving visually coherent environment lighting estimation for Mobile AR is challenging due to several key limitations associated with AR device sensing capabilities, including limitations in device camera FoV and pixel dynamic ranges. Recent advancements in generative AI, which can generate high-quality images from different types of prompts, including texts and images, present a potential solution for high-quality lighting estimation. Still, to effectively use generative image diffusion models, we must address their key limitations of generation hallucination and slow inference process. To do so, in this work, we design and implement a generative lighting estimation system called CleAR that can produce high-quality and diverse environment maps in the format of 360∘superscript360360^{\circ}360 start_POSTSUPERSCRIPT ∘ end_POSTSUPERSCRIPT images. Specifically, we design a two-step generation pipeline guided by AR environment context data to ensure the results follow physical environment visual context and color appearances. To improve the estimation robustness under different lighting conditions, we design a real-time refinement component to adjust lighting estimation results on AR devices. To train and test our generative models, we curate a large-scale environment lighting estimation dataset with diverse lighting conditions. Through quantitative evaluation and user study, we show that CleAR outperforms state-of-the-art lighting estimation methods on both estimation accuracy and robustness. In particular, CleAR achieves 56% - 51% (an average of 53%) accuracy improvement on virtual object renderings across objects with three distinctive types of materials over different reflective properties. Moreover, CleAR supports real-time refinement of lighting estimation results, ensuring robust and timely environment lighting updates for AR applications. Our end-to-end generative estimation takes as fast as 3.2 seconds, outperforming state-of-the-art methods by 110×\times×.","As new augmented reality (AR) hardware and software enter consumer markets, mobile AR technologies have positively impacted various industries, including e-commerce, education, and engineering (Chylinski et al., 2020; Rauschnabel et al., 2019). The growing public adoption of AR technologies demands new standards for content quality and application user experiences, particularly emphasizing the need for visual coherency between virtual and physical content to ensure high-quality user experiences. To create visual coherency, AR applications require an accurate and robust environment lighting estimation, which ensures that virtual objects blend naturally with the physical environment. We define the lighting estimation task for AR systems as estimating a complete environment map (a 360∘ HDR image) from partial observation of the environment (an LDR image with limited FoV) in real-time. This task is critical for supporting three key aspects of photorealistic rendering in real-time AR systems. (i) Reflective object rendering, which requires a complete environment map with visually coherent details. (ii) Highlights and shadows rendering, which requires HDR pixel information in environment maps. (iii) Temporally consistent visual coherency, which requires the system to robustly adapt to changing environmental lighting in real-time. To address this challenging task, traditional systems adopt autoregressive models (Zhao and Guo, 2021). These models extract low-frequency information based on camera input (shown in Fig. 1a). While these methods are able to capture coarse information about environmental lighting conditions, high-frequency details are missing. These details are important for creating a visually coherent AR system. To obtain the details, recent works (Somanath and Kurz, 2021; Yang et al., 2023; Wang et al., 2022) leverage the advancement of controllable generative models. These models have the potential to extract fine-grained environment details (shown in Fig. 1a), empowering AR systems with a better rendering effect. While generative-model-based methods seem promising, we identify two key challenges in using them on mobile AR systems. a Autoregressive Lighting Estimation b Generative Lighting Estimation Figure 1. Comparison between autoregressive and generative lighting estimation methods. (1a) An autoregressive lighting estimation system, Xihe (Zhao and Guo, 2021), estimates omnidirectional low-frequency lighting information from camera images with autoregressive models. The low-frequency lighting information misses important visual details, as visualized in the example environment map. (1b) Generative lighting estimation models can create high-frequency environment lighting estimation results from limited environment observations. The estimation process can be conditioned in several ways, such as partial environment observations and text prompts. First, robust lighting estimation demands accurate estimation under challenging lighting conditions. However, through a measurement study on existing datasets, we found that existing lighting estimation datasets have several distribution biases in key lighting properties, including light intensity and color temperatures. In further tests, we found that these data biases will affect the generalization and robustness of multiple recent lighting estimation models (Wang et al., 2022; Akimoto et al., 2022). Therefore, data diversity and fairness must be ensured when training and testing generative lighting estimation systems. Second, interactive mobile AR applications demand timely updates of lighting estimation results. However, generative models usually experience long inference latency. Without system latency optimization, naively integrating generative lighting estimation models into AR applications can easily violate the temporal constituency of immersive user experiences. In this paper, we address the above issues with CleAR, an edge-assisted novel generative lighting estimation system for mobile AR. To tackle AR device limitations in observation capability, we first design a two-step generative pipeline that estimates 360∘ HDR environment maps from partial LDR environment observations. Our key design insight is to separate the generative model training objective into two domains: LDR environment map completion and high-intensity pixel value estimation. This novel learning objective design addresses the practical challenges of the scarcity of high-quality lighting estimation training data, and allows us to leverage pre-trained large model to tackle each generation step effectively. On top of the two-stepped generative pipeline, CleAR uses controlling signals extracted from AR context data to ensure the estimated environment map aligns with the environment of physical AR devices. Specifically, CleAR uses environment semantic maps, which control the environment map visual details, and device ambient light sensor data, which informs the lighting intensity and color temperatures during generation. Specifically, this information is used as image and text inputs for ControlNet models. This design helps the generative pipeline to handle the significant information increases during environment map generation. Generative model inference latency is high, even on the edge servers. Therefore, we design an edge-device collaborative estimation system architecture with on-device refinement components to adjust edge estimation results to real-time lighting conditions. Specifically, we design a multi-output estimation strategy with an on-device adaptive output selection component. The estimation strategy is configured with the optimal number of generation outputs based on our performance measurement results. Additionally, CleAR uses a color appearance matching technique to efficiently and effectively adjust edge estimation results, even for challenging environment lighting conditions. To evaluate the lighting estimation quality of CleAR, we integrate CleAR into a mobile AR application using Unity, Python, and a recent AR data streaming framework ARFlow (Zhao and Guo, 2024). We compare CleAR’s virtual object rendering quality to ones using environment lighting acquired from three representative baselines: unwrapping a mirror ball (physical reference) (Debevec, 2006), ARKit (Apple, 2022) (commercial), and LitAR (Zhao et al., 2022) (academic). Our evaluation shows that CleAR can generate environment maps with better image details to support more visually coherent virtual object rendering. To quantitatively understand CleAR’s performance, we evaluate CleAR with standard testing dataset (Phongthawee et al., 2023) and compare with state-of-the-art lighting estimation models (Phongthawee et al., 2023; Wang et al., 2022; Akimoto et al., 2022; Gardner et al., 2017). Our evaluation shows that CleAR outperforms DiffusionLight (Phongthawee et al., 2023) by up to 56% using the three-sphere evaluation protocol (Wang et al., 2022). Also, notably, CleAR achieves 110X estimation latency reduction compared to DiffusionLight due to our efficient generative pipeline and generation control design. Furthermore, we verify the robustness of CleAR under different lighting conditions by testing CleAR on an augmented Laval dataset with diverse lighting conditions of light intensity and color temperatures. Qualitatively, our user study also confirms the effectiveness and robustness of CleAR, which shows at least 12% higher quality ratings. Related works on mobile AR lighting estimation systems seek to extract environment information from physical light probes (Prakash et al., 2019), user dynamics, and learning-based solutions (Zhao and Guo, 2021; Gardner et al., 2019; Wang et al., 2022; Yang et al., 2023). While physical light probes provide the most comprehensive environment observations, their use in practical applications is typically constrained due to the need for physical light probe presence. While environment lighting can also be extracted from dynamic environment observations, the estimation is usually incomplete as AR devices often do not have comprehensive environment observations. In contrast, leveraging learned models to estimate environment lighting from AR device camera images is a more feasible solution for AR applications. Over the past couple of decades, learning-based methods have evolved from discovering scene lighting cues from image details, such as highlights and shadows (Yu et al., 1999), to regress omnidirectional environment lighting representations (Zhao and Guo, 2020, 2021; Gardner et al., 2019). However, autoregressive models cannot effectively tackle the environment information generation in lighting estimation. For example, Xihe (Zhao and Guo, 2021) provides real-time low-frequency lighting estimation to AR applications but fails to support detailed environment reflections on object rendering. More recently, a new opportunity has arisen as generative models can support highly detailed image content for environment map estimation (Wang et al., 2022; Yang et al., 2023). In this work, we focus on novel AR system integrations with image-generative models to provide high-quality environment lighting estimation. We specifically focus on adapting generative lighting estimation models to achieve robust estimation under challenging environmental lighting conditions. We summarize our main contribution as the following: • We design and implement a novel AR context-guided generative lighting estimation system, CleAR. Our design leverages generative models to tackle the environmental observation limitation of AR devices. Specifically, our system uses a novel two-step generative lighting estimation pipeline to estimate an accurate environment map with visual details that match the AR user’s physical environment. To train the generative models, we craft a large-scale lighting estimation dataset with approximately 30K data items. • We present a measurement study on recent lighting estimation datasets to understand the complexity of environmental lighting conditions and explore the challenges of achieving robust lighting estimation. With the insights from this study, we design and conduct lighting estimation robustness testing experiments to evaluate several lighting estimation systems, including ours. • We develop two real-time estimation refinement techniques to improve the estimation quality of our system. Our first refinement component can automatically select the best environment map from generated results to match the current lighting conditions. Our second refinement component can match the color appearances between generated environment maps with real-time camera view to improve estimation robustness. • We implement CleAR as an end-to-end edge-assisted framework that can be integrated into third-party AR applications. We demonstrate the integration with an example object placement-based AR application. We will provide the dataset, system source code, and demo application links once the paper is accepted for publication to encourage follow-up research. • We conduct comprehensive experiments to evaluate the effectiveness of CleAR. Specifically, we tested CleAR with SoTA lighting estimation systems on standard testing datasets and our robustness testing dataset. We also conduct a quality assessment study with human perception feedback on the lighting estimation results. Overall, the CleAR rating score ranked the top with 12% higher than the second best method. Additionally, the overall standard deviation of the rating values for CleAR is also lower than the second best method (1.67) by 7%."
https://arxiv.org/html/2411.01866v1,Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales,"When interacting with each other, humans adjust their behavior based on perceived trust. However, to achieve similar adaptability, robots must accurately estimate human trust at sufficiently granular timescales during the human-robot collaboration task. A beta reputation is a popular way to formalize a mathematical estimation of human trust. However, it relies on binary performance, which updates trust estimations only after each task concludes. Additionally, manually crafting a reward function is the usual method of building a performance indicator, which is labor-intensive and time-consuming. These limitations prevent efficiently capturing continuous changes in trust at more granular timescales throughout the collaboration task. Therefore, this paper presents a new framework for the estimation of human trust using a beta reputation at fine-grained timescales. To achieve granularity in beta reputation, we utilize continuous reward values to update trust estimations at each timestep of a task. We construct a continuous reward function using maximum entropy optimization to eliminate the need for the laborious specification of a performance indicator. The proposed framework improves trust estimations by increasing accuracy, eliminating the need for manually crafting a reward function, and advancing toward developing more intelligent robots. The source code is publicly available. 111Source code and media materials are available at https://github.com/resuldagdanov/robot-learning-human-trust","Human decisions are often influenced by their perceptions of how trustworthy they are perceived by others [1, 2]. Research in human-robot collaboration (HRC) indicates that when robots act in accordance with a human co-worker’s trust, collaboration effectiveness is enhanced [3, 4, 5]. However, to make trust-aware decisions, robots need to accurately estimate how much their co-worker trusts them [6, 7]. Trust in a robot can change throughout a task, making it essential for the robot to estimate trust in real-time at fine-grained timescales. By continuously estimating trust during the task rather than only at its conclusion, the robot can adapt its behavior immediately, either enhancing or reducing trust to address the pitfalls of overtrust or undertrust [8, 9, 10]. There is growing HRC research interest in computational models to estimate human trust toward robots [11, 12, 13, 14]. These models are based on robot performance, which is the most significant factor influencing human trust [15, 16, 10]. Furthermore, probabilistic models that capture uncertainty and bias in human subjectivity show great promise in this context [17, 18, 19, 20]. Consequently, the proposed framework in this paper entails a probabilistic estimation of human trust based on robot performance, as illustrated in Fig. LABEL:fig:_Teaser. The probabilistic models proposed in [17, 18, 19, 20] fail to capture the continuous changes in human trust as a robot performs a task. This limitation arises because human co-workers assess performance in a binary manner (e.g., success or failure) only after task completion, neglecting performance changes during the task. This results in a static estimation of trust dynamics, often referred to as a “snapshot” view [19]. An intelligent robot needs to adjust its behavior in real-time in response to changes in human trust in order to address the pitfalls of overtrust and undertrust. For instance, trust may shift during a collision avoidance task if the robot navigates too close to obstacles, raising concerns about its reliability and safety. Capturing these trust dynamics in real-time is essential because it could enable the robot to adapt its behavior immediately rather than wait until the task is complete. In this example, the robot could deliberately navigate around the obstacle to prevent further deterioration of trust. The robot could account for performance and trust-related objectives if it estimates trust at fine-grained timescales. Formulating a task-specific performance function that accounts for the objective aspects of robotic tasks is labor-intensive, time-consuming, and requires a deep understanding of the task [21, 22, 23]. This process involves determining appropriate weights for the factors influencing task objectives and aligning them with desired outcomes. This limits the autonomy and adaptability of the robot to various tasks and highlights the need for a framework that enables fine-grained estimation of human trust, facilitating real-time trust-aware robot decision-making with minimal labor-intensive effort. We propose a new framework for the accurate estimation of human trust at granular timescales, as visualized in Fig. LABEL:fig:_GeneralFramework. We construct a continuous reward function using maximum entropy optimization, which enables us to efficiently capture the underlying performance dynamics throughout the task. Section II provides a brief literature review on human trust modeling. Section III presents a mathematical background of the problem. Section IV details an implementation of the proposed framework. Section V outlines the experimental evaluations. Finally, Section VI concludes this paper."
https://arxiv.org/html/2411.01796v2,Constrained Human-AI Cooperation: An InclusiveEmbodied Social Intelligence Challenge,"We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints—e.g., unable to reach high places or confined to a wheelchair—in performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human’s intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). To benchmark this challenge, we create four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learning-based baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at https://github.com/UMass-Foundation-Model/CHAIC.","Humans possess a remarkable ability to observe, infer, and help others, even when others have different mental models and physical constraints in the world from themselves (Warneken and Tomasello, 2006). From a young age, humans are able to watch other people attempt to perform a task, and if other people fail, they can develop plans of action that best assist them. In contrast, AI agents struggle to exhibit such basic social skills and fail to adjust their plans for the specific humans they wish to aid (Valmeekam et al., 2022; Ngo et al., 2022), rendering them poor personalized helpers. For AI agents to best assist human partners in performing tasks in the real world, they must possess two fundamental capabilities: (1) contextual perception, i.e., the ability to follow and observe human behavior and identify the specific goals and constraints faced by each human; and (2) cooperative planning, i.e., the ability to plan actions that are best tailored to helping each human with different goals and constraints. While there have been some embodied benchmarks and environments designed to test general multi-agent intelligence (Puig et al., 2021, 2023b; Gan et al., 2021), such efforts have largely excluded the unique accessibility challenges that real humans may possess in the world and neglect the differences among individuals. Moreover, outdoor scenarios and emergencies are also prevalent in human life, but receive little attention in the embodied intelligence community (Deitke et al., 2022). This paper introduces the first large-scale embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC). In this challenge, an embodied agent with egocentric visual observation must actively perceive and cooperate with a human partner possibly with physical constraints in a near photo- and physically realistic virtual environment to complete common household and outdoor tasks as efficiently as possible. This is motivated by the idea that people who need the most help from autonomous agents are those who are currently not explicitly accounted for in embodied intelligence frameworks. In CHAIC, a helper agent needs to follow and observe the human partner to infer their goals and constraints; then, the agent plans a user-tailored strategy for aiding the human in efficiently performing tasks together; moreover, with the existence of unexpected emergencies, the agent needs to be reactive and adjust its strategy accordingly. To create the challenge with accessibility in mind, we design and implement four new agents with real physical constraints that reflect the rich diversity of human partners in the real world. For example, a human partner confined to a wheelchair struggles to move past obstacles or a human partner struggles with heavy furniture when moving house in an outdoor scene, shown in Figure 1, and eight long-horizon tasks featuring both indoor and outdoor scenes on top of the ThreeDWorld (Gan et al., 2021), explicitly motivating the development of embodied agents that prioritize accessibility efforts when learning and planning and can thrive in rich scenarios. We benchmark several baseline models, including planning- and learning-based agents, especially those powered by foundation models. We also introduce a new method for building agents that combines the behavior modeling capabilities of video models with the reasoning ability of large language models. Our benchmark results suggest that current baselines have difficulty modeling partner behaviors from raw RGB images, and LLM-driven agents are competitive agents in decision-making. We hope this new challenge will advance the study of social intelligence in embodied agents in complex scenarios including diverse human partners with constraints and rich indoor and outdoor scenes. This initiative calls on the community to develop and evaluate embodied agents with a strong emphasis on accessibility and inclusivity. Our contributions include: • We design and implement four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes on top of ThreeDWorld (Gan et al., 2021), simulating rich human constraints and scenarios in the real world. • We introduce a new embodied social intelligence challenge with accessibility explicitly in mind: Constrained Human-AI Cooperation (CHAIC), to test embodied agents’ ability to actively perceive human partners’ intents and constraints from egocentric visual observations and make user-tailored cooperative plans to help constrained human partners in rich scenarios. • We benchmark several baseline models, including those powered by foundation models, especially a new agent with behavior modeling introduced by us, and conduct comprehensive analyses to identify and discuss the persisting challenges related to inter-agent perception and cooperation within complex environments."
https://arxiv.org/html/2411.01704v1,Understanding the decision-making process of choice modellers,"Discrete Choice Modelling (DCM) serves as a robust framework for modelling human choice behaviour across various disciplines. Building a choice model is a semi-structured research process that involves a combination of a priori assumptions, behavioural theories, and statistical methods. This complex set of decisions, coupled with diverse workflows, can lead to substantial variability in model outcomes. To better understand these dynamics, we developed the Serious Choice Modelling Game, which simulates the real-world modelling process and tracks modellers’ decisions in real-time using a stated preference dataset. Participants were asked to develop choice models to estimate Willingness-to-Pay values to inform policymakers about strategies for reducing noise pollution. The game recorded actions across multiple phases, including descriptive analysis, model specification, and outcome interpretation, allowing us to analyse both individual decisions and differences in modelling approaches. In this first application, there were 28 participants involved, with 24 reporting their modelling results to policymakers. While our findings reveal a strong preference for using data visualisation tools in descriptive analysis, it also identifies gaps in missing values handling before model specification. We also found significant variation in the modelling approach, even when modellers were working with the same choice dataset. Despite the availability of more complex models like Mixed Logit and Latent Class, simpler models such as Multinomial Logit were often preferred, suggesting that modellers tend to avoid complexity when time and resources are limited, which may result in the underexploration of important modelling factors like sociodemographic interactions and non-linear transformations. Participants who engaged in more comprehensive data exploration and iterative model comparison tended to achieve better model fit and parsimony, which demonstrate that the methodological choices made throughout the workflow have significant implications, particularly when modelling outcomes are used for policy formulation.","Discrete Choice Theory (DCT) is a theoretical framework used across various scientific disciplines to study human choice behaviour. These fields include but are not limited to, transport, health, and environmental economics (Louviere, 2000; Hess and Daly, 2024; Mariel et al., 2021; Haghani et al., 2021). This theory informs analysts on how to specify Discrete Choice Models (DCMs) for estimating and predicting choices (Ben-Akiva and Lerman, 1985). On the one hand, by calibrating models on empirical choice data, choice modellers can estimate and infer preferences over alternatives and their attributes, which correspond to features or qualities that define them. On the other hand, by using the estimated parameters to simulate choice scenarios, they can predict future behaviour and responses to changes in policy or market conditions. This allows analysts not only to study the decision-making process and the factors that influence individual decisions, but it also allows them to analyse choice behaviour in different contexts to forecast demand and evaluate policies (Ben-Akiva & Bierlaire, 2003). Discrete choice modelling brings together individuals with diverse backgrounds and expertise to understand and forecast choice behaviour through a series of research steps. Whatever the purpose, choice modellers typically engage in workflows that involve a research question formulation, experimental design, data collection, data exploration, descriptive analysis, model specification, outcome interpretation, and reporting (Ben-Akiva and Lerman, 1985; Hensher et al., 2015; Mariel et al., 2021). Throughout each research phase, these professionals balance various formal behavioural theories with statistical methods, experimental applications, their own knowledge and professional judgements to develop models that represent the choices in the data under study (Paz et al., 2019). Although these phases generally follow a chronological order, they are often carried out in a semi-structured manner. This nature allows modellers to decide how and when to incorporate subjective knowledge gathered during this process to ultimately select a model after evaluating a set of specifications (Rodrigues et al., 2020; Van Cranenburgh et al., 2022). This inherent flexibility, combined with different workflows and subjective decision-making, can lead to model specifications that do not truly capture the data generation process. Modellers may interpret data differently, emphasise different model aspects on the functional forms, or even select model families that do not reflect the preferences in observed choices, which in turn leads to considerable variability in modelling results and conclusions. For this reason, choice modelling is often considered an art, requiring decision-making with a high degree of freedom that allows modellers to use their expertise to make decisions within their research. Hitherto studies have focused primarily on the model specification phase (Daly et al., 2012; McFadden, 1974; McFadden and Train, 2000; Walker and Li, 2007). This phase involves a trial-and-error process where choice modellers determine both the model structure and the parameters to be considered in the model (Paz et al., 2019). For instance, analysts must decide on the choice of model family, the inclusion of linear or non-linear transformations to variables, the incorporation of observed heterogeneity, the distributions of the random coefficients and their correlations, among other considerations (Train, 2009; Beeramoole et al., 2023; Mariel et al., 2021). This iterative and time-consuming process continues until modellers have estimated each specification, and obtained goodness-of-fit indicators and validation metrics to assess model fit, parameter consistency, and alignment with existing literature (Parady et al., 2021). As modellers may use different cost functions to balance these goodness-of-fit metrics, this process may end with various specifications that they consider acceptable to address their research question. The current choice modelling landscape reveals a knowledge gap in understanding modellers’ decision-making processes. Despite recent developments have introduced algorithms designed to assist in the model specification (using goodness-of-fit indicators as objective) (Paz et al., 2019; Ortelli et al., 2021; Beeramoole et al., 2023), these algorithms do not take into account relevant aspects of the modelling process and only partially replace some model specification decisions. They do not consider the modeller’s decision-making during the descriptive analysis phase, the trade-offs made during model specification to constrain the search space, nor the model selection at the end of this process. This inherent flexibility of choice modellers’ workflows can lead to diverse results, interpretations, and conclusions even when working with the same choice dataset. Similar concerns have been observed in psychology, where concepts such as ‘researcher degrees of freedom’ (Simmons et al., 2011) and the ‘garden of forking paths’ (Gelman and Loken, 2013) emphasise how flexibility in data collection and multiple potential tests based on the same dataset, along with the pursuit of meaningful parameters, can increase the risk of false positives. Furthermore, crowd-science experiments have demonstrated significant variability in research processes, a lack of consensus in decision-making, and divergent outcomes when different researchers analyse the same data (Botvinik-Nezer et al., 2020; Wicherts et al., 2016; Silberzahn et al., 2018) . While this degree of freedom promotes exploration and efficient methodological progress, it also carries a risk of poor decisions and undesirable outcomes. A better understanding of these processes not only encourages debate about best practices, but also paves the way for improved practices within the modelling community. This study aims to shed light on the choice modelling research process by employing the serious game methodology, empirical data collection, and sequential pattern mining analysis. Specifically, this study introduces the first serious game designed to collect real-time decision-making data from choice modellers in a simulated research environment, covering the entire process from descriptive analysis through modelling to the reporting of results. This game was applied to a group of choice modellers from the International Conference on Travel Behaviour Research and their actions were tracked, and these data can subsequently be used to explore workflows, understand the degrees of freedom available to choice modellers, and analyse how their decisions influence their final reported results. The remainder of the paper is structured as follows. Section 2 discusses related work, focusing on the progression from data analysis to modelling results. It also introduces a conceptual framework, which outlines the modelling phases that guide our serious game design. Section 3 describes the methodology, covering the serious game design, the in-game tools available at each choice modelling phase, and the stated preference choice dataset used. While Section 4 details the serious gaming session, Section 5 presents the results and discussion of the modellers’ workflows. Finally, Section 6 presents the main conclusions."
https://arxiv.org/html/2411.01200v1,GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation,"Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work. Our project page is available at: https://garmentlab.github.io/","The next-generation assistant robots should possess not only the abilities to separately manipulate a wide variety of objects, including rigid, articulated[59], and deformable objects[58], but also the capability to leverage interactions between those physical media, including flow and fluids, in order to assist humans[39]. Among various daily tasks [69, 59, 56], garment manipulation stands out as one of the most challenging, crucial, and extensively discussed tasks in the robotics and computer vision, due to its demanding requirements for understanding dynamic properties of physical instances and interactions between them. For instance, washing clothes entails the interaction between garments and fluids, while dressing up requires collaboration between robots and humans. Figure 1: GarmentLab provides realistic simulation for diverse garments with different physical propoerties, benchmarking various novel garment manipulation tasks in both simulation and the real world. Garment manipulation tasks mainly presents three challenges. Firstly, each individual garment possesses nearly infinite states and exhibits complex kinematic and dynamic properties. Therefore, it is crucial for models to comprehend the various self-deform states of garments, which usually requires large amount of training data [17, 6] (C1). Secondly, garment manipulation involves interactions with various types of objects, including rigid (e.g., clothes hanger) and articulated (e.g., wardrobe) objects, as well as fluids and human body. Consequently, enabling models to understand these interactions across diverse physical media presents great significance (C2). Finally, considering that strategies for manipulating garments are often highly complex, and visual perception of garments is more challenging due to their diverse states and patterns, manipulating garments faces a greater sim2real gap [63, 29] (C3). Training a powerful agent capable of overcoming these challenges necessitates a vast amount of data encompassing robot-object interactions. However, directly collecting data from the real world is impractical. Thus, researchers have long pursued benchmarks for garment manipulation [30, 4, 6, 64]. Current deformable simulations suffer from drawbacks such as missing garment meshes [30]. Additionally, they offer a limited range of tasks, hindering further research endeavors. Therefore, we present GarmentLab (Figure 1), a unified environment and benchmark for garment manipulation. GarmentLab has three novel components to satisfy the demands for diversity and realism: The powerful GarmentLab Engine, which is built upon Omniverse Isaac Sim [71] and supports a variety of physical simulation methods. The simulator not only supports Particle-Based-Dynamic (PBD) [3], Finite-Element-Method (FEM) [11], to simulate garments, fluid and deformable objects but also makes integration with ROS [42] to provide an efficient teleoperation pipeline for data collection. GarmentLab Assets is a large-scale indoor dataset comprising 1) garments models covering 11 categories of daily garments from ClothesNet [70] 2) various kinds of robot end-effector including gripper, suction and dexterous hands. 3) high-quality 3D assets including 20 scenes and 9000+ object models from ShapeNet [7]. Based on realistic simulation and rich assets, we propose GarmentLab Benchmark containing 20 tasks divided into 5 groups to evaluate state-of-the-art vision-based and reinforcement learning based algorithm. To tackle above challenges, our environment has three characteristics:1) Efficient. Garment manipulation involves nearly infinite object state and action spaces, requiring substantial data for models to understand garment structure and deformation. To meet this demand, our highly parallelized GPU-based simulator provides a significant training advantage. Larger batch sizes enhance RL-based algorithms [33], while faster data collection speeds reduce training time for perception-based algorithms (tackling C1). 2) Rich. The richness of our simulator can be categorized into two aspects: the diversity of simulation content offered by GarmentLab Assets and the depth of physical interaction facilitated by GarmentLab Engine. We emphasize multi-physics simulation, encompassing rigid-articulated, deformable-garment, fluid dynamics, and flow, along with their interactions. This focus is vital for training agents capable of comprehending real-world physical properties [48] (tackling C2). You can refer to videos in supplementary material for our simulation effects. 3) Real. As the sim-to-real gap emerges as the main obstacle in developing embodied agents, GarmentLab Engine surpasses Omniverse capabilities by providing mature sim-to-real algorithms, such as Teleoperation [41] utilized in the RL field, and the Visual Sim-Real Alignment Algorithm employed in perception algorithms. We also make integration with ROS [42] and MoveIt [10], which is beneficial for narrowing sim2real gap by introducing real-world robot motions into simulation (tackling C3). Our benchmark experiments highlight the significant challenges current algorithms face, even with seemingly simple tasks like unfolding. These difficulties arise from a lack of understanding of physical interactions and the complexities of high-dimensional states. Vision-based algorithms demonstrate limited generalization, with performance strongly affected by the initial state of objects. RL-based algorithms also encounter difficulties with tasks requiring long-horizon planning. These insights offer valuable guidance for improving methods for garment and deformable object manipulation. In summary, we have made the following contributions in GarmentLab: • We propose GarmentLab Environment, a realistic and rich environment for garment manipulation, featuring diverse simulation methods, assets, object physics and multi-material interactions. • Based on GarmentLab Environment, we propose GarmentLab Benchmark, benchmarking a large variety of garment manipulation tasks, and providing the first real-world garment manipulation benchmark that can be reproduced internationally. • We integrate different sim2real methods into GarmentLab, providing solutions to narrowing the sim2real and further facilitating the real-world applications. • Extensive experiments and detailed analyses of different types of garment manipulation algorithms facilitate and enlight future research on garment manipulation."
https://arxiv.org/html/2411.01057v1,Online Moderation in Competitive Action Games: How Intervention Affects Player Behaviors,"Online competitive action games have flourished as a space for entertainment and social connections, yet they face challenges from a small percentage of players engaging in disruptive behaviors. This study delves into the under-explored realm of understanding the effects of moderation on player behavior within online gaming on an example of a popular title - Call of Duty®: Modern Warfare®II. We employ a quasi-experimental design and causal inference techniques to examine the impact of moderation in a real-world industry-scale moderation system. We further delve into novel aspects around the impact of delayed moderation, as well as the severity of applied punishment. We examine these effects on a set of four disruptive behaviors including cheating, offensive user name, chat, and voice. Our findings uncover the dual impact moderation has on reducing disruptive behavior and discouraging disruptive players from participating. We further uncover differences in the effectiveness of quick and delayed moderation and the varying severity of punishment. Our examination of real-world gaming interactions sets a precedent in understanding the effectiveness of moderation and its impact on player behavior. Our insights offer actionable suggestions for the most promising avenues for improving real-world moderation practices, as well as the heterogeneous impact moderation has on indifferent players.","The proliferation of online social interactions and competitive action games has enriched the virtual landscape, providing opportunities for entertainment, improved well-being, and social connections (Kriz, 2020; Bourgonjon et al., 2016). However, this digital frontier is not without challenges. While the majority of players engage in respectful and enjoyable gameplay, a small percentage have leveraged these platforms to exhibit disruptive behaviors such as cheating, trolling, and offensive speech (Cook et al., 2019). Moderation, or the regulation of user behavior by platforms, has thus emerged as an essential component of online gaming. The process demands tangible resources, including employees, infrastructure, and time, particularly when human review is required to mitigate complex challenges that can’t easily be handled via automation alone, such as domain shift (changes in expression of disruptive behavior or its definition) (Srikanth et al., 2021) and strategic classification (when the players strategically alter their behavior to circumvent automated systems and avoid detection) (Frommel and Mandryk, 2022). Platforms face tangible constraints in moderating user behavior. Though algorithmic tools can lessen the content requiring review, the human element is vital (Levkovitz, 2023). This is due to aspects challenging to automate such as interpretation ambiguity (Beres et al., 2021), the need for broader contextual understanding (Frommel and Mandryk, 2022), and the need for common-sense judgment. Optimal moderation efforts often require collaboration between human judgment and technological tools (Rieder and Skop, 2021; Link et al., 2016). Unfortunately, the ratio of human moderators to the volume of content requiring moderation leads to bottlenecks in the review process (Gorwa et al., 2020). Thus many important scientific challenges exist for moderating distributive player behavior (Kocielnik et al., 2023). Prior work: Despite the importance of moderation, empirical work exploring the causal effects of this practice within the gaming community remains sparse (Wijkstra et al., 2023), with most work focusing more on developing novel data-driven approaches to detecting toxic behaviors (Canossa et al., 2021; Weld et al., 2021), testing theory-informed hypotheses related to the emergence of toxicity (Kwak et al., 2015), or studying the toxicity in various social communication platforms rather than in games themselves (Ghosh, 2021). These studies, however, don’t examine the effects that moderation of toxic behavior has on players in real-world gaming situations. The few prior studies that do examine the impact of moderation relied only on small sample survey-based examinations around the self-reported perceptions of players (Ma et al., 2023; Kou and Gui, 2021; Kordyaka and Kruse, 2021) or moderators (Cullen and Kairam, 2022; Aguerri et al., 2023). Both of these lack the scale to draw conclusions about the effectiveness of different types and properties of moderation at scale in real-world gaming titles. Indeed a recent review of intervention systems for toxicity highlighted that only a few interventions are evaluated with players and in commercial settings (Wijkstra et al., 2023), highlighting the potential for more research with higher external validity. Our study fills this gap by examining real-world large-scale moderation data from one of the more renowned titles in the industry - 2022’s Call of Duty: Modern Warfare II (COD:MWII). Our work: In this paper, we utilize a quasi-experimental design and the latest causal machine-learning methods (causalML)(Kaddour et al., 2022) to analyze the impact of moderation on player behavior. We specifically examine and compare the behavior of players who were moderated as compared to those who were not in terms of the impact on offensive behavior (repeated offenses) and the number of days with matches played (participation rate). We focus on players who were eventually subjected to human moderation (to control for false reports) and control for consistent types of behavior. Taking into account the principle of immediacy, we also examine the effect of delayed consequences by examining the player’s behavioral measures post-moderation in the context of delayed versus quick interventions. Finally, we evaluate the impact the severity of applied moderation actions has on immediate post-moderation player behavior. Findings: Our results reveal a dual impact of moderation on reducing disruptive behavior and on discouraging some disruptive players from participating. We also uncover trade-offs between quick and delayed moderation, as well as varying severity of punishment. Specifically, our analysis of player behavior shows that moderation effectively lowers disruptive behavior by up to 70% but can lead to up to 12% fewer matches played per day by disruptive players. Quick moderation is more effective at reducing disruptive behavior. Cheaters seem to respond differently to moderation, in that moderation results in a larger reduction of participation (days with matches) for these players than for toxic players. These results offer insights into how moderation affects participation rate and disruptive behavior, including the unique response of cheaters. Contributions: This study offers several significant contributions: (1) We present one of the first studies examining large-scale real-world moderation efforts from one of the most popular gaming titles - Call of Duty: Modern Warfare II (COD:MWII). (2) We uncover an important dual impact of moderation in terms of reducing disruptive behavior and discouraging disruptive players from participation, as well as the importance and impact of quick versus delayed moderation. (3) Our findings lead to actionable insight into the moderation practices, such as the need for quick moderation and the need to understand why some disruptive players change their behavior, while others reduce their participation. We provide a discussion of the real-world implications, setting the agenda for comprehensive analysis and future opportunities. (4) Our study highlights effective strategies for mitigating disruptive behaviors in online environments, offering guidance for gaming and online platforms to foster safer, more engaging communities. By examining how different moderation techniques influence player behavior, our findings provide actionable knowledge that extends beyond gaming to inform digital interaction and community management across various platforms."
https://arxiv.org/html/2411.00980v1,Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO,"Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) frequently face challenges with articulation, leading to dysarthria and resulting in atypical speech patterns. In healthcare settings, coomunication breakdowns reduce the quality of care. While building an augmentative and alternative communication (AAC) tool to enable fluid communication we found that state-of-the-art (SOTA) automatic speech recognition (ASR) technology like Whisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack of training data. Our work looks to leverage SOTA ASR followed by domain specific error-correction. English dysarthric ASR performance is often evaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this dataset where phrases overlap between training and test speakers. Our work proposes an algorithm to break this prompt-overlap. After reducing prompt-overlap, results with SOTA ASR models produce extremely high word error rates for speakers with mild and severe dysarthria. Furthermore, to improve ASR, our work looks at the impact of n-gram language models and large-language model (LLM) based multi-modal generative error-correction algorithms like Whispering-LLaMA for a second pass ASR. Our work highlights how much more needs to be done to improve ASR for atypical speakers to enable equitable healthcare access both in-person and in e-health settings.","Healthcare professionals rely on augmentative and alternative communication (AAC) software to support telehealth and in-person appointments for patients with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) [1]. Damage to the nervous system can result in paralysis or weakness of the muscles responsible for speech, leading to dysarthria and atypical speech patterns in individuals with ALS or cerebral palsy. Atypical speakers who are verbal, often prefer to use their own voice to communicate their needs. Modern AAC applications like VoiceItt111www.voiceitt.com or our own AAC application SpeakEase [2] allow for audio input from the speaker with the intention to provide a faithful transcription. Mulfari et al. [3] propose a low-power, on-device, deep-learning based isolated word ASR system to work in an “always-on” mode for dysarthric speakers with reduced mobility. Such a system has promise to enable communication in healthcare and home settings. Figure 1: Dysarthric automatic speech recognition followed by error correction There is little or no data available on the open domain for atypical speakers. On the other hand web-scale speech datasets like Mozilla Common Voice [4] and GigaSpeech [5] allow for state-of-the-art speech recognition for typical speakers. Dysarthric speech recognition is a low-resource out-of-domain problem [6]. To leverage well-developed typical speaker ASR systems, our work looks for a first pass transcription from such a system followed by error-correction (EC) as show in Figure 1. The figure shows disordered input speech with an imperfect noisy transcription after ASR followed by error-correction. ASR systems are trained with audio and correct transcription pairs. EC systems are trained with inputs consisting of multiple hypotheses of transcribed text (referred to as n-best lists), possibly with errors, with outputs mapping to the correct target text. 1 Ref: he slowly takes a short walk in the open air each day ASR: he shlly takes a wall in the week a eh day EC: he slowly takes a short walk in the open air each day 2 Ref: usually minus several buttons ASR: usually min sell fold buttons EC: usually sell fold buttons 3 Ref: you wished to know all about my grandfather ASR: u’ wal awarke youar gread fap EC: you wished to know all about my grandfather Figure 2: Inference samples for error-correction (EC) for speaker M05. Ref shows the reference transcription, ASR shows the transcription output which serves as input to the EC model. Notice how the EC system has memorized transcripts due to prompt overlap in TORGO. To evaluate English ASR for dysarthric speakers, a well-known dataset called the TORGO dataset [7] is widely used. The TORGO dataset for dysarthric speech has data from speakers with either ALS or CP. Other dysarthric ASR databases such as the Nemours corpus222The authors were unable to obtain a recent copy of this database due to a lack of information on the internet [8], UASpeech [9] and the HomeService corpus [10] are either hard to obtain or largely consist of isolated word utterances. The TORGO dataset is one of the few containing both isloated word and a few sentence level utterances. Figure 2 shows inference examples from our initial experiments of error-correction(EC) following ASR. The EC model memorizes the target transcription without doing any error-correction. This issue stems from the dataset design, which features a significant amount of prompt overlap among the speakers. The research community acknowledges that the TORGO dataset has a very high degree of prompt overlap between speakers [11, 12]. This data leakage prevents the dataset from being used to evaluate ASR and EC algorithms for real-world applications like telehealth and e-health. Our work in this paper makes the following contributions: • Develop an algorithm based on mixed-integer linear programming to partition the TORGO dataset with no prompt overlap with the constraint to minimize data loss. This dataset is called no-prompt overlap TORGO or NP-TORGO. • Understand the impact of removing prompt overlap on dysarthric ASR performance using SOTA baseline ASR models. • Understand the impact of out-of-domain language modelling using text data from the training utterances from NP-TORGO, and Librispeech [13] . • Understand the impact of error-correction (EC) without ASR system fine-tuning, with a state-of-the-art cross-modal error-correction system such as Whispering-LLAMA [14]. This paper is organized as follows. Section II puts our current work in the context of prior work. Section III introduces the TORGO dataset, and Section IV introduces our approach to remove prompt overlap. Section V presents our experimental setup, and experimental results are presented in Section VI. Section VII provides a discussion of our work, and Section VIII concludes the paper."
https://arxiv.org/html/2411.00927v1,"ReSpAct: Harmonizing Reasoning, Speaking, and ActingTowards Building Large Language Model-Based Conversational AI Agents","Large language model (LLM)- based agents have been increasingly used to interact with external environments (e.g., games, APIs etc.) and solve tasks. However, current frameworks do not enable these agents to work with users and interact with them to align on the details of their tasks and reach user-defined goals, instead, in ambiguous situations these agents may make decisions based on assumptions. This work introduces ReSpAct (Reason, Speak, and Act), a novel framework that synergistically combines the essential skills for building task-oriented ""conversational"" agents. ReSpAct addresses this need for agents, expanding on the ReAct approach. ReSpAct framework enables agents to interpret user instructions, reason about complex tasks, execute appropriate actions, and engage in dynamic dialogue to seek guidance, clarify ambiguities, understand user preferences, resolve problems, and use the intermediate feedback and responses of users to update their plans. We evaluated ReSpAct with GPT-4 in environments supporting user interaction, such as task-oriented dialogue (MultiWOZ) and interactive decision-making (Alfworld, WebShop), ReSpAct is flexible enough to incorporate dynamic user feedback and addresses prevalent issues like error propagation and agents getting stuck in reasoning loops. This results in more interpretable, human-like task-solving trajectories than baselines relying solely on reasoning traces. In two interactive decision-making benchmarks, AlfWorld and WebShop, ReSpAct outperforms strong reasoning-only method ReAct by an absolute success rate of 6% and 4%, respectively. In the task-oriented dialogue benchmark MultiWOZ, ReSpAct improved Inform and Success scores by 5.5% and 3%, respectively.","Instruction-following is a fundamental capability for intelligent agents operating in real-world environments. Recent works such as Wei et al. (2022); Huang et al. (2022); Yao et al. (2022b); Shinn et al. (2024) have focused primarily on building agents that can follow individual instructions without considering the importance of feedback and interaction. In realistic settings, instruction-following often involves a back-and-forth exchange between the agent and the user to reduce uncertainties, correct mistakes, and handle exceptions. Consider a scenario where a user instructs an embodied agent: ""Go to the kitchen and bring me the pan."" If the agent encounters multiple pans in the kitchen, it may need to seek clarification from the user about which specific pan to bring. Additionally, if the agent accidentally brings a wrong pan, immediate feedback from the user can help the agent correct its mistake and complete the task successfully. Without the ability to engage in a dialogue and incorporate user feedback, the interactive agents would act based on assumptions, wasting time and resources while searching for the wrong objects in the wrong places or performing actions that do not satisfy user needs and preferences. This method of operating without communication is not intuitive for agents. For instance, if a user requests, ""Arrange a trip to Hawaii,"" they likely don’t expect the agent to book hotels, flights, and rental cars for random dates without confirming the details first. Existing reasoning and decision-making approaches for language agents augment the agent’s action space with a language model, allowing the agent to generate free-form thoughts in natural language that help contextualize and reason about the task at hand. By alternating between task-solving actions and language thoughts, these agents can perform multi-step reasoning and compose useful information for solving complex tasks. However, such frameworks do not explicitly incorporate user interaction and feedback into the agent’s reasoning process. In real-world scenarios, engagement with users can provide valuable information, clarification, and guidance that can significantly improve an agent’s task-solving capabilities. In this paper, we propose ReSpAct, a framework for task-oriented conversational agents that allows the agent to actively engage with users through dialogue actions. By introducing a new action space for user interaction, the agent can ask clarifying questions, request feedback, and incorporate user responses into its evolving context. This human-in-the-loop approach enables the agent to leverage user insights, adapt to user preferences, and refine its task-solving strategy based on user input. The ReSpAct framework, as shown in Fig. 1, is a critical step towards moving from agents to ""conversational"" agents, which can proactively solicit information from the user, give feedback or take any follow-up directions. Figure 2 shows an example from AlfWorld setting, contrasting ReAct with ReSpAct interactions. In the second turn, the agent cleverly asks the user the possible location of a cloth, making the task easier for itself. ReSpAct framework aims to enable such an experience with LLMs, expanding on the ReAct framework. This would result in a more controllable dialogue experience instead of letting the agent struggle to handle all cases by itself without any help. Recent work on the developer side, such as LangGraph111https://www.langchain.com/langgraph attempts to remedy this problem by building a manually crafted dialogue flow. In contrast, the ReSpAct approach allows the model to decide on when to ""speak"" based on its reasoning. Our key contributions are as follows: • We introduce the ReSpAct framework enabling interactive agents with LLMs, expanding on the ReAct approach. • We perform experiments showing the value of ""conversations"" in goal completion for task-oriented Conversational AI using multiple datasets. • We perform ablation studies discussing the extent of conversations to optimize task success without annoying the users."
https://arxiv.org/html/2411.00822v1,"EEG-based Multimodal Representation Learningfor Emotion Recognition††thanks:This work was supported by the National Research Foundation of Korea (NRF) grant funded by the MSIT (No. 2022-2-00975, MetaSkin: Developing Next-generation Neurohaptic Interface Technology that enables Communication and Control in Metaverse by Skin Touch) and the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant, funded by the Ministry of Science and ICT (MSIT) (No. RS-2019-II190079, Artificial Intelligence Graduate School Program).","Multimodal learning has been a popular area of research, yet integrating electroencephalogram (EEG) data poses unique challenges due to its inherent variability and limited availability. In this paper, we introduce a novel multimodal framework that accommodates not only conventional modalities such as video, images, and audio, but also incorporates EEG data. Our framework is designed to flexibly handle varying input sizes, while dynamically adjusting attention to account for feature importance across modalities. We evaluate our approach on a recently introduced emotion recognition dataset that combines data from three modalities, making it an ideal testbed for multimodal learning. The experimental results provide a benchmark for the dataset and demonstrate the effectiveness of the proposed framework. This work highlights the potential of integrating EEG into multimodal systems, paving the way for more robust and comprehensive applications in emotion recognition and beyond.","I INTRODUCTION Multimodal representation learning has gained significant attention in the field of artificial intelligence, particularly in tasks that involve complex human behaviors such as emotion recognition [1]. By combining data from multiple modalities, such as video, audio, and physiological signals, multimodal systems can capture diverse and complementary information to improve model performance [2, 3, 4]. However, integrating electroencephalogram (EEG) data into such frameworks introduces unique challenges due to the inherent variability, noise, and limited availability of EEG datasets compared to other modalities. Despite its potential to provide direct insights into brain activity, the effective utilization of EEG data in multimodal settings remains an open research problem. EEG data, widely used in neuroscience and clinical research [5], offer a non-invasive window into the electrical activity of the brain. This modality has the advantage of capturing cognitive and emotional states in real-time, making it particularly valuable for emotion recognition tasks [6]. However, EEG signals are often noisy [7], highly variable across subjects and sessions, and recorded at a higher dimensionality than traditional modalities like video and audio. These characteristics complicate the feature extraction process and hinder the straightforward integration of EEG into multimodal learning frameworks [8]. Existing work on emotion recognition has largely focused on conventional modalities, with many studies opting to exclude EEG due to these challenges [9]. As a result, the full potential of EEG data in improving emotion recognition systems remains underexplored [10, 11]. Figure 1: Overview of the proposed attention-based multimodal emotion recognition framework, extracting EEG, audio, and visual features with specifically tailored transformers and integrating them through self-attention fusion. Recent advances in multimodal learning have explored various fusion strategies to integrate heterogeneous data sources, such as feature concatenation [12], attention mechanisms [13], and joint embedding spaces [14]. These methods have shown promise in combining modalities like video and audio, where data characteristics are more homogeneous. However, the integration of EEG data presents a distinct set of limitations [15]. Current models often struggle to dynamically adjust to the variability in feature importance across different modalities [16], especially when EEG is involved. Additionally, existing multimodal systems may not be flexible enough to handle the varying input sizes and feature distributions that arise when combining EEG with other sensory data [17]. This often results in suboptimal performance or requires manual tuning to accommodate the unique nature of EEG signals [18, 19]. In this paper, we propose a novel multimodal framework for emotion recognition that integrates EEG, video, and audio data. Our approach dynamically adjusts attention weights to prioritize key features from each modality and adapts to varying input sizes. Evaluated on a new multimodal emotion recognition dataset, our model sets a new benchmark, highlighting the potential of incorporating EEG in emotion recognition. By addressing the limitations of existing multimodal frameworks and incorporating EEG data, our work paves the way for more robust and versatile applications in emotion recognition. The proposed framework highlights the value of using diverse data sources to capture complex emotional states, offering a more comprehensive understanding of human emotions than unimodal systems alone."
https://arxiv.org/html/2411.00699v1,Algorithmic Transparency in Forecasting Support Systems,"Most organizations adjust their statistical forecasts (e.g. on sales) manually. Forecasting Support Systems (FSS) enable the related process of automated forecast generation and manual adjustments. As the FSS user interface connects user and statistical algorithm, it is an obvious lever for facilitating beneficial adjustments whilst discouraging harmful adjustments. This paper reviews and organizes the literature on judgemental forecasting, forecast adjustments, and FSS design. I argue that algorithmic transparency may be a key factor towards better, integrative forecasting and test this assertion with three FSS designs that vary in their degrees of transparency based on time series decomposition. I find transparency to reduce the variance and amount of harmful forecast adjustments. Letting users adjust the algorithm’s transparent components themselves, however, leads to widely varied and overall most detrimental adjustments. Responses indicate a risk of overwhelming users with algorithmic transparency without adequate training. Accordingly, self-reported satisfaction is highest with a non-transparent FSS.","Man or machine—who is the better decision-maker? This question, some would argue, was answered almost 70 years ago in favor of machines by Meehl, (1954) in the context of clinical versus statistical disease prediction. Despite recent advances in artificial intelligence—making machines even more flexible yet precise decision-makers—humans often insist on having the last say. Past misadventures with decision automation—for instance, the automated demand forecasting issues at Nike (Worthen,, 2003)—and legitimate warnings about a potential ’intelligence explosion’ in the future (Russell et al.,, 2015) suggest that this hesitation is justifiable. However, relying solely on human decision-making brings with it a long list of documented flaws. Using their respective strengths, the reconciliation and integration of human and algorithmic decision-making appears to be a sensible overarching goal for research and practice. Forecasting is a crucial organizational function, which supports the Sales and Operations Planning Process (S&OP). The generation and adjustment of forecasts is often facilitated and supported by a Forecasting Support System (FSS) (Boylan and Syntetos,, 2010, p.233), a subcategory of Decision Support Systems (DSSs). Its common understanding is that of software that connects to the organization’s database, visualizes data, produces statistical forecasts, and allows for incorporating managerial judgment (Fildes et al.,, 2006, p.352). Most organizations first automatically create quantitative forecasts, e.g., on sales or demand, and subsequently adjust them manually (Fildes et al.,, 2009), often via an FSS user interface. Case studies on forecasting processes suggest that the effects of adjustments on forecasting accuracy are highly variable, often resulting in only a small net improvement at the expense of valuable working hours (Fildes and Goodwin,, 2021). Controlled laboratory studies on judgmental forecasting and adjustments paint an even more pessimistic picture of human extrapolation capabilities. Human forecasters suffer from biases, noise111Biases are systematic deviations from rational choices, while noise refers to random variations in judgment that have received less scientific attention as of yet (Kahneman et al.,, 2021) (Hogarth and Makridakis,, 1981), and frequently unwarranted aversions to algorithmic advice (Dietvorst et al.,, 2015). Mitigating these problems and facilitating more accurate forecasts requires understanding the forecasting process from the manager’s perspective. For that purpose, I propose a simple task-oriented framework by which I will conceptualize the forecasting process and the related literature. Its central actor is the human decision-maker. They generate, adjust, and sign off quantitative forecasts for a given entity. Doing so, they face four primary inputs, which, mediated by human behavior, predict the main output: forecasting accuracy. Namely, these inputs are: 1. The historical time series to be extrapolated 2. Extra information relating to that time series 3. The task environment dictating explicit and implicit goals 4. The FSS facilitating forecast interactions The inputs are not independent of each other. Some inputs dictate the potential scope of others or can be viewed in a hierarchical relation. For example, the scope of available information is highly influenced by the task environment, specifically the organization’s information and knowledge management. In turn, the available information dictates what kind of algorithms are applicable. The type of algorithm (FSS backend) influences how transparent the FSS frontend can be. These considerations show that while the compartmentalized examination of the components at play is valuable and necessary, a complete and relevant understanding requires the subsequent integration considering all their interdependencies. This work’s empirical study focuses on the central role of the FSS’s Guided User Interface (GUI). Its facilitative role in forecast generation and adjustment and interaction with the other inputs make it an obvious lever for mitigating detrimental user inputs while encouraging beneficial ones. As noted by Goodwin, (2015), commercial FSS frequently lack a user-centric and behaviorally informed design. A behaviorally informed FSS design reflects and extends the user’s understanding of the decision object, their “mental model” (O’Donnell and David,, 2000), and thereby promises more informed decision-making. One aspect of such systems is algorithmic transparency, “the extent [to] which the inner workings or logic of the automated systems are known to human operators to assist their understanding about the system” (Seong and Bisantz,, 2008, p.611). Algorithmic transparency is thought to promote user trust and thereby mitigate unnecessary adjustments (Sheridan,, 1988; Lehmann et al.,, 2022). In the domain of time series forecasting, decomposing algorithms are the natural choice for a transparent representation because their components, e.g., seasonal effects, and their integration through addition or multiplication, are familiar, tangible concepts. Therefore, the research presented here asks: To what extent can algorithmic transparency through decomposition facilitate more accurate forecasts by mitigating harmful adjustments whilst promoting beneficial adjustments? To start answering this question, I implement three FSS designs with varying degrees of transparency. Transparency is achieved by dynamically displaying each forecast’s constituent additive components: trend level, weekly cycle, yearly cycle, and events. The transparently adjustable design allows the individual adjustment of those components. I conduct an empirical study on Amazon Mechanical Turk (mTurk) to examine the effects of transparency on adjustment volume, adjustment quality, and user satisfaction utilizing real-world Walmart sales data from the M5 competition (University of Nicosia,, 2020). The following section presents an overview of the relevant literature, informing when, why, and how human judges adjust statistical forecasts. Additionally, findings on pure judgmental forecasting are included due to their close relation to adjustments. Based on the research gaps identified in the literature review, Section 3 derives this work’s hypotheses. Section 4 lays out the algorithmic model and dataset used for the experiment. Section 5 details the experimental procedure and results, focusing on algorithmic transparency, and discusses the findings. Section 6 concludes this work."
https://arxiv.org/html/2411.00419v1,Argus: Multi-View Egocentric Human Mesh Reconstruction Based on Stripped-Down Wearable mmWave Add-on,"In this paper, we propose Argus, a wearable add-on system based on stripped-down (i.e., compact, lightweight, low-power, limited-capability) mmWave radars. It is the first to achieve egocentric human mesh reconstruction in a multi-view manner. Compared with conventional frontal-view mmWave sensing solutions, it addresses several pain points, such as restricted sensing range, occlusion, and the multipath effect caused by surroundings. To overcome the limited capabilities of the stripped-down mmWave radars (with only one transmit antenna and three receive antennas), we tackle three main challenges and propose a holistic solution, including tailored hardware design, sophisticated signal processing, and a deep neural network optimized for high-dimensional complex point clouds. Extensive evaluation shows that Argus achieves performance comparable to traditional solutions based on high-capability mmWave radars, with an average vertex error of 6.5 cmtimes6.5centimeter6.5\text{\,}\mathrm{cm}start_ARG 6.5 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG, solely using stripped-down radars deployed in a multi-view configuration. It presents robustness and practicality across conditions, such as with unseen users and different host devices.","Figure 1. Argus is the first multi-view, egocentric mmWave sensing system enabling continuous HMR, breaking through the limitations of frontal-view solutions. Human pose estimation (HPE), including skeleton tracking and 3D human mesh reconstruction (HMR), remains a perennial research topic due to its broad application value in tasks such as fitness coaching (Wang et al., 2019), health monitoring (Liu et al., 2022), and virtual reality (Zhou et al., 2023). Consequently, it has received significant attention from researchers (Sun et al., 2022). Existing solutions attempted to solve this problem use various modalities, such as visual modalities (Cao et al., 2017; Kocabas et al., 2020; Goel et al., 2023; Kang et al., 2023; Kang and Lee, 2024) (e.g., RGB, depth), wearable modalities (Mollyn et al., 2023; DeVrio et al., 2023; Yu et al., 2024) (e.g., IMU, electromyography), and wireless modalities (Zhao et al., 2019; Jiang et al., 2020; Ren et al., 2021; Xue et al., 2021; Lee et al., 2023; Shibata et al., 2023; Mahmud et al., 2023) (e.g., ultrasound, Wi-Fi, mmWave). However, vision-based solutions are highly dependent on light conditions and struggle to darkness or smoke; while wearable-based solutions suffer from cumbersomeness and lack of user-friendliness. Based on this, many wireless solutions for HMR focus on using wireless signals. Among them, human sensing and reconstruction based on mmWave (Zhang et al., 2023) is a representative research direction because it offers high precision, better penetration compared to Wi-Fi, and better interference resistance compared to low-frequency ultrasound. Based on this fact, it has garnered significant attention and led to many representative studies (Xue et al., 2021; Chen et al., 2022; Xue et al., 2022; Zhang et al., 2022; Xue et al., 2023; Lee et al., 2023; Yang et al., 2024; Zhang et al., 2024) in the field. Radio frequency (RF) signals are renowned for their non-contact, imperceptible, and user-friendly characteristics, making RF-based human sensing a subject of significant interest and leading to numerous practical applications (Shuai et al., 2021; Xie et al., 2022; Xu et al., 2022; Qian et al., 2020; Dai et al., 2023; Han et al., 2024; Bae et al., 2024). Previously, researchers successfully used Wi-Fi for human skeleton tracking (Zhao et al., 2018) and mesh reconstruction (Zhao et al., 2019). However, because of the ubiquity of Wi-Fi signals, they are prone to interference, and the sensing granularity is coarse. As a result, researchers have recently shifted towards using mmWave for human pose estimation and reconstruction, leading to the development of a series of studies. Xue et al. proposed mmMesh (Xue et al., 2021), which is the first human mesh reconstruction solution based on a commodity mmWave radar. Later, several follow-up studies have been proposed to address remaining challenges or to improve the solution from different perspectives. For example, SynMotion (Zhang et al., 2022) and mmGPE (Xue et al., 2023) were later proposed to improve generalization by synthesizing mmWave signals; M4esh (Xue et al., 2022) and m3Track (Kong et al., 2022) were proposed for multi-target tracking and reconstruction; several mmWave-native studies try to improve HPE performance by introducing an additional mmWave radar (Lee et al., 2023; Zhang et al., 2024) or employing advanced deep learning methods (Yang et al., 2024). However, all of the above works focus solely on mmWave-based HPE/HMR from a frontal view perspective, neglecting an intriguing perspective—mmWave-based HMR from an egocentric view. Table 1. Comparison with other solutions (❍–Not Available, ⚫–Available; Cons.: Consumption, TX #: Number of Transmitters, RX #: Number of Receivers). Solutions Radar TX # RX # Board Size Weight Power Cons. Multi-View Sensing View Body Part mmMesh (Xue et al., 2021) AWR1843 3 4 8.3 cmtimes8.3centimeter8.3\text{\,}\mathrm{cm}start_ARG 8.3 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 6.4 cmtimes6.4centimeter6.4\text{\,}\mathrm{cm}start_ARG 6.4 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼245 gtimes245gram245\text{\,}\mathrm{g}start_ARG 245 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼2.08 Wtimes2.08watt2.08\text{\,}\mathrm{W}start_ARG 2.08 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Frontal Full SynMotion (Zhang et al., 2022) IWR1443 3 4 8.3 cmtimes8.3centimeter8.3\text{\,}\mathrm{cm}start_ARG 8.3 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 6.4 cmtimes6.4centimeter6.4\text{\,}\mathrm{cm}start_ARG 6.4 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼245 gtimes245gram245\text{\,}\mathrm{g}start_ARG 245 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼2.1 Wtimes2.1watt2.1\text{\,}\mathrm{W}start_ARG 2.1 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Frontal Full mmGPE (Xue et al., 2023) AWR1843 3 4 8.3 cmtimes8.3centimeter8.3\text{\,}\mathrm{cm}start_ARG 8.3 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 6.4 cmtimes6.4centimeter6.4\text{\,}\mathrm{cm}start_ARG 6.4 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼245 gtimes245gram245\text{\,}\mathrm{g}start_ARG 245 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼2.08 Wtimes2.08watt2.08\text{\,}\mathrm{W}start_ARG 2.08 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Frontal Full M4esh (Xue et al., 2022) AWR1843 3 4 8.3 cmtimes8.3centimeter8.3\text{\,}\mathrm{cm}start_ARG 8.3 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 6.4 cmtimes6.4centimeter6.4\text{\,}\mathrm{cm}start_ARG 6.4 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼245 gtimes245gram245\text{\,}\mathrm{g}start_ARG 245 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼2.08 Wtimes2.08watt2.08\text{\,}\mathrm{W}start_ARG 2.08 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Frontal Full m3Track (Kong et al., 2022) AWR1443 3 4 7.8 cmtimes7.8centimeter7.8\text{\,}\mathrm{cm}start_ARG 7.8 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 6.4 cmtimes6.4centimeter6.4\text{\,}\mathrm{cm}start_ARG 6.4 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼245 gtimes245gram245\text{\,}\mathrm{g}start_ARG 245 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼2.1 Wtimes2.1watt2.1\text{\,}\mathrm{W}start_ARG 2.1 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Frontal Full HUPR (Lee et al., 2023) IWR1843 3 4 7.8 cmtimes7.8centimeter7.8\text{\,}\mathrm{cm}start_ARG 7.8 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 6.4 cmtimes6.4centimeter6.4\text{\,}\mathrm{cm}start_ARG 6.4 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼245 gtimes245gram245\text{\,}\mathrm{g}start_ARG 245 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼2.08 Wtimes2.08watt2.08\text{\,}\mathrm{W}start_ARG 2.08 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Frontal Full SUPER (Zhang et al., 2024) IWR6843 3 4 6.8 cmtimes6.8centimeter6.8\text{\,}\mathrm{cm}start_ARG 6.8 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 5.5 cmtimes5.5centimeter5.5\text{\,}\mathrm{cm}start_ARG 5.5 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼137 gtimes137gram137\text{\,}\mathrm{g}start_ARG 137 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼1.75 Wtimes1.75watt1.75\text{\,}\mathrm{W}start_ARG 1.75 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Frontal Upper mmEgo (Li et al., 2023) IWR6843 3 4 6.8 cmtimes6.8centimeter6.8\text{\,}\mathrm{cm}start_ARG 6.8 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 5.5 cmtimes5.5centimeter5.5\text{\,}\mathrm{cm}start_ARG 5.5 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼137 gtimes137gram137\text{\,}\mathrm{g}start_ARG 137 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ∼similar-to\sim∼1.75 Wtimes1.75watt1.75\text{\,}\mathrm{W}start_ARG 1.75 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ❍ Egocentric Full Ours BGT60TR13C 1 3 3.9 cmtimes3.9centimeter3.9\text{\,}\mathrm{cm}start_ARG 3.9 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ×\times× 2.4 cmtimes2.4centimeter2.4\text{\,}\mathrm{cm}start_ARG 2.4 end_ARG start_ARG times end_ARG start_ARG roman_cm end_ARG ∼similar-to\sim∼7.5 gtimes7.5gram7.5\text{\,}\mathrm{g}start_ARG 7.5 end_ARG start_ARG times end_ARG start_ARG roman_g end_ARG ×\times× 2 ∼similar-to\sim∼0.35 Wtimes0.35watt0.35\text{\,}\mathrm{W}start_ARG 0.35 end_ARG start_ARG times end_ARG start_ARG roman_W end_ARG ×\times× 2 ⚫ Egocentric Full This interesting sensing view has several advantages over the frontal-view solutions. First, frontal-view solutions are subject to various limitations, such as restricted sensing range and susceptibility to interference from others’ movements (i.e., multipath effect) or occlusion. In contrast, using mmWave signals to sense a user from an egocentric perspective is highly promising to circumvent these challenges. Due to its on-body setup, the mmWave sensing field moves with the target, and the controllable sensing range can effectively avoid interference from others. Recently, the discovery of this ingenious research perspective brought about the first mmWave-based HMR solution from an egocentric view called mmEgo (Li et al., 2023). However, using such an egocentric view and mmWave signals to sense the user’s human body typically implies a solution lies at the intersection of wearable and wireless sensing. Therefore, how to elegantly combine the characteristics of both remains a significant challenge. Although the mmWave radar (i.e., IWR6843ISK-ODS (Instruments, 2024)) used in mmEgo offers excellent sensing performance due to its multiple transmit and receive antennas, its size, weight, and high power consumption make it unsuitable for wearable solutions, rendering mmEgo far from practical. Motivated by this, we propose our solution—a pair of compact, lightweight, low-power mmWave-based add-ons named Argus that can be magnetically attached to various common host devices, such as VR headsets and headphones. However, realizing this idea presents numerous unique challenges: (1) How can multiple factors be fully considered in hardware design? All previous studies are based on well-developed Texas Instruments (TI) mmWave radars, such as the IWR6843, IWR1843, IWR1443 series. These radars come with comprehensive development kits, including the mmWave SDK and mmWave Studio, which provide high-precision point cloud data, essential for advanced applications. However, these radars are relatively large in size and energy consumption, making them unsuitable for wearable devices that offer mmWave sensing in egocentric view. The development of Argus adopts compact sensors that help reduce the size of the hardware by significantly compromising the radar’s capability, such as having fewer transmit and receive antennas. In addition, these compact radars often lack the sophisticated development kits needed to achieve high-precision point clouds, posing a major challenge of this paper. (2) How to obtain the ground truth label in a more practical and user-friendly manner? Previous solutions all rely on expensive and high-precision Motion Capture (MoCap) systems (e.g., VICON (Vicon, 2023), OptiTrack (OptiTrack, 2024), Azure Kinect (Microsoft, 2024)) to provide high-quality labels for model training. However, such expensive MoCap systems are not commonly found in personal applications, and their deployment is often constrained by the available space, making mmWave-based HMR applications less widespread and challenging to implement practically. Therefore, acquiring high-quality labels for training using cost-effective commodity devices is a significant challenge in enhancing the practicality of the system. (3) How does Argus overcome the dual challenges of self-occlusion and specular reflection? Li et al. has already mentioned the issue of self-occlusion of the lower body by the upper body in egocentric views (Li et al., 2023). However, they used a tricky approach by deploying the mmWave radar extended far in front of the user’s head. Although this approach alleviates the self-occlusion problem and avoids specular reflections from the shoulders, it makes the system cumbersome and unsuitable for common small devices such as headphones. Therefore, effective integration of form factor design, signal processing pipeline, and deep neural network is essential to effectively address this intractable challenge and provide a better user experience. To address these challenges, we proposed a series of solutions: (1) To holistically consider the multiple factors inherent in hardware design, we propose an innovative solution (Fig. 1) that considers multiple factors by employing a pair of compact mmWave radars (i.e., BGT60TR13C (Technologies, 2023)) as an add-on, magnetically attached to the host device. Argus is the first portable and egocentric mmWave system for HMR that analyzes multi-view mmWave data (i.e., left and right), featuring a small size, lightweight, and low power consumption (details shown in Table 1). Furthermore, we have overcome the limitations imposed by the stripped-down mmWave radar through advanced signal processing techniques and deep learning. (2) To facilitate the widespread application of Argus, we employ monocular-based human mesh estimation using a single RGB-only camera (e.g., web camera, front camera of a smartphone) to acquire training labels, instead of relying on cumbersome and expensive MoCap systems. (3) To overcome the challenges of self-occlusion and specular reflection, we leverage multi-view sensing, namely, using dual egocentric-view mmWave sensing fields from left ear and right ear to collaboratively construct body meshes for the first time. Furthermore, we propose a tailored range-gating and energy-compensation solution for Argus. Moreover, Kolmogorov–Arnold Networks (KAN) (Liu et al., 2024) is introduced to improve learning efficiency due to its superior capability in handling non-linearities, which are essential for modeling complex high-dimensional relationships (e.g., multi-view egocentric HMR). The contributions of this paper can be summarized as follows: • To the best of our knowledge, Argus is the first system realize the multi-view egocentric HMR by proposing a holistic solution, including prototype design, FMCW signal design and processing, and deep learning, etc. • We first achieve multi-view mmWave sensing based on our wearable prototype. Argus is based on the joint analysis of mmWave fields with complementary fields of view, effectively addresses the self-occlusion and shoulder specular reflection issues. • We propose and adopt a series of tailored techniques, such as clutter removal, range gating, energy compensation, and the introduction of KAN to enhance the system’s learning ability for high-dimensional non-linear representations. These techniques make it possible to reconstruct human meshes using compact and limited-capability mmWave radars. • We perform a comprehensive evaluation of Argus, including its performance on unseen users and several micro-benchmarks. The evaluation results show that Argus outperforms two state-of-the-art (SOTA) baselines, demonstrating both robustness and practicality."
https://arxiv.org/html/2411.00381v1,Tappy Plugin for Figma: Predicting Tap Success Rates of User-Interface Elements under Development for Smartphones,"Tapping buttons and hyperlinks on smartphones is a fundamental operation, but users sometimes fail to tap user-interface (UI) elements. Such mistakes degrade usability, and thus it is important for designers to configure UI elements so that users can accurately select them. To support designers in setting a UI element with an intended tap success rate, we developed a plugin for Figma, which is modern software for developing webpages and applications for smartphones, based on our previously launched web-based application, Tappy. This plugin converts the size of a UI element from pixels to mm and then computes the tap success rates based on the Dual Gaussian Distribution Model. We have made this plugin freely available to external users, so readers can install the Tappy plugin for Figma by visiting its installation page (https://www.figma.com/community/plugin/1425006564066437139/tappy) or from their desktop Figma software.","User interface (UI) designers recognize that smartphone users sometimes mistakenly tap outside the intended elements, such as buttons, hyperlinks, link-embedded pictures, etc. Such errors have a significant negative effect on usability (Banovic et al., 2013; Yamanaka, 2018). Textbooks on UI design have thus suggested making UI elements large enough to tap easily (Clark, 2016; Hoober and Berkman, 2011; Johnson, 2014; Neil, 2014). To predict how accurately users tap UI elements on webpages, we have launched a web-based application called Tappy (https://tappy.yahoo.co.jp) (Usuba et al., 2024). This application computes the tap success rate in % for each UI element on a page. Tappy is designed for understanding tap accuracy for existing webpages, but we assume that predicting tap success rates is also useful when designers need to determine UI element sizes for their applications and webpages that are in development. We therefore implemented a plugin for Figma111https://www.figma.com/ to provide the tap success rate prediction feature of Tappy. Figma is widely used, modern software for designing applications and webpages for smartphones, PCs, and tablets. We developed and launched this plugin for internal use within our corporation and publicly released it for external users on October 9th, 2024. By October 30th, 2024, a total of 79 users had installed this plugin. Researchers in the field of human-computer interaction (HCI) have derived numerous models related to UI operations (e.g., variations of Fitts’ law (Accot and Zhai, 2003; Bi et al., 2013), success-rate models (Bi and Zhai, 2016; Usuba et al., 2022)). Although it is typically claimed that such models are useful for designing UIs (Yamanaka and Usuba, 2020; Zhang et al., 2023; Huang et al., 2020), developing and launching a tool that utilizes these models to support professional designers’ work is rare. Because the Tappy plugin for Figma has been used within our corporation, we have evidence that professionals have benefited from this plugin. In this paper, we explain the theoretical background of the Tappy plugin and how it works in Figma."
https://arxiv.org/html/2411.00351v1,Understanding and Co-designing Photo-based Reminiscence with Older Adults,"Reminiscence, the act of revisiting past memories, is crucial for self-reflection and social interaction, significantly enhancing psychological well-being, life satisfaction, and self-identity among older adults. In HCI and CSCW, there is growing interest in leveraging technology to support reminiscence for older adults. However, understanding how older adults actively use technologies for realistic and practical reminiscence in their daily lives remains limited. This paper addresses this gap by providing an in-depth, empirical understanding of technology-mediated, photo-based reminiscence among older adults. Through a two-part study involving 20 older adults, we conducted semi-structured interviews and co-design sessions to explore their use and vision of digital technologies for photo-based reminiscence activities. Based on these insights, we propose design implications to make future reminiscence technologies more accessible and empowering for older adults.","Reminiscence, the act of revisiting past memories, is crucial for self-reflection and social interaction. It significantly enhances psychological well-being, life satisfaction, and self-identity, especially among older adults (Tam et al., 2021; Xu et al., 2023; Westerhof and Bohlmeijer, 2014). Photos, with their strong visual connection to autobiographical memories, serve as powerful triggers for reminiscence (Conway, 2009; Greenberg et al., 2005; Rubin, 2005; El Haj et al., 2020). They also function as conversational anchors, facilitating social activities and storytelling through photo sharing (Sit et al., 2005). Reminiscence and related activities are crucial for older adults, enabling them to reflect on the past and connect with a broader family narrative (Lindley, 2012). In the fields of HCI and CSCW, there is growing interest in using technology to support reminiscence for older adults. Researchers have developed and tested various reminiscence technologies aimed at making these activities more enjoyable and meaningful, while also enhancing cognitive health, engagement, and social participation among older adults (e.g., (Tang et al., 2007; Lee et al., 2014; Piper et al., 2013; Li et al., 2023; Baker et al., 2021)). These technologies often use photos to prompt memories and facilitate storytelling and social interactions, with features such as photo annotation (Lee et al., 2014) and sharing (Baker et al., 2021). While promising, most of these technologies are still in the prototype stage, with evaluations based on short-term testing rather than long-term real-world use. Research on how older adults use technologies for reminiscence in their daily lives is limited, which is essential for designing solutions that meet the real-life needs and scenarios of older adults. As photo interactions increasingly move into digital spaces, the prevalence of digital photo tools suggests that reminiscence with photos is experiencing a digital shift (Keightley and Pickering, 2014). This shift creates opportunities to study how older adults use digital tools like digital albums and social media platforms as reminiscence technologies in daily life. Prior work suggests that this transition poses challenges, as older adults may have different understandings of digital photo operations compared to younger generations (Axtell and Munteanu, 2019a). Despite the widespread availability of digital photo tools, their practical use and inclusivity for older adults remain underexplored, which can lead to inconsistent and unimaginative solutions (Axtell and Munteanu, 2019a). Additionally, emerging platforms, such as short video applications, offer new opportunities for older adults to create content, fostering new forms of sharing and social interaction (Tang et al., 2023). However, the specific challenges and opportunities these tools present for comprehensive reminiscence practices are not yet fully understood. Gaining a broader understanding is crucial for designing effective photo tools and interactions, as well as innovative reminiscence technologies. Addressing these gaps requires a perspective that considers how older adults use photos in their daily lives for reminiscence, beyond just storytelling or social activities in research settings. This paper aims to fill existing gaps by providing an in-depth, empirical understanding of technology-mediated, photo-based reminiscence among older adults. We conducted a two-part user study with 20 older adults in China. In the first part, we used semi-structured interviews to explore how they currently use personal photos for reminiscence in daily life. We mapped their digital photo activities and identified their challenges and unmet needs with digital tools using the PhotoUse model, which outlines modern photo activities (Broekhuijsen et al., 2017b). In the second part, we held co-design sessions to explore their ideal photo-based reminiscence experiences mediated by technologies. Our findings indicate that realistic photo-based activities significantly support reminiscence among older adults. However, current digital tools often fall short in providing adequate support, particularly for tasks such as annotating digital photos. We also identified new opportunities introduced by emerging tools, such as those that facilitate the creation of videos with old photos, fulfilling needs for memory authoring and creative expression. These insights go beyond prototype designs to address real-life needs and scenarios. Additionally, our co-design sessions uncovered various creative ideas from participants, such as co-creating photo artifacts with friends by attaching voice to photos. Based on these findings, our research suggests that reminiscence technologies for older adults should be accessible, sensitive, connected, creative, and culturally aware, offering novel directions and implications for future reminiscence technologies. The main contribution of this work lies in the empirical insights we present on how older adults use and envision digital tools for practical and realistic photo-based reminiscence activities. These insights lead to design implications aimed at enhancing technologies that support reminiscence among older adults through photo-related activities. This work extends prior research on reminiscence technologies and photo-related tools for older adults and is broadly applicable to understanding reminiscence in photo-related activities."
https://arxiv.org/html/2411.00295v1,Fabrication and Performance of Textile Pneumatic Actuators,"Soft pneumatic actuators have a wide range of applications, including providing haptic feedback embedded in smart garments. Here we investigate actuators fabricated from thermoplastic coated textiles. We measure the effects of fabrication parameters on the robustness and airtightness of small, round pneumatic pouch actuators made from heat-sealed thermoplastic polyurethane-coated nylon, which we call PneuDots. We determine the optimal temperature, time, and pressure for heat-pressing of the textile to create strong bonds and identify the most effective glue to create an airtight seal at the inlet. Compared to elastomeric pneumatic actuators, PneuDots reduce the thickness of the actuator by 96.4% and the mass by 57.2%, increasing their wearability while maintaining a strong force output. We evaluated the force output of the actuators, along with their performance over time. In a blocked force test, PneuDot maximum force transmission was 36.1 N, which is 95.3% of the peak force output of an elastomeric pneumatic actuator with the same diameter and pressure. Cyclical testing showed that PneuDots had more stable behavior over time. These results provide best practices for fabrication and indicate the feasibility of textile pneumatic actuators for future wearable applications.","Figure 1: A. Thermoplastic pouch pneumatic actuators in a vest [Nunez2022AActuation]. B. 3D-printed elastomeric pneumatic actuators in a sleeve [duPasquier2023ASleeve]. C. PneuDots textile pneumatic actuators, which are developed and tested in this paper. We show an enlarged view of the 3D-printed inlet. Pneumatic actuators are preferred over other types of soft robotic actuation due to their high strength-to-weight ratio [Jang2023DesignActuation][Niiyama2014PouchRobotics]. Textile and film-based actuators are slim and lightweight pouches which can conform to the body and produce large force outputs [ONeill2017ATesting]. A thin and flexible pouch inflates into a large and stiff volume; this transformation is particularly attractive for wearable devices, and specifically haptic devices. Pneumatic haptic devices have emerged as a promising alternative to vibration-based systems, offering a broader and more nuanced range of haptic feedback [duPasquierHaptiknit:Haptics]. Using pneumatic pouches in contact with the skin, these devices mimic the feeling of human touch [Talhan2024SoftTouch], provide situational awareness [Raitor2017WRAP:Guidance][Rognon2019SoftDrone], and enhance immersion in virtual reality [Delazio2018ForceExperiences]. Pneumatic actuators have been incorporated into bracelets [Pohl2017Squeezeback:Notifications][Jumet2022ADevices], sleeves [Zhu2020PneuSleeve:Sleeve][Liu2021ThermoCaress:Stimulation][Choi2023DevelopmentActuator][Jumet2023FluidicallyTextiles], fingertip bands [VanBeek2024ValidationFeedback][Frediani2020TactileFingertip], leg bands [Endow2021Compressables:Interfaces][Fan2009PilotAmputee], and vests [Nunez2022AActuation][Delazio2018ForceExperiences][Jadhav2023ScalableActuators][Kang2023PneumaticSimulation] to provide a wide variety of cues and sensations, ranging from comforting human touch to collisions in virtual reality games. Many pneumatic actuators for haptic devices are made of thermoplastic films (polyethylene (PE), polyurethane (TPU), PVC, or similar) [Niiyama2014PouchRobotics][Niiyama2015PouchDesign][Sanchez2019DevelopmentInteraction][Yamaoka2018AccordionFab:Sheets][Connolly2019Sew-freeRobots][Do2021Macro-MiniDisplays][Wu2019WearableArm], cast elastomers [Choi2023DevelopmentActuator][Endow2021Compressables:Interfaces][Kang2023PneumaticSimulation], or coated textiles [Jumet2023FluidicallyTextiles][Ou2016AeroMorphDesign]. Thermoplastic film tubes are inexpensive and straightforward to adapt into rectangular pouches [Nunez2022AActuation][Do2021Macro-MiniDisplays]. They can be embedded into wearable devices, such as the vest shown in Fig. 1A, thanks to their thin profile. Cast elastomeric pneumatic actuators are considerably more bulky, but can stretch to inflate further than inextensible materials [Choi2023DevelopmentActuator][Endow2021Compressables:Interfaces]. 3D-printed elastomeric pneumatic actuators can be fabricated with complex custom geometries suited to specific applications or fitting specific regions of the body. However, these thermoplastic and elastomeric actuators can cause discomfort and overheating when placed in direct skin contact for extended periods of time. Lack of breathability causes the user to sweat, which then causes the actuator to stick to the user’s skin [Winterhalter2012EffectsComfort][Zhong2006TextilesOverview.]. Coated textiles provide the same heat-sealing capabilities as thermoplastic films, but with a key advantage: unlike thermoplastic films, which are fully meltable, the textile substrate offers structural support, and only the coated side acts as an adhesive. Additionally, the uncoated side offers a soft fabric surface which is comfortable against the skin. Incorporating durable textiles like ripstop nylon enhances the longevity of the pouches while also enabling the actuators to be sewn directly into garments. Many pneumatic devices require off-board air supplies in the form of heavy pumps or compressed air to achieve high bandwidth [Jumet2023FluidicallyTextiles]. This limits their application in the home environment and as mobile devices. To make lightweight portable wearable devices, actuators should be low-volume and airtight to allow for operation by small, portable pneumatic sources, such as on-board pumps. In [duPasquierHaptiknit:Haptics], we used 3D-printed elastomeric pneumatic actuators, shown in Fig. 1B, that could be operated by a customized version of the untethered FlowIO system by Shtarbanov et al. [Shtarbanov2021FlowIORobotics]. These elastomeric pneumatic actuators were capable of generating up to 37.9 N of output force when the 491 mm2 contact area was inflated to 230 kPa absolute pressure, but are bulkier than film or textile actuators. We present modular textile pneumatic haptic actuators that are lightweight, low-profile, comfortable, and flexible, which we call PneuDots (Fig. 1C). We focus on refining the manufacturing process to ensure strong, airtight seals to produce durable actuators. We measure the performance of the actuators and compare them to the elastomeric 3D-printed actuators proposed by du Pasquier et al. [duPasquierHaptiknit:Haptics]. This paper is structured as follows. First, we present the manufacturing method for PneuDots and examine the results of varying parameters in the manufacturing process. Then, we evaluate the performance of the textile pneumatic actuators and compare it against previous work. Finally, we contextualize the importance of this work in wearable haptic devices."
https://arxiv.org/html/2411.00283v1,GLAT: The Generative AI Literacy Assessment Test,"The rapid integration of generative artificial intelligence (GenAI) technology into education necessitates precise measurement of GenAI literacy to ensure that learners and educators possess the skills to effectively engage with and critically evaluate this transformative technology. Existing instruments often rely on self-reports, which may be biased. In this study, we present the GenAI Literacy Assessment Test (GLAT), a 20-item multiple-choice instrument developed following established procedures in psychological and educational measurement. Structural validity and reliability were confirmed with responses from 355 higher education students using classical test theory and item response theory, resulting in a reliable 2-parameter logistic (2PL) model (Cronbach’s alpha = 0.80; omega total = 0.81) with a robust factor structure (RMSEA = 0.03; CFI = 0.97). Critically, GLAT scores were found to be significant predictors of learners’ performance in GenAI-supported tasks, outperforming self-reported measures such as perceived ChatGPT proficiency and demonstrating incremental validity. These results suggest that GLAT offers a reliable and more objective method for assessing GenAI literacy, with the potential to inform educational practices and policy decisions that aim to enhance learners’ and educators’ GenAI literacy, ultimately equipping them to navigate an AI-enhanced future.","Generative artificial intelligence (GenAI) has rapidly emerged as a transformative force in higher education, challenging traditional pedagogical frameworks while simultaneously presenting novel opportunities for teaching, learning, and assessment. Tools like OpenAI’s ChatGPT, Google’s Gemini, and Anthropic’s Claude hold the potential to transform how personalised tutoring service can be delivered, how instructional materials can be generated, how lectures can be transcribed for accessibility, and how creativity can be nurtured through multimedia content generation (Bahroun et al., 2023; Yan et al., 2024a, b; Khosravi et al., 2023; Lee et al., 2023). However, the integration of these technologies is accompanied by complex challenges, including ethical considerations, risks of misinformation from model ""hallucinations,"" and concerns regarding academic integrity (Ji et al., 2023; McDonald et al., 2024). Such complexities necessitate a deeper focus on fostering AI literacy, particularly GenAI literacy, among both educators and learners to fully harness GenAI’s benefits and mitigate its associated risks (Ng et al., 2021b; Zhao et al., 2024a). AI literacy refers to the set of competencies that enable individuals to effectively interact with AI technologies, encompassing understanding fundamental AI concepts, engaging in critical evaluation, and using AI tools ethically in diverse contexts (Long and Magerko, 2020; Ng et al., 2021b). Within this broad framework, GenAI literacy emerges as a specialised subset, focusing on skills required to engage with GenAI systems that can autonomously produce text, visuals, or other forms of media (Yan et al., 2024a; Annapureddy et al., 2024a). Developing GenAI literacy involves more than just foundational knowledge; it requires proficiency in crafting prompts, interpreting AI-generated outputs, and understanding the socio-ethical implications of using such tools (Zhao et al., 2024a; Bozkurt, 2024a). As GenAI becomes increasingly embedded in educational systems, it is imperative for learners and educators to acquire these competencies to effectively leverage the technology while minimising potential pitfalls such as biases or inaccuracies (Lyu et al., 2024b; Chiu, 2024a). Numerous instruments have been developed to assess AI literacy, reflecting the diversity of competencies that individuals need to navigate AI technologies. Conventional AI literacy assessments often rely on self-reported surveys, which are effective in capturing perceived knowledge but may lack the objectivity needed to accurately measure actual competencies, especially given the tendency for individuals to overestimate their understanding (Lintner, 2024; Laupichler et al., 2023b). Most existing instruments address general AI literacy, focusing on technical knowledge, awareness, and ethical considerations, but fail to adequately capture the unique skills required for GenAI (Koch et al., 2024; Zhao et al., 2024a). There is a growing demand for more nuanced and context-specific instruments to evaluate GenAI literacy, particularly as generative tools become integral to both physical and digital learning environments (Koch et al., 2024; Zhao et al., 2024a; Yan et al., 2024b). Current AI literacy assessments can be broadly categorised into two types: self-reported and performance-based measures. Self-reported instruments, while commonly used, provide insights into individuals’ perceived abilities but may introduce biases that obscure a more objective measure of literacy levels (Ng et al., 2021b; Lintner, 2024). In contrast, performance-based assessments evaluate actual competencies through direct engagement, offering a more reliable measure of skills. This distinction is especially pertinent for GenAI literacy, where there is often a gap between learners’ perceived understanding and their real ability to effectively utilise generative tools (Lyu et al., 2024b). GenAI technologies necessitate iterative, context-specific interactions that require both sophisticated prompting skills and the ability to critically assess AI outputs, areas where self-reports may fall short (Chiu, 2024a; Bozkurt, 2024a). Therefore, developing performance-based instruments is essential to provide an accurate assessment of individuals’ abilities to engage with these advanced technologies in educational settings. However, to the best of our knowledge, there are still limited performance-based tools for measuring students’ GenAI literacy in higher education, particularly those that have been rigorously developed and validated according to established psychological and educational measurement standards (Thorndike et al., 1991; American Educational Research Association et al., 2014). The current study contributes to the field of AI in education and AI literacy by introducing the GenAI Literacy Assessment Test (GLAT), a performance-based instrument specifically designed to evaluate GenAI literacy within higher education contexts. The GLAT aims to fill a critical gap in existing assessment tools by providing a more objective, comprehensive evaluation of the key competencies required to interact with GenAI tools. Unlike existing assessments that focus predominantly on general AI skills, GLAT targets the unique skills necessary for effective engagement with generative technologies, including technical proficiency, ethical awareness, and the capacity for critical evaluation of GenAI-generated outputs. This instrument is grounded in rigorous methodologies from psychological and educational measurement (Thorndike et al., 1991; American Educational Research Association et al., 2014), ensuring both validity and reliability. By focusing on performance-based metrics, GLAT provides educators and researchers with an accurate tool to assess how well students and educators understand and can leverage GenAI technologies, ultimately informing targeted interventions that can enhance these competencies."
https://arxiv.org/html/2411.00731v1,"Nightbeat: Heart Rate Estimation From a Wrist-Worn Accelerometer During Sleep††thanks:1Department of Computer Science, ETH Zurich, Zürich, SwitzerlandEmails: firstname.lastname@inf.ethz.ch (e.g., bjoern.braun@inf.ethz.ch)","Today’s fitness bands and smartwatches typically track heart rates (HR) using optical sensors. Large behavioral studies such as the UK Biobank use activity trackers without such optical sensors and thus lack HR data, which could reveal valuable health trends for the wider population. In this paper, we present the first dataset of wrist-worn accelerometer recordings and electrocardiogram references in uncontrolled at-home settings to investigate the recent promise of IMU-only HR estimation via ballistocardiograms. Our recordings are from 42 patients during the night, totaling 310 hours. We also introduce a frequency-based method to extract HR via curve tracing from IMU recordings while rejecting motion artifacts. Using our dataset, we analyze existing baselines and show that our method achieves a mean absolute error of 0.88 bpm—76% better than previous approaches. Our results validate the potential of IMU-only HR estimation as a key indicator of cardiac activity in existing longitudinal studies to discover novel health insights. Our dataset, Nightbeat-DB, and our source code are available on GitHub: https://github.com/eth-siplab/Nightbeat.","Wrist-worn accelerometers are commonly used in longitudinal studies to analyze patients’ activity patterns, such as exercise routines, physical activity metrics, or sleep and wake times. For example, the UK Biobank combines wrist-based accelerometer recordings from 100,000 patients during a week-long sub-study [1]. This study alone has revealed various activity and sleep patterns as risk factors for depression [2], cardiovascular disease [3], different types of cancer [4], and overall mortality [5]. Besides the accelerometer, no other sensing modality was included. Because the UK Biobank and comparable studies [6, 7] pair week-long accelerometer recordings with decades of electronic health records, using the accelerometer recordings to extract physiological features that are otherwise not included in the study could advance early disease detection and risk factor identification on a population level. Heart rate dynamics have been shown to link to sleep quality and the progression of rare diseases alike, hence, showing great value for identifying novel risk factors on a population level [8, 9, 10, 11, 12]. Figure 1: Our dataset comprises continuous motion signals from a wrist-worn 3-axis accelerometer (Axivity AX3, 100 Hz) and corresponding ECG signals (movisens EcgMove 4) from 42 patients and nights. From the ballistocardiogram (BCG) captured by the vibrations reaching the wrist-based sensor, our signal processing method estimates the patient’s heart rate using a combination of filters and heuristics for pre-processing,a short-term frequency analysis to identify HR curve, and a filter stage to precisely select the peak of the BCG wave for inter-beat interval detection. We combine the HR curve with the detected inter-beat intervals and a 5-minute median smooth to make a prediction for every 20-second window. Recent studies have shown promise for analyzing cardiac activity from the data recorded by body-worn accelerometers, specifically heart rate (HR [13, 14]). These approaches leverage the ballistocardiogram (BCG [15, 16]), which captures the subtle mechanical vibrations caused by heartbeats. Extracting robust HR estimates from BCG recordings in real-world settings is a substantial challenge, particularly when sampled from the wrist. Even simple daytime activities such as walking or eating exhibit larger motion magnitudes than those in the BCG [15, 13], rendering such estimation near impossible in practical settings. Moments of sleep mark a notable exception—they lend themselves to sampling BCG signals, as voluntary movements are minimal [15, 14, 13]. Previous studies have extracted HR values from individual patients’ recordings in controlled sleep laboratories [15, 14, 13], where sensor placement was verified by staff. Here, even individual heartbeats can be detected from wrist-worn accelerometers during sleep [13], as long as motion-afflicted segments are reliably removed (e.g., up to 80% of recordings [14]). Despite this promise, no existing methods reach practical accuracies in their estimates for uncontrolled real-world environments—and no datasets with continuous ground-truth references exist to facilitate and evaluate developments of the former. In this paper, we introduce Nightbeat-DB, a novel dataset for BCG-based HR estimation tasks during sleep. Our dataset replicates the settings of large longitudinal studies such as the UK Biobank [1] to allow for the evaluation of techniques in similarly uncontrolled environments. 42 patients received a wrist-worn activity tracker (Axivity AX3, which embeds a 3-axis accelerometer) for wear on their dominant wrist for one night at home while following their regular sleep routine at home. Patients also received an ECG chest belt (movisens EcgMove 4) to record reference signals for ground-truth values of HR and inter-beat intervals (IBI). Patients also indicated if they shared their beds or slept alone, which is important for BCG analyses, as vibrations couple through bed mattresses and can lead to interfering signals [17]. We complement the dataset with Nightbeat, an estimation method for HR from 3-axis wrist-based accelerometer recordings by removing motion artifacts, tracing HR curves in the frequency domain, detecting individual heartbeats after a filter and frequency detection stage, followed by simple post-processing. We validate our method on Nightbeat-DB and show its robustness to intermittent motion during the night. Compared to 3 baseline methods, our method achieves the lowest error (M⁢A⁢E=0.88𝑀𝐴𝐸0.88MAE=0.88italic_M italic_A italic_E = 0.88 bpm), lowering the error of previous approaches (M⁢A⁢E=3.68𝑀𝐴𝐸3.68MAE=3.68italic_M italic_A italic_E = 3.68) by 76%, while removing the same amount of data (22%) [13]. For female participants, we find that sharing a bed leads to an average increase in MAE of 32%. For male participants, the difference is only 2%. In summary, this paper makes the following contributions: 1. a novel dataset of continuous wrist-based accelerometer signals and paired ECG reference signals during the night for HR estimation tasks, designed to replicate the uncontrolled at-home setting of existing large-scale datasets that do not include continuous HR recordings “in-the-wild” [1, 6, 7], 2. a signal processing method for HR estimation from accelerometer signals that rejects motion artifacts, detects the HR via curve tracing in the frequency domain, and estimates heartbeats and inter-beat intervals, and 3. a comparison of existing approaches for HR estimation from wrist-worn accelerometers using BCG sensations on our dataset in the context of uncontrolled at-home settings where some patients share their beds. Our method achieves the lowest error compared to 3 (recent) baseline methods (M⁢A⁢E=0.88,76%𝑀𝐴𝐸0.88percent76MAE=0.88,76\%italic_M italic_A italic_E = 0.88 , 76 % reduction compared to recent baselines) and an average correlation of 0.810.810.810.81 across participants. Taken together, we demonstrate the suitability of our method for application to existing and future datasets collected in free-living and at-home conditions with activity monitors that do not include alternative sensors for HR estimation (e.g., UK Biobank). Our method achieves accuracy that is suitable for medical analysis [13], making it valuable for future activity studies and monitoring cardiac activity in natural settings."
https://arxiv.org/html/2411.00634v1,Does GenAI Make Usability Testing Obsolete?,"Ensuring usability is crucial for the success of mobile apps. Usability issues can compromise user experience and negatively impact the perceived app quality. This paper presents UX-LLM, a novel tool powered by a Large Vision-Language Model that predicts usability issues in iOS apps. To evaluate the performance of UX-LLM we predicted usability issues in two open-source apps of a medium complexity and asked usability experts to assess the predictions. We also performed traditional usability testing and expert review for both apps and compared the results to those of UX-LLM. UX-LLM demonstrated precision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38, indicating its ability to identify valid usability issues, yet failing to capture the majority of issues. Finally, we conducted a focus group with an app development team of a capstone project developing a transit app for visually impaired persons. The focus group expressed positive perceptions of UX-LLM as it identified unknown usability issues in their app. However, they also raised concerns about its integration into the development workflow, suggesting potential improvements. Our results show that UX-LLM cannot fully replace traditional usability evaluation methods but serves as a valuable supplement particularly for small teams with limited resources, to identify issues in less common user paths, due to its ability to inspect the source code.","With the rapid growth of the app market over the last decade, developing “good” apps has become crucial to vendor success [1, 2]. Studies have shown that users tend to favour apps that perform as expected and are easily understandable [3, 4]. Software usability is a key factor that crucially influences how users perceive the quality of an app [2, 5]. Usability can be broadly defined as “a concept that essentially refers to how easy it is for users to learn a system, how efficient they can be once they have learned it, and how enjoyable it is to use it” [2, 6]. Usability issues are problems that compromise the usability of an app, hindering a positive user experience [7]. It is thus crucial for developers to thoroughly detect and address usability issues for improving their apps [2, 5]. There are different ways to systematically identify usability issues. Conventional usability evaluation methods include usability testing in labs with users as well as theoretical analyses with experts [8, 9]. Once usability issues are identified, developers can address them and offer a refined app to their users [10]. However, it can be challenging, especially for small app development teams, to channel the resources and expertise needed for implementing an effective usability evaluation [11]. Generative Artificial Intelligence (GenAI) is a maturing technology that is increasingly getting attention for the purpose of automating various tasks in different domains, as it is capable of generating meaningful texts, images, and videos [12, 13]. Particularly, Foundational Models such as Large Language Models (LLMs) process a user textual prompt and construct responses by predicting next tokens in a partially formed context [14, 15]. Recently Foundation Models gained considerable interest in the software engineering domain, as they can be employed for a variety of purposes, including generating code and documentation, or fixing bugs [16, 17, 18]. This work investigates the extent to which GenAI can support or even automate usability evaluation for mobile apps. We introduce UX-LLM, a novel open-source tool that uses Foundation Models to detect usability issues in the individual views of native iOS apps. As input, UX-LLM requires a brief description of the app context, the source code, and an image of the analysed view. Section II introduces the implementation details of UX-LLM and the underlying prompt engineering. This constitutes our first contribution. To evaluate UX-LLM performance and how it compares with conventional usability evaluation methods, we conducted a multi-method study consisting of expert assessments, expert reviews, and usability testing for two open-source iOS apps: a Quiz and a To-Do app. To understand how development teams perceive the support of such tools and explore possible concerns, we conduced a focus group within an app development project. Section III presents the design of our evaluation study. The encouraging results confirm that LLM-based approaches are able to identify valid app usability issues. The results also indicate that GenAI approaches do not fully replace traditional methods but rather complement them—highlighting the potential as a supportive tool that can enhance usability evaluations during the development process. Section IV reports on the results of our evaluation study, which constitutes together with the data [19] our second contribution. The remainder of the paper discusses the work limitations and the threats to validity in Section VI, related work in Section V, and summarises the findings with their implication in Section VII."
https://arxiv.org/html/2411.00448v1,ConceptFactory: Facilitate 3D Object Knowledge Annotation with Object Conceptualization,"We present ConceptFactory, a novel scope to facilitate more efficient annotation of 3D object knowledge by recognizing 3D objects through generalized concepts (i.e. object conceptualization), aiming at promoting machine intelligence to learn comprehensive object knowledge from both vision and robotics aspects. This idea originates from the findings in human cognition research that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i) ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization, and ii) ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite. Our approach enables researchers to effortlessly acquire or customize extensive varieties of object knowledge to comprehensively study different object understanding tasks. We validate our idea on a wide range of benchmark tasks from both vision and robotics aspects with state-of-the-art algorithms, demonstrating the high quality and versatility of annotations provided by our approach. Our website is available at https://apeirony.github.io/ConceptFactory.","In the current data-driven era, the availability of a large amount of training data with dense annotations has become an indispensable factor for the successful implementation of deep neural networks in a wide range of 3D object understanding tasks. Particularly, for tasks like segmentation, pose estimation and more sophisticated robot manipulation, current approaches [1, 2, 3, 4, 5, 6, 7, 8] require a substantial volume of annotations of semantic, pose and affordance knowledge to fully demonstrate their power. However, there are two primary issues demanding attention in 3D object knowledge annotation. On one hand, some types of knowledge such as affordance for manipulation are highly complicated to manually annotate [7], resulting in few existing datasets being available for such labels. On the other hand, common practices of acquiring these knowledge annotations [9, 10, 6] follow the conventional paradigm that only a single type of knowledge is labeled on one object at a time, for which researchers develop different annotation platforms to adapt to various knowledge types and let annotators engage in multiple rounds of annotations, taking significant time and human effort. In this paper, We present ConceptFactory as a novel annotation paradigm that addresses these existing issues and facilitates more efficient annotation of 3D object knowledge. The idea behind ConceptFactory originates from the well-known ’Recognition-by-Components’ theory [11] in human cognition research, which finds that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components. Inspired by this theory, we devise an efficient knowledge annotation paradigm performing in two steps. i) Describe the shape of an object with generalized geometric concepts, or in other words, object conceptualization. ii) Procedurally define (different types of) knowledge on these generalized concepts. In this manner, all types of knowledge defined on the concepts can be automatically propagated to the object as various types of annotations, taking advantage of correspondence between the concepts and the object shape. ConceptFactory provides a favorable solution to both aforementioned issues. First, manual knowledge annotation on 3D objects, which can be very complicated in some cases, is no longer required. Instead, researchers only need to procedurally define a type of knowledge with mathematical rules on certain concepts, and these knowledge will be automatically propagated to all target objects consisting of such concepts. Second, intensive human effort is required only once during object conceptualization, compared to the conventional annotation paradigm where significant labor and time resources are repeatedly expended for annotating each type of knowledge. ConceptFactory comes with two critical components. The first one is ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization. The STL-C consists of 263 concept templates that comprehensively covers the essential structure of daily objects, and the conceptualization platform guides users to select and parameterize concept templates in STL-C to describe a given object and thereby obtains the conceptualization result. Then, a wide range of knowledge, which is procedurally defined on the templates, can be automatically propagated to the object as annotations. The other component is ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite, containing 4380 objects from 39 categories involving 39k template instances and 295k parameters. We present such asset considering that the object conceptualization process still requires certain human effort, thereby offering already conceptualized objects to the community would make it convenient for researchers to use and study on, e.g. customizing their own knowledge and conduct experiments with them. The knowledge annotations offered by our approach are mathematically grounded and functionally aligned, serving as a catalyst for machine intelligence to recognize and interact with objects. We demonstrate the effectiveness of our idea from both vision and robotic aspects on a wide range of benchmark tasks including segmentation, pose estimation and robot manipulation through state-of-the-art algorithms, figuring out that our approach can easily gather various types of annotations, with quality comparable or even better than those acquired through conventional annotation paradigms."
