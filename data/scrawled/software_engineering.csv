URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04704v1,Distinguishing LLM-generated from Human-written Code by Contrastive Learning,"Large language models (LLMs), such as ChatGPT released by OpenAI, have attracted significant attention from both industry and academia due to their demonstrated ability to generate high-quality content for various tasks. Despite the impressive capabilities of LLMs, there are growing concerns regarding their potential risks in various fields, such as news, education, and software engineering. Recently, several commercial and open-source LLM-generated content detectors have been proposed, which, however, are primarily designed for detecting natural language content without considering the specific characteristics of program code. This paper aims to fill this gap by proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a contrastive learning framework and a semantic encoder built with UniXcoder. To assess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated code from human-written code, we first curate a large-scale Human and Machine comparison Corpus (HMCorp), which includes 550K pairs of human-written and ChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs). Based on the HMCorp dataset, our qualitative and quantitative analysis of the characteristics of ChatGPT-generated code reveals the challenge and opportunity of distinguishing ChatGPT-generated code from human-written code with their representative features. Our experimental results indicate that CodeGPTSensor can effectively identify ChatGPT-generated code, outperforming all selected baselines.","As large language models (LLMs) continue to advance, the quality of LLM-generated content has significantly improved. As the milestone of LLMs, ChatGPT (OpenAI, 2022), developed by OpenAI, has attracted wide attention and sparked extensive discussions in academia and industry. ChatGPT is fine-tuned from a model in the GPT-3.5 series using Reinforcement Learning from Human Feedback (RLHF)111https://openai.com/blog/chatgpt , which demonstrates the capacity to understand user questions, comprehend conversational context, and generate coherent responses, as evidenced by its evaluations across a wide range of domains, including story-writing (Yao et al., 2019), medical education (Kung et al., 2023) and code generation (Chen et al., 2021). There has been a growing number of research on LLMs for software engineering tasks. In particular, LLMs for automated code generation, exemplified by Codex (Chen et al., 2021), have emerged as promising tools to improve coding productivity and efficiency (Xu et al., 2022). General text generation LLMs like ChatGPT (OpenAI, 2022) have also demonstrated promising performance in generating code. Despite the considerable benefits, surprisingly, a sentiment analysis study shows that fear is the dominant emotion people feel about the code generation capabilities of ChatGPT (Feng et al., [n. d.]). Concerns have been raised about how LLMs for automated code generation impact software engineering, the programming community, and education. For example, Stack Overflow, a famous online community for developers, has banned contributions by ChatGPT as a precaution against ChatGPT-generated unreliable content that may shatter the community’s long-established trust (Overflow, 2023). Several studies in the education domain report that students have already begun to use ChatGPT to complete their class assignments or even cheat on exams (Nietzel, 2023; Susnjak, 2022). Applying LLMs for code generation also raises concerns in the software industry, particularly regarding ethical and code quality problems. For example, Copilot, which is built on Codex, is questioned with code plagiarism222https://twitter.com/mitsuhiko/status/1410886329924194309 and generating vulnerable code (Bannister, 2021). Liu et al. (Liu et al., 2023a) find that ChatGPT-generated code often faces quality issues like code style and maintainability problems, wrong outputs, compilation and runtime errors, and performance inefficiencies. Zhong et al. (Zhong and Wang, 2023) find that LLM-generated code often contains API misuses. Several studies demonstrate that ChatGPT often produces insecure code that exhibits vulnerabilities (Khoury et al., 2023; Liu et al., 2023b). One possible solution to mitigate the above issues is to develop a technique that predicts how likely a given piece of code is generated by LLM so that LLM-generated code can be differentiated from human-written code. This technique can help community administrators block answers with code snippets that are likely to be generated by LLMs. Besides, the estimated probability can also be a reference factor for evaluating students’ scores on programming assignments and exams. Since the use of LLM-generated code in real software projects involves concerns related to provenance, security, and maintainability, it is also important to distinguish LLM-generated code during code review and quality assurance processes. For example, code commits that are likely to contain code generated by LLMs should be prioritized for inspection and given extra attention. However, existing AI-generated content (AIGC) detectors primarily focus on detecting natural language content generated by LLMs, such as prose and news articles (OpenAI, 2019; Mitchell et al., 2023; Hello-SimpleAI, 2023a; GPTZero, 2023; Copyleaks, 2023; Writer, 2023; OpenAI, 2023a). These AIGC detectors have not considered the fundamental differences between code and natural language text and have been reported to be unreliable in detecting AI-generated code in a recent empirical study (Pan et al., 2024). To fill this gap, we aim to explore the technology to address the challenge of distinguishing between LLM-generated and human-written code. Inspired by the recent success of contrastive learning in code-related tasks such as code clone detection (Jain et al., 2020), code retrieval and summarization (Bui et al., 2021), in this paper, we propose a novel ChatGPT-generated code detection method, CodeGPTSensor, based on the contrastive learning framework (Hoffer and Ailon, 2015) with the code semantic encoder UniXcoder (Guo et al., 2022). UniXcoder is a pre-trained model that incorporates semantic and syntax information from the source code to support code-related tasks. To learn distinct code representations for differentiating ChatGPT-generated code from human-written code, we utilize a contrastive learning framework to fine-tune UniXcoder, which ensures that pairs of samples with the same label exhibit smaller distances in the embedding space compared to pairs with different labels. To investigate the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated code from human-written code, we first curate a dataset with 550K pairs of human-written and ChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs), namely the Human and Machine comparison Corpus (HMCorp). The human-written samples in HMCorp are sourced from functions with corresponding docstrings collected from about 17K real-world projects on GitHub. Each human-written function is paired with a function generated by ChatGPT based on the corresponding docstring. With our curated dataset, we first conduct a human study with developers to investigate whether they can manually distinguish between AI-generated code and human-written code. The results reveal that the differentiating task is a big challenge for developers without any assistant tool, where their decisions mainly rely on coding style, code logic, and intuition based on their knowledge. Following that, we make a qualitative and quantitative analysis of the characteristics of ChatGPT-generated code against human-written code. Eventually, we conduct large-scale experiments on HMCorp to assess the performance of CodeGPTSensor in detecting ChatGPT-generated code and compare it against existing AIGC detectors, including two commercial tools, two zero-shot methods, and four training-based methods. Experimental results indicate that CodeGPTSensor outperforms the selected approaches in effectively detecting code generated by ChatGPT. In brief, the main contributions of this paper are summarized as follows: • We curate a large-scale dataset named HMCorp, which contains 288K Python and 222K Java pairs of human-written and ChatGPT-generated code snippets. • We conduct a human study on manually identifying ChatGPT-generated code, followed by quantitative and qualitative analysis to discern the differences between ChatGPT-generated and human-written code, which reveals several characteristics inherent to ChatGPT-generated code. • We propose CodeGPTSensor, a contrastive learning-based approach of the first to detect code generated by LLMs. To facilitate future research in LLM-generated code detection, we have open-sourced our dataset and source code (Xu, 2024)."
https://arxiv.org/html/2411.04605v1,Mint: Cost-Efficient Tracing with All Requests Collection via Commonality and Variability Analysis,"Distributed traces contain valuable information but are often massive in volume, posing a core challenge in tracing framework design: balancing the tradeoff between preserving essential trace information and reducing trace volume. To address this tradeoff, previous approaches typically used a ‘1 or 0’ sampling strategy: retaining sampled traces while completely discarding unsampled ones. However, based on an empirical study on real-world production traces, we discover that the ‘1 or 0’ strategy actually fails to effectively balance this tradeoff.To achieve a more balanced outcome, we shift the strategy from the ‘1 or 0’ paradigm to the ‘commonality + variability’ paradigm. The core of ‘commonality + variability’ paradigm is to first parse traces into common patterns and variable parameters, then aggregate the patterns and filter the parameters. We propose a cost-efficient tracing framework, Mint, which implements the ‘commonality + variability’ paradigm on the agent side to enable all requests capturing. Our experiments show that Mint can capture all traces and retain more trace information while optimizing trace storage (reduced to an average of 2.7%) and network overhead (reduced to an average of 4.2%). Moreover, experiments also demonstrate that Mint is lightweight enough for production use.","As software systems grow larger and more complex (yu2023logreducer, ), distributed tracing has become a critical infrastructure, providing visibility into systems’ end-to-end runtime behavior (sifter, ). Tracing frameworks like Jaeger (jaeger, ), OpenTelemetry (opentelemetry, ), and Zipkin (zipkin, ) have been widely adopted by major internet companies (hindsight, ). The trace data they generate, which visualizes the end-to-end paths of requests through service instances, has proven to be extremely helpful for profiling systems (GMTA, ), detecting anomalies (tracead, ; traceCRL, ; microhecl, ), and diagnosing failures (TraceAnomaly, ; gan2023sleuth, ; Microrank, ; Sage, ). Although distributed traces are helpful, they are often voluminous (hindsight, ), making their collecting, storing, and processing extremely expensive, especially in production environments (sifter, ). For instance, as shown in Fig. 1, a large-scale e-commerce system in Alibaba (alibaba, ) generates approximately 18.6-20.5 pebibytes (PBs) of traces per day. Therefore, reducing tracing overhead and efficiently preserving the valuable information within trace data at an acceptable cost is a crucial and significant task (wset, ). Figure 1. A tracing system in Alibaba generates 18.6-20.5 PBs of traces per day between Feb. 21 and Mar. 20, 2024. In current tracing systems, the de facto practice to handle this task is through trace sampling (i.e., retaining only a portion of traces) (sigelman2010dapper, ; kaldor2017canopy, ; wset, ; sifter, ; samplehst, ; sieve, ; hindsight, ; trastrainer, ). The main workflow is to first determine which traces should be sampled, and then retain the sampled traces while completely discarding the unsampled ones (we call it the ‘1 or 0’ strategy). Depending on the timing of the sampling decision and the sampling rules, these methods are typically categorized as head sampling (sigelman2010dapper, ; kaldor2017canopy, ), tail sampling (sifter, ; wset, ; samplehst, ; sieve, ), and the recently introduced retroactive sampling (hindsight, ). However, our research revealed significant shortcomings of the prevailing trace sampling techniques utilising the ‘1 or 0’ strategy, as evidenced by an empirical trace study (§ 2.2) conducted on real-world systems. (1) The drawback of completely discarding unsampled traces. While current sampling methods try to retain valuable traces through certain rules, our findings indicate that those discarded traces may also be queried by Site reliability engineers (SREs) because the characteristics of the traces needing analysis are often unpredictable. This is evidenced by the observation that the current sampling strategy results in a query miss rate of approximately 27.17% in our study. Trace query failure is a potential source of impediment to the SRE diagnosis process. (2) Lack of effective compression of individual trace volumes. Previous trace reduction methods only reduce the number of traces without lightweighting each individual trace. However, each trace can contain more detailed information than debug-level log (sigelman2010dapper, ), making it necessary to compress traces based on their characteristics. On the other hand, general-purpose compression tools (bzip2, ; gzip, ; lzma, ) and previously proposed log compression techniques (liu2019logzip, ; liu2019logzip, ; CLP, ) are ineffective for trace compression because traces have a topological data structure. These methods fail to fully utilize trace characteristics, resulting in poor compression performance (details shown in § 5.3). To address the above limitations, we shift the strategy of trace overhead reduction from the ‘1 or 0’ paradigm to the ‘commonality + variability’ paradigm which parses trace data into common patterns and variable parameters, and processes them individually. Through our empirical study (§ 2.2.3), we find that the widely existing commonality and variability in traces can be leveraged to preserve more trace information at a lower cost. By leveraging commonality (e.g., constructing common patterns), we can cluster and store the basic information of all traces at a low cost. Moreover, by utilizing variability (e.g., extracting parameters), we can better filter and efficiently record the differing parts. To implement this, we develop Mint, a cost-efficient distributed tracing framework that captures all requests and retains near-full trace information. The workflow of Mint is to first analyze commonality and variability at two levels within traces to parse them into patterns and parameters. Then it mounts metadata of all traces onto their corresponding patterns using a low-cost method (i.e., Bloom Filter (bloom, )), and filters and retains valuable information from the variable parameters. Mint does not directly discard the unsampled traces, under the ‘commonality + variability’ paradigm, the difference between handling sampled and unsampled traces is whether the variability part is sent to the tracing backend. For unsampled traces, only the basic information (i.e., commonality part) of them is preserved at a low cost, which is sufficient for analysis. For sampled traces, their full information (i.e., both of commonality and variability part) is retained, and their volume is reduced by compression. As a practical tool, Mint reduces traces on the agent generation side, thus saving both network bandwidth and storage space. We conducted extensive experiments to verify Mint’s effectiveness and performance. Experiments show that Mint reduces total trace storage overhead to 2.7% and network overhead to 4.2%, while recording all requests. For practical application, Mint has been deployed in the production environment of Alibaba for over two months, successfully reducing trace volume while capturing all requests. In summary, our study makes the following contributions. • We conduct an empirical study on traces in real-world systems and obtain three observations that can facilitate the trace reduction task. • We point out the limitations of current trace reduction methods based on the ‘1 or 0’ paradigm, and introduce the ‘commonality + variability’ paradigm to retain more trace information at a lower cost. • We propose a practical distributed tracing framework named Mint, which applies the ‘commonality + variability’ paradigm on the agent side, enabling cost-efficient retention of all requests. • We conduct extensive experiments to evaluate Mint and demonstrate its effectiveness in reducing trace volume while capturing all requests. We also assess Mint’s efficiency, showing that it is a practical tool."
https://arxiv.org/html/2411.04444v1,An Empirical Study on the Potential of LLMs in Automated Software Refactoring,"Software refactoring is an essential activity for improving the readability, maintainability, and reusability of software projects. To this end, a large number of automated or semi-automated approaches/tools have been proposed to locate poorly designed code, recommend refactoring solutions, and conduct specified refactorings. However, even equipped with such tools, it remains challenging for developers to decide where and what kind of refactorings should be applied. Recent advances in deep learning techniques, especially in large language models (LLMs), make it potentially feasible to automatically refactor source code with LLMs. However, it remains unclear how well LLMs perform compared to human experts in conducting refactorings automatically and accurately. To fill this gap, in this paper, we conduct an empirical study to investigate the potential of LLMs in automated software refactoring, focusing on the identification of refactoring opportunities and the recommendation of refactoring solutions. We first construct a high-quality refactoring dataset comprising 180 real-world refactorings from 20 projects, and conduct the empirical study on the dataset. With the to-be-refactored Java documents as input, ChatGPT and Gemini identified only 28 and 7 respectively out of the 180 refactoring opportunities. However, explaining the expected refactoring subcategories and narrowing the search space in the prompts substantially increased the success rate of ChatGPT from 15.6% to 86.7%. Concerning the recommendation of refactoring solutions, ChatGPT recommended 176 refactoring solutions for the 180 refactorings, and 63.6% of the recommended solutions were comparable to (even better than) those constructed by human experts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of the 137 solutions suggested by Gemini were unsafe in that they either changed the functionality of the source code or introduced syntax errors, which indicate the risk of LLM-based refactoring. To this end, we propose a detect-and-reapply tactic, called RefactoringMirror, to avoid such unsafe refactorings. By reapplying the identified refactorings to the original code using thoroughly tested refactoring engines, we can effectively mitigate the risks associated with LLM-based automated refactoring while still leveraging LLM’s intelligence to obtain valuable refactoring recommendations. Our evaluation results suggest that RefactoringMirror accurately identified and reapplied 94.3% of the refactorings conducted by LLMs, and successfully avoided all of the buggy solutions.","Software refactoring is widely employed to improve software quality, especially its readability, maintainability, and reusability (Fowler, 1999; Mens and Tourwé, 2004; Baqais and Alshayeb, 2020). To facilitate software refactoring, a large number of approaches/tools have been proposed to identify refactoring opportunities (Tourwé and Mens, 2003; Tsantalis and Chatzigeorgiou, 2009, 2011), to recommend refactoring solutions (Mkaouer et al., 2017; Alizadeh and Kessentini, 2018; Alizadeh et al., 2020), and to automatically conduct specified refactorings (Ge et al., 2012; Foster et al., 2012; Alizadeh et al., 2019). Refactoring engines like IntelliJ IDEA (ide, 2024) and JDeodorant (Fokaefs et al., 2007; Tsantalis et al., 2008) have been successfully adopted to automate the execution of refactorings. However, before the refactorings could be executed automatically, developers should explicitly specify which part of the source code should be refactored, what kind of refactorings should be applied, and the detailed parameters of the refactorings (e.g., a new method name for rename method refactoring). Even with the support of the state-of-the-art refactoring approaches/tools, it remains challenging and time-consuming for developers to make such decisions (Feitelson et al., 2022; Peruma et al., 2022), which in turn prevents software refactoring from reaching its maximal potential. Large language models (LLMs), like GPT-4 (Achiam et al., 2023) and Gemini (Team et al., 2023), have the ability to learn complex and massive knowledge (Liu et al., 2023b). Consequently, LLMs emerge as a potential solution that could significantly advance automated software refactoring. It is reported that LLMs have demonstrated promising results in various software engineering tasks, like code generation (Mu et al., 2023), fault location (Wu et al., 2023), and program repair (Xia and Zhang, 2023). LLMs’ impressive capability in understanding and generating natural languages and source code makes it potentially feasible to automatically refactor source code with LLMs. However, it remains unknown whether LLMs can conduct refactorings automatically, whether the refactorings suggested/conducted by LLMs are of high quality, and whether refactorings conducted by LLMs are reliable. To fill this gap, in this paper, we conduct a comprehensive empirical study to obtain a profound insight into the challenges and opportunities of LLM-based refactoring as well as to understand the extent to which LLMs can automate software refactoring to alleviate the burden on developers. We select two representative LLMs, i.e., GPT-4 and Gemini, for the study, and answer the following two research questions: • RQ1: How well do LLMs (GPT-4 and Gemini) work in the identification of refactoring opportunities? • RQ2: How well do GPT-4 and Gemini work in the recommendation of refactoring solutions? To answer the above questions, we first construct a high-quality refactoring dataset comprising 180 real-world refactorings from 20 projects. After that, we request LLMs to refactor the entire Java documents that contain the discovered refactoring opportunities. We conduct a quantitative analysis of LLMs’s effectiveness in identifying refactoring opportunities. We also request three human experts to manually and independently assess the quality of the refactoring solutions suggested by LLMs. Our evaluation results suggest that LLMs have the potential for automated software refactoring. However, their performance varies significantly among different types of refactorings, and thus we refine their performance by a taxonomy of refactorings. We also noticed that LLM-based refactoring could be risky. 22 out of the 313 solutions suggested by GPT-4 and Gemini were unsafe in that they either changed the functionality of the source code or introduced syntax errors. To improve the safety of LLM-based refactoring, we propose a detect-and-reapply tactic (called RefactoringMirror) to avoid unsafe refactorings conducted by LLMs. When a to-be-refactored source code (noted as c𝑐citalic_c) is fed to LLMs, it generates an improved version of the code (noted as c′superscript𝑐′c^{\prime}italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT). RefactoringMirror takes c𝑐citalic_c and c′superscript𝑐′c^{\prime}italic_c start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT as input, compares their differences, and identifies a list of refactorings that have been applied to c𝑐citalic_c. It then reapplies the identified refactorings to c𝑐citalic_c by well-tested refactoring engines (like IntelliJ IDEA), and generates a reliable and improved version of the code (noted as c^^𝑐\hat{c}over^ start_ARG italic_c end_ARG). As a result, it not only keeps the beneficial refactorings but also substantially improves the safety of the refactorings, ensuring that no functional changes are introduced. Our evaluation results suggest that it successfully avoided all of the buggy refactoring solutions conducted by GPT-4 and Gemini. In this paper, we make the following contributions: • A comprehensive empirical study on evaluating LLMs’ potential in automated software refactoring. It reveals the strengths and weaknesses of LLMs in software refactoring, refined by a refactoring taxonomy. • A detect-and-reapply tactic to mitigate the risk of LLM-based software refactoring. • A new high-quality refactoring dataset validated with multiple tools and refactoring experts. The rest of the paper is structured as follows. Section 2 details the study design. Section 3 evaluates the capability of LLMs in identifying refactoring opportunities. Section 4 evaluates the capability of LLMs in recommending refactoring solutions. Section 5 otulines the overall workflow of RefactoringMirror. Section 6 delves into the threats to validity, limitations, and implications. Section 7 provides an overview of related work. Finally, Section 8 concludes the paper and suggests directions for future work."
https://arxiv.org/html/2411.04387v1,Automated Update of Android Deprecated API Usages with Large Language Models,"Android apps rely on application programming interfaces (APIs) to access various functionalities of Android devices. These APIs however are regularly updated to incorporate new features while the old APIs get deprecated. Even though the importance of updating deprecated API usages with the recommended replacement APIs has been widely recognized, it is non-trivial to update the deprecated API usages. Therefore, the usages of deprecated APIs linger in Android apps and cause compatibility issues in practice. This paper introduces GUPPY, an automated approach that utilizes large language models (LLMs) to update Android deprecated API usages. By employing carefully crafted prompts, GUPPY leverages GPT-4, one of the most powerful LLMs, to update deprecated-API usages, ensuring compatibility in both the old and new API levels. Additionally, GUPPY uses GPT-4 to generate tests, identify incorrect updates, and refine the API usage through an iterative process until the tests pass or a specified limit is reached. Our evaluation, conducted on 360 benchmark API usages from 20 deprecated APIs and an additional 156 deprecated API usages from the latest API levels 33 and 34, demonstrates GUPPY’s advantages over the state-of-the-art techniques.","The mobile technology realm has evolved to become an indispensable facet of our daily lives. Mobile devices cater to diverse functionalities, from navigation and social interaction on platforms like social media to educational apps and immersive gaming experiences. Among the vast array of mobile operating systems, Android distinctly dominates, holding a market share of over 70%[4]. Android’s extensive adoption has led to the Google Play Store hosting over 3.5 million Android apps as of 2022[8]. These apps are developed using the Android application programming interface (API). Along with the Android operating system, these APIs get updated to let the developers use new and updated features in their apps. With each iteration of the Android API levels, new API methods and fields are introduced while existing ones get changed or removed. As of now, the latest version of the Android API level is 34. However, this API evolution brings challenges. In particular, Android API deprecation, where APIs are marked as deprecated and are often replaced with more efficient replacement APIs — introduces layers of complexities for developers. The usage of deprecated APIs can linger in apps, potentially leading to compatibility issues. The Android framework adheres to a ""deprecate-replace-remove"" cycle [40] that ensures that developers are equipped with the necessary resources and time to update these deprecated API usages to their recommended alternatives or replacement APIs. A grace period is typically extended before these deprecated APIs are eventually removed from the framework. This structured approach underscores the importance of updating the deprecated API usages, enabling developers to align with the API updates to enhance the quality of their apps. Recent studies have identified the challenges associated with API deprecation and proposed automatic update of deprecated API usages in the app code [23, 25, 26, 56, 55]. For example, Haryono et al. [26] proposed an approach to updating deprecated API usages, but the approach requires proper update examples and cannot update all types of API usages. Zhao. et al. [63] proposed a template-based method to automatically detect and fix compatibility issues in Android apps, but the method requires manual effort to generate those templates. In this paper, we propose GUPPY, an approach to updating deprecated API usages with large language models (LLMs) for Android apps. In particular, GUPPY leverages GPT-4, one of the most powerful LLMs, to update the deprecated API usages and make them compatible with both old and new Android API levels. Furthermore, GUPPY also leverages GPT-4 to generate Robolectric [2] tests to quickly refute incorrectly updated API usages. The generated tests are executed on both the old and new API levels, and if the tests fail on either API level, GUPPY uses the failure information to refine the API usages or generate new tests. This iterative process continues until the tests pass or a user-specified bound is reached. We evaluate the performance of GUPPY by comparing it against the state-of-the-art technique AndroEvolve in updating two sets of deprecated API usages: 360 benchmark API usages of 20 deprecated APIs from AndroEvolve’s study and a new set of 156 deprecated API usages from 156 deprecated APIs we collected from the two latest API levels 33 and 34. The experimental results show that GUPPY correctly updated 331 out of 360 benchmark deprecated API usages and 139 out of 156 more recent deprecated API usages, while AndroEvolve correctly updated only 301 and 83 deprecated API usages respectively. This paper makes the following contributions: • Approach. GUPPY, an approach to updating deprecated API usages for Android apps. To the best of our knowledge, GUPPY is the first approach that leverages LLMs for updating deprecated API usages. • Evaluation. An experimental evaluation of GUPPY, which shows that GUPPY outperforms the state-of-the-art technique AndroEvolve in updating benchmark deprecated API usages and more recent deprecated API usages. • Data. A public release of the new set with 156 deprecated APIs from the two latest Android API levels 33 and 34, source code [12] of this work to facilitate the replication of our study and its application in more extensive contexts."
https://arxiv.org/html/2411.04299v1,An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We?,"Artificial Intelligence (AI) techniques, especially Large Language Models (LLMs), have started gaining popularity among researchers and software developers for generating source code. However, LLMs have been shown to generate code with quality issues and also incurred copyright/licensing infringements. Therefore, detecting whether a piece of source code is written by humans or AI has become necessary. This study first presents an empirical analysis to investigate the effectiveness of the existing AI detection tools in detecting AI-generated code. The results show that they all perform poorly and lack sufficient generalizability to be practically deployed. Then, to improve the performance of AI-generated code detection, we propose a range of approaches, including fine-tuning the LLMs and machine learning-based classification with static code metrics or code embedding generated from Abstract Syntax Tree (AST). Our best model outperforms state-of-the-art AI-generated code detector (GPTSniffer) and achieves an F1 score of 82.55. We also conduct an ablation study on our best-performing model to investigate the impact of different source code features on its performance.","Artificial Intelligence (AI), such as machine learning techniques, has been widely used to tackle software development tasks, especially for the generation of source code [77, 59, 31, 78]. More recently, Large Language Models (LLMs) that were pre-trained on large and diverse data corpora have shown state-of-the-art performance in code generation [25, 54, 60, 73, 56, 86]. The generative LLMs such as ChatGPT [1], Gemini Pro [10] and Starcoder2 [58] are able to generate code that is very similar to what a human developer would produce given the natural language specification. While a large amount of previous research works have explored a variety of fine-tuning/prompting techniques [71, 57] to further boost model’s performance in generating high-quality source code, numerous LLM-based tools (i.e., Github Copilot [9]) have been implemented to assist developers to design software architecture, generate production code/test cases, and refactor the existing code base. Therefore, leveraging LLMs for source code generation or assisting programming-related tasks is becoming popular among software practitioners. However, the wide adoption of LLMs for code generation has raised a variety of concerns among researchers and software practitioners. Researchers have questioned the evaluation process of the source code quality generated by LLMs [55], and the correctnesss of the generated code could be easily impacted by the wording in the LLMs’ prompts [61]. In addition, it’s been proven that around 35% of Github Copilot-generated code snippets on Github have security issues of various types [38, 20], indicating the security risks of the code generated by LLMs. Moreover, the violations of intellectual property rights have also been found in LLMs such as generation of licensed code [87]. Therefore, it has become necessary to determine whether a code snippet is written by humans or generated by the LLMs. While there exist a variety of automated tools (i.e., GPTZero [6], Sapling [7], and more) to detect Artificial Intelligence Generated Content (AIGC), such tools were built for detecting natural language texts and their performance in detecting AI-generated source code still remains far from being perfect [64, 66]. To fill this gap, Nguyen et al. [64] proposed GPTSniffer by fine-tuning CodeBERT [36] to classify a code snippet as either human-written or LLM-generated. However, they only considered the code that is written in Java programming language and is generated only by ChatGPT. It has been proven that different LLMs are good at generating code for different sets of coding problems [86]; namely, they tend to perform differently given the same set of coding tasks. Thus, GPTSniffer’s generalizability to code that’s written in other programming languages or generated by LLMs other than ChatGPT is not investigated. A better detection approach for detecting AI-generated source code is still missing. In this paper, we first conduct a comprehensive empirical study to evaluate the performance of existing AIGC detectors for detecting AI-generated source code. The goal was twofold: first, as a complement to prior studies [64, 66], we investigate the widely-adopted AIGC detectors’ ability to detect AI-generated source code that is written in various programming languages, generated by multiple popular generative LLMs, and from different domains (i.e., programming questions, open-source development tasks). Second, we analyzed the performance of the current state-of-the-art detector specifically for source code, GPTSniffer. Therefore, we asked the following research questions: RQ1: How do existing AIGC detectors perform on detecting AI-generated source code? RQ2: How can we improve the performance of AI-generated source code detection? RQ3: How do the source code features captured by embeddings contribute to the overall effectiveness? The significance of our contributions are following: • We show that existing AIGC detectors for text perform poorly in detecting AI-generated source code. • We show that the current state-of-the-art technique for AI-generated code detection, GPTSniffer, fails to generalize effectively across different programming languages, programming tasks, and generative LLMs. • We built a variety of machine learning and LLM-based classifiers to detect AI-generated code, which outperform the compared techniques and show decent performance across multiple programming languages, programming tasks, and generative LLMs. The remainder of this paper is organized as follows: in Section II, we provide related works of AIGC detection and background about LLM-based code generation and pre-trained source code embeddings. We outlined our data collection, model building, and performance analysis in Section III. Next, we present the evaluation results and observations in Section IV. Then, we discuss the implication for our study in Section V. Section VI shows potential threats to the validity of our approaches and findings. Finally, we conclude with a summary of the findings in Section VII."
https://arxiv.org/html/2411.04156v1,Crystal: Illuminating LLM Abilities on Language and Code,"Large Language Models (LLMs) specializing in code generation (which are also often referred to as code LLMs), e.g., StarCoder and Code Llama, play increasingly critical roles in various software development scenarios. It is also crucial for code LLMs to possess both code generation and natural language abilities for many specific applications, such as code snippet retrieval using natural language or code explanations. The intricate interaction between acquiring language and coding skills complicates the development of strong code LLMs. Furthermore, there is a lack of thorough prior studies on the LLM pretraining strategy that mixes code and natural language. In this work, we propose a pretraining strategy to enhance the integration of natural language and coding capabilities within a single LLM. Specifically, it includes two phases of training with appropriately adjusted code/language ratios. The resulting model, Crystal, demonstrates remarkable capabilities in both domains. Specifically, it has natural language and coding performance comparable to that of Llama 2 and Code Llama, respectively. Crystal exhibits better data efficiency, using 1.4 trillion tokens compared to the more than 2 trillion tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by analyzing the training process and observe consistent improvements in most benchmarks. We also adopted a typical application adaptation phase with a code-centric data mixture, only to find that it did not lead to enhanced performance or training efficiency, underlining the importance of a carefully designed data recipe. To foster research within the community, we commit to open-sourcing every detail of the pretraining111Webpage: https://www.llm360.ai/#crystal, including our training datasets222Datasets: https://huggingface.co/datasets/LLM360/CrystalCoderDatasets, code333Code: https://github.com/LLM360/crystalcoder-train, loggings444Wandb: https://wandb.ai/llm360/CrystalCoder and 136 checkpoints555Model weights: https://huggingface.co/LLM360/CrystalCoder and https://huggingface.co/LLM360/CrystalChat throughout the training.","Figure 1: The multi-phase training process for Crystal. Figure 2: Crystal shows a good balance of language and coding abilities. The y𝑦yitalic_y-axis is the average over ARC-C, HellaSwag, MMLU, and GSM8K. The x𝑥xitalic_x-axis is the average of MBPP and HumanEval. Large Language Models (LLMs) for code generation (i.e., code LLMs), such as Codex (Chen et al., 2021a), StarCoder (Li et al., 2023a), and Code Llama (Roziere et al., 2023), are advancing rapidly due to their strong capability in generating code-related content (e.g., functions), which helps improve the efficiency of software engineers and developers (Cognition Labs, 2024; Chen et al., 2021b; Li et al., 2023a; Roziere et al., 2023). These LLMs excel at generating functions and designing web page components based on engineers’ instructions (e.g., “Return True if all numbers in the list L𝐿Litalic_L are below threshold T𝑇Titalic_T.”) (Calò & De Russis, 2023). However, the abilities of code-oriented LLMs are constrained in development contexts that necessitate interpreting high-level human instructions (e.g., through prompts or function descriptions) and producing comprehensive, structured code accompanied by natural language documentation. Examples of such scenarios include solving GitHub issues (Jimenez et al., 2023), searching for code snippets based on natural language queries, generating entire Python libraries (which include their complete code along with documentation and tutorials (Liu et al., 2023a; Luo et al., 2024)), or developing source code for websites, e.g., “Create a ticketing platform for travelers” (Calò & De Russis, 2023). This underscores the ambition to create LLMs proficient in both natural language processing and coding. Achieving this goal, however, is non-trivial. For instance, Code Llama, despite being continuously pretrained with code datasets on top of Llama2, suffers from catastrophic forgetting of natural language capabilities. In open-sourced LLMs, we observe a prevalent issue: most models are tailored to specialize in either language or code, not both. For example, StarCoder is exclusively trained on code datasets accompanied by function documentation, thus limiting its exposure to varied natural language data. This trend indicates a notable gap in the design of most open-source LLMs, where there’s a lack of a comprehensive curriculum that addresses both coding and natural language processing. Therefore, we are intrigued by the following research question: “Can an LLM efficiently obtain both language and coding abilities?” Existing studies have shown that the simultaneous acquisition of coding and language capabilities by LLMs is governed by complex dynamics: these skills may either conflict (Li et al., 2023a; Roziere et al., 2023) or complement (Ma et al., 2024) each other, influenced by the data recipe and the model’s learning phase. In this work, we propose a pretraining strategy designed specifically for code LLMs. Our strategy is inspired by techniques such as multi-phase pretraining, curriculum learning (Bengio et al., 2009), continuous pretraining (Roziere et al., 2023), and multi-language training, and has two phases. We start the pretraining process with a data mixture of 95% natural language and 5% code. In the second phase, the data mixture is enriched to include 63% code data alongside 37% natural language. This two-phase design mimics the human learning process, where the acquisition of general language knowledge precedes the development of coding skills, aiming to replicate this learning sequence. Pretraining using our strategy yields Crystal, a code LLM that exhibits strong ability across both natural language (e.g., common sense reasoning) and code generation. Our strategy also demonstrates good data efficiency. That is, Crystal, pretrained with 1.4 trillion tokens, performs comparably to Llama 2 and Code Llama, each pretrained with more than 2 trillion tokens. Throughout the pretraining process, we continuously tracked the model’s performance on downstream benchmarks, observing steady enhancements in both language and coding abilities across the two training phases. Despite a slight performance decline due to the distribution shift between Phase 1 and Phase 2, performance in Phase 2 swiftly recovers and surpasses that of Phase 1. Additionally, implementing an experimental application adaptation phase, aimed at further enhancing coding abilities by incorporating an increased percentage of code data, could potentially boost performance. This phase is inspired by the Python-specialized pretraining phase of Code Llama and StarCoder (Roziere et al., 2023; Li et al., 2023a). Contrary to expectations, we observe mixed results from this phase, including a decline in language ability but marginal improvement in coding performance (see § 5.2), underscoring the necessity for a carefully crafted data strategy. Conducting thorough ablation studies for the entire pretraining process is computationally daunting. To mitigate these challenges, we embrace the principles of the LLM360 initiative (Liu et al., 2023b), ensuring full transparency in our pretraining process to support further scientific exploration and discoveries by the community. We release our training and fine-tuning datasets, source code for training, fine-tuning, and data preprocessing, and 152 intermediate model checkpoints. We also release a chat version, fine-tuned from Crystal, namely CrystalChat, for user convenience."
https://arxiv.org/html/2411.04372v1,Benchmarking Large Language Models with Integer Sequence Generation Tasks,"This paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the Online Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the o1 series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.","Benchmarking plays a crucial role in the development and evaluation of large language models (LLMs), helping researchers and users gauge their abilities across various domains such as natural language understanding, knowledge retrieval, and mathematical reasoning. The progress that LLMs have made on challenging benchmarks is remarkable—matching even the performance of humans with a Ph.D. on advanced problems in the human’s domain of expertise. With the release of more powerful models like OpenAI’s o1 series, there is a need for benchmarks that can rigorously test more advanced abilities of these systems. In this paper, we introduce a novel benchmark based on integer sequence generation tasks sourced from the Online Encyclopedia of Integer Sequences (OEIS) [1, 2]. OEIS provides a wealth of information on these sequences under a CC BY-SA 4.0 license, including sequence values that can be used for unit tests as well as sequence definitions and commentary that provide the LLM with information about the sequence. The difficulty of these tasks ranges from trivial (A000004 is the sequence of all zeros) to extremely difficult and interesting (A000001 is the number of groups of order n – “a fascinating function” for which Conway et al. [3] recently provided only an approximation of the series). These tasks are particularly challenging for LLMs, as they require the models not only to understand the sequences but also to implement efficient algorithms that can run quickly (both to test the algorithms and for the expediency of the benchmark). This makes integer sequence generation an excellent testbed, especially for models like o1-mini and o1-preview, which are optimized for tasks that demand reasoning skills such as mathematics and coding. Our benchmark consists of a diverse set of 500 integer sequences labeled “easy” and “hard” in OEIS. We evaluate o1-preview [4], o1-mini [5], GPT-4o [6], GPT-4o-mini, and Claude 3.5 Sonnet [7], Llama 3.1 405b and 70b [8], and Gemini 1.5 [9] Pro and Flash on their ability to write code that generates these sequences. The codes are subject to a time limit that is allowed to vary (similar to a pass@k metric, where different values of k are used), analyzing their performance in terms of both accuracy and efficiency. While o1 models, with their reasoning capabilities, outperform the other models, it still struggles, especially with the hard sequences. Additionally, we introduce mechanisms for detecting and preventing the use of lookup tables to verify that models write legitimate code rather than relying on a memorized version of the sequence entries. See Figure 1 for an overview of our approach. Our contributions are as follows: (1) We introduce a new benchmark for LLMs based on integer sequence generation, emphasizing mathematical and computational reasoning and efficiency. (2) We evaluate numerous frontier LLMs, demonstrating their strengths and limitations in handling these algorithmic tasks. (3) We provide a framework for detecting and mitigating the use of lookup tables in sequence generation tasks, bolstering the integrity of the evaluation process. Figure 1: The process for curating easy and hard sequences, using an LLM to generate code that computes the sequence values, and evaluating the code is shown."
https://arxiv.org/html/2311.15189v2,Using Rely/Guarantee to Pinpoint Assumptions underlying Security Protocols,"This paper explores the application of a state-based specification notation (VDM) and associated rely-guarantee ideas to security. The verification of security protocols is essential in order to ensure the absence of potential attacks. However, verification results are only valid with respect to the assumptions under which the verification was performed. These assumptions are often difficult to identify and hidden in documentation, making it unclear whether a given protocol is safe to deploy in a particular environment. Rely-guarantee ideas offer a way to reason abstractly about interference from an environment: using this approach, assumptions can be recorded and made precise. This paper investigates this approach on the Needham-Schroeder Public Key protocol and Lowe’s extension, showing that the technique can effectively uncover –and provide a way to record– the assumptions under which the protocol can withstand attacks from intruders.","This paper is an experiment in specification and reasoning styles for security protocols; these are certainly not solved problems. Most protocol descriptions actually start with a sketch of the intended communications between “principals”; attacks are commonly handled as post facto test cases that show how specific properties might be violated — but these desirable properties are rarely made clear as a pre-specification. Moreover, protocols –even corrected ones– do not come with recorded assumptions about the behaviour of unintended (or undesired) participants. For example, in Lowe’s attack [22] on the Needham-Schroeder [24] protocol, the nonces appear to be a means-to-an-end (of authentication) but are then sometimes used as session keys. Even in [7] the roles of Authentication and Key Establishment (AKE) are not always distinct. Protocol specification cannot be achieved simply: standard pre/post specifications record things that should be achieved by the conclusion of executions, but leakage of information is about situations that should not occur during execution. Rely-guarantee conditions (see below) were proposed to facilitate compositional development of shared-variable concurrent software but have subsequently been shown to be useful for recording assumptions about external interference from components that are not under the control of designers. Assumptions are fundamental to establishing the viability of security protocols. This invites the question of whether rely-guarantee conditions can be useful in the formal specification and justification of security protocols; specifically, for recording the assumptions made about attackers and proving protocol safety with respect to the assumptions. This paper explores evidence for a positive answer to the question. Furthermore, it is argued that challenging the assumptions provides a method for incorporating run-time checks that facilitate a reduction of assumptions about the behaviour of miscreants in protocols. It is well known that the original Needham-Schroeder (N-S) protocol [24] is “flawed”; but we want to identify (and record) a set of assumptions which might have resulted in the residual flaw; furthermore, we consider Gavin Lowe’s “corrected” protocol [22] and identify assumptions that appear to be made in justifying his suggested correction. Perhaps more importantly, what protocol designers (including Needham, Schroeder and Lowe) try to do is to build checks into their designs to reduce the assumptions that are made about the behaviours of intended and unintended participants. Here again, clear recording of assumptions is imperative. Examples of assumptions include: 1. secret keys are never shared 2. brute force attacks are not economic 3. if α𝛼\mathit{}\alphaitalic_α sends a secret under the public key of β𝛽\mathit{}\betaitalic_β, β𝛽\mathit{}\betaitalic_β will not leak the original secret. 4. if α𝛼\mathit{}\alphaitalic_α sends a secret under the public key of β𝛽\mathit{}\betaitalic_β, receipt of that same secret (under the public key of α𝛼\mathit{}\alphaitalic_α) justifies the conclusion that any accompanying new secret must have been generated by β𝛽\mathit{}\betaitalic_β. 5. a process is communicating with its intended recipient. The first two of these relate narrowly to encryption; the other assumptions also relate to the content of the messages sent. Being explicit about the assumptions is desirable because it invites discussion (and pre-deployment confirmation); it can also expose attack vectors; furthermore, it can help designers to increase the checking in a protocol and such checks can be shown to yield protocols that need less dangerous assumptions. Above all, if the claim is that a protocol manages to reduce the assumptions by adding checks, it must be worth making the assumptions explicit; furthermore, there should be a record of the argument as to correctness under the stated assumptions. 1.1 Rely-guarantee thinking Space limitations make it impossible to describe all of the background material. The long-established VDM notation is not described here but can be studied in [18]; there is also an international standard on VDM [14]. A convenient publication outlining the rely-guarantee ideas is [12]; early papers on rely/guarantee ideas include [15, 16, 17]; an algebraic presentation and more recent references are covered in [13]. R-G was conceived as a way of decomposing a specified component into sub-components that executed concurrently. It was subsequently observed (e.g. [20]) that rely conditions could be used to describe assumptions about external components that were not being designed: they were part of the problem space. In a further development (e.g. [8]) it was shown how layered R-G conditions could be used to describe fault-tolerant systems: optimistic rely conditions are linked to optimal behaviour but useful behaviour can still be achieved under weaker assumptions."
https://arxiv.org/html/2411.03815v1,How to Drawjectory? - Trajectory Planning using Programming by Demonstration,"A flight trajectory defines how exactly a quadrocopter moves in the three-dimensional space from one position to another. Automatic flight trajectory planning faces challenges such as high computational effort and a lack of precision. Hence, when low computational effort or precise control is required, programming the flight route trajectory manually might be preferable. However, this requires in-depth knowledge of how to accurately plan flight trajectories in three-dimensional space.We propose planning quadrocopter flight trajectories manually using the Programming by Demonstration (PbD) approach – simply drawing the trajectory in the three-dimensional space by hand. This simplifies the planning process and reduces the level of in-depth knowledge required. We implemented the approach in the context of the Quadcopter Lab at Ulm University.In order to evaluate our approach, we compare the precision and accuracy of the trajectories drawn by a user using our approach as well as the required time with those manually programmed using a domain specific language. The evaluation shows that the Drawjectory workflow is, on average, 78.778.778.778.7 seconds faster without a significant loss of precision, shown by an average deviation 6.676.676.676.67 cm.","Life is full surprising coincidences. Wolfang Reif twice had – in totally unrelated circumstances – a profound impact on the Institute of Software Engineering and Programming Languages at Ulm University. First, Wolfgang Reif was professor at the institute – then called Software Engineering and Compiler Construction – from 1994 to 2000 (before moving on to heading the Institute of Software and Systems Engineering at the University of Augsburg) working on formal verification and the KIV system. He put Ulm on the international map as a renowned place for formal methods research in the software engineering community. Second, Wolfgang Reif gave me (Matthias Tichy) in 2010 the first opportunity for independent research and teaching by offering me the position as acting professor for Self-Organizing Systems at the University of Augsburg for one year. While my goal was then to leave academia to join industry, seizing that opportunity allowed me to grow as a person and as an academic and put me on my academic path. Finally, those two unrelated circumstances surprisingly met when i was offered to become head of the Institute of Software Engineering and Compiler Construction at Ulm University in 2015. Impressed by the research of Wolfgang Reif’s group in the area of autonomous quadrocopters, i decided to build a quadrocopter lab as a research pillar in Ulm as well. Wolfgang Reif and his group, particularly, Andreas Angerer and Alwin Hoffmann, have been a tremendous help in building up that lab and our research. Hence, we chose to present one of our lab’s recent outcomes in this paper to commemorate Wolfgang Reif’s 65th birthday. Quadrocopters have a wide range of applications, from automated parcel delivery to search and rescue operations. Therefore, it is not surprising that the topic of planning flight routes or trajectories for drones is becoming increasingly important [19, 4]. Generally speaking, trajectory planning “consists in assigning a time law to the geometric path” [15]. Trajectory planning distinguishes two planning strategies: manual and automatic planning. When considering automatic trajectory planning, there are several promising approaches, such as genetic algorithms, artificial neural networks, or simple A* algorithms [4, 15]. However, some tasks require high precision and specific points of interest to be visited. Thus, the mentioned strategies may not always be ideal for trajectory planning [36]. For instance, an old building in danger of collapse might need to be screened by a drone with the drone having to check certain rooms. In this case, the creation of a route through the building, encompassing all points of interest, can be a particularly time-consuming and error-prone endeavor, particularly when the objective is to implement an automatic planning process, as this approach does not permit any form of intervention in the selection process of the points to be visited. One possible solution to ensure that all points of interest are visited is manual trajectory planning, whereby the drone’s flight path is not determined automatically but by hand. There are alternative approaches to manual planning including i) programming the path (mostly using low-code approaches) and ii) creating interactive points in a 3D environment on a PC [34, 20]. However, these methods often require knowledge of either how to program or how to interact with the points, making planning unintuitive. Accordingly, there is a desire for a user-friendly and still accurate method for manual trajectory planning, such as demonstrating the trajectory in the real world. This concept of “transfer[ring] new skills to a machine by relying on demonstrations from a user” [10] is called Programming by Demonstration. Due to its high intuitiveness, it is ideal for domain experts in a certain topic who need the support of quadrocopters, despite their potential lack of experience in controlling quadrocopters [26], which is why programming by demonstration has seen significant growth [31]. Based on the aforementioned problem and goals, we formulate the following research questions: RQ 1 How to automatically plan a trajectory by demonstrating the flight path once? RQ 2 How does trajectory planning by demonstration compare against trajectory planning by manual programming using a domain specific language in terms of accuracy and effort? We developed an approach for planning trajectories from a user’s demonstration, thus applying the programming by demonstration paradigm answering RQ1. Specifically, we track the demonstration, i.e., the point sequence of the desired flight path. Subsequently, a selection of points, designated as waypoints, is made from the recorded point sequence. These waypoints are then used to interpolate a trajectory using natural cubic splines. To answer RQ2, we evaluate the accuracy and the planning effort by systematically using both approaches to plan three different classes of trajectories: planar geometric figures, planar non-trivial figures, 3D figures. We measure the time to plan those trajectories as well as the accuracy of the planned trajectory compared to the intended trajectory. The evaluation shows that the Drawjectory workflow is, on average, 78.778.778.778.7 seconds faster without a significant loss of precision, shown by an average deviation 6.676.676.676.67 cm compared to using a domain specific language to manually program the trajectories. 2 and 3 introduces the quadrocopter lab setting as well as relevant foundations of trajectory planning. Thereafter, 4 presents our trajectory planning by demonstration approach and the proof-of-concept implementation. 5 presents the results of the evaluation. After discussing related work in6, we conclude in 7 by summarizing the work and addressing limitations as well as discussing potential future work."
https://arxiv.org/html/2411.03657v1,How do practitioners gain confidence in assurance cases?,"CONTEXT: Assurance Cases (ACs) are prepared to argue that the system’s desired quality attributes (e.g., safety or security) are satisfied. While there is strong adoption of ACs, practitioners are often left asking an important question: are we confident that the claims made by the case are true? While many confidence assessment methods (CAMs) exist, little is known about the use of these methods in practice.OBJECTIVE: Develop an understanding of the current state of practice for AC confidence assessment: what methods are used in practice and what barriers exist for their use?METHOD: Structured interviews were performed with practitioners with experience contributing to real-world ACs. Open-coding was performed on transcripts. A description of the current state of AC practice and future considerations for researchers was synthesized from the results.RESULTS: A total of n=19𝑛19n=19italic_n = 19 practitioners were interviewed. The most common CAMs were (peer-)review of ACs, dialectic reasoning (‘‘defeaters’’), and comparing against checklists. Participants preferred qualitative methods and expressed concerns about quantitative CAMs. Barriers to using CAMs included additional work, inadequate guidance, subjectivity and interpretation of results, and trustworthiness of methods.CONCLUSION: While many CAMs are described in the literature there is a gap between the proposed methods and needs of practitioners. Researchers working in this area should consider the need to: connect CAMs to established practices, use CAMs to communicate with interest holders, crystallize the details of CAM application, curate accessible guidance, and confirm that methods are trustworthy.Keywords: Assurance Cases, Safety Cases, Confidence Assessment, Practitioner Interviews","1 Introduction Critical systems are those where failure can lead to significant losses in terms of economic/financial harm, environmental damage, injury to humans, or loss of life, and are referred to as business-, mission-, security-, or safety-critical systems. The challenge of assuring these systems has increased as novel technologies and capabilities (e.g., artificial intelligence, autonomy, self-adaptation) are relied upon to realize critical functions. For instance, autonomous vehicles depend on artificial intelligence (mainly machine learning) methods to perceive their environment and detect nearby agents, and perception system failures have lead to severe consequences [45, 46]. Significant effort is expended to develop these systems and avoid such loss events. Assurance Cases (ACs) contain arguments, supported by evidence such as analyses or verification results, that a system will satisfy essential quality attributes within its defined operating environment [9]. ACs are sometimes also referred to as safety cases or security cases to illustrate the primary quality attribute they are assuring. ACs are required for compliance with technical standards and regulations in a range of industries, including: automotive [2, 5, 3], rail signalling and control [1], nuclear power systems [24], defence [51], and oil & gas [41]. There is also interest in preparing ACs for medical devices [25] and commercial aviation systems [37, 62]. ACs are especially useful for assuring novel systems, where established approaches to systems assurance are less effective or difficult to apply. For instance, a group of leading experts recently (re-)affirmed the role of ACs in assuring autonomous vehicles [59]. ACs are also increasingly recognized as ‘‘live’’ models of assurance that evolve alongside their system, particularly in the fields of autonomous and (self-)adaptive systems; such ACs are referred to as dynamic assurance cases [15, 12, 8]. While there is strong adoption of ACs as a means of capturing arguments and supporting evidence, not all arguments or evidence are equal. For instance, arguments that are convincing in one context might not hold for another, or evidence might be produced through untrustworthy means. Moreover, for dynamic ACs, the evidence might change over time, potentially invalidating claims that were previously thought to be true. So, when preparing or managing an AC, an important question arises: are we confident that the claims made in an AC are true? Confidence in ACs has been the subject of significant research for nearly two decades. Many authors have developed methods for assessing confidence in ACs [33, 36, 28, 39, 40]. Throughout the rest of this paper, we collectively refer to these (and other procedures, techniques, approaches, and activities intended to assess confidence in an AC) as confidence assessment methods (CAMs). While the development of each CAM was likely informed by its creator’s experience interacting with real-world systems and problems, there is a deficit of empirical research investigating if practitioners use these methods to prepare ACs for real-world systems and whether they find them useful. A consequence of this is that CAMs are developed or extended without a complete understanding of the needs of their end-users: the practitioners creating ACs for real-world critical systems. 1.1 Summary of Contribution This paper reports on a series of structured interviews with AC practitioners. The overall objective of this study was to develop an understanding of practitioners’ use of CAMs while preparing or managing ACs for real-world systems, including the specific CAMs used and barriers to their use. In other words, our aim is to understand how practitioners gain confidence in the claim(s) made in ACs. This study adopted a grounded-theory perspective and used open coding to analyze the interview transcripts and extract themes and concepts. The result is a description of the current state of practice for the study’s sample of practitioners for ACs and CAMs. Additionally, the findings are synthesized into five considerations for developing (or extending) CAMs. While other authors have published empirical studies (interviews, questionnaires) in the area of ACs [60, 50, 19, 13, 6], this is the first empirical work investigating the real-world use of CAMs by practitioners. 1.2 Paper Structure The remainder of this paper is structured as follows. To begin with, Section 2 surveys existing CAMs from the literature. Next, the study’s method is described in Section 3 and the results follow in Section 4. Then, we review similar empirical studies on ACs and compare our results in Section 5. Finally, we present a summary of our study’s findings and considerations for researchers developing CAMs in Section 6 and Section 7 closes with concluding remarks."
https://arxiv.org/html/2411.03346v1,Fixing Security Vulnerabilities with AI in OSS-Fuzz,"Critical open source software systems undergo significant validation in the form of lengthy fuzz campaigns. The fuzz campaigns typically conduct a biased random search over the domain of program inputs, to find inputs which crash the software system. Such fuzzing is useful to enhance the security of software systems in general since even closed source software may use open source components. Hence testing open source software is of paramount importance. Currently OSS-Fuzz is the most significant and widely used infrastructure for continuous validation of open source systems. Unfortunately even though OSS-Fuzz has identified more than 10,000 vulnerabilities across 1000 or more software projects, the detected vulnerabilities may remain unpatched, as vulnerability fixing is often manual in practice.In this work, we rely on the recent progress in Large Language Model (LLM) agents for autonomous program improvement including bug fixing. This is also the first such study with large-scale vulnerability fixing on real projects to the best of our knowledge. We customise the well-known AutoCodeRover agent for fixing security vulnerabilities. This is because LLM agents like AutoCodeRover fix bugs from issue descriptions, via code search. Instead for security patching, we rely on the test execution of the exploit input to extract code elements relevant to the fix. Our experience with the vulnerability data from OSS-Fuzz leads us to many observations. We note that having autonomy in the LLM agent is useful for successful security patching, as opposed to approaches like Agentless where the control flow is fixed. More importantly our findings show that we cannot measure quality of patches by code similarity of the patch with reference codes (as in CodeBLEU scores used in VulMaster), since patches with high CodeBLEU scores still fail to pass given the given exploit input. Our findings indicate that security patch correctness needs to consider dynamic attributes like test executions as opposed to relying of standard text/code similarity metrics.","Security vulnerabilities are one of the major threats to modern software systems. Once exploited by malicious attackers, security vulnerabilities can cause significant damage to the software and its users, incurring financial loss, data breaches, and more. In 2023, 30,927 new Common Vulnerabilities and Exposures (CVEs) are recorded by the National Vulnerability Database (NVD), and half of these vulnerabilities were classified as high or critical severity (skybox-report, ). The number of new CVEs has increased by 17% compared to the previous year, underscoring the accelerated pace of vulnerability detection and the critical need for timely remediation. The recent advancement in automatic programming with generative AI could further exacerbate the security issues, since some parts of the application code could come from Large Language Models (LLMs) with little security assurance. To safeguard the software systems, researchers and practitioners have made advances in both vulnerability detection and remediation. To detect security vulnerabilities before they are discovered/exploited by attackers, various techniques from static analysis (semgrep, ; codeql, ) to fuzzing (libfuzzer, ; afl, ) have been developed and also adopted in the industry. Static analysis techniques can be applied to detect a wide range of vulnerabilities. However, they are known to report false-positive warnings since they are often based on abstraction and conservative approximation of the program semantics (guo2023mitigating, ). Fuzzing, on the other hand, employs a biased random search in the program’s input space and dynamically executes the program. The dynamic nature of fuzzing ensures that a reported bug is a true positive. Fuzzing has been employed by major software companies to continuously scan for vulnerabilities in their development process (chromium-security, ; onefuzz, ). Google’s OSS-Fuzz, announced in 2016, provides continuous fuzzing for various core open-source software (ossfuzz-announced, ). As of August 2023, OSS-Fuzz has identified over 10,000 vulnerabilities across 1,000 projects (ossfuzz, ). While vulnerability detection techniques like fuzzing have shown to be both mature and effective, detection is only the first step in comprehensive software protection. A detected bug should be patched as soon as possible to reduce the time of exposure and the risk of being exploited. A previous study in 2021 has shown that the median time-to-fix (i.e. time from bug reporting to patch verification) to be 5.3 days for bugs detected by OSS-Fuzz (ding2021empirical, ), and 10% of the reported bugs are not fixed within the 90-day disclosure deadline. The rising number of detected vulnerabilities in recent years may require developers to invest even more time and effort in manually patching them. There is an urgent need for automated vulnerability remediation in continuous fuzzing pipelines to both ease the developers’ workload and minimize the window of vulnerability exposure. Recent advancements in generative AI and LLM agents have shown promise in autonomous vulnerability remediation in programs (zhang2024acr, ; ruan2024specrover, ; yang2024sweagent, ; xia2024agentless, ). These LLM agents are designed for general software engineering tasks, including bug fixing and feature development. They operate in real-world scenarios where tasks are described by users in natural language. Using the task description and the software codebase as inputs, the agents generate code modification suggestions to fulfill the specified requirements. Since repairing security vulnerabilities is a specialized software engineering task, we hypothesize that with appropriate adaptation, general-purpose LLM agents for software engineering can be repurposed for this task. These repurposed agents can potentially be integrated into existing vulnerability detection pipelines such as fuzzing, where they can provide the remediation after detection and complete the software protection cycle. CodeRover-S In this paper, we present a large scale real-world study on using LLM agents for security vulnerability repair. To enhance the realism of our effort, we use as dataset the OSS-Fuzz projects, which seek to enhance the state of practice of open source security (ossfuzz, ). We repurposed the open-source LLM agent AutoCodeRover (zhang2024acr, ; ruan2024specrover, ) to repair security vulnerabilities, and implemented a version named CodeRover-S (i.e. AutoCodeRover for security). With the vulnerability report and an exploit input produced by a fuzzing campaign, CodeRover-S autonomously generates patches that fix the detected vulnerability. In the process of adapting LLM agents for vulnerability repair, we identified that one of the main challenge was the insufficient information contained in the auto-generated vulnerability report. Unlike human-written issue report for general software engineering tasks, vulnerability reports are often auto-generated by the fuzzer and only contain information like the bug type and crash stacktrace. To enrich the vulnerability report, we extract dynamic call graph information from the exploit input found by fuzzing, which is then used to augment the report generated by the fuzzer. In addition, we perform a type-based analysis at the program locations identified as faulty by the agent, and use the additional type information to improve the compilation rate of generated patches. With these adaptation, we built CodeRover-S which can autonomously repair security vulnerabilities detected in a fuzzing pipeline. To evaluate the efficacy of CodeRover-S in a real-world setup, we conducted experiments on real C/C++ vulnerabilities previously detected by OSS-Fuzz. Each detected vulnerability comes with an exploit input that resulted in a crash from sanitizers (e.g. AddressSanitizer (asan, ), MemorySanitizer (msan, )), and the crash report generated by the sanitizer. Experiments on 588 real-world vulnerabilities from a previously curated dataset (mei2024arvo, ) show that CodeRover-S can repair 52.4% of these vulnerabilities by resolving the crash from the exploit input. We also comparatively study the efficacy of other deep learning or LLM-based systems in this realistic vulnerability repair scenario. Firstly, we apply a general-purpose LLM coding agent directly to the vulnerability repair setting and observe a lower repair efficacy. Secondly, we evaluate the state-of-the-art deep learning based vulnerability repair system VulMaster (zhou24vulmaster, ) in the OSS-Fuzz dataset. Existing deep learning based vulnerability repair techniques often make strong assumptions such as the perfect fix location is provided at either function- or line-level. This localization assumption is too strong for real-world vulnerability repair, and we observe a low repair efficacy when they are used in a realistic repair setup. Furthermore, current evaluation of vulnerability repair tools often focus on how closely the generated patches match the developer’s patch, using metrics like exact match or similarity scores such as BLEU (papineni2002bleu, ) or CodeBLEU (ren2020codebleu, ). However, we find that these metrics may not accurately reflect the true efficacy of the repairs. Therefore, it is important to assess vulnerability repair systems on datasets with executable inputs in future research. In summary, our contributions are as follows: • We explore the feasibility of adapting general-purpose LLM programming agents for the repair of security vulnerabilities. We integrate call graph information and type-based analysis to provide richer context for LLM agent-based vulnerability repair, resulting in improved patch quality. Our approach is implemented as a new agent CodeRover-S which is specialized for security vulnerability repair. • We conduct an empirical study on the use of LLM agents to repair real-world security vulnerabilities identified by the industrial fuzzing service OSS-Fuzz. Our findings indicate that leveraging LLM agents for vulnerability remediation is a promising approach to complement existing detection pipelines and complete the software protection life cycle. • We present empirical evidence suggesting that similarity scores may not accurately measure the effectiveness of vulnerability repair systems. To better evaluate learning-based vulnerability repair systems, future research should consider test-based validation methods."
https://arxiv.org/html/2411.03292v1,Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?,"Converting webpage design into functional UI code is a critical step for building websites, which can be labor-intensive and time-consuming. To automate this design-to-code transformation process, various automated methods using learning-based networks and multi-modal large language models (MLLMs) have been proposed. However, these studies were merely evaluated on a narrow range of static web pages and ignored dynamic interaction elements, making them less practical for real-world website deployment.To fill in the blank, we present the first systematic investigation of MLLMs in generating interactive webpages. Specifically, we first formulate the Interaction-to-Code task and build the Interaction2Code benchmark that contains 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories. We then conduct comprehensive experiments on three state-of-the-art (SOTA) MLLMs using both automatic metrics and human evaluations, thereby summarizing six findings accordingly. Our experimental results highlight the limitations of MLLMs in generating fine-grained interactive features and managing interactions with complex transformations and subtle visual modifications. We further analyze failure cases and their underlying causes, identifying 10 common failure types and assessing their severity. Additionally, our findings reveal three critical influencing factors, i.e., prompts, visual saliency, and textual descriptions, that can enhance the interaction generation performance of MLLMs. Based on these findings, we elicit implications for researchers and developers, providing a foundation for future advancements in this field. Datasets and source code are available at https://github.com/WebPAI/Interaction2Code.","As the Internet continues to evolve and expand, more and more websites emerge, contributing to the diverse and ever-growing online world. As of 2024, the digital landscape comprises approximately 1.09 billion websites (sta, 2024), supporting a variety of applications in people’s daily lives. The design and development of Graphical User Interfaces (GUIs) are vital for creating a website. A well-designed GUI not only enhances the website’s visual attractiveness but also improves usability and user satisfaction. In such a process, GUI design involves shaping the website’s aesthetics(UIt, 2024), such as layout, colors, and typography (Chen et al., 2018a; Kuusinen and Mikkonen, 2013). In contrast, GUI development is about implementing that aesthetic through programming languages. Nevertheless, such conversion is a complex and time-consuming task. Developers must manually map visual elements to their corresponding implementation details, which can lead to errors and discrepancies between the original design and the final looks (Chen et al., 2018a; Nguyen and Csallner, 2015a; Lelli et al., 2015; Moran et al., 2018b; Zeidler et al., 2013). To allow developers to transform design diagrams into functional GUI code more easily, several automated GUI code generation methods have been proposed, which can be further categorized into two types: learning-based and LLM-based approaches. The learning-based methods, such as Pix2code (Beltramelli, 2018), design a novel method based on CNN and LSTM to generate user interface code by reverse-engineering a single GUI image input. Chen et al.(Chen et al., 2018b) present a neural machine translator to extract visual features in UI images, encode these features’ spatial layouts, and generate GUI skeletons in a unified neural network framework. However, these deep learning-based methods exhibit compromised performance and fail in generalizing to diverse web page elements due to their limited knowledge learning from training samples. Recently, incorporating visual information into Large Language Models (LLMs) has led to the development of Multimodal Large Language Models (MLLMs) (Yin et al., 2023; Chen et al., 2022b; Tsimpoukelli et al., 2021; Alayrac et al., 2022; Li et al., 2023b). Leading models in this domain, such as GPT-4o (OpenAI, 2024a), Claude-3.5 (Anthropic, 2024), and Gemini-1.5 (Google, 2024), have achieved excellent performance in visual understanding tasks (Yang et al., 2023; Dai et al., 2023). Furthermore, research has shown that LLMs have remarkable performance on various code intelligence tasks (Hou et al., 2023), including code generation (Yu et al., 2023; Li et al., 2023a; Du et al., 2023; Dong et al., 2023; Jiang et al., 2023; Gilbert et al., 2023), code completion (Nijkamp et al., 2022; Ding et al., 2023; Dibia et al., 2022; Li et al., 2023c, 2022; Chen et al., 2021), and code summarization (Mastropaolo et al., 2021, 2022; Gu et al., 2022; Chen et al., 2022a; Gao et al., 2023; Arakelyan et al., 2023). These advances create new opportunities for the Design-to-Code task, i.e., generating code from screenshots to replicate web page elements, layout, text, and colors. For example, Design2Code (Si et al., 2024) designs three types of prompts to stimulate MLLMs’ web content understanding and self-refined capabilities for GUI code generation. DCGen (Wan et al., 2024) proposes a divide-and-conquer-based approach to prompt MLLMs to generate webpage elements more accurately. Regardless of the continuous investigation on promoting the models’ capability, their evaluation scope is restricted to static pages. More specifically, existing research (Si et al., 2024; Yun et al., 2024; Gui et al., 2024) only focuses on the static appearance of the webpage (e.g., color, layouts), ignoring the dynamic interactive properties and functionality of elements, such as size selection list, quantity adjustment button shown in Fig. 1(a), and other designs for user engagements. Additionally, we observe that such interactive elements account for a large proportion of the webpage in real-world software practices. We randomly select 10 real-world webpages with different topics to analyze the ratio of interactive elements, the results in Fig. 1(b) indicate that interactive elements take up more than 50% cases. Then we utilize GPT-4o (OpenAI, 2024a) to generate the GUI code containing interactive elements. As shown in Fig. 1(c), fewer than 15% of interactive elements are correctly implemented, highlighting the current limitations in handling webpage interactive design. (a) Example of interactive elements. (b) Ratio of interactive and static elements. (c) Implemented vs. unimplemented interactive elements ratio of GPT-4o. Figure 1. Interaction example and interactive elements ratio of different types of webpages. Static webpages inherently limit user interaction with web elements, hindering access to new content (such as browsing images via carousel buttons) or impeding task completion (like selecting clothing sizes from drop-down menus), thereby impairing user experience. In this context, evaluations of static pages become inadequate for real-world webpage deployments, where dynamic elements are prevalent. Therefore, We argue that a benchmark for webpages that includes interactive elements is essential to enhance the practicality, usability, and user engagement of studies on auto-generated GUI code. In this paper, we emphasize the importance of webpage interactions by investigating the following question: to what extent MLLMs can produce interaction code based on the visual design? To this end, we provide a systematic analysis of MLLMs’ capability in reproducing dynamic interactions on webpages. Specifically, we first define the Interaction-to-Code task, i.e., generating code from a series of screenshots representing webpage interactions to replicate interactive elements. Then we build the Interaction2Code benchmark that encompasses a diverse array of webpages and interactions. It comprises 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories. By curating a wide range of interaction types, we offer a representative and diverse evaluation dataset for assessing the capabilities of MLLMs producing dynamic webpages in a more realistic scenario. We mainly investigate the following six research questions (RQs): Table 1. Summarization of key findings. Aspects Findings Limitations MLLMs exhibit limited performance in reproducing fine-grained interaction features, such as structure, text, and position (Finding 1). Performance based on the type of interaction: MLLMs excel at handling interaction with fixed pattern (e.g., selection list) and clear changes (e.g., new window creation), while struggle with interactions that involve complex changes (e.g., iframe, progress) and subtle visual modifications (Finding 3). Failure Types The predominant failures are “No interaction”, “Partial implementation”, “Interactive element missing”, and “Wrong position after interaction”. The most critical failures include “Interactive element Missing”, “Effect on wrong element”, “Wrong Function” and “No interaction” (Finding 4). Key Factors ★Well-designed prompts are effective: Chain-of-Thought enables step-by-step interaction analysis, while marking interaction areas provides essential visual signals. Both approaches improve the quality of generated interactions (Finding 2). ★ Enhanced visual saliency significantly improves interaction generation, particularly in complicated cases (Finding 5) ★Supplementary textual descriptions substantially boost MLLMs’ interaction generation capabilities (Finding 6). • RQ1: How do different MLLMs perform in Interaction-to-Code task under different prompts? • RQ2: How do humans evaluate the usability of interactions generated by MLLMs? • RQ3: How do MLLMs perform in code generation across different interaction scenarios? • RQ4: What types of mistakes do MLLMs make in generating interactions? • RQ5: How does visual saliency influence the quality of generated interactions? • RQ6: Which representation modality – visual signals or textual description, enhances MLLMs to generate interaction code? To address RQ1, we design three distinct prompt types: direct prompts, Chain-of-Thought prompts, and Mark prompts (which mark the interaction areas) to evaluate the performance of three state-of-the-art MLLMs under varying prompt conditions. For RQ2, we conduct user studies where participants interact with the generated webpages to assess the usability. In RQ3, we analyze MLLMs’ performance across different interaction scenarios by calculating usability rates for various interaction types. To answer RQ4, we invite human annotators to categorize and discuss webpage generation failures, followed by data analysis to reveal the most prevalent error patterns and their severity. For RQ5, we evaluate the generated interactions across varying saliency levels to investigate their impact on interaction generation performance. Finally, for RQ6, we examine the influence of interaction representation modality by comparing three input configurations: visual-only, textual description-only, and combined visual-textual. Based on our experimental results, we present six key findings, shown in Table 1, including the limitations of MLLMs, failure types and key factors for enhancing interaction generation performance. Our contributions are summarized as follows: • Task formulation. To the best of our knowledge, this is the first study to formulate the Interaction-to-Code task and present a systematic study on the code generation capabilities of MLLMs for dynamic interaction of webpages. • Benchmark. We build the first real-world webpage interaction datasets Interaction2Code containing 97 webpages and 213 interactions, spanning 15 webpage topics and 30 interaction categories. • Key Findings. Our in-depth analysis reveals the limitations of MLLMs, identifies 10 representative failure types and their underline cause, and provides key factors for enhancing performance on the Interaction-to-Code task. This key findings offer valuable implications for researchers and developers engaged in automated front-end development."
https://arxiv.org/html/2411.03291v1,"Using Assurance Cases to Guide Verification and Validation of Research
Software","Research software engineers can use Assurance Cases (ACs) to guide Verification and Validation (VnV) efforts. An AC is a structured argument that a property like correctness holds. We illustrate how ACs can guide VnV activities via a case study of software for automatically extracting the 3D segmentation of the aorta from medical images of the chest. The AC argument suggests that the following evidence is required: comparison to a pseudo-oracle; traceability between requirements, design, code and tests; review of all artifacts by a domain expert with proper credentials; documentation of input assumptions; and a warning that only qualified people should use the software. The case study highlights that code is not the only artifact of interest for building confidence and that making an explicit distinction between software and user responsibilities is useful.","Our world faces challenges, like climate change, a global pandemic and an energy crisis, that are unprecedented in their impact and global scope. At the heart of any approach to address these challenges is Research Software (RS). To use RS, we must trust RS. Unfortunately, trusting RS requires a significant effort be invested in Verification and Validation (VnV) activities. Many techniques exist for VnV (unit testing, symbolic execution, metamorphic testing [1], the method of manufactured solutions [2], code inspection, formal proof, etc.), but which techniques should we pick? How much time and effort do we need to invest to achieve an acceptable level of confidence? How should our limited resources be deployed? What VnV activities are most valuable for building confidence in correctness? Assurance Cases (ACs) can be used to answer these questions. Assurance Cases (ACs) provide a structured argument that some property (like correctness, reproducibility, safety, etc) holds. Here we look at arguments for correctness (safety is out of the current scope) in research software. The structure of the AC decomposes a top-level claim into sub-claims that are themselves potentially decomposed. The higher level claims are too big and too abstract to prove, but if the argument is structured properly, the bottom claims can be supported with evidence. The evidence, together with a logical, convincing argument to justify the decomposition, supports the top-level claim. Therefore, the evidence required shows us exactly the amount of VnV activities needed to support the AC, no more and no less. In Section II we provide background information on assurance cases and on the case study used to illustrate and explain how an assurance case guides the evidence requirements. The case study is software for extracting a 3D segmentation of the aorta from chest Computed Tomography (CT) data. We present the structured argument for the case study in Section III and the required evidence in Section IV. The final section, Section V, provides concluding remarks."
https://arxiv.org/html/2411.03079v1,Utilizing Precise and Complete Code Context to Guide LLM in Automatic False Positive Mitigation,"Static Application Security Testing (SAST) tools are vital in modern software development for early bug detection, improved code quality, and enhanced efficiency. However, they often produce many false positive alerts requiring manual review, slowing down development. Automating false positive mitigation is thus essential for advancing SAST tools. Previous approaches have utilized static and dynamic analysis techniques, or integrated machine learning models to differentiate true positives from false positives. The advent of Large Language Models (LLMs), adept at understanding natural language and code, offers promising ways to improve the accuracy and usability of SAST tools. However, existing LLM-based methods need improvement in two key areas: first, extracted code snippets related to warnings are often overly broad and cluttered with irrelevant control and data flows, reducing precision; second, critical code contexts are often missing, leading to incomplete representations that can mislead LLMs and cause inaccurate assessments. Overcoming these limitations is crucial to fully leveraging LLMs for developing precise, context-aware SAST tools.To ensure the use of precise and complete code context, thereby avoiding misguidance and enabling LLMs to reach accurate conclusions, we propose LLM4FPM framework. One of its core components is eCPG-Slicer, which builds an extended code property graph and then extracts line-level, precise code contexts for warnings. Moreover, LLM4FPM incorporates our FARF algorithm, which builds a file reference graph and identifies strongly connected components. This enables efficient detection of all files associated with a given warning in linear time, allowing eCPG-Slicer to perform thorough analysis across these dependent files, ensuring a complete code context representation of the warning. We evaluate LLM4FPM on the Juliet dataset, where it comprehensively outperforms the baseline, achieving an F1 score above 99% across various Common Weakness Enumerations(CWEs). By using a free, lightweight open-source model, LLM4FPM avoids dependence on costly alternatives, significantly reducing inspection costs, saving up to $2758 per run on Juliet—with an average inspection time of 4.7s per warning, underscoring its efficiency. Moreover, further tests on popular real C/C++ projects demonstrate its practical value.This paper emphasizes the critical impact of precise and complete code context and highlights the potential of combining program analysis with LLMs, advancing automated vulnerability analysis and improving the quality and efficiency of modern software development.","Static Application Security Testing (SAST) tools are indispensable in modern software development, providing automated analysis to detect potential bugs, security vulnerabilities, and other safety or security issues. These tools can be used not only for bug detection (Vassallo et al., 2020; Habib and Pradel, 2018), but also for security code review (Charoenwet et al., 2024) and other work related to code safety/security (Chess and McGraw, 2004; Livshits and Lam, 2005; Wei et al., 2018). Companies like Facebook (Distefano et al., 2019) and Google (Sadowski et al., 2018) have scaled the use of SAST tools across their massive code bases, demonstrating their practicality and value in real-world applications. Existing SAST approaches generally fall into two categories: rule-based systems and learning-based systems (Croft et al., 2021). Rule-based methods rely on predefined sets of coding patterns, while learning-based methods incorporate machine learning models to predict potential issues by analyzing large code base. However, a consistent challenge with SAST tools is their generation of numerous warnings, many of which are false positives (Aloraini et al., 2019; Yang et al., 2019). A false positive occurs when a tool incorrectly flags a piece of code as problematic, even though it is actually correct and free of any issues. These false positives cause significant frustration among developers, leading to wasted time and resources. While the warnings produced by SAST tools are crucial for code safety and security, the high proportion of false positives dilutes their utility, reducing overall trust in the tools themselves. In some cases, false positives may even cause developers to give up using SAST tools(Johnson et al., 2013), thus bringing risks. Johnson et al. (2013); Christakis and Bird (2016) highlight false positives as a major issue for developers, recommending that the false positive rate should be below 20%. Manual review of false positives is time-consuming and resource-intensive, especially in large-scale projects. Therefore, automatic false positives mitigation (FPM) is therefore a key focus for improving SAST tools. Some researchers have used traditional dynamic and static program analysis techniques to attempt to mitigate false positives, such as combining dependency flow (Liang et al., 2012; Muske et al., 2019; Livshits et al., 2009; Giet et al., 2019), program slicing (Rival, 2005b, a), code tainting (Baca, 2010) and other methods (Chimdyalwar et al., 2015; Chebaro et al., 2012). Before the era of large language models (LLMs), some researchers used machine learning methods to train classifiers for FPM tasks (Koc et al., 2017; Ranking, 2014). The advent of LLMs, with their powerful capabilities in natural language processing and code understanding, brings new potential for programming and code safety/security (Li et al., 2023; Wang et al., 2024a; Wu et al., 2023; Yang et al., 2024; Yu et al., 2024; Pearce et al., 2023; Li et al., 2024), opening pathways for more effective FPM through combined program analysis. Wen et al. (2024) are the first to leverage LLM for the FPM task. They focus on extracting function bodies based on bug reports to help LLM determine if these warnings are false positives. However, there are two main limitations to this approach. First, the extracted code snippets need to be more precise and concise, eliminating irrelevant information that may hinder the analysis. In real-world projects, a function may contain many lines of code with complex control flows and data flows, but only a few critical elements are relevant to the warning, while the rest become distractions. Irrelevant flows can mislead LLMs, leading to incorrect assessments. Additionally, longer code snippets result in longer response times and token consumption. Existing studies also indicate that LLMs struggle with long tokens and complex control flow code, limiting their effectiveness (Kwan et al., 2023; Wang et al., 2024b; Anand et al., 2024; Mordechai et al., 2024). Second, the extracted function bodies derived from bug reports may lack essential code contexts related to the warning, creating gaps in contextual understanding. Existing SAST tools often fail to include all files related to the warning, such as those containing associated global variables and external function definitions. These variables and function definitions significantly impact control and data flow, and their absence can lead to incorrect conclusions. To address the aforementioned issues, we strive to achieve three goals: (1) Precise Code Context. The extracted code snippet should focus on control flows and data flows relevant to the warning, capturing the precise code context while omitting unnecessary parts that might distract the LLM and lead to incorrect or ambiguous conclusions. (2) Complete Code Context. The analysis should account for key information often missing from bug reports, such as references to global variables or invoked functions located in other files. Without this, the extracted context remains incomplete. (3) Correct Conclusions. After obtaining precise and complete code context, there is an opportunity to more effectively guide the LLM to make accurate judgments on bug reports from SAST tools. To provide precise code context related to the warning within the given files, we first investigate current slicer tools including Frama-C Slicing(Frama-C, 2007) for C, Flowistry(Crichton, 2021) for Rust, Pyan3(Edmund Horner, 2018) and PyCG(Salis et al., 2021) for Python, as well as JavaSlicer(Galindo et al., 2022) for Java. However, none of the tools are suitable for our scenarios, as they fail to provide inter-procedure dependence relations based on arbitrary slicing criteria. Inspired by Zheng et al. (2021) utilizing Joern (Yamaguchi et al., 2014) to create a new code representation for vulnerability detection, we propose eCPG-Slicer, a line-level precise code slicer, which slices on a new extended Code Property Graph (eCPG). The eCPG enhances the merged code property graph (CPG) and program dependence graph (PDG) generated by Joern. It adds dependence edges linking function call sites to their definitions, essential syntactic structure such as if to their subordinates, and variable declarations to corresponding identifiers. This enriched information enables eCPG-Slicer to obtain the precise code context related to the warning within the provided files. To ensure complete code context for warnings, whole-program analysis is theoretically feasible. However, in real projects, it is a resource-intensive task, requiring significant time and memory. Since each run of a SAST tool generates numerous warnings, applying whole-program analysis to each warning is unsustainable. To mitigate the overhead, we propose a dependent file extractor, which employs our custom-designed FARF algorithm. This algorithm can extract code files related to a warning in linear time. These extracted files are then analyzed by our eCPG-Slicer, enabling it to obtain the complete code context associated with the warning while avoiding the significant overhead of whole-project analysis. We integrate the aforementioned eCPG-Slicer and dependent file extractor with LLM to establish the LLM4FPM framework, which aims to determine whether a bug report for an open-source project is a false alarm. With the help of the eCPG-Slicer, this framework can obtain the precise and complete code context related to a warning, allowing the LLM to act as a code expert to determine whether the warning is a false positive or not. We evaluate our LLM4FPM on the Juliet dataset (for Assured Software, 2017), a widely used open-source dataset for assessing the detection capabilities of SAST tools, containing a large number of known buggy and bug-free C/C++ programs, and further test its effectiveness on 8 real-world open-source projects. The results indicate that our LLM4FPM outperforms existing work in all aspects, achieving an F1 score above 99% across various CWEs and eliminating over 85% false positive warnings in real-world projects. Additionally, LLM4FPM operates at a low cost, with an average inspection time of 4.7 seconds per bug and relying solely on free open-source LLM, saving $2758 per run on the Juliet dataset. This demonstrates the positive guidance provided to LLMs by precise and complete code context. Overall, our contributions can be summarized in the following three points: • First, we propose a line-level precise code slicer eCPG-Slicer. It constructs an extended Code Property Graph (eCPG) and then extracts line-level code context related to the warning within the given files. • Second, we propose a linear complexity algorithm, FARF, which is used to identify source files that have dependencies related to a warning, enabling the slicer to extract the complete code context. • Third, we integrate our eCPG-Slicer and FARF algorithm into our LLM4FPM framework. LLM4FPM can efficiently drive LLMs to give judgements for a given warning generated by SAST tools. The following sections are organized as follows. In section 2, we present our motivation using two simplified examples and introduce related work. In section 3, we describe LLM4FPM and the detailed design and implementation of its components. In section 4, we evaluate LLM4FPM and analyze the results. In section 5, we summarize the limitations and propose future work. Finally, in section 6, we provide a summary and conclusion of the entire paper."
https://arxiv.org/html/2411.02462v1,Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study,"The advent of large language models (LLMs) like GitHub Copilot has significantly enhanced programmers’ productivity, particularly in code generation. However, these models often struggle with real-world tasks without fine-tuning. As LLMs grow larger and more performant, fine-tuning for specialized tasks becomes increasingly expensive. Parameter-efficient fine-tuning (PEFT) methods, which fine-tune only a subset of model parameters, offer a promising solution by reducing the computational costs of tuning LLMs while maintaining their performance. Existing studies have explored using PEFT and LLMs for various code-related tasks and found that the effectiveness of PEFT techniques is task-dependent. The application of PEFT techniques in unit test generation remains underexplored. The state-of-the-art is limited to using LLMs with full fine-tuning to generate unit tests. This paper investigates both full fine-tuning and various PEFT methods, including LoRA, (IA)3, and prompt tuning, across different model architectures and sizes. We use well-established benchmark datasets to evaluate their effectiveness in unit test generation. Our findings show that PEFT methods can deliver performance comparable to full fine-tuning for unit test generation, making specialized fine-tuning more accessible and cost-effective. Notably, prompt tuning is the most effective in terms of cost and resource utilization, while LoRA approaches the effectiveness of full fine-tuning in several cases.","Large language models (LLMs) have been developed for various tasks, including general language and code-related activities, significantly enhancing programmers’ performance. Training LLMs on extensive code bases like GitHub give them seemingly magical capabilities. However, despite their proficiency in multiple code-related tasks, LLMs still struggle with cost-effectiveness in their training. The traditional approach to handling these specialized tasks has been fine-tuning the model specifically for each task. Given that LLMs often have billions, if not trillions, of parameters, traditional fine-tuning, which adjusts all parameters, is extremely computationally expensive and resource-intensive. However, state-of-the-art approaches [1] still rely on energy-costly full fine-tuning. Advancements in instruction-tuned LLMs have encouraged investigations into various prompting and tuning techniques, e.g., [2, 3, 4], that do not require fine-tuning. This challenge has led to the development of multiple techniques for fine-tuning larger models with limited resources, commonly referred to as parameter-efficient fine-tuning (PEFT). The application of various PEFT methods has been explored for code-related generation tasks, such as code completion, code summarization, and more [5, 6, 7, 8, 9, 10, 11, 12, 13]. Evaluations of the PEFT methods show that they could achieve performance comparable to full fine-tuning while significantly reducing the computational burden by strategically fine-tuning only a chosen subset of parameters during the code-related generation process. However, the effectiveness of PEFT techniques on different tasks varies. LLMs have been used to generate unit test cases and assertion statements [14, 12, 2, 3, 4, 1]. However, to the best of our knowledge, the area of unit test generation using PEFT remains largely unexplored. Therefore, in this work, we empirically evaluate the performance of various PEFT methods for unit test generation. We focus on answering the following research questions: • RQ1: How well does PEFT perform on unit test generation? • RQ2: What is the relation between resource utilization and performance of PEFT in unit test generation? To answer the research questions, we compared full fine-tuning with three popular PEFT methods: LoRA (Low-Rank Adaptation) [15], (IA)3 (Infused Adapter by Inhibiting and Amplifying Inner Activations) [16], and prompt tuning [17]. We conducted these comparisons using ten LLMs from three open-source, decoder-only LLM families. The LLMs have varying architectures and sizes, ranging from 350 million to 16 billion parameters. We measured and compared the unit test generation performance of full fine-tuning and PEFT methods using well-established benchmark datasets rooted in real-world unit test cases. Our results show that LoRA is generally the most reliable method for improving the quality of the generated unit tests and is, in many cases, on par with or better than full fine-tuning. Conversely, (IA)3 appears to be the least effective, often resulting in minimal improvements while consistently retaining prior knowledge in the LLMs. Prompt tuning demonstrates significant variability, excelling with some models, particularly larger ones, while underperforming with others. This variability highlights the importance of considering model size and characteristics when choosing the appropriate tuning method. Prompt tuning can be considered ”low-hanging fruit,” while LoRA should be employed for greater stability. Our contributions are as follows: • We conducted the first empirical study to extensively evaluate training LLMs using various PEFT methods—LoRA, (IA)3, and prompt tuning—for unit test generation across a wide range of LLM models. • We offered a comprehensive comparison of PEFT methods versus full fine-tuning. • Our findings yield practical guidelines highlighting the potential gains and limitations of the various PEFT methods and full fine-tuning in resource utilization and performance. The rest of the paper is organized as follows. We provide preliminaries in Section II. Section III introduces related work. We explain our experimental design in Section IV. Section V presents the experimental results. Section VI discusses our results and limitations. Section VII concludes the study and proposes future work."
https://arxiv.org/html/2411.02320v1,An Empirical Study on the Code Refactoring Capability of Large Language Models,"Large Language Models (LLMs) aim to generate and understand human-like text by leveraging deep learning and natural language processing techniques. In software development, LLMs can enhance the coding experience through coding automation, reducing development time and improving code quality. Code refactoring is a technique used to enhance the internal quality of the code base without altering its external functionalities. Leveraging LLMs for code refactoring can help developers improve code quality with minimal effort. In this paper, we conduct an empirical study to assess the quality of the refactored code generated by StarCoder2, which is an LLM designed for code generation. Specifically, we (1) evaluate whether the code refactored by the LLM or by developers is more effective at improving code quality, (2) understand the differences between the types of refactoring applied by the LLM and developers and compare their effectiveness, and (3) evaluate whether the quality of the refactored code generated by the LLM can be improved through one-shot prompting and chain-of-thought prompting. We analyze the refactoring capabilities of StarCoder2 and developers on 30 open-source Java projects. We find that StarCoder2 reduces code smells by 20.1% more than developers on automatically generated refactorings. StarCoder2 excels in reducing more types of code smells, such as Long Statement, Magic Number, Empty Catch Clause, and Long Identifier. Developers perform better in fixing complex issues, such as Broken Modularization, Deficient Encapsulation, and Multifaceted Abstraction. Furthermore, StarCoder2 outperforms developers in refactoring types that are more systematic and repetitive. However, developers surpass in refactorings that require a deeper understanding of code context and architecture. Our findings show that One-shot prompting improves unit test pass rate over the zero-shot prompt by 6.15% and reduces code smells at a 3.52% higher rate. When generating five refactorings per input, StarCoder2 achieves a unit test pass rate of 28.8% higher than when generating one refactoring per input, which indicates that the combination of one-shot prompting with multiple refactoring generations per input leads to the best performance. By providing insights into the capabilities and best practices for integrating LLMs like StarCoder2 into the software development process, our study aims to enhance the effectiveness and efficiency of code refactoring in real-world applications.","Automatic code generation allows developers to produce code more efficiently and consistently (Svyatkovskiy et al., 2020). The advancement of Large Language Models (LLMs) has demonstrated its ability in generating code snippets and solving complex programming problems (Fan et al., 2023b; Wei et al., 2023). LLMs are trained using an enormous amount of open-source code; therefore, if the training code has flaws, the generated code could suffer from poor design (Xu et al., 2022c) or introduce technical debts (Chang et al., 2024). Therefore, the integration of the generated code into the software development process may increase maintenance costs and reduce the overall quality of the software (Xu et al., 2022c). Code refactoring is the process of enhancing the internal quality of the code without altering its external behavior (Fowler et al., 1999; Fowler, 2018; Noei et al., 2023). Refactoring types are the specific techniques used, such as renaming variables for clarity, extracting methods to reduce complexity, and moving classes or methods (Guru, 2024). Refactoring is typically associated with the elimination of code smells (Palomba et al., 2018) which are design flaws that violate design principles and compromise the maintainability (Van Emden and Moonen, 2002). As a result, the refactored code can improve software design, make code easier to understand, and remove dependencies among software components (Mens and Tourwe, 2004; Opdyke and Johnson, 1992; Noei et al., 2023). Human developers bring contextual knowledge and experience to refactoring, which is challenging for automated systems to replicate (Van Emden and Moonen, 2002; Fowler et al., 1999; Fowler, 2018). Recent studies have explored the potential of LLMs (e.g., GPT-3. (Shirafuji et al., 2023)) in automating code refactoring, where models are provided with a prompt and the code to be refactored (Shirafuji et al., 2023; Choi et al., 2024). However, there are no systematic studies to understand the impact of LLM-generated refactorings on code quality improvement (Alshahwan et al., 2024). To have a better understanding of how LLMs manage refactoring tasks without compromising the functionality of the code, we conduct an empirical study to evaluate the effectiveness of LLMs in refactoring tasks and compare their performance against that of human developers. We aim to provide an evaluation framework to determine how well LLMs can replicate or surpass human expertise in code refactoring tasks. Given the wide range of refactoring types that developers can perform, we strive to gain insight into the coverage of refactoring types that LLMs can perform, compared to those performed by developers. Moreover, we explore the possibility of improving the capabilities of LLMs in refactoring by applying one-shot and chain-of-thought prompting to enhance LLMs’ performance in refactoring. The LLMs are trained on public data, including open-source repositories (Xu et al., 2022a). Therefore, if the LLMs are trained using refactored code from the developers that we aim to compare LLM-generated refactored code with, it cannot truly reflect LLMs’ refactoring capabilities. Overfitting could occur if the LLM has been trained on the same code, potentially leading to artificial inflation of its performance and resulting in over-optimistic results (Brownlee, 2020). Popular LLMs such as OpenAI’s GPT-4 (OpenAI et al., 2024), Google’s PaLM (Chowdhery et al., 2022), and Meta’s LLaMA (Touvron et al., 2023) do not provide their training datasets publicly. To mitigate the risk of data leakage, we choose StarCoder2-15B-instruct (Lozhkov et al., 2024), trained on the publicly available dataset, The Stack v2 (Lozhkov et al., 2024). Therefore, we ensure that our selection of open-source projects for evaluating the LLM has not been included in the LLM’s training dataset. StarCoder2 has exceptional performance on the HumanEval benchmark (Zheng et al., 2023) which assesses the model’s ability to generate functional code for a variety of programming tasks, making it a strong indicator of the model’s effectiveness in real-world coding scenarios. StarCoder2 achieves 46.3% for the pass@1 metric on the HumanEval benchmark, meaning that 46.3% of the generated code snippets perform the required task correctly on the first attempt. In this paper, we conduct an empirical study using 30 open-source Java projects, that are not included in the Stack-v2 (Li et al., 2023a). We extract 5,194 refactoring commits from the 30 projects. To compare the refactorings conducted by human developers and the ones generated by StarCoder2, we leverage the rich commit history available in the subject projects. We aim to extract the code in a file level before a developer’s refactoring and the code after the developer’s refactoring. Then we use prompt engineering to instruct StarCoder2 to refactor the code before a developer conducts the refactoring on the same code. We compare the refactorings produced by StarCoder2 and the ones by developers by analyzing the reduction of code smells, as well as the impact of different refactoring types on code quality improvement. We aim to investigate the following four research questions: RQ1: Can LLMs outperform developers in code refactoring? We aim to assess whether StarCoder2 can be used as a reliable solution to automate code refactoring. We compare the distribution of the refactoring operations performed by StarCoder2 and developers. We evaluate their effectiveness with the reduction of code smells and their improvement on code quality measured by various code metrics. We observe that StarCoder2 achieves a significantly higher performance in reducing code smells by 44.36%, compared to a 24.27% reduction rate for the developer. StarCoder2 excels in improving code quality measured by code metrics, often surpassing developers in these areas. RQ2: Which types of code smells are most effectively reduced by LLMs or developers? To measure the quality improvement capabilities of StarCoder2 in refactoring, we identify the types of code smells that can be effectively removed by StarCoder2 or developers. We observe that StarCoder2 outperforms developers to address systematic and repetitive issues, such as condensing a long statement or shortening a long parameter list. However, developers show superiority in handling complex, context-dependent code smells, such as correcting a broken modularization or deficient encapsulation. RQ3: Which refactoring types are most effective for improving code quality? To understand the capabilities of StarCoder2 in handling different types of refactorings, we compare the refactoring types that it performs to effectively eliminate code smells and improve code quality with those performed by developers. Our findings show that StarCoder2 is particularly effective in refactoring types that improve code within a class. Developers on the other hand excel in refactoring types that involve changes affecting multiple classes. RQ4: How does prompt engineering affect the quality of LLM-generated refactorings? To explore the impacts of prompt engineering to improve the capabilities of StarCoder2, we use chain-of-thought and one-shot prompting techniques. The results show that the one-shot prompting yields the highest unit test pass rate of 34.51%, marking an improvement of 6.15% over zero-shot prompting, and a smell reduction rate (SRR) of 42.97%, which is an increase over the zero-shot prompt by 3.52%. The chain-of-thought prompt, where we give the LLM refactoring type suggestions along with a definition, achieves a 32.22% unit test pass rate and a 42.34% SRR, which improves upon the numbers from the zero-shot prompting by 3.86% and 2.89% respectively. Our work makes the following main contributions: • We conduct a comprehensive evaluation of the capabilities of StarCoder2 in automated code refactoring. • We develop an evaluation framework for measuring the effectiveness of LLMs in code refactoring, with a focus on code quality improvement and functionality preservation. • We compare different prompting techniques, including chain-of-thought and one-shot prompting, to assess their impact on the generation of high-quality refactorings and offer practical guidance on their application. • We provide a replication package to enable the reproducibility of our study. The replication package of the study can be accessed at: https://github.com/Software-Evolution-Analytics-Lab-SEAL/LLM_Refactoring_Evaluation Paper Organization. The remainder of our study is organized as follows. Section 2 describes the experiment setup of this study. Section 3 presents the motivation, approaches, and results of our research questions. Section 4 discusses the threats to the validity of our findings. Section 5 surveys related studies and compares them to our work. Finally, we conclude our paper and present future research directions in Section 6."
https://arxiv.org/html/2411.02318v2,Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast,"Static verification is a powerful method for enhancing software quality, but it demands significant human labor and resources. This is particularly true of static verifiers that reason about heap manipulating programs using an ownership logic. LLMs have shown promise in a number of software engineering activities, including code generation, test generation, proof generation for theorem provers, and specification generation for static verifiers. However, prior work has not explored how well LLMs can perform specification generation for specifications based in an ownership logic, such as separation logic.To address this gap, this paper explores the effectiveness of large language models (LLMs), specifically OpenAI’s GPT models, in generating fully correct specifications based on separation logic for static verification of human-written programs in VeriFast. Our first experiment employed traditional prompt engineering and the second used Chain-of-Thought (CoT) Prompting to identify and address common errors generated across the GPT models. The results indicate that GPT models can successfully generate specifications for verifying heap manipulating code with VeriFast. Furthermore, while CoT prompting significantly reduces syntax errors generated by the GPT models, it does not greatly improve verification error rates compared to prompt engineering.","Auto-active (Hoare-logic styled (Hoare, 1969), static) verifiers, such as Viper (Müller et al., 2016), Verus (Lattuada et al., 2023), Dafny (Leino, 2010), Gillian (Fragoso Santos et al., 2020), and VeriFast (Jacobs et al., 2011), are powerful as they can prove the absence of large classes of bugs in code. Ideally, users of such tools need only specify the intended behavior of their code on the code itself (as pre- and postconditions), and the tool will automatically provide feedback on whether or not the code is provably correct with respect to this behavior. In reality, auto-active verifiers require many more auxiliary specifications (such as loop invariants, lemmas, folds, unfolds, etc.) to achieve this goal, burdening their users. In recent years, large language models (LLMs) have been effective in generating code (Chen et al., 2022; Sarsa et al., 2022), test-cases (Deng et al., 2023; Lemieux et al., 2023; Rao et al., 2023; Schäfer et al., 2023; Wang et al., 2024; Xia et al., 2024), and proofs in theorem provers (proof assistants) (Zheng et al., 2023; Yang et al., 2024; Jiang et al., 2021; Welleck and Saha, 2023; First et al., 2023). LLMs have also been shown to be effective for generating specifications supported by auto-active verifiers (Ma et al., 2024; Kamath et al., 2023; Misu et al., 2024; He et al., 2024; Mugnier et al., 2024). However, related work has not explored whether or not off-the-shelf LLMs can generate specifications based on a permissions logic, such as separation logic (Reynolds, 2002), that can be verified by auto-active verifiers such as VeriFast, Gillian, and Viper. Thanks to such specifications, these verifiers do well at verifying programs that manipulate the heap for both memory safety and functional properties. But, permissions logic based specifications (auxiliary and non-auxiliary) are particularly cumbersome to write, because they must specify the shape of the heap alongside functional constraints. This leads to specifications containing a number of predicates that hide heap details; and as a result, numerous lemmas, folds, unfolds, and special loop invariants that are used to connect the content of these predicates. While such specifications are difficult to reason about, they are written in a patterned way that may be amenable to generation via LLMs. Therefore, this paper evaluates how effective LLMs are at generating specifications that can be verified by VeriFast (Jacobs et al., 2011), which supports separation logic based verification of C and Java code. We specifically target OpenAI’s GPT models in this preliminary work, and employ two different prompt engineering techniques on the models. We develop input-output pairs to prompt the models with and use as ground truth for the models’ output, respectively. The input-output pairs are generated from a subset of 150 publicly available, statically verified examples on VeriFast’s Github. The GPT models’ output after prompting is inspected manually for correctness compared to the ground truth and results are recorded. Results indicate that GPT models can generate specifications for verification with VeriFast using traditional prompt engineering. When they fail to generate correct specifications, errors range from syntax to deeper verification errors. The second prompting approach based on Chain of Thought Prompting reduced syntax error rates significantly, but not verification error rates."
https://arxiv.org/html/2411.02278v1,Is This the Same Code? A Comprehensive Study of Decompilation Techniques for WebAssembly Binaries,"WebAssembly (abbreviated WASM) is a low-level bytecode language designed for client-side execution in web browsers. As WASM continues to gain widespread adoption and its security concerns, the need for decompilation techniques that recover high-level source code from WASM binaries has grown. However, little research has been done to assess the quality of decompiled code from WASM. This paper aims to fill this gap by conducting a comprehensive comparative analysis between decompiled C code from WASM binaries and state-of-the-art native binary decompilers. To achieve this goal, we presented a novel framework for empirically evaluating C-based decompilers from various aspects, thus assessing the proficiency of WASM decompilers in generating readable and correct code when compared to native binary decompilers. Specifically, we evaluated the decompiled code’s correctness, readability, and structural similarity with the original code from current WASM decompilers. We validated the proposed metrics’ practicality in decompiler assessment and provided insightful observations regarding the characteristics and constraints of existing decompiled code. By encouraging improvements in these tools, we seek to enhance their use in critical tasks such as auditing and sandboxing third-party libraries. This, in turn, contributes to bolstering the security and reliability of software systems that rely on WASM and native binaries.","WebAssembly (WASM) is a portable, low-level language designed for near-native execution on the web (Rossberg, 2023). Since it was first introduced by Haas et al. in 2017 (Haas et al., 2017) and initially developed for web browsers (Wagner, 2017), its application has extended to diverse areas, including Internet of Things (Gurdeep Singh and Scholliers, 2019; Liu et al., 2021), mobile devices (Pop et al., 2022), smart contracts (McCallum, 2019), and its dedicated runtime environments (Gohman, 2023; Akbary, 2023). Notably, WASM is commonly used as a compilation target for popular high-level languages like C, C++, and Rust (Hilbig et al., 2021). Despite its reputation for safety, WASM binaries still exhibit security vulnerabilities. Around 65% of WASM binaries suffer from classic stack-based exploits commonly found in native binaries (Hilbig et al., 2021), such as stack-based buffer overflows (McFadden et al., 2018; Bergbom, 2018) and stack overflows (Hilbig et al., 2021). The absence of standard memory protections, like stack canaries or guard pages, renders these vulnerabilities effective. Given the growing adoption of WASM, inspecting third-party binaries for potential security vulnerabilities has become imperative. However, the low-level nature of WASM bytecode makes it challenging to audit compared to high-level code, such as C. Additionally, 28.8% of WASM binaries are minified (Hilbig et al., 2021), obfuscating variable/function names and making manual inspection cumbersome. To address these challenges, security experts can leverage decompilers to analyze high-level code instead of grappling with thousands of lines of minified low-level WASM code. However, WASM decompilers have received less attention than decompilers designed for native binaries. Over the years, significant progress has been made in developing powerful native binary decompilers that can accurately generate decompiled code for C and C++ programs. Recent studies have also focused on enhancing the readability of decompiled code (Chen et al., 2010; Schulte et al., 2018a). To this end, we perform a comprehensive study to assess the effectiveness of state-of-the-art WASM decompilers. We will be approaching from two directions, as there are two different types of decompilers for WASM: decompilers tailored for readability, and decompilers focus on correctness, i.e., the decompiled code adheres to the behavior of the original WASM program. Their performance is compared with off-the-shelf native binary decompilers (NSA, 2023; Křoustek et al., 2017). To evaluate these decompilers, we utilize various widely-used complexity metrics for source code and adopt methodologies presented in previous studies (Schulte et al., 2018b; Yakdan et al., 2015; Liu and Wang, 2020). Our study focuses on the following three aspects: • Correctness of the decompiled code (Section 3.1); • Readability of the decompiled code (Section 3.2); • Structural similarity between the decompiled code and the original code (Section 3.3). With our research, we aim to draw attention to the capabilities of WASM decompilers and the performance of native binary decompilers. By encouraging improvements in these tools, we seek to enhance their use in essential tasks such as auditing and sandboxing third-party libraries (Narayan et al., 2020). This, in turn, contributes to bolstering the security and reliability of software systems that rely on WASM and native binaries. In summary, this paper makes the following contributions: • First attempt to evaluate WebAssembly decompilers: As far as we know, we are the first ones to investigate the correctness, readability, and structural similarity of decompilers for WebAssembly. We have created quantifiable and comprehensive metrics, which can be used as useful tools to evaluate the quality of decompiled code. The decompilers are tested on popular benchmarks, synthesized programs, and real-life scenarios to assess their adaptability to various inputs. • Inconsistencies of decompiling WASM vs. native binaries: Our investigation delved into the underlying reasons for inconsistencies arising when decompiling WASM vs. native binaries. These observations highlighted several critical issues, including aggressive compiler optimization, WASM language features, and platform-specific concerns. • First analysis framework for decompiled C code: We propose the first comprehensive analysis framework to empirically measure the quality of decompiled C code. We will open-source our benchmark and analysis framework, which can be used for future studies and further advancements in decompilers and WASM analysis. The rest of the paper is structured as follows: In Section 2, we present examples of the decompiled code from current WASM decompilers. In Section 3 and Section 4, we describe the metrics and mechanisms we used to evaluate the decompilers. The results of our evaluation are presented in Section 5. In Section 6, we discuss the limitations of our study and future work. Finally, we discuss related work in Section 7 and conclude the paper in Section 8."
https://arxiv.org/html/2411.02093v1,Do Advanced Language Models Eliminate the Need for Prompt Engineering in Software Engineering?,"Large Language Models (LLMs) have significantly advanced software engineering (SE) tasks, with prompt engineering techniques enhancing their performance in code-related areas. However, the rapid development of foundational LLMs such as the non-reasoning model GPT-4o and the reasoning model o1 raises questions about the continued effectiveness of these prompt engineering techniques. This paper presents an extensive empirical study that reevaluates various prompt engineering techniques within the context of these advanced LLMs. Focusing on three representative SE tasks, i.e., code generation, code translation, and code summarization, we assess whether prompt engineering techniques still yield improvements with advanced models, the actual effectiveness of reasoning models compared to non-reasoning models, and whether the benefits of using these advanced models justify their increased costs. Our findings reveal that prompt engineering techniques developed for earlier LLMs may provide diminished benefits or even hinder performance when applied to advanced models. In reasoning LLMs, the ability of sophisticated built-in reasoning reduces the impact of complex prompts, sometimes making simple zero-shot prompting more effective. Furthermore, while reasoning models outperform non-reasoning models in tasks requiring complex reasoning, they offer minimal advantages in tasks that do not need reasoning and may incur unnecessary costs. Based on our study, we provide practical guidance for practitioners on selecting appropriate prompt engineering techniques and foundational LLMs, considering factors such as task requirements, operational costs, and environmental impact. Our work contributes to a deeper understanding of effectively harnessing advanced LLMs in SE tasks, informing future research and application development.","Large Language Models (LLMs) (OpenAI, 2021; DeepSeek-AI, 2024; Anthropic, 2023; DeepMind, 2023; AI, 2023) have achieved remarkable results across various domains (Wu et al., 2024; Yan, 2024; Qin et al., 2023; Satpute et al., 2024; Chen et al., 2024), demonstrating human-like intelligence, especially in natural language processing (NLP) (Sun et al., 2023; Yi et al., 2024; Mo et al., 2024). This success has prompted a growing number of Software Engineering (SE) researchers to integrate LLMs into solving diverse SE tasks, yielding promising outcomes (Sun et al., 2024; Xia and Zhang, 2023; Chen et al., 2024; Liu et al., 2024; Dong et al., 2024; Kang et al., 2023, 2024). Despite these successes, significant challenges persist in effectively using LLMs for task completion (Satpute et al., 2024; Yu et al., 2024; Koo et al., 2023; Chen et al., 2024). Harnessing LLMs in SE to maximize their exceptional in-context learning and reasoning capabilities (Gao et al., 2023; Yao et al., 2022; Wei et al., 2022; Wang et al., 2023), relies on the prompts used, the information provided, and the specific ways in which models are invoked (Sun et al., 2024; Xu et al., 2023b; Zhong et al., 2024). In response, various strategies have emerged, often termed “prompt engineering techniques”, which aim to optimize LLM performance beyond simple model calls. These techniques111To avoid confusion, following previous work (Sun et al., 2024), the term “techniques” in this paper specifically refers to prompt engineering techniques, while specific approaches based on these techniques will be referred to as “approaches”. include few-shot prompting (Gao et al., 2023), Chain-of-Thought (CoT) prompting (Wei et al., 2022; Yao et al., 2022), critique prompting (Ma et al., 2024; Kim et al., 2024), expert prompting (Xu et al., 2023b), and so on. While these techniques have proven effective across different software development and maintenance tasks, such as code generation, code understanding, software testing, and debugging, challenges like hallucinations and inaccuracies remain (Zhong et al., 2024; Dong et al., 2024; Pan et al., 2024; Chen et al., 2024). To address these limitations, researchers have shifted focus to dynamic strategies (Dong et al., 2024; Huang et al., 2023; Pan et al., 2024; Yang et al., 2024) involving continuous interaction with the LLMs, task decomposition, and result verification. Building upon these concepts, recent approaches employ prompt engineering techniques such as multi-agent systems (Ma et al., 2024; Dong et al., 2024), iterative refinement processes (Ma et al., 2024; Yang et al., 2024; Zhong et al., 2024), and the integration of additional contextual information to refine the LLM’s output (Zhong et al., 2024; Yang et al., 2024). By leveraging these prompt engineering techniques, LLMs can deliver more reliable results in complex SE tasks, paving the way for further advancements in SE research. With the rapid advancements in large language model training (OpenAI, 2021; DeepSeek-AI, 2024; Anthropic, 2023; DeepMind, 2023; AI, 2023), foundational models are being iterated and updated at an accelerated pace (OpenAI, 2024b, c, d). More advanced foundational models demonstrate improved understanding and generating capabilities. When OpenAI released the GPT-4o model (OpenAI, 2024b), its performance outperformed that of most prompt engineering techniques developed for earlier foundational LLMs in coding tasks (pap, 2024). The subsequent o1, o1-mini models (OpenAI, 2024c, d) integrate CoT reasoning, allowing reasoning-based LLMs to autonomously decompose complex problems into a series of simpler steps, thereby forming effective strategies for tackling intricate logical issues. However, many prompt engineering techniques for code (Gao et al., 2023; Xu et al., 2023b; Dong et al., 2024) were developed based on the capabilities of the earlier model, ChatGPT-3.5 (OpenAI, 2021), as it was the only option available at the time. This overlooks the enhancements offered by the more advanced GPT-4o (OpenAI, 2024b) and the reasoning capabilities of the o1 and o1-mini models (OpenAI, 2024c, d). Moreover, OpenAI’s guidance indicates that using complex prompts is not recommended for reasoning LLMs (OpenAI, 2024c, d). Thus, this raises the first question about ① the effectiveness of these prompt engineering techniques on the more advanced models. Furthermore, while it is claimed that the reasoning LLMs, i.e., o1 and o1-mini, may provide enhanced performance, ② what is its actual effectiveness compared to non-reasoning models, and what are its respective advantages and disadvantages? Additionally, the reasoning LLMs typically incur higher operational costs, both in terms of monetary expenditure and time efficiency (OpenAI, 2024c; ope, 2024). In addition to computational and token-based costs, the varying levels of carbon emissions are a critical consideration. This raises the third question: ③ do the benefits of utilizing these advanced models justify their increased costs? To explore these questions, this paper presents the first extensive study aimed at revisiting and re-evaluating a variety of prompt engineering techniques within the context of the GPT-4o and o1-mini222Due to the high cost of the o1-preview (OpenAI, 2024c; ope, 2024), we utilize the o1-mini (OpenAI, 2024d) as a replacement. models. For Question ①, we investigate the effectiveness of these prompt engineering techniques on more advanced models, assessing whether they still yield significant improvements or if methodological adjustments are required for adaptation to new LLMs. For Question ②, we examine the actual performance of the reasoning LLM, compared to non-reasoning models, exploring its advantages and disadvantages, particularly in light of its capability to autonomously decompose complex problems and form reasoning strategies. For Question ③, we provide practical guidance to users on selecting appropriate prompt engineering techniques and foundational LLMs based on our findings, particularly considering whether the benefits of utilizing these advanced models justify their increased operational costs. To comprehensively assess the effectiveness of prompt engineering strategies within advanced LLMs, we deliberately select three representative code-related tasks (Chen et al., 2024; Liu et al., 2024) based on their prevalence and significance in the SE field: code generations (Zhong et al., 2024; Huang et al., 2023; Dong et al., 2024), code translation (Ahmad et al., 2023; Pan et al., 2024; Yang et al., 2024), and code summarization (Gao et al., 2023; Xu et al., 2023b; Sun et al., 2024; Ahmed et al., 2024; Wang et al., 2023; Kim et al., 2024). These tasks correspond to the common scenarios of text-to-code, code-to-code, and code-to-text in SE research (Lu et al., 2021), respectively, and they encompass a wide range of practical applications. To evaluate these approaches, we used three widely recognized datasets: HumanEval (Chen et al., 2021) for code generation, CodeTrans (Yang et al., 2024) for code translation, and CodeSearchNet (Husain et al., 2019) of CodeXGLUE (Lu et al., 2021) benchmark for code summarization. These datasets are standard benchmarks in the field and provide a solid foundation for comparative analysis. For each task, we identify and include several state-of-the-art approaches that utilize distinct prompt engineering techniques, ensuring a diverse and representative evaluation. Specifically, we select 11 approaches (Zhong et al., 2024; Huang et al., 2023; Dong et al., 2024; Ahmad et al., 2023; Pan et al., 2024; Yang et al., 2024; Gao et al., 2023; Xu et al., 2023b; Sun et al., 2024; Ahmed et al., 2024; Wang et al., 2023; Kim et al., 2024) that employ techniques such as few-shot prompting, CoT prompting, critique prompting, and multi-agent collaboration. We replace the underlying LLMs with the latest foundational models, including both non-reasoning and reasoning models (OpenAI, 2024b, d), to evaluate their performance across these SE tasks. This comprehensive assessment aims to provide new insights into the effectiveness of prompt engineering strategies within advanced LLMs. From this study, we uncover several notable findings, some of which are summarized as follows: (1) Effectiveness of Prompt Engineering Techniques: When applied to more advanced LLMs, the improvements achieved by prompt engineering techniques developed for earlier LLMs are often less pronounced than those reported in prior studies. In some cases, these techniques even introduce performance drawbacks. For the reasoning LLM, the LLM’s ability to self-correct through internal reasoning makes most prompt engineering techniques less effective than using a simple zero-shot prompt. The formulation of prompts itself is less critical than effectively leveraging accurate information and addressing errors based on reliable feedback, such as execution details in software testing. (2) The actual effectiveness of reasoning models compared to non-reasoning models: For tasks that need many steps of reasoning, the reasoning LLMs can achieve better performance than non-reasoning LLMs. For tasks that do not require complex reasoning, performance differences between reasoning models and non-reasoning models are minimal, even worse than prompt engineering techniques based on non-reasoning LLMs. The output format and content of reasoning LLMs are more flexible and longer, which may be unnecessary and hard to handle. (3) Practical guidance to users on utilizing these advanced models: Given the additional costs, time efficiency, and potential environmental impacts associated with reasoning models, non-reasoning models may be a more cost-effective option for tasks where advanced reasoning capabilities are not essential. When the expected output is not long, such as code summarization, it is recommended to use non-reasoning LLMs. Furthermore, when utilizing reasoning LLMs, the expected format and content should be restricted precisely. In summary, this paper makes the following contributions: • To the best of our knowledge, this study is the first to empirically evaluate a diverse range of prompt engineering techniques on more advanced LLMs within code-related tasks, specifically focusing on code generation, code translation, and code summarization. It includes an evaluation of the non-reasoning and reasoning LLMs. • We provide a detailed analysis of the performance of approaches leveraging various prompt engineering techniques. We find that some prompt engineering techniques will decrease and lose their effectiveness on advanced LLMs, and the reasoning LLMs can not always outperform non-reasoning LLMs. • Based on our findings, we offer insights and implications that can guide the adoption and future development of LLMs in SE tasks. We also provide practical guidance to users when selecting prompt engineering techniques and foundational LLMs, considering the monetary expenditure, time costs, and potential impact on the environment."
https://arxiv.org/html/2411.02091v1,"Fast Fixes and Faulty Drivers: An Empirical Analysis of Regression Bug
Fixing Times in the Linux Kernel","Regression bugs refer to situations in which something that worked previously no longer works currently. Such bugs have been pronounced in the Linux kernel. The paper focuses on regression bug tracking in the kernel by considering the time required to fix regression bugs. The dataset examined is based on the regzbot automation framework for tracking regressions in the Linux kernel. According to the results, (i) regression bug fixing times have been faster than previously reported; between 2021 and 2024, on average, it has taken less than a month to fix regression bugs. It is further evident that (ii) device drivers constitute the most prone subsystem for regression bugs, and also the fixing times vary across the kernel’s subsystems. Although (iii) most commits fixing regression bugs have been reviewed, tested, or both, the kernel’s code reviewing and manual testing practices do not explain the fixing times. Likewise, (iv) there is only a weak signal that code churn might contribute to explaining the fixing times statistically. Finally, (v) some subsystems exhibit strong effects for explaining the bug fixing times statistically, although overall statistical performance is modest but not atypical to the research domain. With these empirical results, the paper contributes to the efforts to better understand software regressions and their tracking in the Linux kernel.","A regression bug, or simply a regression, is a bug related to a feature that does not currently work, although it worked previously. It is possible to consider regressions at different levels of abstraction, including a fine-grained level of commits to a version control system. When operating at a higher level of software releases, a regression is a bug that exists in some version but does not exist in a previous version or a software’s whole previous version history. To mitigate regression bugs, a simple rule of thumb is often used: any regression bug fix should come with a test case that can later on verify that the bug will not reoccur in the future. In other words, regression “testing involves repetitive tests and aims to verify that previously working software still works after changes to other parts” (Engstrom10, , p. 10). In addition to testing, code reviews are often used too as a mitigative measure (Braz22, ). Nevertheless, test suites that can be automatically executed align well with so-called continuous software engineering practices, including continuous integration and continuous delivery in particular (Stolberg09, ). These practices, in turn, align with other current software development paradigms, such as continuous refactoring. Refactoring is important also in the highly complex context of operating system kernels for paying back technical and architectural debt (Spinellis21, ). However, it should be emphasized that refactoring can also cause regression bugs (Rachatasumrit12, ). Thus, automated tests are important also for improving refactoring. Refactoring is inevitably related also to the concept of code churn, which generally refers to rapid and recurrent commit-by-commit changes made to a version control system. Due to many new features constantly introduced, deprecating of old features, constant refactoring, bug fixes, and a large amount of developers, the Linux kernel is also notable for its extensive amount of code churn (Palix11, ; Ruohonen19RSDA, ). These characteristics of the Linux kernel development help to understand why also regression bugs have been very common. In fact, it has been reported that as much as a half of all bugs in the Linux kernel have been regressions, and new features or changes to existing features account for over half of the regression bugs (Xiao19, ). New features are also the most typical reason behind vulnerabilities introduced (JiangJiang24, ). The present paper continues this line of previous empirical research by focusing on regression bug tracking, including particularly the time required to fix regression bugs. The previous results noted justify the paper’s relevance; regression bugs continue to be a big problem in the Linux kernel and its development (Xiao19, ), and thus shedding more light on these bugs and their tracking constitute a relevant research topic. Furthermore, this topic has not been examined previously—at least to the extent presented in the current work. The Linux kernel is also an always excellent case study due do its importance, including both in the open source software world and the commercial software industry. By regression bug tracking, it is meant that fixing and keeping track of regression bugs require distinct software engineering activities. In particular, the starting point is a so-called bisection, a binary search, for finding a particular commit that introduced a given regression. After finding the commit, further search may be required to trace follow-up commits that possibly also contributed to the given regression bug. After these tasks, traditional so-called bug triaging follows; a developer responsible and available needs to be located, a given regression bug needs to be evaluated for its severity and impact, a bug fix needs to be developed, the fix needs to be preferably reviewed and tested, the fix needs to be integrated into a mainline branch and a future release, the given bug report needs to be closed in a bug tracking system, and so forth. In addition to these rather conventional software engineering tasks, in large-scale software projects, such as the Linux kernel, it is further important to systematically archive regression bugs for later analysis, including during fixing of further regression bugs. For instance, systematic tracking and archiving are important because sometimes fixing a regression bug introduces a new regression or even a vulnerability (JiangJiang24, ; Xiao20, ). A further point about the importance of tracking and archiving is that some regression bugs have security consequences (Braz22, ), which further enlarge the scope of triaging and coordination, and the need for traceability. Archiving provides also data for software analytics. Due to these and related reasons, regzbot, an automation solution for regression bug tracking in the Linux kernel was introduced (regzbot24a, ). The present paper analyzes data extracted from the automation bot, thus also demonstrating its value for software analytics and empirical software engineering research. It can be also remarked that regzbot provides a small counterargument to recently expressed criticism that bug detection and automation tools in general have still received limited use in the Linux kernel development (Suzuki24, ). Against this backdrop, it should be also emphasized that the Linux kernel development relies heavily on code reviews for improving software quality. While code reviews should supposedly reduce the amount of regression bugs, peer reviewing might also decrease regression bug fixing times because multiple people have already been involved. Once a new regression bug is reported, it should be easy to find also these other developers who are already familiar with the contributing faulty commit due to their reviewing activity. With these motivating points in mind, the present paper examines the following five research questions (RQs): • RQ.1: How long does it take to fix regression bugs in the Linux kernel? • RQ.2: Which kernel subsystems are prone to regression bugs, and do the bug fixing times vary across subsystems? • RQ.3: How many of the commits that fixed regressions were reviewed, tested, or both reviewed and tested, and do the associated regression bug fixing times vary in terms of reviewing, testing, or both? • RQ.4: Can code churn explain the bug fixing times? • RQ.5: How well the regression bug fixing times can be predicted, and what factors particularly explain the fixing times? The five research questions are further motivated in the opening Section 2 by better connecting them to existing research. Then, Section 3 elaborates the empirical data used and the methodology for analyzing it, including the software metrics derived and used for answering to RQ.5. Results are presented in the subsequent Section 4. The final Section 5 provides a concluding discussion."
https://arxiv.org/html/2411.01789v1,Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs,"Software testing remains the most widely used methodology for validating quality of code. However, effectiveness of testing critically depends on the quality of test suites used. Test cases in a test suite consist of two fundamental parts: (1) input values for the code under test, and (2) correct checks for the outputs it produces. These checks are commonly written as assertions, and termed test oracles. The last couple of decades have seen much progress in automated test input generation, e.g., using fuzzing and symbolic execution. However, automating test oracles remains a relatively less explored problem area. Indeed, a test oracle by its nature requires knowledge of expected behavior, which may only be known to the developer and may not not exist in a formal language that supports automated reasoning. Our focus in this paper is automation of test oracles for clients of widely used Java libraries, e.g., java.lang and java.util packages. Our key insight is that Javadocs that provide a rich source of information can enable automated generation of test oracles. Javadocs of the core Java libraries are fairly detailed documents that contain natural language descriptions of not only how the libraries behave but also how the clients must (not) use them. We use large language models as an enabling technology to embody our insight into a framework for test oracle automation, and evaluate it experimentally. Our experiments demonstrate that LLMs can generate oracles for checking normal and exceptional behaviors from Javadocs, with 98.8% of these oracles being compilable and 96.4% accurately reflecting intended properties. Even for the few incorrect oracles, errors are minor and can be easily corrected with the help of additional comment information generated by the LLMs.","Software testing is the most widely used methodology for validating code quality(Wang et al., 2024). However, effectiveness of testing critically depends on the quality of test suites used. A test suite consists of multiple test cases and each test case has two fundamental parts: (1) input values used to run the code under test, and (2) correctness checks run to validate the outputs it produces. These checks are commonly written as assertions, and serve as executable test oracles. In the last few decades, researchers have made much progress in automated test input generation, e.g., using fuzzing(Zhu et al., 2022; Xia et al., 2023) and symbolic execution(Baldoni et al., 2018; Khurshid et al., 2003; King, 1976). Generating test oracles is inherently challenging as it requires an understanding of the expected behavior, which often is known only to the developer and may not be documented in a formal language specification to automated reasoning. This problem occurs in different scenarios, including mobile applications, embedded systems, quantum systems, etc.(Meng et al., 2022; Jeon et al., 2002; Lewis et al., 2023). Besides, existing test oracle generation techniques typically focus on verifying alignment between implementations and their project-specific documentation(Liu and Nakajima, 2020; Dinella et al., 2022; Hossain and Dwyer, 2024). However, there remains a gap in research on verifying if implementations align with broader language contracts, such as JDK Javadocs. Figure 1. An overview of software testing workflow, the dashed box represents our work’s scope Software bugs, ranging from unexpected exceptions to incorrect outputs, frequently stem from mismatches between the intended behavior described in natural language and its actual implementation in code (Afshan et al., 2013; Harman et al., 2010). This problem has intensified with the rise of LLM-assisted programming, where developers use large language models (LLMs) to generate code from natural language prompts (Endres et al., 2024; Ouyang et al., 2023). In particular, when working with Javadoc, client developers might unintentionally create inconsistencies by overlooking or misinterpreting constraints specified in the documentation. Therefore, a testing approach is essential to identify discrepancies between Javadoc descriptions and the corresponding client code that either overrides standard Java classes or utilizes standard Java interfaces. Test oracles play a crucial role in identifying discrepancies by ensuring that software behavior aligns with its intended functionality (Leitner et al., 2007; Shin et al., 2024). Traditional specification mining techniques have largely focused on deriving oracles from structured code or constrained specifications (Tan et al., 2012), often neglecting the rich potential of natural language descriptions. In contrast, while neural generative models (Dinella et al., 2022) offer a more flexible approach, they face challenges in generating precise oracles due to the vast range of possible assertions. Furthermore, their effectiveness can vary significantly across different real-world Java projects, making it difficult to develop universally applicable test oracles (Hossain et al., 2023). Ideally, a technique should accurately generate oracles that capture the developer’s intent, maintain a low false positive rate, and be generalizable across projects to effectively test discrepancies between Javadoc and its client implementations. In this paper, we explore the potential of LLMs as intermediaries between natural language documentation and Java test oracles. Recently, LLMs have shown strong capabilities in tasks that require both natural language comprehension and code-related functionalities (Li et al., 2023b; Nam et al., 2024). Trained on extensive corpora, including large codebases and documentation, these models possess an implicit understanding of programming syntax and semantics. Research suggests that LLMs can effectively align user inputs with their pre-existing knowledge (Liu et al., 2024), and they even display emergent abilities to handle tasks uncommon in their training data (Schaeffer et al., 2024). In our context, the richness of JDK Javadocs allows us to derive general contracts directly from documentation, without needing code implementation. These contracts, when formatted as oracles, provide essential guidelines for client developers to follow. We introduce a novel methodology that utilizes LLMs to automate test oracle generation directly from JDK Javadocs, aiming to address subtle conventions that Java projects must conform to within JDK contracts. As illustrated in Figure 1, our approach integrates into the broader software testing process without requiring any actual code implementation. Unlike traditional methods that generate test oracles from each project’s documentation for use within that project, our approach leverages JDK Javadocs as the primary input. We focus on the JDK because it is foundational to all Java applications, making its conventions essential for Java projects, and its documentation is meticulously maintained to ensure quality (Tan and Croft, 2008; Lee et al., 2012). By extracting key properties from the JDK, our generated test oracles represent general contracts that all client Java projects should adhere to. Our experimental results highlight the effectiveness of LLMs in generating relevant and accurate test oracles. Specifically, our approach successfully produces 97% compilable test oracles without the need for additional compilation fix tools. Our technique also shows strong applicability and completeness: LLMs cover 90.3% of the properties outlined in JDK Javadocs, with 96.0% of the generated oracles accurately validating these properties. Additionally, we generate oracles for 98.9% of exceptions defined in JDK Javadocs, with 97.2% of these oracles effectively capturing the intended exceptions, demonstrating LLMs’ capability in creating comprehensive and precise exception oracles. With high-quality comments and clear naming conventions, the generated oracles are intuitive and straightforward, facilitating efficient testing. These promising results underscore the potential of LLMs in identifying misalignments between software specifications and their implementations. To summarize, this paper makes the following contributions: • Idea. We introduce the idea of test oracle automation using Javadocs for standard Java libraries in order to automate testing of the clients of these libraries. • Approach. We use large language models (LLMs) as an enabling technology to embody our insight into a technique for automating test oracles. We propose a prompt template which is flexible and allows generation of test oracles for a variety of JDK Javadocs. • Evaluation. We conducted an experimental evaluation using some of the most widely used JDK Java classes and interfaces from java.lang and java.util (e.g., Object, String, Map, Set, List) and present the resulting data. Our findings confirm that LLMs can generate highly effective and applicable test oracles, capable of accurately capturing expected behaviors and intended exceptions within the JDK. • Artifact. We release our prompt template and the complete set of generated test oracles for reproducibility. This allows developers to adapt our approach to their specific needs, ensuring that Java projects conform to JDK contract with minimal manual intervention."
https://arxiv.org/html/2411.01606v1,DesignRepair: Dual-Stream Design Guideline-Aware Frontend Repair with Large Language Models,"The rise of Large Language Models (LLMs) has streamlined frontend interface creation through tools like Vercel’s V0, yet surfaced challenges in design quality (e.g., accessibility, and usability). Current solutions, often limited by their focus, generalisability, or data dependency, fall short in addressing these complexities. Moreover, none of them examine the quality of LLM-generated UI design. In this work, we introduce DesignRepair, a novel dual-stream design guideline-aware system to examine and repair the UI design quality issues from both code aspect and rendered page aspect. We utilised the mature and popular Material Design as our knowledge base to guide this process. Specifically, we first constructed a comprehensive knowledge base encoding Google’s Material Design principles into low-level component knowledge base and high-level system design knowledge base. After that, DesignRepair employs a LLM for the extraction of key components and utilizes the Playwright tool for precise page analysis, aligning these with the established knowledge bases. Finally, we integrate Retrieval-Augmented Generation with state-of-the-art LLMs like GPT-4 to holistically refine and repair frontend code through a strategic divide and conquer approach. Our extensive evaluations validated the efficacy and utility of our approach, demonstrating significant enhancements in adherence to design guidelines, accessibility, and user experience metrics.","The advent of advanced large language models (LLMs), such as GPT-4 [1] and Meta’s Llama [2], has catalyzed transformative changes across various fields like legal [3, 4, 5, 6], finance [7], and software development [8], unlocking a plethora of new possibilities. For frontend software development, commercial and open-source tools, such as Vercel’s V0[9], Imagica[10] and OpenV0 [11], have attracted considerable interest using the capabilities of LLMs. These applications adeptly transform user-provided textual or visual prompts into concrete, well-structured high-fidelity user interfaces, along with their associated front-end code, providing substantial support to designers and developers. To augment the quality of the generated UI and its accompanying code, these tools commonly integrate with established UI design libraries (e.g., HTML, CSS, JS frameworks, and libraries), including popular ones like React [12], Shadcn [13] and Tailwind CSS [14]. These libraries serve as foundational knowledge bases, empowering these tools to generate aesthetically pleasing and technically sound designs. Despite their advanced capabilities, these tools continue to face two significant challenges. Firstly, while they are adept at converting prompts into well-structured UI elements, their compliance with established design principles, such as material design [15], often falls short (see Section IV). This inadequacy is critical, as stringent adherence to these principles is fundamental to crafting effective, accessible, and user-friendly interfaces. Such high-quality interfaces play a pivotal role in setting an application apart from its competitors, boosting user downloads, reducing user complaints, and enhancing user retention [16, 17, 18, 19, 20, 21]. Secondly, these tools exhibit limitations in the repair and refinement of generated designs. While they allow for additional prompts for UI adjustments, they often rely on users to manually identify and describe needed changes. This process can be cumbersome, requiring extensive labeling and in-depth knowledge [18, 19, 22]. Fig. 1(a) shows an example. When prompted with “URL shorten Chrome extension,” Vercel’s V0 successfully generated an appealing user interface. Nonetheless, upon closer examination, it exhibited five distinct design issues related to contrast and color in text and button elements, marked in red. (a) Initial UI Displaying Text and Button Color Contrast Issues, Highlighted in Red (b) Repaired UI using our method, showcasing resolved issues Figure 1: “URL Shortener” by Vercel’s V0: Initial Design vs. Improved Design. Many studies have been proposed to assess different facets of UI design development to ensure its quality, encompassing accessibility [23, 24, 25, 26, 27, 28], tapability [29], animation effects [18], and broader design principles and concerns [19, 30, 16]. Industrial tools like Playwright [23] and Google Lighthouse [24] primarily address accessibility concerns, such as text contrast for readability. Swearing et al.[29] utilized a classifier-based technique to identify discrepancies between the perceived and intended tappability of UI elements. Yang et al. [19] introduced UIs-Hunter, a rule-based method for detecting deviations from material design principles in UIs. Despite these advancements, they either tend to focus on specific, isolated issues within UI design, or they encounter challenges in generalizing to a broader spectrum of design aspects. Additionally, while some solutions [31, 21] have been proposed to rectify detected issues, they too tend to concentrate on limited aspects of design principles. The narrow scope of rule-based methods reveals that encoding every guideline for different platforms is labor-intensive and only effective in specific environments. To tackle the aforementioned challenges and fill the existing research gaps, we introduce DesignRepair, a novel knowledge-driven LLM-based approach specifically designed for enhancing the quality assurance and remediation of frontend UI and code. DesignRepair adopts Material Design 3 [15] as the standard for evaluating quality and as the foundation of our knowledge base. To guarantee efficient and effective knowledge retrieval, catering to varying levels of repair granularity and diverse design requirements, we meticulously analyzed and restructured Material Design 3 into two organized knowledge bases: the component knowledge base and the system design knowledge base. The component knowledge base focuses on the detailed, fine-grained design principles of individual components, whereas the system design knowledge base provides broader guidance on the overall design perspective, ensuring coherence in elements such as layout and color harmony. Given the frontend code and its corresponding rendered page, DesignRepair initially extracts essential information, such as components and property groups, followed by retrieving relevant knowledge from our knowledge bases. This approach, considering both the source code and the user-perceived experience, provides a holistic and detailed examination. Finally, DesignRepair adopts a divide and conquer approach to iteratively repair the code. The process progresses from focusing on individual elements to considering the overall design, ensuring a comprehensive and systematic approach to repair frontend code. We conducted an evaluation to measure the effectiveness and usefulness of DesignRepair. To carry this, we collected 196 design issues from AI-generated frontend code and 115 from GitHub projects consisting of UI design and its frontend code. Our evaluation shows that DesignRepair is able to identify and repair design issues, with a recall of 89.3% and precision of 86.6% for AI-generated issues and a recall of 85.2% and precision of 90.7% for GitHub issues. Additionally, our experiments on each step highlights the effectiveness of the strategies we implemented. Finally, a user study with 26 participants further confirms the high quality of our repairs. Our contributions are as follows: • We systematically analyzed Material Design guidelines and systematically converted them into structured knowledge bases, i.e., low level component knowledge base and high-level system design knowledge base, enabling efficient and effective utilization of knowledge in design quality assurance processes. • We introduced DesignRepair, a novel dual-stream knowledge-driven, LLM-based method, uniquely designed to systematically detect and repair design quality issues in frontend code. This approach simultaneously considers both the source code and the user-perceived rendered page, ensuring outstanding performance in design quality enhancement. • We conducted extensive experiments and a user study with 26 participants, which demonstrate the effectiveness and usefulness of our approach."
https://arxiv.org/html/2411.01414v1,A Deep Dive Into Large Language Model Code Generation Mistakes: What and Why?,"Recent advancements in Large Language Models (LLMs) have led to their widespread application in automated code generation. However, these models can still generate defective code that deviates from the specification. Previous research has mainly focused on the mistakes in LLM-generated standalone functions, overlooking real-world software development situations where the successful generation of the code requires software contexts such as external dependencies. In this paper, we considered both of these code generation situations and identified a range of non-syntactic mistakes arising from LLMs’ misunderstandings of coding question specifications. Seven categories of non-syntactic mistakes were identified through extensive manual analyses, four of which were missed by previous works. To better understand these mistakes, we proposed six reasons behind these mistakes from various perspectives. Moreover, we explored the effectiveness of LLMs in detecting mistakes and their reasons. Our evaluation demonstrated that GPT-4 with the ReAct prompting technique can achieve an F1 score of up to 0.65 when identifying reasons for LLM’s mistakes, such as misleading function signatures. We believe that these findings offer valuable insights into enhancing the quality of LLM-generated code.","Recent advancements in Large Language Models (LLMs) have garnered significant attention from both industry and academia, leading to a surge in LLM-based applications and research publications [76, 17, 72, 30, 5]. These models have been trained with large data corpora that contain natural language texts and source code, showing high competency in following human instructions in a wide range of downstream tasks [22, 25, 18]. Thus, developers have begun integrating LLMs into their daily coding tasks in various stages of software development [66, 8, 22, 25]. Code generation techniques aim to automate these coding tasks by using LLMs to generate source code given a natural language requirement (i.e., a coding question) [77, 52, 15, 43]. These techniques heavily rely on the learned knowledge and coding ability of the LLMs. While LLMs accumulated vast knowledge, research indicates that they can produce factually incorrect outputs, known as hallucinations [12, 45]. These hallucinations pose a significant threat to the reliability of LLMs, causing serious real-world consequences [44, 58, 9]. For example, an AI tool incorrectly suggested that it is legal for an employer to fire a worker for complaining about sexual harassment, not disclosing a pregnancy, or refusing to cut their dreadlocks. In the software industry, these misleading suggestions can potentially lead to major legal repercussions for the company [16]. Moreover, LLMs could recommend nonexistent third-party libraries to software developers, contributing to the distribution of malicious software [67]. Therefore, uncovering and understanding the mistakes in LLM responses and their root causes has become urgent. In recent years, the Software Engineering (SE) community has started investigating the mistakes in LLM-generated code and their causes. For example, Fan et al. [17] identified four categories of syntactic mistakes in LLM-generated code, while Song et al. [62] found seven categories of syntactic and non-syntactic mistakes. However, these studies only analyzed the mistakes in the standalone functions (i.e., functions that only require built-in/standard libraries) generated by the LLMs using datasets such as HumanEval [28], MBBP [51]. Only focusing on the analysis of the mistakes in such functions would miss those in the production code from real-world software development. Therefore, the findings have limited applicability to production code generation, which entails various types of software contexts (i.e., dependency information). Moreover, none of the existing works has systematically investigated the reasons behind the mistakes in LLM-generated code, which can be crucial for an in-depth understanding of these mistakes. In this study, we aim to identify and explain non-syntactic mistakes in LLM-generated code that arise from LLMs’ misunderstanding of the coding question. To the best of our knowledge, our work is the first to investigate LLMs’ non-syntactic mistakes and their underlying causes in both standalone function and production-level function generation. We focused on non-syntactic mistakes over syntactic ones to highlight potential flaws in the LLMs’ coding abilities and knowledge application, offering more valuable insights than previous works. Specifically, we first identified the non-syntactic mistakes from the LLM-generated code and then categorized them. Next, we systematically analyzed the reasons behind these mistakes. Additionally, we developed a benchmark of 216 coding questions that LLMs could not solve and experimented with various LLM-based approaches to automatically identify the mistakes and their reasons. Overall, our experiments aimed to address the following research questions: • RQ1: What mistakes do LLMs make in the code generation task? • RQ2: Can LLMs detect the mistakes in the generated code? • RQ3: What are the underlying reasons for the LLM’s mistakes in the code generation task? • RQ4: How effectively can LLMs identify the reasons for mistakes in the code generation task? Specifically, this paper makes the following contributions: 1. A derived list of LLMs’ mistakes in the code generation task. 2. A derived list of reasons behind LLMs’ mistakes in the code generation task. 3. An empirical investigation on LLM’s ability to critique its generated code. 4. An empirical investigation on LLM’s ability to identify the reasons for their mistakes in the code generation task. 5. A benchmark of 216 coding questions that LLMs could not solve correctly, along with the reasons."
https://arxiv.org/html/2411.01102v1,\binenhance: A Enhancement Framework Based on External Environment Semantics for Binary Code Search,"Binary code search plays a crucial role in applications like software reuse detection, and vulnerability identification. Currently, existing models are typically based on either internal code semantics or a combination of function call graphs (CG) and internal code semantics. However, these models have limitations. Internal code semantic models only consider the semantics within the function, ignoring the inter-function semantics, making it difficult to handle situations such as function inlining. The combination of CG and internal code semantics is insufficient for addressing complex real-world scenarios. To address these limitations, we propose \binenhance, a novel framework designed to leverage the inter-function semantics to enhance the expression of internal code semantics for binary code search. Specifically, \binenhance constructs an External Environment Semantic Graph (EESG), which establishes a stable and analogous external environment for homologous functions by using different inter-function semantic relations (e.g., call, location, data-co-use). After the construction of EESG, we utilize the embeddings generated by existing internal code semantic models to initialize nodes of EESG. Finally, we design a Semantic Enhancement Model (SEM) that uses Relational Graph Convolutional Networks (RGCNs) and a residual block to learn valuable external semantics on the EESG for generating the enhanced semantics embedding. In addition, \binenhance utilizes data feature similarity to refine the cosine similarity of semantic embeddings. We conduct experiments under six different tasks (e.g., under function inlining scenario) and the results illustrate the performance and robustness of \binenhance. The application of \binenhance to HermesSim, Asm2vec, TREX, Gemini, and Asteria on two public datasets results in an improvement of Mean Average Precision (MAP) from 53.6% to 69.7%. Moreover, the efficiency increases fourfold.","Software development often reuses open-source code to reduce costs. However, this trend inadvertently propagates vulnerabilities of open-source code into billions of software systems [2] [14], including industrial software and firmware. The extensive workload of code auditing and the complexity of recursive code reuse results in substantial delays in vulnerability patching, with some systems experiencing an average delay of 352 days [27]. In 2023, a review [20] conducted by Synopsys on 1703 software projects exposed a notable problem: 96% of these projects utilize open-source code, and 84% of them have at least one known vulnerability. In reaction to this widespread security risk, the emergence of binary code search has been proven to be a powerful method for automating the detection of insecure software components [43]. This facilitates the prompt patching of identified vulnerabilities. Binary code search entails the meticulous analysis of numerous binary codes to identify the most similar ones. Hence, its applications span a wide range, including software reuse detection [51, 21, 42], vulnerability search [43, 40, 46, 33], firmware security analysis [59, 63], and patch presence testing [57, 36, 31]. Developing a general and effective binary code search solution is extremely challenging. The syntactic structure of binary code can vary dramatically due to different compiler settings, such as optimization options. Additionally, binary codes with similar syntactic structures may have different semantics [30]. Therefore, a comprehensive understanding of the semantics of binary code is key to addressing this task. With the rapid development of deep learning technology, current solutions for binary code search are to convert binary code into embeddings and obtain the similarity of binary code by calculating the similarity between embeddings. Based on the definition of Section II-A, we classify them into two categories: internal code semantic models and external environment semantic models. Current solutions predominantly focus on learning the internal code features of functions. Gemini [56] and Asm2vec [26] employ neural networks to perform semantic encoding within program structures, specifically control flow graphs. Similarly, TREX [44], Palmtree [39], and others [53] [60] utilize Transformer [52] models for assembly code representation. Other studies, such as HermesSim [30], Asteria [59], VulHawk [43] and XLIR [28], utilize deep neural networks to convert intermediate representations, such as pseudocode and toy IR, into embeddings. Furthermore, recent solutions integrate function call relationships with internal code features. α𝛼\alphaitalic_αdiff [41] and BMM [29] utilize function call graph to aid in calculating similarity. Despite notable performance improvements in binary code search, these solutions have exhibited some limitations (example in Section II-B), particularly as the number of binary codes requiring comparison increases. Firstly, the internal code semantics of functions may exhibit substantial variations due to different compilation settings, encompassing factors like function inlining and splitting (identified as P1). Function inlining, an optimization strategy during compilation, introduces the actual codes of other functions called by the function at the corresponding address (as shown in Figure 2.(a)). Function splitting, conversely, divides a single function into multiple sub-functions. Study [32] demonstrates that compiler-caused function inlining can reach up to 70%, leading to a significant decrease in the performance of binary code search. Moreover, the selective inlining method [23] is lack of flexibility. This limitation underscores the inadequacy of relying solely on internal code semantics while overlooking the significance of external environment semantics. Secondly, exclusive reliance on function call graphs (CG) for assistance is insufficient for addressing complex real-world scenarios (P2). Relying solely on the external environment semantics of function calls introduces two primary issues that impact the effectiveness of binary code search: missing calls and similar calls. Missing calls occur when function call dependencies are absent, potentially eliminated during code reuse or optimization processes, such as strategies like function inlining. Similar calls, meanwhile, involve similar function call relationships between non-homologous functions, leading to imprecise similarity assessments and false positives. Thirdly, current solutions exhibit limited scalability and struggle to cope with large-scale function search tasks (P3). The most recent Transformer-based methods, such as TREX [44], typically generate function embeddings with a dimension of 768. However, real-world scenarios often involve millions of functions, necessitating rapid comparison speeds. Our experiments (in Section IV-F) revealed that in a pool of 10,000 functions, the time required to recall all the homologous functions of a binary file containing around 2500 functions is around 22 minutes using TREX’s 768-dimension embeddings. The time increases exponentially with the growth of the function pool and the binary file size. In contrast, 128-dimension embeddings only require 5 minutes, emphasizing the efficiency challenges in large-scale function search. Moreover, simply reducing the dimensionality for retraining usually leads to substantial performance loss [39]. Our central insight focuses on comprehensively utilizing external environment semantics (definition in Section II-A) to enhance the internal code semantic models. To achieve it, several challenges must be addressed. First, it is crucial to carefully select environment features that can serve external environment semantics for a function within a complex environment. These selected features should positively contribute to the performance of binary code search tasks and exhibit robustness. Second, the integration of various external environment features, each representing distinct structures and semantic spaces, presents a challenge. Determining an effective method for combining these diverse features to improve search tasks is crucial. Finally, an approach needs to be devised to appropriately apply the external environment semantics of the function, significantly enhancing its internal code semantics. Figure 1: Examples of Internal Code Feature Figure 2: A Motivation Example of external environment semantics. In this paper, we introduce \binenhance, a framework based on external environment semantics, specifically designed to tackle the aforementioned challenges. \binenhance differentiates itself from prior approaches by not solely relying on function call graph (P2). Instead, we have designed a novel External Environment Semantic Graph (EESG), which constructs four edges (C⁢D𝐶𝐷CDitalic_C italic_D, D⁢C⁢U𝐷𝐶𝑈DCUitalic_D italic_C italic_U, A⁢A𝐴𝐴AAitalic_A italic_A, and S⁢U𝑆𝑈SUitalic_S italic_U) among functions to model a function’s external environment since these semantic edges are robust among different scenarios when they coexist. For P3, we employ whitening transformations [49] to reduce the dimension of node initial embeddings generated by existing internal code semantics models [30] [59] [56] [44] [26] and MPNET [47], improving search task efficiency. To address P1, after initializing the nodes of EESG, \binenhance proposes a Semantic Enhancement Model (SEM), which introduces external environment semantics into internal code semantic embedding. SEM utilizes Relational Graph Convolutional Networks (RGCNs) [45] for updating external environment semantic embeddings in EESG and merges the dual embeddings of the function via a residual block to obtain an enhanced semantic embedding. Lastly, \binenhance includes a similarity combination module, leveraging data feature similarity to fine-tune semantic embedding similarity. In summary, we have made the following contributions: • Perspective. This paper presents a novel perspective on binary code search by incorporating external environment semantics into internal embeddings. To the best of our knowledge, it is also the first work to demonstrate that this integration can significantly improve the identification of homologous functions. Our approach enriches the semantics of internal embeddings and addresses the problem of false filtering of those approaches based on function calls. Additionally, it can be easily extended to other internal code semantic methods. Moreover, this paper demonstrates the promising potential of designing new external features to enhance internal representations, rather than solely focusing on developing new features for internal code. • Technique. We implement \binenhance111https://github.com/wang-yongpan/BinEnhance, a general framework that proposes four novel external semantic edges of our EESG to model stable external contexts in complex external environments and introduce an SEM to incorporate external semantics into internal embeddings to improve its representation ability. • Study. We conduct comprehensive experiments to evaluate the effectiveness of the \binenhance, including different function pool sizes, cross-architectures, and cross-optimization options, as well as scenarios such as function inlining and real-world vulnerability detection. Our results show that all existing binary code search methods overlook the importance of external environment semantics, leading to suboptimal performance."
https://arxiv.org/html/2411.01063v2,InterTrans: Leveraging Transitive Intermediate Translations to Enhance LLM-based Code Translation,"Code translation aims to convert a program from one programming language (PL) to another. This long-standing software engineering task is crucial for modernizing legacy systems, ensuring cross-platform compatibility, enhancing performance, and more. However, automating this process remains challenging due to many syntactic and semantic differences between PLs. Recent studies show that even advanced techniques such as large language models (LLMs), especially open-source LLMs, still struggle with the task.Currently, code LLMs are trained with source code from multiple programming languages, thus presenting multilingual capabilities. In this paper, we investigate whether such capabilities can be harnessed to enhance code translation. To achieve this goal, we introduce InterTrans, an LLM-based automated code translation approach that, in contrast to existing approaches, leverages intermediate translations to bridge the syntactic and semantic gaps between source and target PLs. InterTrans contains two stages. It first utilizes a novel Tree of Code Translation (ToCT) algorithm to plan transitive intermediate translation sequences between a given source and target PL, then validates them in a specific order. We evaluate InterTrans with three open LLMs on three benchmarks (i.e., CodeNet, HumanEval-X, and TransCoder) involving six PLs. Results show an absolute improvement of 18.3% to 43.3% in Computation Accuracy (CA) for InterTrans over Direct Translation with 10 attempts. The best-performing variant of InterTrans (with the Magicoder LLM) achieved an average CA of 87.3%-95.4% on three benchmarks.","Automatically translating source code between different programming languages (PLs) can significantly reduce the time and effort required for software development teams. In the literature, researchers have proposed various automated code translation methods. Data-driven learning-based approaches [roziere2021leveraging, szafraniec_code_2023] have shown impressive improvements over traditional rule-based methods [c2rust, cxgo, sharpen]. Unlike rule-based approaches, which rely on handcrafted rules and program analysis techniques, learning-based methods can automatically learn syntactic and semantic patterns from large-scale code repositories. Large language models (LLMs) represent the most advanced learning-based approaches developed in recent years and have demonstrated promising results across various software engineering tasks [fan2023large]. Pre-trained on vast amounts of code (across dozens of PLs) and text data, and equipped with billions of parameters, LLMs can be applied directly to code translation without the need for task-specific continuous training/fine-tuning. This would eliminate the need for costly and time-consuming processes involved in collecting training datasets and developing specialized models for code translation. However, recent studies have shown that the performance of LLM-based automated code translation, particularly with open-source LLMs, is still far from the production level, with correct translations ranging from 2.1% to 47.3% [pan_understanding_2023, yang2024exploring]. These studies found that many errors in LLM-generated code translations stem from the models’ lack of understanding of syntactic and semantic discrepancies between source and target languages, which can vary significantly across different pairs. For instance, 80% of the errors in translating from C++ to Go are due to syntactic and semantic differences, while only 23.1% of such errors occur when translating from C++ to C [pan_understanding_2023]. This variation is intuitive, as certain PLs naturally share more similarities in syntax and semantics than others. A similar phenomenon has been observed in machine translation for human languages, where translating between certain languages is easier than others [kolovratnik2009statistical]. To improve translations for challenging language pairs, a common strategy is to use parallel corpora with a pivot (bridge) language [kim2019pivot]. In fact, traditional statistical machine translation between non-English languages, such as French to German, often involves pivoting through English [wu2007pivot]. This approach remains effective with the rise of multilingual neural machine translation models. For instance, in a recent work by Meta [fan2021beyond], training language pairs were collected based on linguistic families and bridge languages, facilitating translation across numerous language pairs without exhaustively mining every possible pair. Inspired by this idea, this paper explores the potential of leveraging transitive intermediate translations from a source PL into other PLs before translating to the desired target PL, an idea not previously explored in the field of automated code translation. For example, to translate a program written in Python to Java, we might first translate it from Python to C++ and then from C++ to Java, as illustrated in Figure 1. This process is done through prompting, without additional training data, thanks to code LLMs that are pre-trained on text and code across multiple PLs and naturally possess multilingual capabilities. While this idea is inspired by machine translation, its potential in the inference stage of LLM-based translation approaches has not been explored. Despite the conceptual simplicity of the idea, a major challenge to address is the choice of the number and type of intermediate language(s), since the optimal choice might be different for each pair of PLs or even each pair of code snippets. The idea of utilizing existing PLs as “bridges” is different than earlier work, TransCoder-IR [szafraniec_code_2023], a non-LLM learning-based method that enhances source code pairs by incorporating their corresponding low-level, language-agnostic compiler Intermediate Representations (IR), such as LLVM IRs [lattner2004llvm], into the training dataset. Instead of relying on one unified IR to bridge any pair of cross-PL translations, we systematically explore different potential transitive intermediate translations using multiple existing PLs. InterTrans, our novel LLM-based code translation approach that enhances source-target translations via transitive intermediate translations, operates in two stages. In the first stage, a method called Tree of Code Translations (ToCT) generates a translation tree containing all potential translation paths for a specific source-target PL pair, conditioned to a set of pre-defined intermediate PLs and the maximum number of intermediate translations to be explored. In the second stage, translation paths are turned into LLM prompts that are executed in a breadth-first order. InterTrans then uses a readily available test suite to validate whether the generated translation to the target language is correct, enabling early termination of translation path exploration if a successful path is found before completely exploring the translation tree. To evaluate the effectiveness of InterTrans, we conducted experiments using three code LLMs (Code Llama [roziere2023code], Magicoder [wei_magicoder_2023], and StarCoder2 [lozhkov2024starcoder]) on 4,926 translation problems sourced from three datasets, i.e., CodeNet [puri2021codenet], HumanEval-X [zheng2023codegeex], and TransCoder [roziere2020unsupervised]. Each translation problem aims to translate a program writing in a source PL to a target PL. These problems involve 30 different source-target PL pairs across six languages: C++, JavaScript, Java, Python, Go, and Rust. Our results show that InterTrans consistently outperforms direct translation (i.e., without intermediate language translation) with 10 attempts, achieving an absolute Computational Accuracy (CA) improvement of 18.3% to 43.3% (median: 28.6%) across the three LLMs and datasets. Through ablation studies, we analyzed the effects of varying the number and selection of intermediate languages on InterTrans’s performance. Generally, increasing the number of intermediate translations enhances CA, though the benefits taper off after three translations. Similarly, incorporating more intermediate languages is advantageous, but gains slow after including three languages. The effectiveness of specific intermediate PLs varies across translation pairs, with notable patterns observed in translations from C++/Python to Java via Rust and from Rust to Go via C++. The main contributions of this paper are as follows: • We present the first study demonstrating that intermediate translations based on existing PLs can enhance the performance of LLM-based code translation. • We propose ToCT, a novel planning algorithm designed to explore intermediate translations effectively. We also introduce InterTrans, an LLM-based code translation approach that uses ToCT and is orthogonal to existing approaches for code translation. • We conducted a comprehensive empirical study to evaluate InterTrans. Our results highlight the effectiveness of InterTrans in enhancing LLM-based code translation. We also provide insights for the practical application of InterTrans. The code for implementing InterTrans, the datasets, and the notebooks for generating the experiment results are available at: https://github.com/RISElabQueens/InterTrans."
https://arxiv.org/html/2411.01012v1,PairSmell: A Novel Perspective InspectingSoftware Modular Structure,"Enhancing the modular structure of existing systems has attracted substantial research interest, focusing on two main methods: (1) software modularization and (2) identifying design issues (e.g., smells) as refactoring opportunities. However, re-modularization solutions often require extensive modifications to the original modules, and the design issues identified are generally too coarse to guide refactoring strategies. Combining the above two methods, this paper introduces a novel concept, PairSmell, which exploits modularization to pinpoint design issues necessitating refactoring. We concentrate on a granular but fundamental aspect of modularity principles—modular relation (MR), i.e., whether a pair of entities are separated or collocated. The main assumption is that, if the actual MR of a pair violates its ‘apt MR’, i.e., an MR agreed on by multiple modularization tools (as raters), it can be deemed likely a flawed architectural decision that necessitates further examination.To quantify and evaluate PairSmell, we conduct an empirical study on 20 C/C++ and Java projects, using 4 established modularization tools to identify two forms of PairSmell: inapt separated pairs 𝐼𝑛𝑆𝑒𝑝𝐼𝑛𝑆𝑒𝑝\mathit{InSep}italic_InSep and inapt collocated pairs 𝐼𝑛𝐶𝑜𝑙𝐼𝑛𝐶𝑜𝑙\mathit{InCol}italic_InCol. Our study on 260,003 instances reveals that their architectural impacts are substantial: (1) on average, 14.60% and 20.44% of software entities are involved in 𝐼𝑛𝑆𝑒𝑝𝐼𝑛𝑆𝑒𝑝\mathit{InSep}italic_InSep and 𝐼𝑛𝐶𝑜𝑙𝐼𝑛𝐶𝑜𝑙\mathit{InCol}italic_InCol MRs respectively; (2) 𝐼𝑛𝑆𝑒𝑝𝐼𝑛𝑆𝑒𝑝\mathit{InSep}italic_InSep pairs are associated with 190% more co-changes than properly separated pairs, while 𝐼𝑛𝐶𝑜𝑙𝐼𝑛𝐶𝑜𝑙\mathit{InCol}italic_InCol pairs are associated with 35% fewer co-changes than properly collocated pairs, both indicating a successful identification of modular structures detrimental to software quality; and (3) both forms of PairSmell persist across software evolution. This evidence strongly suggests that PairSmell can provide meaningful insights for inspecting modular structure, with the identified issues being both granular and fundamental, making the enhancement of modular design more efficient.","Software Modularity is an essential quality attribute reflecting how a system is structured into different parts (modules) [1]. This attribute has demonstrated a substantial impact on software reuse [2], and has been considered in various modern design scenarios, e.g., microservices-based systems [3] and LLM-enabled systems [4]. Although the debate over “what constitutes a single module” has sparked broad academic interests, determining appropriate modules is still challenging in practice. The reason is that modules can evolve quickly [5], due to updated business features and infrastructure technologies. Modules that worked well in the past might not fit into the current system. Thus, a large part of the community’s effort was spent on providing methodological support to improve the modularity of existing systems [6, 7, 5, 8], focusing primarily on two methods: On one hand, software modularization has been extensively investigated for nearly 30 years, with at least 143 papers published in the past decade [9]. Relevant work [7, 10, 11, 6], treats modularization as an optimization problem, and searches for a (near-)optimal modular solution to replace the original modules. Such solutions often ask for expensive changes to original systems, which might prevent developers from adopting them. For example, even with refactoring effort as the optimization objective, a solution may introduce up to 170 move class [12] operations to a system [5]. On the other hand, some studies focus on identifying issues in modular structure, e.g., based on quality metrics [13, 14, 15], anti-patterns or smells [16, 17, 18]. The issues are regarded as opportunities for refactoring in subsequent development, aiming at improving the degraded modules. The problem is that most of the issues are coarse at the module level, making it difficult for developers to determine refactoring strategies [19]. A typical example is Cycle Dependency [20], where the chain of relations among several modules breaks the desirable acyclic nature of modules’ dependency structure. Although we know that cycle dependencies should be broken, it is difficult to decide which dependencies to break [21]. (a) Dependency matrix of the system. “X” is a directional dependency between files. (b) Solutions from WCA (left) and FCA (right). “O” means two files collocated in the same module. (c) Consensus MRs.Blue: collocated; Red: separated. Figure 1: MRs agreed upon by multiple modularization tools are more reliable, as they comply with multiple rule sets. Our idea in this paper builds on both aforementioned methods. Rather than replacing the original modules, we propose that modularization tools can help identify issues that require refactoring. We focus on issues with a specific granularity: whether an entity (file) pair is collocated or separated within the same module, termed modular relation (MR). This characteristic is central to several fundamental modularity principles like Common Closure Principle [22] and Single Responsibility Principle [23]. For instance, the Common Closure Principle suggests grouping entities that often change together [22]. Moreover, entity pairs and their relationships are fundamental to many architecture analyses [24, 25, 26]. Our assumption is that if multiple modularization tools consensually design a MR as collocated or separated, it can be deemed a promisingly ‘apt MR’, due to the consideration of diverse viewpoints. This assumption, inspired by consensus clustering [27, 28] where similar cluster assignments indicate strong grouping between a pair of entities, reflects the consensus-based decision making in software development (e.g., [29, 30, 31]). On the contrary, if the MR of a pair violates the apt one, this violation indicates an inappropriate architectural decision [32], which we refer to as Pairwise Modular Smell (PairSmell). In a nutshell, PairSmell offers granular yet fundamental insights, helping developers inspect and improve software modules more effectively. It aims to identify issues necessitating refactoring based on multiple modularization tools, making development effort more targeted. In this paper, we introduce, quantify, and evaluate PairSmell as a novel type of issue for inspecting modular structure. To assess the severity of this issue, we conducted an empirical study involving 20 C/C++ and Java projects from GitHub. To support this study, we developed a tool, integrating 4 established modularization tools, to automatically detect PairSmell from the modular structure of a development architectural view. We mined 22,528 code commits across 473 diverse snapshots, and inspected 146,668,710 separated and 3,866,940 collocated pairs of entities. Based on the dataset, our study identified 260,003 PairSmells, including 73,536 inapt separated pairs (a.k.a., 𝐼𝑛𝑆𝑒𝑝𝐼𝑛𝑆𝑒𝑝\mathit{InSep}italic_InSep) and 186,467 inapt collocated pairs (𝐼𝑛𝐶𝑜𝑙𝐼𝑛𝐶𝑜𝑙\mathit{InCol}italic_InCol). The empirical results reveal that: (1) PairSmell is prevalent among projects, with 𝐼𝑛𝑆𝑒𝑝𝐼𝑛𝑆𝑒𝑝\mathit{InSep}italic_InSep and 𝐼𝑛𝐶𝑜𝑙𝐼𝑛𝐶𝑜𝑙\mathit{InCol}italic_InCol instances covering 14.60% and 20.44% entities on average; (2) on average, entities in 𝐼𝑛𝑆𝑒𝑝𝐼𝑛𝑆𝑒𝑝\mathit{InSep}italic_InSep MRs co-change 190% more than in other separated pairs, and entities in 𝐼𝑛𝐶𝑜𝑙𝐼𝑛𝐶𝑜𝑙\mathit{InCol}italic_InCol MRs co-change 35% less than other collocated pairs, dramatically deviating from well-structured modules; (3) PairSmell persists in software projects if left unaddressed, where the percentages of 𝐼𝑛𝑆𝑒𝑝𝐼𝑛𝑆𝑒𝑝\mathit{InSep}italic_InSep and 𝐼𝑛𝐶𝑜𝑙𝐼𝑛𝐶𝑜𝑙\mathit{InCol}italic_InCol pairs remain stable as systems grow. In summary, our study makes the following contributions: 1. A novel type of architectural smell and its identification approach are proposed, enabling the revelation of granular yet fundamental modular issues. 2. An empirical study on the architectural smell is present, revealing that such smells are prevalent but detrimental to software maintenance and change, and could persist for long if left unaddressed. 3. The novel smell is discussed, and its implications for practice and future research are illustrated. 4. The benchmarks and replication package collected from 20 open source projects are publicly available [33] for continued research of the novel type of smell."
https://arxiv.org/html/2411.00865v1,DemoCraft: Using In-Context Learning to Improve Code Generation in Large Language Models,"Generating executable code from natural language instructions using Large Language Models (LLMs) poses challenges such as semantic ambiguity and understanding task-specific contexts. To address these issues, we propose a system called DemoCraft, which enhances code generation by leveraging in-context learning and demonstration selection, combined with latent concept learning. Latent concept learning introduces additional concept tokens, which are trainable embeddings that capture task-specific knowledge. We then test our system on two major datasets: MBPP and Humaneval. Our experimental results demonstrate that the proposed system achieves an approximate 2x increase in the pass@k metric compared to baseline models. Furthermore, we introduce two novel evaluation metrics: correctness@k and similarity@k. Our empirical studies indicate that our system attains nearly a 3x improvement in these metrics as well.","The problem of generating code from natural language using Large Language Models (LLMs) involves creating systems capable of translating human language instructions into executable code accurately. This requires the LLM to understand the semantics of the natural language input, grasp the intent behind the instructions, and convert it into syntactically correct and functional code in a specified programming language. Key challenges include handling ambiguous or imprecise language, ensuring the generated code is both correct and efficient, and covering a wide range of programming scenarios and languages. Figure 1: Large Language Models struggling at Code Generation Code generation remains a significant challenge for large language models, as evidenced by Google’s AlphaCode[1], developed specifically for competitive programming tasks. When evaluated on the CodeContests benchmark, AlphaCode achieves a maximum Codeforces rating of only 1238, placing it in approximately the top 28th percentile. Furthermore, a comprehensive survey on code generation using large language models [2] reports a maximum pass@1 rate of around 30%. These studies have been conducted under zero-shot conditions, highlighting the necessity for few-shot learning approaches. Few-shot learning allows models to leverage relevant demonstrations associated with the prompt prior to generating the output, potentially improving performance."
https://arxiv.org/html/2411.01604v1,Large Language Model Supply Chain: Open Problems From the Security Perspective,"Large Language Model (LLM) is changing the software development paradigm and has gained huge attention from both academia and industry. Researchers and developers collaboratively explore how to leverage the powerful problem-solving ability of LLMs for specific domain tasks. Due to the wide usage of LLM-based applications, e.g., ChatGPT, multiple works have been proposed to ensure the security of LLM systems. However, a comprehensive understanding of the entire processes of LLM system construction (the LLM supply chain) is crucial but relevant works are limited. More importantly, the security issues hidden in the LLM SC which could highly impact the reliable usage of LLMs are lack of exploration. Existing works mainly focus on assuring the quality of LLM from the model level, security assurance for the entire LLM SC is ignored. In this work, we take the first step to discuss the potential security risks in each component as well as the integration between components of LLM SC. We summarize 12 security-related risks and provide promising guidance to help build safer LLM systems. We hope our work can facilitate the evolution of artificial general intelligence with secure LLM ecosystems.","Large Language Models (LLMs) lead the new era of artificial intelligence (AI). With this success, LLM-driven applications have achieved exciting and even human-better results in multiple domains, such as video generation [1], mathematical competition [2], code generation [3], and autonomous driving [4]. Recently, many big-tech companies have been trying to develop their own LLMs and construct relevant systems for specific tasks and products, pushing researchers and developers to explore more reliable LLM system construction roads. Similar to conventional software systems, the construction of LLM systems consists of multiple components and participators, e.g., data providers and model developers. The integration of those complex components is called supply chain (SC) from the perspective of software engineering where software (LLM) serves as the core surrounding numerous upstream and downstream participants. More importantly, as LLMs are widely used nowadays, and sometimes in safety and security-critical situations such as autonomous driving systems, ensuring the reliability of the whole LLM system becomes important. While the supply chain of conventional software systems has been extensively studied over the past decade, research specifically focused on the supply chain of AI systems, particularly LLMs, remains limited. Although recent studies explore security-related issues in LLMs, most of them remain focused on the model itself. We analyzed 59 relevant papers from recent surveys [5, 6, 7, 8], and found that 19 of these still concentrate on model-level security, while 24 are exclusively focused on ChatGPT, largely overlooking the broader supply chain. For instance, researchers have shown that adversarial attack techniques can be used to exploit LLMs [9], and LLMs can be easily jailbroken [10]. However, the model is just one component of an LLM system. Even if the model security is ensured, vulnerabilities in other parts of the LLM supply chain, such as third-party dependencies or deployment environments, can still pose significant security risks and lead to an unreliable LLM system. Therefore, there is a critical need for research on LLM supply chain security. While the recent study [5] explores components of the LLM supply chain, including infrastructure, model lifecycle, and the application ecosystem, its focus remains primarily on the model lifecycle, considering each component separately. The dependencies (i.e., upstream and downstream components) within the LLM supply chain and the associated security risks along the supply chain are still unclear. Figure 1: Security risks in LLM supply chain. To fill this gap, we take the first step in analyzing potential security risks in the LLM supply chain and propose guidelines to mitigate these risks. Specifically, we begin by defining each component of the LLM supply chain, analyzing dependencies of those components, e.g., from upstream data providers to downstream LLM applications (end users). We then focus on identifying security risks that originate from upstream components, which attackers could exploit to impact downstream participants. For instance, attackers may inject poisoned data into training datasets (upstream), affecting the trained LLMs and, consequently, the deployed LLM-based applications (downstream). In total, we identify 12 security risks related to the LLM supply chain and offer guidance to support the construction of secure LLM systems. We believe that our work can help researchers and developers better understand LLM supply chain security and build more reliable LLM systems. To summarize, the main contributions of this paper are: • We are the first to explore security risks considering the integration of dependent components in the LLM SC where we summarize 12 relevant LLM SC risks. • We provide promising guidelines to mitigate the risks in developing LLM systems."
https://arxiv.org/html/2411.00642v1,LLM-Based Misconfiguration Detection for AWS Serverless Computing,"Serverless computing is an emerging cloud computing paradigm that enables developers to build applications at the function level, known as serverless applications. Amazon Web Services (AWS), the leading provider in this domain, provides the Serverless Application Model (AWS SAM), the most widely adopted configuration schema for configuring and managing serverless applications through a specified file. However, misconfigurations pose a significant challenge in serverless development. Traditional data-driven techniques, which learn configuration patterns from historical data to identify anomalies, may struggle with serverless applications because the complexity of serverless configurations hinders pattern recognition, and it is challenging to gather complete datasets that cover all possible configurations. Recent advancements in Large Language Models (LLMs) have shown promise in tackling various software engineering tasks. Leveraging vast amounts of publicly available data during pre-training, LLMs can have the potential to assist in identifying and explaining misconfigurations in serverless applications. In this paper, we introduce SlsDetector, the first framework leveraging LLMs to detect misconfigurations in serverless applications. SlsDetector utilizes effective prompt engineering with zero-shot learning to identify configuration issues. It designs multi-dimensional constraints specifically tailored to the configuration characteristics of serverless applications and leverages the Chain of Thought technique to enhance LLMs inferences, alongside generating customized structured responses. We evaluate SlsDetector on a curated dataset of 110 configuration files, which includes correct configurations, real-world misconfigurations, and intentionally injected errors. Our results show that SlsDetector, based on ChatGPT-4o (one of the most representative LLMs), achieves a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%, outperforming state-of-the-art data-driven approaches by 53.82, 17.40, and 49.72 percentage points, respectively. Furthermore, we investigate the generalization capability of SlsDetector by applying recent LLMs, including Llama 3.1 (405B) Instruct Turbo and Gemini 1.5 Pro, with results showing consistently high effectiveness across these models.","Serverless computing is an emerging cloud computing paradigm that allows developers to build and run applications, known as serverless applications, without managing underlying infrastructure tasks (Wen et al., 2023). It has been widely adopted across diverse application domains (Yu et al., 2021; Shankar et al., 2020; Ao et al., 2018), attracting growing interest from research communities, such as Software Engineering (SE) (Wen et al., 2023) and Systems (Li et al., 2022), and from industry. To support the development and execution of serverless applications, leading cloud providers have introduced serverless platforms. Among these providers, Amazon Web Services (AWS) stands out as the leader in serverless computing (Wen et al., 2023; Jonas et al., 2019; Wen et al., 2021). Serverless computing supports two primary service models: Function-as-a-Service (FaaS) and Backend-as-a-Service (BaaS) (Hassan et al., 2021; Eismann et al., 2021). FaaS allows developers to build applications as small, event-driven functions (i.e., serverless functions), while BaaS provides ready-to-use cloud services such as storage (e.g., AWS S3 (S3, 2024)), database, and API gateway management. This collaboration between FaaS and BaaS enables developers to efficiently create serverless applications. To configure and manage functions and required cloud resources for serverless applications, AWS provides the Serverless Application Model (AWS SAM) (sam, 2024), the most widely adopted configuration schema in the serverless computing practice (pop, 2024; exa, 2024; SAR, 2024). It can streamline the development process and reduce complexities associated with resource management in serverless applications. However, misconfigurations have emerged as a major challenge in serverless application development (Sun et al., 2020; Li et al., 2018; Wen et al., 2021). Misconfigurations in serverless computing can lead to significant security vulnerabilities and operational issues. For example, as reported (rea, 2024b), a coronavirus testing company exposed over 50,000 patients’ scanned IDs and thousands of COVID-19 test results due to a misconfiguration in an AWS S3 bucket, used in conjunction with serverless applications. Similarly, another company experienced a data breach affecting 4.9 million customers due to API misconfigurations within serverless environments (rea, 2024a). These examples underscore that misconfigurations are not isolated incidents but represent systemic issues that pose significant risks to serverless applications, suggesting the urgent need for effective misconfiguration detection for serverless computing. Misconfigurations have become one of the major causes of system software failures (Xu and Zhou, 2015). Despite the promise of existing data-driven methods for misconfiguration detection in other scenarios (Zhang et al., 2014; Santolucito et al., 2017; Zhou et al., 2023; Santolucito et al., 2016), they have low effectiveness to serverless computing. Data-driven approaches, which rely on anomaly detection or pattern recognition based on training data, suffer from limitations such as incomplete or incorrect datasets (Zhang et al., 2014; Zhou et al., 2023). Additional strategies that incorporate extensive knowledge, such as predefined templates and official documentation, lack flexibility and adaptability. These problems make the data-driven approach not enough to detect configuration problems of serverless applications. Moreover, serverless application configurations involve intricate structures, including domain-specific languages, complex dependency relationships, and nested objects across over 800 cloud resource types, which further complicates their detection. Recent advancements in Large Language Models (LLMs) offer a promising new approach to this problem. LLMs have demonstrated significant success in various SE tasks, such as code summarization (Ahmed and Devanbu, 2022), program repair (Fan et al., 2023), unit test generation (Yuan et al., 2024), and log parsing (Xu et al., 2024). Trained on vast amounts of publicly available data, LLMs can potentially understand and recognize configuration patterns, best practices, and common pitfalls in serverless application configurations. This makes LLMs well-suited for detecting potential misconfigurations in serverless applications. In this paper, we introduce SlsDetector, the first LLM-based framework specifically designed to detect misconfigurations in serverless applications. By leveraging advanced prompt engineering in conjunction with zero-shot learning, which requires no prior examples, SlsDetector efficiently identifies configuration problems with minimal effort. This marks a significant advancement in detecting misconfigurations within serverless environments. SlsDetector accepts a configuration file of the serverless application as input and outputs detected misconfigurations along with detailed explanations for each issue. To achieve this, SlsDetector features a prompt generation component that dynamically integrates the configuration file, task description, multi-dimensional constraints, and customized responses. Multi-dimensional constraints are tailored to the specific characteristics of serverless applications, taking into account resource types, configuration entries, values, and different levels of dependencies. This context-aware design provides targeted guidance for detection. Furthermore, SlsDetector employs the Chain of Thought technique (CoT, 2024; Chu et al., 2023), a reasoning strategy that enhances the problem-solving process, into these constraints. The customized response provides the content demand and format demand of LLM outputs, ensuring that responses are not only structured but also actionable answers aligned with detailed explanations. To evaluate SlsDetector, we curate a dataset of 110 configuration files, including 26 correctly configured files, 58 with real-world misconfigurations, and 26 with injected errors. Our results show that SlsDetector, based on ChatGPT-4o (one of the most representative LLMs known for outstanding performance), achieves a precision of 72.88%, recall of 88.18%, and F1-score of 79.75%. It outperforms the state-of-the-art data-driven approach by 53.82, 17.40, and 49.72 percentage points in precision, recall, and F1-score, respectively. We further explore the generalization capability of SlsDetector on other representative LLMs, including Llama 3.1 (405B) Instruct Turbo and Gemini 1.5 Pro. The results show that SlsDetector consistently achieves high effectiveness across these models. In summary, this paper makes the following contributions: • We present SlsDetector, the first LLM-based approach specifically designed for detecting misconfigurations in serverless computing. • We conduct an empirical study using our benchmark dataset to evaluate the effectiveness of our misconfiguration detection approach, demonstrating that it outperforms baseline methods."
https://arxiv.org/html/2411.00636v1,The Name of the Title Is Hope,"A clear and well-documented LaTeX document is presented as an article formatted for publication by ACM in a conference proceedings or journal publication. Based on the “acmart” document class, this article presents and explains many of the common variations, as well as many of the formatting elements an author may use in the preparation of the documentation of their work.","ACM’s consolidated article template, introduced in 2017, provides a consistent LaTeX style for use across ACM publications, and incorporates accessibility and metadata-extraction functionality necessary for future Digital Library endeavors. Numerous ACM and SIG-specific LaTeX templates have been examined, and their unique features incorporated into this single new template. If you are new to publishing with ACM, this document is a valuable guide to the process of preparing your work for publication. If you have published with ACM before, this document provides insight and instruction into more recent changes to the article template. The “acmart” document class can be used to prepare articles for any ACM publication — conference or journal, and for any stage of publication, from review to final “camera-ready” copy, to the author’s own version, with very few changes to the source."
https://arxiv.org/html/2411.00634v1,Does GenAI Make Usability Testing Obsolete?,"Ensuring usability is crucial for the success of mobile apps. Usability issues can compromise user experience and negatively impact the perceived app quality. This paper presents UX-LLM, a novel tool powered by a Large Vision-Language Model that predicts usability issues in iOS apps. To evaluate the performance of UX-LLM we predicted usability issues in two open-source apps of a medium complexity and asked usability experts to assess the predictions. We also performed traditional usability testing and expert review for both apps and compared the results to those of UX-LLM. UX-LLM demonstrated precision ranging from 0.61 and 0.66 and recall between 0.35 and 0.38, indicating its ability to identify valid usability issues, yet failing to capture the majority of issues. Finally, we conducted a focus group with an app development team of a capstone project developing a transit app for visually impaired persons. The focus group expressed positive perceptions of UX-LLM as it identified unknown usability issues in their app. However, they also raised concerns about its integration into the development workflow, suggesting potential improvements. Our results show that UX-LLM cannot fully replace traditional usability evaluation methods but serves as a valuable supplement particularly for small teams with limited resources, to identify issues in less common user paths, due to its ability to inspect the source code.","With the rapid growth of the app market over the last decade, developing “good” apps has become crucial to vendor success [1, 2]. Studies have shown that users tend to favour apps that perform as expected and are easily understandable [3, 4]. Software usability is a key factor that crucially influences how users perceive the quality of an app [2, 5]. Usability can be broadly defined as “a concept that essentially refers to how easy it is for users to learn a system, how efficient they can be once they have learned it, and how enjoyable it is to use it” [2, 6]. Usability issues are problems that compromise the usability of an app, hindering a positive user experience [7]. It is thus crucial for developers to thoroughly detect and address usability issues for improving their apps [2, 5]. There are different ways to systematically identify usability issues. Conventional usability evaluation methods include usability testing in labs with users as well as theoretical analyses with experts [8, 9]. Once usability issues are identified, developers can address them and offer a refined app to their users [10]. However, it can be challenging, especially for small app development teams, to channel the resources and expertise needed for implementing an effective usability evaluation [11]. Generative Artificial Intelligence (GenAI) is a maturing technology that is increasingly getting attention for the purpose of automating various tasks in different domains, as it is capable of generating meaningful texts, images, and videos [12, 13]. Particularly, Foundational Models such as Large Language Models (LLMs) process a user textual prompt and construct responses by predicting next tokens in a partially formed context [14, 15]. Recently Foundation Models gained considerable interest in the software engineering domain, as they can be employed for a variety of purposes, including generating code and documentation, or fixing bugs [16, 17, 18]. This work investigates the extent to which GenAI can support or even automate usability evaluation for mobile apps. We introduce UX-LLM, a novel open-source tool that uses Foundation Models to detect usability issues in the individual views of native iOS apps. As input, UX-LLM requires a brief description of the app context, the source code, and an image of the analysed view. Section II introduces the implementation details of UX-LLM and the underlying prompt engineering. This constitutes our first contribution. To evaluate UX-LLM performance and how it compares with conventional usability evaluation methods, we conducted a multi-method study consisting of expert assessments, expert reviews, and usability testing for two open-source iOS apps: a Quiz and a To-Do app. To understand how development teams perceive the support of such tools and explore possible concerns, we conduced a focus group within an app development project. Section III presents the design of our evaluation study. The encouraging results confirm that LLM-based approaches are able to identify valid app usability issues. The results also indicate that GenAI approaches do not fully replace traditional methods but rather complement them—highlighting the potential as a supportive tool that can enhance usability evaluations during the development process. Section IV reports on the results of our evaluation study, which constitutes together with the data [19] our second contribution. The remainder of the paper discusses the work limitations and the threats to validity in Section VI, related work in Section V, and summarises the findings with their implication in Section VII."
https://arxiv.org/html/2411.00622v1,Lingma SWE-GPT :An Open Development-Process-Centric Language Model for Automated Software Improvement,"Large language models (LLMs) have demonstrated remarkable performance in code generation, significantly enhancing the coding efficiency of developers. Recent advancements in LLM-based agents have led to significant progress in end-to-end automatic software engineering (ASE), particularly in software maintenance (e.g., fixing software issues) and evolution (e.g., adding new features). Despite these encouraging advances, current research faces two major challenges. First, state-of-the-art performance primarily depends on closed-source models like GPT-4, which significantly limits the technology’s accessibility, and potential for customization in diverse software engineering tasks. This dependence also raises concerns about data privacy, particularly when handling sensitive codebases. Second, these models are predominantly trained on static code data, lacking a deep understanding of the dynamic interactions, iterative problem-solving processes, and evolutionary characteristics inherent in software development. Consequently, they may face challenges in navigating complex project structures and generating contextually relevant solutions, which can affect their practical utility in real-world scenarios.To address these challenges, our study adopts a software engineering perspective. We recognize that real-world software maintenance and evolution processes encompass not only static code data but also developers’ thought processes, utilization of external tools, and the interaction between different functional personnel. Our objective is to develop an open-source large language model specifically optimized for software improvement, aiming to match the performance of closed-source alternatives while offering greater accessibility and customization potential. Consequently, we introduce the Lingma SWE-GPT series, comprising Lingma SWE-GPT 7B and Lingma SWE-GPT 72B. By learning from and simulating real-world code submission activities, Lingma SWE-GPT systematically incorporates the dynamic interactions and iterative problem-solving inherent in software development process—such as repository understanding, fault localization, and patch generation—thereby achieving a more comprehensive understanding of software improvement processes. We conducted experimental evaluations using SWE-bench-Verified benchmark (comprising 500 real GitHub issues), recently proposed by OpenAI. The results demonstrate that Lingma SWE-GPT 72B successfully resolves 30.20% of the GitHub issues, marking a significant improvement in automatic issue resolution (22.76% relative improvement compared to Llama 3.1 405B), approaching the performance of closed-source models (31.80% issues of GPT-4o resolved). Notably, Lingma SWE-GPT 7B resolves 18.20% of the issues, surpassing the 17.20% resolution rate of Llama 3.1 70B, highlighting the potential for applying smaller models to ASE tasks.","Automated software engineering (ASE) has long been a vision pursued by both the software engineering (SE) and artificial intelligence (AI) communities. Recent advancements in large language models (LLMs) have shown significant potential in advancing this field. Initially, the HumanEval benchmark (Chen et al., 2021) was developed to assess LLMs’ capabilities in function-level code generation. Both closed-source (e.g., GPT-4o (OpenAI, 2024a)) and open-source models (e.g., Llama 3.1 405B (Meta, 2024)) have performed well on these tasks, solving more than 90% of problems. However, function-level code generation represents only a fraction of the challenges encountered in real-world software development. To evaluate LLMs’ capabilities in more realistic scenarios, the SWE-bench benchmark (Jimenez et al., 2023) series was introduced. The evaluation process in SWE-bench is designed to simulate real-world software improvement tasks: given a natural language description of an issue and the corresponding Github repository, the model is expected to generate a patch that resolves the issue. This approach tests not only code generation capabilities but also the model’s ability to understand real-world software issue, locate relevant code across multiple files, and make appropriate modifications while maintaining the integrity of the entire codebase. In response to these challenges, both SE and AI communities have focused on developing sophisticated software engineering agents (Chen et al., 2024; Liu et al., 2024b; Cognition, 2023; Ma et al., 2024). Systems such as SWE-agent (Yang et al., 2024a) and AutoCodeRover (Zhang et al., 2024b) exemplify these approaches, leveraging the general capabilities of LLMs to iteratively and autonomously formulate plans, execute tool calls, and observe feedback. For example, SWE-agent employs an agent-computer interface to execute operations like opening files, searching code lines, and running tests. In contrast, approaches like Agentless (Xia et al., 2024a) utilize LLMs within predefined workflows that guide the problem-solving process through fixed, expert-designed steps. While this method limits the autonomy and generality of LLM, it also shows promising results by effectively structuring tasks. However, our analysis reveals that current ASE approaches still face significant limitations due to two primary factors. Over-reliance on Closed-Source Models. The top-performing submissions on the SWE-bench (Jimenez et al., 2023) leaderboard are exclusively based on closed-source models like GPT-4/4o (Achiam et al., 2023; OpenAI, 2024a) and Claude 3.5 Sonnet (Anthropic, 2024a). In contrast, submissions utilizing open-source models, such as SWE-Llama 13B, solve only 1.00% of the problems. Although recent research has shown progress in open-source model performance—with Qwen2 72B instrut achieving 9.34% on SWE-bench Lite (Liu et al., 2024c)—this advancement, while encouraging, still lags significantly behind closed-source models. Moreover, considering the privacy of user code repositories in software development, utilizing closed-source models for code analysis and modification may raise security concerns. This factor restricts the applicability of closed-source models in real-world software engineering scenarios, highlighting the urgent need for high-performance open-source models. Lack of Comprehensive Development Process Data. General-purpose and code-specific large models are typically trained on vast amounts of static code data (Hui et al., 2024; Zhu et al., 2024b; Meta, 2024). While this approach has significantly enhanced their code generation capabilities, it overlooks the dynamic interactions and development process crucial to comprehensively understanding real-world software engineering practices. Real-world software improvement encompasses a complex reasoning cycle of issue identification, tool utlization, code navigation, and iterative problem-solving, which is not captured by merely training on static codebases. Although some existing models (Muennighoff et al., 2023; Lozhkov et al., 2024) utilize pull request submission processes recorded on GitHub, these data only include commit messages and final patches, lacking the detailed thought processes and reasoning of developers during problem-solving. Furthermore, current training methods primarily focus on isolated code snippets or single files, neglecting broader project structures and cross-file information. This results in models lacking a global perspective when dealing with complex software systems, making it challenging to accurately understand and handle cross-module interactions. Figure 1. Overview of an iterative development-process training framework for automated software improvement. Our Approach. To address these challenges, we introduce the Lingma SWE-GPT series, which includes models of two size, 7B and 72B, specifically designed for software improvement. As shown in Figure 1, our approach simulates the software improvement process through a three-stage process: repository understanding, fault localization, and patch generation. Each stage operates in a Chain-of-Thought (CoT) manner. To address real-world GitHub issues, Lingma SWE-GPT initially conducts a comprehensive analysis of the software repository, examining the codebase hierarchically from the overall file structure to specific classes and functions. Specifically, Lingma SWE-GPT identifies a set of potentially relevant files based on the natural language description of the issue and the repository’s directory tree structure. It then identifies relevant classes and functions based on the skeletons of these files and formulates a plan for issue resolution. Following this repository understanding, Lingma SWE-GPT retrieves pertinent code context by invoking specialized search APIs (e.g., search_func(’resize’)). These APIs, leveraging Abstract Syntax Tree analysis, extract contextual information such as method and class implementations from the codebase. The model iteratively refines its understanding of both the issue and the repository, strategically selecting which APIs to use in subsequent iterations until potential fault locations are identified. In the concluding phase, Lingma SWE-GPT generates and applies patches to address the localized issues. This phase encompasses concrete solution generation, code replacement, and iterative debugging processes based on syntax validation and git operations, ensuring the development of applicable patches. To further enhance the model’s capabilities, we employ a development process-centric iterative training strategy. This involves optimizing model performance through maximizing the conditional probability of generating development process outputs, including thought processes, tool utilization record and final results. By incorporating curriculum learning, the model progressively tackles increasingly complex tasks, establishing a robust foundation on simpler ones. Additionally, we implement a rejection sampling process to ensure the quality of synthesized data, selectively retaining high-quality instances that closely mimic real-world software development practices. Our extensive evaluation on SWE-bench Verified and SWE-bench Lite demonstrates the effectiveness of Lingma SWE-GPT: the 72B version successfully resolves 30.20% of issues on SWE-bench Verified, marking a significant improvement over existing open-source models(22.76% relative improvement compared to Llama 3.1 405B) and approaching the performance of leading closed-source alternatives (31.80% issues of GPT-4o resolved). Additionally, the 7B version achieves an impressive 18.20% success rate, surpassing the 17.20% resolution rate of Llama 3.1 70B, showcasing the potential of smaller, more efficient models in automated software engineering tasks. Contributions. In summary, we make the following novel contributions: • We introduce Lingma SWE-GPT, a novel series of open-source large language models specifically optimized for automated software improvement111https://github.com/LingmaTongyi/Lingma-SWE-GPT. • We propose a development process-centric training approach that captures the dynamic nature of software engineering, including tool utilizing, reasoning, and interactive problem-solving capabilities. • We demonstrate the effectiveness of our approach through comprehensive evaluations on SWE-bench Lite and Verified, showing significant improvements over existing open-source models and competitive performance against closed-source alternatives. • We provide insights into the model’s fault localization capabilities and performance consistency, offering valuable directions for future research in AI-assisted software engineering."
https://arxiv.org/html/2411.00006v1,Personality-Guided Code Generation Using Large Language Models,"Code generation, the automatic creation of source code from natural language descriptions, has garnered significant attention due to its potential to streamline software development. Inspired by research that links task-personality alignment with improved development outcomes, we conduct an empirical study on personality-guided code generation using large language models (LLMs). Specifically, we investigate how emulating personality traits appropriate to the coding tasks affects LLM performance. We extensively evaluate this approach using seven widely adopted LLMs across four representative datasets. Our results show that personality guidance significantly enhances code generation accuracy, with improved pass rates in 23 out of 28 LLM-dataset combinations. Notably, in 11 cases, the improvement exceeds 5%, and in 5 instances, it surpasses 10%, with the highest gain reaching 12.9%. Additionally, personality guidance can be easily integrated with other prompting strategies to further boost performance.","Code generation, which aims to automatically produce source code from natural language descriptions, has attracted significant attention from both academia and industry due to its potential to streamline software development (Jiang et al., 2024). The emergence of large language models (LLMs) has advanced this field by enabling the effective generation of complete, executable code (Chen et al., 2021; Karmakar and Robbes, 2021). Additionally, specialized LLMs, such as CodeaLlama Roziere et al. (2023) and DeepSeek-Coder Zhu et al. (2024), have further refined these capabilities by focusing specifically on programming tasks. Previous research has observed that software development outcomes improve when individuals are assigned tasks that match their personality types (Capretz et al., 2015). Furthermore, personality diversity within teams has been shown to correlate with higher-quality software deliverables (Pieterse et al., 2018; Capretz and Ahmed, 2010). In the code generation literature, LLMs are frequently tasked with role-playing as programmers to generate code (Jiang et al., 2024). However, it is still unclear whether assigning these “programmers” with appropriate personalities and increasing personality diversity across tasks could further enhance code generation accuracy. To fill this knowledge gap, we present an empirical study on personality-guided code generation using LLMs. Specifically, we first use GPT-4o, an advanced general-purpose LLM, to generate a programmer personality tailored to each coding task. Next, we assign various LLMs to emulate the roles of programmers with these generated personalities and evaluate whether this enhances their code generation accuracy. We conduct a comprehensive evaluation of personality-guided code generation using seven widely-used LLMs and four well-recognized datasets. These LLMs, developed by leading vendors such as OpenAI, Meta, Alibaba, and DeepSeek, are extensively employed for code generation in both research and real-world applications (Hou et al., 2023; Fan et al., 2023; OpenAI, 2024b; Mistral, 2024). For personality characterization, we use the Myers-Briggs Type Indicator (MBTI) framework (Myers, 2003), which is widely applied in project management to align tasks with individual personality types and improve team dynamics (Capretz and Ahmed, 2010). Our results demonstrate that personality guidance significantly enhances code generation accuracy, with pass rates improving in 23 out of 28 LLM-dataset combinations. In 11 cases, the improvement exceeds 5%, and in 5 instances, it surpasses 10%, with the highest gain reaching 12.9%. Additionally, several factors appear to influence the effectiveness of personality-guided code generation, including LLM performance, dataset difficulty, and personality diversity. Specifically, moderate-performance LLMs benefit more from personality guidance compared to very strong or very weak models. Similarly, larger improvements are observed on datasets of moderate difficulty, as opposed to very easy or very difficult ones. Furthermore, greater personality diversity enhances the effectiveness of this approach, aligning with previous findings that diverse personality profiles in development teams are associated with higher-quality software outcomes (Pieterse et al., 2018; Capretz and Ahmed, 2010). Moreover, personality-guided code generation can be easily integrated with other prompting strategies. For instance, when combined with Chain of Thought (Wei et al., 2022), a widely-used prompting strategy in the code generation literature (Jiang et al., 2024), we observe additional improvements in accuracy, with the highest gain reaching 13.8%."
https://arxiv.org/html/2411.00005v1,Mastering the Craft of Data Synthesis for CodeLLMs,"Large language models (LLMs) have shown impressive performance in code understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future research directions, and offer practical guidance for new researchers entering the field.","Code intelligence leverages machine learning techniques to enhance software development by improving both code quality and programmer productivity Allamanis and Sutton (2013); Allamanis et al. (2018). The rise of LLMs, such as ChatGPT OpenAI (2023), Gemini Anil et al. (2024), Claude Anthropic (2023), and Llama Dubey et al. (2024), has significantly reshaped the automation of code-related tasks, including code completion Guo et al. (2023), translation Szafraniec et al. (2023), repair Olausson et al. (2024), and documentation Khan and Uddin (2022). Tools like GitHub Copilot Chen et al. (2021), CodeGeeX Zheng et al. (2023), and Cursor CursorAI (2024) hold great promise in substantially increasing human programmer efficiency and revolutionizing the software industry, attracting considerable attention from both academia and industry. Recently, specialized LLMs for code-related tasks (denoted as CodeLLMs) have emerged, including Code Llama Rozière et al. (2024), StarCoder Li et al. (2023a); Lozhkov et al. (2024), DeepSeek-Coder Guo et al. (2024); Zhu et al. (2024), and CodeQwen Bai et al. (2023). Recent advancements Gunasekar et al. (2023); Gandhi et al. (2024) in LLMs have highlighted the critical role of high-quality data in building strong, robust models. Similarly, for CodeLLMs, diverse, high-quality datasets are essential for improving performance across a wide range of code-related tasks. Significant efforts have been devoted to collecting and curating code-related corpora. Prominent examples include the Pile Gao et al. (2021), the Stack Kocetkov et al. (2023); Lozhkov et al. (2024) and BigScience ROOTS Laurençon et al. (2022), which draw primarily from open-source and permissively licensed platforms such as GitHub and Stack Overflow. However, relying solely on human-generated data for code-related tasks poses several challenges. First, collecting large-scale human data is labor-intensive and expensive, particularly for high-quality instruction tuning and preference alignment data. Second, human-generated data is prone to biases and errors Hosking et al. (2024); Singh et al. (2024), as it reflects the varying skill levels of programmers, and may not be optimal for model training. Third, data integrity concerns, such as the risk of sensitive personal/corporate information leakage, complicate data collection. Lastly, for low-resource programming languages—–either due to limited popularity or proprietary restrictions—–data scarcity hinders the effectiveness of CodeLLMs in specialized fields and systems programming Mora et al. (2024). Consequently, synthetic data generated by LLMs has emerged as a valuable alternative to complement natural data. Leveraging their vast knowledge and advanced linguistic capabilities, LLMs can generate high-quality data, providing a valuable foundation for model training in code-related tasks. While generating synthetic datasets for code-related tasks may appear straightforward, achieving both high accuracy and sufficient diversity is a complex process requiring meticulous design and advanced techniques Gandhi et al. (2024). This makes a systematic exploration of LLM-driven synthetic data generation both essential and timely. Although there are survey papers in the fields of general data engineering Liu et al. (2024b); Long et al. (2024); Wang et al. (2024c); Ding et al. (2024a) and code intelligence Wan et al. (2023); Jiang et al. (2024); Zhang et al. (2024d); Sun et al. (2024a), there is a notable gap in literature focusing specifically on data synthesis and filtering techniques for code-related tasks. To fill this gap, we present a targeted review of recent advancements in synthetic data generation and filtering for training CodeLLMs, covering over 50 recent works across 23 topic categories from the past two years. The techniques discussed are organized into a taxonomy (Fig. 1) and analyzed in terms of their motivation, methodologies, and key contributions. Our goal is to provide an in-depth overview of the current state of the field, highlight key challenges, and offer insights to guide researchers and practitioners in building efficient and robust CodeLLMs through effective data engineering practices. The remainder of this paper is structured as follows. In Section 2, we provide an overview of the data curation pipeline, define the scope and taxonomy of this survey, and highlight key distinctions from other related works. Sections 3 and 4 cover the core techniques for data synthesis and filtering, respectively. In Section 5, we discuss key challenges and outline potential future research directions. We present our conclusions in Section 6. Additionally in Appendix A, we offer a practical guide with best practices and considerations for selecting techniques and models."
https://arxiv.org/html/2411.00447v1,An Empirical Study of VulnerabilityHandling Times in CPython,"The paper examines the handling times of software vulnerabilities in CPython, the reference implementation and interpreter for the today’s likely most popular programming language, Python. The background comes from the so-called vulnerability life cycle analysis, the literature on bug fixing times, and the recent research on security of Python software. Based on regression analysis, the associated vulnerability fixing times can be explained very well merely by knowing who have reported the vulnerabilities. Severity, proof-of-concept code, commits made to a version control system, comments posted on a bug tracker, and references to other sources do not explain the vulnerability fixing times. With these results, the paper contributes to the recent effort to better understand security of the Python ecosystem.","Underneath the Python programming language are so-called virtual machines that compile Python code into byte code before execution. These machines are embedded into the language’s interpreters. While there are many interpreters, including examples such as Cython and Jython, the reference implementation, CPython, is the most popular one. Over the years, many vulnerabilities have also been reported for this reference implementation [15]. The present paper examines how long the handling of these have taken, what factors explain the handling times, and how well these can be predicted. By handling it is meant that addressing of vulnerabilities requires many distinct software engineering work tasks. A vulnerability needs to be obviously fixed, but a given fix needs to be also integrated into releases, often including distinct release branches. In addition, the vulnerability requires coordination between multiple parties [14, 27], which in the open source context include particularly so-called downstream distributors, such as Linux distributions. Coordination is also required with the non-profit MITRE corporation to get a Common Vulnerabilities and Exposures (CVEs) identifier. Although the Python Software Foundation (PSF), who as an organization is behind CPython, is a CVE numbering authority and can thus allocate CVEs on its own, it may be that additional coordination is still required with some CVEs before they are published by MITRE and later on archived into the National Vulnerability Database (NVD) [18], the world’s foremost vulnerability database. For these reasons, the paper concentrates on two distinct timelines within a vulnerability’s overall handling time: (a) the time required to fix a given vulnerability and (b) the time required for a CVE for it to be published. Hereafter, the former is known as fixing time and the latter as CVE coordination time. The questions examined and the paper’s topic in general are easy to motivate. According to benchmarks, Python is the most popular programming language today [7], and because CPython is the most popular interpreter for the language, the vulnerabilities affecting the interpreter affect large user and deployment bases. In addition, as pointed out in the opening Section II, the handling times proxy not only software engineering effort but also security risks. A further motivating point is that the paper’s topic has not been examined previously, despite a large reference literature base on bug and vulnerability handling times, including their fixing times. The paper’s remaining structure is simple. After the already noted Section II on related work, the dataset and methods for examining it are elaborated in Section III. Then, the empirical results are presented in Section IV. Finally, Section V summarizes the conclusions reached, pinpoints some limitations, and discusses the implications particularly for further work."
https://arxiv.org/html/2411.00442v1,FPRev: Revealing the Order of Floating-Point Summation by Numerical Testing,"The order of floating-point summation is a key factor in numerical reproducibility. However, this critical information is generally unspecified and unknown for most summation-based functions in numerical libraries, making it challenging to migrate them to new environments reproducibly. This paper presents novel, non-intrusive, testing-based algorithms that can reveal the order of floating-point summation by treating functions as callable black boxes. By constructing well-designed input that can cause the swamping phenomenon of floating-point addition, we can infer the order of summation from the output. We introduce FPRev, a tool that implements these algorithms, and validate its efficiency through extensive experiments with popular numerical libraries on various CPUs and GPUs (including those with Tensor Cores). FPRev reveals the varying summation orders across different libraries and devices, and outperforms other methods in terms of time complexity. The source code of FPRev is at https://github.com/microsoft/RepDL/tree/main/tools/FPRev.","With the rapid evolution of heterogeneous hardware and diverse software stacks, the lack of reproducibility in numerical computing has become a recognized problem (he_using_2001, ; villa_effects_2009, ; taufer_improving_2010, ; bailey_facilitating_2016, ; chen_towards_2022, ; liu_reproducibility_2022, ). The same numeric function can produce varying results when software is migrated to new hardware or when numerical libraries are updated. Non-reproducible results pose significant challenges in scientific research, software engineering, deep learning, and applications that rely on numerical models for decision making. These challenges undermine the credibility of findings, hinder progress by obscuring errors in programs, and can lead to incorrect conclusions or suboptimal decisions, ultimately affecting the reliability and trustworthiness. A primary cause of numerical non-reproducibility is discrepancies in the order of floating-point summation (robey_search_2011, ; demmel_fast_2013, ; arteaga_designing_2014, ; demmel_parallel_2015, ; collange_numerical_2015, ). The result of floating-point summation depends on the order of computation due to the non-associative nature of floating-point addition. For example, as shown in Table 1, the sum of 0.1, 0.2 and 0.3 is order-dependent, because (0.1+0.2)+0.3≠0.1+(0.2+0.3)0.10.20.30.10.20.3(0.1+0.2)+0.3\neq 0.1+(0.2+0.3)( 0.1 + 0.2 ) + 0.3 ≠ 0.1 + ( 0.2 + 0.3 ) in IEEE-754 (noauthor_ieee_2019, ) binary64 (also known as float64). There is no general specification that stipulates the order of floating-point summation. Consequently, without well-defined specifications, numerical libraries usually compute floating-point summation in various orders in different environments, leading to inconsistent numerical output. Table 1. Examples of the non-associative nature of float64 addition. (0.1+0.2)+0.30.10.20.3(0.1+0.2)+0.3( 0.1 + 0.2 ) + 0.3 0.1+(0.2+0.3)0.10.20.30.1+(0.2+0.3)0.1 + ( 0.2 + 0.3 ) (−260+260)+1superscript260superscript2601(-2^{60}+2^{60})+1( - 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT ) + 1 −260+(260+1)superscript260superscript2601-2^{60}+(2^{60}+1)- 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + ( 2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + 1 ) Decimal 0.60000000000000008882 0.59999999999999997780 1 0 Hexadecimal 0x1.3333333333334p-1 0x1.3333333333333p-1 0x1p0 0x0p0 Knowing the order of summation is critical for reproducibility. Consider a function based on floating-point summation (e.g., matrix multiplication) that produces inconsistent output in new environments, which is undesirable. To fix the issue, the order of summation must be known. This information can serve as a valuable guide and constraint in determining the appropriate order of summation when migrating the function to the new environments. However, the information is virtually unknown. Existing numerical libraries, such as Intel MKL (mkl, ), OpenBLAS (OpenBLAS, ), and NVIDIA cuBLAS (cublas, ), do not specify this information in their documentation, and there is no specialized tool to reveal the information. Revealing the order of summation is a challenging task. For example, people can manually determine the order by analyzing the static source code, but many libraries or hardware implementations are black-box, which limits the static approach. Even if the function’s trace is obtained and analyzed, no tool can automatically generate the computational graph of the summation. We build a non-intrusive, testing-based tool called FPRev to reveal the order of summation. FPRev treats the summation-based function as a callable black box, generates specialized test cases, and infers the order of summation from the function’s output. FPRev provides two versions of algorithms: FPRev-basic and FPRev-advanced. Both leverage the swamping phenomenon in floating-point addition to generate well-designed numerical input. When two floating-point numbers differing by many orders of magnitude are added, the smaller number is swamped and does not contribute to the sum. For example, 260+1superscript26012^{60}+12 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT + 1 equals 260superscript2602^{60}2 start_POSTSUPERSCRIPT 60 end_POSTSUPERSCRIPT when using float64. Based on the phenomenon, we can utilize large numbers as masks to hide certain summands from the sum. In FPRev-basic, we first generate several “masked all-one arrays”. Each array is predominantly composed of the floating-point number 1.0, with exactly two non-one elements: M𝑀Mitalic_M and −M𝑀-M- italic_M. Here, M𝑀Mitalic_M represents a large positive number that can cause the swamping phenomenon in floating-point arithmetic. Specifically, let n𝑛nitalic_n denote the number of summands. M𝑀Mitalic_M satisfies ±M+μ=±Mplus-or-minus𝑀𝜇plus-or-minus𝑀\pm M+\mu=\pm M± italic_M + italic_μ = ± italic_M for all non-negative number μ<n𝜇𝑛\mu<nitalic_μ < italic_n. Next, we call the tested function multiple times with different masked all-one arrays. Each output reveals how many summands are swamped by ±Mplus-or-minus𝑀\pm M± italic_M during summation and how many are not. This information relates to the structure of the computational graph. The graph is a full binary tree that accurately depicts the order of operations. Each output equals the number of leaf nodes out of the subtree rooted at the lowest common ancestor of the nodes corresponding to the indexes of M𝑀Mitalic_M and −M𝑀-M- italic_M. For example, Table 2 demonstrate the information for Algorithm 1, whose computational graph is Figure 1. Finally, we use the output information to construct the summation tree. This involves a tree-algorithm problem: how to construct a full binary tree given {(i,j,li,j):0≤i<j<n}conditional-set𝑖𝑗superscript𝑙𝑖𝑗0𝑖𝑗𝑛\{(i,j,l^{i,j}):0\leq i<j<n\}{ ( italic_i , italic_j , italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT ) : 0 ≤ italic_i < italic_j < italic_n }, where li,jsuperscript𝑙𝑖𝑗l^{i,j}italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT denotes how many leaf nodes are in the subtree rooted at the lowest common ancestor of the i𝑖iitalic_i-th and j𝑗jitalic_j-th leaf nodes. We construct the tree in a bottom-up (leaf-to-root) way. We begin by constructing subtrees with two leaf nodes (corresponding to li,j=2superscript𝑙𝑖𝑗2l^{i,j}=2italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT = 2). Subsequently, larger subtrees (corresponding to next larger li,jsuperscript𝑙𝑖𝑗l^{i,j}italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT) are built from existing subtrees or isolate nodes, and the process is repeated until the entire tree is generated. Algorithm 1 An example of summation (n=8𝑛8n=8italic_n = 8). {minted} C++ float sum = 0; for (int i=0; i¡8; i+=2) sum += a[i] + a[i+1]; Figure 1. The summation tree of Algorithm 1. The numbers in the leaf nodes denote the indexes. Table 2. The outputs and order-related information for Algorithm 1 with different masked all-one arrays. i𝑖iitalic_i j𝑗jitalic_j Ai,jsuperscript𝐴𝑖𝑗A^{i,j}italic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT sum(Ai,jsuperscript𝐴𝑖𝑗A^{i,j}italic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT) li,jsuperscript𝑙𝑖𝑗l^{i,j}italic_l start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT 0 1 (M,−M,1,1,1,1,1,1)𝑀𝑀111111(M,-M,1,1,1,1,1,1)( italic_M , - italic_M , 1 , 1 , 1 , 1 , 1 , 1 ) 6 2 0 2 (M,1,−M,1,1,1,1,1)𝑀1𝑀11111(M,1,-M,1,1,1,1,1)( italic_M , 1 , - italic_M , 1 , 1 , 1 , 1 , 1 ) 4 4 0 3 (M,1,1,−M,1,1,1,1)𝑀11𝑀1111(M,1,1,-M,1,1,1,1)( italic_M , 1 , 1 , - italic_M , 1 , 1 , 1 , 1 ) 4 4 0 4 (M,1,1,1,−M,1,1,1)𝑀111𝑀111(M,1,1,1,-M,1,1,1)( italic_M , 1 , 1 , 1 , - italic_M , 1 , 1 , 1 ) 2 6 0 5 (M,1,1,1,1,−M,1,1)𝑀1111𝑀11(M,1,1,1,1,-M,1,1)( italic_M , 1 , 1 , 1 , 1 , - italic_M , 1 , 1 ) 2 6 0 6 (M,1,1,1,1,1,−M,1)𝑀11111𝑀1(M,1,1,1,1,1,-M,1)( italic_M , 1 , 1 , 1 , 1 , 1 , - italic_M , 1 ) 0 8 0 7 (M,1,1,1,1,1,1,−M)𝑀111111𝑀(M,1,1,1,1,1,1,-M)( italic_M , 1 , 1 , 1 , 1 , 1 , 1 , - italic_M ) 0 8 ………… 2 3 (1,1,M,−M,1,1,1,1)11𝑀𝑀1111(1,1,M,-M,1,1,1,1)( 1 , 1 , italic_M , - italic_M , 1 , 1 , 1 , 1 ) 6 2 2 4 (1,1,M,1,−M,1,1,1)11𝑀1𝑀111(1,1,M,1,-M,1,1,1)( 1 , 1 , italic_M , 1 , - italic_M , 1 , 1 , 1 ) 2 6 ………… FPRev-basic has a time complexity of Θ⁢(n2⁢t⁢(n))Θsuperscript𝑛2𝑡𝑛\Theta(n^{2}t(n))roman_Θ ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_t ( italic_n ) ), where t⁢(n)𝑡𝑛t(n)italic_t ( italic_n ) is the time complexity of the tested function. As a contrast, the naive brute-force method has a time complexity of O⁢(4n/n3/2⋅t⁢(n))𝑂⋅superscript4𝑛superscript𝑛32𝑡𝑛O(4^{n}/n^{3/2}\cdot t(n))italic_O ( 4 start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT / italic_n start_POSTSUPERSCRIPT 3 / 2 end_POSTSUPERSCRIPT ⋅ italic_t ( italic_n ) ). Building on FPRev-basic, we propose FPRev-advanced, which has a time complexity of Ω⁢(n⁢t⁢(n))Ω𝑛𝑡𝑛\Omega(nt(n))roman_Ω ( italic_n italic_t ( italic_n ) ) and O⁢(n2⁢t⁢(n))𝑂superscript𝑛2𝑡𝑛O(n^{2}t(n))italic_O ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_t ( italic_n ) ) and supports multi-term fused summation (fasi_numerical_2021, ) used by matrix accelerators like NVIDIA GPU’s Tensor Cores (markidis_nvidia_2018, ). We evaluate efficiency of FPRev by comprehensive experiments. We test FPRev with three popular numerical libraries across six different CPUs and GPUs. Experimental results show that FPRev-advanced is significantly faster than FPRev-basic, demonstrating its lower time complexity. We also showcase the discrepancies in the revealed orders across different libraries and devices. In summary, the contributions of this paper include the following: (1) We propose novel testing-based algorithms to reveal the order of summation for functions based on floating-point summation. The time complexity of the algorithms is polynomial, in contrast to the exponential time complexity of the naive approach. (2) We develop FPRev, a tool that enables automatic revelation of the order of floating-point summation. This tool is significantly helpful in debugging non-reproducible programs, and provides useful information for reproducing the program. (3) We demonstrate the practical efficiency of FPRev with extensive experiments. (4) We reveal the order of summation for common numerical libraries like cuBLAS for the first time."
