URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.04762v1,JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA: Service Delay Minimization for Aerial MEC-assisted Industrial Cyber-Physical Systems,"In the era of the sixth generation (6G) and industrial Internet of Things (IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of sensor devices and computing-intensive tasks. To address the limited resources of IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution, providing flexible and cost-effective services in close proximity of IIoT sensor devices (ISDs). However, leveraging aerial MEC to meet the delay-sensitive and computation-intensive requirements of the ISDs could face several challenges, including the limited communication, computation and caching (3C) resources, stringent offloading requirements for 3C services, and constrained on-board energy of UAVs. To address these issues, we first present a collaborative aerial MEC-assisted ICPS architecture by incorporating the computing capabilities of the macro base station (MBS) and UAVs. We then formulate a service delay minimization optimization problem (SDMOP). Since the SDMOP is proved to be an NP-hard problem, we propose a joint computation offloading, caching, communication resource allocation, computation resource allocation, and UAV trajectory control approach (JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA). Specifically, JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA consists of a block successive upper bound minimization method of multipliers (BSUMM) for computation offloading and service caching, a convex optimization-based method for communication and computation resource allocation, and a successive convex approximation (SCA)-based method for UAV trajectory control. Moreover, we theoretically prove the convergence and polynomial complexity of JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA. Simulation results demonstrate that the proposed approach can achieve superior system performance compared to the benchmark approaches and algorithms.","Industry 5.0 marks a transformative era where human intelligence synergizes with advanced technologies to enhance the industrial efficiency and customization. Central to this shift is industrial cyber-physical systems (ICPS), which seamlessly integrate physical processes with computational and networking capabilities. Specifically, ICPS leverages the industrial Internet of Things (IIoT) to interconnect sensors, facilitating real-time monitoring, automatic control, and refined management across diverse industrial environments, which leads to an exponential growth in multifarious sensor devices. Concurrently, the advancement of sixth generation (6G) technology has further propelled the proliferation of emerging mobile applications within ICPS, resulting in an increasing number of computation-intensive and delay-sensitive tasks such as virtual reality and online gaming. However, it is challenging to perform these tasks locally on IIoT sensor devices (ISDs) due to their constrained energy and computational capabilities and their physical sizes. To reduce the computational load on ISDs, cloud computing has been proposed as an effective solution. However, due to the geographical separation between the cloud infrastructure and ISDs, they may experience long communication latency. Mobile edge computing (MEC)-assisted ICPS has been identified as a promising paradigm and has been extensively studied to offer low-latency offloading services close to ISDs. For example, Peng et al. [1] developed an MEC-assisted ICPS, where the terrestrial MEC nodes are deployed to support real-time transmission. Moreover, Ji et al. [2] proposed an intelligent edge sensing and control framework by deploying MEC in the ICPS. However, these studies primarily focus on terrestrial MEC servers dependent on fixed ground infrastructures, which can result in frequent non-line-of-sight connections, high deployment costs, and limited environmental adaptability. This is particularly problematic in scenarios such as natural disasters, where deploying stationary infrastructures can be challenging. To address the aforementioned challenges, the concept of unmanned aerial vehicle (UAV)-assisted MEC, i.e., aerial MEC, supported by features such as high maneuverability, flexibility, cost-effectiveness, and line-of-sight (LoS) connections, has been introduced to elevate the MEC facilities into the skies to enhance the flexibility of edge computing services. Consequently, by offloading computing tasks to the nearby UAVs, ISDs can flexibly enjoy cloud-like services and free themselves from the burden of heavy computation tasks anytime and anywhere. Recently, several studies investigated the aerial MEC-assisted ICPS. For example, Tang et al. [3] considered a UAV-enabled ICPS, where a UAV is dispatched as an aerial edge server to assist IIoT data processing. Additionally, Shi et al. [4] proposed an energy-constrained multi-UAV assisted ICPS, where multiple UAVs work together to adjust their frequencies based on the task sizes. However, most of these studies assume that the UAVs have relatively sufficient computing resources and can cache all services required by ISDs. However, this assumption may not be realistic due to the constrained resources of UAVs, especially in scenarios with dense offloading requirements. Fully exploring the benefits of UAV-assisted MEC to provide satisfactory offloading services for ICPS encounters significant challenges. i) Resource management. In contrast to cloud servers with abundant resources, UAV-assisted MEC servers are typically equipped with limited communication, computation, and caching (3C) resources. However, the proliferation of ISDs, coupled with various delay-sensitive and computation-intensive applications, places unprecedented demands on 3C resources. Accordingly, the stringent 3C requirements of ISDs and the constrained 3C resources of MEC servers create difficulties in designing efficient 3C resource management to meet the demands of 3C-intensive tasks. ii) Computing offloading. Different ISDs have heterogeneous offloading requirements for 3C services, while different MEC servers offer limited 3C resources. Moreover, random arrival of tasks lead to the spatiotemporal distribution of requirements, while the varying geographical deployment and capacities of different UAVs create a spatiotemporal distribution of 3C resources. Therefore, an effective computation offloading scheduling is crucial and challenging for the server load balancing. iii) Trajectory control. While UAV-assisted MEC servers provide flexible 3C resources for computation offloading, a limited battery capacity of UAVs inherently restricts the service time, posing challenges for the energy-efficient UAV trajectory control. The abovementioned challenges necessity efficient optimization of 3C resource management, computation offloading, and UAV trajectory control. However, focusing on just one aspect of these components is insufficient due to the following reasons. On the one hand, the optimization variables are mutually coupled. For example, the computation offloading decision depends on both the cached services and UAV locations. However, the limited storage capacity and energy of UAVs require efficient service caching and trajectory control. Moreover, the trajectory of a UAV also affects the communication quality and density of ISDs within its coverage, which in turn influences the communication and computing resource allocation. On the other hand, these optimization variables collectively determine the system performance. For example, offloading more tasks to nearby UAVs may reduce local computing delay but can lead to frequent caching and higher energy consumption for both computing and flying. Therefore, these interconnected optimization variables should be jointly optimized to achieve overall superior system performance, as it can effectively capture the intricate and coupling interactions and trade-offs among various optimization components. Consequently, we propose a collaborative optimization approach of computation offloading, service caching, communication resource allocation, computing resource allocation, and UAV trajectory control to enhance the performance of the ICPS. The main contributions of our work are summarized as follows: • Collaborative Aerial MEC-assisted Architecture. We propose a three-layer collaborative aerial MEC-assisted ICPS architecture, which consists of a set of ISDs, a collaborative UAV cluster, and a macro base station (MBS). Specifically, the UAVs act as the aerial MEC servers to collaboratively provide aerial computing services close to ISDs, and the MBS functions as the terrestrial MEC server to alleviate the overload of the UAV cluster. • Service Delay Optimization Problem Formulation. Considering the delay-sensitive requirements of the ISDs, we formulate a service delay minimization optimization problem (SDMOP). Specifically, the SDMOP aims to minimize the total delay of task completion under the energy constraints of ISDs and UAVs. Besides, we prove that this problem is an NP-hard and mixed integer nonlinear programming (MINLP) problem. • Joint Optimization Approach. Since the formulated problem is difficult to be directly solved, we propose a joint computation offloading, caching, communication resource allocation, computing resource allocation, and UAV trajectory control approach (JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA). JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA decomposes the original optimization problem into three subproblems, i.e., the computation offloading and service caching, the communication and computation resource allocation, and the UAV trajectory control. Specifically, for the subproblem of computation offloading and service caching, we employ the block successive upper bound minimization method of multipliers (BSUMM) to solve it. Moreover, the convex optimization methods are adopted to solve the subproblems of communication and computation resource allocation, and UAV trajectory control. Although we decompose the problem into three sub-problems to decouple the interdependent decision variables, we still achieve joint optimization, as a solution of one sub-problem will affect those of other sub-problems, thereby preserving the joint benefits of considering these decisions within the same problem. • Performance Validation. The effectiveness and performance of the proposed JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA are validated through both theoretical analysis and simulation experiment. We theoretically prove the convergence and polynomial complexity of JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA. Moreover, the simulation results demonstrate that JC5superscriptC5\text{C}^{5}C start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPTA achieves superior performance than the comparative approaches and algorithms. The rest of this work is organized as follows. Section II presents the relevant models. Section III gives the problem formulation and analysis. Section IV elaborates the proposed approach. Section V showcases the simulation results. Finally, the conclusions are presented in Section VI."
https://arxiv.org/html/2411.04474v1,The Impact of Traffic Characteristics on System and User Performance in 5G/6G Cellular Systems,"The statistical characteristics of the propagation environment and traffic arrival process are known to affect the user performance in 5G/6G millimeter wave (mmWave) and sub-terahertz (sub-THz) systems. While the former topic has received considerable attention recently, little is known about the impact of traffic statistics. In this study, we characterize the effects of correlation and variability in the session arrival process on the performance of 5G/6G mmWave/sub-THz systems. To this end, we use the tools of stochastic geometry and queuing theory to model the service process at base stations (BS) and specifics of the mmWave/sub-THz radio part. The metrics considered include the system resource utilization and session loss probability. Our results show that the normalized autocorrelation function (NACF), coefficient of variation (CoV), and variance of the resource request distribution have a significant impact on the considered parameters. For the same arrival rate, high values of lag-1 NACF and CoV may lead the system out of the operational regime, affecting the loss probability and resource utilization by up to an order of magnitude. Even a slight deviation from the uncorrelated Poisson process decreases the utilization by 10-20% and increases the session loss probability multiple times. Radio and environmental characteristics may further increase the variability in resource request distribution and decrease resource utilization. In general, the use of the commonly accepted Poisson assumption leads to a severe underestimation of the actual performance of 5G/6G mmWave/sub-THz systems. Therefore, both traffic arrival and propagation statistics are equally important for accurate performance assessment of such systems.","The emergence of high capacity radio access technologies (RAT) that operate in the millimeter wave (mmWave) band, 30-100 GHz, and sub-terahertz (sub-THz) band, 100-300 GHz, is expected to enable the support of novel services at the air interface, such as high-resolution streaming, holographic communications, and extended virtual reality (x-VR) [1, 2]. In contrast to modern applications, these services are rate-greedy in nature and require quality-of-service (QoS) guarantees in terms of bitrate stability [3, 4]. User and system performance in 5G/6G mmWave/sub-THz systems are affected by the statistical features of the traffic arrival process and propagation environment. Specifically, the propagation in the mmWave/sub-THz bands is a complex phenomenon that has an impact on session service continuity. In addition to the large propagation losses, limiting the coverage of prospective mmWave/sub-THz base stations (BS), the blockage of propagation paths by various objects in the channel, such as human bodies, may lead to frequent loss of connectivity [5]. As a result, performance evaluation models for 5G/6G mmWave/sub-THz cellular systems proposed in the past have mainly concentrated on capturing the propagation phenomenon (e.g., [6, 7, 8, 9, 10]). In contrast to propagation specifics, little is currently known about the impact of traffic arrival statistics in mmWave/sub-THz systems on user- and system-centric performance metrics. The unique feature of our study is that we account for burstiness of the session arrival process in cellular deployments. This is in contrast to conventional assumptions of the Poisson arrivals taken in many studies of cellular systems. Such type of burstiness naturally happens in any type of telecommunications network including cellular systems, where arrivals can be modulated by some external events [11, 12], e.g., flash crowds, event-based services, social interactions of users within the cell coverage, etc. While the burniness of the session arrival process in cellular systems is confirmed in several studies, the authors are unaware of rigorous investigation of exact distributional and correlational properties in cellular systems. To this aim, in our study we concentrate on two main metrics affecting burstiness phenomenon, coefficient of variation (CoV) and normalized autocorrelation function (NACF). It has long been known that these parameters of the session arrival process may have a negative impact on key performance indicators such as system resource utilization and session loss probability [13, 14]. Furthermore, it is known that the resource requirements of the sessions also depend on environmental factors such as propagation conditions and density of blockers [5, 15]. Understanding the impact of these factors is therefore critical for the deployment of mmWave/sub-THz 5G/6G systems. In our study, we consider the so-called “last mile”, i.e., the service process of users’ at the 5G/6G base station operating in mmWave/sub-THz band. The aim of this paper is to characterize the impact of temporal correlation and variability in arrival traffic patterns on service performance in 5G/6G mmWave/sub-THz systems. To this end, we utilized stochastic geometry to capture the propagation properties and characterize the resource requirements of arriving sessions and queuing-theoretic tools to evaluate the delivered system- and user-oriented key performance indicators. In our numerical results, we determined the impact of the temporal correlation and variability of the arrival process on system resource utilization and session loss probability. The contributions of our study include: • mathematical model to account for the impact of the temporal correlation and variability of the session arrival process in 5G/6G mmWave/sub-THz systems with blockages and directional antennas; • numerical results showing that variability and temporal clustering in the session arrival process negatively impact both system- and user-oriented performance indicators, decreasing resource utilization multiple times and increasing session loss probability; • observation showing that the radio part and environmental parameters affect the system’s performance mainly via the variability of the session’s resource request distribution and thus need to be accurately captured to predict the actual system performance. The reminder of this paper is organized as follows. We review related work in Section II. The system model is presented in Section III. The model is formalized and solved in Section IV and then parameterized in Section V. Numerical results demonstrating the impact of correlation and variability on user- and system-centric performance metrics are presented in Section VI. Finally, the conclusions are provided in the last section."
https://arxiv.org/html/2411.04365v1,"Towards Secured Smart Grid 2.0: Exploring Security Threats, Protection Models, and Challenges","Many nations are promoting the green transition in the energy sector to attain neutral carbon emissions by 2050. \acSG2 is expected to explore data-driven analytics and enhance communication technologies to improve the efficiency and sustainability of distributed renewable energy systems. These features are beyond smart metering and electric surplus distribution in conventional smart grids. Given the high dependence on communication networks to connect distributed microgrids in \acSG2, potential cascading failures of connectivity can cause disruption to data synchronization to the remote control systems. This paper reviews security threats and defense tactics for three stakeholders: power grid operators, communication network providers, and consumers. Through the survey, we found that \acSG2’s stakeholders are particularly vulnerable to substation attacks/vandalism, malware/ransomware threats, blockchain vulnerabilities and supply chain breakdowns. Furthermore, incorporating artificial intelligence (AI) into autonomous energy management in distributed energy resources of \acSG2 creates new challenges. Accordingly, adversarial samples and false data injection on electricity reading and measurement sensors at power plants can fool AI-powered control functions and cause messy error-checking operations in energy storage, wrong energy estimation in electric vehicle charging, and even fraudulent transactions in peer-to-peer energy trading models. Scalable blockchain-based models, physical unclonable function, interoperable security protocols, and trustworthy AI models designed for managing distributed microgrids in \acSG2 are typical promising protection models for future research.","Integrating distributed energy systems is a major topic in the green and renewable energy era with sustainable goals of usage efficiency, autonomous intelligence, and resilience capability against sudden failures. These new energy integration capabilities expects to be the core of Smart Grid 2.0 (SG2) [1, 2]. \acSG2 aims to enhance energy distribution and usage efficiency with the help of communication technologies [3, 4, 5]. These interdependent power grid and communication networks can help connect distributed electricity, gas, and cooling systems, offering unprecedented opportunities for remote control capability and flexibility [6]. However, with reliance on digital communication technologies, as shown in Fig. 1, SG2 faces significant threats, targeting connectivity among power grid providers, two-way communication network systems, and consumer entities (industrial, residential, and commercial users). For example, ransomware attacks have recently been recorded to cause prominent blackouts in many countries [7]. Figure 1: The illustration shows the crucial role of communication technologies in synchronizing measurement data from substations, enabling remote control capabilities for efficient power distribution. However, the dependency of power grids on communication technology creates fresh threats of security attacks to energy security, e.g., ransomware to disable control systems and denial of services against transmission lines to stop data exchange. Recent drone attacks and supply chain risks also threaten critical facilities (economic loss) and even endanger public safety (e.g., power outages in the cooling systems of nuclear plants). Also, providing sophisticated communication networks to millions of charging stations and diverse renewable energy sources in SG2, while not overburdening the distribution network or destabilizing the grid, is also a challenge. Understanding the threats and challenges is the critical step toward developing robust defense approaches for guaranteeing energy security, and further national safety. This article aims to explore the various security threats in \acSG2, particularly its communication infrastructure and \acSG2 enabling technologies, such as peer-to-peer energy trading and AI-powered grid network functions. In addition, the study discusses emerging strategies for safeguarding \acSG2 from cascading failures and for developing effective distribution grid restoration plans in disaster scenarios and severe security attacks in the future. I-A State-of-the-art literature review Exploring security threats for smart grids has been a hot topic for years but few studies address \acSG2’s security matters in a comprehensive manner. Fig. 2 presents \acSG2’s essential components and security concerns, together with state-of-the-art relevant studies. Accordingly, most articles focus on security threats in the conventional smart grid that features electricity sources and distribution. For example, the authors in [8] provide a comprehensive survey of typical security attacks and vulnerabilities of authentication and security protocols in the conventional energy model. AI and blockchain for conventional smart grids and related security is briefed in [9, 10, 11]. However, the paper covers few aspects of security protection or \acAI role in specific functions in each layer or from related stakeholders (power generator, communication provider, and consumers). Similarly, the surveys in [12, 13, 14] cover a narrow scope of security in specific smart grid networks, e.g., the connection between home and grid supply, metering data collection and transmission [15]. Recently, the survey papers focus on the security threats in communications among power generation and distribution components of the newer smart grid model with battery storage systems [16, 17, 18, 19]. As technology and infrastructure continue to advance, renewable energy (e.g., wind, solar, geothermal, hydropower) plays an increasingly vital role in the global energy transition toward a cleaner and more sustainable future. Energy storage systems are required to maintain the stability of such distributed sources. Several surveys on the safety of smart inverters [20], battery storage/swap [21], or control systems [22] against remote attacks or physical tampering is also presented. On the other hand, the authors in [23, 24, 25] provide a holistic view of control and communication strategies in multi-energy generation grids or robust models against cascading failure in interdependent power-communication networks. However, the studies did not address the security threats or specific attacks for each entity (power provider, communication network provider, consumer). Unlike prior studies, this work aims to investigate weaknesses in the interdependence of microgrids that heavily rely on distributed energy sources and communication technologies. Additionally, there will be a focus on identifying new risks associated with AI-powered energy control and novel energy trading/storage models. This research will be particularly important as many new small energy sources (e.g., from solar roofs) are integrated into management networks. Overall, the first goal of our work is to provide a comprehensive view of cybersecurity in these new elements, referring to \acSG2’s energy security principles, which have received little attention in the existing literature. Figure 2: This work addresses energy security principles for SG2 from a view of interdependent power grid communication networks, notably with the introduction of new technologies for three entities (power provider, communication network provider, consumer), such as energy storage, 5G/6G, AI-powered functions, and peer-to-peer energy trading models. Besides, many governments considered grid security as a national security matter and proposed measures to improve information security protection [26, 27, 28, 29] and resilience strategies in disaster and crisis scenarios [23, 30, 31]. Grid security refers here to the consistent and reliable availability of all fuels and electricity sources in a timely, sustainable, and cost-effective manner. For example, several standards for information security in smart grids, such as the framework developed by the \acNIST, have been developed. The US Department of Energy is carrying out the Cybersecurity for Energy Delivery Systems (CEDS) program [32], aimed to enhance the security and resilience of the country’s energy infrastructure. The \acNERC has established network security standards for the power industry in North America. The \acENISA has issued guidelines for safeguarding EU’s smart grids [29]. The \acIEC has established standards for network security in power systems (IEC 62443, 62351 standards) [27]. Therefore, this survey’s second goal is to determine which features have not yet been defined in the security standards and what standards the nationals apply for their energy management systems based on facility availability, deployment cost, and environment compatibility. Further, understanding energy restoration plans for potentially cascading failures of communication technologies is critical to consult a proper model for deploying \acSG2. I-B Review methodology Given the difficulties of installing from scratch owing to high costs, \acSG2 will likely inherit many control components, existing facilities, and communication infrastructure from the current smart grid. Inspired by this fact, we present possible security concerns in \acSG2, as viewed through the mirrors of two aspects’ lessons learned. The first phase involves examining energy security principles, identifying the primary risks to the components of a smart grid system, and assessing the security vulnerabilities in legacy technologies with examples of well-known energy crises and blackout events. Additionally, this involves energy restoration strategies in the event of probable cascade failures resulting from security attacks on communication lines. The rest is to figure out security flaws in emerging technologies and new decentralized energy models that are expected to be the main vehicles towards \acSG2, such as AI-powered energy control functions, battery storage technologies, and the integration of advanced communication technologies for charging stations and distributed renewable energy sources. Security threats are often the motivating factor behind the need to change countermeasure approaches. These attacks typically reveal system faults or protocol issues that were not expected during the design process. Analyzing and learning from attacks like this gives significant insights into the essential security changes for \acSG2, particularly in addressing known weaknesses exploited in prior generations. Finally, this work also addresses security matters from a top-down approach where security threats against the interdependent relationship of stakeholders (power grid operator, communication network provider, consumer) will be assessed and suggested with corresponding defense strategies. The unsolved problems become possible targets for \acSG2 improvements, which serve as the foundation for proposing future solutions. Figure 3: The following is a summary of the major findings from our survey on security and protection strategies for Smart Grid 2.0. The decorative colors for technologies match those for the three entities (power provider, communication network provider, and customer) illustrated in the previous figures. I-C Contributions Given the slow transition from legacy to new technologies, it is difficult to predict when \acSG2 will be in full operation. However, by drawing a line of relative differences between the current smart grid platform and the expected \acSG2 architecture, this research can help the developers and researchers determine the security weaknesses and find the right starting point of \acSG2’s technologies to improve. The primary contributions in this work are summarized as follows. 1. The first attempt to thoroughly investigate the principles of SG2 security for national safety, taking into account the whole perspective of security measures for communication links among power grid operators, communication network providers, and consumers. The study examines the relationship between power grids and communication networks in terms of cascading failures. It identifies potential solutions and necessary improvements for \acSG2, specifically in energy restoration planning and communication isolation. 2. The first attempt to offer a comprehensive perspective on the security risks of \acSG2 enabling technologies that need to be adapted to meet the evolving requirements of \acSG2, e.g., blockchain-based energy management/trading, AI-aided grid operations, the networks of electrified transportation systems (EV charging stations) and distributed renewable energy sources. Given that \acSG2 follows economic trajectory of the technology evolution, a systematic review of the transition process and potential changes in supply chain management and new communication methods can guide power grid operators and network providers in effectively upgrading their security infrastructure and countermeasure techniques in the future. 3. This study summarized lessons learned from the limitations of current protection implementations in SG1 and the vulnerabilities of SG2 emerging technologies that can aid researchers and developers in determining the problem formulation for further studies. To the best of our knowledge, this survey represents the initial endeavor to comprehensively assess security threat aspects for \acSG2, spanning from vulnerabilities in distributed renewable energy sources to EV charging network architecture, and then AI-powered grid management. I-D Structure of the paper The rest of this paper is organized as follows. Section II briefs the fundamental information about \acSG2 architecture, energy security principles, and overall strategies to protect the power grid-communication networks’ infrastructure. The security attacks and defense approaches for power providers, communication network providers, and consumer stakeholders are then detailed in Section III, IV, V, respectively. Section VI outlines security risks and some countermeasure techniques in emerging technologies and their role in securing \acSG2 is detailed in Section VII. Section VIII discusses lessons learned and future research. Section IX concludes this paper. Fig. 3 summarizes the main points of our survey. The acronyms used in this work are listed as follows. \printacronyms [sort=true]"
https://arxiv.org/html/2411.04336v1,Demo: Paving the Way for Smart Manufacturing with 5G/TSN Convergence and Augmented Reality,"The fifth-generation (5G) mobile/cellular and time-sensitive networking (TSN) technologies are widely recognized as the key to shaping smart manufacturing for Industry 4.0 and beyond. Converged operation of the two offers end-to-end real-time and deterministic connectivity over hybrid wired and wireless segments. On the other hand, the augmented reality (AR) technology provides various benefits for the manufacturing sector. To this end, this demonstration showcases AR-aided remote assistance use-case over a hybrid TSN and 5G system. The demonstration setup comprises off-the-shelf 5G and TSN devices, a near product-grade 5G system, and an AR solution based on smart glasses. The demonstration shows the viability of over-the-air transmission of scheduled TSN traffic and real-time assistance for a local user from a remote environments. Performance results from the demonstration setup are also shown.","I-A Background and Motivation The global smart manufacturing market111https://www.marketsandmarkets.com/Market-Reports/smart-manufacturing-market-105448439.html is expected to reach USD 240B by 2028. Smart manufacturing can be described as a collaborative and integrated production system, capable of responding in real time to changing demands. It resonates with the Industry 4.0 as well as the Industry 5.0 frameworks [1]. The manufacturing industry is increasingly adopting the augmented reality (AR) technology as it improves operational efficiencies. AR-aided remote assistance can bridge the gap between field workers and experts. It improves workforce productivity, reduces production downtime, eases access to data for analytics, and simplifies training. Time-sensitive Networking (TSN) provides deterministic, low-latency, and highly reliable communication over standard Ethernet networks [2]. With its precise timing and synchronization capabilities, TSN is poised to revolutionize various industries, including manufacturing, automotive, and energy. The IEEE 802.1Qbv TSN standard introduces time-aware scheduling, allowing for the reservation of time slots for critical traffic [3]. It also ensures that high-priority traffic is transmitted at specific times without interference from best-effort traffic. These features of IEEE 802.1Qbv are particularly promising for industrial automation. The fifth generation (5G) mobile/cellular technology is designed to provide faster speeds, lower latency, and higher capacity. The ultra-reliable low-latency communication (uRLLC) capability of 5G provides millisecond-level latency which is crucial for real-time applications such as autonomous driving, and remote surgery, and smart manufacturing. With network slicing, it ensures dedicated resources and performance guarantees for different services [4]. Integrating TSN and 5G leverages the strengths of both technologies, providing ultra-reliable, low-latency communication as well as timely and reliable data transmission over hybrid wired/wireless segments. It also supports applications requiring both high mobility and precise timing, typically found in smart manufacturing. Integration and converged operation of the two technologies is an important step in the evolution of industrial networks. From the perspective of Industry 4.0, such integration provides design simplifications by flattening the automation pyramid. From the perspective of Industry 5.0, it enables human-centric applications over converged wired and wireless segments, paving the way for advanced manufacturing operations. I-B Demonstration Overview and Distinguishing Aspects To this end, this demonstration aims at showcasing the benefits of integrating 5G and TSN technologies for smart manufacturing through AR-aided remote assistance use-case. The demonstration is based on our end-to-end hybrid 5G and TSN setup, which been used for evaluating IEEE 802.1Qbv scheduled traffic [5], and off-the-shelf TSN, 5G, and AR devices, along with a remote assistance solution. We demonstrate real-time operation of AR-aided remote assistance between a local user and a remote expert over a hybrid 5G and TSN system, along with end-to-end latency performance of the system. Prior demonstrations at similar venues have mainly shown the capabilities of private 5G, network slicing, or TSN features. To the best of our understanding, there has been no demonstration showing a real application over a hybrid wireless (5G) and wired (TSN) system. Demonstrating an AR-aided remote assistance use-case over 5G or TSN is a unique aspect as well. Figure 1: Demonstration setup for AR-aided remote assistance use-case over a hybrid 5G and TSN system."
https://arxiv.org/html/2411.04159v1,Cooperation and Personalization on a Seesaw: Choice-based FL for Safe Cooperation in Wireless Networks,"Federated learning (FL) is an innovative distributed artificial intelligence (AI) technique. It has been used for interdisciplinary studies in different fields such as healthcare, marketing and finance. However the application of FL in wireless networks is still in its infancy. In this work, we first overview benefits and concerns when applying FL to wireless networks. Next, we provide a new perspective on existing personalized FL frameworks by analyzing the relationship between cooperation and personalization in these frameworks. Additionally, we discuss the possibility of tuning the cooperation level with a choice-based approach. Our choice-based FL approach is a flexible and safe FL framework that allows participants to lower the level of cooperation when they feel unsafe or unable to benefit from the cooperation. In this way, the choice-based FL framework aims to address the safety and fairness concerns in FL and protect participants from malicious attacks.","The growing scale and heterogeneity of wireless networks have boosted the applications of data-driven artificial intelligence (AI) algorithms in wireless networks. Federated learning (FL) is an innovative distributed AI technique that allows participants to train models cooperatively without sharing data. It has been rapidly promoted in recent years by the research community with the benefits of privacy preservation and the utilization of on-device intelligence [zhang2023device]. However, the practical application of FL in wireless networks is still in its infancy. It has not yet gained widespread acceptance in the industry, mainly due to the security concerns and inherent problems of the cooperative mechanism. The distributed and open system architecture introduces new threats and makes FL models more vulnerable to poisoning attacks compared with centralized machine learning models [zhang2023distributed]. Several existing studies explore how to defend against vulnerabilities and enhance the security of FL models. Some commonly used defense schemes include anomaly detection, trusted execution authentication, and robust model aggregation. Nevertheless, these state-of-the-art defense techniques have adopted a sporadic approach, incorporating add-on techniques into the FL framework in a fix-and-patch manner. In addition, the attackers are becoming intelligent and can learn to adapt to existing defense schemes [li2022learning]. This makes defenses more difficult, especially without knowing the type and principle of attacks. Therefore, there is still great potential to improve and promote the security level of FL in wireless communications. Figure 1: The examples of tunable cooperation in FL and their applications to FL-based wireless networks. The left part shows the applications and vulnerabilities of FL in wireless networks. The right part shows examples of tunable cooperation in existing personalized FL techniques. By changing the parameters related to the cooperation degree, given personalized FL techniques will convert between fully cooperative FL and fully personalized independent learning. Personalized FL schemes have been proposed by academia as effective solutions to the problem of local data and problem-setting heterogeneity in FL. A thorough survey is presented in [tan2022towards] on recent advances in personalized FL and gives a taxonomy of personalized FL techniques in terms of personalization strategies. However, most existing studies focus on describing detailed implementations of different personalized FL approaches, with limited exploration of the commonalities in these designs. After an exhaustive investigation of the personalized FL, we propose a new understanding of the essence of existing personalized FL frameworks. To the best of our knowledge, this is the first time that the tunable cooperation concept in personalized FL is proposed and discussed. In our view, the traditional FL schemes create a fixed cooperative paradigm for multiple independent participants. Participants must be fully committed to the cooperation and share the same cooperation output. On this basis, personalized FL methods create more flexible cooperative paradigms by introducing parameters that regulate the closeness degree of cooperation between participants. Therefore, we conclude that cooperation and personalization in FL are two ends of a seesaw. By controlling the parameters representing the degree of personalization and cooperation, it is possible to control which side of the seesaw is higher according to the problem and environment settings, thus realizing tunable cooperation in FL. Motivated by the above ideas, we introduce a flexible choice-based FL framework that dynamically adjusts the degree of cooperation and personalization of participants during the use of FL. It is also a more equitable and safer framework that allows participants to not cooperate when they feel unsafe or unable to benefit from cooperation. Note that, our choice-based FL can not only be used to solve the heterogeneity problem of traditional FL but also can be applied to noise and security concerns. When the system is under attack, participants will be more inclined to personalized training to avoid interference from attacks and noise. Through these structural changes, FL is given a higher degree of flexibility and can be more widely used in wireless networks. The contributions of this work are three-fold: First, we present an up-to-date survey of FL in wireless networks and illustrate the benefits and concerns of FL. Second, we provide a new perspective on the commonalities of existing personalized FL algorithms and propose the concept of tunable cooperation. Based on this concept, we introduce a flexible choice-based FL framework that solves major concerns of FL, including heterogeneity, fairness, and poisoning attacks. Third, we provide a case study of implementing a choice-based FL framework with clustering and knowledge distillation scheme under a cell sleep control scenario. By testing the framework with heterogeneous data and intelligent attacks, we prove the superiority of choice-based FL in terms of robustness and security. The remainder of this work is organized as follows. Section II explains the benefits and vulnerabilities of FL in wireless networks. Section III presents the tunable cooperation concept in personalized FL. Section IV introduces the choice-based FL framework and shows how it solves the vulnerabilities of FL. Section V demonstrates our case study and Section VI concludes the paper."
https://arxiv.org/html/2411.04139v1,Diffusion-based Auction Mechanism for Efficient Resource Management in 6G-enabled Vehicular Metaverses,"The rise of 6G-enable Vehicular Metaverses is transforming the automotive industry by integrating immersive, real-time vehicular services through ultra-low latency and high bandwidth connectivity. In 6G-enable Vehicular Metaverses, vehicles are represented by Vehicle Twins (VTs), which serve as digital replicas of physical vehicles to support real-time vehicular applications such as large Artificial Intelligence (AI) model-based Augmented Reality (AR) navigation, called VT tasks. VT tasks are resource-intensive and need to be offloaded to ground Base Stations (BSs) for fast processing. However, high demand for VT tasks and limited resources of ground BSs, pose significant resource allocation challenges, particularly in densely populated urban areas like intersections. As a promising solution, Unmanned Aerial Vehicles (UAVs) act as aerial edge servers to dynamically assist ground BSs in handling VT tasks, relieving resource pressure on ground BSs. However, due to high mobility of UAVs, there exists information asymmetry regarding VT task demands between UAVs and ground BSs, resulting in inefficient resource allocation of UAVs. To address these challenges, we propose a learning-based Modified Second-Bid (MSB) auction mechanism to optimize resource allocation between ground BSs and UAVs by accounting for VT task latency and accuracy. Moreover, we design a diffusion-based reinforcement learning algorithm to optimize the price scaling factor, maximizing the total surplus of resource providers and minimizing VT task latency. Finally, simulation results demonstrate that the proposed diffusion-based MSB auction outperforms traditional baselines, providing better resource distribution and enhanced service quality for vehicular users.","With the rapid development of Sixth Generation (6G) technology, the concept of 6G-enabled Vehicular Metaverses is revolutionizing intelligent transportation systems by offering ultra-reliable, low-latency communication, massive connectivity, and advanced network capabilities that seamlessly link the physical and virtual worlds[1]. Vehicle Twins (VTs) are digital replicas of physical vehicles within 3D virtual environments, offering accurate lifecycle representations and managing vehicular applications like large Artificial Intelligence (AI) model-based Augmented Reality (AR) navigation [2]. In 6G-enabled Vehicular Metaverses, vehicular users can receive large AI model-based vehicular services called VT tasks by constructing and updating the VTs in nearby terrestrial infrastructure like ground Base Stations (BSs). Given limited coverage of ground BSs and rapid movement of vehicles, a single ground BS is unable to continuously provide immersive vehicular services to vehicular users [3]. Consequently, to ensure an uninterrupted and immersive vehicular user experience, VT tasks must be migrated to the next ground BS as the vehicle is about to exit the coverage of the current ground BS [4]. Recently, integrating Vehicular Metaverses into urban environments represents a significant advancement in immersive experiences, where users can interact with digital content in real time through large AI model-based vehicular systems [5]. However, as the scale of the Vehicular Metaverse expands and the number of vehicular users increases, a critical challenge emerges when VT tasks are migrated, particularly large AI model-based VT tasks, i.e., network resource limitation. This issue becomes particularly prominent in areas with high vehicular user density, such as intersections, urban zones, and commercial zones, where seamless communication and immersive Vehicular Metaverse services are required. To address these challenges, air-ground integrated networks, composed of Unmanned Aerial Vehicles (UAVs) and ground BSs, provide dynamic resource allocation, expanding network coverage and alleviating communication bottlenecks in densely populated zones [6, 7]. Therefore, the capability of air-ground integrated networks to dynamically manage resources can particularly enhance service quality in user-dense regions, offering timely responses and improving the quality of experience of vehicular users. In air-ground integrated networks, ground BSs and UAVs serve as critical resource providers, delivering computing and communication resources to support executing VT tasks migration within 6G-enable Vehicular Metaverses [8]. These VT task migrations often require continuous and high-quality resource allocation, especially for large AI model-based VT tasks, which is essential for maintaining an immersive experience for vehicular users. However, the dynamic nature of resource demand in vehicular environments presents challenges, e.g., the information asymmetry between ground BSs and UAVs regarding the demand for VT task migration. Specifically, ground BSs can utilize historical data to accurately predict and assess the resource requirements of vehicular users with their fixed infrastructures and stable operations [9]. Although UAVs can offer flexible and dynamic coverages, they face challenges in collecting real-time feedback due to their mobility. Therefore, this mobility results in delayed information gathering (i.e., the demand for VT task migration), leading to less precise estimations of resource valuation compared to ground BSs [10]. To effectively address the resource allocation challenges in air-ground integrated networks, where ground BSs and UAVs serve as critical resource providers for vehicular users executing VT tasks in Vehicular Metaverses, it is essential to design mechanisms that mitigate the information asymmetry between these resource providers [11]. Traditional auction mechanisms, such as first-price or second-price auctions, often lead to inefficient resource allocation due to strategic bidding and adverse selection, which is not suitable for solving the resource allocation challenges in air-ground integration networks. The Modified Second Bid (MSB) auction offers significant advantages by incorporating a price scaling factor that ensures more efficient resource pricing and allocation. As introduced in [12], the MSB auction is strategy-proof and effectively eliminates adverse selection, making it a robust solution in environments characterized by information asymmetry [13]. Therefore, we propose a new MSB auction mechanism that evaluates resource allocation based on two key metrics, i.e., the latency of processing VT tasks and the precision of these VT task outcomes (e.g., resolution or pixel accuracy), which represent the common value and match value of resource provider, respectively. The rise of Generative Artificial Intelligence (GAI) presents transformative potential, surpassing conventional AI paradigms by incorporating a wide range of models and methodologies, e.g., Transformer, Generative Adversarial Networks (GANs), and Generative Diffusion Models (GDMs) [14]. Among these, GDMs are particularly noteworthy due to their unique approach to data generation and their ability to model complex data distributions with high precision [15]. By incorporating the diffusion model and Reinforcement Learning (RL), we propose a Diffusion-based RL algorithm, which is a generative approach that excels at learning and representing intricate environments through noise injection and denoising processes [14]. In this paper, we employ the Diffusion-based RL algorithm to solve the MSB auction problem, optimizing the price scaling factor to maximize the surplus of resource providers and minimize the latency of VT tasks. The main contributions of this paper are summarized as follows. • We propose a new resource allocation framework to address information asymmetry between UAVs and ground BSs due to the demand for VT task migration, providing an immersive experience for vehicular users in 6G-enabled Vehicular Metaverses. • In this framework, we design a new MSB auction, considering the VT task latency and the accuracy of these VT tasks (i.e., pixel points or resolution). Additionally, we prove that the proposed MSB auction is strategy-proof and effectively eliminates adverse selection, making it a robust solution in environments characterized by information asymmetry. • Unlike traditional DRL, we propose a Diffusion-based RL algorithm that has powerful learning and characterization capabilities, applying the MSB auction system to maximize the total surplus of resource providers and minimize the average latency of VT tasks. The rest of this paper is organized as follows. In Section 2, we provide a review of related works. Section 3 presents the system model for handling large AI model-based VT tasks within 6G-enabled Vehicular Metaverses. In Section 4, we formulate the problem and propose the market design. In Section 5, we introduce the proposed Diffusion-based MSB auction. Section 6 discusses the simulation setup and results, followed by the conclusion in Section 7."
https://arxiv.org/html/2411.04138v1,NetworkGym: Reinforcement Learning Environments for Multi-Access Traffic Management in Network Simulation,"Mobile devices such as smartphones, laptops, and tablets can often connect to multiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously. Recent advancements facilitate seamless integration of these connections below the transport layer, enhancing the experience for apps that lack inherent multi-path support. This optimization hinges on dynamically determining the traffic distribution across networks for each device, a process referred to as multi-access traffic splitting. This paper introduces NetworkGym, a high-fidelity network environment simulator that facilitates generating multiple network traffic flows and multi-access traffic splitting. This simulator facilitates training and evaluating different RL-based solutions for the multi-access traffic splitting problem. Our initial explorations demonstrate that the majority of existing state-of-the-art offline RL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average. This illustrates the urgent need to evaluate offline RL algorithms against a broader range of benchmarks, rather than relying solely on popular ones such as D4RL. We also propose an extension to the TD3+BC algorithm, named Pessimistic TD3 (PTD3), and demonstrate that it outperforms many state-of-the-art offline RL algorithms. PTD3’s behavioral constraint mechanism, which relies on value-function pessimism, is theoretically motivated and relatively simple to implement.","There exists a general lack of standardized benchmarks for reinforcement learning (RL) in the domain of computer networking. Whereas RL has shown promise in addressing various challenges in computer networking, such as congestion control, routing, and resource allocation, the field lacks widely accepted benchmarks that would facilitate systematic evaluation and comparison of different RL approaches. Hence, we propose NetworkGym, a high-fidelity, end-to-end, full-stack network Simulation-as-a-Service framework that leverages open-source network simulation tools, such as ns-3 Henderson et al. (2008). Furthermore, NetworkGym offers a closed-loop machine learning (ML) algorithm development and training pipeline via open-source gym-like APIs. The components of NetworkGym achieve the following objectives: • Open APIs for ML Training and Data Collection: The Agent is fully customizable and controlled by the developer. The network simulation Environment is hosted in the cloud. By utilizing the open-source NetworkGym Client and APIs, an Agent can interact with an Environment to collect measurement data and take actions that allow training for the desired use case. • Flexibility of Programming Language: The separation of Agent and Environment provides the freedom to employ different programming languages for the ML algorithm and network simulation. For instance, a Python-based Agent can smoothly interact with a C++ (ns-3) based simulation Environment. This is a critical aspect of our framework, as previous networking frameworks would have required modern ML algorithms to be coded in the same language(s) as the simulation environment. • Independent and Modular Deployment: Such separation also allows the Agent and Environment to be deployed on different machines or platforms, optimized for specific workloads. For example, when training online on-policy algorithms, such as PPO Schulman et al. (2017) and SAC Haarnoja et al. (2018a, b), it is often critical to parallelize environment instances to accelerate training and improve generalization capability Wijmans et al. (2019); Makoviychuk et al. (2021). This would be difficult to accomplish if the Agent and Environment were coupled. They can also be developed and maintained by different entities. Access to the Environment is controlled through NetworkGym APIs to hide the details of how a network function or feature is implemented from developers. Figure 1: GMA Protocol. A UE interfaces with the GMA gateway over UDP. ""APP"" refers to the application layer at the client or server level, ""IP"" refers to the Internet Protocol layer, facilitating the addressing and routing of packets, and ""PHY"" refers to the physical layer in the network responsible for the actual transmission of data over the network medium. The GMA gateway handles multi-access traffic splitting at the edge. Motivation from Computer Networking. As the mobile industry evolves toward 6G, it is becoming clear that no single access technology will be able to meet the great variety of requirements for human and machine communications. Multi-access traffic management for integrating multiple heterogeneous wireless networks, e.g., Wi-Fi, cellular, satellite, etc., into a virtualized and unified network becomes vital for addressing today’s ever-increasing performance requirements and future applications. Recently, the Generic Multi-Access (GMA) protocol has been proposed in the Internet Engineering Task Force (IETF) to address this need Zhu and Zhang (2024), and the 3rd Generation Partnership Project (3GPP) has also developed the access traffic steering, switching, and splitting (ATSSS) feature, which enables simultaneous use of one 3GPP and one non-3GPP connection to deliver data flows ats (2018). We defer more technical details of these protocols to Appendix A. One effective method for managing multi-access traffic is through traffic splitting between different network types. Specifically, for each user equipment (UE), traffic is allocated between a 3GPP connection (e.g., LTE) and a non-3GPP connection (e.g., Wi-Fi), with the ratio adjusted at frequent intervals, as in Figure 1. It is natural to consider using RL for learning adaptive and data-driven decision policies on the traffic-splitting ratios. Applying RL, however, is notoriously hard. One may run online RL on real networks, but the initial decisions made by the algorithms can be suboptimal, leading to poor network traffic splitting and diminished user experience. Notably, in applications such as robotic control over networks, it is critical to ensure high reliability and low packet-loss ratios to maintain operational effectiveness. An alternative is to run offline RL on the logged data from real networks, but data coverage is a big issue. Even if the learned policy from offline RL improves over the baseline, one cannot know for sure until testing it online with real network traffic. Moreover, the networking environment is not static and most challenging scenarios occur in the long tail of the data distribution. NetworkGym is timely as it allows us to not only evaluate any learned RL policies, but also stress-test them in challenging scenarios. One could also use NetworkGym to simulate the entire workflow of offline RL for policy improvement before deploying the workflow on real networks. Frictionless Reproducibility for ML Researchers interested in Computer Networking. The intended use of NetworkGym is to allow machine learning (especially RL) researchers to evaluate their algorithms on a faithfully simulated environment in computer networking without having to understand the intricate networking protocols and their interactions in a multi-access traffic splitting system. To facilitate “frictionless reproducibility” Donoho (2024), we conduct preliminary experiments on NetworkGym with popular offline RL algorithms and make the code to setup such experiments available. Our results provide the following take-home messages: • Offline RL for Policy Improvement. Offline RL algorithms can effectively improve the performance in networking systems using data collected from three behavioral policies. • Transferability of Scientific Advances. Methods that work well on standard OpenAI gym environments may not work well on networking problems. Comparative advantages of State-of-the-Art algorithms on D4RL Fu et al. (2020) do not transfer to NetworkGym. • Details matter. Seemingly arbitrary choices in the parameterization and state/action representation (e.g., normalization) have more substantial impact than the choice of RL algorithms. • Success of principles. “Pessimism” in offline RL works for networking problems. A more theory-inspired pessimistic bonus is more effective than the popular Behavioral Cloning (BC). We hope NetworkGym lowers the entry-barrier into computer networking research and enables new collaboration in the emerging research area of machine learning for networking across academia and industry."
https://arxiv.org/html/2411.04137v1,Generative AI Enabled Matching for 6G Multiple Access,"In wireless networks, applying deep learning models to solve matching problems between different entities has become a mainstream and effective approach. However, the complex network topology in 6G multiple access presents significant challenges for the real-time performance and stability of matching generation. Generative artificial intelligence (GenAI) has demonstrated strong capabilities in graph feature extraction, exploration, and generation, offering potential for graph-structured matching generation. In this paper, we propose a GenAI-enabled matching generation framework to support 6G multiple access. Specifically, we first summarize the classical matching theory, discuss common GenAI models and applications from the perspective of matching generation. Then, we propose a framework based on generative diffusion models (GDMs) that iteratively denoises toward reward maximization to generate a matching strategy that meets specific requirements. Experimental results show that, compared to decision-based AI approaches, our framework can generate more effective matching strategies based on given conditions and predefined rewards, helping to solve complex problems in 6G multiple access, such as task allocation.","The matching problem is an important branch of combinatorial optimization, typically studied in graph theory. It has extensive research and applications in real-world scenarios, such as task scheduling, market matching, network design, and resource allocation. For example, in the transportation and supply demand field, passengers are matched with suitable vehicles and routes to maximize potential expected revenue [1]. In market matching, determining how to match buyers and sellers to achieve optimal transactions directly affects the efficiency and fairness of the market. The widespread application of the matching problem demonstrates its value as a crucial method for addressing practical challenges, enhancing resource utilization, and promoting sustainable social development. In the context of 6G multiple access, such as non-orthogonal multiple access (NOMA) and rate-splitting multiple access (RSMA), the importance of studying the matching problem is self-evident due to the needs for user coordination and interference management within the network. For instance, in NOMA-assisted wireless networks, user equipment and channel resources can be abstracted as nodes, and their allocation relationships as edges. Efficient matching methods can thus be used to manage and reduce interference, improving overall communication quality and system capacity [2]. Therefore, applying matching methods can serve as a crucial tool for resource management, performance optimization, and interference coordination. A considerable body of literature has delved into the matching problem in wireless networks, primarily focusing on stable matching and those methods that generate adaptive matching strategies using machine learning and deep learning models. However, these traditional approaches often face the following challenges and limitations. • High Complexity. When dealing with large-scale and highly complex systems (such as RSMA networks or edge computing systems), the complexity of traditional matching algorithms increases sharply due to the large number of players involved in the matching process, making real-time solutions unattainable [3]. • Dependence on complete information. Stable matching and deep learning-aided methods typically assume that participants can provide a complete and fixed preference list [4]. However, in practice, participants may struggle to fully express their preferences, making traditional methods that rely on complete information difficult to apply. • Low exploration and convergence speed. Decision-making AI, such as deep reinforcement learning (DRL), typically rely on repeated exploration and feedback for policy optimization, resulting in a training process that often requires numerous iterations to find a stable matching strategy [5]. Generative artificial intelligence (GenAI) can learn the distribution characteristics of a given graph and generate new graphs based on changes in external conditions. These graphs can be viewed as forms of matching, where nodes represent matching objects and edges represent associations. Therefore, utilizing GenAI to construct graph-based matching provides a new approach to addressing these challenges. In 6G multiple access wireless networks, applying GenAI to generate matching strategies involves several key steps [6]. First, it is necessary to collect data on network status, user demands, and channel conditions, and preprocess this data to ensure its quality and the effectiveness of model training. Next, using this data, train the selected generative models, such as variational auto-encoders (VAEs), generative adversarial networks (GANs), Transformers, and generative diffusion models (GDMs), enabling them to learn the latent associations between nodes and generate matching strategies based on graph theory. Using the trained generative models, generate new matching graphs according to the current network topology and user characteristics. The matching results represented by these graphs plays a crucial role in the design and optimization of multiple access networks. On one hand, it can flexibly and efficiently manage interference and decode user noise to minimize inter-user interference, thereby enhancing system reliability and communication rate performance. On the other hand, match generation can optimize resource allocation and adjust network load to enhance communication network performance and achieve load balancing. These benefits are essential for creating more efficient and reliable wireless networks. Considering the widespread application of matching problems in multiple access and the potential solutions offered by GenAI, this paper explores the application of GenAI-assisted matching generation methods in 6G multiple access wireless networks. First, we extensively summarize the applications of matching problems in various fields such as task allocation and next generation multiple access. Then, we analyze and discuss the main GenAI models used for matching generation. Finally, we propose a matching generation framework based on diffusion models and illustrate how this framework supports image generation and wireless transmission in RSMA networks through a case study. The main contributions of this paper are summarized as follow. • We discuss the basic matching theory, and introduce commonly used GenAI models centered around the goal of matching generation, including their implementation principles, strengths, and weaknesses. This foundational knowledge reviews how different GenAI models can be used to generate efficient matching strategies. • We investigate the applications of matching generation in multiple areas such as potential drug targets prediction and task offloading in vehicular networks, providing a comprehensive summary and analyzing the advantages of GenAI-based matching methods in 6G multiple access schemes. • We propose a effective matching generation framework based on generative diffusion models and demonstrate its effectiveness through a case study of AIGC service provider selection problem in RSMA-enabled wireless networks."
https://arxiv.org/html/2411.04136v1,"Large Language Models (LLMs) for Wireless Networks: An Overview from the Prompt Engineering Perspective††thanks:Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Xi Chen, and Xue Liu are with the School of Computer Science, McGill University, Montreal, QC H3A 0E9, Canada. (mails:hao.zhou4, chengming.hu, dun.yuan, ye.yuan3, xi.chen11@mail.mcgill.ca, xueliu@cs.mcgill.ca);
Di Wu is with the School of Electrical and Computer Engineering, McGill University, Montreal, QC H3A 0E9, Canada. (email: di.wu5@mcgill.ca);
Hina Tabassum is with the Department of Electrical Engineering and Computer Science at York University, Toronto, ON M3J 1P3, Canada. (e-mail:hinat@yorku.ca).","Large language models (LLMs) have been successfully applied to many fields, showing outstanding comprehension and reasoning capabilities. Despite their great potential, LLMs usually require dedicated fine-tuning for domain-specific applications such as wireless networks. Such domain adaptation can be extremely demanding for computational resources, while many network devices have limited computation capacity. To this end, this work explores LLM-enabled wireless networks from the prompt engineering perspective, i.e., designing input prompts to guide LLMs to generate desired output. Compared with other approaches to using LLMs, prompt engineering is more flexible and resource-efficient, which better aligns with wireless network features. In particular, we first introduce LLM fundamentals and compare different prompting techniques such as in-context learning, chain-of-thought, and self-refinement. Then we propose two novel prompting schemes for network applications, namely iterative prompting for wireless network optimization, and self-refined prompting for network prediction. The case study shows that the proposed schemes can achieve comparable performance as conventional machine learning techniques, and our prompting-based methods avoid the complexity of dedicated model training and fine-tuning, which is one of the key bottlenecks of existing machine learning techniques.","TABLE I: Summary of various LLM usage approaches for wireless networks. Approaches & Steps Main features Advantages Potential difficulties Computational resources & Time costs Possible wireless network applications Pre-training LLMs Training LLMs from scratch on large-scale datasets, typically involving hundreds of billions of tokens for the next token prediction. It is a foundational step of LLM development and usage, including dataset collection and model training. Pre-training enables LLMs fundamental and crucial capabilities, such as comprehension, reasoning, zero-shot generation, and instruction following, which can address various downstream tasks. Pre-training is extremely resource-intensive, requiring massive labelled data, long training times, substantial energy consumption, and a tremendous amount of hardware accelerators like GPUs or TPUs. High Thermal Design Power GPUs such NVIDIA H100100100100-80808080G or A100100100100-80808080G are required. For example, training Llama3.13.13.13.1-405405405405B took approximately 30.8430.8430.8430.84M GPU hours with 15151515T tokens from publicly available sources and 25252525M synthetic data. A network-specific LLM indicates great benefits, but training a network-specific LLM from scratch may be inappropriate due to the computational resource requirements. Fine-tuning Fine-tuning refers to adapting a pre-trained LLM to specific tasks by partially or completely updating the model weights or introducing additional adapter modules to fit smaller and domain-specific datasets. Fine-tuning significantly reduces time and computational cost compared to pre-training, producing LLM models tailored to specific tasks. Fine-tuning has to consider the risk of overfitting and losing parts of the generalizability. Labelled data or pairwise comparison data may be needed, involving additional human labour force. The cost may vary between model sizes and fine-tuning methods. Typically, if full fine-tuning a 7B parameter model with 8 A100100100100-80808080G GPUs, it could be 100100100100-200200200200 GPU hours on a 100100100100M token dataset (assuming a maximum length of 512512512512 tokens). Fine-tuning is a realistic approach to apply LLMs to wireless networks, adapting a general-domain LLM to specific tasks such as network troubleshooting, project coding, and network configuration. Retrieval augmented generation (RAG) RAG combines LLMs with an external knowledge base, enabling the model to retrieve relevant information during inference. It enhances LLM’s capability to provide accurate and up-to-date responses, particularly in knowledge-intensive tasks. RAG can improve LLM’s response accuracy by leveraging external knowledge sources. It is particularly useful when the model’s training data is outdated or lacks specific information. RAG involves extra integration complexity, as it requires efficient retrieval algorithms since the inference latency may increase if the retrieval process is not optimized. Moreover, RAG highly depends on a well-structured knowledge base, which cannot be easily created and maintained. The computational cost depends on the retrieval method and the size of the knowledge base. Inference may take seconds to minutes per query, depending on the size of the base language model. Requires a balance between retrieval speed and accuracy, which can influence the choice of hardware, e.g., using faster storage for the knowledge base. Considering the cost of fine-tuning, RAG is a more practical approach for implementing LLMs, especially when existing network-specific datasets are available. It ensures the LLM has access to the most relevant and up-to-date network data, improving its accuracy for addressing the latest network tasks. Prompt engineering Prompting engineering refers to crafting specific input prompts to guide a pre-trained LLM to generate desired outputs. It leverages the existing capabilities of the LLM without any model retraining or tuning. Prompt engineering has low computational costs and quick implementation. It is also flexible and can be easily adapted to different tasks without additional training. It requires expertise in crafting effective prompts, as the quality of the output heavily depends on how the prompt is formulated. It is also limited by the inherent capabilities of the pre-trained LLMs. It has minimal computational costs and often requires just a single GPU or a few GPUs. Crafting and experimenting with different prompts may take only a few hours. Time costs are significantly lower compared to other methods, making it suitable for rapid prototyping and testing. Prompt engineering enables rapid decision-making and responses in dynamic wireless network environments. It aligns well with many network tasks, especially considering the limited computational resources of network devices. 1 Note that these approaches can be combined to achieve better performance, e.g., fine-tuning network LLMs and then prompting. As a sub-field of generative AI, large language models (LLMs) have received considerable interest from industry and academia [1]. After being trained on a large number of datasets, LLMs have shown revolutionary comprehension and reasoning capabilities. The advancement of generative AI and LLMs also provides promising opportunities for 6G wireless networks, including integrated satellite-aerial-terrestrial networks [2], vehicular networks [3], semantic communication [4], etc. Despite the great potential, integrating LLMs into wireless networks still faces several challenges. Firstly, wireless networks are complex large-scale systems with various knowledge domains, i.e., signal processing and transmission, network architecture and design, protocol, standards, and so on. Applying general-domain LLMs directly to domain-specific network tasks may lead to poor performance. Secondly, LLMs are extremely demanding for computational resources. LLM pre-training and fine-tuning are usually implemented on high-performance GPUs such as NVIDIA A100 and H100. However, wireless network devices usually have limited computational and storage capacities, which may prevent practical LLM network applications. In addition, LLMs can be applied in various approaches, including pre-training models from scratch, fine-tuning LLMs for domain-specific tasks, retrieval augmented generation (RAG), prompt engineering, etc. Each method has unique features and potential difficulties, and it is critical to identify the most efficient methods to adapt LLMs to wireless networks. Given the above opportunities and challenges, this work focuses on prompt engineering, which is regarded as one of the most resource-efficient approaches to using LLMs. In particular, prompting refers to designing input prompts to guide pre-trained LLMs to generate desired outputs. It takes advantage of the inherent inference capabilities of pre-trained LLMs, and avoids the complexity of dedicated model training or fine-tuning. Compared with other approaches, prompting-based techniques have several key advantages: 1) Resource-efficient: Compared with pre-training and fine-tuning, prompt engineering techniques have much lower requirements on computational resources like a single GPU. Such a resource-efficient approach can be easily deployed in many network devices. 2) Higher flexibility: Prompting-based methods can quickly adapt to various tasks by crafting the corresponding demonstrations and queries. It indicates an efficient method to customize LLMs to address a wide range of network tasks. 3) Fast implementation: Prompt engineering relies on the inference capabilities of LLMs, and it avoids the time cost of updating LLM parameters. Therefore, the low response time can better handle low-latency services in wireless networks. LLMs have been discussed in several existing studies, but they mainly focus on system-level discussions and module designs, e.g., edge intelligence [5], grounding and alignment[6], 6G security[7], and our previous work also investigates LLM-enabled power control and traffic prediction [8, 9]. However, this work is different from existing studies by systematically exploring prompt engineering and wireless network applications, providing detailed prompt designs and specific case studies. The main contributions include: 1) Firstly, it presents in-depth analyses of LLM fundamentals such as pre-training, fine-tuning, RAG, and prompt engineering. We compare these techniques regarding main features, computational resources and time costs, potential difficulties, and wireless network applications. Then, it overviews multiple prompting techniques, such as in-context learning, chain-of-thought, prompt-based planning, and self-refinement, and we further investigate how various prompting techniques can adapt to wireless networks in terms of main features, advantages, and potential issues. 2) We propose two novel prompting techniques, namely iterative prompting and self-refined prompting, aiming to address network optimization and prediction problems, respectively. Specifically, iterative prompting can learn from previous experience and exploration to improve LLM’s performance on target tasks iteratively, which is mainly designed for network optimization problems. By contrast, self-refined prompting allows them to correct their outputs through iterative feedback and refinement demonstration prompts, aiming to address network prediction tasks. These schemes rely on LLM’s inference capabilities, and such resource-efficient techniques align well with the limited computational capacities of network devices. In addition, our proposed algorithms can be easily generalized to various wireless network applications, and the case studies show that they achieve satisfactory performance in network power control and traffic prediction problems."
https://arxiv.org/html/2411.04672v1,Semantic-Aware Resource Management for C-V2X Platooning via Multi-Agent Reinforcement Learning,"This paper presents a semantic-aware multi-modal resource allocation (SAMRA) for multi-task using multi-agent reinforcement learning (MARL), termed SAMRAMARL, utilizing in platoon systems where cellular vehicle-to-everything (C-V2X) communication is employed. The proposed approach leverages the semantic information to optimize the allocation of communication resources. By integrating a distributed multi-agent reinforcement learning (MARL) algorithm, SAMRAMARL enables autonomous decision-making for each vehicle, channel assignment optimization, power allocation, and semantic symbol length based on the contextual importance of the transmitted information. This semantic-awareness ensures that both vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications prioritize data that is critical for maintaining safe and efficient platoon operations. The framework also introduces a tailored quality of experience (QoE) metric for semantic communication, aiming to maximize QoE in V2V links while improving the success rate of semantic information transmission (SRS). Extensive simulations has demonstrated that SAMRAMARL outperforms existing methods, achieving significant gains in QoE and communication efficiency in C-V2X platooning scenarios.","I-A Background With the rapid development of intelligent transportation systems (ITS) [1, 2, 3], ensuring safe and efficient transportation is increasingly crucial [4, 5, 6]. A key component of ITS is platooning systems, where multiple autonomous vehicles travel closely together to enhance traffic flow and safety [7, 8, 9, 10, 11]. In these systems, a designated platoon leader (PL) manages formation, while platoon members (PMs) maintain coordinated speed and distance. Effective intra-platoon and inter-platoon communication (platoon-to-platoon or platoon-to-infrastructure) is essential for optimizing overall efficiency and safety. Integrating cellular vehicle-to-everything (C-V2X) communication is imperative for achieving effective communication. C-V2X supports two main types: vehicle-to-vehicle (V2V), which allows vehicles to share cooperative awareness messages (CAMs) for synchronized movement, and vehicle-to-infrastructure (V2I), enabling communication with base stations for traffic and safety information [12, 13, 14, 15, 16, 17, 18]. These modalities facilitate timely responses to dynamic traffic conditions, enhancing the safety and efficiency of platooning systems [19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]. However, increased connectivity and communication demands in C-V2X platooning introduce complexities in managing network resources, which are critical for reliable communication in autonomous driving [30, 31, 32, 33]. The frequency of information exchange among vehicles directly impacts their ability to react to obstacles, underscoring the need for efficient resource management [34, 35, 36, 37, 38]. A promising approach to these challenges is semantic communication [39], which focuses on transmitting meaningful information rather than raw data. This enhances data transmission efficiency in C-V2X systems by ensuring only relevant information is shared [40], improving decision-making and reducing unnecessary data transfer. As task complexity rises, there is a shift from unimodal to multi-modal tasks that integrate various data types [41, 42, 43, 44]. This integration fosters a richer understanding of the environment, enhancing network robustness and flexibility [45]. Traditional centralized resource management schemes often face inefficiencies and high signaling overhead due to their reliance on global information, particularly under dynamic channel conditions [46, 47]. To address these challenges, we propose a distributed semantic-aware multi-modal resource allocation (SAMRA) framework leveraging multi-agent reinforcement learning (MARL), termed SAMRAMARL111The source code has been released at: https://github.com/qiongwu86/Semantic-Aware-Resource-Management-for-C-V2X-Platooning-via-Multi-Agent-Reinforcement-Learning. By employing MARL, vehicles can make decentralized decisions based on local observations, reducing reliance on centralized control while enhancing scalability and adaptability to dynamic network conditions [48]. I-B Related Work and Motivation Resource allocation in platooning systems has garnered significant attention within intelligent transportation systems (ITS). Platooning involves multiple autonomous vehicles traveling in coordination, necessitating efficient communication between the platoon leader (PL), other platoon members (PMs), and infrastructure like base stations [49]. Various studies have explored traditional methods to address these challenges. For instance, Guo et al. [50] proposed a joint optimization approach for LTE-V2V radio resource allocation and vehicle control parameters to enhance platoon stability. Hong et al. [51] designed a framework using relays and adaptive distributed model predictive control (DMPC) to improve safety in failure scenarios. Wang et al. [52] developed a two-step resource allocation strategy optimizing platoon formation and power control through branch and bound methods. Wen et al. [53] focused on optimizing inter-vehicle communication topology in LTE-V2V networks. However, the dynamic nature of channel conditions complicates effective resource management due to uncertainties in estimating channel state information (CSI). To address these complexities, deep reinforcement learning (DRL) has emerged as a promising method for resource allocation in vehicular networks [54, 55, 56, 57]. For example, Liing et al. [54] utilized a multi-agent reinforcement learning (MARL) approach with deep Q-networks to enhance V2I capacity. Nasir et al. [55] proposed a model-free DRL-based power allocation scheme for wireless networks. Xu et al. [56] applied DRL for multi-objective resource allocation, focusing on transmission success and communication quality. Despite these advancements, distributed DRL still faces challenges in high-data-volume scenarios, leading to elevated signaling costs and delays, especially in dynamic environments [58]. In response, semantic communication has gained traction. Unlike traditional methods that focus on raw data transmission, semantic communication prioritizes the meaning behind the conveyed information. This approach can enhance coordination and safety in platooning systems [59]. For instance, Bourtsoulatze et al. [60] developed a method for efficient image transmission under low signal-to-noise ratio (SNR) conditions, while Huang et al. [61] used generative adversarial networks for semantic image compression. Moreover, integrating unimodal and multi-modal tasks in ITS is an important research area [62]. Unimodal tasks handle specific data types, while multi-modal tasks provide a comprehensive understanding of the environment [63]. However, existing studies often lack a unified resource management framework that leverages both semantic communication and the integration of unimodal and multi-modal tasks [64]. Despite these advancements, there remains a gap in the literature. Existing approaches have not effectively combined distributed DRL, semantic communication, and the handling of both unimodal and multi-modal tasks into a cohesive resource management framework. This paper proposes a novel approach to optimize resource management in platooning systems by leveraging the strengths of semantic communication and distributed decision-making. I-C Contributions In this work, we present a distributed semantic-aware multi-modal resource allocation (SAMRA) algorithm based on MARL, termed SAMRAMARL, tailored for platooning systems. The primary contributions of our research are summarized as follows: 1) We investigate the extraction of semantic and multi-modal information for C-V2X platooning systems, and redefine the metrics suitable for semantic and multi-modal data, as well as concept of quality of experience (QoE). 2) We formulate a joint optimization problem to maximize QoE and the success rate of semantic information transmission (SRS) in V2V links ,which is the first work to introduce semantic communication to address resource allocation challenges in platooning systems. 3) We design the SAMRAMARL algorithm, employing MARL to optimize various aspects of resource allocation, including channel assignment, power allocation, and the length of transmitted semantic symbols in both single-modal and multi-modal contexts. 4) Simulation results demonstrate that our SAMRAMARL algorithm significantly outperforms existing algorithms in terms of QoE and SRS. The rest of the paper is structured as follows: Section II introduces the system model and the formulated problem of maximizing QoE and SRS; Sections III present the proposed SAMRA algorithm; Section IV provides and discusses simulation results; and Section V concludes the paper."
https://arxiv.org/html/2411.04561v1,Joint wireless and computing resource management with optimalslice selection in in-network-edge metaverse system,"This paper presents an approach to joint wireless and computing resource management in slice-enabled metaverse networks, addressing the challenges of inter-slice and intra-slice resource allocation in the presence of in-network computing. We formulate the problem as a mixed-integer nonlinear programming (MINLP) problem and derive an optimal solution using standard optimization techniques. Through extensive simulations, we demonstrate that our proposed method significantly improves system performance by effectively balancing the allocation of radio and computing resources across multiple slices. Our approach outperforms existing benchmarks, particularly in scenarios with high user demand and varying computational tasks.","As technology advances, the new wave of networks is blending virtual and augmented realities with our real world. This creates a whole new space called the metaverse, where the digital and physical worlds come together [1]. Creating and maintaining the Metaverse requires enormous resources, including computing resources for intensive data processing to support the Extended Reality, storage resources, and communication resources for maintaining ultra-high-speed and low-latency connections [2]. Given these demands for resources, there is a need for a solution that efficiently manages and allocates these diverse types of resources to power the applications and functionalities within the Metaverse. Network slicing, where multiple end-to-end networks are created on shared physical infrastructure, solves this problem. Different network slices can be used for specific applications or services [3]. They can also scale up and down according to the service requirements, and network resources to the metaverse can be allocated according to the demand to facilitate the recommended QoE to the user while optimizing the resource usage in the network. The metaverse consists of a large-scale number of virtual worlds that require interoperability with each other. These virtual worlds must be rendered in real-time and synchronized with the physical world. The MEC provides a distributed infrastructure located near the user, however it fails to meet concurrent user demands, which results in high delay and severely affects the full realization of the metaverse[4]. According to Internet Research Task Force (IRTF) [5], the Computing in the network (COIN) research group, 6G edge nodes acting as task executors are not only purpose-built servers. They encompass any edge node augmented with computing resources. This offers a promising solution, utilizing untapped network resources for task execution, thereby diminishing latency and fulfilling quality of experience (QoE) requirements. However, augmenting computing resources or enabling COIN increases power consumption [6]. Effectively allocating COIN resources in real-time to adapt to dynamic user demands while ensuring overall system availability still presents a critical challenge. Despite the advancement in resource management problems, challenges persist. Network slicing and in-network computing have been extensively investigated in the literature to address such challenges. Previous approaches assume each application is dynamically assigned to a specific slice with a static resource pool based on workload and SLA requirements [7]. However, dynamic assignment leads to mixed workloads in slices and consequently demands flexibility in managing these resources. Jošilo et al. [8] tackled the resource management and dynamic assignment of slices, by solving complex algorithms in an edge computing system, but with the presence of in-network computing, where multiple nodes act as potential task executors, efficiently managing these resources still remains a challenge. Unlike the above works, the authors in [9] used a Water-Filling-based heuristic algorithm to address joint network slicing and in-network computing resource allocation. However, their focus was solely on managing resources between slices (inter-slice) without considering the resource management issues within the slices (intra-slice). In this paper, we address the joint network slicing, inter-slice radio, intra-slice radio and in-network resource management problem and make the following key contributions: • We formulated the problem as a mixed-integer non-linear programming problem (MINLP) and achieved the optimal solution through a standard optimization solver. • We performed an extensive evaluation under different load and task settings and compared it against some benchmark solutions."
https://arxiv.org/html/2411.04027v1,Prototyping O-RAN Enabled UAV Experimentation for the AERPAW Testbed,"The Open Radio Access Network (O-RAN) architecture is reshaping the telecommunications landscape by enhancing network flexibility, openness, and intelligence. This paper establishes the requirements, evaluates the design tradeoffs, and introduces a scalable architecture and prototype of an open-source O-RAN experimentation platform within the Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW), an at scale testbed that integrates unmanned aerial vehicles (UAVs) with advanced wireless network technologies, offering experimentation in both outdoor testbed and emulation via a custom digital twin (DT). Through a series of aerial experiments, we evaluate FlexRIC, an open-source RAN Intelligent Controller, within the AERPAW hardware-software platform for network data monitoring, providing valuable insights into the proposed integration and revealing opportunities for leveraging O-RAN to create custom service based optimizations for cellular connected UAVs. We discuss the challenges and potential use cases of this integration and demonstrate the use of a generative artificial intelligence model for generating realistic data based on collected real-world data to support AERPAW’s DT.","The advent of the Open Radio Access Network (O-RAN) architecture marked a significant advancement in telecommunications, enhancing flexibility, openness, and intelligence across diverse network configurations. O-RAN’s modular design, featuring open interfaces, supports real-time resource management and artificial intelligence (AI) driven optimizations, fostering interoperability and innovation. This allows operators to tailor networks to evolving demands, reduce operational costs, and enhance service delivery. Key components of the O-RAN architecture include the near-real time RAN Intelligent Controller (near-RT RIC) and the non-RT RIC. These components, O-RAN’s disaggregated RAN, and the open interfaces enhance network flexibility, reduce costs via competitive vendor markets, and improve network intelligence [1]. There are several efforts in the United States and the rest of the world to promote open RANs and O-RAN. One of the primary goals is vendor diversity, which spurs domestic deployment, allows customized network scaling with more control over cellular network technology and parameters, and supports technology adoption and evolution toward 6G in a given region or country. The Platforms for Advanced Wireless Research (PAWR) program in the United States plays a pivotal role in integrating O-RAN technologies into its diverse testbeds, providing critical infrastructure for exploring the practical applications of O-RAN in real-world settings. The PAWR program supports five Wireless Community Testbeds (WCTs) designed to provide a controlled environment where researchers and developers can experiment with and evaluate new wireless technologies, protocols, and applications. The Cloud Enhanced Open Software Defined Mobile Wireless Testbed for City-Scale Deployment (COSMOS) focuses on millimeter wave technology, providing an urban environment to explore advanced communications and collect data. The Platform for Open Wireless Data-Driven Experimental Research (POWDER) centers around sub-6 GHz deployments, offering a flexible, programmable environment for wireless experimentation and data collection. The Aerial Experimentation and Research Platform for Advanced Wireless (AERPAW) integrates wireless communications with aerial platforms, exploring protocols for aerial networks and gathering relevant data. The Agriculture and Rural Communities (ARA) testbed focuses on providing connectivity solutions for rural and underserved areas, supporting a range of wireless technologies for exploring innovative approaches for broader network coverage. Colosseum is a large-scale wireless network emulator that offers a highly configurable environment for testing and evaluating advanced wireless technologies. These platforms feature software-defined radio (SDR) hardware and software along with specialized radio equipment. In the realm of wireless research, the O-RAN architecture is reshaping how WCTs are used for experimentation and data collection as exemplified by O-RAN use cases in POWDER such as RAN slicing [2]. At this time COSMOS, POWDER, and Colosseum offer O-RAN experiments while AERPAW and ARA consider offering them in the future [3]. The existing O-RAN capabilities available through WCTs are either for an emulated radio frequency (RF) channel or with no or limited mobility [4]. AERPAW offers distinctive advanced wireless research features, facilitating controlled three-dimensional (3D) mobility in an at-scale outdoor testbed and in a digital twin (DT). While [4] provides insights into some of the essential requirements for O-RAN experimentation within AERPAW, it does not actualize the integration of O-RAN into the testbed. Unmanned aerial vehicles (UAVs) require adaptive networks, which O-RAN facilitates, for reliable connectivity. This paper outlines the design requirements and evaluates the available deployment choices before proposing an architecture and prototype for constructing a reproducible open-source O-RAN experimentation platform for advanced wireless UAV research. This platform furnishes the necessary interfaces and performance metrics to facilitate extensive research opportunities involving cellular network-connected UAVs. Our proposed platform lays the groundwork and the first practical integration for pioneering O-RAN experiments in AERPAW, enabling research on AI-driven UAV communication, network, and trajectory optimization [5]. The contributions of this work are: • We describe the tradeoffs and design choices for a testbed that combines O-RAN components with AERPAW’s capabilities, creating an open-source, experimental research platform for UAV communications (Section II). • We offer experimental 5G key performance indicators collected between a UAV and ground nodes and introduce a generative AI (GAI) method to generate new data samples based on collected data for AERPAW’s DT, enabling further research and analysis (Section III). • We identify the key challenges in integrating O-RAN research capabilities into AERPAW and propose practical solutions and future research and development (R&D) opportunities for O-RAN enabled UAV communications (Section IV)."
https://arxiv.org/html/2411.03943v1,Towards Achieving Energy Efficiency and Service Availability in O-RAN via Formal Verification,"As Open Radio Access Networks (O-RAN) continue to expand, AI-driven applications (xApps) are increasingly being deployed enhance network management. However, developing xApps without formal verification risks introducing logical inconsistencies, particularly in balancing energy efficiency and service availability. In this paper, we argue that prior to their development, the formal analysis of xApp models should be a critical early step in the O-RAN design process. Using the PRISM model checker, we demonstrate how our results provide realistic insights into the thresholds between energy efficiency and service availability. While our models are simplified, the findings highlight how AI-informed decisions can enable more effective cell-switching policies. We position formal verification as an essential practice for future xApp development, avoiding fallacies in real-world applications and ensuring networks operate efficiently.Keywords: Formal Verification Probabilistic Model Checking PRISM O-RAN xApp Energy Efficiency","The evolution of Open Radio Access Networks (O-RAN)111A table in Section Acronyms lists all acronyms in the paper. is accelerating with deployments by multiple operators, such as Vodafone, in worldwide locations [27, 4, 1], including the UK [34]. O-RAN integrates AI-based applications (xApps) to enhance the responsiveness and efficiency of network management [28]. Although machine learning models have been used to predict the behaviour of 3G, 4G, and 5G networks [32], O-RAN’s approach to AI integration in 6G significantly advances real-time network adaptability [10]. We do not explicitly model such AI-based predictors, but model the probabilistic nature of their output, as accompanied by some level of confidence. Precisely, we assume that AI-based xApp can learn probabilities for User Equipment (UEs) being on or off, and such probabilities are used to control the network. Traditional cellular networks often fail to capture real-world variability [39] and usually use static rule-based policies, such as indefinitely switching off radio cells (RCs) when no users are connected. These static policies cannot analyse dynamic behaviour, resulting in unclear situations for when and how long before reactivating RCs. This leads to issues with network availability, compromises in quality of service (QoS), and inefficiencies in power consumption. For example, highly frequent RC on/off can result in the deterioration of user-perceived delay, unnecessary frequent handovers of UEs, and higher costs associated with RC mode transitions.[29, 15] Achieving a balance between availability, QoS, and power efficiency has become a critical challenge for the communications industry. There have been significant contributions toward addressing these issues in different contexts [33, 24, 7, 31, 19]. Recent efforts, such as those in [4, 7], have explored how the O-RAN architecture can support power management and how implementing specific architectural approaches can drastically reduce energy consumption while meeting the requirements for QoS and service availability. Additionally, Zhang et al.[18] optimise strategies for switching RCs on and off, as well as user association policies, while ensuring users’ QoS is maintained. However, traditional optimisation methods often demand significant computing power and struggle to adapt to the dynamic and evolving nature of network environments. In contrast, ML-based optimisation methods can overcome such difficulties. In [35], an intelligent network application utilising deep learning enables network slicing in O-RAN, allowing emerging IoT services to coexist while ensuring compliance with required service level agreements. Sbella et al. [30] employ supervised and unsupervised learning technique to predict the network behaviour and dynamically switch base stations on and off to conserve energy in a mobile cellular network. However, this policy may affect the QoS due to the switching off at the macro layer. In the context described above, O-RAN and AI-based management applications introduce new challenges that traditional models did not face. One example is misconfiguration risks [38]. Our work aims to bridge this gap by emphasising the role of formal verification in developing xApps for O-RAN. This links to a lack of policy that accounts for the necessary dynamics of O-RAN networks. The ML is adopted to predict the dynamics of O-RAN networks based on the data set, and then the formal verification generates the decision based on the prediction. We strongly advocate for formal verification as an essential part of ensuring that AI-based xApps for O-RAN avoid critical logical fallacies during development. By analysing dynamics early, we can prevent future availability issues caused by static cell-switching policies. In this paper, we demonstrate, through simulations with a well-known formal verification tool, the PRISM model checker [20] that AI-based decisions can improve energy efficiency and availability, providing robust results that can guide real-world development. In this paper, we outline key contributions that support our position: • We argue that formal verification of xApp models, using tools like PRISM, should be the first step in the development of AI-driven applications for O-RAN. • We present simulations that highlight how AI-based predictions can enhance cell-switching policies. By predicting user behaviour, RCs can make informed decisions about when to switch on or off, improving both energy efficiency and service availability. • Our analysis of energy efficiency versus availability thresholds reveals that static policies often lead to self-imposed denial-of-service scenarios. Incorporating AI-based predictions mitigates these risks and ensures better resource management. The remainder of this paper is organised as follows. Section 2 provides the necessary background about O-RAN, energy management, and probabilistic model checking for further presentation of our work in the subsequent sections. In Section 3, we introduce a dynamic UE scenario, its corresponding management policy, and discuss considerations in modelling. After that, we present the PRISM model in Section 4 and discuss the interesting properties and related verification results. Finally, we discuss future work in Section 5."
https://arxiv.org/html/2411.03780v1,An Ordinary Differential Equation Framework for Stability Analysis of Networks with Finite Buffers,"We consider the problem of network stability in finite-buffer systems. We observe that finite buffer may affect stability even in simplest network structure, and we propose an ordinary differential equation (ODE) model to capture the queuing dynamics and analyze the stability in buffered communication networks with general topology. For single-commodity systems, we propose a sufficient condition, which follows the fundamental idea of backpressure, for local transmission policies to stabilize the networks based on ODE stability theory. We further extend the condition to multi-commodity systems, with an additional restriction on the coupling level between different commodities, which can model networks with per-commodity buffers and shared buffers. The framework characterizes a set of policies that can stabilize buffered networks, and is useful for analyzing the effect of finite buffers on network stability.","Network overloading occurs ubiquitously due to burst flow injection, transmission link failures, or node malfunction. A typical feature of network overloading is queue backlog accumulation, which often results in large queuing delay, network congestion and performance degradation. Therefore, a core engineering problem is to design transmission policies to avoid network overloading, which is commonly referred to as stabilizing the network. In their seminal paper on network stability, Tassiulas and Ephremides showed that backpressure routing can stabilize networks whenever the packet arrival rates are within the network stability region [1]. Their result elegantly solves the network stability problem for systems with unbounded buffers, and serves as a basic policy design framework for a large body of works that capture utility maximization [2, 3, 4, 5], delay minimization [6, 4, 7], and network fairness [8, 3, 9, 10]. However, most of the related works rely on the assumption of unbounded buffers, which deviates from the fact that in reality buffers are finite [11]. In practice, internal nodes in a communication network often have limited buffers [12, 13]. For example, on-chip networks have very small internal buffers due to area and power limitation, and similarly, satellite networks have small buffers on-board the satellite. In constrast, buffers of the source nodes of the arriving packets have sufficient capacity to absorb bursty packet arrivals [14], e.g. in a satellite network the buffer at the ground terminal can be relatively large. Therefore in this paper, we assume that the internal buffers are finite while the source buffers are unbounded, i.e., sufficiently large to not saturate. The existence of finite buffers gives rise to many additional design issues, including whether buffer overflow is permitted, whether the buffer is shared by different commodities, and whether the queue backlog of different commodities in the buffer is controllable. In this paper, we consider general buffer settings, while we do not allow buffer overflow, i.e., packet transmission to a saturated node. This is desirable in practice since it prevents packet dropping and significantly reduces the retransmission delay due to buffer overflow [14]. Therefore, network stability is guaranteed as long as the queue backlog at all source nodes, which have unbounded buffers, is bounded. The introduction of finite buffers may induce critical difference in network performance. Consider the toy example in Fig. 1, showing that a policy may no longer stabilize the network when buffers are finite. The system transmits two commodities with arrival rate λ1subscript𝜆1\lambda_{1}italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and λ2subscript𝜆2\lambda_{2}italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, and destination node T1subscript𝑇1T_{1}italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and T2subscript𝑇2T_{2}italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT respectively. We assume that a policy based on the principal idea of backpressure is applied111Details of this policy will be presented as (5) in Section II-B., where link (ℓ,K)ℓ𝐾(\ell,K)( roman_ℓ , italic_K ) transmits with rate equal to the capacity value cℓ⁢Ksubscript𝑐ℓ𝐾c_{\ell K}italic_c start_POSTSUBSCRIPT roman_ℓ italic_K end_POSTSUBSCRIPT when the queue backlog in node ℓℓ\ellroman_ℓ is longer than in node K𝐾Kitalic_K (ℓ=1,2ℓ12\ell=1,2roman_ℓ = 1 , 2), while otherwise it does not transmit, in any time unit. The buffer at node K𝐾Kitalic_K is shared by the two commodities. We observe that when bKsubscript𝑏𝐾b_{K}italic_b start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT, the buffer size of node K𝐾Kitalic_K, is infinite, then this policy can stabilize commodity 2 for λ2∈[0,3]subscript𝜆203\lambda_{2}\in[0,3]italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ [ 0 , 3 ] when λ1>2subscript𝜆12\lambda_{1}>2italic_λ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 2. However, when bKsubscript𝑏𝐾b_{K}italic_b start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT is finite and small enough such that it can be saturated, for example bK=6subscript𝑏𝐾6b_{K}=6italic_b start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT = 6 in Fig. 1, then this policy can stabilize commodity 2 only for λ2∈[0,1.5]subscript𝜆201.5\lambda_{2}\in\left[0,1.5\right]italic_λ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ∈ [ 0 , 1.5 ]222Brief explanation is in the caption of Fig. 1, and derivation of the result is deferred to Section III-D.. This unveils that finite buffer affects stability even in the simplest one-hop network structure, giving rise to the need to study the stability of the queue dynamics of general buffered systems. Figure 1: Finite buffer may affect stability result. On the right is an example of the queue dynamics in node K𝐾Kitalic_K following the backpressure policy. Due to its higher injection rate into the buffer of K𝐾Kitalic_K, commodity 1 takes up higher ratio in node K𝐾Kitalic_K and squeezes out commodity 2 under backpressure, and in the final state we can show that average number of commodity 2 packets in the buffer of node K𝐾Kitalic_K is 1.51.51.51.5, which arises from 1.5=(μ1/c1⁢K)×c2⁢K1.5subscript𝜇1subscript𝑐1𝐾subscript𝑐2𝐾1.5=(\mu_{1}/c_{1K})\times c_{2K}1.5 = ( italic_μ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / italic_c start_POSTSUBSCRIPT 1 italic_K end_POSTSUBSCRIPT ) × italic_c start_POSTSUBSCRIPT 2 italic_K end_POSTSUBSCRIPT, with details deferred to Section III-D. Therefore the actual throughput of commodity 2 is 1.51.51.51.5, less than μ2=3subscript𝜇23\mu_{2}=3italic_μ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 3, due to the finite buffer. A plethora of works have tried to incorporate finite buffers in network analysis. Giaccone et. al. studied the throughput region of finite-buffer network systems and discussed the relationship between buffer size and throughput under two proposed policies [11]. Le et. al. studied the relationship between buffer size and network utility under a modified backpressure mechanism [14], and Lien et. al. designed a dynamic algorithm to stabilize any admissible traffic conditioned on a finite internal buffer with size larger than a certain bound [15]. All of these works propose specific policies and analyze their performance on finite-buffer systems, under certain assumptions such as deterministic routing [11], separate buffers for different commodities [11, 14], equal buffer sizes [11, 14], and minimum buffer size requirements [15]. Thus, a systematic study of buffered networks with general buffer size setting and general control policies is still lacking. In this paper, we propose an ordinary differential equation (ODE) model to analyze network stability of a general buffered system. Unlike prior works that study stochastic dynamics, our approach uses deterministic dynamics which simplifies the analysis while still capturing the key aspects of finite-buffer systems and their impact on network stability. Our main goal is to show that the ODE framework serves as a promising approach for analyzing finite-buffer systems, which elegantly models transmission policies under arbitrary buffer settings. To that end, we are able to obtain more general results as compared to prior works on finite-buffer system stability. Based on the ODE framework, our contributions include: (i) For single-commodity systems, we derive a sufficient condition for a set of local policies to stabilize the network based on ODE stability theory, and our condition has similar physical intuition to backpressure; (ii) For multi-commodity systems, we similarly derive a sufficient condition for network stability, with an additional condition that captures the coupling level between different commodities. The core idea is that these conditions reduce the network stability problem to a problem of testing the existence of an equilibrium point, and thus facilitate stability analysis. The existence or lack of equilibrium points in multi-commodity systems can explain the effect of finite buffer on stability shown in Fig. 1. Our framework resembles the fluid model [16, 17] which was proposed as a flow-based approximation to the discrete network systems to obtain results for throughput [18] and delay [19]. However, the fluid model captures the scaled limit of the queue backlogs which for nodes with finite buffers is not very meaningful. A more closely related framework is the ODE model used to study the Transmission Control Protocol (TCP) [20, 21]. While sharing similar queue dynamics modeling with [20, 21], our approach can capture more general policies."
https://arxiv.org/html/2411.03749v1,Fundamental Limits of Routing Attack on Network Overload,"We quantify the threat of network adversaries to inducing network overload through routing attacks, where a subset of network nodes are hijacked by an adversary. We develop routing attacks on the hijacked nodes for two objectives related to overload: no-loss throughput minimization and loss maximization. The first objective attempts to identify a routing attack that minimizes the network’s throughput that is guaranteed to survive. We develop a polynomial-time algorithm that can output the optimal routing attack in multi-hop networks with global information on the network’s topology, and an algorithm with an approximation ratio of 2222 under partial information. The second objective attempts to maximize the throughput loss. We demonstrate that this problem is NP-hard, and develop two approximation algorithms with multiplicative and additive guarantees respectively in single-hop networks. We further investigate the adversary’s optimal selection of nodes to hijack that can maximize network overload. We propose a heuristic polynomial-time algorithm to solve this NP-hard problem, and prove its optimality in special cases. We validate the near-optimal performance of the proposed algorithms over a wide range of network settings. Our results demonstrate that the proposed algorithms can accurately quantify the risk of overload given an arbitrary set of hijacked nodes and identify the critical nodes that should be protected against routing attacks.","Recent years witnessed the escalating prevalence and severity of network attacks, evidenced by the growing number of incidents and their devastating impact [1]. These attacks often result in substantial degradation in network performance, such as lower throughput and higher latency. For example, the 2018 Pakistan Telecom hijacking incident caused considerable network disruption and extensive delays [2], while the 2016 Dyn DDoS attack led to widespread outages that significantly impaired platforms such as Twitter and Netflix [3]. In this paper, we focus on the degradation of network performance due to routing attacks, wherein adversaries hijack network servers and change their routing decisions [4]. Routing attacks are a notable form of network attacks with broad impact, which can last for several hours or longer before being resolved [1, 5]. Examples of routing attacks include BGP hijacking, where an attacker falsely claims ownership of an IP prefix to affect routing [6, 7]; routing table poisoning, where false routing information is injected into a victim’s routing table [8, 9]; OSPF attacks, which involve fabricating topology information to control routing [10]; and blackhole attacks, which diverts the traffic to non-existent destinations [11]. Routing attacks have been detected in a wide range of networks, including data center networks [12], software-defined networks [13], and wireless ad hoc networks [8]. This paper aims to quantify the threat of network adversaries applying routing attacks to induce network overload, where traffic injected into the network cannot be fully transmitted to the destination, resulting in throughput loss due to network link saturation [14, 15, 16]. A high overload level contributes to performance degradation such as queue instability [17, 18] and high latency [19, 20], which has become increasingly prevalent in enterprise-level data center networks [14, 21, 20], due to the demand surge of machine learning workloads [22, 23] and the slower growth of electrical switch capacity [21, 24]. We develop routing attack strategies for two objectives related to overload: (i) no-loss throughput minimization and (ii) loss maximization. The no-loss throughput of a network represents the maximum traffic arrival rate at the source that can be delivered to the destination without loss. The difference between the two objectives is that minimizing the no-loss throughput is equivalent to maximizing the range of traffic arrival rates to the network that will cause overload, while maximizing the loss is equivalent to minimizing the network throughput given the traffic arrival rate. Evaluating the routing attacks’ ability to inducing network overload remains a relatively unexplored research area [1], unlike other attack types including denial-of-service [25], link removal [16], and node removal [26]. Routing attacks have presented high risks due to their low implementation cost for adversaries [12], inadequate design of detection and defense mechanisms [1], and broader impacts in software-defined networks where a hijacked controller in the control plane may affect the routing decisions of multiple nodes in the data plane [21, 27]. We are motivated to develop algorithms for network service providers to evaluate the potential impact of network overload that can be caused by routing attacks. We demonstrate that our proposed algorithms can well approximate the maximum level of overload given arbitrary sets of hijacked nodes, and identify the critical nodes that are of highest priority to be protected from routing attacks. We summarize the contributions of this work as follows. (i) No-Loss Throughput Minimization: We develop an exact polynomial-time routing attack algorithm which can minimize the no-loss throughput in general multi-hop networks with arbitrary sets of hijacked nodes. We further propose a 2-approximation routing attack algorithm purely based on the partial network information downstream to the hijacked nodes. (ii) Loss Maximization: We establish the NP-hardness of the loss maximization problem. We develop two approximation algorithms with multiplicative and additive performance guarantee respectively in single-hop networks. (iii) Optimal Selection of Nodes to Hijack: We investigate the adversary’s optimal selection of nodes to hijack over a set of candidate nodes to optimize the aforementioned two objectives via routing attacks. We prove its NP-hardness in general networks, propose heuristic algorithms and prove the optimality for no-loss throughput minimization when candidate nodes for the adversary are parallel to each other in the network. (iv) Performance Evaluation: We evaluate the proposed algorithms and demonstrate their near-optimal performance under a wide range of network settings, including different network densities, default routing policies, and numbers of hijacked nodes. To the best of our knowledge, this is the first quantitative study of routing attacks on network overload with both theory and empirical validation. The rest of this paper is organized as follows: Section II introduces the network models we study and defines the optimal routing attack problem. Section III investigates the no-loss throughput minimization problem. Section IV investigates the loss maximization problem. Section V investigates the adversary’s optimal selection of nodes to hijack and conduct routing attack for the aforementioned two metrics; Section VI presents the evaluation of the proposed routing attack algorithms under various network settings."
https://arxiv.org/html/2411.03686v1,"Learn to Slice, Slice to Learn : Unveiling Online Optimization and Reinforcement Learning for Slicing AI Services","In the face of increasing demand for zero-touch networks to automate network management and operations, two pivotal concepts have emerged: “Learn to Slice” (L2S) and “Slice to Learn” (S2L). L2S involves leveraging Artificial intelligence (AI) techniques to optimize network slicing for general services, while S2L centers on tailoring network slices to meet the specific needs of various AI services. The complexity of optimizing and automating S2L surpasses that of L2S due to intricate AI services’ requirements, such as handling uncontrollable parameters, learning in adversarial conditions, and achieving long-term performance goals. This paper aims to automate and optimize S2L by integrating the two concepts of L2S and S2L by using an intelligent slicing agent to solve S2L. Indeed, we choose two candidate slicing agents, namely the Exploration and Exploitation (EXP3) and Deep Q-Network (DQN) from the Online Convex Optimization (OCO) and Deep Reinforcement Learning (DRL) frameworks, and compare them. Our evaluation involves a series of carefully designed experiments that offer valuable insights into the strengths and limitations of EXP3 and DQN in slicing for AI services, thereby contributing to the advancement of zero-touch network capabilities.","Network slicing (NS) has emerged as a pivotal tool in wireless networks, providing a mechanism to configure networks specifically for diverse applications and services. One groundbreaking application within this domain is integrating Artificial Intelligence (AI) technologies for automating the management and orchestration of the network’s computational and network resources through NS, paving the way for the development of zero-touch networks to serve many services in parallel and on the same physical infrastructure. This allows for the emergence of specialized research areas termed Learn to Slice (L2S) and Slice to Learn (S2L). The former aims to use AI to optimize network slices for varying general network services (e.g., remote monitoring). On the other hand, S2L focuses on the allocation of network slices and AI hyperparameters (e.g., number of epochs) to varying AI services for optimal overall results (e.g., highest overall accuracy or F1-score) without compromising their performance or reliability. S2L is beneficial in the case of AI task offloading when multiple devices or vehicles with limited resources offload their diverse machine learning tasks to nearby edge networks for training. Unlike L2S, S2L presents a myriad of challenges specific to AI models that demand innovative solutions. These challenges come from the fact that AI is data and model-specific. Some of these complexities may include the ability of the slicing agent to : • Find the optimal choice of resources and AI hyperparameters for different models with different architectures (e.g., ANN, CNN, and GPT), complexity, and goals (e.g., classification) with minimal overloading to other services. • Consider the existence of multiple uncontrollable parameters that affect the accuracy of the models and their generalization, such as data quality (e.g., being independent and identically distributed (i.i.d), balanced, unique, and complete.., etc.). • Adapt to different models and data requirements. • Learn within potentially untrusted environments (e.g., adversarial attacks by providing misleading data quality to the slicing agent to acquire more resources). • Adhering to long-term goals (e.g., reducing total cost or preserving components’ reliability in the long run). • Take into account the stochastic and diverse nature of calculating the models’ accuracy, as each model’s accuracy is only known after applying a specific AI hyperparameter with certain data quality. Addressing these challenges is crucial for successfully implementing and automating the creation of perfect slicing solutions for AI. To tackle most of these formidable challenges and harness the potential of S2L, this paper mainly focuses on integrating intelligent slicing agents from L2S into S2L, effectively incorporating AI into the S2L process. Specifically, we explore two different slicing agents from two renowned learning frameworks: Online Convex Optimization (OCO) and Deep Reinforcement Learning (DRL). These frameworks are aptly presented by the Exponential-weight algorithm for Exploration and Exploitation 3 (EXP3) and the Deep Q-Network (DQN) algorithms. Their inherent characteristics make them suitable for supporting NS performance, especially catering to various AI services, as in the case of S2L. DQN, a key DRL framework, an agent interacts with its environment by observing states and selecting actions, receiving rewards based on action effectiveness. It uses neural networks to optimize cumulative rewards by refining action-state associations. Typically pre-trained in a specific environment, DQN can also adapt its training to real-world testing for added flexibility. Instead, the EXP3, rooted in the OCO framework, is inherently an online learning algorithm. Without needing prior experience in the testing phase, EXP3 dynamically selects actions. Based on the rewards received, it adjusts the probability of specific actions, increasing their likelihood exponentially while ensuring a periodic exploration of new actions. Confronted with the aforementioned challenges, thoroughly examining such algorithms becomes imperative. This paper seeks to thoroughly analyze the performance of the aforementioned frameworks in the context of the S2L problem. We dissect their strengths, limitations, and practical implications, aspiring to illuminate the pathway for optimized S2L. Therefore, our contributions can be summarised as follows: 1. First, we discuss the efforts and challenges associated with S2L in 5G and beyond networks. 2. Next, we formulate the problem of S2L, with the objective of maximizing the average accuracy of various AI services for classification tasks. This formulation focuses on the joint slicing of communication and computation resources at the Radio access network (RAN) and transport edge nodes while considering the diverse requirements of the different AI services related to computation, delay, and cost. 3. Then, we solve the formulated problem utilizing two of the most renowned algorithms, EXP3 and DQN. We demonstrate the effectiveness and limitations of these techniques in supporting S2L while adapting to varying environmental conditions such as sudden environmental change, timely constrained learning, adversary existence, and long-term goals. 4. Lastly, we conclude by highlighting some challenges and future research directions that are worth investigating. In what follows, Section II discusses the key challenges associated with S2L and explores related works. Section III introduces the system architecture under consideration and outlines the formulated slicing optimization problem. Section IV presents our analysis and assessment of OCO and DRL frameworks in addressing the formulated problem. Finally, Section V concludes the paper and explores potential directions for future research. TABLE I: Summary of some of the related Work Ref Application Goal Optimization method Limitation [1] S2L Maximizing admitted training jobs in distributed machine learning (ML) through joint data collection and resource allocation. Approximation via randomized rounding • No account for system dynamics or adversarial existence. • Hyper-parameter tuning is not considered. • No consideration of AI services KPIs. • No consideration of long-term goals. [2] S2L Cost minimization through efficient allocation of edge and cloud resources for distributed ML training jobs Classical optimization [3] S2L Energy minimization for federated edge learning (FEEL) through efficient radio resource management. Convex optimization • No account for system dynamics or adversarial existence. • Considers single AI service and single access point (or edge server). • Hyper-parameter tuning is not considered (for reference [3]). • No consideration of long-term goals. [4] S2L Minimize time and energy consumption through efficient AI service placement and resource allocation supporting model inference Convex optimization [5] S2L Maximizing inference accuracy through the joint allocation of edge computing resources between training and inference tasks serving video analytics applications and selecting the best configurations for these tasks. Classical optimization [6] S2L Accuracy maximization and delay minimization supporting AI training services through efficient task scheduling and resource allocation. Non-dominated Sorting Genetic Algorithm • No account for system dynamics (for reference [6]). • Hyper-parameter tuning is not considered (for references [6, 7]). • No account for adversarial existence. • No consideration of long-term goals. [7] S2L Efficiently allocating computing resources to maximize the accuracy improvement of model retraining tasks while incorporating the reuse of previous models to avoid redundant re-training. Classical optimization This article S2L Maximize the accuracy of multiple competing AI services + Assessing the benefits and limitations between DQN & EXP3 as candidate solutions. AI techniques: DRL & OCO • Multiple services with different KPIs. • Learning in a dynamic environment. • Learning under adversarial existence. • Scalability is discussed. • Considers long-term goals."
https://arxiv.org/html/2411.03654v1,PyroGuardian: An IoT-Enabled System for Health and Location Monitoring in High-Risk Firefighting Environments,"First responders risk their lives to reduce property damage and prevent injuries during disasters. Among first responders, firefighters work with fires in residential properties, forests, or other locations where fire occurs. We built the PyroGuardian system that uses wearable modules to transmit unit information over Long Range (LoRa) to an Android tablet. The tablet runs our application, PyroPortal, to assign each firefighter’s stats, such as body temperature, heart rate, and GPS location. PyroPortal displays this information on unit dashboards, and markers on Google Maps represent the firefighter’s location and the direction they are facing. These dashboards can help the incident commander (IC) make more informed decisions on mission control operations and remove specific units whose health stats, such as oximeter and pulse, passed certain thresholds. PyroGuardian completes all these tasks at an affordable cost and in an impressive maximum range between the units and IC. In addition, PyroGuardian has various application scenarios, such as law enforcement and military operations, besides firefighting. We also conducted a sample mission inside a burning building while real firefighters watched. After the demonstration, they completed a survey on system usability and PyroGuardian’s potential to meet their requirements.","First responders operate immediately after a disaster, such as earthquakes, floods, nuclear leakages, fires, and explosions (Girma et al., 2020). Their goal is quickly reaching the disaster point to save lives and reduce property damage (Girma et al., 2020). Significant incidents, such as the terrorist attacks of September 11, 2001, the anthrax attacks of 2001, and the response and recovery efforts of the 2004 Southeast Asia tsunami, have emphasized the role of first responders (Benedek et al., 2007). However, they are not always safe and can suffer severe duty-related consequences. Among first responders, firefighters frequently suffer injuries, as the NFPA 2015 National Fire Experience Survey from fire departments indicated that 68,085 firefighter injuries occurred in the line of duty in 2015 in the US (Haynes and Molis, 2015). Another study found that in 2019, 48 firefighters died while on duty in the US (Fahy et al., 2020). The US Fire Department stated there is a fire in a residential area every 85 seconds (Shokouhi et al., 2019). These facts make firefighters’ efficiency and safety a crucial matter for the public as firefighting is one of the most life-threatening, emotionally traumatic, and stressful occupations (Meina et al., 2020). There are several factors in fire scenes, such as smoke and noise. A firefighter is deafened when they walk inside a building. They cannot see well, and radio communications are challenging (mic, 2021). Thus, an IC will not know their units’ well-being. This lack of communication makes the job more difficult and dangerous. A system is needed to preserve communication between the IC and units. ICs should monitor their critical information and location to interfere in time before any injury. Relying on technological equipment can solve this problem (Yizhe, 2021; Technology in the Fire Service, 2020). Existing solutions tackle this field with techniques like monitoring units’ vitals or tracking their outdoor location. Some issues with existing solutions include the lack of GPS, preventing outdoor localization for wildfires, or vital health information monitoring. We have not found a solution that combines all these tasks into a unified dashboard for the IC. In addition, we have not found a solution that aims to keep the IC behind the safety line while allowing real-time data streaming. To fill this gap and develop a solution while considering pricing and user convenience, we created PyroGuardian: an IoT-powered framework for firefighter mission control. PyroGuardian will warn the IC when firefighters’ vitals or environment variables are dangerous. It can also stream real-time unit location and health data to the IC with a range of 610 meters. The PyroGuardian consists of a tablet, an Android application named PyroPortal, an external USB LoRa adapter for PyroPortal, PyroStrap, and PyroHelm. The IC will use PyroPortal, connected to the LoRa adapter. Each firefighter will equip one PyroHelm and PyroStrap. The PyroHelm’s casing is attachable to the exterior of the standard firefighter gas masks and helmets. The PyroHelm broadcasts sensor data, such as GPS, temperature, and 3-D inertial information, to the PyroPortal via the wireless protocol, LoRa. We used an external LoRa adapter as tablets do not have built-in LoRa. PyroPortal will receive additional sensor data from the PyroStrap via LoRa broadcasting. The tablet has a transceiver LoRa adapter connected to its USB-C port. Once the LoRa adapter receives the sensor data, it inputs the data into the PyroPortal, displaying each unit’s location and vital health information on its map. The IC can then interpret this data to navigate his officers and remove endangered units from the scene. Another contribution of this paper includes creating a real-world fire scenario in the Illinois Fire Service Institute and observing PyroGuardian’s efficiency during the mission. We invited 34 firefighters to watch this mission and provide feedback on our surveys. We evaluate PyroGuardian’s system usability factor to ensure it is convenient for tradition-focused fire departments. Furthermore, the survey also asked them if PyroGuardian meets their needs. Finally, we will analyze the feasibility of our solution in other first-responder fields. Our contributions to this field include: • A novel IoT-powered solution for firefighting mission control that is cheap and easy to use. • A unified dashboard that monitors the unit location and health together • A user study from fire department personnel. • An analysis of extending our framework to other first responder departments, such as law enforcement. Section two gives background regarding some wireless tools and sensors integrated into PyroGuardian. The following section describes a high-level overview of PyroGuardian’s model, workflow, and configuration. The “Implementation” section elaborates on PyroGuardian’s sensors and breakout boards. It will explain in detail the communication between each component. In addition, “Evaluation” presents our results, a comprehensive user study, and PyroGuardian’s performance based on our metrics, such as cost. Section six, Discussion, interprets our results and extends PyroGuardian to a broader scope. Finally, the conclusion states the future trends and ends the paper with our final remarks."
https://arxiv.org/html/2411.03635v1,Digital Twin-Assisted Robust and Adaptive Resource Slicing in LEO Satellite Networks,"Resource slicing in low Earth orbit satellite networks (LSN) is essential to support diversified services. In this paper, we investigate a resource slicing problem in LSN to reserve resources in satellites to achieve efficient resource provisioning. To address the challenges of non-stationary service demands, inaccurate prediction, and satellite mobility, we propose an adaptive digital twin (DT)-assisted resource slicing scheme for robust and adaptive resource management in LSN. Specifically, a slice DT, being able to capture the service demand prediction uncertainty through collected service demand data, is constructed to enhance the robustness of resource slicing decisions for dynamic service demands. In addition, the constructed DT can emulate resource slicing decisions for evaluating their performance, enabling adaptive slicing decision updates to efficiently reserve resources in LSN. Simulation results demonstrate that the proposed scheme outperforms benchmark methods, achieving low service demand violations with efficient resource consumption.","The 6G networks are envisioned to provide reliable, low-latency, and ubiquitous communication to support diversified network services [1]. Low Earth orbit (LEO) satellite networks (LSN) are considered as indispensable components of 6G networks, with their extensive communication coverage and dense deployment, to enlarge the limited communication coverage of terrestrial networks and alleviate network congestions, thereby supporting multifarious and ubiquitous applications [2, 3]. As an innovation in service-oriented network management techniques for the 5G era, resource slicing will continue to play a pivotal role in satisfying service requirements of diversified services via efficient resource provisioning [4]. Different from resource slicing in 5G targeting fixed network infrastructures such as base stations, resource slicing in LSN needs the construction of virtual networks, referred to as slices, upon different satellites with high mobility while satisfying diverse service-level agreements for different services in any target area. As shown in Fig. 1, in LSN, the ground controller in an area is responsible for managing network slices in a large time scale, named as a slicing window, by reserving resources in satellites serving the area to accommodate service demands. Considering different service durations during which satellites cover a given area, the resource slicing decision is first made at the beginning of each slicing window, and then executed by reserving resources for the slice in each satellite when the satellite first covers the area within the slicing window. Figure 1: Resource slicing architecture in LSN. There exist some research efforts on resource slicing in satellite networks, by considering the heterogeneity of satellite and terrestrial networks to satisfy different QoS requirements in [5] and developing a scalable resource slicing scheme to coordinate multiple cells for resource efficiency in [6]. Despite these research efforts, realizing resource slicing in LSN still meets the following two challenges. First, due to the mobility of satellites, the number of satellites covering a designated area varies over time, resulting in inconsistent available resources for network services. Meanwhile, time-varying positions of satellites result in differentiated communication capabilities due to different channel conditions. Second, the service demand of LSN is time-varying, and such temporal variations may be non-stationary, thereby challenging the accurate prediction of service demand for proactive resource slicing. Unlike resource slicing in terrestrial networks, where the reserved resources are generally fixed during the slicing window, resource slicing in LSN has more flexibility. This flexibility stems from the ability of LSN to dynamically adjust the reserved resources from satellites to accommodate dynamic service demands due to the time-varying available resources from LSN. Despite the flexibility, efficient resource slicing is still challenging, since the reserved resources from satellites can result in an accumulated impact of under-/over-provisioning of resources considering the inaccurate service demand prediction in the slicing window. Slice DT, as a digital representation of a network slice for the service, attracts wide attention, which can facilitate resource slicing with a data-driven approach [1, 7]. To deal with the aforementioned challenges, in this paper, we propose an adaptive digital twin (DT)-assisted resource slicing (ADTRS) scheme for resource provisioning in LSN. Our objective is to efficiently reserve resources in LSN for the target area to minimize the overall costs related to resource usage and delay, taking into account the dynamic network environment and the uncertainty in service demand prediction. To achieve the objective, a chance-constrained programming problem is formulated to find robust and adaptive resource slicing decisions. Then, a slice DT is constructed in the ground controller to depict the dynamic service demand and slicing performance. Specifically, in the slice DT, we develop a feature extraction module, a prediction module, and a distribution fitting module to handle the uncertainty of service demand prediction. These modules operate by periodically collecting service demand information of the target area from the ground controller. By processing the collected data, the designed modules in the slice DT can predict service demand with information on prediction uncertainty to allow robust resource slicing decision-making. Moreover, an emulation module is devised in the slice DT to evaluate the performance of resource slicing decisions in dynamic environments, facilitating adaptive decision updating in LSN. Extensive simulations demonstrate that the proposed scheme can outperform benchmark algorithms in terms of low service demand violation and high resource efficiency."
https://arxiv.org/html/2411.03503v1,TwiNet: Connecting Real World Networks to their Digital Twins Through a Live Bidirectional Link,"The wireless spectrum’s increasing complexity poses challenges and opportunities, highlighting the necessity for real-time solutions and robust data processing capabilities. Digital Twin (DT), virtual replicas of physical systems, integrate real-time data to mirror their real-world counterparts, enabling precise monitoring and optimization. Incorporating DTs into wireless communication enhances predictive maintenance, resource allocation, and troubleshooting, thus bolstering network reliability. Our paper introduces TwiNet, enabling bidirectional, near-real-time links between real-world wireless spectrum scenarios and DT replicas. Utilizing the protocol, MQTT, we can achieve data transfer times with an average latency of 14 ms, suitable for real-time communication. This is confirmed by monitoring real-world traffic and mirroring it in real-time within the DT’s wireless environment. We evaluate TwiNet’s performance in two distinct use cases: (i) enhancing Safe Adaptive Data Rate (SADR) systems by assessing risky traffic configurations of UEs, resulting in approximately 15% improved network performance compared to original network selections; and (ii) deploying new CNNs in response to jammed pilots, where the DL pipeline achieves up to 97% accuracy by training on artificial data and deploying a new model in as low as 2 minutes to counter persistent adversaries. TwiNet enables swift deployment and adaptation of DTs, addressing crucial challenges in modern wireless communication systems.","In recent years [1], researchers have begun to look away from the physical world and toward the virtual one. By being able to offload data for decision-making and predictive analytics, devices with hardware limitations can gain better capabilities without significant adjustments [2]. One of these methods is called a Digital Twin (DT), a virtual model designed to accurately mirror a physical object or system. For a virtual model to be considered a DT, an automated bidirectional communication link must connect both realms, facilitating continuous data exchange to aid in decision-making [1]. Smart cities often utilize DT technology to improve data quality [3]. DTs enable real-time simulations and analytics of urban systems, aiding in decision-making, resource optimization, and predictive maintenance [3]. As shown in Fig. 1, real-world data is exchanged with DTs to support informed decision-making. DTs offer advantages over real-world testing, including improved efficiency via real-world mirroring for ongoing performance optimization, and near real-time communication through constant updates to closely mirror real-world counterparts for enhanced research, design, and evaluation of novel solutions in a risk-free environment, [1, 2]. Figure 1: High-level representation of a digital twin for a smart city. Figure 2: High-level overview of TwiNet, showing the link between the real world and DT along with the spectrum and deep learning capabilities. Integrating DTs in wireless communication revolutionizes network efficiency, reliability, and optimization. Traditional methods lag behind the exponential growth of wireless devices and system complexity. Virtual replicas of physical network components, created through DTs, enable real-time monitoring, predictive analysis, and scenario emulation. This approach addresses challenges, optimizes performance, and enhances network resilience. By accurately mirroring wireless spectrum behavior, DTs identify issues, optimize resource allocation, and improve service quality. Implementing machine learning and Artificial Intelligence (AI), they automate decision-making and adaptively optimize network configurations. Seamless setup and optimization of connections between real and virtual worlds are crucial for realizing their full potential. In this paper, we introduce TwiNet, a generalized approach for synchronizing novel wireless spectrum scenarios between a DT and its real-world counterpart. A fast, bidirectional communication link is crucial for successful DT operation, facilitating seamless data exchange. We establish this link by leveraging the Internet of Things (IoT) Message Queuing Telemetry Transport (MQTT) protocol, emphasizing its publish/subscribe method and reliance on a single data broker. The asynchronous nature of MQTT enhances scalability and flexibility, owing to its distinctive publishing method. Fig. 2 outlines TwiNet’s architecture and its utilization of MQTT in various DT scenarios. It facilitates flexible management across the DT, enabling task allocation. Two configurations are depicted: wireless spectrum replication and Deep Learning (DL)-based predictive analysis, each with specific data requirements. MQTT optimizes data transmission, targeting relevant DT components, highlighting scalability and adaptability crucial for large-scale deployment. The main contributions of this paper are as follows: 1. We introduce TwiNet, a bidirectional communication link between real-world scenarios and its DT, achieving data transfer latencies as low as 14 ms per packet, and demonstrate real-time traffic monitoring and mirroring experiments between the real-world and its DT counterpart. 2. In the first use case, we implement a Safe Adaptive Data Rate (SADR) system that exploits the capabilities of the DT to evaluate traffic requests from the User Equipments (UEs) to identify and prevent risky actions and states that can lead to outages, improving the performances of the real network. 3. In the second use case, we implement a DL pipeline that allows DTs to create new models to protect pilot carriers for base stations that do not have the resources to recreate models on the fly. The remainder of the paper goes as follows: Section II reviews prior work relevant to the paper’s scope. Section III elaborates on TwiNet and our testbed setup, presenting our traffic mirroring proof-of-concept and findings. Section IV delineates setups for risky state testing and adversarial DL classifier for pilot jamming. Section V presents experimental findings. Lastly, Section VI summarizes the paper’s conclusions."
https://arxiv.org/html/2411.03326v1,"This version of the paper has been submitted for publication to IEEE and may be removed upon request or copyright issues.xApp-Level Conflict Mitigation in O-RAN, a Mobility Driven Energy Saving Case","This paper investigates the emerging challenges of conflict detection and mitigation in Open Radio Access Network (O-RAN). Conflicts between xApps can arise that affect network performance and stability due to the disaggregated nature of O-RAN. This work provides a detailed theoretical framework of Extended Application (xApp)-level conflicts, i.e., direct, indirect, and implicit conflicts. Leveraging conflict graphs, we further highlight how conflicts impact Key Performance Indicators and explore strategies for conflict detection using Service Level Agreements and Quality of Service (QoS) thresholds. We evaluate the effectiveness of several mitigation strategies in a simulated environment with Mobility Robustness Optimization (MRO) and Energy Saving (ES) xApps and present experimental results showing comparisons among these strategies. The findings of this research provide significant insights for enhancing O-RAN deployments with flexible and efficient conflict management.","The introduction of O-RAN architecture has promised enhanced flexibility and interoperability in the telecommunications industry. However, this disaggregated approach of integrating components from multiple vendors presents a significant challenge of conflict management. While the traditional Radio Access Network (RAN) relied on single-vendor solutions that come with in-house vendor spesific conflict resolution strategies, O-RAN’s multi-vendor nature needs a robust conflict mitigation strategy to cope with conflicts between different vendors’ components. Conflicts arise when components compete for shared resources or set conflicting configurations that control the same network environment. This can lead to performance degradation, instability, and security vulnerabilities in the network. Therefore, handling these conflicts are now a priority and a major obstcle for wide deployment on O-RAN. The following provides an initial understanding of different types of conflicts in O-RAN based on the state-of-the-art. I-A Categorization of Conflicts: Conflicts are categorized into horizontal and vertical conflicts, which are further classified into intra-component and inter-component conflicts [1, 2, 3]. Horizontal conflicts occur between components at the same hierarchical level, for instance, between two xApps (applications for near-real-time control) within a Near Real Time RAN Intelligent Controller (Near-RT-RIC) [2]. It is further classified into intra-component and inter-component conflicts: • Intra-component conflicts occur within a component, like conflicting configurations between xApps in a Near-RT-RIC [2, 3]. These are further classified into three types: – Direct Conflict: These occur when two or more xApps attempt to simultaneously control the same network parameter [1, 4, 2, 5, 6]. For instance, one xApp might try to increase the transmission power for a specific cell, while another aims to decrease it for energy saving. Direct conflicts are generally easier to detect since the conflicting parameter is directly observable [7, 3]. However, simply prioritizing one xApp’s preference over another might not be the optimal solution. A more effective approach would involve finding a configuration that balances these conflicting objectives, potentially maximizing overall network utility [2]. – Indirect Conflicts: These arise when different xApps control distinct parameters that ultimately influence the KPI [1, 6, 8, 2, 9]. For example, one xApp might modify the cell individual offset (CIO) to balance the load across cells, while another adjusts antenna tilts. Both actions indirectly impact the cell boundaries and handover decisions, potentially causing conflicts [10, 8]. Detecting indirect conflicts necessitates post-action analysis, observing how different xApp actions collectively impact the relevant KPI. This is more challenging than detecting direct conflicts as the relationship between parameter changes and KPI impacts might not be immediately apparent [3]. – Implicit Conflicts: These present the most significant challenge for detection and mitigation [1, 7]. Implicit conflicts occur when the actions of multiple xApps, while individually aligned with their specific objectives, result in an undesirable overall network state [4, 8]. This often involves subtle interactions between xApps with different goals, making the source of the conflict difficult to pinpoint. For example, an xApp trying to maximize quality of service (QoS) for a group of users might interfere with another xApp aiming to minimize handover rates, leading to unexpected performance degradation. Identifying and resolving implicit conflicts often requires sophisticated monitoring and analysis of network behavior over time [1]. • Inter-component conflicts involve components from different areas, such as conflicting decisions from xApps in neighboring Near-RT-RICs [3, 2]. Vertical conflicts involve components from different hierarchical levels, such as a conflict between a Near-RT-RIC and a Non-RT RIC [2, 3]. Addressing various conflicts requires a multi-pronged approach. Existing literature suggests several strategies: I-A1 Conflict Detection and Mitigation Frameworks: Adamczyk and Kliks [1] propose a conflict mitigation framework (CMF) within the Near-RT-RIC. The framework focuses on detecting direct, indirect, and implicit conflicts between xApps. Direct conflicts arise from consecutive, contradictory decisions affecting the same parameters. Indirect conflicts occur when xApps impact shared KPIs through different parameters. Implicit conflicts involve xApp actions aligning with individual objectives but contradicting overall network goals. A more detailed description of these conflicts are discussed in Section. II. The framework in [1] utilizes a Conflict Mitigation (CM) component in the Near-RT-RIC to identify and resolve these conflicts. However, current implementations primarily focus on specific xApps, like Mobility Robustness Optimization (MRO) and Mobility Load Balancing (MLB). I-A2 Team Learning and Cooperative Approaches: Zhang et al. in [11] suggest a team learning algorithm based on Deep Q-learning to encourage cooperation between xApps. By sharing information about their intended actions, xApps can make more informed decisions that benefit the overall network performance. This approach demonstrates promising results, leading to improved throughput and reduced packet drop rates. However, scalability remains a concern, as the complexity increases with the number of cooperating xApps. I-A3 QoS-Aware Conflict Mitigation: Recognizing the importance of maintaining Quality of Service (QoS), our previous work [3] introduces the QACM (QoS-Aware Conflict Mitigation) method. QACM considers the QoS benchmarks of individual xApps during conflict mitigation, ensuring their requirements are met. It utilizes game theory principles, specifically Nash’s Social Welfare Function (NSWF) and Eisenberg-Galle (EG) solutions, to find an optimal balance between conflicting parameters. Initial results indicate QACM’s effectiveness in upholding QoS thresholds compared to benchmark methods. However, further research is needed to develop more sophisticated KPI prediction models and test its practicality in real-world RAN deployments. I-A4 Motivation and Contribution of this Research: While significant progress has been made in understanding and mitigating conflicts in Open RAN, several challenges remain: • As Open RAN is a new concept, and there is limited knowledge about conflicts within it. The subtle difference between indirect and implicit conflicts makes reliably detecting and evaluating these potential issues challenging. Developing a low-latency detection mechanism is crucial to ensure the reliability and stability of O-RAN deployments. • Dynamic Detection and Mitigation: Existing solutions often focus on static conflict resolution. However, the dynamic nature of network traffic and user demands necessitates adaptive and real-time conflict detection and mitigation strategies. • Scalability and Complexity: As O-RAN networks expand in size and complexity, managing conflicts between numerous xApps and Remote Applications from diverse vendors will become increasingly challenging. Scalable and efficient conflict detection and mitigation mechanisms are essential for the large-scale adoption of Open RAN. In light of these challenges, this research focuses on: • Providing a detailed theoretical framework to explain each type of xApp conflict that helps simplifying conflict detection and mitigation. • Utilizing conflict graphs for more clear representation and better understanding of potential conflict. • Identifying differences between conflict types systematically and developing a low-latency conflict detection framework adaptable to evolving network conditions. • Reviewing and analyzing the performance of state-of-the-art mitigation strategies. The structure of the paper is as follows: Section II provides a theoretical modeling and evaluation of xApp conflicts, Section III presents a discussion on conflict detection and mitigation strategies, Section IV discusses simulation experiments and results with ES and MRO xApps, and the paper concludes in Section V."
https://arxiv.org/html/2411.04114v1,Age of Gossip With Time-Varying Topologies,"We consider a gossiping network, where a source node sends updates to a network of n𝑛nitalic_n gossiping nodes. Meanwhile, the connectivity topology of the gossiping network changes over time, among a finite number of connectivity “states,” such as the fully connected graph, the ring graph, the grid graph, etc. The transition of the connectivity graph among the possible options is governed by a finite state continuous time Markov chain (CTMC). When the CTMC is in a particular state, the associated graph topology of the gossiping network is in the way indicated by that state. We evaluate the impact of time-varying graph topologies on the freshness of information for nodes in the network. We use the version age of information metric to quantify the freshness of information at the nodes. Using a method similar to the first passage percolation method, we show that, if one of the states of the CTMC is the fully connected graph and the transition rates of the CTMC are constant, then the version age of a typical node in the network scales logarithmically with the number of nodes, as in the case if the network was always fully connected. That is, there is no loss in the age scaling, even if the network topology deviates from full connectivity, in this setting. We perform numerical simulations and analyze more generally how having different topologies and different CTMC rates (that might depend on the number of nodes) affect the average version age scaling of a node in the gossiping network.","Recent advances in wireless communication have led to significant improvements in information rate and reliability. The roll-out of 5G has made users more connected than ever before. These new technologies enable easy addition of and massive connectivity for new devices to the network. The enhancement in connectivity has lead to the exploration of new use cases, such as drone networks, sensor networks in remote areas, and self-driving car networks. These are usually dense networks, or may be far away from base stations. Hence, gossiping, i.e., peer-to-peer message dissipation, has emerged as one way of disseminating information fast in such networks. Gossiping protocols are simple algorithms which involve spreading information across the entire network by communication between neighbors. Some of these applications may not have a fixed structure due to mobility or channel unreliability. Hence, there is a need to analyze how dynamic changes in graph connectivity structure impacts such networks. This analysis also helps us in the design of adaptive protocols for networks that fall under this paradigm. Quantification of freshness of information necessitates going beyond the commonly used metrics such as latency and throughput [1]. Age of information was introduced as a metric to capture the freshness of information [2, 3, 4]. Networks have been analyzed and designed with the age of information metric in mind [5, 6]. Several other metrics have been proposed to quantify the freshness of information in application-specific settings, including the age of incorrect information [7], the age of synchronization [8], binary freshness metric [9], and the version age of information [10, 11, 12]. Figure 1: A description of the time evolution of the connectivity of the gossip network. The CTMC represents the connectivity graph (topology) of the gossip network. In this example, the CTMC has four states, representing the fully connected (FC) topology, the ring topology, the grid topology and the disconnected (DC) topology. The current topology is represented using a red dashed circle. In this example, the network starts in the fully connected topology, moving to the ring topology, the disconnected topology and back to the ring topology. Since the rates of the CTMC are different, the time spent in a state will be random, which is also shown in the figure. In this paper, we use the version age of information in a gossiping setting (age of gossip) to quantify the freshness of information. The version age of information can be defined for a node in the gossiping network as the number of versions behind the node is compared to the source node that generates the updates. These updates are typically generated by a random process. Yates [10] was the first to use the stochastic hybrid system (SHS) framework to analyze gossip networks and provide a set of recursive equations to find the version age of a subset of nodes. In particular, the version age of any subset depends on the version age of subsets containing exactly one more neighboring node. Yates showed that the long term average version age of a node in the fully connected network scales as Θ⁢(log⁡n)Θ𝑛\Theta(\log{n})roman_Θ ( roman_log italic_n ), where n𝑛nitalic_n is the number of nodes. Following Yates’ work, many works have used the SHS framework to analyze gossiping networks under various settings, including community structures [13], adversarial settings [14] and distributed computation [15, 16]. The survey [17] discusses works in this area. Recently, [18] showed that for any gossip network, the age of a vertex (i.e., node) is equal in distribution to the first passage time from that vertex to the source. Similarly, [19] investigates how version age is affected by non-Poisson connections between nodes of the gossiping network. Many of these works have analyzed how version age scales with the number of nodes for specific topologies of the gossiping network. [13] shows that the version age under the ring topology scales as Θ⁢(n)Θ𝑛\Theta(\sqrt{n})roman_Θ ( square-root start_ARG italic_n end_ARG ). [20] shows that the version age scales as O⁢(n13)𝑂superscript𝑛13O(n^{\frac{1}{3}})italic_O ( italic_n start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 3 end_ARG end_POSTSUPERSCRIPT ) in the square grid topology, and as O⁢(log⁡n⁢log⁡log⁡n)𝑂𝑛𝑛O(\log{n}\log{\log{n}})italic_O ( roman_log italic_n roman_log roman_log italic_n ) in the unit hypercube topology. [21] analyzes complete bipartite graphs, shows that d𝑑ditalic_d-regular graphs have logarithmic version age scaling and analyzes Erdos-Renyi random graphs as well. [18] further analyzes new graphs and gives tight upper and lower bounds for the expected version age of any vertex using a new combinatorial quantity. Gossiping networks with mobility have been analyzed using the spatial mean field regime methodology in [22] and SHS framework in [23]. However, no paper has analyzed networks where the gossip links have time-varying rates while the source updates the gossiping network at constant rate. In this work, we use a new method inspired by the first passage percolation methods to analyze how varying gossiping network topologies influence the version age of nodes in the network. In the system we consider, the gossiping network changes topologies according to a continuous time Markov chain (CTMC) process. These topologies are arbitrary fixed graphs, which could be a fully connected network, a ring network, a grid network or some other network. It is possible to write SHS equations for this framework in a similar way to [5]. However, it is not clear if the system of equations obtained are stable and if the limiting version age exists. As an example, assume that we have two possible topologies, the fully connected topology and the ring topology, and we switch between these two states with linear rates (in the number of nodes). Then, we can show that the version age of a node moves from Θ⁢(log⁡n)Θ𝑛\Theta(\log{n})roman_Θ ( roman_log italic_n ) to Θ⁢(n)Θ𝑛\Theta(\sqrt{n})roman_Θ ( square-root start_ARG italic_n end_ARG ) depending on the current state. Hence, instead of analyzing the limiting average version age, we analyze how the version age of a node depends on the current topology, and if the existence of certain topologies in the network ensure low or high version age for the node. In particular, we analyze a network which has constant CTMC rates, where one of the topologies is the fully connected topology. For this case, we show that the version age of a node scales as Θ⁢(log⁡n)Θ𝑛\Theta(\log{n})roman_Θ ( roman_log italic_n ), which is the same if the network was always in a fully connected mode. That is, we show that, in this setting, there is no loss in the age scaling, even if the network topology deviates from full connectivity."
https://arxiv.org/html/2411.03376v1,An Open API Architecture to Discover the Trustworthy Explanation of Cloud AI Services,"This paper presents the design of an open-API-based explainable AI (XAI) service to provide feature contribution explanations for cloud AI services. Cloud AI services are widely used to develop domain-specific applications with precise learning metrics. However, the underlying cloud AI services remain opaque on how the model produces the prediction. We argue that XAI operations are accessible as open APIs to enable the consolidation of the XAI operations into the cloud AI services assessment. We propose a design using a microservice architecture that offers feature contribution explanations for cloud AI services without unfolding the network structure of the cloud models. We can also utilize this architecture to evaluate the model performance and XAI consistency metrics showing cloud AI services’ trustworthiness. We collect provenance data from operational pipelines to enable reproducibility within the XAI service. Furthermore, we present the discovery scenarios for the experimental tests regarding model performance and XAI consistency metrics for the leading cloud vision AI services. The results confirm that the architecture, based on open APIs, is cloud-agnostic. Additionally, data augmentations result in measurable improvements in XAI consistency metrics for cloud AI services.","Artificial Intelligence (AI) as a service is a rapidly expanding technology paradigm. The AI market is expected to grow as more companies adopt AI technology to remain competitive. Major cloud providers, including Amazon Web Services [1], Microsoft Azure [2], Google Cloud Platform [3], Alibaba Cloud [4], Oracle Cloud, IBM Cloud, and Salesforce Service Cloud, offer AI as a service. This enables customers to develop and deploy AI models using cloud-based platforms. Cloud AI services commonly achieve learning accuracy by using standard metrics such as precision, recall, and F1 score [1, 2, 3]. However, certain services such as Alibaba Cloud [4] may not explicitly provide these performance metrics. Additionally, a recent study [5] conducts a detailed investigation into the maintenance of AI services, focusing on computer vision. This research uncovers inconsistencies and evolution risks in AI services. The explainable AI (XAI) aims to develop models and methods that enable human users to comprehend, trust, and manage AI models [6]. A study [7] discusses the role of XAI in computer vision-based decisions. They emphasize that XAI can promote trust in AI computer vision systems through improved understanding and prediction. Another work [8] presents the XAI criterion that refines the functionality objectives for XAI methods. One criterion involves the analysis of feature influence and feature causality. XAI is increasingly adopted in applications that require transparency, fairness, and trustworthiness in decision-making, particularly in sensitive domains [9]. The limitation of the XAI practices is the existing XAI techniques developed are often tailored to specific types of models or cases [10], which makes the XAI practices less reusable and versatile to other applications. Meanwhile, numerous cloud AI services have been provided by cloud platforms to support general applications across domains [1, 2, 3, 4]. Together, the trend underscores the need to integrate XAI methods with cloud AI services to foster trust in cloud-based AI applications. XAI demands that activities conducted during an XAI process be traceable and reproducible [11]. Ensuring that the data utilized in the AI models and XAI methods are trustworthy becomes important. Addressing data provenance in XAI operations is essential for guaranteeing that the generated explanations are reliable, verifiable, and consistently reproducible across diverse settings. Despite the numerous XAI frameworks available, a noticeable gap exists among essential components [10] including data processing, methods configuration, and evaluation metrics. These components collectively form complex pipelines. This observation motivates our proposal: a design by a cloud-native paradigm based on the microservice architecture. This architectural style benefits from its capacity for the independent deployment of diverse components, streamlined communication via RESTful APIs, and a built-in adaptability that accommodates the introduction of new XAI methods, substitutions of AI models, and adjustments in pipeline configurations. Furthermore, our proposed architecture offers precise record capabilities. This ensures that the provenance of every XAI operation is transparent, facilitating the reproduction of XAI tasks or entire workflows. For black-box models or cloud-based AI services, the task of revealing the internal structures of AI models becomes unfeasible, especially for those AI services that encapsulate models behind standard RESTful APIs. We address this challenge by drawing inspiration from XAI methods that focus on feature influence and causality, for example, SHAP [12]. Besides, we propose a method that approximates the black-box AI model with a custom-built model and computes the feature contribution values, providing interpretable insights even from opaque AI models. Confronted by these challenges, our work is directed towards the following research questions. • RQ1: How to obtain and evaluate XAI results without unfolding the cloud AI service model structure? We investigate the cloud AI services and XAI methods in Section II-A and II-B. This enables us to understand the communication between cloud AI services and the specific requirements for XAI methods. Subsequently, we briefly summarize the applicable XAI methods in the taxonomy, Section IV-A. We also seek the packaged XAI frameworks listed in Section II-C. However, most frameworks are not explicitly compatible with cloud AI services. Therefore, we propose workflow as Figure 2 that integrates Cloud AI with Post hoc XAI, expressed in Section IV-B. Ultimately, scenarios one and two in Section VII-C and VII-B compute and evaluate the XAI results from integrating three major cloud AI services. • RQ2: What are the essential components required for XAI service architecture to deliver feature contribution explanations for models? To implement XAI within a service-oriented framework, Section V delineates the key architectural components critical for integration with existing cloud-based AI services. Following this, Section VII presents four illustrative scenarios using the designed XAI service architecture to explore typical discovery situations. • RQ3: How to collect XAI provenance data from operations to ensure traceability within the XAI service? Referring to the related works in Section II-D, we notice that the provenance data is necessary for XAI operations. Referring to the key components in XAI operations, we provide a graph format design for the XAI provenance data. Section VI introduces how to automatically collect the provenance data from various XAI operations within the XAI service. By retrieving the provenance data, we can identify differences and edit configurations to the XAI operations. In section VII-D, scenario three, we showcase a scenario that optimizes the model by modifying and executing reproduction. This scenario leads to improvements in both model performance and the XAI evaluation metrics. With the operations traceable and reproducible, we present the cloud-agnostic reproduction in scenario four, section VII-E. In this work, we propose an innovative XAI service architecture specifically designed to feature contribution explanations, illustrated through a showcase scenario drawn from computer vision cloud AI services. This method involves the utilization of approximation models to generate images, emphasizing the most contributing features. These masked images then act as inputs to create the AI services’ predictions. We calculate the prediction changes value between the original and masked images. Leveraging these prediction changes, we compute a comprehensive explanation summary for the AI services, providing a transparent overview. The main contributions are summarized as follows: • Design cloud-platform-independent XAI service framework. The open API architecture is independent of the cloud-specific AI service. The architecture accommodates first-class entities in the XAI process as unified micro-services. The communication is open API-based, thus encapsulating the variance of models, XAI methods, and inputs and outputs from feature engineering. • Provide explanations across multiple cloud AI services. Based on the definition of the XAI consistency metric, we derive an explanation summary cross-validated on multiple clouds to observe both the learning performance of AI services and data augmentation effects. • Reproduce XAI operations through configure-and-rerun. The configuration of services is the receipt of composing an end-to-end explanation workflow. By reserving the configuration of each service given a workflow definition, we accumulate the provenance of how each explanation is produced. Through the coordination center of the XAI framework, we can rerun the XAI workflow to reproduce the explanation. We demonstrate the XAI service architecture with four discovery scenarios in Section VII, including (1) Cloud AI performance evaluation, (2) XAI consistency evaluation, (3) Probing of data augmentation effect, (4) Cloud-agnostic reproduction on three major cloud service platforms includes Azure Cognitive Service [2], Google Cloud Vertex AI [3], and Amazon Web Services Rekognition [1]. Our study employs consistency metric [8] to assess the explanations derived from multiple cloud AI services. The experimental results help us observe and discover that data augmentation techniques not only enhance all cloud AI service learning performance but also improve evaluation results from the different XAI methods. The adoption of XAI frameworks is designed for data science and machine learning engineers, effectively functioning as a tool for assessment in the development of complex AI systems. A recent study [13] proposes a multi-level governance pattern that integrates team-level XAI practices with organization-level ethical standards, thereby organizing ethical principles. This work introduces an XAI service framework for AI service practitioners, ensuring alignment with ethical guidelines and organizational values. The remaining sections are structured as follows: Section II explores related works on cloud-based AI services and their explainability challenges. Section III summarizes the employed background knowledge. In Section IV, we delve into post-hoc XAI methods and their integration into cloud services. Section V presents our microservices-based XAI architecture. Section VI emphasizes the tracing and reproducibility aspects of XAI operations using provenance data. Section VII presents the setup and results of the experiment. Section VIII evaluates the XAI service from the system aspect. The paper concludes by summarizing our findings in Section IX."
https://arxiv.org/html/2411.03371v1,Blockchain-Based Multi-Path Mobile Access Point Selection for Secure 5G VANETs,"This letter presents a blockchain-based multi-path mobile access point (MAP) selection strategy for secure 5G vehicular ad-hoc networks (VANETs). The proposed method leverages blockchain technology for decentralized, transparent, and secure MAP selection, while the multi-path transmission strategy enhances network reliability and reduces communication delays. A trust-based attack detection mechanism is integrated to ensure network security. Simulation results demonstrate that the proposed algorithm reduces both handover frequency and average communication delay by over 80%, and successfully identifies and excludes more than 95% of Sybil nodes, ensuring reliable and secure communication in highly dynamic vehicular environments.","The rapid advancement of 5G technology has enabled vehicular ad-hoc networks (VANETs) to support diverse applications, from real-time traffic management to autonomous driving and safety-critical communications [1, 2, 3, 4]. However, maintaining reliable, low-latency communication in dynamic vehicular environments remains a key challenge [5]. Traditional VANETs, which rely on fixed infrastructure like roadside units (RSUs) and cellular base stations [6], often experience issues such as frequent handovers, fluctuating connectivity, and network congestion due to high vehicle mobility and varying traffic density [7, 8]. To address these limitations, mobile access points (MAPs) have been proposed, where vehicles dynamically serve as relay nodes, enhancing communication coverage and reducing dependence on fixed infrastructure. However, existing MAP selection methods, such as heuristic or static algorithms, struggle to adapt to the dynamic nature of VANETs, especially in scenarios with high mobility or security threats [4, 9]. Moreover, the absence of secure and decentralized decision-making leaves the network vulnerable to attacks, like Sybil attacks, where malicious nodes create fake identities to disrupt communication [5, 3]. In this letter, we propose a blockchain-based multi-path MAP selection strategy for secure 5G VANETs. Blockchain technology enables decentralized, transparent, and secure decision-making for MAP selection, while multi-path transmission ensures reliable communication and reduces handover frequency [7]. Additionally, an integrated Sybil attack detection mechanism enhances network security by mitigating malicious nodes [6]. Our contributions are: (1) a blockchain-based decentralized MAP selection strategy with Sybil attack detection, (2) a multi-path transmission approach that improves network reliability and reduces latency, and (3) simulation results that show significant reductions in handovers and communication delays. In summary, the key novelty of this work lies in the integration of blockchain technology for decentralized decision-making combined with a multi-path transmission strategy. This dual approach not only enhances security against Sybil attacks but also reduces handover frequency and improves overall network performance, which is critical for highly dynamic environments like VANETs."
https://arxiv.org/html/2411.03365v1,Enhanced Real-Time Threat Detection in 5G Networks: A Self-Attention RNN Autoencoder Approach for Spectral Intrusion Analysis,"In the rapidly evolving landscape of 5G technology, safeguarding Radio Frequency (RF) environments against sophisticated intrusions is paramount, especially in dynamic spectrum access and management. This paper presents an enhanced experimental model that integrates a self-attention mechanism with a Recurrent Neural Network (RNN)-based autoencoder for the detection of anomalous spectral activities in 5G networks at the waveform level. Our approach, grounded in time-series analysis, processes in-phase and quadrature (I/Q) samples to identify irregularities that could indicate potential jamming attacks. The model’s architecture, augmented with a self-attention layer, extends the capabilities of RNN autoencoders, enabling a more nuanced understanding of temporal dependencies and contextual relationships within the RF spectrum. Utilizing a simulated 5G Radio Access Network (RAN) test-bed constructed with srsRAN 5G and Software Defined Radios (SDRs), we generated a comprehensive stream of data that reflects real-world RF spectrum conditions and attack scenarios. The model is trained to reconstruct standard signal behavior, establishing a normative baseline against which deviations, indicative of security threats, are identified. The proposed architecture is designed to balance between detection precision and computational efficiency, so the LSTM network, enriched with self-attention, continues to optimize for minimal execution latency and power consumption. Conducted on a real-world SDR-based testbed, our results demonstrate the model’s improved performance and accuracy in threat detection.Keywords: 5G Security, spectrum access security, self-attention, real-time intrusion detection, RNN autoencoder, LSTM, time series anomaly detection.","The advent of the fifth generation (5G) of wireless communication systems has ushered in an unprecedented era of connectivity and innovation. With its promise of higher data rates, reduced latency, and increased capacity, 5G is set to revolutionize various sectors, including smart cities, autonomous vehicles, and the Internet of Things (IoT) [1]. Meanwhile, on the other side, it introduces significant security vulnerabilities, particularly in radio frequency (RF) communications, caused by high-density networks with a large number of access points and user equipment (UE). The flexible allocation of spectrum in 5G renders the monitoring and securing channel access more complicated [2]. Among diverse threats, RF jamming attacks emerge as a substantial threat, undermining the reliability and functionality of critical 5G network services that are fundamental to sectors like IoT and autonomous vehicles. For instance, remote surgery and autonomous driving [3], require ultra-reliable low-latency communications, which can be compromised by malicious interference. Unfortunately, traditional network security mechanisms cannot effectively cope with the threats due to multiple factors. The difficulties for traditional methods may include new evolved threats; highly dynamic spectrum access and large-scale networks. Therefore, the variety and sophistication of potential attacks, such as advanced persistent threats (APTs) and intelligent jamming, necessitate more advanced detection mechanisms. In fact, taking into account the traditional intrusion detection systems (IDS) in the context of 5G’s unique demands and threat landscape, the potential challenges to address the RF jamming attacks are detailed as follows. Dynamic spectrum access in 5G networks introduces a layer of complexity where the 5G’s spectrum is characterized by its fluidity – bandwidths vary, access policies shift frequently, and modulation schemes adapt in real-time [4]. This presents a significant challenge for traditional IDS, which are typically engineered for more static environments. The evolving threat landscape in 5G networks incorporate advanced threats like adaptive jamming and complex advanced persistent threats (APTs) [5]. These modern attacks often do not follow repetitive patterns and are designed to adapt to countermeasures, making them particularly challenging to detect by traditional IDS. The exponential growth in the number of connected devices and network nodes within 5G networks, coupled with the openness and flexibility introduced by the adoption of Open Radio Access Network (O-RAN) architecture, exacerbates these challenges. The open interfaces and disaggregated components in O-RAN can be exploited by attackers, creating vulnerabilities that traditional security methods are not equipped to handle effectively [6]. To effectively counter the aforementioned challenges in 5G networks, an advanced intrusion detection system (IDS) is paramount. This IDS must be both reactive, to counter known threats, and proactive, to adapt to emerging, unseen attack patterns. Critically, it must achieve this balance while being scalable and resource-efficient, ensuring that the intrinsic performance benefits of 5G are not compromised as the network expands in size and complexity. Addressing these requirements, our research proposes a novel IDS framework that synergizes an efficient self-attention mechanism with a recurrent neural network (RNN)-based autoencoder. This combination is strategically chosen to tackle the unique challenges posed by 5G networks. The self-attention mechanism of our solution enables the IDS to adaptively focus on specific spectrum parts more prone to anomalies [7] to enhance its efficacy in safeguarding against spectrum-related vulnerabilities. The integration of unsupervised learning capabilities in the self-attention-equipped RNN autoencoder enables the detection of both known and novel attack patterns. By learning complex dependencies within the data, the model is equipped to identify emerging cyber threats that were not part of its initial training set. The self-attention mechanism computational efficiency translates into the ability for parallel processing, a crucial factor in reducing the computational load. This ensures that the IDS can keep pace with the growing size and complexity of 5G networks, offering robust threat detection without significant resource overheads. Our research presents a sophisticated approach to addressing the complex challenges associated with RF intrusion detection in 5G networks. We have developed a model that effectively combines the temporal processing capabilities of Recurrent Neural Networks (RNNs) with the contextual sensitivity afforded by self-attention mechanisms. This integration results in a robust, efficient, and scalable system. Specifically designed to process and analyze the time-series data characteristic of the RF spectrum, our model excels at identifying anomalies indicative of potential jamming attacks. A notable aspect of our work is the deployment of a 5G Radio Access Network (RAN) test-bed, along with comprehensive databases, to facilitate the training and inference phases of our model. We have structured a sequential two-part methodology focusing on anomaly detection and subsequent classification. This approach is further bolstered by rigorous experimental validation and an extensive analysis of performance metrics. Our model demonstrates a high proficiency in detecting a diverse range of cyber threats, positioning it as a viable and effective tool for practical application in real-world 5G scenarios. The ensuing sections of this paper will explore the background, methodology, experimental setup, results, and provide a detailed analysis of our proposed model. This comprehensive examination will underscore the model’s effectiveness in protecting 5G networks against sophisticated and evolving RF threats, thus showcasing its potential as a critical asset in modern network security frameworks."
https://arxiv.org/html/2411.03354v1,LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks,"In this paper, we present an adaptive framework designed for the continuous detection, identification and classification of emerging attacks in network traffic. The framework employs a transformer encoder architecture, which captures hidden patterns in a bidirectional manner to differentiate between malicious and legitimate traffic. Initially, the framework focuses on the accurate detection of malicious activities, achieving a perfect recall of 100% in distinguishing between attack and benign traffic. Subsequently, the system incrementally identifies unknown attack types by leveraging a Gaussian Mixture Model (GMM) to cluster features derived from high-dimensional BERT embeddings. This approach allows the framework to dynamically adjust its identification capabilities as new attack clusters are discovered, maintaining high detection accuracy. Even after integrating additional unknown attack clusters, the framework continues to perform at a high level, achieving 95.6% in both classification accuracy and recall. The results demonstrate the effectiveness of the proposed framework in adapting to evolving threats while maintaining high accuracy in both detection and identification tasks. Our ultimate goal is to develop a scalable, real-time intrusion detection system that can continuously evolve with the ever-changing network threat landscape.","Network systems are becoming increasingly complex due to ongoing technological advancements. Next generations of network are expected to enable various interactions and exchanges over the Internet, supporting services such as digital brain-computer interfaces, in-body health networks, and extended reality. This transition will be underpinned by achieving data rates around terabits per second with ultra-low latency. This continuous evolution and expansion of network infrastructure lead to an extremely heterogeneous environment characterized by massive volumes of data. This can complicate network monitoring, exposing systems to malicious activities. Furthermore, as new services emerge from network improvements, sophisticated threats are likely to increase correspondingly, becoming harder to detect [8]. Therefore, it is imperative to develop robust security measures within these highly heterogeneous ecosystems. Various machine learning methods have been explored to enhance security across diverse network environments, including smart grids [16], virtual private networks [13], and IoT ecosystems [14]. Among these methods, Support Vector Machines (SVMs) are widely utilized for their ability to classify data into distinct categories, making them effective for detecting intrusions and anomalies within network traffic. Boosting methods enhance the performance of weak classifiers by combining them into a strong ensemble classifier, thereby improving detection rates for malicious activities. Deep Neural Networks (DNNs) leverage multiple layers of neurons to learn intricate patterns in data, which proves particularly useful for identifying advanced threats in large-scale networks. Recurrent Neural Networks (RNNs) are specifically designed to analyze sequences of network packets, enabling the identification of patterns indicative of cyber threats. Long Short-Term Memory (LSTM) networks excel at detecting temporal patterns in network traffic and are frequently employed in applications such as detecting Distributed Denial of Service (DDoS) attacks [9]. In addition to their diversity, the data in these ecosystems are dynamic, presenting the challenge of encountering unknown data flows during analysis. Machine learning-empowered intrusion detection systems (IDSs) often struggle to keep pace with the constant evolution of network flows, as they are typically trained to detect a fixed or predefined set of attacks [20]. However, in real-world scenarios, intrusion data are collected incrementally. As new data emerge, these models may suffer from catastrophic forgetting, a prevalent issue in machine learning where previously learned classes are forgotten when training on new ones [7]. Recently, techniques involving LLMs have demonstrated an ability to effectively manage vast volumes of data while understanding the underlying context within them. Given the unknown pattern in next generation of networks, leveraging LLMs presents a promising approach for efficiently processing and analyzing such data. Studies have shown that LLMs can address various challenges in telecommunications, such as optimizing the reward process in reinforcement learning [6], and providing zero-shot image classification in complex signal transmissions [22]. However, it is important to note that LLMs were not initially designed for these applications; their primary utility lies in natural language processing (NLP). Consequently, directly applying them to domain-specific tasks can be challenging, necessitating fine-tuning for optimal performance. To the best of our knowledge, this work proposes the first hybrid incremental intrusion detection framework that leverages LLMs to address emerging threats. Our contributions in this work are as follows: • We investigate prior works related to intrusion detection, focusing on continuous detection. • We propose a framework that leverages language models to efficiently capture contextual information within network traffic. • We evaluate the continuous detection capability of our framework on a popular IDS dataset, highlighting its effectiveness."
https://arxiv.org/html/2411.03237v1,On the Detection of Non-Cooperative RISs: ScanB𝐵Bitalic_B-Testing via Deep Support Vector Data Description,"In this paper, we study the problem of promptly detecting the presence of non-cooperative activity from one or more Reconfigurable Intelligent Surfaces (RISs) with unknown characteristics lying in the vicinity of a Multiple-Input Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division Multiplexing (OFDM) transmissions. We first present a novel wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces, which captures both the effect of the RIS actuation time on the channel in the frequency domain as well as the difference between changing phase configurations during or among transmissions. Considering that RISs may operate under the coordination of a third-party system, and thus, may negatively impact the communication of the intended MIMO OFDM system, we present a novel RIS activity detection framework that is unaware of the distribution of the phase configuration of any of the non-cooperative RISs. In particular, capitalizing on the knowledge of the data distribution at the multi-antenna receiver, we design a novel online change point detection statistic that combines a deep support vector data description model with the scan B𝐵Bitalic_B-test. The presented numerical investigations demonstrate the improved detection accuracy as well as decreased computational complexity of the proposed RIS detection approach over existing change point detection schemes.","Reconfigurable Intelligent Surfaces (RISs), comprising numerous metamaterials with dynamically tunable electromagnetic responses [1], constitute ultra-lightweight planar structures that can be used to coat building facades, room walls, or vehicles, and are recently considered as one of the candidate technologies for the next generation of wireless networks [2]. They can enable over-the-air signal propagation programmability in an energy-efficient manner, thus, transforming wireless channels to software-defined entities [3] that can be optimized for various objectives, e.g., enhanced multi-user connectivity [4], localization [5], and integrated sensing and communications [6]. The core features of RISs, with more pronounced their low hardware footprint recently including even almost transparent designs, have been also lately leveraged for eavesdropping and jamming [7, 8, 9, 10, 11, 12]. RIS-enabled threat models and proactive countermeasure designs of active and reflective beamforming as well as artificial noise were presented in [7] and [8] for different knowledge levels of the eavesdropper’s channel and unawareness of the presence of a malicious RIS. Jamming attacks based on RISs that are difficult to detect were presented in [11]. Scenarios where the adversary takes control of a legitimate RIS were discussed in [10], while [9] studied the case where an adversary places an RIS in the vicinity of a communication pair for information leakage [9]. The potential of RISs to decrease the received signal strength in a legitimate link, while enhancing it towards an eavesdropper, was analyzed in [12]. However, none of the latter works focused on detecting the presence of malicious RISs in environments where legitimate communications take place, which could then trigger efficient reactive legitimate physical-layer designs. On the other hand, as described in one of the RIS deployment use cases presented in [13], metasurfaces can be deployed from a single cellular operator in an area of intended coverage, where one or many other operators are also active. Those RISs should have a carefully designed bandwidth of influence to provide reconfigurable reflections to the owner operator, while leaving unaltered the signals spanning the bandwidth allocated to the other operator(s). The latter implies that those RISs should react similar to the surface material that hosts them, upon impinging signals from any of the unintended operators. However, such an RIS design for very closely allocated frequency bands is hard to achieve with up-to-date hardware technologies [14]. To this end, an approach that detects third-party non-cooperating RISs would help operators to sense relevant unwanted activity provoking countermeasure designs. Motivated by the latter two applications of unwanted RIS operations, we focus, in this paper, on the problem of detecting the operation of one or more non-cooperative RISs in the vicinity of point-to-point Multiple-Input Multiple-Output (MIMO) communication systems. Capitalizing on the discrete-time multipath channel model of [15] and the coupled-dipole formalism of RIS-parametrized channels of [16], we first introduce a wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces. Then, we present a novel formulation of the RIS activity detection problem as an online sequential change point detection problem [17]. Considering that the characteristics of any non-cooperative RISs in the wireless environment of interest are unknown to the intended MIMO OFDM communication system, we design a novel online, distribution-free change point detection statistic that combines a deep Support Vector Data Description (dSVDD) [18, 19] model with the scan B𝐵Bitalic_B-test [20]. Our extensive simulation results for the considered RIS detection problem showcase the superiority of our detection approach over existing change point detection methods, both in terms of detection accuracy as well as computational complexity. Notations: Lower case bold letters refer to vectors, e.g. 𝐱𝐱\mathbf{x}bold_x, and upper case bold letters indicate matrices, e.g. 𝐗𝐗\mathbf{X}bold_X. Calligraphic letters, e.g., 𝒳𝒳\mathcal{X}caligraphic_X, are reserved for sets and E⁢[⋅]𝐸delimited-[]⋅E[\cdot]italic_E [ ⋅ ] denotes the expectation operator and Pr⁢[⋅]Prdelimited-[]⋅{\rm Pr}[\cdot]roman_Pr [ ⋅ ] returns the probability. 𝟎N×Msubscript0𝑁𝑀\mathbf{0}_{N\times M}bold_0 start_POSTSUBSCRIPT italic_N × italic_M end_POSTSUBSCRIPT and 𝐈Nsubscript𝐈𝑁\mathbf{I}_{N}bold_I start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT denote the N×M𝑁𝑀N\times Mitalic_N × italic_M zero matrix and N×N𝑁𝑁N\times Nitalic_N × italic_N identity matrix, respectively. ℝℝ\mathbb{R}blackboard_R and ℂℂ\mathbb{C}blackboard_C are the sets of the real and complex numbers. Finally, ȷ≜−1≜italic-ȷ1\jmath\triangleq\sqrt{-1}italic_ȷ ≜ square-root start_ARG - 1 end_ARG is the imaginary unit and x∼𝒞⁢𝒩⁢(0,σ2)similar-to𝑥𝒞𝒩0superscript𝜎2x\sim\mathcal{CN}(0,\sigma^{2})italic_x ∼ caligraphic_C caligraphic_N ( 0 , italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) represents a complex normal random variable with zero mean and variance σ2superscript𝜎2\sigma^{2}italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT."
https://arxiv.org/html/2411.03048v1,UNet: A Generic and Reliable Multi-UAV Communication and Networking Architecture for Heterogeneous Applications,"The rapid growth of UAV applications necessitates a robust communication and networking architecture capable of addressing the diverse requirements of various applications concurrently, rather than relying on application-specific solutions. This paper proposes a generic and reliable multi-UAV communication and networking architecture designed to support the varying demands of heterogeneous applications, including short-range and long-range communication, star and mesh topologies, different data rates, and multiple wireless standards. Our architecture accommodates both adhoc and infrastructure networks, ensuring seamless connectivity throughout the network. Additionally, we present the design of a multi-protocol UAV gateway that enables interoperability among various communication protocols. Furthermore, we introduce a data processing and service layer framework with a graphical user interface of a ground control station that facilitates remote control and monitoring from any location at any time. We practically implemented the proposed architecture and evaluated its performance using different metrics, demonstrating its effectiveness.","Unmanned Aerial Vehicle Networks (UAVNs) consist of unmanned aerial vehicles (UAVs) equipped with actuators, sensors, autopilot, and wireless communication, enabling them to share data with each other and a ground control station (GCS) [1]. UAVs can autonomously perform tasks without human intervention, offering flexibility, low operating costs, and easy deployment. These advantages make UAVs suitable for diverse applications in healthcare, agriculture, military, and more, including courier services, border security, disaster management, precision agriculture, and aerial photography [2]. However, reliable communication between UAVs and the GCS is essential for monitoring, controlling, and executing tasks successfully in real-time. The demands of these UAV applications vary in terms of communication distance, end-to-end delay, topology (e.g., point-to-point (P2P), point-to-multipoint (P2M), mesh), network size, and bandwidth. Some applications require a single UAV, while others may need multiple. Additionally, some cases need only remote monitoring with pre-instructed paths, while others demand real-time monitoring and control. These diverse requirements indicate the importance of designing a robust UAV communication and networking architecture for heterogeneous applications. This design raises key questions: how to support various networking demands, wireless protocols, real-time data service to users, and multiple applications concurrently. To address the aforementioned problem, existing works [3, 4, 5, 6, 7, 8] proposed UAV communication and networking architectures based on either single UAV or multi-UAV setups for specific applications. Venkatesh et al. [3] proposed a fully autonomous UAV communication architecture using the TS832 serial wireless module, focusing on video transmission over P2P communication. However, this architecture is only suitable for single UAV applications. To address multi-UAV needs, Singh et al. [5] and Hong et al. [8] proposed multi-UAV communication architectures. Singh et al. [5] suggested a relay network of UAVs for video transmission to GCS using the Edimax WAP module, but it supports only short-range communication and lacks dynamic routing and mesh topology. On the other hand, Hong et al. [8] presented an LTE-based UAV communication architecture with P2P and P2M communication, but it does not support mesh topology and always depends on infrastructure networking. To overcome these issues, Krichen et al. [6] and Jawhar et al. [7] introduced an abstract concept of multi-UAV communication architecture using different wireless standards, but without the in-depth analysis needed to address the challenges in designing a generic UAV communication and networking architecture. Unfortunately, they did not provide any solutions for overcoming these design challenges. Existing works mostly focused on specific applications using a single wireless protocol and topology, limiting their applicability to homogeneous applications. These architectures are often incompatible with both infrastructure and infrastructure-less (adhoc) networks. Cellular networks (e.g., 3G, 4G, 5G) offer reliable connectivity in urban areas but face coverage issues in rural, forest, and border regions and lack support for mesh topology, which is crucial for UAV swarms. In contrast, infrastructure-less networks support long-range, mesh, and star topologies with moderate data rates, allowing network expansion based on application needs. However, these networks are confined to specific regions where they are established. Therefore, there is a need for a generic UAV communication architecture that provides seamless connectivity between UAVs and GCS while supporting heterogeneous applications concurrently. The trend of using IP-based wireless solutions allows UAVs to connect to the Internet for remote monitoring and control, making UAV data accessible from anywhere. However, existing architectures [9, 5, 3, 10, 11] limit GCS accessibility to local areas, restricting global data service. Some works [7, 8, 12] have proposed multi-UAV communication architectures with global GCS accessibility, but they lack support for concurrent applications and heterogeneous wireless protocols. Thus, there is a need for a generic, reliable multi-UAV communication architecture that supports heterogeneous applications and wireless standards while ensuring global GCS accessibility. In brief, the contributions in this manuscript are as follows. • We propose a generic UAV communication and networking architecture UNet that supports heterogeneous applications concurrently. UNet is designed to meet the various demands of applications, including short-range and long-range communications, star and mesh topologies, various data rates, and heterogeneous wireless standards. • To ensure seamless connectivity between UAVs and GCS, we combine support for both adhoc and infrastructure networks. • Additionally, we propose the design of a multi-protocol UAV gateway to enable interoperability among UAVs utilizing different communication protocols. • We propose a data processing and service framework integrated with the GCS to enable remote control and monitoring from anywhere, at any time. • The proposed architecture is implemented and thoroughly evaluated in outdoor settings using metrics such as handover delay, data processing delay, data delivery delay, throughput, packet loss probability, task execution time, exchanged traffic over time, and reliability. The remainder of this paper is organized as follows: Section II reviews the state-of-the-art UAV communication and networking architectures, highlighting their limitations. Section III describes the problem scenario and introduces the novel UNet architecture. Section IV presents the design and operational flow of UNet’s components. Section V discusses the implementation of UNet. Section VI presents the experimental setup and evaluates the system’s performance. Finally, Section VII concludes the paper and suggests future research directions. TABLE I: Comparison of UNet with State-of-art existing works Author Muti-UAVs Adhoc Mesh U2I Range Scalability Generic Arch Imple GCS Remarks UNet Yes Yes Yes both short and long Yes Yes Yes Globally Prototyping Li et al.[9] Yes Yes No – Yes No No Locally Abstract idea Singh et al.[5] Yes No full mesh No Short No No Lab scale Locally Venkateshet et al.[3] No No No Short No No Yes Locally Hayat et al.[10] Yes Yes No Short No No Yes Locally Krichen et al.[6] Yes Yes Yes – – – No – Abstract idea Jawhar et al.[7] Yes Yes Yes Long Yes No Yes Globally Just idea Chriki et al.[11] Yes No No – No No No Locally Hong et al.[8] Yes No Yes Long Yes No No Globally Always Infrastructure -dependent Wang et al.[12] Yes – Yes Long Yes No No Globally"
https://arxiv.org/html/2411.02757v1,Energy Efficient and Balanced Task Assignment Strategy for Multi-UAV Patrol Inspection System in Mobile Edge Computing Network,"This paper considers a patrol inspection scenario where multiple unmanned aerial vehicles (UAVs) are adopted to traverse multiple predetermined cruise points for data collection. The UAVs are connected to cellular networks and they would offload the collected data to the ground base stations (GBSs) for data processing within the constrained duration. This paper proposes a balanced task assignment strategy among patrol UAVs and an energy-efficient trajectory design method. Through jointly optimizing the cruise point assignment, communication scheduling, computational allocation, and UAV trajectory, a novel solution can be obtained to balance the multiple UAVs’ task completion time and minimize the total energy consumption. Firstly, we propose a novel clustering method that considers geometry topology, communication rate, and offload volume; it can determine each UAV’s cruise points and balance the UAVs’ patrol task. Secondly, a hybrid Time-Energy traveling salesman problem is formulated to analyze the cruise point traversal sequence, and the energy-efficient UAV trajectory can be designed by adopting the successive convex approximation (SCA) technique and block coordinate descent (BCD) scheme. The numerical results demonstrate that the proposed balanced task assignment strategy can efficiently balance the multiple UAVs’ tasks. Moreover, the min-max task completion time and total energy consumption performance of the proposed solution outperform that of the current conventional approach.","The Unmanned Aerial Vehicle (UAV) has emerged as a powerful tool in the low-altitude economy. Thanks to its exceptional mobility, flexibility, and scalability, UAVs can be deployed in various scenarios such as aerial photography, emergency rescue, and tracking. By equipping UAVs with communication devices, they can integrate with cellular networks, thereby enhancing their operational capabilities and ensuring safety [1]. Therefore, UAVs hold great promise as a valuable asset in large-scale patrol inspection systems. Recent research has optimized UAV trajectories and power management to enhance energy efficiency and communication performance in wireless networks. The work in [2] proposed an innovative approach for integrating power control with 3D trajectory optimization in UAV-enabled wireless-powered communication networks, significantly improving energy utilization and overcoming environmental obstacles. Similarly, the authors in [3] introduced a pioneering strategy for joint scheduling and trajectory optimization of charging UAVs in wireless rechargeable sensor networks, improving charging efficiency by reducing both hovering points and flight distance. However, due to the limited payload capacity and endurance of UAVs, they cannot be equipped with powerful computational devices to handle resource-intensive tasks individually, such as lidar signal processing and high-definition video analysis. To address this challenge, the UAV patrol inspection system utilizes mobile edge computing (MEC) technology, which leverages computing resources deployed at the ground base station. By integrating cellular-connected UAVs with MEC, the patrol inspection system can efficiently manage complex tasks such as power inspection, military reconnaissance, cargo delivery, and remote sensing across a wide area. For the purpose of accomplishing the implementation of extensive inspection tasks in a more efficient and quicker manner, multi-UAV collaboration is adopted to conduct the inspection operations for the patrol inspection system. Due to the diversity of inspection and flight tasks, how to balance the task allocation among multiple UAVs will consequently become an important issue. In this paper, we will analyze the energy-efficient and balanced task assignment strategy to optimize the total energy consumption and task completion time. It can assist multiple UAVs to accomplish complex inspection tasks within a broader scope by means of division of labor and cooperation. I-A Related Work To maximize the benefits of UAVs in communication, researchers have devoted considerable attention to developing UAV-assisted communication systems. The authors in [4] introduce a novel optimization framework to address the complex problem of resource scheduling in UAV-aided device-to-device (D2D) networks, significantly enhancing network capacity and energy efficiency. Considering the practical challenges of imperfect channel state information (CSI) and coordinate uncertainties, the work in [5] further proposes a robust resource allocation algorithm for energy-harvesting-based D2D communication, substantially improving network robustness and efficiency under real-world conditions. With the increasing prevalence of GBSs, integrating UAVs into cellular networks for various tasks has become a new trend. Compared with traditional UAVs operating solely as aerial base stations, cellular-connected UAVs offer significant advantages such as greater task execution range and real-time connectivity [6]. Extensive research has explored the field of cellular-connected UAVs. For instance, the work in [7] and [8] focuses on optimizing communication performance and energy efficiency between UAVs and base stations through UAV trajectory design. In the context of MEC, due to the limited computational capabilities of UAVs, it is often necessary to offload computing tasks to nearby edge computing devices. Specifically, in time-sensitive scenarios, the paper [9] primarily focuses on minimizing service delay and energy consumption. Reference [10] considers UAV trajectory optimization for serving Internet of Things (IoT) terminal devices within a finite time. Literature [11] provides a comprehensive introduction to the UAV patrol inspection platform. The application of UAVs in the inspection field offers advantages in efficiency and safety compared to manual methods [12]. Recently, several studies have focused on optimizing UAV inspection scenarios. For instance, the authors in [13] explore using patrol UAVs as aerial base stations to enhance channel conditions for users located at the cell edges. The paper [14] optimizes the three-dimensional trajectories of patrol UAVs passing through fixed points to minimize energy consumption, though it does not consider data processing issues. Considering the limited onboard energy of UAVs, the work in [15] focuses on enhancing the endurance of patrol UAVs through path planning, enabling multiple round-trips to charging stations. The authors in [16] propose a deterministic algorithm to minimize patrol time while adhering to battery constraints. Additionally, to efficiently utilize onboard energy, the work in [17] studies the maximization of UAV energy efficiency using non-orthogonal multiple access (NOMA) and imperfect CSI. However, since a single UAV is inefficient in performing large-scale tasks, it is particularly important to jointly perform tasks through multiple UAVs. To address the resource allocation problem of multiple UAVs, the authors in [18] propose a novel graph-attention multi-agent trust region reinforcement learning framework for optimizing trajectory design and resource assignment in multi-UAV communication, demonstrating superior convergence and strategy optimization. Literature [19] employs an adaptive genetic algorithm for planning UAV swarm missions, aiming to complete tasks using the shortest path and least flight time, though it does not consider energy consumption during this process. To optimize overall UAV energy consumption, the work in [20] determines the optimal power allocation strategy through a non-cooperative game model power allocation (NGPA) scheme, aiming to minimize data transmission energy consumption. Furthermore, the authors in [21] propose a resource allocation strategy based on the K-means algorithm to enhance overall UAV energy efficiency. I-B Motivations and Main Contributions From the above discussion, it can be seen that while many studies have explored improving communication quality or energy efficiency for cellular-connected UAVs by designing their flight trajectories, no literature specifically addresses enhancing overall energy efficiency through balanced task allocation for multiple inspection UAVs performing data collection and offloading tasks simultaneously. Current research on multi-UAV inspections mainly focuses on reducing task completion time and total energy consumption during the inspection process, but overlooks the data offloading aspect. In other words, to ensure the completion of data transmission tasks, UAVs must adjust their flight trajectories to achieve higher communication gains, which in turn lengthens the flight paths and incurs additional energy consumption. Therefore, this paper aims to design an efficient task allocation algorithm that minimizes the energy consumption of UAVs during task execution while ensuring the balanced and efficient completion of data transmission tasks. Recent research has explored deep learning and generative artificial intelligence (AI) methods, such as multi-task learning and diffusion models, to solve complex optimization problems [22, 23]. These methods have demonstrated significant potential in improving efficiency and solution quality for complex network and resource optimization tasks. However, considering the limited computational power of edge devices and the substantial prior information and computing resources these methods often require, we focus on leveraging easily accessible, deterministic information, such as cruise point locations, data offloading volumes, cellular network topology, etc. This allows us to optimize task assignment and UAV trajectories with lower computational time and learning cost while ensuring higher reliability. In addressing task assignment and energy consumption challenges within the patrol inspection scenario, our approach diverges from existing studies [24, 25, 26], presenting a novel balanced task allocation scheme. This paper concentrates on minimizing the total energy consumption and task completion time in a cellular-connected Multiple UAVs-MEC system, with a specific focus on factors such as data size, communication intensity, and the moving distance of multiple UAVs during patrol. The primary objective is to equalize geometry topology, communication rate, and offload volume, thereby enhancing the efficiency and stability of multi-UAV patrols. The key contributions of this paper are outlined as follows: 1. This paper proposes a UAV-MEC framework for inspection scenarios, where UAVs capture real-time environmental information and have computing capabilities to make decisions on complex tasks. Considering the high real-time data requirements in patrol scenarios, UAVs must promptly offload collected data to nearby GBSs for processing. 2. For the multi-UAV inspection problem, we propose an Energy-Efficient and Balanced Task Assignment Strategy (EBTAS). This strategy uses an improved balanced clustering algorithm to partition the patrol area into sub-regions based on the positions of patrol points, communication conditions, and offload volumes, ensuring an even distribution of tasks. Each sub-region is then treated as a hybrid Time-Energy Traveling Salesman Problem to optimize UAV traversal sequences, minimizing energy consumption and task completion time while ensuring the completion of inspection tasks. 3. Numerical results validate the effectiveness of this algorithm, showing advantages in energy consumption and task completion time compared to traditional task allocation strategies."
https://arxiv.org/html/2411.02418v1,"Data Matters:The Case of Predicting Mobile Cellular Traffic††thanks:The authors gratefully acknowledge the support received from Academy of Finland via the Centre of Excellence in Randomness and Structures, decision number 346308 and the computational resources provided by the Aalto Science-IT project.","Accurate predictions of base stations’ traffic load are essential to mobile cellular operators and their users as they support the efficient use of network resources and sustain smart cities and roads. Traditionally, cellular network time-series have been considered for this prediction task. More recently, exogenous factors such as points of presence and other environmental knowledge have been introduced to facilitate cellular traffic forecasting. In this study, we focus on smart roads and explore road traffic measures to model the processes underlying cellular traffic generation with the goal to improve prediction performance. Comprehensive experiments demonstrate that by employing road flow and speed, in addition to cellular network metrics, cellular load prediction errors can be reduced by as much as 56.5%.percent56.556.5\%.56.5 % . The code and more detailed results are available on https://github.com/nvassileva/DataMatters.","Accurate mobile cellular traffic predictions are critical to mobile operators as the efficient use of network resources and delivery of services depends largely on them. Smart cities and roads are highly dependent on the availability of mobile cellular infrastructure too. Traditionally, the traffic load at a base station (BS) is predicted based on mobile cellular key performance indicators (KPIs). Recently, with the introduction of deep learning models [1], exogenous factors, in addition to cellular time-series data, are incorporated to further facilitate the forecasting process. Typically considered are points-of-presence such as commercial centers, touristic spots or sport venues. More recent works [2] broaden this contextual modeling to include more environmental factors that characterize the surroundings of each BS. In an effort to capture mobility dynamics the handover rate is harnessed in [3]. Nonetheless, data that captures population dynamics remains mainly unexplored. Our goal is to model better the cellular traffic generation process by employing information about the density of the potential sources of cellular load together with the cellular time-series data. In contrast to the majority of the works, we focus on highways instead of urban scenarios and on short-term forecasting, which is arguably more challenging than mid- and long-term forecasting yet relevant to low-latency and high-reliability services, especially those related to smart traffic and autonomous driving. Our major contributions can be summarized in: • We capture the intricate dynamics of the mobile cellular traffic generation process by combining road traffic metrics that account for the potential sources of cellular communication load with cellular traffic time series data. • We develop a methodology for generating cellular traffic volumes based on road traffic. • We conduct comprehensive experiments for a real-world highway under a diversity of mobile cellular and road traffic conditions, which complement our first study [4]. The methodology we develop for creating cellular traffic data sets based on road traffic is explained in Section II. A description of the real highway scenario used in the experiments can be found in Section III. The traffic prediction model and the learning model we use for solving it are formulated in Section IV. We discuss the experimental setting and performance results in Section V, and conclude the paper with a summary of the main results and future prospects in Section VI."
https://arxiv.org/html/2411.02412v1,Slicing for AI: An Online Learning Framework for Network Slicing Supporting AI Services,"The forthcoming 6G networks will embrace a new realm of AI-driven services that requires innovative network slicing strategies, namely slicing for AI, which involves the creation of customized network slices to meet Quality of service (QoS) requirements of diverse AI services. This poses challenges due to time-varying dynamics of users’ behavior and mobile networks. Thus, this paper proposes an online learning framework to optimize the allocation of computational and communication resources to AI services, while considering their unique key performance indicators (KPIs), such as accuracy, latency, and cost. We define a problem of optimizing the total accuracy while balancing conflicting KPIs, prove its NP-hardness, and propose an online learning framework for solving it in dynamic environments. We present a basic online solution and two variations employing a pre-learning elimination method for reducing the decision space to expedite the learning. Furthermore, we propose a biased decision space subset selection by incorporating prior knowledge to enhance the learning speed without compromising performance and present two alternatives of handling the selected subset. Our results depict the efficiency of the proposed solutions in converging to the optimal decisions, while reducing decision space and improving time complexity.","It is anticipated that the 6G networks will have the capability to cater to an array of services, each with distinct quality of service (QoS) specifications. Such services may include multisensory extended reality, autonomous driving, and hologram video streaming. To ensure diversity in services, similar to the 5G networks, network slicing is employed to create multiple slices for various services over a shared physical network infrastructure. An economical management strategy for network slicing can enable the fulfillment of QoS requirements throughout the different phases of the lifecycle, such as preparation, planning, and scheduling [1]. Additionally, newly emerging technologies for 6G networks such as the Open Radio Access Network (O-RAN) architecture will allow the integration of native Artificial intelligence (AI) solutions to accommodate heterogeneous service deployments [2, 3, 4]. In this context, AI will become omnipresent in 6G networks, meaning that it will penetrate every aspect of the network, creating a state of ubiquitous intelligence. Network nodes will possess in-built AI capabilities, not only allowing for intelligent network management but also promoting the growth of AI-based services, such as machine learning, natural language processing, and computer vision. To fulfil diverse requirements of different AI services, it is important to implement customized network slices, namely slicing for AI. This approach allows for the creation of tailored network slices that can cater to the distinct requirements of various AI services (i.e., accuracy, learning speed, etc.), thereby optimizing the network’s overall performance[5]. Indeed, slicing for AI refers to the creation of network slices with customized resources to meet QoS requirements of diverse AI services. Hence, it can play a crucial role in the process of dynamically distributing and assigning resources in meta learning systems by allocating resources such as computation power, memory, and data samples efficiently and effectively to improve the learning process and overall performance of the meta learning algorithms. However, this problem is challenging due to the dynamics of diverse learning algorithms and mobile networks. For example, the quality and distribution of acquired data are time-varying and heavily affect the performance of AI services. Hence, novel solutions are needed to adapt to such non-stationary dynamics. Thus, this paper aims to optimize the decision-making process by proposing online learning techniques [6] that continuously observe the system’s performance without prior knowledge of the expected behavior. Several works have addressed the problem of network slicing in 5G and beyond networks to support the heterogeneous requirements of various conventional vertical services, such as ultra-reliable low-latency communication (URLLC), enhanced mobile boradband (eMBB), and massive machine-type communication (mMTC) [7, 8, 9, 10, 11]. However, few works considered the problem of slicing for AI. Most of the proposed solutions in this context leveraged offline classical optimization and reinforcement learning (RL) techniques. Classical optimization approaches assume full knowledge about the behavior of the environment while leveraging mathematical formulations to model different objectives and constraints. However, such an assumption may be impractical with some services and mobile network systems, which are highly dynamic and time-variant. On the contrary, RL approaches aim to learn a policy in a stateful system by mapping between states and actions. These approaches consider an environment that is stationary (i.e., behaving according to a non-varying distribution). Also, classical optimization and the training phase of RL are usually performed in an offline manner on an available dataset/environment. Nonetheless, an offline approach may fail in adapting to dynamic/time-varying systems. Thus, in this paper, we propose an online learning framework to address the problem of slicing for AI. The proposed framework can adapt to different system dynamics and uncertainty with regret guarantees. The main contributions of this paper are as follows: • We formulate the problem of slicing for AI services as an optimization problem with the objective of maximizing performance, which turns out to be NP-hard. • We introduce an online learning framework and propose a basic online solution to solve the formulated problem. We propose two alternatives of the solution where each adopts a pre-learning decision space reduction process to expedite the learning. The first alternative merges similar decisions to obtain a compact structure of the decision space, while the second builds upon the obtained compact structure and identifies candidates of the optimal decision to optimize the size of the action space. • We propose a subset selection of the original decision space with prior knowledge to accelerate the convergence of the solution. We consider two alternative approaches of manipulating the solution by biasing the selected subset. • We assess the solution’s performance by comparing it to optimal allocation and fixed allocation benchmarks, demonstrating its ability to converge towards optimal resource allocation. We examine the complexity and convergence of the proposed alternative online solutions. Furthermore, we compare between the trade-offs of the two biased subset selection approaches. The rest of the paper is organized as follows. We present some related work in Section II. Section III introduces the system model and the formulation of the problem as an optimization problem. Section IV presents the proposed solutions and modelling the problem into an online learning framework. Section V evaluates the performance of the solution. Finally, we conclude the paper in Section VI."
https://arxiv.org/html/2411.02805v1,NinjaDoH: A Censorship-Resistant Moving Target DoH ServerUsing Hyperscalers and IPNS,"We introduce NinjaDoH, a novel DNS over HTTPS (DoH) protocol that leverages the InterPlanetary Name System (IPNS), along with public cloud infrastructure, to create a censorship-resistant moving target DoH service. NinjaDoH is specifically designed to evade traditional censorship methods that involve blocking DoH servers by IP addresses or domains by continually altering the server’s network identifiers, significantly increasing the complexity of effectively censoring NinjaDoH traffic without disruption of other web traffic. We also present an analysis that quantifies the DNS query latency and financial costs of running our implementation of this protocol as a service. Further tests assess the ability of NinjaDoH to elude detection mechanisms, including both commercial firewall products and advanced machine learning-based detection systems. The results broadly support NinjaDoH’s efficacy as a robust, moving target DNS solution that can ensure continuous and secure internet access in environments with heavy DNS-based censorship.","The Domain Name System (DNS) is a crucial component of the Internet, responsible for translating human-readable domain names into machine-readable IP addresses. This service traditionally operates over port 53/udp and is integral to the functionality of the Web. However, traditional DNS queries are sent in plaintext, making them vulnerable to various attacks, including DNS spoofing, hijacking, and surveillance by third parties. These vulnerabilities are often exploited by governments and organizations that enforce censorship through DNS-based firewalls. DNS-based firewalls monitor DNS requests and can block access to specific domain names, thus restricting users from visiting certain websites or services. DNS-based firewalls have been used to enforce censorship against platforms such as Wikipedia, TikTok, and political websites in several regions [1, 2, 3]. To address these vulnerabilities, encrypted DNS protocols, such as DNS over TLS (DoT) [4], DNS over QUIC (DoQ) [5], DNSSEC [6], and DNS over HTTPS (DoH) [7], were developed. DoT, DoQ, and DNSSEC encrypt DNS traffic, protecting it from eavesdropping, but operate on distinct ports (853/tcp, 853/udp, and 53/tcp, respectively), which make them easy to identify and block. In contrast, DoH integrates DNS requests with regular HTTPS traffic on port 443/tcp, the same port utilized for most encrypted web traffic. This makes it much harder for firewalls to block DoH without disrupting regular internet activities, as DoH is effectively masked within standard HTTPS traffic. As a result, DoH has the advantage of bypassing firewalls that block all outgoing DNS queries on port 53 or 853. Efforts to counteract DoH’s ability to evade DNS-based censorship have led to several research initiatives focusing on specifically detecting and blocking DoH traffic. Early approaches relied primarily on list-based methods, which involve maintaining a static blocklist of well-known public DoH server IP addresses and domains to block them at the network-level firewall. However, this approach is limited due to the increasing ease of self-hosting DoH servers [8, 9] on cloud infrastructure with vast IP address spaces that can make an infrequently updated blocklist ineffective. Entirely blocking outbound HTTPS traffic to hyperscalers such as AWS [10], Google Cloud [11], or Azure [12] is impractical for most censors as these hyperscalers also host numerous essential web services. As a result, blanket blocking of hyperscalers to target DoH services would cause significant unintended disruptions, making it an ineffective censorship strategy. More sophisticated DoH detection mechanisms have also been developed that utilize machine learning (ML)-based techniques [13, 14, 15, 16]. These ML models analyze traffic flow patterns and other features to distinguish DoH traffic from regular HTTPS traffic. While some of these detection techniques have high false positive rates, their potential utility towards blocking DoH traffic emphasizes the necessity for more censorship-resistant DNS solutions. In this paper, we introduce NinjaDoH, a novel DoH client-server protocol designed to be censorship-resistant. The NinjaDoH protocol employs a moving target defense by dynamically changing the server IP address through the use of public cloud infrastructure, and securely sharing the latest server IP address with the client(s). NinjaDoH’s client-side software continuously updates the operating system to use the most recent server IP address for DoH queries. To mitigate propagation delays in sharing of new IP addresses with the client, the server temporarily keeps older IP addresses active (alongside the new IP address), ensuring continuous availability for clients in-between IP updates. While there are various methods to securely share the latest server IP address, we leverage the InterPlanetary File System (IPFS) [17] for its decentralized nature, which makes it more difficult for adversaries to detect or block [18]. The NinjaDoH client integrates fully within the operating system, making it compatible with all browsers and applications without requiring any special configuration or additional plugin. This is in contrast to out-of-band DNS methods like DNS in Google Sheets [19] or DNS over Discord [20], which lack seamless integration with user environments. This moving target architecture makes NinjaDoH highly resilient against list-based blocking methods, as the server IP addresses are frequently rotated, making it practically impossible to maintain an accurate blocklist. We also evaluate NinjaDoH’s effectiveness against ML-based detection methods, which attempt to analyze traffic patterns to block DoH queries. NinjaDoH demonstrates the ability to evade even these advanced detection systems while maintaining a high level of performance. Our key contributions in this paper are as follows: • Design and Implementation of NinjaDoH Protocol: We design and then implement the NinjaDoH protocol, a moving target DoH server that leverages public cloud infrastructure and IPNS to dynamically rotate its IP addresses, making it resilient against list-based DNS blocking and detection methods. • Comprehensive Performance Evaluation: We evaluate NinjaDoH’s performance in terms of DNS query latency and compare it against other DNS services, including well-known DoH providers and censorship-resistant alternatives like DoH over Tor. Our results demonstrate that NinjaDoH delivers low-latency performance, comparable to well-known public DoH services. • Evaluation of Censorship Resistance: We empirically demonstrate NinjaDoH’s ability to evade both static, list-based blocking and more advanced ML-based detection systems that attempt to identify DoH traffic. This adaptability ensures the DoH service remains accessible in networks with heavy censorship. • Cost Analysis: We provide analysis of NinjaDoH’s operational costs, showing that it is an affordable solution for individuals and organizations seeking a censorship-resistant DNS service. While NinjaDoH empowers users in censored networks to bypass DNS firewalls and freely access the internet, it simultaneously presents challenges for enterprise administrators who rely on protective DNS filtering for security [21]. NinjaDoH can bypass such critical defenses, highlighting the dual nature of censorship-resistant tools."
https://arxiv.org/html/2411.02617v1,TeleOracle: Fine-Tuned Retrieval-Augmented Generation with Long-Context Support for Networks,"The telecommunications industry’s rapid evolution demands intelligent systems capable of managing complex networks and adapting to emerging technologies. While large language models (LLMs) show promise in addressing these challenges, their deployment in telecom environments faces significant constraints due to edge device limitations and inconsistent documentation. To bridge this gap, we present TeleOracle, a telecom-specialized retrieval-augmented generation (RAG) system built on the Phi-2 small language model (SLM). To improve context retrieval, TeleOracle employs a two-stage retriever that incorporates semantic chunking and hybrid keyword and semantic search. Additionally, we expand the context window during inference to enhance the model’s performance on open-ended queries. We also employ low-rank adaption for efficient fine-tuning. A thorough analysis of the model’s performance indicates that our RAG framework is effective in aligning Phi-2 to the telecom domain in a downstream question and answer (QnA) task, achieving a 30% improvement in accuracy over the base Phi-2 model, reaching an overall accuracy of 81.20%. Notably, we show that our model not only performs on par with the much larger LLMs but also achieves a higher faithfulness score, indicating higher adherence to the retrieved context.","The recent interest in large language models, motivated by their unprecedented performance on various downstream tasks, has led to their widespread adoption across many domains. These models gained notable attention with a rising number of applications in text summarization, classification, and generation. In telecom, the potential gain from the incorporation of these models has not gone unnoticed [1, 2, 3, 4, 5, 6]. The integration of LLMs into IoT devices represents a crucial step towards creating autonomous, agentic systems capable of making real-time decisions at the network edge [7]. Foundation LLM models, trained on vast amounts of diverse data, offer versatility and exhibit strong performance across generalized tasks. However, this broad knowledge can result in so-called negative transfer or knowledge interference in specialized domains such as telecommunications, where domain-specific terms and concepts often conflict with their general usage. Indeed, protocols and concepts in telecom sometimes follow distinct logic patterns that differ from broader contexts. This misalignment leads to poor performance when foundation LLM models, such as GPT4, attempt to reconcile their general knowledge with telecom-specific concepts, as the former can actively interfere with the latter. In addition, the strict latency requirements and resource constraints of edge devices, particularly in massive IoT deployments, hinder the deployment of LLMs, necessitating a shift toward smaller models [8]. \Acp SLM, such as Microsoft’s Phi-2 with 2.7B parameters[9] and Gemini Nano 2 with 3.2B parameters [10], present a potential solution by offering competitive performance while maintaining the efficiency crucial for telecom applications. Notably, despite its smaller size, Phi-2 performs on par with state-of-the-art language models 25 times its size on various benchmarks. Telecom is a rapidly evolving domain that is continually adapting to accommodate the fast-paced technology development. As a result, technical papers and documents describing new ideas, standards, and protocols undergo frequent modifications, requiring artificial intelligence (AI) systems developed for the domain to be equally flexible in updating their knowledge. This limits the utility of using fine-tuning to specialize the model to the domain. Unlearning in LLMs is also a significant challenge [11]. Therefore, a more dynamic and adaptable approach, such as RAG is needed for greater flexibility and cost-effectiveness. This approach enables the LMs to seamlessly incorporate new data without the need for extensive re-training. RAG minimizes hallucinations by grounding model responses in factual information through a two-step process: (1) storing domain-specific documents in a database (in semantic space), and (2) supplementing the user’s query with relevant retrieved context at inference time. The retrieved context can also include user-specific information or real-time system updates, allowing the model to leverage the user interaction history to produce tailored responses and remain aligned with current data. For these reasons, RAG is particularly well suited for telecom applications. However, while conventional RAG models provide significant enhancement to language models’ capabilities, telecom applications require special considerations due to the complex and technical nature of the domain. Telecom standardization documents do not follow a unified structure, introducing problems when representing data in the RAG database. This becomes an issue at the retrieval stage as the lack of standardization complicates the identification and extraction of relevant information, often resulting in suboptimal matches and reduced accuracy in responses. Additionally, common terms that recur across multiple areas in telecom documents often have nuanced meanings depending on the context. This contextual complexity necessitates careful filtering of the retrieved documents to provide the generator with a list of highly relevant context chunks. These context chunks supplement the generator with the necessary information in order to allow it to adequately respond to the the given query. Moreover, open-ended or vague user queries require a larger amount of retrieved-context chunks. This demand can exceed the small language model (SLM)’s capacity, limiting its ability to handle the longer prompts required for such questions. To address the aforementioned challenges, this paper makes the following contributions to specialized LMs for telecommunications: • We develop TeleOracle, a specialized RAG framework that effectively addresses the unique challenges of processing telecommunications documentation through strategic integration of multiple techniques; • We implement an optimized document processing pipeline that combines semantic chunking with a two-stage retrieval process, enabling precise and context-aware selection of relevant technical information; • We leverage SelfExtend to extend the generator’s context window at inference time, accommodating a greater volume of retrieved information and enabling more comprehensive responses to complex or open-ended queries; • We conduct a comprehensive analysis of the proposed architecture. Notably, we show that our model not only performs on par with state-of-the-art LLMs models but also is higher on the faithfulness score, indicating higher adherence to retrieved context. The remainder of this paper is organized as follows. Section II presents the relevant works. Section III provides a comprehensive overview of the TeleOracle architecture. Our experiments and their corresponding results are detailed in Section IV. Finally, section V concludes the paper with a summary of findings and potential directions for future research."
https://arxiv.org/html/2411.02267v1,Technical Report: Performance Comparison of Service Mesh Frameworks: the MTLS Test Case,"Service Mesh has become essential for modern cloud-native applications by abstracting communication between microservices and providing zero-trust security, observability, and advanced traffic control without requiring code changes. This allows developers to leverage new network capabilities and focus on application logic without managing network complexities. However, the additional layer can significantly impact system performance, latency, and resource consumption, posing challenges for cloud managers and operators.In this work, we investigate the impact of the mTLS protocol—a common security and authentication mechanism—on application performance within service meshes. Recognizing that security is a primary motivation for deploying a service mesh, we evaluated the performance overhead introduced by leading service meshes: Istio, Istio Ambient, Linkerd, and Cilium. Our experiments were conducted by testing their performance in service-to-service communications within a Kubernetes cluster.Our experiments reveal significant performance differences (in terms of latency and memory consumption) among the service meshes, rooting from the different architecture of the service mesh, sidecar versus sidecareless, and default extra features hidden in the mTLS implementation. Our results highlights the understanding of the service mesh architecture and its impact on performance.","A service mesh is a dedicated infrastructure which controls service-to-service communication over a network and provides a way to control how different parts of an application share data with one another. Service mesh is most commonly used on top of the Kubernetes platform to overcome the challenges of a large-scale microservice-based system. A survey of the Cloud Native Computing Foundation (CNCF) [1] found that 70% of the respondents run a service mesh in production or development and 19% are in evaluation. The number of service mesh providers is constantly growing, with each offering different features, performance, ease of use, and pricing, while continuously expanding capabilities. Typically, third parties develop and maintain the service mesh layer, allowing application developers to focus on business logic without worrying about network complexities. As services work by responding to incoming requests and issuing outgoing requests, the flow of requests becomes a critical determining factor of how the application behaves at runtime. Thus, standardizing the management of this traffic is crucial for guaranteeing the application’s runtime. A CNCF survey [1] examined factors driving organizations to adopt service meshes. Security was in the top concerns, with 79% of respondents relying on techniques like mTLS to reduce attack risks. This aligns with the growing adoption of the zero-trust security model, which requires all system users to be authenticated before accessing data [2]. While service meshes offer benefits like improved reliability and consistency, their impact on application performance remains a concern for software architects. The performance overhead introduced by service meshes, typically assessed through metrics like throughput, latency, and resource consumption (CPU and memory), arises from the additional hops required for each system call. This overhead can increase request and response times, potentially reducing the overall benefits that service meshes are designed to offer. The goal of this work is to compare the performance of four common service mesh models—Istio [3], Istio Ambient [4], Linkerd[5], and Cilium [6]—in terms of latency and resource usage. Further, we aim to study the following core questions: How does the mTLS protocol affect network performance? What is the performance cost of offloading mTLS logic to the service mesh? We seek to offer a comprehensive assessment of the trade-offs involved in using service meshes in performance-critical environments. To evaluate the service mesh frameworks, we set up a testing environment that simulates a production-like cloud environment. During testing, we monitor the system’s behavior under load through standard monitoring tools such as Prometheus[7]. Table I summarizes the main difference between the service-mesh architectures different characteristics (explained in detailed in Section II) and the main experiment results (explained in detailed in Section V). We included GitHub star counts as an indication of popularity. Service Mesh Model Proxy GitHub Stars P99 Latency (seconds) CPU (cores) Memory (MiB)1111 MiB (Mebibyte) is equal to 220superscript2202^{20}2 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT bytes Baseline - - - 0.22 client: 0.08 client: 152 server: 0.12 server: 71 Istio Sidecar Envoy 35.9k +0.380.38+0.38+ 0.38 client: +0.810.81+0.81+ 0.81 client: +255255+255+ 255 server: +0.870.87+0.87+ 0.87 server: +169169+169+ 169 Istio Sidecarless Ztunnel 35.9k +0.020.02+0.02+ 0.02 client: +0.230.23+0.23+ 0.23 client: +2626+26+ 26 Ambient (node agent) server: +0.230.23+0.23+ 0.23 server: +2626+26+ 26 Linkerd Sidecar Linkerd2 10.6k +0.090.09+0.09+ 0.09 client: +0.290.29+0.29+ 0.29 client: +6262+62+ 62 -proxy server: +0.220.22+0.22+ 0.22 server: +6363+63+ 63 Cilium Sidecarless Envoy 20.1k +0.220.22+0.22+ 0.22 client: +0.120.12+0.12+ 0.12 client: +9595+95+ 95 (node agent) server: +0.080.08+0.08+ 0.08 server: +9595+95+ 95 TABLE I: Comparison of Service Mesh Models Performance (3200 RPS, 1600 connections, intra-node communication). The order of preference is: green >>> yellow >>> orange >>> red. The numbers are relatively (+++) to the baseline. Service Mesh Model Proxy GitHub Stars P99 Latency (seconds) CPU (cores) Memory (MiB) Baseline - - - 0.22 client: 0.08 client: 152 server: 0.12 server: 71 Istio Sidecar Envoy 35.9k +0.37 client: +0.74 client: +355 server: +0.74 server: +173 Istio Sidecarless Ztunnel 35.9k +0.03 client: +0.23 client: +86 Ambient (node agent) server: +0.24 server: +97 Linkerd Sidecar Linkerd2 10.6k +0.08 client: +0.22 client: +53 -proxy server: +0.15 server: +55 Cilium Sidecarless Envoy 20.1k +0.22 client: +0.10 client: +93 (node agent) server: +0.03 server: +94 TABLE II: Comparison of Service Mesh Models Performance (3200 RPS, 1600 connections, inter-node communication). The order of preference is: green >>> orange >>> red. The numbers are relative (+) to the baseline. Our experiments reveal significant performance differences among the service meshes. Enforcing mTLS increased latency across all tested providers, with increases of 166% for Istio, 8% for Istio Ambient, 33% for Linkerd, and 99% for Cilium. In some tests, Istio’s latency increase was almost four times that of Linkerd and more than 6 times that of Istio Ambient. Furthermore, our results highlight the performance benefits of a sidecarless architecture and the use of eBPF. To understand the root cause of Istio’s high latency, we discovered that specific steps in request processing, such as HTTP parsing, which were part of the default mTLS implementation, significantly contribute to the overall performance overhead. Our findings can be used by decision-makers to select the most appropriate service mesh provider and architecture for their applications."
https://arxiv.org/html/2411.02164v1,A Survey on AI-driven Energy Optimisation in Terrestrial Next Generation Radio Access Networks,"This survey uncovers the tension between AI techniques designed for energy saving in mobile networks and the energy demands those same techniques create. We compare modeling approaches that estimate power usage cost of current commercial terrestrial next-generation radio access network deployments. We then categorize emerging methods for reducing power usage by domain: time, frequency, power, and spatial. Next, we conduct a timely review of studies that attempt to estimate the power usage of the AI techniques themselves. We identify several gaps in the literature. Notably, real-world data for the power consumption is difficult to source due to commercial sensitivity. Comparing methods to reduce energy consumption is beyond challenging because of the diversity of system models and metrics. Crucially, the energy cost of AI techniques is often overlooked, though some studies provide estimates of algorithmic complexity or run-time. We find that extracting even rough estimates of the operational energy cost of AI models and data processing pipelines is complex. Overall, we find the current literature hinders a meaningful comparison between the energy savings from AI techniques and their associated energy costs. Finally, we discuss future research opportunities to uncover the utility of AI for energy saving.","Energy and carbon reductions for mobile networks have never been more important given the goal to meet net-zero by 2050 and user data traffic is estimated to rise five-fold in moving to fifth generation (5G). The radio access network (RAN) remains a significant energy consumer (estimated 87% of network operations and up to 40% of operational expenditure (OPEX)) [1]. This has led to a push for artificial intelligence (AI) driven solutions for energy reduction in RAN deployments [2]. However, AI itself can have a large energy cost. Estimates for the energy cost of training a large-language model (LLM) such as OpenAI’s GPT-3 stand at 1,287 MWh, whereas estimates for operational energy demand stand at 564 MWh [3]. Meta [4] estimates the energy footprint of AI inference of an in-house recommendation model (RM) to account for 40% of the whole model energy consumption. Similarly, Google [5] estimates AI inference alone accounted for 9% of their total energy use between 2019 and 2021. This survey paper focuses on the RAN and looks at how AI/ machine learning (ML) can be used to reduce power consumption but also to consider the power consumption of the required AI inference. In particular, we investigate if the power cost of algorithms to reduce energy consumption can ever approach or exceed the energy saved. A high-level overview of the topics covered can be found in Fig. 1. We begin with a survey of RAN power consumption models asking whether the research community has a good and well-evidenced model of the power used by a RAN and this will be the basis for an accurate estimate of power saved. Following this we look at the different optimization models used to reduce power consumption considering the physical techniques used (what RAN parameters are being changed to get the power savings) and what AI techniques are being used to achieve this. We limit our survey to techniques that are already deployed or standardized and ready to deploy RAN technologies and report results with improvements in energy saving or energy efficiency. Finally, we investigate the question of how much energy might be consumed by AI models deployed for energy reduction. Because timeliness is vital in a rapidly moving field like this one we have chosen papers published in 2020 or afterwards with a few exceptions where older papers are a vital part of later understanding. To answer the questions above, this survey is structured as follows. The remainder of this section reviews related survey papers highlighting the key differences of this work. This is followed by an outline of the scope of this survey. Section II introduces the 5G RAN architecture as a grounding for discussing power models in Section III. In Section IV, we survey the literature on energy-saving techniques, highlighting the key contributions in the time, frequency, power and spatial domains. In Section V, we review the areas that impact the energy cost of AI inference in the next-generation radio access network (NG-RAN) and, where required, draw in the broader research literature. Finally, in Section VI, we present concluding remarks with suggestions for future research directions. A note on terminology: the terms ML and AI are often used somewhat interchangeably, to avoid the somewhat clumsy ML/AI we will use AI throughout in this survey unless there is a good reason to prefer the term ML in context (for example where the authors of a paper use this term). Many (but not all) techniques discussed have both a training phase (done once only or at infrequent intervals) which produces the parameters used by the model and an inference phase that produces the answer given a set of parameters. The training phase is typically more computationally intensive but, in a production network, the inference phase needs to be used every time an answer is required hence cannot be avoided as an operational cost. {forest} for tree= grow’=east, forked edges, draw, align=center, edge=-latex, anchor=west, child anchor=west, l sep+=10pt, tier/.wrap pgfmath arg=tier #1level() [ ,draw=none, edge=none [RAN Power Consumption [Analytical modeling] [Empirical modeling] ] [Energy saving techniques [Time domain] [Frequency domain] [Power level adjustment] [Spatial optimization] ] [Operational costs of AI [Computation costs in general] [RAN specific considerations] ] ] Figure 1: High-level taxonomy of topics covered in this survey I-A Related Survey Papers TABLE I: A comparison of our work with other survey papers since 2020 Refs Topic [6] [7] [8] [9] [10] [11] Our work Current RAN challenges Empirical RAN power models AI power factors Sleep modes Rate splitting Interference management Reviews that focus on AI for power-saving in the RAN are well studied and the major competing surveys in this space since 2020 are [6, 7, 8, 9, 10, 11]. To the best of the authors knowledge, the novelty in this work is an emphasis on also considering the energy cost of AI. A summary of the other papers compared with this one is given in Table I. Some surveys prefer to give their attention to future enablers for 6G technology [6, 10, 11] that are not covered by this paper. By contrast, our focus is on technologies deployed today or standardized and ready for deployment, by studies where quantitative energy savings are reported which could be immediately beneficial. While three of the other studies include power consumption models [7, 9, 11] the surveys [7, 9] do not break down models into analytical or empirical and the other [11] uses only older third generation (3G) models of power. This survey, by contrast, offers a timely breakdown of the analytical and empirical power consumption models using current generation technology. Most surveys do not cover the downside of optimization, the energy cost of AI. While [9] highlights computational effort as the number of operations per second, this still misses a huge number of factors that contribute to algorithmic power consumption. By contrast, this survey details the factors involved in the power consumption of an AI algorithm. The only survey the authors found that covers this field reasonably is [12] but this survey is now five years old whereas we focus on AI techniques from 2020 onward. This is the key differentiator between this survey and others in the field. Other surveys have included a number of works that look at techniques to manage energy consumption in the RAN but we believe this to be the most up-to-date and complete. Both [7, 10] give extensive explanations on how sleep modes and different levels of shutdowns work for power saving at a base station, whereas [11] focus on ways to maximize sleep duration. These surveys are from 2022 and 2023 respectively so our survey complements and updates them. Interference management for energy efficiency is covered in [6, 11] but the former focuses only on remote radio head clustering in cloud RAN and the latter on only techniques that modify transmit power. The survey [10] highlights the novelty of rate splitting (RS) for efficiency which we also cover. In this survey, we look at how scheduling techniques can help to reduce delay, power consumption and maximize profit for an operator. This is a promising area of research, but discussions in the literature have been sparse in recent works. For instance, [7] do not consider it and [9] limits their discussion to one study. In contrast, [10] covers multiple operator sharing and baseband workload scheduling. I-B Scope and Contributions This survey focuses on the impact of AI-based algorithms on reducing power usage and the energy cost of that AI and focuses on developments since 2020 (although older papers are included, particularly in considering power estimation, where they remain the state-of-the-art). The survey is of viable techniques used in current 5G installations where energy savings are explicitly reported. We categorically do not cover supply-side power management technologies such as improved power generation, renewable energy, battery, or smart grid technologies. We recognize the potential utility of post 5G technologies, such as nonterrestrial networks [13], optical wireless communication (OWC) [14, 15] and terahertz (THz) [16, 17] communications but these are not our focus here. Our main contributions include: 1. Identifying the analytical and empirical power consumption models in the RAN. We compare how power consumption models are delineated based on the scope and architecture. 2. A timely review of leading research on RAN energy efficiency (EE), classifying the studies by their leading degree of freedom (e.g. time, frequency, power, and spatial domains). 3. Discussion of the factors that impact the operational energy cost of using AI techniques as this may mitigate any savings made. Following the survey, we highlight the gaps in the existing research, providing insights into directions for future research on AI for improving RAN EE."
https://arxiv.org/html/2411.02086v1,Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism,"Railway Turnout Machines (RTMs) are mission-critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. Due to frequent operations and exposure to harsh environments, RTMs are susceptible to failures and can potentially pose significant safety hazards. For safety assurance applications, especially in early-warning scenarios, RTM faults are expected to be detected as early as possible on a continuous 7x24 basis. However, limited emphasis has been placed on distributed model inference frameworks that can meet the inference latency and reliability requirements of such mission-critical fault diagnosis systems, as well as the adaptation of diagnosis models within distributed architectures. This has hindered the practical application of current AI-driven RTM monitoring solutions in industrial settings, where single points of failure can render the entire service unavailable due to standalone deployment, and inference time can exceed acceptable limits when dealing with complex models or high data volumes. In this paper, an edge-cloud collaborative early-warning system is proposed to enable real-time and downtime-tolerant fault diagnosis of RTMs, providing a new paradigm for the deployment of models in safety-critical scenarios. Firstly, a modular fault diagnosis model is designed specifically for distributed deployment, which utilizes a hierarchical architecture consisting of the prior knowledge module, subordinate classifiers, and a fusion layer for enhanced accuracy and parallelism. Then, a cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is developed to minimize the overhead resulting from distributed task execution and context exchange by strategically partitioning and offloading model components across cloud and edge. Additionally, an election consensus mechanism is implemented within CEC-PA to ensure system robustness during coordinator node downtime. Comparative experiments and ablation studies are conducted to validate the effectiveness of the proposed distributed fault diagnosis approach. Our ensemble-based fault diagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset collected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA demonstrates superior recovery proficiency during node disruptions and speed-up ranging from 1.98x to 7.93x in total inference time compared to its counterparts.","Railway transportation offers a high-capacity, cost-effective, and environmentally friendly solution for long-distance travel, making it a popular choice for passenger and freight services in Europe, Asia, and North America. According to M&M market research [1], the global railway system was valued at $25.1 billion in 2022 and is estimated to reach $30.9 billion by 2027. The Railway Turnout Machines (RTMs), also known as the Railway Point Machines (RPMs), are critical components of the railway transportation infrastructure, responsible for directing trains onto desired tracks. However, RTMs are prone to failures due to wearing caused by frequent operations and exposure to harsh outdoor environments. Statistical analysis reveals RTMs as one of railside equipment that experience the highest failure rates, accounting for 18% of all documented railway system failures occurring between 2011 and 2017 [2]. The malfunction of RTMs can lead to catastrophic accidents such as collisions and train derailments, resulting in severe casualties and property losses. This typically involves the concept of preventive maintenance [3], which calls for regularly scheduled inspections and repairs targeting at the prevention of failures before they occur. For a long time, such condition-based maintenance mainly depends on the expert knowledge and experience of railway workers and thus can be time-consuming and labor-intensive. Therefore, an unsupervised, resilient, and responsive RTM fault early-warning system for train drivers and maintenance groups has raised lots of concern in the industry. With the advent of information technology, Railside Monitoring Units (RMUs) are deployed to collect runtime data during the operation of RTMs. Numerous fault diagnosis methods have been developed utilizing the collected data on vibration [4], current [5, 6, 7], torque and acoustic signals [8], etc. Previous endeavors have been primarily dedicated to enhancing model accuracy, while paying little attention to the performance and reliability issues caused by inappropriate deployment methods [9]. For safety assurance applications, especially in early-warning scenarios, we expect faults to be detected as early as possible to provide drivers and maintenance groups with more response time. The high computational overhead and complex procedures of these fault diagnosis models can make real-time inference challenging on resource-constrained devices such as Personal Computers (PCs). The traditional standalone deployment [10], where all the model components are deployed on a single device or platform, is also susceptible to system-wide unavailability in case of any software or hardware malfunctions on that centralized node [11]. Cloud computing has then become a common approach to wide range of fault diagnostic applications in Industry 4.0 [12], micro-electromechanical systems (MEMS) [13], Cloud Native [14], etc. However, the data gathered must be sent to the cloud to harness the high-performance and elastic advantages of cloud computing. In addition to privacy concerns [15] stemming from the sensitive nature of sensor data (e.g., route schedules and geographical locations), the transmission of data in railway environments like underground tunnels, inevitably leads to data loss and network latency issues [16]. These factors significantly impair the real-time capabilities of cloud-based solutions and hinder their effectiveness in monitoring mission-critical infrastructure [17]. In the past decade, academic interest has grown in combining edge computing with fault detection for model deployment, also known as Edge Intelligence (EI) [18]. This novel approach shifts computation from centralized cloud servers to the network edge, offering latency [19], energy consumption [20], Quality of Service (QoS) [21] and mobility [22] enhanced solutions. Federated Learning (FL) [23] has emerged as a potent approach for preserving privacy during model training, which enable each distributed client to train a local replica of the global model with its own dataset before sending updates to aggregate the shared global model. However, limited emphasis has been placed on distributed model inference frameworks that can meet the latency and reliability requirements of the fault diagnosis model deployment, or on tailoring the diagnosis models to perform optimally within distributed architectures. The inherent complementarity of cloud and edge computing has fostered the concept of cloud-edge collaboration [24], a paradigm that dynamically allocates and coordinates computational tasks across cloud and edge. This collaborative approach has inspired new paradigms for AI-driven real-time and downtime-tolerant monitoring tasks in mission-critical industrial applications [25] , where such systems benefit from the high availability characteristic of modern cloud computing infrastructure and the low-latency capabilities afforded by edge computing deployments. Therefore, a RTM fault diagnosis model optimized for distributed deployment, coupled with its edge-cloud collaboration empowered model inference framework is proposed in this paper, where model components are strategically partitioned and offloaded jointly across cloud and edge rather than relying solely on cloud or local to facilitate reliability and faster response. The main contributions of this paper can be summarized as: • A parallel-optimized RTM fault diagnosis model is developed with model integration technique. The model incorporates an enhanced three-stage segmentation scheme as prior knowledge and the outputs of multiple sub-classifiers are fused by a fuzzy-based ensemble mechanism to form the final classification result. • A cloud-edge collaborative framework leveraging pipeline parallelism, namely CEC-PA, is proposed to address the real-time and robustness challenges of distributed fault diagnosis. CEC-PA partitions the integrated model components into pipelines and intelligently schedules them across all worker nodes. Additionally, a downtime-tolerant mechanism is proposed to ensure system robustness. • Extensive experiments are conduced to evaluate the effectiveness of the proposed fault detection model and CEC-PA framework. Results showcase our ensemble-based fault diagnosis model produce accurate predictions across all fault types and CEC-PA outperform other approaches in terms of real-time performance and reliability. The rest of this paper is organized as follows: Section II discusses previous works on parallelization techniques in distributed AI. Section III presents the preliminary discussion on the working principle and current pattern analysis of three-stage turnouts. Section IV establishes the time consumption model and multi-objective optimization problem of the proposed cloud-edge RTM fault early-warning system. Section V implements the parallel-optimized turnout fault diagnosis scheme and provides a detailed description of the interactions between each module. Section VI presents the design details of CEC-PA. Section VII demonstrates the effectiveness of the fault diagnosis model and CEC-PA through comparative experiments. Finally, Section VIII draws a conclusion of this paper and highlights its future research directions. TABLE I: Comparison of Different Parallelization Strategies Key Characteristics Data Parallelization Model Parallelization Pipeline Parallelization Applicable scenarios Large datasets with smaller models Extremely large models Long pipelines Proof of convergence ✓✓\checkmark✓ ×\times× ✓✓\checkmark✓ Heterogeneous cluster support ✓✓\checkmark✓ ×\times× ✓✓\checkmark✓ Load balance ×\times× ✓✓\checkmark✓ ✓✓\checkmark✓ Communication overhead High Low Moderate Implementation difficulty Low High Moderate Scalability High Moderate High"
https://arxiv.org/html/2411.01924v1,Fairness-Utilization Trade-off in Wireless Networks with Explainable Kolmogorov-Arnold Networks,"The effective distribution of user transmit powers is essential for the significant advancements that the emergence of 6G wireless networks brings. In recent studies, Deep Neural Networks (DNNs) have been employed to address this challenge. However, these methods frequently encounter issues regarding fairness and computational inefficiency when making decisions, rendering them unsuitable for future dynamic services that depend heavily on the participation of each individual user. To address this gap, this paper focuses on the challenge of transmit power allocation in wireless networks, aiming to optimize α𝛼\alphaitalic_α-fairness to balance network utilization and user equity. We introduce a novel approach utilizing Kolmogorov-Arnold Networks (KANs), a class of machine learning models that offer low inference costs compared to traditional DNNs through superior explainability. The study provides a comprehensive problem formulation, establishing the NP-hardness of the power allocation problem. Then, two algorithms are proposed for dataset generation and decentralized KAN training, offering a flexible framework for achieving various fairness objectives in dynamic 6G environments. Extensive numerical simulations demonstrate the effectiveness of our approach in terms of fairness and inference cost. The results underscore the potential of KANs to overcome the limitations of existing DNN-based methods, particularly in scenarios that demand rapid adaptation and fairness.","The advent of 6G wireless networks heralds a new era of connectivity, promising to revolutionize sectors such as healthcare, education, logistics, and transportation [1]. These next-generation networks are poised to deliver unprecedented capabilities, including ultra-high data rates, massive device connectivity, and adaptive responses to highly dynamic environments [2]. Central to realizing these advancements is the efficient allocation of user transmit powers, a critical factor that directly influences network performance, user experience, and energy efficiency. In recent years, the complexity of this challenge has led researchers to explore innovative solutions leveraging Machine Learning (ML) techniques, with a particular focus on Deep Neural Networks (DNNs). Among the studies addressing the transmit power allocation problem in this rapidly evolving field, several notable approaches stand out. Nasir et al. [3] and Sheu et al. [4] employed Deep Q-Learning (DQL) to maximize the sum data rate of users, utilizing channel information as input. Li et al. [5] extended the same approach to a distributed setting. Jamous et al. [6] applied DQL to optimize transmission energy efficiency. Zhang et al. [7] innovated by using convolutional DNNs with users’ geographical information to maximize aggregate data rates. In a different approach, Zhang et al. [8] implemented Proximal Policy Optimization (PPO) with signal strength inputs to ensure predefined Signal-to-Interference-plus-Noise Ratio (SINR) thresholds. Huang et al. [9] also utilized PPO, focusing on maximizing the sum of data rates. While existing DNN-based methods have demonstrated considerable performance, they face significant challenges in two key areas: balancing network utilization with fairness, and achieving computational efficiency during inference. Most of the existing studies have primarily focused on system-wide performance indicators, such as aggregate data rates, often at the expense of equitable resource allocation among individual users. This oversight becomes particularly critical in the context of future services, where semantic-aware communication is expected, and ensuring fair participation for each user is essential to maintain the quality and diversity of outcomes, thereby mitigating potential biases from specific sources [10]. Furthermore, the DNN-based techniques prevalent in the literature are predominantly black-box models, necessitating complex computations for each inference. This computational intensity often results in prolonged inference times [11]. Such inefficiency is particularly problematic in the dynamic environments anticipated for 6G services, where rapid adaptation to changing environmental conditions is paramount. To address the gaps in existing research, this paper focuses on investigating the power allocation problem with the objective of optimizing α𝛼\alphaitalic_α-fairness. The α𝛼\alphaitalic_α-fairness metric offers a versatile framework for balancing the trade-off between fairness and utilization in resource allocation. By modulating α𝛼\alphaitalic_α, we can achieve various fairness objectives, providing a flexible approach suitable for dynamic future services. To tackle this problem, we employ a novel class of machine learning models known as Kolmogorov-Arnold Networks (KANs), which have been proposed as an alternative to conventional DNNs [12]. KANs are designed to approximate continuous multivariate functions using learnable activation functions within a relatively simple architecture, offering improved generalization capabilities. The reliance on these functions renders KANs fully explainable, significantly reducing the computational overhead typically associated with inference. This characteristic makes KANs particularly well-suited for time-sensitive and resource-constrained environments, offering an attractive solution for next-generation communication systems. The remainder of this paper is structured as follows. Section II presents the system model, provides a comprehensive problem formulation, and proves the NP-hardness of the considered problem. Section III elucidates the proposed KAN-based solution, encompassing its fundamental principles, as well as the proposed dataset generation and decentralized training algorithms. In Section IV, we present and analyze numerical results, with a particular focus on evaluating the efficiency of the proposed solution in terms of fairness and inference cost. Finally, Section V concludes the paper with a summary of our findings and closing remarks on the implications and potential future directions of this research."
https://arxiv.org/html/2411.01906v2,Connection Performance Modeling and Analysis of a Radiosonde Network in a Typhoon,"This paper is concerned with the theoretical modeling and analysis of uplink connection performance of a radiosonde network deployed in a typhoon. Similar to existing works, the stochastic geometry theory is leveraged to derive the expression of the uplink connection probability (CP) of a radiosonde. Nevertheless, existing works assume that network nodes are spherically or uniformly distributed. Different from the existing works, this paper investigates two particular motion patterns of radiosondes in a typhoon, which significantly challenges the theoretical analysis. According to their particular motion patterns, this paper first separately models the distributions of horizontal and vertical distances from a radiosonde to its receiver. Secondly, this paper derives the closed-form expressions of cumulative distribution function (CDF) and probability density function (PDF) of a radiosonde’s three-dimensional (3D) propagation distance to its receiver. Thirdly, this paper derives the analytical expression of the uplink CP for any radiosonde in the network. Finally, extensive numerical simulations are conducted to validate the theoretical analysis, and the influence of various network design parameters is comprehensively discussed. Simulation results show that when the signal-to-interference-noise ratio (SINR) threshold is below -35 dB, and the density of radiosondes remains under 0.01/km³, the uplink CP approaches 26%, 39%, and 50% in three patterns.","Typhoon, commonly known as a tropical cyclonic weather event, typically develops in tropical and subtropical oceanic zones, exhibiting forceful cyclonic storms with a well-defined eye and encircling convective bands. Typhoons frequently bring about powerful winds, substantial precipitation, and surging storms, making devastating impacts on coastal regions and human activities [1, 2, 3]. Understanding the formation and developmental patterns of typhoons is of significant importance for scientific inquiry, meteorological prediction, and ensuring public safety. A precise prediction of typhoons is crucial for mitigating the adverse impacts caused by these weather events. Detecting typhoons serves as an essential prerequisite in this regard. A number of studies utilized Internet of Things (IoT) networks to detect typhoons [4, 5, 6, 7, 8, 9]. However, these networks are typically deployed on the ground or the ocean rather than within the typhoon itself, making it challenging to obtain real-time meteorological data from the upper layers of the typhoon. Consequently, typhoon detection suffers from reduced accuracy and timeliness. To overcome the limitation, the deployment of a radiosonde network becomes essential. Deploying a radiosonde network is recognized as a pivotal method for typhoon detection [10, 11, 12]. The radiosonde network consists of a number of radiosondes (i.e., a type of IoT devices) capable of sensing and transmitting information about the inner structure of typhoons. Receivers are installed on a high-altitude platform (e.g., an airship). Primarily, through instantaneous measurement of vertical atmospheric factors such as temperature, humidity, and pressure, radiosondes provide valuable information for scientists to comprehensively analyze the inner structure and evolutionary progression of typhoons[13]. Overall, the data measured and collected from the radiosonde network provides a crucial foundation for improving the accuracy of typhoon detection and forecasts [14]. As a type of IoT network, many significant problems (e.g., network throughput and connection probability (CP) [15, 16]) should be investigated for the radiosonde network. This paper focuses on the study of CP of transmission links. Researchers in this field generally utilize stochastic geometry methods to theoretically analyze the CP of transmission links. This process involves modeling and analyzing the distribution of nodes, transmission performance, interference, etc., in a two-dimensional (2D) or three-dimensional (3D) environment. Ultimately, a closed-form expression of CP is derived through rigorous mathematical derivation. I-A Related Work The analysis of CP in wireless communications has garnered considerable attention from the research community. For instance, the authors in [17] analyzed the CP of a reconfigurable intelligent surface (RIS) aided cellular network using the stochastic geometry theory. This work derived the approximate distributions of the composite channel gains with RIS-assisted transmission. The results represented the desired signal channel and the interference channel, respectively. In [18], a stochastic geometry approach was employed to model and analyze the coverage performance of a downlink small cell network (SCN). The impact of the network statistical parameters on the coverage performance of the SCN was discussed. In [19], the authors investigated the coverage performance of a cellular network consisting of multiple base stations (BSs) and users with second-order macro-diversity, in particular, with non-coherent joint transmission (NCJT). A stochastic geometry-based analytical framework was proposed to derive the the signal-to-interference-noise ratio (SINR) coverage probability of a typical user. In summary, the above works concentrated on the CP analysis of wireless networks in a 2D space without considering the effect of vertical height. A 3D space offers a more comprehensive set of information compared to a 2D space. It allows for the consideration of an object’s position not only in a plane but also along the vertical axis, enabling a more accurate description of its location and distribution. Therefore, many researchers turned to investigating the connection performance of 3D wireless networks. For instance, in [20], the authors leveraged concepts from stochastic geometry to investigate the downlink performance of a vertical heterogeneous network (VHetNet) consisting of aerial base stations (ABSs). This work derived exact and approximate analytical expressions of a user’s CP and achievable rate. In [21], a stochastic geometry-based approach was used for the CP modeling and analysis of single- and multi-swarm unmanned aerial vehicle (UAV) networks. Simulation results revealed that there existed an optimal height and density of UAV swarms that maximized the CP. The authors in [22] utilized stochastic geometry tools to analyze the packet reception ratio (PRR) of direct communications between UAVs, with a discussion on the impact of UAV deployment height. In [23], the authors investigated the radio frequency (RF) energy harvesting and connection performance of a two hop aerial base station-aided (ABS-aided) communication system. This work obtained optimal deployment altitudes of ABSs for the best RF energy harvesting and connection performance. In [24], the coverage performance of an integrated high-altitude platform (HAP) and low-altitude platform (LAP) network was analyzed based on stochastic geometry theory. The impact of network parameters such as aerial platform altitude and LAP density was discussed. The authors in [25] presented an analytical framework to evaluate the downlink coverage performance using a stochastic geometry tool. In this framework, the locations of UAVs and users follow a typical Poisson cluster process (PCP) distribution. Simulation results revealed that the height and the density of UAVs had significant influence on the network coverage probability. The research mentioned above delved into the analysis of network performance within a 3D environment and properly considered the influence of the height parameter. Nonetheless, the height parameter was regarded as a variable rather than a random variable subject to a specific distribution in the aforementioned studies. As a result, theoretical analysis methods proposed to evaluate the connection performance of 2D wireless networks can be applied to some extent. The recent studies in [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44] have investigated the connection performance of 3D wireless networks with a specific assumption on the statistical distribution of node deployment altitude. For example, in [26], the authors proposed a novel stacked Poisson line hardcore point process (PLHCP) by introducing the randomness and the safety distance in the altitude dimension. Based on PLHCP, two tractable approximations were proposed to investigate the uplink CP of a network node. The authors in [27] studied the connection performance of a non-orthogonal multiple access (NOMA) enhanced UAV-to-Everything (U2X) network, where receivers were assumed to be distributed in a 3D sphere. They derived the closed-form expressions for the outage probability and the ergodic rate of the paired NOMA receivers using stochastic geometry tools. Besides, in [28], stochastic geometry tools were utilized to compare the CP and the average achievable rate of two 3D wireless aerial networks. The considered networks were a truncated octahedron-based aerial network and a binomial-Voronoi aerial network, where a 3D Poisson point process (PPP) was leveraged to model the locations of aerial nodes in both networks. Nevertheless, the above works did not investigate the impact of 3D environment and node mobility on theoretical analysis. These factors directly affected spatial distributions of network nodes, which in turn affected the performance analysis results. To this aim, some works in recent two years discussed the impact of their influences. For instance, in [45], the authors designed a mathematical framework to characterize the performance of an autonomous underwater vehicles (AUVs) network and theoretically analyzed the connection performance. In the AUVs network, multiple AUVs dynamically changed their 3D locations according to the random way-point (RWP) mobility model in a given underwater region. Drawing on a stochastic geometry theory, the authors in [46] studied the downlink coverage probability in a mmWave network of ground bases stations assisted by a finite number of UAV base stations under 3D blockage effects of buildings. Besides, in [47], the authors proposed interference coordination via power control under 3D blockage effects in urban environments and derived a theoretical expression of coverage probability of UAV networks with Nakagami-m fading assumption based on stochastic geometry. I-B Motivations and Challenges The existing studies indicate that the theoretical analysis of connection performance in wireless communication networks in 3D space has become a significant development trend in the research field [41, 42, 43]. Researchers typically utilize the stochastic geometry theory to conduct the theoretical analysis of network connection performance. However, most of the existing works focused on node-dense networks and assumed that network nodes were uniformly or spherically distributed in 3D space. As far as we know, the connection performance of a radiosonde network deployed in a typhoon has not been investigated. Nevertheless, a number of researchers have recently discussed the link quality of communication systems under extreme weather conditions (e.g., typhoons) [48, 49, 50, 51]. For example, the authors in [48] studied the evaporation duct’s impact on electromagnetic (EM) wave propagation during a typhoon and the effects of a typhoon eye on EM wave propagation at different signal frequencies and antenna heights. The authors in [49] investigated the effects of typhoons on underwater optical path loss. The results showed that typhoons significantly reduced the maximum detectable depth of underwater electro-optical identifiers by about 15 m. Besides, the authors in [50] proposed to enhance the link quality using a multi-connectivity strategy such that communication systems can adapt to extreme weather conditions. However, few of them investigated the impact of movement characteristics of a typhoon on network performance. The movement characteristics of a typhoon are extremely complex, and its internal aerodynamic and fluid dynamics have a strong influence on the spatial distribution of radiosondes within it. Therefore, it is essential to review the studies on the internal aerodynamic and fluid dynamics of a typhoon. Currently, some scholars are focusing on the study of the internal structure and fluid mechanics of typhoons by analyzing the development patterns of past typhoons[52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]. For instance, the authors in [52] focused on the precipitation intensification and ice-phase microphysical processes in the spiral rainband of typhoons. The study showed that the cold rain process was crucial for the precipitation intensification in the spiral rainband, with the production and growth of graupel being the most effective process for this intensification. In [56], the authors proposed a mathematical model to study the development of a tropical cyclone. The growth and maintenance of a tropical cyclone mainly depended on the air updraft caused by the latent heat of steam condensation. This mechanism is accompanied by the Coriolis force, which causes the circular air motion, and by the friction of water drops (or ice pieces) against air, which decelerates the air updraft. In [63], the authors investigated the role of the upper-level vertical wind shear (VWS) on the rapid intensification of typhoons. The simulation showed that under moderate easterly VWS, the tilting-induced convective asymmetry moved from the downshear to upshear quadrant, wrapping around the storm center, which enhanced upward motion at the upshear flank and generated upper-level divergent flow. From the research results, we may conclude that the motion patterns of radiosondes in a typhoon are particular, and we cannot directly apply the theoretical analysis approaches proposed in existing works [26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47] to modeling and analyzing CP of a radiosonde network in a typhoon. Additionally, during the process of deriving formulas such as the cumulative distribution function (CDF) and probability density function (PDF) of direct propagation distance, complex functional, differential, and integral operations are encountered. This poses a significant challenge to the theoretical derivation. I-C Contributions This paper aims to theoretically analyze the connection performance of a radiosonde network deployed in a typhoon. The main contributions of this paper are summarized as below: • We investigate two particular motion patterns of radiosondes in a typhoon. Based on the motion patterns of radiosondes, we separately model the distributions of horizontal and vertical distances from a radiosonde in a typhoon to its receiver. With the obtained distributions, we derive the closed-form expressions of CDF and PDF of the radiosonde’s 3D propagation distance to its receiver. • Based on the 3D distance distribution, we further derive the analytical expression of the uplink CP for any radiosonde in the network. • Extensive numerical simulations are conducted to validate the theoretical analysis. Besides, the impact of various network design parameters is comprehensively discussed, and some useful insights for deploying a radiosonde network are revealed. Simulation results indicate that when the SINR threshold drops below -35 dB, and the density of radiosondes stays below 0.01/km³, the uplink CP reaches around 26% and 50% in two distinct patterns and 39% in a hybrid pattern. This paper is organized as follows. Section II presents the system model, horizontal distance models, and vertical distance models. The theoretical analysis of the uplink CP is performed in Section III. Numerical results are presented and discussed in Section IV. This paper is concluded in Section V. Besides, a list of the key mathematical notations is summarized in Table I. TABLE I: LIST OF KEY NOTATIONS Notation Definition Notation Definition 𝒵𝒵\mathcal{Z}caligraphic_Z Set of the interference radiosondes 𝒫tsubscript𝒫𝑡\mathcal{P}_{t}caligraphic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT Transmit power of a radiosonde Risubscript𝑅𝑖R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, risubscript𝑟𝑖r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i∈{T⁢R,𝒵}𝑖𝑇𝑅𝒵i\in\{TR,\mathcal{Z}\}italic_i ∈ { italic_T italic_R , caligraphic_Z } Set of horizontal distance from a radiosonde to RR 𝒫rsubscript𝒫𝑟\mathcal{P}_{r}caligraphic_P start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT Received power of the RR Hisubscript𝐻𝑖H_{i}italic_H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, hisubscriptℎ𝑖h_{i}italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i∈{T⁢R,𝒵}𝑖𝑇𝑅𝒵i\in\{TR,\mathcal{Z}\}italic_i ∈ { italic_T italic_R , caligraphic_Z } Set of vertical distance from a radiosonde to RR gtsubscript𝑔𝑡g_{t}italic_g start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT Transmitting antenna gain Lisubscript𝐿𝑖L_{i}italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, lisubscript𝑙𝑖l_{i}italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i∈{T⁢R,𝒵}𝑖𝑇𝑅𝒵i\in\{TR,\mathcal{Z}\}italic_i ∈ { italic_T italic_R , caligraphic_Z } Set of direct propagation distance from a radiosonde to RR grsubscript𝑔𝑟g_{r}italic_g start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT Receiving antenna gain θisubscript𝜃𝑖\theta_{i}italic_θ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, i∈{T⁢R,𝒵}𝑖𝑇𝑅𝒵i\in\{TR,\mathcal{Z}\}italic_i ∈ { italic_T italic_R , caligraphic_Z } Set of elevation angle from a radiosonde to RR ArTsubscript𝐴subscript𝑟𝑇A_{r_{T}}italic_A start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT Rain attenuation of TR link 𝒦i,i∈{T⁢R,𝒵}subscript𝒦𝑖𝑖𝑇𝑅𝒵\mathcal{K}_{i},i\in\left\{TR,\mathcal{Z}\right\}caligraphic_K start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i ∈ { italic_T italic_R , caligraphic_Z } Set of all communication links ℳℳ\mathcal{M}caligraphic_M Set of rain attenuation of IR links gi,i∈{T⁢R,𝒵}subscript𝑔𝑖𝑖𝑇𝑅𝒵g_{i},i\in\{TR,\mathcal{Z}\}italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i ∈ { italic_T italic_R , caligraphic_Z } Small-scale Rayleigh fading caused by abundant scatterers γRsubscript𝛾𝑅\gamma_{R}italic_γ start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT Rain attenuation ratio dsi,i∈{T⁢R,𝒵}subscriptsubscript𝑑𝑠𝑖𝑖𝑇𝑅𝒵{d_{s}}_{i},i\in\{TR,\mathcal{Z}\}italic_d start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i ∈ { italic_T italic_R , caligraphic_Z } Distance through the rainfall region r𝑟ritalic_r Distance correction factor α𝛼\alphaitalic_α Path loss exponent ϵitalic-ϵ\epsilonitalic_ϵ Power control factor λnsubscript𝜆𝑛\lambda_{n}italic_λ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT, λrsubscript𝜆𝑟\lambda_{r}italic_λ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT Distribution density of radiosondes σ2superscript𝜎2\sigma^{2}italic_σ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT Thermal noise of the RR kssubscript𝑘𝑠k_{s}italic_k start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT Shape parameter in Weibull distribution λssubscript𝜆𝑠\lambda_{s}italic_λ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT Scale parameter in Weibull distribution pcsubscript𝑝𝑐p_{c}italic_p start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT Connection probability of the uplink RR⁢Xm⁢a⁢xsubscriptsuperscript𝑅𝑚𝑎𝑥𝑅𝑋R^{max}_{RX}italic_R start_POSTSUPERSCRIPT italic_m italic_a italic_x end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_R italic_X end_POSTSUBSCRIPT Maximum horizontal distance within the communication range of RR HR⁢Xm⁢a⁢xsubscriptsuperscript𝐻𝑚𝑎𝑥𝑅𝑋H^{max}_{RX}italic_H start_POSTSUPERSCRIPT italic_m italic_a italic_x end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_R italic_X end_POSTSUBSCRIPT Maximum vertical distance HR⁢Xm⁢i⁢nsubscriptsuperscript𝐻𝑚𝑖𝑛𝑅𝑋H^{min}_{RX}italic_H start_POSTSUPERSCRIPT italic_m italic_i italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_R italic_X end_POSTSUBSCRIPT Minimum vertical distance"
https://arxiv.org/html/2411.01902v1,Efficient Conflict Graph Creation for Time-Sensitive Networks with Dynamically Changing Communication Demands,"Many applications of cyber-physical systems require real-time communication: manufacturing, automotive, etc. Recent Ethernet standards for Time Sensitive Networking (TSN) offer time-triggered scheduling in order to guarantee low latency and jitter bounds. This requires precise frame transmission planning, which becomes especially hard when dealing with many streams, large networks, and dynamically changing communications. A very promising approach uses conflict graphs, modeling conflicting transmission configurations. Since the creation of conflict graphs is the bottleneck in these approaches, we provide an improvement to the conflict graph creation. We present a randomized selection process that reduces the overall size of the graph in half and three heuristics to improve the scheduling success. In our evaluations we show substantial improvements in the graph creation speed and the scheduling success compared to existing work, updating existing schedules in fractions of a second. Additionally, offline planning of 9000 streams was performed successfully within minutes.","References [1] “IEEE Standard for Local and metropolitan area networks – Bridges and Bridged Networks - Amendment 25: Enhancements for Scheduled Traffic,” IEEE Std 802.1Qbv-2015 (Amendment to IEEE Std 802.1Q-2014 as amended by IEEE Std 802.1Qca-2015, IEEE Std 802.1Qcd-2015, and IEEE Std 802.1Q-2014/Cor 1-2015), pp. 1–57, 2016. [2] T. Stüber, L. Osswald, S. Lindner, and M. Menth, “A survey of scheduling in time-sensitive networking (tsn),” 2022. [Online]. Available: https://arxiv.org/abs/2211.10954 [3] F. Dürr and N. G. Nayak, “No-wait packet scheduling for IEEE time-sensitive networks (TSN),” in Proceedings of the 24th International Conference on Real-Time Networks and Systems, ser. RTNS ’16. New York, NY, USA: Association for Computing Machinery, 2016, p. 203–212. [Online]. Available: https://doi.org/10.1145/2997465.2997494 [4] M. Vlk, K. Brejchová, Z. Hanzálek, and S. Tang, “Large-scale periodic scheduling in time-sensitive networks,” Computers & Operations Research, vol. 137, p. 105512, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0305054821002549 [5] C. Xue, T. Zhang, Y. Zhou, M. Nixon, A. Loveless, and S. Han, “Real-time scheduling for 802.1qbv time-sensitive networking (tsn): A systematic review and experimental study,” 2024. [6] J. Falk, F. Dürr, and K. Rothermel, “Time-triggered traffic planning for data networks with conflict graphs,” in 2020 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS), 2020, pp. 124–136. [7] J. Falk, H. Geppert, F. Dürr, S. Bhowmik, and K. Rothermel, “Dynamic qos-aware traffic planning for time-triggered flows in the real-time data plane,” IEEE Transactions on Network and Service Management, vol. 19, no. 2, pp. 1807–1825, 2022. [8] A. C. T. d. Santos, B. Schneider, and V. Nigam, “Tsnsched: Automated schedule generation for time sensitive networking,” in 2019 Formal Methods in Computer Aided Design (FMCAD), 2019, pp. 69–77. [9] W. Steiner, “An evaluation of smt-based schedule synthesis for time-triggered multi-hop networks,” in 2010 31st IEEE Real-Time Systems Symposium, 2010, pp. 375–384. [10] R. Serna Oliver, S. S. Craciunas, and W. Steiner, “IEEE 802.1Qbv gate control list synthesis using array theory encoding,” in 2018 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS), 2018, pp. 13–24. [11] J. Falk, F. Dürr, and K. Rothermel, “Exploring practical limitations of joint routing and scheduling for TSN with ILP,” in 2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA), 2018, pp. 136–146. [12] M. Vlk, Z. Hanzálek, and S. Tang, “Constraint programming approaches to joint routing and scheduling in time-sensitive networks,” Computers & Industrial Engineering, vol. 157, p. 107317, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0360835221002217 [13] H. Geppert, F. Dürr, S. Bhowmik, and K. Rothermel, “Just a second - scheduling thousands of time-triggered streams in large-scale networks,” 2023. [14] C. Gärtner, A. Rizk, B. Koldehofe, R. Guillaume, R. Kundel, and R. Steinmetz, “On the incremental reconfiguration of time-sensitive networks at runtime,” in 2022 IFIP Networking Conference (IFIP Networking). IEEE, 2022, pp. 1–9. [15] M. L. Raagaard, P. Pop, M. Gutiérrez, and W. Steiner, “Runtime reconfiguration of time-sensitive networking (TSN) schedules for fog computing,” in 2017 IEEE Fog World Congress (FWC), 2017, pp. 1–6. [16] IEC SC65C/WG18 and IEEE 802.1 TSN TG, “IEC/IEEE 60802 TSN profile for industrial automation,” 2021. [Online]. Available: https://1.ieee802.org/tsn/iec-ieee-60802/ [17] “IEEE Standard for Local and Metropolitan Area Networks–Timing and Synchronization for Time-Sensitive Applications,” IEEE Std 802.1AS-2020 (Revision of IEEE Std 802.1AS-2011), pp. 1–421, 2020. [18] “IEEE Standard for Local and metropolitan area networks— Bridges and Bridged Networks - Amendment 24: Path Control and Reservation,” IEEE Std 802.1Qca-2015 (Amendment to IEEE Std 802.1Q-2014 as amended by IEEE Std 802.1Qcd-2015 and IEEE Std 802.1Q-2014/Cor 1-2015), pp. 1–120, 2016. [19] “IEEE Standard for Local and Metropolitan Area Networks–Bridges and Bridged Networks – Amendment 31: Stream Reservation Protocol (SRP) Enhancements and Performance Improvements,” IEEE Std 802.1Qcc-2018 (Amendment to IEEE Std 802.1Q-2018 as amended by IEEE Std 802.1Qcp-2018), pp. 1–208, 2018. [20] M. Newman, Networks, 2nd ed. Oxford University Press, 2018. [21] B. Waxman, “Routing of multipoint connections,” IEEE Journal on Selected Areas in Communications, vol. 6, no. 9, pp. 1617–1622, 1988. [22] F. Dürr, T. Kohler et al., “Comparing the forwarding latency of openflow hardware and software switches,” Fakultät Informatik, Elektrotechnik Informationstechnik, Univ. Stuttgart, Stuttgart, Germany, Tech. Rep. TR, vol. 4, p. 2014, 2014. [23] M. L. Raagaard and P. Pop, “Optimization algorithms for the scheduling of IEEE 802.1 time-sensitive networking (TSN),” Tech. Univ. Denmark, Lyngby, Denmark, Tech. Rep, 2017. [24] M. Pahlevan, N. Tabassam, and R. Obermaisser, “Heuristic list scheduler for time triggered traffic in time sensitive networks,” SIGBED Rev., vol. 16, no. 1, p. 15–20, feb 2019. [Online]. Available: https://doi.org/10.1145/3314206.3314208 [25] J. Y. Yen, “Finding the K Shortest Loopless Paths in a Network,” Management Science, vol. 17, no. 11, pp. 712–716, 1971. [26] M. Mohaqeqi, M. Nasri, Y. Xu, A. Cervin, and K.-E. Årzén, “Optimal harmonic period assignment: complexity results and approximation algorithms,” Real-Time Systems, vol. 54, pp. 830–860, 2018. [27] V. C. Gungor, D. Sahin, T. Koçak, S. Ergüt, C. Buccella, C. Cecati, and G. P. Hancke, “A survey on smart grid potential applications and communication requirements,” IEEE Trans. Ind. Informatics, vol. 9, no. 1, pp. 28–42, 2013. [Online]. Available: https://doi.org/10.1109/TII.2012.2218253 [28] X. Zhu, M. H. F. Wen, V. O. K. Li, and K.-C. Leung, “Optimal pmu-communication link placement for smart grid wide-area measurement systems,” IEEE Transactions on Smart Grid, vol. 10, no. 4, pp. 4446–4456, 2019. [29] D. Bian, M. Kuzlu, M. Pipattanasomporn, and S. Rahman, “Analysis of communication schemes for advanced metering infrastructure (ami),” in 2014 IEEE PES General Meeting, Conference and Exposition, 2014, pp. 1–5. [30] M. Kerai, “Smart meter statistics in great britain: Quarterly report to end september 2022,” online, 2022. [Online]. Available: https://www.gov.uk/government/statistics/smart-meters-in-great-britain-quarterly-update-september-2022 [31] G. López, J. Moreno, H. Amarís, and F. Salazar, “Paving the road toward smart grids through large-scale advanced metering infrastructures,” Electric Power Systems Research, vol. 120, pp. 194–205, 2015, smart Grids: World’s Actual Implementations. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0378779614001862 [32] Y. Yan, Y. Qian, H. Sharif, and D. Tipper, “A survey on smart grid communication infrastructures: Motivations, requirements and challenges,” IEEE Communications Surveys & Tutorials, vol. 15, no. 1, pp. 5–20, 2013."
https://arxiv.org/html/2411.01544v1,Building the Self-Improvement Loop: Error Detection and Correction in Goal-Oriented Semantic Communications,"Error detection and correction are essential for ensuring robust and reliable operation in modern communication systems, particularly in complex transmission environments. However, discussions on these topics have largely been overlooked in semantic communication (SemCom), which focuses on transmitting meaning rather than symbols, leading to significant improvements in communication efficiency. Despite these advantages, semantic errors—stemming from discrepancies between transmitted and received meanings—present a major challenge to system reliability. This paper addresses this gap by proposing a comprehensive framework for detecting and correcting semantic errors in SemCom systems. We formally define semantic error, detection, and correction mechanisms, and identify key sources of semantic errors. To address these challenges, we develop a Gaussian process (GP)-based method for latent space monitoring to detect errors, alongside a human-in-the-loop reinforcement learning (HITL-RL) approach to optimize semantic model configurations using user feedback. Experimental results validate the effectiveness of the proposed methods in mitigating semantic errors under various conditions, including adversarial attacks, input feature changes, physical channel variations, and user preference shifts. This work lays the foundation for more reliable and adaptive SemCom systems with robust semantic error management techniques.","To address the growing demand for intelligent and efficient communication in 6G networks, and to surpass the limits of traditional Shannon capacity, goal-oriented semantic communication (SemCom) has emerged as a promising paradigm [1]. Unlike conventional communication systems, which generally aim to maximize data throughput and minimize errors in bit-level transmission, SemCom prioritizes the transmission of relevant, task-specific knowledge, reducing redundant or irrelevant information [2]. This data-driven approach can significantly enhance communication efficiency, especially in scenarios where shared knowledge between transmitter and receiver allows for context-aware encoding and decoding. SemCom is viewed as a key enabler for 6G network [3, 4]. The promise of SemCom has recently attracted research interests from various perspectives. One popular research avenue involves the investigation of semantic compression strategies, which focus on identifying and interpreting only the essential information to convey meaning. The semantic compression technologies are developed for specific tasks such as text [5], image [6], and video [7], which are particularly useful in scenarios like human-robot interaction, autonomous driving, or Internet of Things (IoT), where reducing the data load while retaining critical meaning can lead to significant performance gains in terms of power consumption, bandwidth, and latency. Another trendy research area is semantic transmission, which leverages the fact that not all bits in a message are equally important for understanding the underlying meaning. Particularly, techniques such as joint source-channel coding (JSCC) are proposed to optimize the end-to-end communication process [8, 9, 10]. Despite extensive research focused on SemCom, most existing literature assumes that SemCom system is primarily unidirectional without either the error handling ability or a control loop with feedback. The semantic error, defined as the discrepancy between the transmitted and recovered information, has received less attention. Unlike physical and medium access control (MAC) layers, where techniques such as forward error correction (FEC) and automatic repeat request (ARQ) are proposed for reliable data transmission, the virtual semantic layer lacks a robust error management strategy. We believe detecting and correcting semantic errors are crucial in a SemCom system for several reasons: (1) the detection and correction processes preserve the intended meaning by ensuring that the sender’s message is accurately conveyed, maintaining contextual accuracy to prevent misinterpretation of context-sensitive information; (2) these mechanisms optimize resource utilization by focusing on transmitting meaning rather than ensuring perfect data accuracy; (3) they enhance human-machine interaction by enabling systems to better understand user intent, even in the presence of input imperfections; and (4) robust communication is ensured by providing resilience against noise and distortions that could compromise the conveyed meaning. While previous studies [11, 12, 13] have explored semantic error detection and correction, they primarily address mismatches at the terminal level without incorporating autonomous intelligence. A comprehensive framework for intelligent semantic error detection and correction applicable across the semantic channel remains absent. Motivated by the gap, in this paper, we give a general definition of semantic error and develop a robust approach for identifying and addressing semantic errors. The main contributions of this paper are summarized as follows. • Semantic error definition and classification: To our knowledge, this is the first study to explore semantic errors over a virtual semantic channel with a focus on practical communication system integration. We define semantic error, semantic error detection, and correction, and outline potential sources of these errors. • Semantic error detection via latent space deviation: We leverage the data distribution in the latent space to quantify the semantic error. A method for monitoring latent vectors using Gaussian Processes (GP) is proposed to detect errors in the SemCom framework. • HITL-RL for SemCom: A human-in-the-loop reinforcement learning (HITL-RL) approach is introduced to further enhance the reliability of SemCom by leveraging human feedback. This feedback informs the training of the RL agent, optimizing hyperparameters for SemCom models and JSCC channels. • Verification of GP for SemCom: We validate the effectiveness of the proposed GP-based error detection in a variational autoencoder (VAE)-implemented SemCom system with JSCC. The method is tested against three factors: input feature changes, physical channel variations, and adversarial attacks targeting the semantic encoder. • HITL-RL validation in multi-user scenarios: The HITL-RL method is evaluated in a multi-user JSCC broadcasting scenario. After training, the RL agent effectively configures the SemCom model and knowledge base (KB) by utilizing user feedback. The rest of this paper is organized as follows. Sec. II defines error detection and correction in SemCom and outlines the root causes of semantic error. Sec. III introduces latent space distribution monitoring and user feedback collection methods for detecting and correcting semantic errors in theory. Sec. IV describes the proposed GP-based approach for semantic error detection through latent space monitoring. Sec. V details the integration of SemCom with HITL-RL to address semantic errors. Sec. VI presents the experimental setting and corresponding results of the GP and HITL-RL used for SemCom. In-depth discussions about the presented framework are presented in Sec. VII, and Sec. VIII concludes the paper."
https://arxiv.org/html/2411.01525v1,Performance Analysis of Resource Allocation Algorithms for Vehicle Platoons over 5G eV2X Communication,"Vehicle platooning is a cooperative driving technology that can be supported by 5G enhanced Vehicle-to-Everything (eV2X) communication to improve road safety, traffic efficiency, and reduce fuel consumption. eV2X communication among the platoon vehicles involves the periodic exchange of Cooperative Awareness Messages (CAMs) containing vehicle information under strict latency and reliability requirements. These requirements can be maintained by administering the assignment of resources, in terms of time slots and frequency bands, for CAM exchanges in a platoon, with the help of a resource allocation mechanism. State-of-the-art on control and communication design for vehicle platoons either consider a simplified platoon model with a detailed communication architecture or consider a simplified communication delay model with a detailed platoon control system. Departing from existing works, we have developed a comprehensive vehicle platoon communication and control framework using 𝙾𝙼𝙽𝙴𝚃++\mathtt{OMNET++}typewriter_OMNET + +, the benchmarking network simulation tool. We have carried out an inclusive and comparative study of three different platoon Information Flow Topologies (IFTs), namely Car-to-Server, Multi-Hop, and One-Hop over 5G using the Predecessor-leader following platoon control law to arrive at the best-suited IFT for platooning. Secondly, for the best-suited 5G eV2X platooning IFT selected, we have analyzed the performance of three different resource allocation algorithms, namely Maximum of Carrier to Interference Ratio (MaxC/I), Proportional Fair (PF), and Deficit Round Robin (DRR). Exhaustive system-level simulations show that the One-Hop information flow strategy along with the MaxC/I resource allocation yields the best Quality of Service (QoS) performance, in terms of latency, reliability, Age of Information (AoI), and throughput. ††Authors Gulabi Mandal and Anik Roy are equal contributors. This work was carried out when they were MTech students at IIEST, Shibpur, and Dr. Basabdatta Palit was a faculty at IIEST Shibpur.","The unprecedented growth in the global density of automobiles in the last decade has triggered an increase in the carbon footprint while compromising on-road safety. For example, in India, while the private ownership of four-wheeler vehicles increased by 11% from 2011 to 2015, the contribution to carbon emission only from passenger vehicles also became nearly 45%, over the same period. Besides, the number of accidents also increased to as high as 449,002 in 2019 in India leading to 151,113 deaths [1]. The panacea for all such problems can be the Co-operative Automated Vehicles [2]. A potential use case of CAV is grouping vehicles into a platoon, which is projected to increase the road traffic capacity through coordinated driving while reducing the carbon footprint. A vehicle platoon is essentially a train of vehicles with a common interest, moving in unison with a small safety gap in between. The members in a vehicle platoon move in a coordinated fashion such that they can accelerate or decelerate coherently, resulting in considerable savings in fuel consumption due to the reduced aerodynamic drag. Coordinated driving requires that the string stability of the vehicle platoons be guaranteed in order to improve on-road traffic efficiency and road safety [3, 4]. This is effectuated through Cooperative Adaptive Cruise Control (CACC), a vehicular control strategy. This in turn requires a periodic, timely, and reliable exchange of Cooperative Awareness Messages (CAM) [5] which contain essential information, such as current positions of the vehicles, changes in speed and acceleration, etc. The timely delivery of CAMs between the Platoon Leader (PL) and Platoon Members depends on the underlying vehicle-to-vehicle (V2V) communication links, which are essentially lossy in nature, primarily due to multipath fading and Doppler spread[6, 7]. A popular standard for V2V communication is the IEEE 802.11p Direct Short Range Communication (DSRC) [8]. However, it fails to deliver the stringent latency requirements associated with vehicle platoons in 4G and beyond networks [9]. So, 3rdsuperscript3rd\text{3}^{\text{rd}}3 start_POSTSUPERSCRIPT rd end_POSTSUPERSCRIPT Generation Partnership Project (3GPP) prescribed the Cellular Vehicle to Everything (cV2X) communication standard in LTE, which executes the V2V communication over the existing 4thth{}^{\text{th}}start_FLOATSUPERSCRIPT th end_FLOATSUPERSCRIPT Generation (4G) cellular infrastructure [10, 11]. cV2X can support longer communication distances between heterogeneous network nodes, such as V2V, vehicle-to-infrastructure (V2I), and vehicle-to-pedestrian (V2P) [12, 13]. With the requirement for ultra-reliable low latency communication among vehicles in 5G, 3GPP has extended cV2X to the enhanced Vehicle-to-Everything (eV2X) standard in Release-16 [14]. Nonetheless, with 5G promising to operate in the higher frequency range, the signal quality fluctuations in the V2V links are expected to increase further due to their increased susceptibility to Doppler spread and multipath fading. One of the methods to counteract this high-frequency channel impairments and improve the reliability of the communication links is to design efficient radio resource allocation methods. As in cV2X, Release 16 also recommends Orthogonal Frequecy Division Multiple Access (OFDMA) for eV2X, albeit with a multi-numerology frame structure. In multi-numerology OFDMA, the bandwidth is divided into orthogonal Bandwidth Parts, and in each BWP the time-frequency Resource Blocks have different bandwidths and time duration to cater to different applications. A judicious allocation of these RBs to serve the CAMs in a Transmission Time Interval (TTI) can ensure reliable and timely delivery of these messages. 3GPP does not specify any RB allocation algorithm for any of the standards. So, it is important to design such algorithms for efficient communication in a vehicle platoon. I-A Related Works The prior art on V2V communication over vehicle platoons can be grouped into two categories. The first category focuses on the performance analysis of cV2X-based platoon communication, with studies focusing on transmission latency [11, 15, 16, 17], and communication reliability [18, 19, 20, 21]. The second category focuses on resource allocation algorithms for platoons, which can be further divided into two subgroups, (i) performance evaluation of existing resource allocation algorithms like Maximum Carrier to Interference Ratio (MaxC/I) and Deficit Round Robin (DRR) [5, 22], and (ii) the design of resource allocation strategies aimed at reducing latency [23, 24, 25, 26, 27, 28, 29, 30]. However, these existing studies [11, 15, 16, 17, 18, 19, 20] do not take into consideration different types of communication (information) flows in platoons and their impact on the latency and reliability performance. Furthermore, the resource allocation works [23, 24, 25, 26, 27, 28, 29, 30] solely focus on improving the latency performance while considering a fixed communication flow. The communication flows as highlighted in [16, 17], and [31] can substantially impact the latency and reliability performance of platoons, which in turn depends on the resource allocation algorithms under use. Clearly, a detailed performance analysis of resource allocation algorithms for platoon configuration, particularly those that achieve desirable latency profiles in 5G eV2X-based vehicular communication, remains unexplored in the existing literature. (a) (b) (c) Figure 1: Platoon IFTs for CAM: (a) Car-to-Server, (b) Multi-Hop, and (c) One-Hop. I-B Contributions In this work, we have undertaken a comprehensive study on the different resource allocation algorithms that can be used to improve the performance of vehicular platoons in 5G. Existing works on platoon management either consider a detailed control law with a simplified communication model [32, 33, 34] or consider an abstraction of a platoon with a well-developed communication protocol [35, 36, 29]. In contrast, in this work, we have used a well-defined control flow topology in addition with the complete protocol stack of 5G using a combindation of the 𝙸𝙽𝙴𝚃𝙸𝙽𝙴𝚃\mathtt{INET}typewriter_INET, 𝚅𝚎𝚒𝚗𝚜𝚅𝚎𝚒𝚗𝚜\mathtt{Veins}typewriter_Veins, 𝙿𝙻𝙴𝚇𝙴𝙿𝙻𝙴𝚇𝙴\mathtt{PLEXE}typewriter_PLEXE, and 𝚂𝚒𝚖𝚞𝟻𝙶𝚂𝚒𝚖𝚞𝟻𝙶\mathtt{Simu5G}typewriter_Simu5G libraries of the benchamrking 𝙾𝙼𝙽𝙴𝚃++\mathtt{OMNET++}typewriter_OMNET + + network simulator. We have used the default Predecessor-Leader Follower (PLF) control strategy of 𝙿𝙻𝙴𝚇𝙴𝙿𝙻𝙴𝚇𝙴\mathtt{PLEXE}typewriter_PLEXE [37], where the control law of the PM updates at regular intervals based on the CAM received from its predecessor and PL. To ensure timely reception of the CAM, we have used multi-numerology OFDMA-based V2V communication over 5G, which has been implemented using 𝚂𝚒𝚖𝚞𝟻𝙶𝚂𝚒𝚖𝚞𝟻𝙶\mathtt{Simu5G}typewriter_Simu5G [11]. The information flows from the PL and the predecessor to the PMs by using one of the three IFT - V2V communication via infrastructure (Car-to-Server), direct V2V communication (One-Hop), and relayed V2V communication (Multi-Hop). Then, we have investigated how resource allocation strategies, such as MaxC/I, Proportional Fair (PF), and DRR [38] affect the information topologies. As discussed earlier, platoons can improve road traffic capacity. Although a longer platoon can streamline road traffic better, the length can negatively impact stability. So, in this work, we have investigated how different resource allocation algorithms affect the reliability of CAM transmissions in platoons with longer lengths. The reliability is quantified in this work in terms of metrics like average End-to-End (E2E) delay, average Age of Information (AoI), average throughput, and reception probability [39]. Thus, the contributions of our work are as follows. • We have designed an end-to-end system model for a vehicle platoon framework considering different IFTs like Car-to-Server, One-Hop, and Multi-Hop, for V2V communication over 5G, by using a combination of the benchmarking tools like 𝚂𝚒𝚖𝚞𝟻𝙶𝚂𝚒𝚖𝚞𝟻𝙶\mathtt{Simu5G}typewriter_Simu5G, 𝚅𝚎𝚒𝚗𝚜𝚅𝚎𝚒𝚗𝚜\mathtt{Veins}typewriter_Veins [40], and 𝙿𝙻𝙴𝚇𝙴𝙿𝙻𝙴𝚇𝙴\mathtt{PLEXE}typewriter_PLEXE in 𝙾𝙼𝙽𝚎𝚃++\mathtt{OMNeT++}typewriter_OMNeT + +. For each of the three IFTs, we have used three different resource allocation algorithms, MaxC/I, PF, DRR. • Through an exhaustive simulation experiment over varying traffic load (platoon length), we have identified the most suitable IFT-resource allocation algorithm combination that meets the corresponding latency and reliability requirements. • We have also carried out exhaustive experiments to identify the most suitable IFT that can support multiple platoons in the same bandwidth. Our framework is a plug-and-play design where one can change the IFT, resource allocation algorithm, number of platoons, and platoon length to carry out the desired experiment in a 5G scenario. Extensive simulation results demonstrate that the combination of the One-Hop IFT and MaxC/I yields the best Quality of Service (QoS) performance in terms of latency and reliability of the platoons, both in single and multi-platoon scenario. To the best of our knowledge, our paper is the first work that studies the impact of various IFTs and resource allocation algorithms on the QoS performance of vehicle platoons in 5G networks. Rest of the paper is organized as follows. Section II provides the system overview and the details of the radio resource allocation algorithms. Section III outlines the methodology. Section IV presents a comparative performance evaluation of three IFTs and the resource allocation algorithms, followed by a discussion of the advantages and limitations of our model in Section V. Finally, Section VI concludes the paper."
https://arxiv.org/html/2411.01503v1,LumosCore: Highly Scalable LLM Clusters with Optical Interconnect,"The emergence of Large Language Model(LLM) technologies has led to a rapidly growing demand for compute resources in models. In response, the enterprises are building large-scale multi-tenant GPU clusters with 10k or even ore GPUs. In contrast to the rapidly growing cluster size, the bandwidth of clusters has also been increasing to meet communication demands, with 800 Gbps optical modules already in practical use and 1.6 Tbps modules on the horizon. However, designing clusters that simultaneously meet the requirements of large scale and high bandwidth is challenging due to the limited capacity of electrical switch chips. Unlike electrical switch chips, the single-port bandwidth of MEMS-OCS is solely determined by the optical module, making it straightforward to achieve both bandwidth and scability requirement. In this paper, we propose an opto-electronic hybrid architecture called LumosCore. We address the issues of L2 protocols incompatibility potential network contention and algorithm time complexity through physical topology and logical topology design. Additionally, we design a polynomial-time complexity link reconfiguration algorithm to reconfigure MEMS-OCS with minimal time overhead. We validate the feasibility of the proposed scheme in a cluster consisting of 128 NPUs, and through simulation based on real traces, we demonstrate the superiority of LumosCore over traditional architectures.","With the success of Large Language Model(LLM) in recent years, major companies have begun to deploy large-scale GPU clusters for multi-tenant LLM task training. A typical LLM task training may require thousands or even tens of thousands of GPUs[11, 15], and this number is expected to grow as the demand for computational power continues to increase. Fig.1 illustrates the development of AI training-related technologies over the years. Corresponding to the rapid growth in computational power requirements, the increase in network bandwidth has not been as significant. Consequently, the network is gradually becoming a potential bottleneck for LLM training. Although GPU clusters have been extensively studied in recent years, constructing large-scale GPU clusters still presents two major challenges. Figure 1: The growth rate of network capacity is much lower than the growth rate of computing capacity and model size. Challenge 1: Increasing the number of switch layers is not an effective solution for expanding cluster size. One of the most straightforward approaches to scaling up GPU clusters is to increase the number of switch layers. If switches with P𝑃Pitalic_P ports are used, and assuming no oversubscription, single data plane with D𝐷Ditalic_D layers can accommodate 2∗(P2)D2superscript𝑃2𝐷2*(\frac{P}{2})^{D}2 ∗ ( divide start_ARG italic_P end_ARG start_ARG 2 end_ARG ) start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT GPUs. However, the introduction of more switch layers will lead to hash polarization issues[15, 5], which in turn can degrade training efficiency. To mitigate the issue of hash polarization, the industry commonly constructs 2-tier or 3-tier GPU clusters. Therefore, we need to consider how to meet the growing scale requirements of GPU clusters with limited switch layers. Challenge 2: Difficult to achieve both larger single-port bandwidth and larger cluster sizes.To meet the communication requirements of LLM models, the industry has been adopting optical modules with increasing bandwidth in clusters. Optical modules with 800 Gbps have already been implemented, while 1.6 Tbps modules are under development. Under such circumstances, the capacity of switch forwarding chips has gradually become one of the bottlenecks limiting the growth of cluster sizes. Currently, 51.2 Tbps forwarding chips have been practically applied in clusters. However, such high-performance forwarding chips can only serve 32 optical modules with 1.6 Tbps.Under this condition, a 3-tier forwarding plane that includes 16*16*32 = 8192 GPUs, severely constraining the growth of cluster sizes. Moreover, the high-performance forwarding chips pose significant energy consumption and thermal management challenges[15], further increasing the construction and operational costs of the clusters. Our solution: LumosCore. In this paper, we share our new optical-electrical hybrid network architecture, LumosCore. Compared to traditional Clos[15, 14] or optical-electrical hybrid network architecture[22, 12], LumosCore makes the following contributions: • Compared to the traditional Clos architecture, LumosCore not only accommodates larger cluster sizes but also meets the demand for high port bandwidth. Additionally, it reduces reliance on high-performance forwarding chips and alleviates the thermal and power consumption pressures on the cluster. • Compared to previous work about MEMS-OCS, LumosCore addresses the challenges faced by MEMS-OCS in practical applications through architectural innovation: the protocol incompatibility issue and the complexity of scheduling algorithms. In this paper, we integrate architecture design with resource scheduling to ensure training efficiency with a polynomial time cost."
https://arxiv.org/html/2411.01078v1,Effective ML Model Versioning in Edge Networks,"Machine learning (ML) models, data and software need to be regularly updated whenever essential version updates are released and feasible for integration. This is a basic but most challenging requirement to satisfy in the edge, due to the various system constraints and the major impact that an update can have on robustness and stability. In this paper, we formulate for the first time the ML model versioning optimization problem, and propose effective solutions, including the automation with reinforcement learning (RL) based algorithm. Without loss of generality, we choose the edge network environment due to the known constraints in performance, response time, security, and reliability. The performance study shows that ML model version updates can be fully and effectively automated with reinforcement learning method as compared to other approaches. We show that with a carefully chosen range of traffic load values, the proper versioning can improve the security, reliability and ML model accuracy, while assuring a comparably lower response time.","The field of AI and ML has seen unprecedented advancements over the past decade, where AI models are continuously being improved in terms of size, performance, robustness and accuracy [1]. In this rapidly evolving domain, ML-based domain applications, such a industry and health care, need to be continuously updated. This comes in addition to related ML software updates as well as data updates. Recent ML Operations (MLOps) advancements recognize that automating and operationalizing ML products presents a new challenge [2]. To assure stability and manage applications and systems that use ML, an effective versioning of ML models, data and code is critical. Considering that most network management operations rely on ML based models, and most applications in IoT, cloud and edge computing today require ML-based functions, this challenge appears even more significant. Model robustness is one of the key challenges in ML today, which requires creation of new versions of ML models [3]. The ML model updates on the other hand add performance overhead, and in general can decrease the stability of the system. In some domain applications, like health care, updating ML model is also a matter of safety [4]. In edge networks with constrained resources, several conflicting objectives need to be considered when updating ML models, such as constraints on accuracy, reliability, security, system stability, and resource utilization [5]. Adding any new and enhanced model is contingent upon modifying the existing system, and any changes fundamentally carry risks to destabilize the performance [6]. Only a few papers so far addressed this issue, like in [7] on how to update ML model for data series prediction, focused on a a single ML model. Today, multiple ML models updates need to be considered with various conflicting constraints. In this paper, we focus on the problem of ML model versioning and updates in constrained networks. To this end, we formulate for the first time the ML versioning optimization problem and propose an RL-based decision making algorithm to automate and optimize the ML model update process. We also propose the related functional network management architecture of the ML model Update Manager in edge networks. We analyze the performance of our proposed approach by comparing it to conventional update methods used in today’s systems. The results show that under a certain network load, RL algorithm achieves a comparably faster response times, while improving model related parameters, such as accuracy, security and reliability. When the network load is higher, the automation with the RL algorithm becomes comparable to a random update process, guess process, which can be explained by the fact that under stressed system, the queue becomes constantly full and replicas remain alive during the whole runtime, not allowing the update agent for downgrading the deployed version to an older one. The rest of the paper is organized as follows. Section II describes the reference architecture. Section III formulates the ML model versioning problem. Section IV describes the RL-based solution. Section V presents numerical results. Section VI concludes the paper. Figure 1: a) Reference architecture, b) Versions representation of an ML model."
https://arxiv.org/html/2411.00792v1,Erlang Model for Multiple Data Streams,"With the development of information technology, requirements for data flow have become diverse. When multiple data streams (MDS) are used, the demands of users change over time, which makes traditional teletraffic analysis not directly applicable. This paper proposes probabilistic models for the demand of MDS services, and analyzes in three states: non-tolerance, tolerance and delay. When the requirement random variables are co-distributed with respect to time, we rigorously prove the practicability of the Erlang Multirate Loss Model (EMLM) from a mathematical perspective by discretizing time and error analysis. An algorithm of pre-allocating resources for communication society is given to guild the construction of base resources.","Communication has become an indispensable part of modern society. For a community, a large number of users will make communication requirements at the same time, and each requirement needs to allocate communication resources, such as telephone lines, time-frequency resource grids, etc. When infrastructure construction is carried out (such as base stations), if there are few preset communication resources, the user demand in the area will be frequently blocked, resulting in a poor user experience, while too many preset resources will lead to increased costs and waste. Therefore, predicting the performance of users’ requirements in the communication society and selecting reasonable resource presets are important steps in infrastructure construction[1]. In 1917, A.K. Erlang obtained his famous formula from the analysis of the statistical equilibrium and laid the foundations of modern teletraffic theory[2]. By modeling the number of arrival users as Poisson random variable and the required time being exponential distributed, Erlang formula can deduce the blocking probability for telephone communication, according to the birth and death process theory. The original Erlang model was only for telephone line, however, with the development of wireless communication, modern forms of communication continued to evolve and expand. Particularly, after the popularization of 5G, users’ requirements have become rich and diverse, including calls, text messages, voice, games, videos, short videos, etc., and we call these multiple needs as multiple data streams (MDS). Clearly, each of requirements has its own characteristics. For example, the traditional communication method of telephone, which presents a stable demand, while the requirement for games or short videos is rapidly changing. Therefore, traditional models are clearly not sufficient to describe modern communication needs. In order to be suitable with complex situations, Erlang formula has been sustainably developing. [3] considered two types of requirements, narrow-band and wide-band, and calculate the related blocking probability, in 1965. With more analysis, the number of types can be generated to any integer K𝐾Kitalic_K and the model was called as Eralng Multirate Loss Model (EMLM) [4]. Some specific policies on link, waiting or tolerance were also added in the model to meet several situations [4], [5], [6]. There are also some works on the assumptions of the arrival user and required time. [7] studied the case when the size of community is not large enough, and used quasi-random call arrival process to replace Poisson arrival process. In addition, the loss was thought as the noise when the band-demand exceeded the total band. [8] set two stages for activated users: ON and OFF. When an user is ON, he requires bksubscript𝑏𝑘b_{k}italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT band if he is in class k𝑘kitalic_k, and when he finishes the stage ON, he becomes OFF with probability σksubscript𝜎𝑘\sigma_{k}italic_σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and leaves with probability 1−σk1subscript𝜎𝑘1-\sigma_{k}1 - italic_σ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. When an user finishes his OFF stage, he ask to become ON. This assumption is called as ON-OFF model, which was enhanced for finite population community [1]. An Erlang model with varied cost in time was studied in [9]. However, different with classical teletraffic situation, MDS implies a queue problem for users with random number of services for a certain time. Existing models are still not able to describe the MDS accurately since the requirement varies with time. Therefore, this paper establishes the probability models and Erlang formulas for MDS. We denote the requirements of user i𝑖iitalic_i at time t𝑡titalic_t as Xi⁢(t)subscript𝑋𝑖𝑡X_{i}(t)italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) (can be different for different i𝑖iitalic_i and t𝑡titalic_t). In Section II, we consider that the requirement depends on the time of demand duration, and build probability models in three cases, non-tolerance, instant tolerance and delay. After assuming Xi⁢(t)subscript𝑋𝑖𝑡X_{i}(t)italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t )s are i.i.d. with respect to i𝑖iitalic_i and t𝑡titalic_t with finite support set, Erlang formula is introduced in Section III. By discrete-time analysis, we find that the stable distribution of requirement is the same whatever Xi⁢(t)subscript𝑋𝑖𝑡X_{i}(t)italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t )s are variable or invariable with t𝑡titalic_t. Therefore, we can solve the stable distribution of requirement for MDS by EMLM. A ON-OFF model is also discussed in Section III. The algorithm for pre-allocating TF resources and example are shown in Section IV, and we conclude our results in Section V. The main contributions of this paper are summarized as follows. 1. For the case that requirements Xi⁢(t)subscript𝑋𝑖𝑡X_{i}(t)italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t )s depend on the time of demand duration, we build probability models for MDS in three cases, non-tolerance, instant tolerance and delay, and obtain the blocking probability. 2. Since the requirements of MDS are time-variable, we apply the discretization for time in order to describe the distribution of total requirement. By error analysis, we find that the stable distribution of requirement is the same whatever Xi⁢(t)subscript𝑋𝑖𝑡X_{i}(t)italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t )s are variable or invariable with t𝑡titalic_t. Thus we conclude that EMLM can be still used for MDS. 3. The algorithm for pre-allocating resources is designed to guild the construction of base stations. Some notations are organized here. Xi⁢(t)subscript𝑋𝑖𝑡X_{i}(t)italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_t ) denotes the requirement of user i𝑖iitalic_i at time t𝑡titalic_t, A⁢(t)𝐴𝑡A(t)italic_A ( italic_t ) denotes the activated users at time t𝑡titalic_t and |A⁢(t)|𝐴𝑡|A(t)|| italic_A ( italic_t ) | denotes its number. We use P⁢o⁢i⁢(λ)𝑃𝑜𝑖𝜆Poi(\lambda)italic_P italic_o italic_i ( italic_λ ) and E⁢(μ)𝐸𝜇E(\mu)italic_E ( italic_μ ) to imply the random variables having Poisson distribution with rate λ𝜆\lambdaitalic_λ and exponential distribution with rate μ𝜇\muitalic_μ respectively, and a∼Asimilar-to𝑎𝐴a\sim Aitalic_a ∼ italic_A means the random variable a𝑎aitalic_a obeys the distribution A𝐴Aitalic_A. We use (⋅)ksuperscript⋅𝑘(\cdot)^{k}( ⋅ ) start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT to represent the k𝑘kitalic_k Cartesian product. N𝑁Nitalic_N always means the total number of users in a community while C𝐶Citalic_C denotes the total number of resources. In the following of this paper, we consider a community with N𝑁Nitalic_N potential users for MDS."
https://arxiv.org/html/2411.00787v1,Novel operational algorithms for ride-pooling as on-demand feeder services,"Ride-pooling (RP) service, as a form of shared mobility, enables multiple riders with similar itineraries to share the same vehicle and split the fee. This makes RP a promising on-demand feeder service for patrons with a common trip end in urban transportation. We propose the RP as Feeder (RPaF) services with tailored operational algorithms. Specifically, we have developed (i) a batch-based matching algorithm that pools a batch of requests within an optimized buffer distance to each RP vehicle; (ii) a dispatching algorithm that adaptively dispatches vehicles to pick up the matched requests for certain occupancy target; and (iii) a repositioning algorithm that relocates vehicles to unmatched requests based on their level of urgency. We also embed the Traveling-Salesman-Problem (TSP) model to generate routing plans for each dispatched vehicle. An agent-based microscopic simulation platform is designed to execute these operational algorithms (via the Operator module), generate spatially distributed random requests (Patron module), and account for traffic conditions (Vehicle module) in street networks. Extensive numerical experiments are conducted to showcase the effectiveness of RPaF services across various demand scenarios in typical morning rush hours. We compare RFaF with two on-demand feeder counterparts proposed in previous studies: Ride-Sharing as Feeder (RSaF) and Flexible-Route Feeder-Bus Transit (Flex-FBT). Comparisons reveal that given the same fleet size, RPaF generally outperforms RSaF in higher service rates (i.e., the percentage of requests served over all requests) and Flex-FBT in shorter average trip times for patrons. Lastly, we illustrate the implementation of RPaF in a real-world case study of the uptown Manhattan network (USA) using actual taxi trip data. The results demonstrate that RPaF effectively balances the level of service (service rate and patrons’ average trip time) with operational costs (fleet size). The proposed simulation platform offers a valuable testbed for evaluating and optimizing various on-demand feeder services.","The rise in ride-hailing mobility has posed significant challenges to traditional urban public transportation systems, causing a global decline in ridership since the 1990s (Graehler et al., 2019; Olayode et al., 2023). The onset of the COVID-19 pandemic has exacerbated this trend, prompting a further shift from public transportation to individual mobility (Luo et al., 2022; Shaheen and Wong, 2022). Evidence shows that transit ridership in most cities worldwide has yet to recover to pre-pandemic levels (Ziedan et al., 2023). This global phenomenon threatens the financial viability of many transit agencies, placing an increased burden on government subsidies. To reverse this downward trend, some transit agencies are exploring innovative solutions to incorporate ride-hailing technologies into On-Demand Transit (ODT) services in hopes of luring back riders (Ghimire et al., 2024). One of the major applications of these technology-enabled ODTs is to connect transportation hubs and their service zone(s) where demand is too scattered and low to support the operation of fixed bus lines (Koffman, 2004). While ODT, sometimes referred to as demand-responsive transit or paratransit, is not a novel concept (Vuchic, 2007), the integration of ride-hailing technologies – such as smartphone-based wireless communication and global position systems (GPS) – elevates the user experience through dynamic responses, location-based services, seamless payment, and transparent information. On the other hand, Transportation Network Companies (TNCs), as the platform provider of ride-hailing mobility, are introducing Ride-Pooling (RP) services as a first/last-mile solution to public transportation, either in partnership with transit agencies or independently (Shaheen and Cohen, 2019). RP services, such as Uber (Express) Pool, Lyft Line/Shuttle, Via Vanpooling, and Didi Express Pool, match multiple requests with similar itineraries to share the ride in the same vehicle. The RP not only improves transit accessibility as a faster feeder mode but also addresses the drawbacks of the conventional non-shared ride-hailing vehicles, which have been criticized for inducing individual trips, worsening traffic congestion, and increasing air pollution (Alonso-Mora et al., 2017; Furuhata et al., 2013; Chan and Shaheen, 2012). Both the technology-enabled ODTs and RP are On-Demand Shared-Ride (ODSR) mobility that offers customized door-to-door routing services. However, they differ in several operational aspects, as noted by Fan et al. (2024a): • ODTs generally adhere to schedule/frequency-based operation strategies, either fixed for reserved requests or flexible for dynamic requests (Shahin et al., 2024). Accordingly, ODTs dispatch vehicles sequentially at headways and match requests within each headway interval to successive vehicles. In such a “sequenced operation”, each ODT vehicle has to traverse the entire service zone where the matched requests are dispersed. • In contrast, RP vehicles are distributed over the network and are matched with the closest requests, with dispatches triggered by successful matches. As a result, this “distributed operation” confines each RP vehicle’s pickup process within the neighborhood area of its current location. Therefore, we see RP’s potential to lower pickup costs for operators and patrons. This motivates us to investigate: (i) whether RP could surpass ODTs in overall system performance; (ii) under what conditions this might occur; and (iii) to what extent the advantage would manifest. To address these questions, we focus on the “feeder problem” (also called first-mile and last-mile problem) framework (e.g., Clarens and Hurdle, 1975; Chang and Schonfeld, 1991b), where feeder services are operated to transport patrons with dispersed locations to/from a common hub. The feeder-problem scenario applies to various contexts, where the hub can be a subway station, a school, a shopping mall, a stadium, a convention center, etc., and the travel demand is derived from commuters, students, shoppers, and audiences.111In principle, the framework also applies to logistics transportation such as last-mile delivery problems. Several pioneering works have been done on designing ODTs in the feeder-problem framework (e.g., Chen and Nie, 2017a; Luo and Nie, 2019; Sangveraphunsiri et al., 2022). For comparison, we propose innovative operational algorithms for RP-as-feeder (RPaF) services. The main contributions of this paper are threefold: (i) Three operational algorithms are tailored for RPaF services. They are (1) a batch-based matching algorithm that functions based on the buffer distance concept, ensuring that each RPaF vehicle is assigned requests within its neighborhood; (2) a dispatching algorithm that employs hard- and soft-target models to deploy vehicles according to fixed and variable occupancy targets, respectively; and (3) a repositioning algorithm that relocates vehicles to unmatched requests based on their level of urgency, which is quantified as a weighted sum of the time elapsed since the request was made and the distance between the request and the vehicle. (ii) A microscopic simulation environment is developed to implement the proposed system as well as its ODSR counterparts. The simulation framework consists of three modules designed to capture the interactions among the operator, patrons, and vehicles. Such a module-based simulation framework serves as a testbed for evaluating the efficacy of the operational algorithms under diverse demand patterns in both stylized and real-world street networks. (iii) Systematic comparisons are conducted between the proposed RPaF and two counterparts, namely Ride-Sharing as Feeder (RSaF) (Daganzo and Ouyang, 2019a; Liu and Ouyang, 2021) and Flexible-Route Feeder-Bus Transit (Flex-FBT) (Chen and Schonfeld, 2022; Liu and Schonfeld, 2020; Kim et al., 2019; Kim and Schonfeld, 2013, 2012; Chang and Schonfeld, 1991b), which have been previously studied as on-demand feeder services. Their operational details are provided in Section 5.3. Our study elucidates the impacts of their operational differences and delineates the application domains of RPaF through extensive experiments. Furthermore, a real-world case study contrasts the three ODSR modes with the current non-shared taxis. The remainder of this paper is organized as follows. The next section reviews related studies. Then, Section 3 develops key operational algorithms for RPaF, which are embodied into the module-based simulation platform in Section 4. Section 5 presents numerical studies that examine the operational algorithms and compare alternative on-demand feeder systems. A real case study is demonstrated in Section 6 using taxi trip data in uptown Manhattan, New York, USA. Finally, Section 7 concludes this paper. Since there are several abbreviations, Table 1 in A lists them along with their meanings for the reader’s convenience."
https://arxiv.org/html/2411.02108v3,Optimizing AoI at Query in Multiuser Wireless Uplink Networks: A Whittle Index Approach,"In this paper, we explore how to schedule multiple users to optimize information freshness in a pull-based wireless network, where the status updates from users are requested by randomly arriving queries at the destination. We use the age of information at query (QAoI) to characterize the performance of information freshness. Such a decision-making problem is naturally modeled as a Markov decision process (MDP), which, however, is prohibitively high to be solved optimally by the standard method due to the curse of dimensionality. To address this issue, we employ Whittle index approach, which allows us to decouple the original MDP into multiple sub-MDPs by relaxing the scheduling constraints. However, the binary Markovian query arrival process results in a bi-dimensional state and complex state transitions within each sub-MDP, making it challenging to verify Whittle indexability using conventional methods. After a thorough analysis of the sub-MDP’s structure, we show that it is unichain and its optimal policy follows a threshold-type structure. This facilitates the verification of Whittle indexability of the sub-MDP by employing an easy-to-verify condition. Subsequently, the steady-state probability distributions of the sub-MDP under different threshold-type policies are analyzed, constituting the analytical expressions of different Whittle indices in terms of the expected average QAoI and scheduling time of the sub-MDP. Building on these, we devise an efficient algorithm to calculate Whittle indices for the formulated sub-MDPs. The simulation results validate our analyses and show the proposed Whittle index policy outperforms baseline policies and achieves near-optimal performance.","In recent years, the widespread adoption of time-critical wireless communication systems has drawn significant attention to the information freshness [1, 2, 3]. As a result, the age of information (AoI) metric, defined as the time elapsed since the generation of the most recently received message at the destination, has been proposed to quantify information freshness, see e.g., [4, 5, 6, 7, 8, 9] and references therein. Most studies focused on the AoI metric have primarily investigated the minimization of the long-term average AoI across various network settings (e.g., see [10, 11, 12, 13, 9, 14, 15, 16]). That is, these works assumed a push-based wireless communication system, where monitors at the destination continuously request the most recently received information. In contrast, the information freshness in pull-based wireless networks has obtained relatively less consideration. Specifically, at the destinations in the pull-based communication systems, a query arrival process occurs at each monitor, and the received messages are sampled and used by the monitor only at specific time instants when queries arrive, such as satellite Internet of Things (IoT) networks and ecological monitoring systems [17, 18, 19, 20]. To more effectively quantify information freshness in pull-based communication systems, the Age of Information at Query (QAoI) metric has been introduced and increasingly explored in the literature, for instance, in[21, 22, 23, 24, 25] and references therein. In particular, QAoI is defined as the instantaneous AoI values when the queries arrive at the destination. There have been some research efforts dedicated to addressing the transmission scheduling problems to optimize the long-term average QAoI in different network scenarios. The QAoI-oriented transmission scheduling coordinations for single-user networks were researched in [21, 22, 23], in which when to transmit the status update of the single user is guided by the query arrivals. Efforts on solving the QAoI-based scheduling problem in wireless two-user uplink networks have also been made in the open literature. In such a scenario, monitors with query arrivals are deployed at a base station (BS), which schedules the transmissions of status update packets from the end devices corresponding to the monitors in the uplink. In [24], the authors investigated the QAoI optimization of a reconfigurable intelligent surface-assisted wireless network, including two energy harvesting sensors and a monitor. The optimization problem was approximated as a convex problem, which was solved by an alternating optimization approach. However, this study cannot be applied to networks with more than two sensors. Zakeri et al. in [25] considered a time-slotted heterogeneous status update system with transmission and sampling frequency limitations with one source in the “stochastic arrival” model, one source in the “generate-at-will” model, and a transmitter deploying a buffer-aided and a monitor. In each time slot, the transmitter decides whether the generate-at-will source is sampled by its transmitter and schedules the transmission of the two sources to minimize the QAoI performance of the network. After modeling the decision-making problem as a constrained Markov decision process (CMDP), the authors selected to optimize the long-term discounted network-wide QAoI reward, since the CMDP is not unichain, by solving the corresponding linear programming (LP) of the CMDP. Then, a reduced-complexity heuristic policy also built upon the LP approach was developed by investigating a weakly coupled CMDP problem relaxed from the original CMDP. Although the proposed heuristic policy was shown applicable to networks including multiple stochastic arrival sources, this method suffers from high computational complexity with a large number of sources. This is because of the increase in the number of variables and constraints in the LP associated with the weakly coupled CMDP, which grows with the number of sources, leading to an exponential rise in the complexity of solving this LP. To the best of our knowledge, an efficient scheduling scheme for optimizing the long-term average QAoI in wireless multiuser uplink networks has not been thoroughly explored in the literature. To fill this gap, in this paper we consider a multiuser scheduling problem for maximizing the expected sum QAoI of a wireless uplink network, where multiple status-updating end devices are scheduled to transmit their updates to the corresponding monitors deployed at a base station (BS). Moreover, each monitor experiences stochastic query arrivals, which follow a binary Markovian process. In this context, the most recent status update packets received by the monitor are requested when queries arrive. We model the considered QAoI-oriented scheduling problem as an MDP problem. The network-wide instantaneous AoI and query arrival situations are jointly defined as the state of the formulated MDP. We summarize the main contributions of this paper as follows. • As the number of end devices increases, the state space grows exponentially, making it challenging to compute the optimal scheduling policy of the formulated MDP using standard methods due to the curse of dimensionality [26]. Hence, we adopt the Whittle index method [27] by regarding the formulated MDP as a restless multi-armed bandit (RMAB). Specifically, we relax the scheduling constraint of the formulated MDP. This allows us to decouple the original problem into multiple constrained problems, which are then formulated as independent sub-MDPs using Lagrange relaxation. • Through an in-depth analysis of the sub-MDP structure, we rigorously prove that the sub-MDP is unichain, and its optimal scheduling policy is threshold-type with two thresholds associated with the query arrival state. Building on this, we establish the Whittle indexability of the sub-MDP by verifying the active condition of indexability, a recently introduced and easy-to-verify criterion [28]. To obtain more design insights, we investigate the steady-state probability distributions of the formulated sub-MDP under the threshold-type policies in a variety of contexts. On this basis, we derive the analytical expressions for different Whittle indices in terms of the expected average QAoI and scheduling time of the sub-MDP under threshold-type policies. • We design an efficient algorithm to calculate Whittle indices for the sub-MDP, inspired by the framework in [29] for computing Whittle indices of MDPs with discounted rewards. The computational complexity of the proposed algorithm is reduced compared to the referenced approach by leveraging the threshold structure and the derived analytical expressions of Whittle indices of the sub-MDP. Moreover, closed-form expressions for the Whittle index of the sub-MDP under error-free channels are also obtained based on the simplified sub-MDP structure. Simulation results validate our theoretical analysis and demonstrate the superior, asymptotically optimal performance of the proposed Whittle index policy compared to baseline schemes. We note that Whittle index methods have been used to address AoI-related scheduling problems without considering the query arrival process (see, e.g., [30, 31, 14, 32]). In these works, the authors carefully analyzed the structures of the formulated RMABs and derived the value functions in closed form. They then established the Whittle indexability of these problems using the closed-form value functions. However, in our problem, the query arrival processes at the BS result in bi-dimensional sub-MDP states and complex state transitions. This makes deriving closed-form value functions and subsequently a closed-form Whittle index challenging, if not impossible. Thus, the methods used to establish Whittle indexability in previous works are not applicable to our problem. Notations: In this paper, ℤ+superscriptℤ\mathbb{Z}^{+}blackboard_Z start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT represents the set of positive integers, 𝔼⁢[⋅]𝔼delimited-[]⋅\mathbb{E}[\cdot]blackboard_E [ ⋅ ] denotes the operator of expectation, entity [⋅]delimited-[]⋅[\cdot][ ⋅ ] is the representation of a vector containing the same type of elements, and ⟨⋅⟩delimited-⟨⟩⋅\langle\cdot\rangle⟨ ⋅ ⟩ denotes a tuple containing different types of elements."
https://arxiv.org/html/2411.01971v1,Adaptive Optimization of TLS Overhead for Wireless Communication in Critical Infrastructure,"With critical infrastructure increasingly relying on wireless communication, using end-to-end security such as TLS becomes imperative. However, TLS introduces significant overhead for resource-constrained devices and networks prevalent in critical infrastructure. In this paper, we propose to leverage the degrees of freedom in configuring TLS to dynamically adapt algorithms, parameters, and other settings to best meet the currently occurring resource and security constraints in a wireless communication scenario. Consequently, we can make the best use of scarce resources to provide tightened security for wireless networks in critical infrastructure.","I Motivation Modern critical infrastructure is increasingly interconnected, while simultaneously being deployed over significantly larger areas [1]. Examples of this trend range from wind parks over power grids to smart cities. However, the special characteristics of such widespread environments often render wired communication infeasible and thus call for the use of cost-efficient wireless network technology such as the 450 MHz LTE-M network for critical infrastructure in Germany [2]. However, the shift to wireless communication as visualized in Figure 1 comes with severe security implications and challenges. Even though the assumption of security through physical separation of a network \raisebox{-.5pt}{\footnotesize1\raisebox{0.5pt}{\footnotesize}}⃝ has long been invalid due to their connection to the Internet \raisebox{-.5pt}{\footnotesize2\raisebox{0.5pt}{\footnotesize}}⃝, deployments are still not adequately protected [3, 4]. For wireless technology, the implications are even more profound, as security goals are particularly easy to compromise even for private infrastructure \raisebox{-.5pt}{\footnotesize3\raisebox{0.5pt}{\footnotesize}}⃝ [5, 6]. Furthermore, wireless network infrastructure is commonly provided by a cellular operator and thus shared with other entities in a dedicated network \raisebox{-.5pt}{\footnotesize4\raisebox{0.5pt}{\footnotesize}}⃝ or even entirely public and routed via the Internet \raisebox{-.5pt}{\footnotesize5\raisebox{0.5pt}{\footnotesize}}⃝. Thus, any communication potentially traverses third-party infrastructures. Figure 1: With critical infrastructure becoming more widespread and interconnected, a shift from traditional wired networks \raisebox{-.5pt}{\fontsize{7}{8}\selectfont1\raisebox{0.5pt}{\footnotesize}}⃝, \raisebox{-.5pt}{\fontsize{7}{8}\selectfont2\raisebox{0.5pt}{\footnotesize}}⃝ to wireless networks with private \raisebox{-.5pt}{\fontsize{7}{8}\selectfont3\raisebox{0.5pt}{\footnotesize}}⃝, shared \raisebox{-.5pt}{\fontsize{7}{8}\selectfont4\raisebox{0.5pt}{\footnotesize}}⃝, and public \raisebox{-.5pt}{\fontsize{7}{8}\selectfont5\raisebox{0.5pt}{\footnotesize}}⃝ infrastructure becomes necessary. The most promising approach to address resulting security concerns is end-to-end security, even if other security mechanisms are in place [7]. In fact, regulators often demand the use of TLS, the most prominent end-to-end security approach, for communication in critical infrastructure, especially when using wireless communication [2, 8]. However, besides all advantages such as flexibility and interoperability, the use of TLS can constitute significant overhead for resource-constrained devices and networks [8]. Still, and providing the main motivation for this work, this overhead is not static as it depends on concrete parameterization, opening the potential to optimize the TLS overhead for specific scenarios. Related Work. Various works study the overhead of TLS through measurements, e.g., with regard to (i) constrained LoRaWAN networks [8], (ii) energy [9], or (iii) CPU, memory, and bandwidth overhead in TLS-secured MQTT [10]. While these works only focus on a particular setting or limited set of parameters, they still indicate that a trade-off for particular resources is possible. For example, Restuccia et al. [11] identify memory overhead variations between TLS and DTLS as well as different implementations. Moreover, for post-quantum algorithms, depending on the used network technology, either bandwidth or computational time is the main limiting factor for TLS connection establishment [12]. Furthermore, proposed optimization efforts are generally focused on particular settings. For instance, Lauer et al. [13] focus on optimizing cryptographic computations on hardware-accelerated devices. Contributions. To adaptively optimize TLS for wireless communication in critical infrastructure and thus enable its widespread use even in challenging scenarios, we propose a two-step approach. First, to obtain a thorough understanding of the TLS overhead, we perform comprehensive measurements along several dimensions, covering all potentially practically relevant settings and a multitude of algorithms and parameters (Sec. II). Second, we turn these insights into use by designing and implementing an approach that can dynamically choose and adjust TLS parameters to meet resource constraints of a given scenario and adapt to changes, e.g., in available bandwidth (Sec. III). Moreover, to illustrate how this approach can be utilized and implemented in real-world deployments, we describe its application to a particular use-case scenario and demonstrate the significance of TLS bandwidth optimization in this scenario through practical measurements (Sec. IV)."
https://arxiv.org/html/2411.00859v1,Profiling AI Models: Towards Efficient Computation Offloading in Heterogeneous Edge AI Systems,"The rapid growth of end-user AI applications, such as computer vision and generative AI, has led to immense data and processing demands often exceeding user devices’ capabilities. Edge AI addresses this by offloading computation to the network edge, crucial for future services in 6G networks. However, it faces challenges such as limited resources during simultaneous offloads and the unrealistic assumption of homogeneous system architecture. To address these, we propose a research roadmap focused on profiling AI models, capturing data about model types, hyperparameters, and underlying hardware to predict resource utilisation and task completion time. Initial experiments with over 3,000 runs show promise in optimising resource allocation and enhancing Edge AI performance.","The rapid growth of end-user AI applications, such as real-time image recognition and generative AI, has led to high data and processing demands that often exceed device capabilities. Edge AI addresses these challenges by offloading computation to the network’s edge, where hardware-accelerated AI processing can occur [1]. This approach is integral to AI and RAN, a key component of future 6G networks as outlined by the AI-RAN Alliance111https://ai-ran.org/working-groups/. In 6G, AI integration across edge-RAN and extreme-edge devices will support efficient data distribution and distributed AI techniques, enhancing privacy and reducing latency for applications like the Metaverse and remote surgery. Despite these benefits, Edge AI faces challenges. Limited resource availability at the edge can hinder performance during simultaneous offloads. Additionally, the assumption of homogeneous system architecture in the existing literature is unrealistic, as edge devices vary widely in processor speeds and architectures (e.g., 1.5GHz vs 3.5GHz, or X86 vs ARM), impacting task processing and resource utilisation. To address these challenges, we propose a research roadmap focused on profiling AI models by analysing their execution dynamics across various bare-metal systems. Our goal is to understand how AI model types (e.g., MLP, CNN), hyperparameters (e.g., learning rate, optimiser), hardware (e.g., architecture, FLOPS), and dataset characteristics (e.g., size, batch size) affect model accuracy, resource use, and task completion time. This Profiling AI Models process allows us to predict resource needs and task completion times, enabling efficient scheduling across edge nodes. Our initial experiments, involving over 3,000 runs with varied configurations, showcase the effectiveness of our approach. Using AI techniques like XGBoost, we achieved a normalised RMSE of 0.001, a significant improvement over MLPs with over 4 million parameters. Figure 1: Research roadmap for profiling based computation offloading"
https://arxiv.org/html/2411.00473v1,Synergistic Interplay of Large Language Model and Digital Twin for Autonomous Optical Networks: Field Demonstrations,"The development of large language models (LLM) has revolutionized various fields and is anticipated to drive the advancement of autonomous systems. In the context of autonomous optical networks, creating a high-level cognitive agent in the control layer remains a challenge. However, LLM is primarily developed for natural language processing tasks, rendering them less effective in predicting the physical dynamics of optical communications. Moreover, optical networks demand rigorous stability, where direct deployment of strategies generated from LLM poses safety concerns. In this paper, a digital twin (DT)-enhanced LLM scheme is proposed to facilitate autonomous optical networks. By leveraging monitoring data and advanced models, the DT of optical networks can accurately characterize their physical dynamics, furnishing LLMs with dynamic-updated information for reliable decision-making. Prior to deployment, the generated strategies from LLM can be pre-verified in the DT platform, which also provides feedback to the LLM for further refinement of strategies. The synergistic interplay between DT and LLM for autonomous optical networks is demonstrated through three scenarios: performance optimization under dynamic loadings in an experimental C+L-band long-haul transmission link, protection switching for device upgrading in a field-deployed six-node mesh network, and performance recovery after fiber cuts in a field-deployed C+L-band transmission link.","The rapid development and extensive cross-applications of artificial intelligence (AI) have propelled the evolution of optical networks from a static and manual mode to an autonomous one [1]. However, previous studies have mainly relied on conventional machine learning or deep learning models with relatively small sizes, which exhibited limited intelligence and could only fulfill one or two specific tasks, but always falling short of multi-task implementations and far from full automation capabilities [2]. Moreover, human involvement is still required, especially in invoking these techniques and conducting data analysis to proceed to the next step in the workflow. Central to the realization of autonomous optical networks is to create a versatile “AI Agent” in the control plane, which can perform comprehensive management systematically and execute various tasks methodically for autonomous operation. Currently, large language models (LLMs) as generative AI techniques have sparked a revolution in all walks of life [3]. Typically, LLMs are generalist models that can effectively solve general-purpose natural language processing (NLP) tasks on a broad scale, but it still faces challenges when tackling specific or complex tasks within specific professional domains [4]. The remarkable advancement and powerful capabilities of LLMs prompt the community to expect the prospects of autonomous networks [5]. Nevertheless, integrating the LLM with optical networks presents several challenges. First, the behaviors of optical networks involve various nonlinear dynamics, which are characterized by mathematical and physical laws (e.g., nonlinear fiber optics). However, general LLMs, originally designed for NLP tasks, are incapable in deeply understanding and accurately describing the intricate behaviors of optical transmission [6]. For instance, LLMs cannot efficiently solve the nonlinear Schrödinger equation (NLSE), which governs fiber channel behavior. This mathematical limitation significantly hinders the application of LLMs in optical networks, as they cannot fully comprehend or predict network dynamics. Secondly, LLMs is expected to access streaming data from optical networks, including device configurations, node powers, signal routes, and transmission performance metrics. This data is crucial during the early stages of prompt engineering or fine-tuning for LLMs and is essential for deriving timely and accurate strategies. How to establish such efficient data transmission under standardized protocols remains a challenge. Third, considering the pivotal role of optical networks in supporting global internet and big data transmission, it is paramount to ensure rigorous stability and mitigate any risks of incorrect operation. Therefore, the derived management strategies of LLMs necessitate thorough preview for performance assessment before they are deployed on field networks. In order to elicit LLM’s proficiency for optical networks, it is imperative to formulate effective prompt strategies, establish specialized knowledge bases, and develop appropriate augmented tools. Recently, digital twin (DT) have been widely studied for optical networks to serve multiple purpose including monitoring current operational states, predicting future behavioral patterns, and estimating quality of transmission (QoT) [7]. The DT of optical network is expected to respond to “what-if” scenarios involving planning, troubleshooting, upgrading, and other proactive analytics [8], which can be used for strategy verification. Benefiting from these advantages, DTs are poised to enhance the capabilities of LLMs for autonomous optical networks in aiding decision-making processes and verifying generated strategies. In this paper, we first implement accurate and dynamic-updating DT in field-trial optical networks with hybrid data-driven and physics-informed approach. The accurately updated DT can provide essential information for decision-making processes within LLMs and serve as a platform for previewing strategies generated by LLMs. In this case study, the advanced Generative Pre-trained Transformer-4 (GPT-4) is selected as the engine of AI Agent in controller of optical networks. We integrate domain knowledge into the LLM through prompt engineering and leverage external plugins and tools to facilitate management of optical networks. The synergistic interplay between LLM and DT is demonstrated through three field-trial optical transmission systems ranging from experimental C+L-band long-haul transmission link to field-deployed mesh networks, including scenarios of dynamic loadings, protection switching, and fiber cuts. The rest of the paper is organized as follows. In Section II, we introduce the framework for the interplay between LLM and DT, where DT collects data from optical networks and provide it to the LLM for further processing. The strategies generated by the LLM are then pushed to the DT for verification and, if deemed effective and safe, can be further implemented in the optical networks. Section III discusses the establishment of DTs using a hybrid data-driven and physics-informed deep learning approach, along with results on parameter refinement from three deployed optical transmission systems. In Section IV, we delve into the LLM-empowered AI agent, detailing its domain knowledge through prompt engineering and its use of external tools via API integration. Section V demonstrates the interaction between the LLM-empowered AI agent and DTs on the three deployed systems. Finally, conclusions are drawn in Section VI."
https://arxiv.org/html/2411.00397v1,Distributed Computation Offloading for Energy Provision Minimization in WP-MEC Networks with Multiple HAPs,"This paper investigates a wireless powered mobile edge computing (WP-MEC) network with multiple hybrid access points (HAPs) in a dynamic environment, where wireless devices (WDs) harvest energy from radio frequency (RF) signals of HAPs, and then compute their computation data locally (i.e., local computing mode) or offload it to the chosen HAPs (i.e., edge computing mode). In order to pursue a green computing design, we formulate an optimization problem that minimizes the long-term energy provision of the WP-MEC network subject to the energy, computing delay and computation data demand constraints. The transmit power of HAPs, the duration of the wireless power transfer (WPT) phase, the offloading decisions of WDs, the time allocation for offloading and the CPU frequency for local computing are jointly optimized adapting to the time-varying generated computation data and wireless channels of WDs. To efficiently address the formulated non-convex mixed integer programming (MIP) problem in a distributed manner, we propose a Two-stage Multi-Agent deep reinforcement learning-based Distributed computation Offloading (TMADO) framework, which consists of a high-level agent and multiple low-level agents. The high-level agent residing in all HAPs optimizes the transmit power of HAPs and the duration of the WPT phase, while each low-level agent residing in each WD optimizes its offloading decision, time allocation for offloading and CPU frequency for local computing. Simulation results show the superiority of the proposed TMADO framework in terms of the energy provision minimization.","The fast development of Internet of Things (IoT) has driven various new applications, such as automatic navigation and autonomous driving [1]. These new applications have imposed a great demand on the computing capabilities of wireless devices (WDs) since they have computation-intensive and latency-sensitive tasks to be executed [2]. However, most WDs in IoT have low computing capabilities. Mobile edge computing (MEC) [3] has been identified as one of the promising technologies to improve the computing capabilities of WDs, through offloading computing tasks from WDs to surrounding MEC servers acted by access points (APs) or base stations (BSs). In this way, MEC servers’ computing capabilities could be shared with WDs [4]. For MEC, there are two computation offloading policies, i.e., the binary offloading policy and the partial offloading policy [5], [6]. The former is appropriate for indivisible computing tasks, where each task is either computed locally at WDs (i.e., local computing mode) or entirely offloaded to the MEC server for computing (i.e., edge computing mode). The latter is appropriate for arbitrarily divisible computing tasks, where each task is divided into two parts. One part is computed locally at WDs, and the other part is offloaded to the MEC server for computing. Works [7] and [8] focused on the partial offloading policy in a multi-user multi-server MEC environment, formulated a non-cooperative game, and proved the existence of the Nash equilibrium. Energy supply is a key factor impacting the computing performance, such as the computing delay and computing rate, and the offloading decisions, such as computing mode selections under the binary offloading policy and the offloading volume under the partial offloading policy. However, most WDs in IoT are powered by batteries with finite capacity. Frequent battery replacement is extremely cost or even impractical in hard-to-reach locations, which limits the lifetime of WDs. To break this limitation, wireless power transfer (WPT) technology [9, 10], which realizes wireless charging of WDs by using hybrid access points (HAPs) or energy access points (EAPs) to broadcast radio frequency (RF) signals, is widely believed as a viable solution due to its advantages of stability and controllability in energy supply [11, 12]. As such, wireless powered mobile edge computing (WP-MEC) has recently aroused increasing attention [13] since it combines the advantages of MEC and WPT technologies, i.e., enhancing computing capabilities of WDs while providing sustainable energy supply. TABLE I: Comparison between this paper and the related works. Ref. Goal Energy supply Multiple servers Binary offloading Long-term optimization Computation data demand of network Distributed computation offloading [14] computation rate √square-root\surd√ √square-root\surd√ √square-root\surd√ [15] energy efficiency √square-root\surd√ √square-root\surd√ [16] energy provision √square-root\surd√ √square-root\surd√ √square-root\surd√ [18] energy provision √square-root\surd√ √square-root\surd√ [19] energy provision √square-root\surd√ [20] energy efficiency √square-root\surd√ [21] energy efficiency √square-root\surd√ √square-root\surd√ [22] computation rate √square-root\surd√ √square-root\surd√ √square-root\surd√ [23] computation delay √square-root\surd√ √square-root\surd√ √square-root\surd√ √square-root\surd√ √square-root\surd√ [24] energy provision √square-root\surd√ Ours energy provision √square-root\surd√ √square-root\surd√ √square-root\surd√ √square-root\surd√ √square-root\surd√ √square-root\surd√ √square-root\surd√ denotes the existence of the feature. Due to the coupling of the energy supply and the communication/computation demands of WDs, the critical issue in WP-MEC networks is how to reasonably allocate energy resource and make offloading decisions, so as to optimize various network performance. Consequently, many works [14, 15, 16, 17, 18, 19, 20, 21] have been done to address the issue. Under the binary offloading policy, Bi et al. [14] maximized the weighted sum computation rate of WDs in a multi-user WP-MEC network by jointly optimizing WDs’ computing mode selections and the time allocation between WPT and data offloading. Under the max-min fairness criterion, Zhou et al. [15] maximized the computation efficiency of a multi-user WP-MEC network by jointly optimizing the duration of the WPT phase, the CPU frequency, the time allocation and offloading power of WDs. While Chen et al. [16] considered a WP-MEC network where a multiple-antenna BS serves multiple WDs, and proposed an augmented two-stage deep Q-network (DQN) algorithm to minimize the average energy requirement of the network. Under the partial offloading policy, Park et al. [17] investigated a WP-MEC with the simultaneous wireless information and power transmission (SWIPT) technique, and minimized the computation delay by jointly optimizing the data offloading ratio, the data offloading power, the CPU frequency of the WD and the power splitting ratio. In a single-user WP-MEC network consisting of a multi-antenna EAP, a MEC server and a WD, Wang et al. [18] minimized the transmission energy consumption of the EAP during the WPT phase by jointly optimizing the energy allocation during the WPT phase at the EAP and the data allocation for offloading at the WD. In a two-user WP-MEC network with the nonorthogonal multiple access (NOMA) protocol, Zeng et al. [19] also minimized the transmission energy consumption of the EAP, similar to [18], under the energy and computing delay constraints, and proposed an iterative algorithm to solve it. Different from the above works, works [20] and [21] focused on the backscatter-assisted WP-MEC network, where WDs harvest energy for local computing and data offloading through active transmission and backscatter communication. Considering the limited computation capacity of the MEC server, Ye et al. [20] respectively maximized the computation energy efficiency and the total computation bits by proposing two resource allocation schemes. By leveraging the NOMA protocol to enhance backscatter communication, Du et al. [21] maximized the computation energy efficiency. The aforementioned works [14, 15, 16, 17, 18, 19, 20, 21], however, only focused on the WP-MEC networks with a single HAP, which makes it difficult to efficiently process the massive amount of the computation data offloaded from a large number of WDs. Recently, few works [22], [23] have studied the WP-MEC networks with multiple HAPs, which are practical for large-scale IoT. Specifically, with the goal of computation rate maximization for a single time slot, Zhang et al. [22] first obtained the near-optimal offloading decisions by proposing an online deep reinforcement learning (DRL)-based algorithm, and then optimized the time allocation by designing a Lagrangian duality-based algorithm. While Wang et al. [23] focused on the long-term average task completion delay minimization problem, and proposed an online learning algorithm implemented distributively for HAPs to learn both the duration of the WPT phase and offloading decisions at each time slot. Actually, besides the computation rate [14], [22], computation efficiency [15], [20], and computation delay [17], [23], the energy provision is also a very important metric for evaluating the design of the WP-MEC networks [24]. However, as far as we know, the energy provision minimization of the WP-MEC networks with multiple HAPs has seldom been studied. Although works [16] and [18] have studied it in the WP-MEC networks with one HAP, the design in [16] and [18] can not be applied in the WP-MEC networks with multiple HAPs due to the complex association between WDs and HAPs. To fill this gap, we study the long-term energy provision minimization problem of a multi-HAP WP-MEC network in a dynamic environment, where WDs harvest energy from the RF signals of HAPs, and then compute or offload the computation data under the binary offloading policy. Besides the energy constraint, the computing delay and computation data demand constraints should be satisfied in order to ensure the computing performance of the network. The computing delay constraint ensures that the generated computation data at each time slot is computed either locally or remotely at HAPs within the acceptable duration. The computation data demand constraint ensures that the total amount of the processed computation data at each time slot is no smaller than the required computation data demand, which is in accordance with the real demand. Since the amount of computation data generated by WDs and the channel gains between HAPs and WDs are uncertain in a dynamic network environment, optimization methods such as convex optimization and approximation method are difficult to well address the problem. Fortunately, DRL has been demonstrated in the literature as a more flexible and robust approach to adapt the MEC offloading decisions and the resource allocation by interacting with the dynamic network environment [25], [26]. Hence we exploit the DRL approach to address the problem. A straightforward implementation of the DRL approach is employing a centralized agent at HAPs to collect all network information and then adapt the actions for HAPs and WDs. However, with the increasing number of HAPs/WDs, the state space and action space increase explosively, resulting in long training time and poor performance. To address this dilemma, adopting a distributed computation offloading framework is a promising solution. We summarize the differences between this paper and the related works in TABLE I, so as to highlight the novelty of this paper. The main contributions are summarized as follows. • In order to pursue a green computing design for a multi-HAP WP-MEC network, we formulate an energy provision minimization problem by jointly optimizing the transmit power of HAPs, the duration of the WPT phase, the offloading decisions of WDs, the time allocation for offloading and the CPU frequency for local computing, subject to the energy, computing delay and computation data demand constraints. The formulated non-convex mixed integer programming (MIP) problem is very challenging to be tackled by proving it for a single time slot is NP-hard. • To efficiently tackle the non-convex MIP problem in a distributed manner, we decompose it into three subproblems, and then propose a two-stage multi-agent DRL-based distributed computation offloading (TMADO) framework to solve them. The main idea is that the high-level agent residing in all HAPs is responsible for solving the first subproblem, i.e., optimizing the transmit power of HAPs and the duration of the WPT phase, while the low-level agents residing in WDs are responsible for solving the second and third subproblems, i.e., the offloading decisions of WDs, the time allocation for offloading and the CPU frequency for local computing. In a distributed way, each WD optimizes its offloading decision, time allocation for offloading, and CPU frequency. • Simulation results validate the superiority of the proposed TMADO framework in terms of the energy provision minimization compared with comparison schemes. It is observed that when the number of HAPs/WDs reaches a certain value, the scheme with only edge computing mode is better than that with only local computing mode in terms of the energy provision minimization due to the reduced average distance between HAPs and WDs. It is also observed that, with the purpose of minimizing energy provision of HAPs, the WDs with high channel gains are prone to select local computing mode, and vice versa. The rest of this paper is organized as follows. In Section II, we introduce the system model of the WP-MEC network with multiple HAPs. In Section III, we formulate the energy provision minimization problem. In Section IV, we present the proposed TMADO framework. In Section V, we present the simulation results. Section VI concludes this paper."
https://arxiv.org/html/2411.00319v1,Task-oriented Age of Information for Remote Monitoring Systems,"The emergence of intelligent applications has fostered the development of a task-oriented communication paradigm, where a comprehensive, universal, and practical metric is crucial for unleashing the potential of this paradigm. To this end, we introduce an innovative metric, the Task-oriented Age of Information (TAoI), to measure whether the content of information is relevant to the system task, thereby assisting the system in efficiently completing designated tasks. Also, we study the TAoI in a remote monitoring system, whose task is to identify target images and transmit them for subsequent analysis. We formulate the dynamic transmission problem as a Semi-Markov Decision Process (SMDP) and transform it into an equivalent Markov Decision Process (MDP) to minimize TAoI and find the optimal transmission policy. Furthermore, we demonstrate that the optimal strategy is a threshold-based policy regarding TAoI and propose a relative value iteration algorithm based on the threshold structure to obtain the optimal transmission policy. Finally, simulation results show the superior performance of the optimal transmission policy compared to the baseline policies.","Generally, conventional communications rely on Shannon’s channel coding theory to achieve reliable transmission from sources to destinations [1]. The core idea is to abstract information into bits and design source coding/decoding, channel coding/decoding, and modulation/demodulation parts to minimize bit/symbol error rate or signal distortion measures (e.g., Mean Square Error) and achieve error-free replication of bits from source to destination [2]. This approach has demonstrated tremendous success across a range of voice and data communication systems [3]. However, with the emergence of intelligent systems such as real-time cyber-physical systems, interactive systems, and autonomous multi-agent systems, the communication goals of these systems are no longer to reconstruct the underlying message but to enable the destination to make the right inference or to take the right decision at the right time and within the right context [4]. This shift poses new technical challenges to conventional communication systems [5]. Therefore, a new communication paradigm, task-oriented communication or goal-oriented communication, has been proposed as a promising solution [6]. Figure 1: An illustration of the task-oriented monitoring system. The key to unlocking the potential of task-oriented communication lies in a comprehensive, universal, and practical metric. This metric measures the relevance of information to system task, thereby significantly reducing computational and transmission costs by only acquiring, transmitting, and reconstructing task-relevant information. In recent years, there have been several studies on metrics for task-oriented communication [7, 8, 9, 10, 11]. In [7], the Age of Information (AoI) was proposed as a pioneering metric to capture the freshness of data perceived by the destination. However, AoI is limited in that it cannot measure the content of information or the dynamic changes in information content. To address this issue, the authors of [8] proposed the Age of Incorrect Information (AoII), which reflects the discrepancy between the receiver’s estimate and the actual system state. In [9], the authors introduced the Age of Changed Information (AoCI), positing that changes in information content to be more beneficial to the system. In [10], the Age of Version (AoV) was introduced, counting the integer difference between the versions at the database and the local cache. The authors of [11] proposed the Age of Actuation (AoA), which captures the elapsed time since the last performed actuation at a destination based on data received by a source. However, none of these metrics directly assess whether the information content is relevant to the system task. In this paper, we consider a remote monitoring system consisting of a sensor, a processor, a transmitter, and a receiver. Specifically, the sensor captures real-time images, which are pre-identified by the processor, and then the receiver decides whether to require the transmitter to transmit the images based on the pre-identification results and monitoring target. A new task-oriented communication metric called Task-oriented Age of Information (TAoI) is introduced, which directly characterizes whether the information content is relevant to the system task. If the transmitted image matches the target, TAoI decreases; otherwise, it increases. We focus on finding the optimal transmission policy for the remote monitoring system to minimize TAoI. By modeling the problem as an infinite time-horizon Semi-Markov Decision Process (SMDP) and transforming it into an equivalent MDP with uniform time steps, we prove that the optimal transmission policy is a threshold-type policy. Furthermore, we propose a low-complexity relative value iteration algorithm based on the threshold structure to obtain the optimal transmission policy. Finally, simulation results demonstrate that the optimal transmission policy outperforms the two baseline policies."
https://arxiv.org/html/2411.00124v1,"Globalping: A Community-Driven, Open-Source Platform for Scalable, Real-Time Network Measurements","We present Globalping, an open-source, community-driven platform for scalable, real-time global network measurements. It democratizes access to network diagnostics by offering every user, including non-technicals, technicals, and companies, the ability to perform ping, traceroute, and DNS lookups from a globally distributed network of user-hosted probes using either the intuitive Globalping front-end or REST API. Unlike solutions like RIPE Atlas, official integrations with other platforms, such as Slack and GitHub, make Globalping even more effective in real-time monitoring and collaboration.","With the expansion of global internet infrastructure in recent years, real-time network diagnostics and monitoring have become extremely important for both technical and non-technical users [8]. Most of the available diagnostic tools in the traditional sense serve a select few professionals rather than more general groups. This paper introduces Globalping, a community-driven, open-source platform that provides democratic access to scalable, real-time network measurements. Unlike similar tools, Globalping empowers all users, including experts and non-technicals, to carry out networking diagnostics such as ping, traceroute, and DNS lookups from a globally distributed network of user-hosted probes. Globalping seeks to bridge technical complexity with ease of use through an intuitive front end and REST API. This paper presents the novel benefits of Globalping compared to existing tools like RIPE Atlas. It illustrates flexible, community-powered architecture, which makes it a valuable resource for real-time performance monitoring of the Internet and network troubleshooting. This paper makes the following contribution to the literature: • Presenting the only community-driven network diagnostic tool for every user base. • A comparative study on Globalping’s advantages over similar tools."
https://arxiv.org/html/2411.00453v2,Diffusion Models as Network Optimizers: Explorations and Analysis,"Network optimization is a fundamental challenge in the Internet of Things (IoT) network, often characterized by complex features that make it difficult to solve these problems. Recently, generative diffusion models (GDMs) have emerged as a promising new approach to network optimization, with the potential to directly address these optimization problems. However, the application of GDMs in this field is still in its early stages, and there is a noticeable lack of theoretical research and empirical findings. In this study, we first explore the intrinsic characteristics of generative models. Next, we provide a concise theoretical proof and intuitive demonstration of the advantages of generative models over discriminative models in network optimization. Based on this exploration, we implement GDMs as optimizers aimed at learning high-quality solution distributions for given inputs, sampling from these distributions during inference to approximate or achieve optimal solutions. Specifically, we utilize denoising diffusion probabilistic models (DDPMs) and employ a classifier-free guidance mechanism to manage conditional guidance based on input parameters. We conduct extensive experiments across three challenging network optimization problems. By investigating various model configurations and the principles of GDMs as optimizers, we demonstrate the ability to overcome prediction errors and validate the convergence of generated solutions to optimal solutions. We provide code and data at https://github.com/qiyu3816/DiffSG.","Network optimization problems arise from the widespread need to maximize or minimize specific objectives while utilizing limited network resources [1]. In the Internet of Things (IoT) network, these problems vary in form depending on the scenario, such as the Internet of Things (IoTs), aiming for optimal resource allocation with highly dense deployed IoT devices [2, 3]. Common resources include communication capacity, storage, computational power, and energy. The optimization objectives typically involve maximizing transmission rates [4, 5], minimizing total delay [6] and energy consumption [7, 8], reducing execution costs [9, 10, 11], or enhancing quality of service (QoS) [12, 13]. The complexity of real-world environments and application requirements introduces challenging mathematical properties to these problems, such as convexity and non-convexity, linearity and non-linearity, and solution spaces that may be discrete, continuous, or mixed-integer. Additionally, these problems can involve both differentiable and non-differentiable components. These properties impact not only the objective functions but also the constraints, resulting in a complex feasible solution space that makes solving such problems particularly challenging. Existing network optimization methods primarily include numerical approaches based on optimization theory [5, 8, 10, 12, 13, 14] and fitting algorithms based on machine learning [9, 11, 15, 16, 17, 18, 4, 19, 20, 21, 22], with some work exploring the use of deep learning to enhance numerical methods [23, 24, 25]. For problems without complex characteristics, it is often straightforward to apply classical algorithms. However, when dealing with more typical challenges—such as non-convex, mixed-integer, multi-objective, or Pareto optimization—custom algorithm design becomes necessary [5, 8, 10, 12, 13, 14]. This process demands a strong understanding of optimization theory and can be labor-intensive in developing effective solutions. In the case of fitting algorithms, standard supervised learning [9, 11, 15, 16, 17] has limited capability to directly learn the mapping from input to optimal solutions, particularly when dealing with non-differentiable objective functions or constraints. Under such conditions, full convergence is nearly impossible, making it difficult to meet complex optimization demands. As a result, reinforcement learning (RL) [4, 19, 20, 21, 22], which interacts with the solution space and environment to make decisions, has become more commonly used. However, RL performance is heavily dependent on extensive interaction and exploration, which makes it challenging to train in high-dimensional, complex solution spaces. Additionally, RL often suffers from issues such as reliance on reward function design and a tendency to get trapped in local optima. In numerical methods enhanced by deep learning [23, 24], certain components of traditional algorithms that are difficult to construct manually are replaced by learning-based approaches. While this can improve performance on some problems, it also demands more specialized designs compared to conventional numerical methods. In summary, while conventional network optimization methods have achieved notable success, they often come with the drawbacks of heavy exclusive designs and inherent limitations in learning capacity. Figure 1: The learning results of generative models differ from those of discriminative models. Discriminative models focus on mapping x𝑥xitalic_x directly to the optimal solution, but they often incur learning errors that are difficult to eliminate. In contrast, generative models learn the distribution of high-quality solutions for a given input x𝑥xitalic_x, with the optimal solution typically lying within this distribution. To address this issue, this paper proposes a novel approach focused on learning high-quality solution distributions for network optimization, which has the potential to overcome the above shortcomings. Accordingly, we introduce generative models for network optimization, especially diffusion models [26, 27], which have demonstrated strong performance in recent years. I-A Our Proposal We first answer the intuitive question: “Why use generative models for network optimization?” Recall that the key difference between generative and discriminative models is that generative models learn the joint probability distribution P⁢(x,y)𝑃𝑥𝑦P(x,y)italic_P ( italic_x , italic_y ), while discriminative models learn the conditional probability distribution P⁢(y|x)𝑃conditional𝑦𝑥P(y|x)italic_P ( italic_y | italic_x ). The core purpose of generative models is to capture a probability distribution and sample from it, whereas discriminative models focus on tasks like classification and regression. Additionally, generative diffusion models can leverage P⁢(y|x)𝑃conditional𝑦𝑥P(y|x)italic_P ( italic_y | italic_x ) to guide the sampling of specific types within the learned distribution [28, 29]. Current fitting algorithms in network optimization are predominantly discriminative models, which aim to map single-point or sequential inputs to optimal solutions. However, existing neural networks struggle to fit complex objective functions directly, and this inverse fitting process is even more constrained. As a result, there is often a discrepancy between the output solution and the true optimal value, as illustrated in Fig. 1. Due to the complexity of the target high-quality solution distribution in most cases, this error is difficult to mitigate using random strategies in RL, and nearly impossible to eliminate in standard supervised learning. However, we find that generative models possess a global awareness of the entire solution space for a given input, along with the ability to repeatedly sample solutions by learning the joint probability distribution from input to output, as illustrated in Fig. 1. Specifically, inspired by [30], we define the conditional generation target of the generative diffusion model as a probability distribution over solutions, pθ⁢(y|x)subscript𝑝𝜃conditional𝑦𝑥p_{\theta}(y|x)italic_p start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_y | italic_x ), where x𝑥xitalic_x is the given input and better solutions have higher probabilities. In this way, the model captures the feasible solution space corresponding to the input x𝑥xitalic_x and probabilistically describes the quality of different solutions within it. Through repeated sampling, the model can approximate or even reach the global optimal solution, effectively closing the gap between traditional discriminative methods and the true optimal value. Notably, this approach to learning the solution distribution is insensitive to the complex characteristics of the original objective and constraints. I-B Contributions This work focuses on applying generative diffusion models (GDMs) to network optimization problems, providing both theoretical and experimental analysis to demonstrate the effectiveness of this emerging approach. Our key contributions are as follows: 1. We analyze the current state of network optimization methods and GDMs for optimization problems, answering the key question: “why use generative models for network optimization?” We show that GDMs can understand the global solution space by learning the distribution of high-quality solutions to generate optimal results and provide solid theoretical support for this approach. 2. We demonstrate the effectiveness of GDMs as network optimizers across several typical network optimization problems, offering a visual illustration of the generation process to enhance understanding of how GDMs converge. 3. Given the limited application of GDMs as network optimizers directly, we conduct extensive exploratory experiments on the configuration of diffusion models. Our results explore applying GDMs to a broader range of optimization problems."
https://arxiv.org/html/2411.00316v1,Quantum Entanglement Path Selection and Qubit Allocation via Adversarial Group Neural Bandits††thanks:* The first two authors contributed equally to this work.,"Quantum Data Networks (QDNs) have emerged as a promising framework in the field of information processing and transmission, harnessing the principles of quantum mechanics. QDNs utilize a quantum teleportation technique through long-distance entanglement connections, encoding data information in quantum bits (qubits). Despite being a cornerstone in various quantum applications, quantum entanglement encounters challenges in establishing connections over extended distances due to probabilistic processes influenced by factors like optical fiber losses. The creation of long-distance entanglement connections between quantum computers involves multiple entanglement links and entanglement swapping techniques through successive quantum nodes, including quantum computers and quantum repeaters, necessitating optimal path selection and qubit allocation. Current research predominantly assumes known success rates of entanglement links between neighboring quantum nodes and overlooks potential network attackers. This paper addresses the online challenge of optimal path selection and qubit allocation, aiming to learn the best strategy for achieving the highest success rate of entanglement connections between two chosen quantum computers without prior knowledge of the success rate and in the presence of a QDN attacker. The proposed approach is based on multi-armed bandits, specifically adversarial group neural bandits, which treat each path as a group and view qubit allocation as arm selection. Our contributions encompass formulating an online adversarial optimization problem, introducing the EXPNeuralUCB bandits algorithm with theoretical performance guarantees, and conducting comprehensive simulations to showcase its superiority over established advanced algorithms.","In recent times, Quantum Data Networks (QDNs) have emerged as a transformative approach, with the potential to reshape information processing and transmission through the evolution of distributed quantum computing [1]. Classical networks have inherent limitations when it comes to ensuring data security during transmission and handling data-intensive processing tasks. QDNs, built upon the principles of quantum mechanics, have the potential to overcome these limitations by leveraging the unique properties of quantum systems, paving the way for unparalleled levels of security, communication efficiency, and computational power [2]. Previous quantum key distribution technique, that aims to generate quantum bits (qubits) and use them to deliver cryptographic keys, can regenerate and retransmit qubits if available [3], while QDNs encode the data information in the data qubits and utilize quantum teleportation technique to avoid the retransmission issue due to no-cloning theorem [4]. Quantum entanglement stands as a cornerstone in various quantum techniques and applications. Consider two physically adjacent quantum computers, Alice and Bob, where entangled qubit pairs are generated on one end, say Alice. One of these entangled qubits is retained while the other is sent to Bob, via a physical fiber-optic channel, establishing an entanglement link. However, this procedure encounters challenges due to the inherent loss in the optical fiber, resulting in a success rate well below one [5]. Creating an entanglement link is probabilistic and uncertain, relying on the presence of quantum channels between the parties and qubits at both ends [6]. Increasing the number of qubits on both ends and utilizing additional available channels can improve the success rate of the simultaneous attempts made. Nevertheless, quantum channels are limited, and each quantum node has constrained quantum memory for storing qubits, further complicating the process. Figure 1: Quantum Data Network. There exist four possible paths between Alice and Bob and one attacker aims to disrupt one of them. Note that different possible paths can have different success rates of establishing entanglement connections As depicted in Figure 1, within QDNs, direct channel connections among quantum computers are often absent. Instead, they are linked through various quantum nodes, known as quantum repeaters. Notably, entanglement links can only be forged between neighboring quantum nodes. To establish a long-range entanglement connection between Alice and Bob, a path must be discovered between them, establishing entanglement links for each successive quantum node along this path and entanglement swapping operations are employed repeatedly at each quantum node along the path [7]. The success rate of establishing such a long-distance entanglement connection is related both to the selected path, such as the length and number of hops of the path and the success rate of each entanglement link on the path and to the allocation of qubits on each quantum node because of the limited quantum memory for storing qubits. This means that more qubits allocated by a quantum node to establish entanglement connections with a predecessor node results in fewer qubits for the entanglement links with the successor. Entanglement routing is a complex problem that involves not only establishing quantum links but also ensuring the reliable transmission of entanglement across a network. Qubit allocation and path selection has been considered as a pivotal part of entanglement routing in many previous studies such as [8, 9, 10] since establishing an entanglement link is probabilistic and unstable, relying on the availability of quantum channels between the parties and qubits on both ends, which directly influences the success of entanglement routing. However, these studies operate under the assumption of pre-known probabilities for creating entanglement links [8, 9, 11, 12, 13, 14, 15, 10, 16]. Meanwhile, they tend to overlook the critical aspect of addressing potential network attackers – a well-explored area in conventional network security [17, 18, 19]. Such an attack can be performed on either the data qubit itself or the classical channel that delivers the Bell State Measurement result for quantum teleportation [20]. Note that both categories of attackers can occur in any given time slot and be distinguished from regular entanglement connection failures, by no-cloning theorem or transmission errors in classical channels. In this paper, we focus on addressing the challenge of online optimal path selection and qubit allocation between two quantum nodes in the presence of a potential QDN attacker. We introduce a novel multi-armed bandits approach grounded, called adversarial group neural bandits. By developing an online adversarial optimization approach using multi-armed bandits, we provide a robust solution that optimizes both path selection and qubit allocation in real-time, without prior knowledge of success rates and under the presence of potential attackers. This enhances the overall efficiency and reliability of quantum entanglement routing, especially in dynamic and adversarial environments. Therefore, our approach contributes to advancing the broader field of entanglement routing by addressing a key challenge that has been largely overlooked in prior studies. Our key contributions encompass the following: • We present an online optimization problem that concurrently addresses path selection and qubit allocation between two quantum nodes. This scenario accounts for an attacker disrupting data qubit transmission and involves no prior knowledge of the success rate for creating each entanglement link. The goal is to maximize the long-term success rate for establishing long-distance entanglement connections between the chosen quantum nodes. • To tackle the intricate nonlinear optimization objective, we frame the problem as an adversarial group neural bandits problem and propose a bandits algorithm, called EXPNeuralUCB, which treats possible paths as groups and performs qubit allocation as arm selection in each group. Moreover, by choosing suitable algorithm parameters, we theoretically prove that the algorithm has a regret upper bound of O⁢(T3/4⁢log⁡T)𝑂superscript𝑇34𝑇O(T^{3/4}\sqrt{\log T})italic_O ( italic_T start_POSTSUPERSCRIPT 3 / 4 end_POSTSUPERSCRIPT square-root start_ARG roman_log italic_T end_ARG ), offering performance guarantees for our algorithm. • We conduct comprehensive simulations to showcase the superiority of our proposed EXPNeuralUCB over other established advanced bandit algorithms."
