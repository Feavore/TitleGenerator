URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10220v1,Escape-from-a-layer approach for simulating the boundary local time in Euclidean domains,"We propose an efficient numerical approach to simulate the boundary local time, as well as the time and position of the associated reaction event on a smooth boundary of a Euclidean domain. This approach combines the standard walk-on-spheres algorithm in the bulk with the approximate solution of the escape problem in the boundary layer. In this way, the most time-consuming simulation of reflected Brownian motion near the boundary is replaced by an equivalent escape event. We validate the proposed escape-from-a-layer approach by comparing simulated statistics of the boundary local time with exact results known for simple domains (a disk, a circular annulus, a sphere, a spherical shell) and with the numerical results obtained by a finite-element method in more sophisticated domains. This approach offers a powerful tool for simulating diffusive processes in confinements and for solving the related partial differential equations. Its applications in the context of diffusion-controlled reactions in chemical physics are discussed.","The boundary local time \ell_{t} plays the central role in the theory of stochastic processes [1, 2, 3]. For instance, reflected Brownian motion \bm{X}_{t} inside a given Euclidean domain \Omega\subset\mathbb{R}^{d} with a smooth boundary \partial\Omega can be constructed as the solution of the Skorokhod stochastic equation [2, 3] d\bm{X}_{t}=\sqrt{2D}\,d\bm{W}_{t}+\bm{n}(\bm{X}_{t})\,d\ell_{t},\qquad\bm{X}_% {0}=\bm{x}_{0},\qquad\ell_{0}=0, (1) where \bm{W}_{t} is the standard Wiener process in \mathbb{R}^{d}, D is the diffusion coefficient, \bm{x}_{0}\in\Omega is the starting point, \bm{n}(\bm{x}) is the unit normal vector at a boundary point \bm{x}\in\partial\Omega oriented inward the domain \Omega, and \ell_{t} is a nondecreasing stochastic process that increments at each encounter of \bm{X}_{t} with the boundary. Qualitatively, the first term in Eq. (1) describes ordinary Brownian motion inside \Omega, whereas the second term ensures that \bm{X}_{t} is reflected back normally into \Omega at each encounter with the boundary. From the physical point of view, one can think of the second term as an infinitely local force field that pushes the diffusing particle back to the confining domain [4]. Curiously, the single stochastic equation (1) determines simultaneously two tightly related stochastic processes: the position \bm{X}_{t} and the boundary local time \ell_{t} (which has, despite its name, units of length). The latter should not be confused with a point local time, which represents the residence time of Brownian motion in a vicinity of a bulk point (see [5, 6, 7]). Note that the boundary local time can be expressed as \ell_{t}=\lim\limits_{\varepsilon\to 0}\frac{D}{\varepsilon}\int\limits_{0}^{t% }dt^{\prime}\,\Theta(\varepsilon-|\bm{X}_{t^{\prime}}-\partial\Omega|), (2) where |\bm{x}-\partial\Omega| is the Euclidean distance between a point \bm{x} and the boundary \partial\Omega, and \Theta(z) is the Heaviside step function: \Theta(z)=1 for z>0 and 0 otherwise. The integral represents the residence time of the process \bm{X}_{t} in a thin layer \partial\Omega_{\varepsilon}=\{\bm{x}\in\Omega:\left|\bm{x}-\partial\Omega% \right|<\varepsilon\} of width \varepsilon near the boundary \partial\Omega, which is rescaled by \varepsilon to get a nontrivial limit, as \varepsilon\to 0. Alternatively, one has \ell_{t}=\lim\limits_{\varepsilon\to 0}\varepsilon\,\mathcal{N}_{t}^{(% \varepsilon)}, (3) where \mathcal{N}_{t}^{(\varepsilon)} is the number of crossings of the layer \partial\Omega_{\varepsilon} up to time t. In other words, the boundary local time \ell_{t} can be understood either as the rescaled residence time in a thin boundary layer, or as the rescaled number of encounters with that boundary. Apart from its major role in the theory of stochastic processes, the boundary local time was recently employed as the conceptual pillar to build the encounter-based approach to diffusion-controlled reactions [4]. This approach laid a theoretical ground for assessing the statistics of encounters between the diffusing particle and the boundary, provided an intuitively clear probabilistic interpretation of partial reactivity and allowed one to consider much more general surface reaction mechanisms [8, 9, 10, 11, 12, 13, 14, 15]. Moreover, these concepts were further extended to describe permeation across membranes [16, 17, 18], diffusive exchange between adjacent compartments [19], and the related snapping out Brownian motion [20, 21]. Despite these advances, the fundamental relation between the statistics of the boundary local time \ell_{t} and the geometrical structure of the confining domain \Omega remains poorly understood. For instance, the distribution of \ell_{t} is known explicitly only for two simple domains: a half-line and the exterior of a sphere [8, 22, 23]. Indeed, the symmetries of these domains allow for the separation of variables and thus reduction to a one-dimensional problem that can be solved exactly. In contrast, most diffusive processes in nature and industrial applications occur in multi-scale media with irregular, sophisticated boundaries, for which numerical tools are needed to access the statistics of encounters. Even though the spectral expansions developed in [4] are formally still applicable in such domains, their numerical implementation (e.g., by a finite-element method) can be very costly and time-consuming, especially in three dimensions. In this light, one may prefer Monte Carlo techniques that offer great flexibility and moderate computations costs [24, 25, 26]. While these techniques are broadly used for simulating stochastic processes and diffusion-controlled reactions in complex domains [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49], their adaptation for studying the boundary local time is not straighforward. The most basic way to simulate the boundary local time consists in approximating reflected Brownian motion by a random walk on a lattice with spacing \varepsilon and counting the number \mathcal{N}_{t}^{(\varepsilon)} of its encounters with the discretized boundary \partial\Omega. According to Eq. (3), \varepsilon\mathcal{N}_{t}^{(\varepsilon)} is an approximation of \ell_{t}, if \varepsilon is small enough. The need for boundary discretization and excessively long trajectories (with 2dDt/\varepsilon^{2} steps) are major drawbacks of this method. The first drawback can be relaxed by performing off-lattice random walks, i.e., a sequence of centered Gaussian jumps of variance \sigma^{2} with normal reflections on the boundary. In this case, one can either count the number \mathcal{N}_{t}^{(\sigma)} of reflections and use again Eq. (3) with \sigma instead of \varepsilon, or calculate the residence time of this random walk inside a boundary layer of width \varepsilon\sim\sigma and use Eq. (2) to approximate \ell_{t} (see also [50] for the discussion of Euler schemes). Since a typical one-jump displacement \sigma should be the smallest length scale of the problem, this method can be efficient for simple domains (see, e.g., [51]) but it becomes too time-consuming in multi-scale media. This issue was partly resolved by Zhou et al. who combined the standard walk-on-spheres (WOS) algorithm by Muller [52] and the fixed-length displacements in a thin layer near boundary [53]. Their hybrid method exploits the well-known advantages of the WOS algorithm for fast simulations of large-scale displacements far from the boundary (see details in Sec. 2.1). As a consequence, the major computational limitation is caused by modeling small displacements inside the boundary layer of small width \varepsilon. This method was further improved and adapted to simulating snapping out Brownian motion in 2D bounded domains by Schumm and Bressloff [54]. In particular, they suggested using walk-on-spheres even inside the boundary layer of width \varepsilon, with the jump distance being fixed to be 2\varepsilon, in order to speed up simulations within that layer. Despite this progress, an accurate estimation of the boundary local time \ell_{t} within a boundary layer requires taking \varepsilon sufficient small so that modeling of reflected Brownian motion within this layer remains the critical bottleneck of these Monte Carlo simulations. In this paper, we propose a different approach to simulate the stochastic process (\bm{X}_{t},\ell_{t}) inside a confining domain. We still employ the WOS algorithm for fast simulations of large-scale displacements far from the boundary. In turn, we replace the detailed time-consuming simulation of the stochastic process within a boundary layer by a single escape event from that layer. A similar approach was recently implemented for simulating the escape of a sticky particle [55]. In fact, once the process arrives inside the layer, we introduce the random escape time \tau from that layer, i.e., the first-passage time to the set \Gamma_{\varepsilon}=\{\bm{x}\in\Omega~{}:~{}|\bm{x}-\partial\Omega|=\varepsilon\} of equidistant points from the boundary (Fig. 1a). The central quantity of interest is the joint probability density of the escape time \tau, the escape position \bm{X}_{\tau}, and the acquired boundary local time \ell_{\tau}. Its exact spectral expansion was recently derived by means of the encounter-based approach [56]. As a smooth boundary is locally flat, the spectral expansion takes a simple form that allows its direct implementation in Monte Carlo simulations. Moreover, when the boundary is composed of flat elements, the simulation of the escape event becomes almost exact. In addition, we propose an improvement to account for the curvature of the boundary in order to enable using a larger width \varepsilon of the boundary layer. In this way, there is no need for simulating multiple reflections on the boundary that was the most time-consuming step and the main source of accumulating errors in former techniques. (a)(b) Figure 1: (a) A zoom near the boundary \partial\Omega: once the WOS arrives inside the boundary layer \partial\Omega_{\varepsilon} of width \varepsilon, one aims at replacing a costly detailed simulation of the random trajectory (in green) from the entrance point \bm{x}_{k} to the exit point \bm{x}_{k+1} by a single escape event. For this purpose, one needs to generate the escape position \bm{x}_{k+1}, the escape time \tau, and the acquired boundary local time \ell^{\prime}_{\tau}. (b) Flat layer approximation: starting from a point (0,y_{0}) (yellow circle), a random trajectory (in blue) experiences numerous reflections on the flat boundary before escaping an infinite stripe of width \varepsilon at the random escape time \tau in a random escape position (x_{\tau},\varepsilon) (red circle). These reflections are characterized by the acquired boundary local time \ell^{\prime}_{\tau}. The random trajectory is thus replaced by a simple jump from (0,y_{0}) to (x_{\tau},\varepsilon). The paper is organized as follows. In Sec. 2, we describe the escape-from-a-layer (EFL) approach. We start with a simpler approximation of a flat boundary layer and then discuss its improvement to curved boundary layers. Section 3 summarizes an algorithmic implementation of the EFL approach. Section 4 validates this method for several confining domains by comparing simulated results to either exact values, or numerical values obtained by a finite-element method. We also compare the performance of our method to the state-of-the-art method by Schumm and Bressloff [54]. In Sec. 5, we discuss further improvements, extensions, applications and future perspectives of this method."
https://arxiv.org/html/2411.10105v1,Parametric Autoresonance with Time-Delayed Control,"We investigate how a constant time delay influences a parametric autoresonant system. This is a nonlinear system driven by a parametrically chirped force with a negative delay-feedback that maintains adiabatic phase locking with the driving frequency. This phase locking results in a continuous amplitude growth, regardless of parameter changes. Our study reveals a critical threshold for delay strength; above this threshold, autoresonance is sustained, while below it, autoresonance diminishes. We examine the interplay between time delay and autoresonance stability, using multi-scale perturbation methods to derive analytical results, which are corroborated by numerical simulations. Ultimately, the goal is to understand and control autoresonance stability through the time-delay parameters.","A specific type of nonlinear system in which the system becomes phase-locked with a driving force in an adiabatic manner, where the frequency of the forcing gradually changes over time. This phenomenon, widely known as autoresonance, leverages the system’s nonlinearity to maintain resonance and effectively increase the system’s amplitude over an extended period, despite changes in the system’s parameters sanjuanbook ; fajans2001autoresonant . Typically, when the parameters of a nonlinear system change, the resonant frequency also shifts, causing the system to become detuned from resonance. In autoresonance, however, the system remains naturally phase-locked with the driving force in an adiabatic manner, allowing for self-adjustment of both amplitude and frequency. Since the initial observation of autoresonance ms_livingstone , a plethora of studies have been conducted across various fields. The use and application of autoresonance have been explored in atomic physics meerson1990strong ; liu1995nonlinear , plasma physics fajans1999autoresonant1 ; fajans1999autoresonant2 ; andresen2011autoresonant ; baker2015electron , nonlinear wave interactions friedland1998autoresonance3 ; friedland1998autoresonant4 ; yaakobi2013complete , planetary dynamics malhotra1999migrating ; friedland2001migration ; lanza2022tidal , and fluid dynamics friedland1999control . The theoretical framework of autoresonance has been extensively developed over the past few years. Some of the foundational work has been carried out by L. Friedland et al. friedland1997variational ; fajans2001dampingeffect ; fajans1999collective ; fajans2000secondharmonic ; nakar1999passage ; barth2014quantum . Studies can be found on asymptotic analysis and stability of autoresonance by L.A. Kalyakin et al. kalyakin2008asymptotic ; kalyakin2013stability . Kovaleva et al. kovaleva2013limiting have contributed an array of excellent works, including the limiting phase trajectory description of autoresonance and investigations of autoresonance in nonlinear coupled chains manevitch2016autoresonant ; kovaleva2018autoresonance ; kovaleva2016autoresonance . Additionally, R. Chacón et al. chacon2005energy ; chacon2010universal ; chacon2008breakdown have developed an energy-based theory and conducted notable research on chaos and the breakdown of autoresonance . While autoresonance induced by external forcing has garnered significant attention, relatively few studies have focused on parametric autoresonance khain2001parametric ; assaf2005parametric ; kiselev2007capture ; friedland2016parametric . In this article, we have investigated the effect of time delay on a parametric autoresonant system. The control of resonance response whether deterministic or stochastic, in various nonlinear systems with time delay has proven to be an efficient strategy over the years hu1998resonances ; mei2009effects ; jeevarathinam2011theory ; cantisan2020delay ; zakharova2017time ; maccari2003vibration . In a recent work, the effect of delay has been discussed in a externally driven autoresonant array of Duffing-Ueda oscillator chacon2024 . Although time delay has been employed in systems with externally applied chirped forcing, its role in parametric autoresonant systems warrants attention due to its potential for effective applications across diverse fields. Driven by an interest in parametric autoresonance, we aim to investigate the effect of time delay in such systems, an aspect that has received limited attention in the existing literature, to the best of our knowledge. Our findings demonstrate that the delay strength can serve as a powerful mechanism to control the growth of the autoresonant system. The structure of this article is as follows. Section \mathrm{II} outlines the essential mathematical formulations. In Sect. \mathrm{III}, we present numerical results to validate our analytical findings. Lastly, Sect. \mathrm{IV} provides the concluding remarks."
https://arxiv.org/html/2411.10048v1,Physics-informed neural networks need a physicist to be accurate: the case of mass and heat transport in Fischer-Tropsch catalyst particles,"Physics-Informed Neural Networks (PINNs) have emerged as an influential technology, merging the swift and automated capabilities of machine learning with the precision and dependability of simulations grounded in theoretical physics. PINNs are often employed to solve algebraic or differential equations to replace some or even all steps of multi-stage computational workflows, leading to their significant speed-up. However, wide adoption of PINNs is still hindered by reliability issues, particularly at extreme ends of the input parameter ranges. In this study, we demonstrate this in the context of a system of coupled non-linear differential reaction-diffusion and heat transfer equations related to Fischer-Tropsch synthesis, which are solved by a finite-difference method with a PINN used in evaluating their source terms. It is shown that the testing strategies traditionally used to assess the accuracy of neural networks as function approximators can overlook the peculiarities which ultimately cause instabilities of the finite-difference solver. We propose a domain knowledge-based modifications to the PINN architecture ensuring its correct asymptotic behavior. When combined with an improved numerical scheme employed as an initial guess generator, the proposed modifications are shown to recover the overall stability of the simulations, while preserving the speed-up brought by PINN as the workflow component. We discuss the possible applications of the proposed hybrid transport equation solver in context of chemical reactors simulations.","The outstanding abilities of neural networks (NNs) in approximating complex relations have resulted in their successful application in many fields, ranging from image recognition and text comprehension to mimicking the solutions of differential equations encountered in complex engineering problems 1. One of the benefits brought by employing NNs as an alternative to traditional numerical methods is shifting the computational burden to the training phase, which is performed only once, thus enabling faster solution generation during the inference phase. This can be especially helpful in accelerating multi-stage simulations when the output of one computational method is used as an input to another one, as often encountered in engineering problems or digital twins designs 2, 3. An illustrative example can be found in chemical engineering problems related to ground-up modeling of chemical reactor or even entire chemical plants. In such applications, theoretical models are commonly available for finding the rates of both the micro-scale phenomena (e.g., molecular-level chemical reactions) and macro-scale phenomena (e.g., heat and mass transport). Their coupling results then in a system of equations which should be solved self-consistently, e.g., by solving the ‘micro-scale’ equations as a sub-task each time when the evaluation of the source terms in ‘macro-scale’ equations is required. Replacing solution of such sub-tasks with NNs is then an attractive option to accelerate the overal simulation. Despite their advantages, NNs, like many other models which learn from data, often lack interpretability. This makes their reliability in scientific or mission-critical applications questionable. Physics-informed neural network (PINN) approach has been proposed to partially overcome this drawback by incorporating the available theoretical knowledge into the NN training process 4. This approach suggests using exact equations known from the theory as the objectives that the function approximated by NN is expected to satisfy. Imposing such type of constraints often appears sufficient to make NN fit the solution of a theory-based (typically, physics-based) equation and in this way to become more interpretable. The physics-informed paradigm also requires minimal changes to the NN architecture and can be conveniently implemented in one of numerous specialized programming frameworks. However, as the loss function which is minimized during any NN training does not commonly reach exactly zero during or after this process, the theory-based constraints can only be satisfied approximately. Thus, the transfer of the theoretical knowledge into the PINN achieved by the physics-informed method remains incomplete. As will be discussed below, this can have significant consequences for incorporating PINNs into the multi-stage simulations. More broadly, PINNs can be viewed in context of wider family of methods, commonly known as the physics-informed machine learning (PIML). Within this family, there are other methods which are also intended to fuse theory-based and data-driven methods into a single computational model, but achieve this by modifying the architecture of the NN itself in order to make its output by design fulfil certain theory-based constraints exactly (e.g., 5, 6, 7, 8). Although none of the PIML approaches typically achieves complete transfer of all available theoretical knowledge into the NN, different approaches prioritize different aspects of it. To make this point more concrete, it is instructive to distinguish between the accuracy of the numerical values produced by NNs and the correctness of their dependence on the NN input parameters on the asymptotics. In this paper, we demonstrate that minor numerical inaccuracies of PINN as a function approximator can significantly affect the overall result of a multi-stage simulation, when the PINN acts as a source term in a diffusion-like equation (subsection 2.3). We further investigate the asymptotics suggested by the theory-based equations (subsection 3.1) and propose a modified PINN architecture which ensures the model follows them by design (subsections 3.2, 3.3). This is done for the particular case of reaction-diffusion system related to Fischer-Tropsch synthesis (FTS) process, which is widely used in chemical industry to produce synthetic hydrocarbons (the underlying equations are reviewed in subsections 2.1 and 2.2, along with the benefits of leveraging PINN for solving them). Finally, we demonstrate that a well-defined and guaranteed asymptotic behavior of a modified PINN is essential for constructing a conventional finite-difference equation solver enabling a stable convergence for the considered problem (subsection 3.4)."
https://arxiv.org/html/2411.09864v1,Uncertainty Propagation within Chained Models for Machine Learning Reconstruction of Neutrino-LAr Interactions,"Sequential or chained models are increasingly prevalent in machine learning for scientific applications, due to their flexibility and ease of development. Chained models are particularly useful when a task is separable into distinct steps with a hierarchy of meaningful intermediate representations. In reliability-critical tasks, it is important to quantify the confidence of model inferences. However, chained models pose an additional challenge for uncertainty quantification, especially when input uncertainties need to be propagated. In such cases, a fully uncertainty-aware chain of models is required, where each step accepts a probability distribution over the input space, and produces a probability distribution over the output space. In this work, we present a case study for adapting a single model within an existing chain, designed for reconstruction within neutrino-Argon interactions, developed for neutrino oscillation experiments such as MicroBooNE, ICARUS, and the future DUNE experiment. We test the performance of an input uncertainty-enabled model against an uncertainty-blinded model using a method for generating synthetic noise. By comparing these two, we assess the increase in inference quality achieved by exposing models to upstream uncertainty estimates.","Deep learning has seen wide success and growing adoption in many fields of study and in various industries. Increasingly, these techniques are employed in reliability-critical tasks, where a model’s poor prediction can lead to grave outcomes, as in medical diagnostics [1], autonomous driving [2], and other applications where human health may be implicated. Similarly, deep learning based modeling is also finding adoption in scientific applications, such as neutrino physics reconstruction [3], control of particle accelerators [4], etc. In such fields, erroneous models can lead to deleterious effects on scientific knowledge and progress. For such applications, in addition to the model’s predictions, we require a measure of the model’s confidence in its predictions, e.g., in the form of prediction intervals, calibrated probabilities, etc. One method for increasing the reliability of predictions is to produce a measure of confidence associated with a given model prediction. This additional information, along with the most probable or mean value, can enable a user to selectively ignore anomalous inferences, synthesize confident and uncertain inferences in a holistic way, and help diagnose mis-modeling for certain inputs. Uncertainty in a model’s prediction can arise due to several sources [5, 6], typically categorized into epistemic and aleatoric. The choice of model’s architecture and complexity defines the space of possible functions which can be expressed, and the disparity between this space and the true process being modeled represents the model’s epistemic uncertainty [7]. Once the structure of a model is decided, it is then trained using a sample drawn from a distribution of possible inputs for the task. In this training process, a second type of uncertainty – aleatoric uncertainty – is introduced, through the disagreement between the training set and the real input space, as well as approximation error accumulated by fitting model weights numerically. In some cases, the input itself may have uncertainty, as it may be the product of a physical measurement, or the output of another uncertainty-enabled model. In this case, a model should propagate the input as a distribution, predicting an output distribution whose shape is a derived from both the input and model uncertainties. With the advent of end-to-end deep learning approaches, most applications use a single model accepting data, carrying out representation learning, and outputting the final predictions [8]. However, in certain cases, it is desirable to approach the problem as a set of multiple discrete tasks in a sequence. This can be particularly useful when there are meaningful representations of the data between the input and final output spaces. These intermediate representations may be physically interesting in and of themselves. They may also correspond to an independently measurable quantity that can be used to form a specialized training or validation set for a separable task in the chain. In such a chain of models, errors and uncertainties from an upstream model in the sequence can affect the performance of the downstream models. Testing the performance of any of the individual models in the chain in isolation may not be an adequate measure of their final performance when deployed in the chain. Thus, it is essential to ascertain the effects of the propagation of uncertainties from the upstream models to the downstream models at each stage in the chain. Additionally, it is essential to understand the dependence of the final uncertainty upon the uncertainty of inputs, for various kinds of inputs with varying degrees of measurement error, including which steps in the chain have an inflationary or reductionary effect. Chains of data driven models have been applied to complex tasks in material science [9], fluid mechanics [10], etc. Furthermore, they are often used in autonomous driving applications. Le et al. [11] compared the performance of chained neural networks to an end-to-end deep learning model for the task of locating nearest objects in geographic images. They found that while both approaches had similar accuracy, chained composite models exhibited an order of magnitude reduction in computational expense for training. In this paper, we will focus on a sequential model which was developed for the reconstruction of neutrino-argon scattering events in a Liquid Argon Time Projection Chamber (LArTPC), a detector design widely used in neutrino oscillation experiments, such as the future Deep Underground Neutrino Experiment (DUNE). The initial input for this model, the detector output, is represented by a sparse point cloud – measurements of ionization signals left in the wake of energetic charged particles interacting in the active liquid argon (LAr) medium. The reconstruction model first performs image-based tasks, including semantic classification of voxels and location of key points like potential vertices, start and end points of track-like fragments. Further downstream models represent detector outputs as an abstracted particle flow description, reconstructing the physical type and kinematic properties of each particle as energy is deposited in the sensitive volume over the course of the interaction’s timeline. A more complete description of this model can be found in Section 2.2. In the context of this reconstruction chain, we study the effects of uncertainty propagation in chained machine learning models. We compare the accuracy and uncertainty predictions of both uncertainty-aware (UA) and uncertainty-blinded models (referred to as “blind”), by producing parallel models trained with and without per-feature uncertainty estimates. We demonstrate improvements in these metrics in the context of the node and edge classification tasks performed by a Graph Neural Network (GNN) (the first model labelled as ”GrapPA“ in Figure 2)."
https://arxiv.org/html/2411.08911v1,A Message Passing Neural Network Surrogate Model for Bond-Associated Peridynamic Material Correspondence Formulation,"Peridynamics is a non-local continuum mechanics theory that offers unique advantages for modeling problems involving discontinuities and complex deformations. Within the peridynamic framework, various formulations exist, among which the material correspondence formulation stands out for its ability to directly incorporate traditional continuum material models, making it highly applicable to a range of engineering challenges. A notable advancement in this area is the bond-associated correspondence model, which not only resolves issues of material instability but also achieves computational accuracy comparable to finite element analysis (FEA). However, the bond-associated model typically requires higher computational costs than FEA, which can limit its practical application. To address this computational challenge, we propose a novel surrogate model based on a message-passing neural network (MPNN) specifically designed for the bond-associated peridynamic material correspondence formulation. Leveraging the similarities between graph structure in computer science theory and the neighborhood connectivity inherent to peridynamics, we construct an MPNN that can transfers domain knowledge from peridynamics into a computational graph and shorten the computation time via GPU acceleration. Unlike conventional graph neural networks that focus on node features, our model emphasizes edge-based features, capturing the essential material point interactions in the formulation. Additionally, an attention mechanism is integrated into the MPNN to enhance its representation of bond-associated dynamics, ensuring that significant interactions are weighted appropriately. A key advantage of this neural network approach is its flexibility: it does not require fixed neighborhood connectivity, making it adaptable across diverse configurations and scalable for complex systems. Furthermore, the model inherently possesses translational and rotational invariance, enabling it to maintain physical objectivity—a critical requirement for accurate mechanical modeling. We validate the accuracy and efficacy of this surrogate model through several numerical examples, demonstrating its potential as a powerful tool for efficient and accurate peridynamic simulations. This work opens new avenues for applying peridynamic models in computationally demanding scenarios, providing a viable alternative to traditional methods with the potential for significant computational savings.","Peridynamics is a nonlocal continuum mechanics theory that addresses the limitations of the classical local theory in dealing with spatial discontinuities and accounting for length scale effects [1, 2, 3, 4, 5, 6]. The development of peridynamics began with the seminal work by Silling [1] on reformulation of elasticity theory for discontinuities and long-range forces, where pairwise bond-based interactions within finite distance called horizon are formulated. In this bond-based formulation, the force density of a bond depends only on its stretch. While it is effective in capturing fracture phenomena, the bond-based formulation is limited in describing general material behaviors such as arbitrary Poisson ratio and nonlinear constitutive relationship, due to the usage of a pairwise potential that is totally independent of all other local conditions [2]. To overcome this limitation, the state-based formulation which leveraged the concept of state to rewrite the material-dependent part of the peridynamic model was introduced [2]. More importantly, the material correspondence formulation, a subset of the state-based formulations, bridges the gap between peridynamics and the classical continuum mechanics theory by allowing direct incorporation of continuum material models into peridynamics. This is achieved by introducing nonlocal deformation gradient and stress tensors in a manner equivalent to the classical continuum mechanics theory but within a nonlocal framework. However, the material correspondence formulation is not without challenges. One well-known issue of the formulation is the existence of material instability or zero-energy modes manifested in the form of oscillation in the displacement field. These modes arise when certain deformation states do not contribute to the strain energy, leading to non-physical solutions and numerical instabilities. Among existing strategies proposed in the literature to address this issue, the bond-associated formulations are the most effective and provide more accurate accounting of bond-level quantities such as deformation gradient and stresses. Chen et al. [7, 8] proposed a family of non-spherical influence function and developed the corresponding material correspondence model to improve the accuracy of bond-level quantities such as deformation gradient and stress. Unlike the conventional formulation, where the influence function is spherical and depends only on the bond length, the proposed non-spherical influence functions take into account both the bond length and the bond relative angle (with respect to a target bond). This novel bond-associated correspondence model has achieved great success in inherently eliminating the material instability in the conventional formulation. However, the computational cost of this bond-associated formulation is higher than conventional ones, which is caused by iterative calculation of the non-spherical influence function. One way to overcome this issue is using GPU acceleration technique, which is commonly adopted in artificial intelligence (AI) or machine learning (ML) models. Graph neural networks (GNNs) are a type of neural network architecture that can operate on graph structures, such as those found in social networks, molecules, and materials. Unlike traditional neural networks, which are designed for tabular data, GNN models can be proposed to operate directly on the graphs with arbitrary edges, giving them the flexibility to train task-specific representations more relevant to the properties of interest. More specifically, GNN captures the dependence of graphs via message-passing between the nodes or edges of graphs. According to the pattern of message-passing and aggregation, it contains different variants, such as graph recurrent network (GRN) [9], graph convolutional network (GCN) [10], graph attention network (GAT) [11], etc. As a sub-class of GNN, message-passing neural networks (MPNNs) have recently shown great success in a wide range of applications[12]. Due to the similarities existed between non-locality of peridynamics and graph structure, bond-associated influence functions and attention mechanism, it is possible to build a MPNN-based surrogate model for the bond-associated material correspondence model. By doing so, we can not only achieve faster running speed by leveraging GPU computation, but also pave the path of discovering new material response through solving inverse problems. Some attempts have been made by incorporating peridynamics with the novel AI/ML methods. Ning et al.[13], Madenci et al.[14, 15] developed physics-informed neural network based on peridynamic differential operator. Yu et al.[16] proposed energy-informed neural network as a surrogate model for computing the displacement, which essentially is also a physics-informed neural network. Jafarzadeh et al.[17] introduced the peridynamic neural operator framework which learns ordinary state-based peridynamic constitutive models from data. So far, few attempts for combining GNNs with peridynamic material correspondence formulation have been reported in the literature. Due to their similarities in both structures and mechanisms, we believe this can be a promising direction for AI-assisted scientific computing. The remaining part of this paper is organized as follows: a brief introduction to bond-associated peridynamics correspondence formulation is presented in section 2; the proposed message passing neural network framework is given in section 3."
https://arxiv.org/html/2411.09584v1,A Sylvester equation approach for the computation of zero-group-velocity points in waveguides,"Eigenvalues of parameter-dependent quadratic eigenvalue problems form eigencurves. The critical points on these curves, where the derivative vanishes, are of practical interest. A particular example is found in the dispersion curves of elastic waveguides, where such points are called zero-group-velocity (ZGV) points. Recently, it was revealed that the problem of computing ZGV points can be modeled as a multiparameter eigenvalue problem (MEP), and several numerical methods were devised. Due to their complexity, these methods are feasible only for problems involving small matrices. In this paper, we improve the efficiency of these methods by exploiting the link to the Sylvester equation. This approach enables the computation of ZGV points for problems with much larger matrices, such as multi-layered plates and three-dimensional structures of complex cross-sections.","In many physics and engineering applications, we encounter parameter-dependent quadratic eigenvalue problems (QEP) of the form W(k,\omega)u:=\big{(}(\mathup{i}\mkern 1.0muk)^{2}L_{2}+\mathup{i}\mkern 1.0% mukL_{1}+L_{0}+\omega^{2}M\big{)}\,u=0, (1) where L_{0}, L_{1}, L_{2}, M are real n\times n matrices, which are usually obtained by a (semi-)discretization of a boundary value problem. The solutions (k,\omega) form eigencurves \omega(k), and we are interested in locating the critical points on these curves, where \omega^{\prime}(k)=\frac{\partial\omega}{\partial k}=0. Although solutions of (1) can be complex, we consider the important case where \omega and k are both real. This work is motivated by the study of (anisotropic) elastic waveguides (see, e.g., [23, 35]), where \omega denotes the angular frequency and k the wavenumber. In this context, the eigencurves are referred to as dispersion curves. The slope c{g}=\omega^{\prime} is called group velocity, which is of practical relevance, as it describes the propagation of energy. Points (k_{*},\omega_{*}) on the dispersion curves where the group velocity vanishes are called zero-group-velocity (ZGV) points. Often, the term is used exclusively for solutions at finite wavenumber k_{*}, as this is the non-trivial case, but here we use the designation for solutions at any k_{*}. In the light of this motivating practical application, we will generally refer to points on the curves formed by eigenvalues of parameter-dependent eigenvalue problems that satisfy \omega^{\prime}(k)=0 as ZGV points, irrespective of their physical interpretation. Recently, a numerical algorithm for the computation of ZGV points in anisotropic elastic waveguides was introduced [23] that can be applied to a general problem of the form (1). The method is based on a generalization of the method of fixed relative distance (MFRD) from [19], which provides good initial approximations that can be refined by a locally convergent Newton-type method. Inspired by the Sylvester-Arnoldi method from [28], we show in this contribution that sophisticated tools from linear algebra substantially speed up the algorithm and reduce its memory requirements. This enables us to solve problems with larger matrices and tackle more complex problems such as multi-layered plates as well as waveguides of arbitrary two-dimensional cross-sections. In the following, we first discuss properties of ZGV points in Section 2. In Section 3, we introduce several tools we will use in the following section; the presentation is intertwined with their application to the computation of ZGV points: the Sylvester equation, multiparameter eigenvalue problems, and the MFRD. Our main contributions are included in Section 4, where we show how we can exploit the structure of the Sylvester equation to apply the MFRD more efficiently, and in Section 5, where we present a scanning algorithm for the computation of ZGV points that combines the MFRD and a locally convergent Gauss-Newton method. In Section 6, we introduce a waveguide model that is used in the numerical experiments in the following section, where we demonstrate the strength of the proposed method. Finally, we discuss possible generalizations and give a conclusion in Sections 8 and 9."
https://arxiv.org/html/2411.09133v1,Computational metaoptics for imaging,"Metasurfaces—ultrathin structures composed of subwavelength optical elements—have revolutionized light manipulation by enabling precise control over electromagnetic waves’ amplitude, phase, polarization, and spectral properties. Concurrently, computational imaging leverages algorithms to reconstruct images from optically processed signals, overcoming limitations of traditional imaging systems. This review explores the synergistic integration of metaoptics and computational imaging, “computational metaoptics,” which combines the physical wavefront shaping ability of metasurfaces with advanced computational algorithms to enhance imaging performance beyond conventional limits. We discuss how computational metaoptics addresses the inherent limitations of single-layer metasurfaces in achieving multifunctionality without compromising efficiency. By treating metasurfaces as physical preconditioners and co-designing them with reconstruction algorithms through end-to-end (inverse) design, it is possible to jointly optimize the optical hardware and computational software. This holistic approach allows for the automatic discovery of optimal metasurface designs and reconstruction methods that significantly improve imaging capabilities. Advanced applications enabled by computational metaoptics are highlighted, including phase imaging and quantum state measurement, which benefit from the metasurfaces’ ability to manipulate complex light fields and the computational algorithms’ capacity to reconstruct high-dimensional information. We also examine performance evaluation challenges, emphasizing the need for new metrics that account for the combined optical and computational nature of these systems. Finally, we identify new frontiers in computational metaoptics which point toward a future where computational metaoptics may play a central role in advancing imaging science and technology.","All imaging systems rely on a combination of optical hardware – that routes photons from a scene onto a detector – and software – that processes the measured signal to generate an image. Both components have been the topic of constant innovation over the past few decades. On the hardware side, metasurfaces – subwavelength arrays of nanostructured optical elements – have revolutionized the field of nanophotonics in the past decade kuznetsov2024roadmap ; yu2014flat ; khorasaninejad2016metalenses ; yu2011light ; genevet2017recent ; khorasaninejad2017metalenses ; arbabi2015dielectric . Metasurfaces allow the control of virtually all properties of an incident electromagnetic wave at the nanoscale, such as its polarization, spectral, and angular distribution (Fig. 1b). The development of integrated nonlinear material platforms has also enabled frequency conversion and quantum optical state generation with metasurfaces li2017nonlinear ; Wang2022metasurfaces ; solntsev2021metasurfaces . On the software side, computational imaging is an interdisciplinary field that combines elements of computer science, optics, signal processing, and imaging technologies to enhance the quality and capabilities of imaging systems. By using algorithms to process and interpret data captured by sensors, computational imaging extends beyond the limitations of optics-only approaches, enabling applications such as high-resolution imaging, volumetric, depth imaging, and advanced object recognition. A foundational aspect of computational imaging is the use of computational methods to reconstruct images from data that may be incomplete, noisy, or otherwise imperfect (some examples shown in Fig. 1c). This approach can involve numerical methods in machine learning, inverse problems, and optimization bertero2021introduction ; donoho2006compressed ; barbastathis2019use . Recent works have explored the fruitful intersection between metaoptics and computational imaging lin2021end ; lin2022end ; huang2022fullcolor ; saragadam2024foveated ; whitehead2022fast ; colburn2018metasurface ; li2024singleshot . This Perspective aims at highlighting this novel and exciting direction for metasurface research which also represents a paradigm shifting opportunity for imaging and sensing technologies. But why is there a natural “marriage” between metaoptics and computational imaging? We focus on four key arguments to answer this question below: (1) the performance of multifunctional metasurfaces will inevitably reach a ceiling. While single-layer metasurfaces are now able to process multiple spectral, polarization, and momentum degrees of freedom, integrating multiple functionalities in a single device is usually done at the price of key performance metrics (e.g., focusing efficiency, Strehl ratio, etc.). An alternative solution is to stack multiple layers of metasurfaces to lift the limitations of single layers while conserving a compact form factor. For instance, realizing the same amount of functionality as a conventional plan achromatic objective requires a dozen such layers lin2021computational . Proof-of-concept experiments with few layers have been realized at telecom wavelengths zhou2018multilayer ; roques2022toward , and with volumetric designs at longer wavelengths camayd2020multifunctional ; roberts20233d ; ballew2023multi , but realizing nanoscale volumetric patterning at optical frequencies remains an incredible technical challenge; (2) many iterative algorithms already used in computational imaging rely on numerical preconditioners that transform the input to facilitate the task of a reconstruction problem saad2003iterative . Metasurfaces are physical preconditioners that can implement simple computational imaging tasks in the optical domain long2021isotropic ; abdollahramezani2020meta ; pors2015analog ; cordaro2023solving ; koenderink2015nanophotonics ; kwon2018nonlocal . In other words, metasurfaces preprocess the light field from a scene incident on a detector, whose signal is input to a numerical image-reconstruction task. Optimization of metasurface designs even allows the automatic discovery of optimal preconditioners that do not require human specification. Moreover, we argue that synergetic endeavors between metaoptics and computational imaging are natural and effortless: (3) any practical imaging application relies on a digital detector whose signal is processed by a computing unit. The use of software for denoising, signal processing, and segmentation is already pervasive, and significant effort has been invested in making application-specific computing units (such as graphics processing units, field programmable gate arrays, and image signal processors owens2007survey ; bailey2023design ; nakamura2017image ) to match the bandwidth and energy requirements of image processing tasks; (4) while optimization is already prevalent in image processing, automatic discovery of optimal metasurface designs (with methods such as inverse design, topology optimization, and surrogate models jensen2011topology ; molesky2018inverse ; pestourie2018inverse ) has permeated the field of metaoptics. Computational metaoptic imaging can therefore rely on the natural co-integration of optimization methods from computational imaging and metaoptics design. This Perspective aims at providing a comprehensive exploration of computational imaging empowered by metaoptics. We begin by revisiting the foundational principles of computational imaging with metasurfaces, underscoring their unique advantages over traditional lens-based systems. We then delve into the end-to-end (inverse) design of computational metaoptics, highlighting how the seamless co-design of hardware and software through gradient backpropagation can lead to superior imaging performance. Our discussion extends to advanced applications enabled by this approach, including phase imaging and quantum photonic state measurement. We also address the challenges of performance evaluation specific to computational metaoptics, proposing suitable metrics for assessing these systems that combine hardware and software. Finally, we outline new frontiers in the field, pointing toward future research directions and opportunities in this rapidly evolving domain. Figure 1: Computational imaging with metaoptics: degrees of freedom, physics, and algorithms. a. The general goal of a computational imaging device is to reconstruct various degrees of freedom of an incident light field, for instance its polarization, frequency, momentum, and complex amplitude distribution. Advanced degrees of freedom (e.g., density matrix of the quantum state of light) may also be of interest. b. Light manipulation is realized by leveraging physical properties of metaoptical devices, such as their ability to locally control the complex amplitude of an incoming wavefront, engineered spectral dispersion, non-locality (spatial dispersion), active control of physical properties (e.g., complex transmission), and nonlinear optical properties. c. Once imaged by a detector, the signal may be reconstructed using various reconstruction and estimation methods, such as least-square error minimization (which may include priors on the reconstructed degrees of freedom, such as high sparsity or low complexity). Black-box methods, such as fully connected neural networks, may also be utilized to classify detected signals and images. Other parameter estimation methods, such as maximum likelihood estimation (MLE) may also be used to estimate the degrees of freedom of the incident light field."
https://arxiv.org/html/2411.09092v1,Pheromone-Guided Navigation of Potential Mates: A Distinct Exploration Strategy,"Animals, especially insects, navigate their environments using complex strategies that integrate responses to chemical signals such as pheromones. The relative importance an insect places on foraging, finding a mate or other tasks can significantly influence its movement patterns. This study uncovers the distinct exploration strategies that emerge when these priorities vary. A new model is introduced that explicitly treats a pair of insects and their response to each other’s pheromones. The strategy results in dynamics that exhibit characteristic anomalous scaling in displacement and non-Gaussian distributions of position; specifically, compressed exponential distributions for the number of encounters and the total duration of encounters. Our model not only elucidates these emergent behaviours but also provides insight into optimizing encounter frequencies. These strategies can be inverted to design pheromone-based traps for luring target species to specific areas for removal. The model is also applicable more broadly to pairs of agents that both respond to the paths traced by themselves and each other.","Animal exploration strategies often deviate from simple random movement [1, 2]. This complexity arises particularly in species relying on chemical signals (semiochemicals) such as pheromones for communication and navigation [3, 4]. Traditional random walk models have proven valuable for studying such processes, yet increasing evidence reveals the need for more sophisticated approaches to capture the nontrivial dynamics observed in nature [5]. Recent efforts have focused on deviations from classical diffusion processes, compelling the development of models that accommodate these complexities [6, 7]. One such complexity arises in systems exhibiting anomalous diffusion, where the mean squared displacement (MSD) scales nonlinearly with time—a phenomenon ubiquitous in physical, financial, artificial, and biological systems [8, 9, 10, 11, 12, 13, 14, 15]. In biological contexts, this behaviour often stems from memory effects or interactions with evolving environments. Insects, for example, modify their environment by laying down pheromone trails, which subsequently influence not only their own movements but also those of others in their vicinity [16, 17, 18]. This phenomenon extends to living cells that alter their surroundings by depositing biochemical signals or mechanically remodelling the extracellular matrix [19, 20, 21], as well as large animals that mark their territories [22, 23]. These feedback loops, reminiscent of self-organizing systems, lead to emergent behaviours that deviate sharply from classical diffusion models. Non-Markovian random walks, in which movement decisions depend on the walker’s history, provide a framework for understanding these anomalous behaviours [24, 25, 26, 27, 28, 29, 30, 31, 32, 33]. One established approach for modelling such memory-driven dynamics is the true self-avoiding walk (TSAW), introduced by Amit, Parisi, and Peliti [24]. In the TSAW, movement (transition) probabilities, p, depend on the visitation history of each site: p\propto e^{-\beta h_{i}}, (1) where h_{i} represents the number of times site i has been visited, and \beta is a constant analogous to inverse temperature, controlling the strength of self-avoidance. The TSAW extends the concept of the self-avoiding walk (SAW) [34, 35] that is well-known for modelling the behaviour of flexible chain polymers. Despite its name, the SAW is more accurately described as a ‘self-terminating’ walk, where revisiting a site terminates the walk. In contrast, the TSAW employs a ‘softer’ self-avoidance, where the walker tries to avoid retracing its steps. Within the TSAW model, when self-interaction is repulsive (\beta>0), the time dependence of the MSD, R^{2}, as a function of time exhibits distinct scaling laws in different dimensions, d, at large times [24, 36, 37, 30, 33, 38]: R^{2}\sim\begin{cases}t^{4/3}\quad&d=1\\ t(\ln t)^{1/2}\quad&d=2\\ t\quad&d\geq 3.\end{cases} (2) In contrast, introducing an attractive self-interaction (\beta<0) dramatically restricts the walker’s movement. This leads to repetitive oscillations between two sites for any dimension d, resulting in a constant MSD at large times [30, 38]: R^{2}\to C, where C is a constant depending on d and \beta. Introducing a saturation limit for h_{i} (e.g., h_{i}=0 for unvisited site i, and h_{i}=1 if visited) transforms the self-attractive case, preventing walker trapping. This modified model, known as the self-attracting true random walk (SATW) or one-step reinforced random walk, [39, 30, 29, 40, 41, 42, 38], exhibits a diverging MSD at large times: R^{2}\sim\begin{cases}t\quad&d=1\\ t^{2/3}\quad&d=2\\ t\quad&d=3,|\beta|<|\beta_{c}|\\ t^{1/2}\quad&d=3,|\beta|>|\beta_{c}|\\ \end{cases} (3) Here, \beta_{c} marks a critical value in three dimensions, signifying the transition to a subdiffusive regime. Natural environments present intricate scenarios, particularly when multiple interacting individuals are involved [43, 44]. For example, the trade-off between food exploration and mate-seeking plays a significant role in shaping insect movement. Insects rely on multiple semiochemicals, such as trail and sex pheromones, to guide their movements and interactions, resulting in complex, correlated behaviour. Foraging insects avoid revisiting previously explored locations due to diminishing chances of finding food, while the presence of sex pheromones draws them toward areas marked by potential mates. This dynamic interaction between food foraging and mate-seeking leads to a collective mode of exploration, particularly in species that deposit different types of semiochemicals [4, 18]. When these behaviours are coupled, they give rise to novel collective dynamics characterized by emergent scaling relations that define new universality classes. As far as we are aware, the study of such active systems, where food-foraging and mate-seeking are intertwined, remains largely unexplored. This research represents an exciting frontier in understanding collective dynamics."
https://arxiv.org/html/2411.08845v1,"Linearization Routines for the Parameter Space Concept to determine Crystal
Structures without Fourier Inversion
(Centrosymmetric cases in two and three-dimensional parameter space)","We present detailed elaboration and first generally applicable linearization routines of the Parameter Space Concept (PSC) for determining 1-dimensionally projected structures of m independent scatterers. This crystal determination approach does not rely on Fourier inversion but rather considers all structure parameter combinations consistent with available diffraction data in a parameter space of dimension m. The method utilizes m structure factor amplitudes or intensities represented by piece-wise analytic hyper-surfaces, to define the acceptable parameter regions. By employing the isosurfaces, the coordinates of the point scatterers are obtained through the intersection of multiple isosurfaces. This approach allows for the detection of all possible solutions for the given structure factor amplitudes in a single derivation. Taking the resonant contrast into account, the spatial resolution achieved by the presented method may exceed that of traditional Fourier inversion, and the algorithms can be significantly optimized by exploiting the symmetry properties of the isosurfaces. The applied 1-dimensional projection demonstrates the efficiency of the PSC linearization approach based on fewer reflections than Fourier sums. The Monte-Carlo simulations, using the projections of various random two- and three-atom structure examples, are presented to illustrate the universal applicability of the proposed method. Furthermore, ongoing efforts aim to enhance the efficiency of data handling and to overcome current constraints, promising further advancements in the capabilities and accuracy of the PSC framework.","Solving crystal structures from diffraction intensities plays a vital role in many areas of solid-state research, e. g. physics, chemistry, mineralogy, materials sciences, biology, and pharmacy, as it forms the fundamental basis for understanding the properties of materials as well as their effects and functionalities. The corresponding databases grow by tens of thousands of structures every year. The state-of-the-art structure determination methodology is based on Fourier Inversion (FI) of the scattering density (e. g. electron density for X-ray diffraction, nuclear density for neutron diffraction). In the early days, the developments in crystallography were mainly based on computationally efficient FI techniques either directly or indirectly such as the charge flipping method [1], algebraic method [2, 3, 4], geometrical methods [5], analytical function methods [6], fit methods such as Rietveld refinement [7], and matching learning algorithms [8, 9, 10]. The currently used FI techniques to construct electron density systematically introduce noise and errors in the calculation due to series termination. Therefore, a large number of terms in the FI series are required, which in turn necessitates a substantial set of experimental observations. Furthermore, the quality of X-ray diffraction intensities greatly influences the FI series, with weaker observations contributing less. However, the experimental database is in most cases incomplete since only the quadratic amplitudes of the Fourier coefficients (i. e. the structure factors via the reflection intensities) can be determined, and the well-known phase problem of crystallography makes the structure solving more challenging [11, 12, 13, 14]. In order to overcome the demerits of FI techniques, alternative methods have been developed. In this study, we examine the relationship between the structure factor and the atomic positions in crystal structures under the aspect of geometrical correlations. In general, an m atomic structure has 3m free positional parameters to be determined, which includes the x, y, and z coordinates of all m atoms. To simplify the structure-solving process with PSC, the task is split into several independent 1-dimensional projections (in real space), each providing the solution of m parameters within an m-dimensional Parameter Space (PS; space of atomic coordinates with the orthogonal basis in \mathbb{R^{\textit{m}}}) [15, 12, 16, 17, 18]. Each point in the PS corresponds to a possible combination of atomic coordinates (e. g. projected onto the z axis) and generates a unique X-ray diffraction intensity for a predefined reflection. Vice versa, the set of points that reproduce the experimentally observed intensity of a particular reflection defines a manifold called isosurface, see Fig. 1(a)–(c). The intersection point from all isosurfaces of different reflections expresses the intended structure, see Fig. 1(d). Figure 1: Basic explanation of the Parameter Space Concept (PSC). (a)–(c) 2-dimensional Parameter Space for the projection of a crystal structure of two equal scatterers for the projection onto the z axis for reflections 00l,l=1,2,3. The color map represents the calculated amplitude for each combination of atomic coordinates. The isosurfaces for positive and negative amplitudes generated by the arbitrary atomic coordinates [0.2,0.12] are highlighted with solid and dashed lines, respectively. (d) Overlay of the isosurfaces from (a)–(c) with their intersecting points highlighted by red and green points. The intersection point at the red circle is the intended structure to be found, and alternative solutions are at the green circles. The theory of PSC has been developed during the past two decades, mainly focusing on equal atoms [16, 17, 18] and aiming to achieve higher spatial resolution than the FI techniques while at the same time using a reduced number of available intensity data sets. Apart from these advantages, PSC will always recognize all possible solutions, which can reproduce the given experimental intensities, although at the cost of parameterizing functions in continuous high-dimensional spaces. Hence, the PSC provides an elegant but computationally expensive method to solve the crystal structures in a step-wise approach splitting the full structure into 1-dimensional projections. In the present work, we develop further theoretical approaches to treat all aspects of m-dimensional PSs, in particular generally applicable linearization routines to parameterize the isosurfaces for efficient functional handling as well as computational storage and determination of intersections. The PSC algorithms are now enhanced to treat the artificial values of the atomic scattering factor of scatterers in the m-dimensional PS. The implemented capability to overcome the previously employed Equal Point Atom (EPA) model (see Sec. II.2) improves the PSC towards a generally applicable structure determination approach. However, to test the developed algorithms and code, in this manuscript, we determine the centrosymmetric structures so far only in 2- and 3-dimensional PS. Nevertheless, on the first estimation, the derived equations may be straightforwardly enhanced towards higher dimensions, which will be the focus of our continuous research efforts. This study is structured as follows: the subsequent sections provide an in-depth exploration of our approach, commencing with the fundamental theory underlying the PSC-based framework. Next, the steps involved in solving a maximum of 3 parameters in the 1-dimensional projection are depicted, including the general description of the linear approximation of isosurfaces. Finally, we present noteworthy generalizations based on Monte-Carlo simulations for the structure determination within the 2- and 3-dimensional centrosymmetric PS."
https://arxiv.org/html/2411.08731v1,Combustion Instabilities in Complex Chamber Geometries of Solid Propellant Rocket Motors,"High-frequency combustion instabilities can lead to significant fluctuations in chamber pressure, affecting the structural integrity and performance of solid rocket motors. Since these instabilities manifest as acoustic oscillations during combustion, a mathematical model has been developed to calculate the chamber modes, providing an accurate method for predicting the acoustic behaviour of the combustion chamber. A novel method for discretising the Laplacian operator is introduced, which allows the calculation of the acoustic modes of complex chamber geometries. This approach uses an efficient numerical algorithm designed for unstructured mesh configurations. Several computational examples are presented to demonstrate the application of the model to complex geometries typical of solid rocket motors. In addition, an analytical procedure is developed in these examples to aid the design process, providing engineers with critical data to optimise the stability and performance of propulsion systems. This research provides valuable insights into the analysis of combustion instabilities, with broad implications for the design and optimisation of propulsion systems in aerospace engineering.",Nomenclature Symbols A Transfer function coefficient A_{b} Area of the burning surface [m^{2}] A_{g} Area of the nozzle throat [m^{2}] a Speed of sound [m/s] B Transfer function coefficient c^{*} Characteristic velocity [m/s] E_{n} Modal space norm f Inhomogeneous boundary term h Inhomogeneous term K_{g} Klemmung k Complex wave number [1/s] k^{*} Characteristic wave number [1/m] L^{*} Characteristic chamber length [m] \Delta l Mesh contour M Mach number \dot{m} Mass flow rate [kg/s] n_{s} Pyrolysis constant \hat{n} Normal vector p_{c} Chamber pressure [N/m^{2}] R Gas constant of mixture [\frac{J}{kgK}] R_{g} Nozzle transfer function R_{p} Burning surface transfer function \dot{r}_{b} Regression rate [m/s] T_{c} Chamber temperature [K] u Fluid velocity [m/s] Greek symbols \alpha Growth rate [1/s] \gamma Relation specific heat capacities \varepsilon_{c} Convergent area relation \vartheta_{c} Volume of the cavity [m^{3}] \kappa Small amplitude \lambda Complex root \rho Density of the mixture [\frac{kg}{m^{3}}] \psi_{N} Unperturbed mode \psi_{n} Eigenfunction \Omega Dimensionless frequency \omega Angular frequency [rad/s] Sub-indexes (\ )_{b} Burning surface (\ )_{l} Convergent region (\ )_{p} Propellant (\ )_{t} Throat (\ )_{w} Wall
https://arxiv.org/html/2411.08575v1,Multiscale simulation of neutral particle flows in the plasma edge,"The plasma edge flow, situated at the intricate boundary between plasma and neutral particles, plays a pivotal role in the design of nuclear fusion devices such as divertors and pumps. Traditional numerical simulation methods, such as the direct simulation Monte Carlo approach and the discrete velocity method, are hindered by extensive computation times when dealing with near-continuum flow conditions. This paper presents a general synthetic iterative scheme to deterministically simulate the plasma edge flows. By alternately solving the kinetic equations and macroscopic synthetic equations, our method substantially decreases the number of iterations, while maintains asymptotic-preserving properties even when the spatial cell size is much larger than the mean free path. Consequently, our approach achieves rapid convergence and high accuracy in plasma edge flow simulations, particularly in near-continuum flow regimes. This advancement provides a robust and efficient computational tool, essential for the advancement of next-generation nuclear fusion reactors.","The edge plasma generally refers to the outer layer of plasma in fusion devices (such as tokamaks), consisting of a mixture of plasma and neutral particles [1]. As shown in Fig. 1, the innermost layer is the inner wall surface of the device, with the majority of the poloidal magnetic field generated by the plasma current I_{p}, approximating a circular shape. An external coil with a current I_{D} is aligned in the same direction as I_{p} to create the so-called “poloidal divertor” structure. The magnetic fields produced by I_{D} and I_{p} cancel each other at the X-points. The magnetic surface passing through an X-point is called the separatrix. A solid plate intercepts the magnetic surfaces around the current I_{D}, transporting the high-temperature plasma from the core region to this solid plate via the magnetic field, and preventing direct contact with the inner wall. The region between the outer magnetic flux surface of the core and the separatrix is referred to as the edge plasma region [2, 3, 4, 5]. Achieving fusion necessitates heating plasma to staggering temperatures, far beyond hundreds of millions of degrees, and maintaining this high-density plasma for an extended period. This demanding criterion places extraordinary stress on the materials utilized in the plasma-facing components and the divertor target plates of fusion devices, given the immense heat and plasma flux that emanate from the device. Neutral particles are pivotal in mitigating the particle and energy flux directed towards the divertor target [6]. Under these conditions, a neutral buffer layer emerges in front of the divertor target, which enhances ion-neutral interactions and shields the target from the direct onslaught of high-energy plasma. Consequently, the accurate simulation of the plasma edge, encompassing both plasma and neutral particles, is indispensable for devising operational strategies and blueprinting the next generation of fusion reactors [7, 8]. Figure 1: A schematic of the edge region in the poloidal plane of a divertor tokamak. In the edge plasma, the plasma can be described by macroscopic fluid equations. For the neutral particles, regions with high collision rates approach the hydrodynamic limit and can be effectively described using macroscopic fluid dynamics equations. Conversely, regions with low collision rates, being far from thermal equilibrium, require kinetic equations for precise characterization [9, 10, 11]. This presents a challenge: relying solely on macroscopic fluid equations falls short in accurately depicting the low-collision regions, while an exclusive dependence on kinetic equations is computationally intensive and resource-demanding due to their inherent high dimensionality [12, 13]. Traditionally, the direct simulation Monte Carlo (DSMC) method has been employed for simulating rarefied gas dynamics. However, this approach can result in excessively long computation times and large consumption of computer memory due to the higher frequency of charge exchange collisions. That is, the time step and spatial cell size should be smaller than the mean collision time and mean free path of the neutral particles, respectively. Moreover, the inherent statistical noise in DSMC can impede convergence when dealing with neutral particle models integrated with plasma fluid equations. Consequently, it is imperative to develop an efficient and accurate computational method that possesses asymptotic preserving properties (enabling the use of fewer spatial cells) and rapid convergence characteristics (allowing for a reduced number of iteration steps). Such a method is essential for the design of next-generation nuclear fusion reactors, as indicated by recent studies [14, 15]. As detailed in the review article [16], a significant breakthrough in the efficient simulation of kinetic equations emerged in the field of neutron transport, where the kinetic equation is solved concurrently with the synthetic equation of the diffusion type. This coupling strategy has been successfully applied to specific rarefied gas flow problems, such as planar Poiseuille flow [17] where the flow direction is perpendicular to the computational domain, and has ultimately evolved into the general synthetic iteration scheme (GSIS) for general rarefied gas flows [18]. GSIS effectively spans the micro-macro gap by iteratively solving the kinetic equation and its corresponding synthetic equations. Within the synthetic equation, the linear Navier-Stokes-Fourier (NSF) equations are integrated with higher-order terms (HoTs), which are directly derived from the velocity distribution function. These HoTs capture rarefaction effects and ensure the validity of the constitutive relations across all Knudsen numbers. In the continuous flow regime, where the Knudsen number is low, the higher-order terms diminish to zero, ensuring that GSIS asymptotically preserves the Navier-Stokes equations in the continuum limit. This preservation allows for the use of larger cell sizes and time steps, which is particularly beneficial for simulating edge plasma. Through rigorous mathematical analysis and extensive numerical testing [19, 20, 21, 22], GSIS has demonstrated a significant reduction in the number of iterations required and an enhancement in simulation efficiency by several orders of magnitude. This study will focus on developing a new GSIS algorithm for neutral particles in the plasma edge, aiming to achieve rapid convergence and asymptotic preservation. This approach is intended to reduce the number of spatial cells (simulation memory) and iterations (simulation time) required. The structure of this paper is as follows. In Section 2 we introduce the kinetic model equations and hydrodynamic equations for neutral particles. In Section 3, we introduce the detailed numerical method to solve the GSIS. In Section 4, the effectiveness of the GSIS algorithm is validated in various numerical simulations. Finally, the conclusions are given in Section 5."
https://arxiv.org/html/2411.08789v1,Physics-Informed Transformation Toward Improving the Machine-Learned NLTE Models of ICF Simulations,"The integration of machine learning techniques into Inertial Confinement Fusion (ICF) simulations has emerged as a powerful approach for enhancing computational efficiency. By replacing the costly Non-Local Thermodynamic Equilibrium (NLTE) model with machine learning models, significant reductions in calculation time have been achieved. However, determining how to optimize machine learning-based NLTE models in order to match ICF simulation dynamics remains challenging, underscoring the need for physically relevant error metrics and strategies to enhance model accuracy with respect to these metrics. Thus, we propose novel physics-informed transformations designed to emphasize energy transport, use these transformations to establish new error metrics, and demonstrate that they yield smaller errors within reduced principal component spaces compared to conventional transformations.","A recent inertial confinement fusion (ICF) Nuckolls et al. (1972) experiment Zylstra et al. (2022) at the National Ignition Facility (NIF) achieved fusion ignition, obtaining a greater fusion energy yield than the input laser energy. Since then, ignition has been repeated several times Tollefson (2024), and it is expected that this result will soon become routine, allowing regular experiments to understand the physics of a self-heating high energy density plasma and to improve the energy gain. In order to optimize this gain with respect to the experimental configuration, many variations in design parameters must be made. Because each experiment is very expensive and time consuming to plan, simulations are used to develop optimum designs. Such simulations have a high computational cost, dominated by the Non-local Thermodynamic Equilibrium (NLTE) model Scott et al. (2022); Frank and Scott (2022). While machine learning (ML) techniques have significantly reduced this computational cost by a factor of ten Kluth et al. (2020); Humbird et al. (2020); Lee and Carlberg (2020); Humbird et al. (2021); Vander Wal et al. (2022); Vander Wal et al. (2023a); Vander Wal (2022); Vander Wal et al. (2023b), it is unclear what the best data representation and error metrics are for fast and accurate ICF simulations. Researchers have employed various data processing techniques to enhance the robustness and scalability of models. One approach involves using a modified logarithmic transformation to reduce the dynamic range of NLTE data. Furthermore, Vander Wal et al. Vander Wal et al. (2022) further proposed replacing the logarithmic transformation with a cube root transformation to address the thresholding effect that artificially inflates low-opacity values for low-Z elements. However, the precise impact of these transformations on the final outcomes of ICF simulations has yet to be thoroughly investigated, and the potential of other transformations to further improve these outcomes has not been fully explored. A more critical issue is the lack of appropriate error metrics used in training ML models, which can result in the oversight of significant effects, physically relevant to ICF simulations. Kluth et al. Kluth et al. (2020) and subsequent studies Humbird et al. (2020, 2021); Vander Wal et al. (2022); Vander Wal (2022); Vander Wal et al. (2023b) have employed Rosseland and Planck means, as well as integrated emissivity, as physics-based error metrics. While these metrics are suitable for evaluating photon energy transmission in optically thick media, optically thin media, and total photon energy emission, respectively, they are not resolved by photon energy groups and are not directly applicable to systems without a well-defined radiation temperature, such as NLTE regions in ICF simulations. To address these issues, we introduce physics-informed transformations (PhITs) and corresponding error metrics. The PhITs are bijective data transformations of absorption and emissivity—two outputs of NLTE models—into variables that describe net energy exchange and photon mean free path, which are fundamental physical quantities. This approach enables the development of generalized models, as the transformed data should adhere to physical laws regardless of the conditions. Additionally, error metrics derived from these transformations allow for a quantitative analysis of how data processing affects radiation properties in ICF simulations, providing guidance for models to minimize these errors and emphasize the most important physical phenomena. In order to provide context for the PhIT transformation, we describe the radiation-hydrodynamics that motivates the development of the PhIT in Section II. Also, Section III details the process by which we generated our training data. In Section IV, we introduce the PhIT as a tool to support machine learning, highlighting its advantages for radiation hydrodynamics simulations. Section V offers a physical interpretation of errors generated through Principal Component Analysis (PCA) and illustrates how the PhIT optimizes errors with respect to the dimensional size of the PCA space. Finally, in Section VI, we provide a summary and offer insights for further testing of the transformation."
https://arxiv.org/html/2411.08760v1,Energy Dissipation Preserving Physics Informed Neural Network for Allen-Cahn Equations,"This paper investigates a numerical solution of Allen–Cahn equation with constant and degenerate mobility, with polynomial and logarithmic energy functionals, with deterministic and random initial functions, and with advective term in one, two, and three spatial dimensions, based on the physics-informed neural network (PINN). To improve the learning capacity of the PINN, we incorporate the energy dissipation property of the Allen-Cahn equation as a penalty term into the loss function of the network. To facilitate the learning process of random initials, we employ a continuous analogue of the initial random condition by utilizing the Fourier series expansion. Adaptive methods from traditional numerical analysis are also integrated to enhance the effectiveness of the proposed PINN. Numerical results indicate a consistent decrease in the discrete energy, while also revealing phenomena such as phase separation and metastability.","Phase field models have a central role to understand the behaviour of the many complicated moving interface problems in material science [1], fluid dynamics [2], fracture mechanics [3, 4], image analysis [5], and mean curvature flow [6]. Among these models, Allen-Cahn introduced in [7] to describe the motion of anti-phase boundaries in crystalline solids at a fixed temperature is a particular case of gradient flows in the form of u_{t}=-\mu(u)\ \frac{\delta\mathcal{E}(u)}{\delta u}, (1) where \frac{\delta\mathcal{E}(u)}{\delta u} represents the variational derivative of the free energy taken in the L^{2}(\Omega)-norm with \Omega\subset\mathbb{R}^{d} (d=1,2,3) as follows \mathcal{E}(u)=\int_{\Omega}\left(\frac{\epsilon^{2}}{2}|\nabla u|^{2}+F(u)% \right)d{\bm{x}}. (2) Specifically, the Allen-Cahn equation corresponds to the following nonlinear partial differential equation (PDE) u_{t}=\mu(u)(\epsilon^{2}\Delta u-f(u)),\quad({\bm{x}},t)\in\Omega\times(0,T], (3) where u denotes the concentration of one the species of the alloy, known as the phase state between materials, \epsilon represents the small interfacial length during the phase separation process, and \mu(u) is the non–negative mobility function. As \epsilon tends towards zero, it forms boundaries between the two stable phrases u=\pm 1, which is a process known as phase separation. Moreover, the nonlinear f(u) is the derivative of a free energy functional F(u), which is characterized by logarithmic or polynomial functions. The logarithmic free energy potential (see, e.g., [8]) can be formulated as F(u)=\frac{\theta}{2}[(1+u)\ln(1+u)+(1-u)\ln(1-u)]-\frac{\theta_{c}}{2}u^{2},% \,\,\,\,\,0<\theta\leq\theta_{c}, (4) where \theta and \theta_{c} are absolute and transition temperatures, respectively. The derivative of the logarithmic free energy potential is then equivalent to f(u)=F^{\prime}(u)=\frac{\theta}{2}\ln[\frac{1+u}{1-u}\Bigg{]}-\theta_{c}u. On the other hand, the polynomial free energy potential, also called as quartic double–well, is an approximation of the logarithmic ones when the temperature \theta closes to \theta_{c}, formulated as follows F(u)=\frac{1}{4}(1-u^{2})^{2}, (5) and its derivative is f(u)=u^{3}-u. It is well-known that the solution u({\bm{x}},t) of the Allen–Cahn equation has two crucial properties; the decay of the total free energy for \mu(u)>0, that is, \frac{d\mathcal{E}(u(t))}{dt}\leq 0, (6) which is a typical property of the gradient flows, and the maximum bound principle. In the double-well potential (5), the solution always stays inside the interval [-1,1], while the solution of the logarithmic potential (4) is bounded in the interval [-s,s] for all the time, where 1>s>0 is the constant satisfying f(s)=0. Due to the existence of a nonlinear term and of the small interfacial length parameter \epsilon, designing of an accurate, efficient, and stable numerical approximation is not always easy task. A numerical investigation of the Allen-Cahn equations has been studied by using different numerical techniques in the spatial domain, such as finite element methods in [9], discontinuous Galerkin methods in [10, 11], and in the temporal domain, such as the implicit-explicit (IMEX) techniques [12, 13], the average vector field (AVF) method [11, 14], and the splitting methods [15]. The aforementioned numerical methods can be accurate up to the given threshold but they have some restrictions such as mesh dependence, high computational burden for the nonlinear partial differential equations (PDEs). On the other hand, in recent years, the usage of deep neural networks (DNNs) has lead to a significant achievement in the several areas, such as visual recognition [16], cognitive science [17] as well as solving differential equations [18, 19, 20, 21, 22, 23, 24]. Among them, Physics-Informed Neural Network (PINN) introduced by Raissi et al. in [22] has received great attention thanks to its flexibility in tackling a wide range of forward and inverse problems involving PDEs. In this structure, weights and biases of the neural network model are optimized according to the loss function, containing the physics of the underlying problem, which are basically governing equations, boundary conditions, and initial condition. Up to now, the PINN has been used to solve different types of problems in computational science and engineering, such as inverse problems [25], fluid dynamics [26, 27], parameter estimation [28], topology optimization [29], fractional PDEs [30], and stochastic PDEs [31, 32]. In addition, different variants of the standard PINN have been proposed to increase the performance of PINN, such as meta-learning [33], gradient-enhanced [34], balance of weights [35], decomposition of spatial domain [36], adaptive sampling [37], adaptivity in temporal domain [38, 39], adaptive activation function [40], and enforcing boundary conditions [41]. Compared to the conventional methods, such as finite difference, finite element, deep learning approaches like PINN is mesh-free thanks to the automatic differentiation and can avoid the curse of dimensionality; see, e.g., [42]. However, for low dimensional PDEs, it is still not straightforward to claim that the computational accuracy obtained from DNNs is better than the ones obtained by the conventional methods; see, e.g., [43] for more discussion. Although the standard PINN (std-PINN) has been widely accepted and has yielded remarkable results across a range of problems in the computational science and engineering, it is not always capable of solving the Allen-Cahn equation, a typical example of phase field models, due to the sharp transition layers, evolution of the layers in time, and high sensitivity to the initial conditions. To develop the performance of PINN, the authors in [44] use two specially designed convolutional neural networks (CNNs) and the loss functions correspond to the full-discrete systems obtained from finite difference methods in both space and time. Similarly, the multi-step discrete time models with adaptive collocation strategy are considered in [45]. The authors in [38] involve the idea of adaptivity in both time and space by sampling the data points, while in [46], the same neural network is retrained over successive time segments, while satisfying the obtained solution for all previous time segments. In addition, the system of the Allen-Cahn equation (3) first reduced into a first-order problem and then the converted minimization problem is approximated by using a deep learning approach in [47]. A theoretical perspective for the propagation of PINN errors is also given in [48] for the Allen-Cahn equations. Unlike the aforementioned studies, we here propose a novel methodology based on preserving of the energy dissipation to predict the dynamics of the Allen-Cahn equation. The proposed network approach guarantees the decay of energy and also plays a key role in accurately learning the dynamics of the Allen-Cahn equation. Embedding of different conservation constraints, such as mass and momentum conservation, into the PINN architecture are also considered in [49, 50, 51] for different types of PDEs. Our specific contributions can be summarized as: • We propose a novel PINN approach based on preserving of the energy dissipation to learn the dynamics of the Allen-Cahn equation more accurately. • We offer a detailed set of benchmark examples to evaluate the performance of the proposed approach. These examples include logarithmic and polynomial free energy potentials, constant and degenerate mobility functions, deterministic and random initial functions, and advective term in one, two, and three spatial dimensions. The rest of this manuscript is outlined as follows: In next section, we briefly review the standard physics-informed neural network (std-PINN). In Section 3, we introduce our main contribution, which is the addition of energy dissipation constraint (6) into the loss function. Section 4 presents numerical strategies utilizing adaptive approaches to enhance the performance of the PINN and the numerical simulations of various benchmark problems. Last, we give some concluding remarks in Section 5 based on the findings in the paper."
https://arxiv.org/html/2411.08598v1,Space-local memory in generalized master equations: Reaching the thermodynamic limit for the cost of a small lattice simulation,"The exact quantum dynamics of lattice models can be computationally intensive, especially when aiming for large system sizes and extended simulation times necessary to converge transport coefficients. By leveraging finite memory times to access long-time dynamics using only short-time data, generalized master equations (GMEs) can offer a route to simulating the dynamics of lattice problems efficiently. However, such simulations are limited to small lattices whose dynamics exhibit finite-size artifacts that contaminate transport coefficient predictions. To address this problem, we introduce a novel approach that exploits finite memory in time and space to efficiently predict the many-body dynamics of dissipative lattice problems involving short-range interactions. This advance enables one to leverage the short-time dynamics of small lattices simulate arbitrarily large systems over long times. We demonstrate the strengths of this method by focusing on nonequilibrium polaron relaxation and transport in the dispersive Holstein model, successfully simulating lattice dynamics in one and two dimensions free from finite-size effects, reducing the computational expense of such simulations by multiple orders of magnitude. Our method is broadly applicable and provides an accurate and efficient means to investigate nonequilibrium relaxation with microscopic resolution over mesoscopic length and time scales that are relevant to experiment.","Lattice models play a key role in understanding physical and chemical phenomena. For instance, the Holstein Holstein (1959a, b) and Fröhlich Fröhlich (1954) models shed light on polaron formation and electrical transport in semiconductors Hulea et al. (2006); Fetherolf, Golež, and Berkelbach (2020), the Hubbard model Hubbard (1963) helps elucidate the mechanisms of high-temperature superconductivity Arrigoni, Fradkin, and Kivelson (2004), and the Ising model Ising (1925) is used to interrogate magnetism Newell and Montroll (1953) and phase transitions Dziarmaga (2005). However, while modern algorithms can efficiently simulate the quantum dynamics of small lattices over short times Tanimura and Kubo (1989); Makri and Makarov (1995); Wang, Thoss, and Miller (2001); Thoss, Wang, and Miller (2001); Suess, Eisfeld, and Strunz (2014); Wang (2015); De Vega and Bañuls (2015); Strathearn et al. (2018); Tamascelli et al. (2019); Schröder et al. (2019); Xie et al. (2019); Kloss, Reichman, and Tempelaar (2019); Makri (2021); Bose and Walters (2022); Gribben et al. (2022); Fux et al. (2023); Lacroix et al. (2024), reaching sufficiently large systems and long times to compare to experiments remains a fundamental challenge. is is because these methods often scale exponentially or, at best, polynomially with lattice size and simulation time, rendering the thermodynamic limit inaccessible. The severity of this limitation becomes clear when calculating dynamic properties, e.g., conductivities, viscosities, and diffusion constants, which are sensitive to finite-size effects Kikugawa, Nakano, and Ohara (2015); Simonnin et al. (2017); Cox and Geissler (2018); Jamali et al. (2018); Samanta, Ghosh, and Mohanty (2018); Bertini et al. (2021); Celebi et al. (2021); Cox and Geissler (2022). For example, finite-size effects can cause simulations to underestimate diffusion constants of polymers near the glass-transition Ray and Binder (1994), relaxation times of glass-forming liquids Berthier et al. (2012), the Curie temperature for Ni nanoparticles Dos Santos, Urbassek, and Bringa (2024), and the diffusion constant and viscosity of model fluids Yeh and Hummer (2004); overestimate the critical fermion-phonon coupling causing the metal-to-Peierls phase transition in a Holstein-Hubbard lattice Hébert et al. (2019); and yield apparently non-converging mobilities of dispersive Holstein polarons Bhattacharyya, Sayer, and Montoya-Castillo (2024a). These examples reveal the need of computing the dynamics of lattice models in thermodynamically large systems over long timescales. Generalized Master Equations (GMEs) have emerged as a powerful tool for reducing the computational cost of dynamical simulations Shi and Geva (2003, 2004); Zhang, Ka, and Geva (2006); Cohen and Rabani (2011); Cohen, Wilner, and Rabani (2013); Kelly and Markland (2013); Kidon, Wilner, and Rabani (2015); Kelly, Brackbill, and Markland (2015); Kelly et al. (2016); Montoya-Castillo and Reichman (2016, 2016); Pfalzgraff et al. (2019); Mulvihill et al. (2019); Ng, Limmer, and Rabani (2021); Mulvihill and Geva (2022); Amati et al. (2022); Lyu et al. (2023); Wang et al. (2023); Sayer and Montoya-Castillo (2023, 2024a, 2024b). GMEs are exact non-Markovian equations of motion for nonequilibrium averages, correlation functions, and even multi-time correlators of select variables that encapsulate the effects of an environment into a memory kernel Nakajima (1958); Zwanzig (1960); Mori (1965). In dissipative systems, the memory kernel decays to zero over a finite memory lifetime, which can be shorter than the relaxation time of the desired correlation function. Thus, in principle, one can use a reference simulation over the memory lifetime to construct a GME that predicts the dynamics of the desired correlation function to arbitrarily long times. This temporal truncation of memory at its lifetime can reduce the computational cost of simulating the quantum or classical dynamics of complex many-body systems in different problems, including charge transfer reactions in solution Pfalzgraff, Kelly, and Markland (2015); Liu, Mulvihill, and Geva (2024), protein folding Cao et al. (2020); Dominic et al. (2023); Cao et al. (2023), nonlinear spectroscopy Ivanov and Breuer (2015); Sayer and Montoya-Castillo (2024a), and transport Yan et al. (2019); Bhattacharyya, Sayer, and Montoya-Castillo (2024b). However, to construct a GME from a short-time reference simulation, it must satisfy two conditions: (1) the simulation time must span the memory kernel lifetime, and (2) the reference calculation must be performed in the same system whose dynamics one intends to interrogate with the GME. If one constructs a GME using a small lattice simulation, particles encounter the lattice boundaries and manifest finite-size effects: one reduces the cost but still obtains the wrong answer. Hence, one must be able to afford an admittedly short-time reference simulation, but of a thermodynamically large lattice. The poor scaling of dynamical methods with system size renders this calculation at best impractical and at worst impossible. Here, we propose a novel approach to lattice problems that exploits our observation that certain GME formulations display a finite spatial memory to motivate truncating memory in time and space. This allows us to employ short-time reference simulations of small lattices to generate the exact quantum dynamics of thermodynamically large lattices over arbitrarily long times for the cost of only the small reference calculation. We demonstrate the strengths of this method by applying it to nonequilibrium polaron formation and transport in dispersive Holstein lattices. Enabled by our space-local GME, we simulate, for the first time, the exact nonequilibrium quantum dynamics of small polaron formation, relaxation, and flow in thermodynamically large one-(1D) and two-dimensional (2D) lattices with up to 900 sites over 100 ps, free of finite-size contamination. Our method is model-agnostic and can be expected to enable the efficient investigation of nonequilibrium excitation dynamics in dissipative lattices displaying local interactions."
https://arxiv.org/html/2411.08210v1,BOSON: Understanding and Enabling Physically-Roust Phtonic erse Design with Adaptive Variation-Aware ubspace ptimization,"Nanophotonic device design aims to optimize photonic structures to meet specific requirements across various applications. Inverse design has unlocked non-intuitive, high-dimensional design spaces, enabling the discovery of compact, high-performance device topologies beyond traditional heuristic or analytic methods. The adjoint method, which calculates analytical gradients for all design variables using just two electromagnetic simulations, enables efficient navigation of this complex space. However, many inverse-designed structures, while numerically plausible, are difficult to fabricate and highly sensitive to physical variations, limiting their practical use. The discrete material distributions with numerous local-optimal structures also pose significant optimization challenges, often causing gradient-based methods to converge on suboptimal designs. In this work, we formulate inverse design as a fabrication-restricted, discrete, probabilistic optimization problem and introduce BOSON-1, an end-to-end, adaptive, variation-aware subspace optimization framework to address the challenges of manufacturability, robustness, and optimizability. We explicitly consider the fabrication process and differentiably optimize the design in the fabricable subspace. To overcome optimization difficulty, we propose dense target-enhanced gradient flows to mitigate misleading local optima and introduce a conditional subspace optimization strategy to create high-dimensional tunnels to escape local optima. Furthermore, we significantly reduce the prohibitive runtime associated with optimizing across exponential variation samples through an adaptive sampling-based robust optimization method, ensuring both efficiency and variation robustness. On three representative photonic device benchmarks, our proposed inverse design methodology BOSON-1 delivers fabricable structures and achieves the best convergence and performance under realistic variations, outperforming prior arts with 74.3% post-fabrication performance. We open-source our codes at link.","Integrated photonics has shown a wide range of applications in computing, communication, and sensing. Currently, many photonic devices are manually architected by tuning a few design parameters via inefficient trial and error, which relies heavily on expert knowledge and time-consuming simulations. In contrast, inverse design requires minimal physical prior knowledge and opens up non-intuitive, high-dimensional design spaces, making it possible to discover highly efficient and compact device designs [8, 13]. The adjoint method-based inverse design [8] is particularly powerful for its ability to compute analytical gradients of an objective with respect to high-dimensional design variables using only two simulations. While the adjoint inverse design can produce numerically plausible designs, a significant gap exists between pre-fab and post-fab performance. Figure 1: Inverse design often yields non-fabricable devices. Optimization difficulty leads to suboptimal designs. As illustrated in Fig. 1, the inverse-optimized design exhibits high performance, but the tiny structures within the design pattern are non-manufacturable, leading to severe post-fabrication performance degradation. Additionally, fabrication variations during lithography and etching, along with operational variations such as temperature drift, introduce robustness concerns that further compromise the performance of inverse-designed devices. Previous approaches have attempted to address manufacturability issues by controlling the minimum feature size (MFS) during optimization through heuristic methods, such as blurring or adding curvature penalties, to eliminate non-fabricable structures and mitigate post-fabrication performance drops [1, 12, 18, 9, 16, 19, 17, 23, 4, 6]. To enhance fabrication robustness, prior work models variations as uniform erosion and dilation of the device geometry and simultaneously optimizes objectives under different variation corner cases. However, this oversimplified method only marginally improves robustness as it fails to capture actual variations accurately. When considering multiple variation effects, exhaustive Monte Carlo sampling of all corner cases induces exponential simulation cost. Beyond fabricability, solving this discrete high-dimensional stochastic optimization problem is particularly challenging. As the adjoint optimizer is only driven by a single objective sparsely defined in the device output port, it leads to a poor objective landscape shown in Fig. 1, making the optimization highly sensitive to initialization and prone to getting trapped in unreasonable suboptimal solutions. To address the photonic device inverse design challenge, which requires both fabricability and robustness, we formulate this task as a fabrication-restricted, robust stochastic optimization problem. We propose a novel inverse design framework, BOSON-1, which enables effective optimization directly within the fabricable subspace with full variation awareness, ensuring efficient inverse design toward robust device structures. Our main contributions are as follows: \bullet We provide a comprehensive analysis of the fabricability and optimization challenges in inverse design and introduce BOSON-1, an adaptive, variation-aware inverse design framework for physically robust photonic devices. \bullet Fabrication-Aware Subspace Optimization: BOSON-1 integrates differentiable fabrication modeling into adjoint optimization to ensure devices in the fabricable subspace. \bullet Loss Landscape Reshaping: BOSON-1 largely reduces the specious local optima by introducing auxiliary dense objectives, enhancing gradient flow and improving optimization. \bullet Local Optima Escaping: We introduce a light-concentrated initialization method with conditional subspace relaxation, facilitating escaping local optima toward better solutions. \bullet Linear-Cost Variation-aware Optimization: We propose an adaptive variation optimization method based on novel axial corner sampling and worse-case optimization, reducing simulation costs from exponential to linear. \bullet On three photonic device benchmarks, our BOSON-1 achieves 74.3% performance enhancement on average compared to previous art, enabling variation-robust photonic inverse design with high efficiency."
https://arxiv.org/html/2411.08122v1,"Physics-Informed Neural Networks with Complementary Soft and Hard
Constraints for Solving Complex Boundary Navier-Stokes Equations","Soft- and hard-constrained Physics Informed Neural Networks (PINNs) have achieved great success in solving partial differential equations (PDEs). However, these methods still face great challenges when solving the Navier-Stokes equations (NSEs) with complex boundary conditions. To address these challenges, this paper introduces a novel complementary scheme combining soft and hard constraint PINN methods. The soft-constrained part is thus formulated to obtain the preliminary results with a lighter training burden, upon which refined results are then achieved using a more sophisticated hard-constrained mechanism with a primary network and a distance metric network. Specifically, the soft-constrained part focuses on boundary points, while the primary network emphasizes inner domain points, primarily through PDE loss. Additionally, the novel distance metric network is proposed to predict the power function of the distance from a point to the boundaries, which serves as the weighting factor for the first two components. This approach ensures accurate predictions for both boundary and inner domain areas. The effectiveness of the proposed method on the NSEs problem with complex boundary conditions is demonstrated by solving a 2D cylinder wake problem and a 2D blocked cavity flow with a segmented inlet problem, achieving significantly higher accuracy compared to traditional soft- and hard-constrained PINN approaches. Given PINN’s inherent advantages in solving the inverse and the large-scale problems, which are challenging for traditional computational fluid dynamics (CFD) methods, this approach holds promise for the inverse design of required flow fields by specifically-designed boundary conditions and the reconstruction of large-scale flow fields by adding a limited number of training input points.The code for our approach will be made publicly available.","Fluid mechanics is an important field in science and engineering that deals with the study of the motion of liquids and gases. The Navier-Stokes equations are the basic partial differential equations that describe the dynamic behavior of viscous fluids, which are highly nonlinear partial differential equations and are widely used in aerodynamics, meteorology, oceanography, and industrial process simulations (Munson et al. 2006). In most cases, its high degree of nonlinearity and complexity makes it difficult to obtain analytical solutions to equations, so almost study is based numerical solutions. Traditional numerical methods, such as finite difference methods (FDM), finite element methods (FEM), and finite volume methods (FVM), have made significant progress in solving the Navier-Stokes equations, commonly referred to as computational fluid dynamics (CFD) methods (McLay 1996). However, these techniques need the sufficient initial and boundary conditions while they are often difficult to obtain in practice(Abdel-Rahman 2011). Even if accurate mathematical systems of NSEs are obtained, they still often encounter challenges with grid generation for the high-dimensional and the complex boundary condition problems, which require substantial computational resources and time, and may lead to issues of numerical instability and loss of accuracy (McLay 1996). Deep learning (DL) (LeCun, Bengio, and Hinton 2015) methods have achieved great success in computer science, particularly deep neural networks (DNNs), which possess a universal function approximation property that renders them promising as CFD surrogate models. The main achievements of deep learning methods have been in the area of image recognition, which relies on being trained on large datasets (Krizhevsky, Sutskever, and Hinton 2017; He et al. 2016). In contrast, data from fluid systems are often sparse and noisy when obtained experimentally. Consequently, approaches that rely solely on data for learning, as in image recognition, are not viable for fluid models (Eivazi, Wang, and Vinuesa 2024; Sharma et al. 2023). Physical Information Neural Network (PINNs) is a type of deep learning approach that incorporates physical constraints by embedding physical equations and boundary conditions into the loss function of a neural network. PINNs were first proposed by Raissi et al. in 2019 (Raissi, Perdikaris, and Karniadakis 2019) for solving one-dimensional partial differential equation problems, such as the Burgers equation, as well as inverse problems for two-dimensional and three-dimensional partial differential equations with a certain amount of labeled data (Raissi, Perdikaris, and Karniadakis 2019; Raissi, Yazdani, and Karniadakis 2020; Karniadakis et al. 2021; Go, Lim, and Lee 2023). PINN offer significantly higher accuracy and efficiency compared to traditional CFD solvers when limited scattered partial spatio-temporal data are available for the flow problem under study. The structure of PINNs is inherently flexible, allowing the same formulation to be used for both forward and inverse problems. This eliminates the need for costly data assimilation techniques, which have historically slowed progress, especially in optimization and design tasks related to fluid dynamics (Cai et al. 2021). Furthermore, PINN offer a unified approach to handling flow problems that exhibit phenomena across different scales (Leung, Lin, and Zhang 2022), for which traditional CFD methods often require different models. However, for fluid models with complex boundary conditions, conventional PINN methods often struggle to accurately approximate both the boundary conditions and the partial differential equations (Hsieh and Huang 2024). Thus, the execution of boundary conditions is of paramount importance, and current methods for execute them in PINNs can be categorized into soft and hard constraints (Barschkis 2023; Lu et al. 2021). Conventional PINNs typically adopt a soft-constraint approach (Raissi, Perdikaris, and Karniadakis 2019; Lai et al. 2023), where boundary conditions and initial conditions are explicitly embedded into the loss function, which is trained simultaneously with the PDE loss. By introducing the laws of physics into the loss function, the stability of the training process can be improved, and the network can avoid learning solutions that violate the laws of physics, especially when data is scarce. This can help models generalize better to unseen data, particularly when they need to extrapolate beyond the range of the training data. However, this method often fails to guarantee the satisfaction of both the PDE and the boundary conditions, resulting in lower accuracy of the outcomes (Cuomo et al. 2022; Bai et al. 2022; Bischof and Kraus 2021). Hard-constraint methods (Lu et al. 2021) enforce boundary conditions by constructing a solution and a distance function that correspond to the boundary conditions, such that the network enforces the conditions during training by simply optimizing the partial differential equation loss, thereby avoiding the issue of tuning the weights of different parts of the loss function (Lan et al. 2023). But the hard-constrained PINN approach encounters certain limitations. The solutions it devises for boundary conditions are often contingent upon a DNN with a limited number of hidden layers, which can hinder their ability to accurately solve complex boundary conditions. During training, this DNN solely uses coordinate points at the boundaries as input to ensure that boundary conditions are met. However, when boundary conditions are complex, the output of the DNN can become highly disordered, leading to extremely slow or non-convergent loss during the training of partial differential equations. Ultimately, this may prevent the model from producing accurate variable outputs. This limitation reduces the network’s generative capabilities, especially in scenarios requiring higher flexibility or when dealing with intricate boundary conditions (Barschkis 2023; Deng et al. 2023). This paper constructs the solution of boundary conditions and the distance function specifically combine the soft- and the hard-constraint methods, enabling their application to various complex-boundary fluid dynamics problems. Its feasibility is demonstrated through testing on two fluid models under different boundary condition scenarios and comparing the results with those from CFD methods and traditional soft and hard constraint approaches. In summary, our work makes the following contributions: 1. We propose a physics-informed neural network approach that combines soft and hard constraints to solve flow fields in regions with irregular obstructing structures. 2. A power function is proposed to redefine the distance metric in the hard-constrained approach for handling complex boundary conditions, thereby improving prediction accuracy. 3. The proposed method is thoroughly validated through extensive experimentation on complex-boundary Navier-Stokes equations, such as the 2D cylinder wake problem and the 2D blocked cavity flow problem with a segmented inlet."
https://arxiv.org/html/2411.07900v2,Hybrid finite element implementation of two-potential constitutive modeling of dielectric elastomers,"Dielectric elastomers are increasingly studied for their potential in soft robotics, actuators, and haptic devices. Under time-dependent loading, they dissipate energy via viscous deformation and friction in electric polarization. However, most constitutive models and finite element (FE) implementations consider only mechanical dissipation because mechanical relaxation times are much larger than electric ones. Accounting for electric dissipation is crucial when dealing with alternating electric fields. Ghosh and Lopez-Pamies [17] proposed a fully coupled three-dimensional constitutive model for isotropic, incompressible dielectric elastomers. We critically investigate their numerical scheme for solving the initial boundary value problem (IBVP) describing the time-dependent behavior. We find that their fifth-order explicit Runge-Kutta time discretization may require excessively small or unphysical time steps for realistic simulations due to the stark contrast in mechanical and electric relaxation times. To address this, we present a stable implicit time-integration algorithm that overcomes these constraints. We implement this algorithm with a conforming FE discretization to solve the IBVP and present the mixed-FE formulation implemented in FEniCSx. We demonstrate that the scheme is robust, accurate, and capable of handling finite deformations, incompressibility, and general time-dependent loading. Finally, we validate our code against experimental data for VHB 4910 under complex time-dependent electromechanical loading, as studied by Hossain et al. [25].","Dielectric elastomers are soft materials that deform significantly when subjected to electric fields. These materials were first reported by Pelrine et al. [39] and have since garnered interest as electromechanical transducers for a wide variety of applications, such as robotics, biomedical engineering, and energy harvesting (check, for example, [26, 2, 44, 5, 10, 28, 7, 38, 56, 29]). To harness their full potential in engineering applications, robust constitutive models and computational schemes that can predict their complex electromechanical behavior under real-life loadings and stimuli are needed. Thus, work on electromechanical constitutive modeling has motivated research for decades [53, 34, 35, 16, 12]. However, most of these models are limited to elastic dielectrics, which are dielectric elastomers that deform and polarize without dissipating energy. This idealization is not true in general for real-life loadings and applications, as dielectric elastomers are inherently dissipative solids that exhibit energy loss through viscous deformation and friction in their electric polarization process. Recognizing the dissipative nature of dielectric elastomers, many models focusing on the mechanical dissipation have also been proposed, but the majority of such models account only for mechanical dissipation and save for a few works most assume ideal dielectric behavior [23, 55, 58]. This is because the electric relaxation time is much smaller than the mechanical relaxation time for most dielectric elastomers. However, accounting for electric dissipation in addition to mechanical dissipation becomes critical for several loading conditions such as in the presence of alternating electric potentials (check, for example, dielectric spectroscopy experiments on prestretched VHB 4910 specimen in [40]). In fact, the measured permittivity in such experiments becomes stretch dependent and was coined as apparent permittivity in [17]. Furthermore, an important use of dielectric elastomers is in composite material discovery in which dielectric elastomers, when filled with solid or liquid inclusions, lead to remarkable macroscopic or effective material properties [57, 18]. The overall dissipative nature of these composites could be far more complex or pronounced than the electric and mechanical dissipation of the constituent elastomer [19, 20, 47], thus highlighting the need for a constitutive model for the coupled electric and mechanical dissipative behavior of dielectric elastomers. A first attempt to propose such a comprehensive constitutive model was made in [17]. The proposed model works for a prominent class of isotropic and incompressible dielectric elastomers that exhibit : a) non-Gaussian elasticity, b) deformation enhanced shear thinning viscosity, c) electrostriction and d) time- and deformation-dependent polarization. At this point, some work that has gone into addressing some of the challenges in FE implementation of constitutive models of dielectric elastomers needs to be acknowledged. These implementations have addressed challenges such as large deformation, electromechanical coupling, and the incorporation of viscoelastic effects ([41], [22], see also [48] for an open-source implementation in FEniCSx) as well as efficient implementations of coupled-physics such as thermo-electro-viscoelasticity ([36]), magnetorheological elastomers ([15, 42]) and notably an efficient FE framework for coupled electromechanics [27]. To this end, the purpose of this paper is five-fold. Firstly, the ease of a robust FE implementation of the constitutive model proposed in [17] is demonstrated. Second, various time-integration schemes are examined along with a re-examination of the time-integration scheme proposed in [17]. Third, the FE framework is employed to describe the electromechanical behavior of the acrylate elastomer VHB 4910 and the results are compared with the experiments of Hossain et al. [25]. Fourth, it is demonstrated that such a FE framework is needed for finding material properties of the proposed model as boundary effects become important in such material characterization experiments invalidating the often made assumptions of biaxility/uniaxiality. This is because the sample geometry and the boundary conditions of the electro-mechanical experiments is such that the resulting electric and deformation fields are no longer homogeneous. This is also acknowledged in [37]. Finally, in a future work the FE solver will be used as a plug-in tool, within a larger framework that could involve using Machine Learning (ML), for data generation for automated constitutive model identification such as in [33] or coupling with Deep Learning tools such as Neural ODEs [50] for data-driven constitutive modeling modeling. The organization of the paper is as follows. In Section 2, the governing equations that take the form of an initial boundary value problem (IBVP) that explain the time-dependent dielectric response of a dielectric elastomer are outlined. The section begins with discussions on kinematics and the balance equations. Following this, the specific model for the prominent class of isotropic and incompressible dielectric elastomers as proposed in [17] is explained. In order to deal with nearly or fully incompressible elastomers, it is more convenient to reformulate the governing equations into a hybrid set of governing equations in which a pressure field (in addition to the deformation field and the electric potential) is an additional unknown field. This hybrid form of the governing equations are described next, followed by the weak form of the equations. Finally, Section 2 concludes with the time- and space- discretized forms of the IBVP that is to be implemented in a FE framework. Section 3 is devoted to the study of explicit and implicit time integration schemes for the solution of the IBVP in the 1D setting. Specifically, the overall accuracy, stable-time-increment, and total-time-to-solution (TTS) for each of the schemes are studied. We note that implicit schemes are much better at handling the disparate time-scales exhibited by the electric and mechanical dissipation processes in elastomers. Hence these are better suited for solving the time-dependent response of dielectric elastomers exhibiting both mechanical and electric dissipation. Section 4 is devoted to the hybrid FE element formulation developed in Section 2 along with an implicit time-stepping algorithm based on Backward Euler discretization of time to solve an initial boundary value problem that mimics the corresponding experiments in [25]. Finally, Section 5 summarizes the findings, and presents concluding remarks along with possible extensions of the work in the future."
https://arxiv.org/html/2411.07953v1,Quantitative Phase-Field Modeling of Rapid Alloy Solidification,"We further develop a recently introduced phase-field model of rapid alloy solidification [K. Ji et al., Phys. Rev. Lett. 130, 026203 (2023)]. This model utilizes enhanced solute diffusivity within the spatially diffuse interface region to quantitatively capture solute trapping with a larger interface width, thereby making simulations on experimentally relevant length and time scales computationally feasible. The main developments presented here include testing the robustness of different variational formulations, extending the model to concentrated alloys by incorporating solid and liquid free energies from thermodynamic databases, as illustrated for hypoeutectic Al-Ag alloys with CALPHAD, extending convergence tests as a function of interface width to 3D, and carrying out simulations in both 2D and 3D to examine existing theories of microstructure development. Our results indicate that the simplest variational formulation that interpolates the bulk free-energy density between its solid and liquid forms is the most robust. Remarkably, for hypoeutectic Al-Ag alloys, this formulation yields a high-velocity nonequilibrium phase diagram that is independent of interface width, thereby demonstrating that the framework of enhanced solute diffusivity framework can be non-trivially extended to concentrated alloys. Other variational formulations have restricted ranges of materials or processing parameters that can be reliably modeled. We use 2D simulations to construct high-velocity microstructure selection maps for dilute Al-Cu alloys. The results validate the important role of latent heat rejection at the interface and extend the limited predictions of linear stability analysis [A. Karma and A. Sarkissian, Phys. Rev. E 47, 513 (1993)] and sharp-interface 1D simulations to fully nonlinear regimes. Furthermore, 3D simulations, carried out using a computationally tractable axisymmetric cellular/dendritic interface shape, demonstrate a good convergence similar to that observed in 2D as a function of interface width. Full 3D simulations, in turn, reveal that the standard theory of absolute stability is a good predictor of the upper critical velocity beyond which steady-state growth becomes unstable, despite the different morphological manifestations of this instability in 2D and 3D.","Alloy microstructures develop during solidification processes that range from conventional casting to advanced additive manufacturing techniques, where the solid-liquid interface velocity V spans six orders of magnitude from micron/s to m/s [1, 2]. Understanding the formation and evolution of these microstructures is essential for controlling the properties of the manufactured alloys, and it requires multiscale computational modeling. The phase-field (PF) method has emerged as a major contributor for modeling and predicting mesoscale microstructures during solidification processes [3, 4, 5, 6, 7]. This method describes a two-phase (solid-liquid) system by a scalar PF that takes constant values in each of the bulk phases and varies smoothly across interfaces of a characteristic thickness W. This scalar field can be coupled to the local thermodynamic state variables, such as temperature and solute concentration, and avoids the need for explicit front tracking. A number of PF studies have successfully predicted and elucidated microstructure patterns at length scales \sim 100 \mum observed in experiments, predominantly under slow solidification conditions where the interface is close to thermodynamic equilibrium [8, 9, 10, 11, 12, 13, 14, 15]. Despite the physical solid-liquid interface thickness being around W_{0}\sim 1 nm—significantly smaller than the scale of microstructural patterns—PF simulations can be conducted at experimentally relevant length and times scales by upscaling the diffuse interface thickness [16, 17, 18, 19, 20, 21], such that W\gg W_{0}. Techniques like the anti-trapping current help eliminate spurious effects from this upscaling [17, 18], ensuring local thermodynamic equilibrium at the interface. As a result, quantitative agreements between PF simulations and well-controlled experiments can be established [10, 12, 15]. Yet, modeling conditions far from equilibrium, as seen in rapid solidification processes like additive manufacturing, introduces unique challenges, encompassing two main aspects detailed below. The first challenge lies in modeling an out-of-equilibrium interface with complex morphologies, where nonequilibrium effects become dominant during rapid solidification. As the solid-liquid interface propagates at an increased velocity, the solid phase will trap excess solute, resulting in a reduction of the peak concentration as illustrated in the schematics of Fig. 1(a). This phenomenon, known as solute trapping, occurs when the semi-liquid interface region fully crystallizes before solute partitioning is complete, where the former takes a time \sim W/V for a diffuse interface moving at V, and the latter is established on the characteristic time \sim W^{2}/D_{l} for solute atoms of diffusivity D_{l} (in the liquid) to diffuse across the interface [22]. When these two time scales become comparable, i.e., V is close to a diffusive speed V_{d}\equiv D_{l}/W, the interface dynamics is largely controlled by the nonequilibrium effects. This is especially true for the range of V approaching or exceeding the so-called absolute stability limit V_{a}, beyond which the solid-liquid interface is theoretically expected to become morphologically stable [23, 24], when the stabilizing effect of surface tension surpasses the destabilizing effect of the solutal diffusion field. Except for very dilute alloy concentrations [25, 26], a smooth transition from cellular-dendritic to planar front growth is not typically observed. Instead, “banded microstructures” form [one example given in Fig. 1(b)], which consist of an alternation between bands exhibiting a microsegregation pattern characteristic of dendritic array growth, and microsegregation-free bands characteristic of planar front growth [27, 28, 29, 30, 31, 32, 33, 34, 35, 36]. Linear stability analyses that incorporate nonequilibrium effects have predicted the existence of oscillatory modes of the planar interface [37, 38, 39]. Nonlinear oscillations of the planar interface have indeed been shown to exist, but are insufficient to fully describe banding. Approximate analytical models of banding have also been developed, but assume instantaneous transitions between steady-state dendritic and planar front growth [29, 30, 34]. They also neglect latent-heat rejection found to strongly affect planar front oscillations [40, 39]. While these studies have provided useful insights, a quantitative modeling approach is essential to address basic questions about dendritic array pattern stability and the underlying mechanisms of banding. To achieve this objective, the computational modeling needs to incorporate alloy phase diagram, relevant thermal conditions, energetic and kinetic anisotropies of a solid-liquid interface, and importantly, nonequilibrium solute trapping effects over a broad velocity range, extending beyond the absolute stability limit. Figure 1: (a) Schematic representations of one-dimensional concentration c(x) and PF \phi(x) profiles during rapid solidification of binary alloys, where the positive x directs to the right side of the figure. c_{\infty} is the nominal concentration away from the interface. The solid and dashed c(x) curves represent lower and higher solidification velocities, respectively. (b)-(c) Comparison of banded microstructures for Al-9 wt.% Cu from the late stage of a thin-film resolidification experiment [36] and a 2D PF simulation with thermal diffusion [41] (\tilde{c}=c/c_{\infty} color map). Reproduced with permissions. The second challenge revolves around carrying out simulations on experimentally relevant length and time scales. While PF models have been shown to reproduce solute trapping properties for a physical choice of interface thickness W_{0}\sim 1 nm in one dimension [42, 43, 44, 45, 46], two-dimensional (2D) and three-dimensional (3D) simulations at the microstructural pattern scale generally necessitate the choice of an increased interface thickness in the PF model, i.e., S\equiv W/W_{0}\gg 1. This selection, however, induces spurious excess trapping. Given that the diffusive speed is inversely related to the chosen interface thickness S through the relation V_{d}\sim D_{l}/W\sim V_{d}^{0}/S (where V_{d}^{0}\equiv D_{l}/W_{0} is a constant), the excess trapping induced by S\gg 1 can profoundly influence interface dynamics, especially when nonequilibrium effects become dominant with V\sim V_{d}. In the context of slow solidification, this complication has been circumvented by introducing an anti-trapping current [17, 18], which effectively eliminates excess solute trapping, thereby restoring local equilibrium at the interface. The form of this current has been modified to also account for a moderate departure from equilibrium below the absolute stability limit [47]. Yet, to describe far-from-equilibrium phenomena, such as banding, a more quantitative strategy is needed to consistently eliminate this excess trapping across a wide velocity range from low to high V. In our recent study [41], a PF model was developed to address the aforementioned dual challenges. This model incorporates well-known nonequilibrium effects, including solute trapping characterized by velocity-dependent forms of the partition coefficient k(V) and liquidus slope m(V), as well as interface kinetics. A computationally tractable choice of W is used to model the solidification of a dilute Al-Cu alloy under far-from-equilibrium conditions. To compensate for the excess solute trapping, the model adopts a novel approach by enhancing the solute diffusivity D(\phi)\equiv D_{l}q(\phi) within the spatially diffuse interface region. Simple forms of q(\phi) are used to reproduce the desired V-dependent forms of k(V) and m(V) across an extremely wide velocity range, from near [k(V)\to k_{e} where k_{e} is the equilibrium value of the partition coefficient] to far from [k(V)\to 1] equilibrium conditions. 2D simulations revealed a new burgeoning instability in dendrite tip growth, driven by solute trapping at velocities nearing the absolute stability limit. These simulations also reproduced the formation of the widely observed banded microstructures and predicated band spacings that agree quantitatively with observations in rapidly solidified Al-Cu thin films [36], as shown in Fig. 1(b)-(c). Furthermore, the bands in the PF simulation form by the same lateral spreading mechanism as observed in the experiment, with a lateral velocity that remarkably matches the experimental measurements [48]. Furthermore, this PF model has recently been used to study the formation of banded microstructures during laser powder-bed fusion of a Magnesium alloy [49]. The strategy of enhancing solute diffusivity to upscale interface thickness has also been implemented in a recent formulation [50] with finite interface dissipation [46] to simulate banded microstructures during rapid solidification. In this paper, we revisit the aforementioned quantitative PF model [41], hereafter referred to as Model A, and present a derivative Model B based on a similar variational framework. We derive the evolution equations of each model from a free-energy functional and perform the 1D asymptotic analysis, demonstrating that the solute trapping properties of both PF models match the predictions of the microscopic Continuous Growth (CG) model [51] in the large V limit. While Model A is derived from a phenomenological free-energy functional introduced in Ref. [43] for a dilute binary alloy, Model B is based on a free energy that interpolates between bulk phases at the interface, allowing for the modeling of both dilute and non-dilute alloys with the integration of free-energy functions from the Calculation of Phase Diagrams (CALPHAD). In addition, we introduce a revised PF model that separates solute diffusion in normal and tangential directions to study the effects of excess surface diffusion. Furthermore, we expand the modeling capabilities by coupling latent-heat diffusion to the PF simulation and extending the simulation to 3D. Numerical simulations in 2D produce microstructure selection maps for dilute Al-Cu alloys under different thermal conditions, including the standard frozen temperature approximation assuming a fixed homogeneous temperature gradient and the time-dependent calculation of thermal diffusion within an adiabatic zone. In the latter, the banded microstructure is selected at a higher composition, which is in good agreement with the results of linear stability analyses [40, 39], demonstrating the significant effects of latent-heat diffusion on banding. The transitions from cellular/dendritic to planar/banding with various alloy compositions are investigated through 2D PF simulations. The PF results with S=1 agree well with the absolute stability limit that is analytically predicted when the cellular/dendritic solution loses stability. The PF model is also extended to 3D, with both axisymmetric and full 3D simulations conducted at velocities close to the absolute stability limit. The latter reveal a tail instability developing in the grooves of dendrite arms, which resembles the corrugated roofs observed in 2D simulations but differs from the hemispherical caps seen in axisymmetric cases. Moreover, we address a numerical issue specific to Model A under certain combinations of alloy and modeling parameters. The free-energy density f_{AB}(\phi,T) in Model A encompasses contributions from both the pure substance and the solute addition, where \phi is the PF varying from +1 in the solid to -1 in the liquid, and T denotes temperature. A phenomenological form of f_{AB}(\phi,T) [43] is chosen in Model A, such that the stationary solution of the PF profile is exactly a hyperbolic tangent function. This is achieved by using different functions of \phi to interpolate the pure-substance and chemical parts of the free energy between the solid and liquid phases. In the dilute limit, this PF formulation is equivalent to the Kim-Kim-Suzuki model [52] that interpolates the concentration in the interface region between its solid and liquid values, and a thermodynamically-based formulation that interpolates the grand potential [20]. The choice of f_{AB}(\phi,T) in Model A, however, can lead to the emergence of an additional spurious minimum of the bulk free-energy density at values other than \phi=\pm 1 within the interval -1<\phi<1 , thereby causing unphysical simulation results. As presented in Sec. III.1, we distinguish between the“safe” and “unsafe” regions within the parameter space. In the safe region, Model A operates under any interface driving force, whereas in the unsafe region, certain driving forces might induce the development of unphysical phases. Typically, the unsafe region is characterized by a high nominal concentration c_{\infty}, a small k_{e}, and/or a large interface thickness S. Even for the case of S=1, a sufficiently small k_{e} can lead to the appearance of unphysical phases under certain driving forces, which holds true for both Model A, as discussed here, and the PF models in Refs. [52, 20]. Within the unsafe region, simulations using these models are still feasible as long as the driving force does not trigger unphysical phases, though extra caution is required. This numerical issue does not arise in Model B, which interpolates the free energy of the solid and liquid phases across a diffuse interface. It should be noted that Model B in its S=1 case is equivalent to the PF formulation introduced in Refs. [53, 54], where the interface is assumed to be in an intermediate state between the two bulk phases. Although the solution of Model B deviates from the hyperbolic tangent profile of the stationary PF solution, this model reproduces the quantitative k(V) and m(V) curves similar to that of Model A for dilute alloys and circumvents the problem of unphysical phases for any combination of alloy and modeling parameters. However, given that Model B introduces a higher energy barrier, it requires more stringent conditions for numerical stability and results in higher computational costs. Thus, we primarily utilize Model A for PF simulations of dilute alloys, resorting to Model B only when parameters fall into the unsafe region. The approach of interpolating the free energy of bulk phases at the interface in Model B also provides additional flexibility. This allows for the integration of Gibbs free energy as functions of temperature and alloy concentration for each phase, commonly used to accurately predict phase properties in CALPHAD. By applying these experimentally verified free-energy functions and enhancing interfacial diffusion for an upscaled interface, we investigate solute trapping for concentrated Al-Ag alloys. The nonequilibrium liquidus and solidus predicted by the PF model are nearly S-independent, enabling the quantitative modeling of rapid solidification of non-dilute binary alloys at experimentally relevant time and length scales. An example 2D PF simulation for a concentrated Al-Ag alloy is provided. The paper is organized as follows. In Sec. II, we first introduce the sharp-interface equations for a moving solid-liquid interface. Then, we present detailed derivations of PF models in Sec. III. Asymptotic analyses, approximate and full PF solutions of solute trapping are presented in Sec. IV. Numerical results in both 2D and 3D are discussed in Sec. V. Lastly, conclusions and perspectives are given in Sec. VI."
https://arxiv.org/html/2411.07704v1,A low-dissipation numerical method based on boundary variation diminishing principle for compressible gas-liquid two-phase flows with phase change on unstructured grid,"A low-dissipation numerical method for compressible gas-liquid two-phase flow with phase change on unstructured grids is proposed. The governing equations adopt the six-equation model. The non-conservative terms included in the volume fraction and total energy equations of the six-equation model are defined on cell boundaries using second-order accurate approximations and calculated without interpolating the spatial derivatives. To capture discontinuities such as contact discontinuities and gas-liquid interfaces with low dissipation, the MUSCL-THINC/QQ-BVD scheme, which combines the Monotone Upstream-centered Schemes for Conservation Laws (MUSCL) method and the THINC method with quadratic surface representation and Gaussian quadrature (THINC/QQ) method, is employed. The MUSCL method is one of the mainstream numerical solvers for compressible flows, achieving second-order accuracy for smooth solutions, but it introduces excessive numerical dissipation errors near discontinuous solutions. The THINC/QQ method uses a reconstruction function developed for interface capturing on unstructured grids, making use of a sigmoidal function with a quadratic surface. By combining these reconstruction functions according to the Boundary Variation Diminishing (BVD) principle, the MUSCL method is selected for smooth solutions, while the THINC/QQ method is chosen for discontinuous solutions, preserving the solution structure accurately. Several benchmark tests are solved, demonstrating that the MUSCL-THINC/QQ-BVD scheme not only captures contact discontinuities with low dissipation but also resolves dynamically generated gas-liquid interfaces due to phase changes clearly.","I First-level heading: The line break was forced via \\ This sample document demonstrates proper use of REVTeX 4.1 (and LaTeX 2ε) in manuscripts prepared for submission to AIP journals. Further information can be found in the documentation included in the distribution or available at http://authors.aip.org and in the documentation for REVTeX 4.1 itself. When commands are referred to in this example file, they are always shown with their required arguments, using normal TeX format. In this format, #1, #2, etc. stand for required author-supplied arguments to commands. For example, in \section{#1} the #1 stands for the title text of the author’s section heading, and in \title{#1} the #1 stands for the title text of the paper. Line breaks in section headings at all levels can be introduced using \\. A blank input line tells TeX that the paragraph has ended. I.1 Second-level heading: Formatting This file may be formatted in both the preprint (the default) and reprint styles; the latter format may be used to mimic final journal output. Either format may be used for submission purposes; however, for peer review and production, AIP will format the article using the preprint class option. Hence, it is essential that authors check that their manuscripts format acceptably under preprint. Manuscripts submitted to AIP that do not format correctly under the preprint option may be delayed in both the editorial and production processes. The widetext environment will make the text the width of the full page, as on page 9. (Note the use the \pageref{#1} to get the page number right automatically.) The width-changing commands only take effect in twocolumn formatting. It has no effect if preprint formatting is chosen instead. I.1.1 Third-level heading: Citations and Footnotes Citations in text refer to entries in the Bibliography; they use the commands \cite{#1} or \onlinecite{#1}. Because REVTeX uses the natbib package of Patrick Daly, its entire repertoire of commands are available in your document; see the natbib documentation for further details. The argument of \cite is a comma-separated list of keys; a key may consist of letters and numerals. By default, citations are numerical; feyn54 author-year citations are an option. To give a textual citation, use \onlinecite{#1}: (Refs. witten2001; epr; Bire82). REVTeX “collapses” lists of consecutive numerical citations when appropriate. REVTeX provides the ability to properly punctuate textual citations in author-year style; this facility works correctly with numerical citations only with natbib’s compress option turned off. To illustrate, we cite several together feyn54; witten2001; epr; Berman1983, and once again (Refs. epr; feyn54; Bire82; Berman1983). Note that, when numerical citations are used, the references were sorted into the same order they appear in the bibliography. A reference within the bibliography is specified with a \bibitem{#1} command, where the argument is the citation key mentioned above. \bibitem{#1} commands may be crafted by hand or, preferably, generated by using BibTeX. The AIP styles for REVTeX 4 include BibTeX style files aipnum.bst and aipauth.bst, appropriate for numbered and author-year bibliographies, respectively. REVTeX 4 will automatically choose the style appropriate for the document’s selected class options: the default is numerical, and you obtain the author-year style by specifying a class option of author-year. This sample file demonstrates a simple use of BibTeX via a \bibliography command referencing the aipsamp.bib file. Running BibTeX (in this case bibtex aipsamp) after the first pass of LaTeX produces the file aipsamp.bbl which contains the automatically formatted \bibitem commands (including extra markup information via \bibinfo commands). If not using BibTeX, the thebibiliography environment should be used instead. Fourth-level heading is run in. Footnotes are produced using the \footnote{#1} command. Numerical style citations put footnotes into the bibliography111Automatically placing footnotes into the bibliography requires using BibTeX to compile the bibliography.. Author-year and numerical author-year citation styles (each for its own reason) cannot use this method. Note: due to the method used to place footnotes in the bibliography, you must re-run BibTeX every time you change any of your document’s footnotes."
https://arxiv.org/html/2411.07701v1,Generating and analyzing small-size datasets to explore physical observables in quantum Ising systems,"We propose a detailed analysis of datasets generated from simulations of two-dimensional quantum spin systems using the quantum Ising model at absolute zero temperature. Our focus is on examining how fundamental physical properties, energy, magnetization, and entanglement entropy, evolve under varying external transverse magnetic fields and system sizes. From the Quantum Toolbox in Python (QuTiP), we simulate systems with 4, 8, and 16 spins arranged in square lattices, generating extensive datasets with 5000 samples per magnetic field value. The Hamiltonian operator incorporates quantum mechanical effects such as superposition and tunneling, challenging classical interpretations of spin states. We compute extended Pauli operators and construct the Hamiltonian to include spin-spin interactions and transverse field terms. Our analysis reveals that as the system size increases, fluctuations in energy and entanglement entropy become more evident, indicating lifted sensitivity to external perturbations and suggesting the onset of quantum phase transitions. Spin-spin correlation functions demonstrate that interactions are predominantly local, but larger systems exhibit more complex and fluctuating correlations. These findings provide valuable insights into the behavior of quantum spin systems and lay the groundwork for future machine learning applications aimed at predicting physical quantities and identifying phase transitions from a quantum perspective.","Predicting phase transitions involves understanding complex systems in which small changes in external conditions can lead to abrupt transformations in the properties of the system. Challenges include dealing with non-linear dynamics, critical fluctuations, and the need for accurate datasets to detect these transitions [1]. The Ising model is an important tool in physics for studying phase transitions, particularly in ferromagnetism. It helps in being aware of how local interactions can lead to collective behavior, acting as an analogy for neurons in machine learning models where local rules determine the overall behavior [2]. The study of phase transitions faces a significant evolution as it moves into the quantum mechanics framework [3, 4]; particularly through the famous two-dimensional (2D) quantum Ising model [5]. This expansion into quantum mechanics introduces novel paradigms for understanding phase behavior, transitions, and critical phenomena, especially where traditional local order parameters are absent [6]. Furthermore, several other models are used to investigate phase transitions in both classical and quantum viewpoints. For example, the Potts model [7] is a generalization of the Ising model and can be used to study phase transitions in systems with more than two possible states per site. It is useful in areas such as magnetism and biology, where interactions can assume multiple discrete states. In the Heisenberg model [8], spins can be oriented in any direction in a three-dimensional space, unlike in the Ising model, where spins are restricted to two directions. This model is fundamental for understanding magnetism in materials where spin-spin interactions are more complex and not restricted to parallel or antiparallel alignments. The Hubbard model [9] studies electronic systems in networks of atoms or ions, where the competition between the kinetic energy of electrons and their interactions can lead to several physical phenomena such as interaction-induced insulation and superconductivity. The XY model [10] considers spins aligned in a plane, capable of freely rotating within it. Such model investigates the Kosterlitz-Thouless phase transition [11], which occurs through the decoupling of vortices and antivortices, a topological phase transition without a change in the symmetry of the order parameter. Finally, the percolation model [12] scrutinizes the formation of random clusters and their ability to form a large network that covers the entire system. It is applied to both material science problems and in studies of complex networks and epidemiology, focusing on the dynamics of connectivity. However, the center of attention of our work is on analyzing datasets generated by simulating quantum spin systems with different numbers of spins at zero temperature, examining their energy, magnetization, and entanglement entropy under varying external magnetic fields. This is justified since we aim to prepare the most suitable datasets for our future machine learning works by making predictions of physical quantities and identifying phase transitions from the viewpoint of quantum systems, analogously to what we previously done in the classical perspective [2]. To achieve this, the Hamiltonian operator is fundamental in quantum mechanics to illustrate how the states and interactions of a system’s particles dictate its energy [13]. Thus, the quantum Ising model is our first choice for analyzing phase transitions in the quantum realm. Its theoretical expansion within statistical physics summarizes ferromagnetic behaviors in magnetic particle systems under quantum effects [5]. Recognized for its multidimensional applicability, this model goes beyond classical constraints, making it indispensable for phase transition studies [14, 15, 16]. Represented by the Hamiltonian operator \hat{H}=-J\sum_{\langle i,j\rangle\in L}\hat{\sigma}_{i}^{z}\hat{\sigma}_{j}^{% z}-h\sum_{i\in L}\hat{\sigma}_{i}^{x}, (1) where J is the uniform coupling constant between spins, h is the uniform external transverse magnetic field, \hat{\sigma}_{i}^{x} and \hat{\sigma}_{i}^{z} are the Pauli matrices acting on site i, and the sum \langle i,j\rangle runs over nearest-neighbor pairs in the lattice L. As mentioned earlier, this system introduces quantum superpositions and tunneling effects, challenging classical parallel and antiparallel states through quantum mechanical rules. The inclusion of the Pauli matrix X, \hat{\sigma}_{i}^{x}, represents quantum state flips, further complicating the system’s dynamics at lower temperatures, where quantum phenomena are more evident [17, 18, 19]. Here, we employ quantum mechanical simulations using the Quantum Toolbox in Python (QuTiP) [20, 21] to generate our datasets. Our primary goal is to analyze the energy, magnetization, and entanglement entropy of these systems and to understand how these properties evolve and stabilize as the system size increases. These datasets provide perceptions into the stability and behavior of the quantum Ising model with different spin configurations. We fix the coupling coefficient J as 1.0 (in arbitrary energy units), and work with a range of external magnetic field strengths h from 1.0 to 5.0 (in the same energy units), at absolute zero temperature, i.e., considering exclusively quantum effects without thermal fluctuations. Each simulation runs for 5000 samples per value of the external magnetic field, ensuring a good exploration of the state space. This paper is organized as follows. In Sec. I, we introduce the background and motivation for studying quantum phase transitions using the two-dimensional quantum Ising model. In Sec. II, we describe the data generation process, including the simulation of quantum spin systems with different numbers of spins and the methods used to compute energy, magnetization, and entanglement entropy. In Sec. III, we analyze the results, discussing how these physical quantities evolve with varying external magnetic fields and system sizes. Finally, Sec. IV concludes the manuscript, outlining our findings and offering potential directions for future research, particularly the application of machine learning algorithms to predict physical quantities and identify phase transitions from a quantum perspective."
https://arxiv.org/html/2411.07615v1,Real-time propagation of adaptive sampling selected configuration interaction wave function,"We have developed a new time propagation method, time-dependent adaptive sampling configuration interaction (TD-ASCI), to describe the dynamics of a strongly correlated system. We employ the short iterative Lanczos (SIL) method as the time-integrator, which provides a unitary, norm-conserving, and stable long-time propagation scheme. We used the TD-ASCI method to evaluate the time-domain correlation functions of molecular systems. The accuracy of the correlation function was assessed by Fourier transforming (FT) into the frequency domain to compute the dipole-allowed absorption spectra. The FT has been carried out with a short-time signal of the correlation function to reduce the computation time, using an efficient alternative FT scheme based on the ESPRIT signal processing algorithm. We have applied the TD-ASCI method to prototypical strongly correlated molecular systems and compared the absorption spectra to spectra evaluated using the equation of motion coupled cluster (EOMCC) method with a truncation at single-doubles-triples (SDT) level.","An explicit solution of the time-dependent Schrödinger equation opens up numerous new possibilities for studying electron dynamics in many-body systems. This includes ultrafast charge and energy migration [1, 2, 3], as well as various spectroscopic techniques, such as photoionization [4, 5], X-ray absorption [6, 7, 8], and valence electron UV/vis spectroscopy [9, 10]. In spectroscopic methods, electronic responses are typically studied in the presence of a time-dependent external electromagnetic field, from which molecular information about energy eigenstates is extracted. While frequency-domain approaches, commonly used in the quantum chemistry community, can also extract such information, these face several challenges: the computational method scales linearly with the number of requested roots, and finding interior roots is often difficult [11, 12]. In contrast, time-domain methods allow us to extract spectral information over a broad frequency range from the resulting signals. We focus here on evaluating one such signal, time-dependent correlation functions, specifically the dipole-dipole autocorrelation function, from which a system’s linear absorption spectrum can be obtained. Just as for time-independent electronic structure theory, in time-domain methods, the underlying description of electron correlation plays a crucial role. The choice of which method is used for this determines: a) whether the time evolution operator will be unitary so that the wave function norm is conserved during long-time evolution, and b) whether the strongly correlated nature of the ground state and the propagated wave function is adequately captured. Amongst the high-accuracy electronic structure methods, configuration interaction (CI) [13, 14] and coupled cluster (CC) [15, 16] wave functions have been used for time propagation in recent works. However, when low excitation rank truncation is employed, both CI-based and CC-based time propagation fail in the strongly correlated regime. Additionally, apart from the simple CIS method, CI-based time propagation is unreliable as the system size is increased, while CC-based time propagation fails to maintain unitarity, particularly when the wave function becomes strongly correlated. Another time propagation method, the time-dependent density matrix renormalization group (TD-DMRG) [17], has achieved greater success than the previously mentioned methods. Nevertheless, because the area law of entanglement, which underlies the efficiency of DMRG ground state calculations, often does not hold for time-propagated wave functions, the success of time-dependent DMRG is somewhat limited compared to time-independent frameworks [18, 19]. These experiences motivate the development of an accurate and efficient time propagation method for a strongly correlated wave function. In this work, we explore the time propagation of another wave function, namely the adaptive sampling configuration interaction (ASCI) wavefunction [20, 21], which can accurately describe strongly correlated ground state wave functions. We refer to this method as TD-ASCI. We shall develop the TD-ASCI methodology that provides unitary electronic dynamics and will explore its ability to describe strong correlation of molecular systems during time propagation. In this first work we have selected several prototypical strongly correlated molecular systems for detailed study, to benchmark the method and analyze its numerical features. In the future, we shall also address extensions and broader applications of the method to reduced dynamics for large numbers of electrons and to open quantum systems dynamics in which electronic excitations are coupled to non-Markovian vibrational degrees of freedom. The choice of numerical integration technique for time propagation is of vital importance for this work. The widely used fourth-order Runge-Kutta (RK4) does not obey the symplectic nature of the time-propagation. Various other schemes, in particular, the split operator (SO) technique [22] and the second order difference (SOD) method [23] employed with TD-CC and TD-CI methods also suffer from the same issue, especially for long-time propagation. Therefore in this work, we shall employ the short iterative Lanczos (SIL) time-integration scheme first proposed by Park and Light [24]. The SIL procedure provides a unitary, symplectic time-integration scheme which can be applied for long-time dynamics with a suitable update of the Krylov subspace. A related time-integration scheme is the Chebyshev orthogonal polynomial-based integrator (CH) [25], which provides a compact global propagator. The accuracy and computational effort of the CH scheme is comparable with that of the method. But, in contrast, the CH scheme is not unitary, is valid only for Hermitian Hamiltonians, and is limited to time-independent Hamiltonians. Despite the advantages of the time-domain methods mentioned above, application of all of these to obtaining high-resolution absorption spectra is limited by the fact that the simulated signal requires a long-time evolution. Many efforts have been made to obtain high-resolution spectra from short-time signals. These include methods such as the filter diagonalization technique [26], the Padé-based [27] FT, and the machine learning(ML)-based [28] FT. In this work, we proposed a new technique to obtain spectra from short-time signals that is inspired by the signal processing technique ESPRIT [29], which has been found to be successful in reducing the circuit depth of quantum phase estimation (QPE) algorithms [30]. The rest of the manuscript is organized as follows. In Section II we first summarize the ASCI algorithm to prepare the ground state. In Section III we outline our implementation of the Lanczos-based time evolution algorithm, with a particular emphasis on how the long-time dynamics is carried out. In Section IV we present the algorithmic details required for evaluation of the dipole autocorrelation function. In Section VI we present the novel FT scheme of the short-time signal based on ESPRIT. Section VIII then presents simulations of the absorption spectra of several multireference molecular systems that demonstrate the efficiency of the dynamics scheme and of the novel FT scheme. Section IX summarizes and concludes with an outlook for future applications."
https://arxiv.org/html/2411.07565v1,"Parallel Multi-Coordinate Descent Methods for Full
Configuration Interaction","We develop a multi-threaded parallel coordinate descent full configuration interaction algorithm (mCDFCI), for the electronic structure ground-state calculation in the configuration interaction framework. The FCI problem is reformulated as an unconstrained minimization problem, and tackled by a modified block coordinate descent method with a deterministic compression strategy. mCDFCI is designed to prioritize determinants based on their importance, with block updates enabling efficient parallelization on shared-memory, multi-core computing infrastructure. We demonstrate the efficiency of the algorithm by computing an accurate benchmark energy for the chromium dimer in the Ahlrichs SV basis (48e, 42o), which explicitly includes 2.07\times 10^{9} variational determinants. We also provide the binding curve of the nitrogen dimer under the cc-pVQZ basis set (14e, 110o). Benchmarks show up to 79.3\% parallel efficiency on 128 cores.","Understanding the chemical properties of molecules relies on solving the many-body time-independent electronic Schrödinger equation. However, traditional methods, such as density functional theory (DFT) or coupled-cluster with single, double, and perturbative triple excitations (CCSD(T)), often struggle to accurately describe the electronic structure of strongly correlated systems. This limitation is particularly evident in molecules with transition metals or those in non-equilibrium geometries. Full configuration interaction (FCI) provides a numerically exact solution under a predefined basis set by describing the wavefunction as a superposition of all possible Slater determinants. However, FCI methods scale exponentially with the number of orbitals and electrons, leading to the curse of dimensionality. To leverage this challenge and apply FCI methods to large systems, it becomes necessary to compress the wavefunction. This can be achieved by employing different wavefunction ansatze, such as the matrix product state (MPS) in the density matrix renormalization group (DMRG) method 1, 2, 3, 4, 5, or by representing the wavefunction as a population of random particles, as in the full configuration interaction quantum Monte Carlo (FCIQMC) method 6, 7, 8, 9, 10. Another approach involves selecting important Slater determinants, guided by the extensive sparsity of the FCI wavefunction 11. The method we describe in this paper falls into this category, which is known as selected CI method. A variety of selected CI methods have been developed, starting from the earliest work in 1973 known as Configuration Interaction using a Perturbative Selection done Iteratively (CIPSI) 12, to recent advancements including Adaptive Sampling CI (ASCI) 13, Heat-bath CI (HCI) 14, Semistochastic Heat-bath CI (SHCI) 15, 16, Coordinate Descent FCI (CDFCI) 17, Fast Randomized Iteration method for FCI (FCI-FRI) 18, Reinforcement Learning CI (RLCI) 19, and others. These methods share the common iterative approach of expanding a primary configuration space, filtering determinants based on some importance estimate, and computing the leading eigenpair to obtain an enhanced approximation of the wavefunction until convergence is achieved. Typically, Epstein–Nesbet second-order perturbation theory is further employed in the secondary space to account for the remaining correlation that is not captured by the variational SCI treatment. Selected CI variants significantly reduce the computational cost of FCI by diminishing the dimension of the primary SCI space compared to the N-electron Hilbert space, while they differ by having distinct selection principles and implementations. This paper extends the coordinate descent FCI (CDFCI)17 method previously proposed by one of our authors and his collaborators, which provides selection rules from an optimization perspective. Initially, it transforms the FCI eigenvalue problem into an unconstrained minimization problem, with local minima corresponding to the ground state of the system. Next, it employs the coordinate descent method for the following advantages: (i) The gradient of the objective function provides a natural determinant selection rule, adding important determinants into the variational space until it reaches the memory limit; (ii) The special structure of the problem allows us to perform an exact line search, accelerating the energy convergence; (iii) In each iteration, updating only one coordinate of the optimization vector involves only one column of the Hamiltonian matrix, avoiding operations with the entire Hamiltonian matrix, thus reducing the computation cost associated with unappreciative determinants. The CDFCI method obtains the ground-state energy and wavefunction without explicitly extracting the Hamiltonian submatrix for direct diagonalization. This makes immense room for the storage of the wavefunction, making it possible for larger systems and more accurate approximations. The effective determinant selection rule and the low storage cost are the main reasons why CDFCI becomes a competitive FCI solver. Although CDFCI demonstrates accelerated performance in experiments, its parallelization capability is restricted by the inherent sequential nature of the method. In this paper, we present a novel algorithm to address the minimization problem, which extends update of one coordinate to multiple coordinates per iteration. To achieve this, the algorithm introduces an additional search dimension to enable the exact line search. This extension not only accelerates convergence but also opens up new possibilities for parallelization. Benefiting from fully parallelizable coordinate updates, our new algorithm achieves an accuracy of 10^{-5} Ha for \chC2 and \chN2 using the cc-pVDZ basis in 10 and 30 minutes respectively, nearly twenty times faster than the single-threaded version reported in the original CDFCI work. When compared to the multi-threaded version of the original CDFCI that supports parallel hashtable updates, our algorithm delivers a 3.0\times speedup. Additionally, it computes the ground state of all-electron \chCr2 with Ahlrichs SV basis in 5.8 days, matching the accuracy of the original CDFCI, which previously required one month for the same task. In the rest of this paper, we present the algorithm in section 2 and discuss the implementation details in section 3. In section 4, we demonstrate the accuracy and the parallel efficiency of our method by applying it to various molecules including \chC2, \chN2 and \chCr2. The binding curve of \chN2 under cc-pVQZ basis is also characterized. Finally, we conclude and look ahead to future work in section 5."
https://arxiv.org/html/2411.07422v2,Impact of Numerical Fluxes on High Order Semidiscrete WENO–DeC Finite Volume Schemes,"The numerical flux determines the performance of numerical methods for solving hyperbolic partial differential equations (PDEs). In this work, we compare a selection of 8 numerical fluxes in the framework of nonlinear semidiscrete finite volume (FV) schemes, based on Weighted Essentially Non–Oscillatory (WENO) spatial reconstruction and Deferred Correction (DeC) time discretization. The methodology is implemented and systematically assessed for order of accuracy in space and time up to seven. The numerical fluxes selected in the present study represent the two existing classes of fluxes, namely centred and upwind. Centred fluxes do not explicitly use wave propagation information, while, upwind fluxes do so from the solution of the Riemann problem via a wave model containing A waves. Upwind fluxes include two subclasses: complete and incomplete fluxes. For complete upwind fluxes, A=E, where E is the number of characteristic fields in the exact problem. For incomplete upwind ones, A<E. Our study is conducted for the one– and two–dimensional Euler equations, for which we consider the following numerical fluxes: Lax–Friedrichs (LxF), First–Order Centred (FORCE), Rusanov (Rus), Harten–Lax–van Leer (HLL), Central–Upwind (CU), Low–Dissipation Central–Upwind (LDCU), HLLC, and the flux computed through the exact Riemann solver (Ex.RS).We find that the numerical flux has an effect on the performance of the methods. The magnitude of the effect depends on the type of numerical flux and on the order of accuracy of the scheme. It also depends on the type of problem; that is, whether the solution is smooth or discontinuous, whether discontinuities are linear or nonlinear, whether linear discontinuities are fast– or slowly–moving, and whether the solution is evolved for short or long time. For the special case of smooth solutions, the expected convergence rates are attained for all fluxes and all orders. However, errors are still larger for the simpler fluxes, though differences diminish as the order of accuracy increases. For all selected cases involving discontinuities, differences among fluxes arise for all orders of accuracy considered. Moreover, there are flow situations for which the differences are huge, independently of the order of accuracy of the scheme. The best fluxes are the complete upwind ones. The difference between the best centred flux, FORCE, and incomplete upwind ones is not dramatic, which constitutes and advantage for good centred methods due to their simplicity and generality.","The vast majority of numerical schemes for solving hyperbolic partial differential equations (PDEs) is based on a discrete representation of their underlying principle: the rate of change in time of some quantities inside a given spatial region is given by what crosses the surface of the same region, expressed by the (normal) flux function, plus what is generated/dissipated within the region, expressed by the source function (if present). Therefore, a numerical method will require corresponding expressions for the numerical flux and the numerical source. Historically, Godunov [Godunov] is credited for having proposed a numerical flux as an integral average of the physical flux evaluated at the solution of the Riemann problem at the interface between the elements of a tessellation of the spatial domain. The resulting Godunov upwind method is a conservative generalization of the CIR scheme, first presented in [courant1952solution] by Courant, Isaacson and Rees. Even after six decades, the design of numerical fluxes remains a fundamental task in the construction of finite volume (FV) [hirsch2007numerical, ToroBook, leveque2002finite, godlewski2021numerical, toro2024computational], finite difference [leveque2007finite] and Discontinuous Galerkin (DG) finite element methods [reed1973triangular, cockburn2000development, cockburn2001runge]. This is so, both in the frameworks of semidiscrete and fully–discrete schemes. Apart from the basic properties of consistency and Lipschitz–continuity, the scheme designer aims for generous stability properties, monotonicity (for the scalar case), minimal numerical diffusion and efficiency [ToroBook]. It is known that for the scalar case the Godunov upwind method is the scheme with the smallest local truncation error, within the class of monotone schemes. The largely pending challenge is to design numerical fluxes with desirable properties for solving nonlinear systems in multiple space dimensions. Over the last few decades, many numerical fluxes with different properties have been put forward, which then prompts a relevant and natural question: among the available fluxes in the current literature, which ones achieve the optimal balance between accuracy and computational cost? At the first order level the current literature provides useful, even if not exhaustive, information on the performance of various numerical fluxes [hirsch2007numerical, ToroBook, leveque2002finite, godlewski2021numerical, toro2024computational]. Much less is known in the setting of higher order (in space and time) numerical methods. A reasonable expectation is that the adoption of a high order discretization could compensate for the deficiency of less accurate numerical fluxes. Despite being the numerical flux a crucial element, the design of numerical schemes for hyperbolic PDEs requires the definition of other important components which depend on the specific discretization framework adopted. Broadly speaking, there are two major discretization frameworks, namely the semidiscrete (or method of lines) and the fully–discrete approaches. In the former setting, the discretization in space is separated from the one in time. Given a discretization in space, the problem remains continuous in time as a system of ordinary differential equations (ODEs). In principle, any ODEs solver can then be deployed to complete the scheme. Instead, in the fully–discrete setting, the discretizations in space and time are intertwined, fully coupled and simultaneously designed in a single step. In the high order semidiscrete FV framework, one needs a suitable spatial reconstruction and a suitable time–stepping strategy. The spatial reconstruction must be nonlinear, so as to prevent or reduce spurious oscillations in the vicinity of discontinuities or large gradients (even in smooth problems). The need for the nonlinear character of the spatial reconstruction emerges from Godunov’s theorem [Godunov] as a necessary condition for a monotone scheme; see [ToroBook] for statement and proof. In this paper, we study the performance of 8 existing numerical fluxes in the setting of an arbitrary high order semidiscrete FV framework, comprising Weighted Essentially Non–Oscillatory (WENO) [liu1994weighted, shu1998essentially, shu1989efficient] spatial reconstruction and Deferred Correction (DeC) [micalizzi2023new, ciallella2022arbitrary, Decremi, minion2003semi, Decoriginal] time discretization. More in detail, we carry out a systematic comparison of the following numerical fluxes: Lax–Friedrichs (LxF) [lax1954weak], First–Order Centred (FORCE) [Toro1996, toro2000centred, chen2003centred], Rusanov [Rusanov1961] (Rus), Harten–Lax–van Leer (HLL) [harten1983upstream], Central–Upwind (CU) [kurganov2001semidiscrete, kurganov2000new], Low–Dissipation Central–Upwind (LDCU) [kurganov2023new], HLLC [toro1992restoration, toro1994restoration], the Godunov flux from the exact Riemann solver [Godunov] (Ex.RS). Most of these are thoroughly described in [ToroBook]. Both the spatial and time discretizations adopted here are well established. As a matter of fact, the WENO–DeC approach has already been investigated up to order 5 in space and time; the performance of such a framework has been shown to be very satisfactory in tackling challenging and realistic problems [ciallella2022arbitrary, ciallella2023arbitrary, ciallella2024high]. In the present work, we investigate the performance of WENO–DeC schemes up to order of accuracy 7 in space and time. Moreover, in addition to the very high order extension, we compare the performance of many numerical fluxes available in the literature within such a framework. The assessment of the resulting methods is through the time–dependent, one– and two–dimensional compressible Euler equations. A judicious choice of suitable problems is performed with the aim of understanding the strengths and limitations of the numerical fluxes under investigation in combination with the aforementioned semidiscrete approach. Key features of the chosen problems include: robustness in the presence of very strong shocks; accuracy in resolving waves associated with intermediate, linear, characteristic fields; and long–time evolution. From the systematic assessment of the methods following the above criteria, we anticipate the following conclusions: • There are several physical situations in which significant differences are seen in the performance of the numerical fluxes under investigation, with HLLC and Ex.RS outperforming by far the other competitors. • There are tests for which the choice of the numerical flux has less impact on the methods performance. But even in such cases, it can consistently be observed that the performance of LxF, FORCE and Rus is inferior to that of the remaining numerical fluxes, which give similar results amongst themselves. In particular, the performance of LxF is always the worst, while the relative performance of FORCE and Rus depends on the specific problem. • For all investigated numerical fluxes, there is an advantage in increasing the space–time order of accuracy of the discretization. The advantage is much more evident in the more diffusive numerical fluxes, that is the centred fluxes LxF and FORCE, and the incomplete upwind numerical flux Rus. The benefits of the enhanced higher order accuracy are less evident as the sophistication of the numerical flux increases. As a matter of fact, to a certain extent and depending on the test problem, increasing the space–time order of accuracy compensates for the deficiencies of a more diffusive numerical flux. However, enhanced higher order accuracy per se, within the range of considered orders, is not sufficient to attain the accuracy delivered by sophisticated upwind fluxes derived from complete Riemann solvers, namely HLLC and Ex.RS. Conversely, depending on the physical situation, a suitable numerical flux, even for a first order method, may be equivalent to implementing higher order space–time discretizations. This is typically the case for slowly–moving linear waves associated with intermediate characteristic fields and for very long–time evolutions of traveling waves. • A surprising outcome has emerged from the implementation of the centred (essentially one-dimensional) numerical fluxes LxF and FORCE in a two–dimensional setting via a simultaneous updating formula. A von Neumann stability analysis of these numerical fluxes in a first order setting shows them to be linearly unstable in two and three space dimensions [toro2000centred, ToroBook]. Curiously, we found that increasing the order of accuracy has a stabilizing effect, as shown in some of our numerical experiments, though an explanation remains illusive. There are other works in the literature concerned with the influence of numerical fluxes on high order methods. Investigations in the semidiscrete framework include [leidi2024performance, qiu2007numerical, qiu2008development, qiu2006numerical, hongxia2020numerical, san2015evaluation]. In [leidi2024performance], the performances of Rus, HLL and a low–dissipation version of HLLC (referred to as “LHCLL” in the reference) are compared for low Mach number flows, in a FV setting with various spatial reconstructions up to order 7. DG schemes with 8 numerical fluxes and spatial accuracy up to order 3 are studied in [qiu2007numerical, qiu2008development], and with 9 numerical fluxes and spatial accuracy up to order 4 in [qiu2006numerical]. In [hongxia2020numerical], in a FV setting with fifth order HWENO space reconstruction the performance of 8 numerical fluxes is assessed. In [san2015evaluation], results from 6 numerical fluxes in a FV framework with WENO space reconstruction up to order 7 are compared on the Kelvin–Helmholtz instability problem. In all previously alluded comparative analyses, a semidiscrete approach with third order time integration was adopted. Two further related works [titarev2005weno, toro2005tvd] are worth mentioning, in which the basic monotone flux utilized in the high order methods is replaced by a total variation diminishing (TVD) flux: in [titarev2005weno] the approach is implemented in a semidiscrete framework, while in [toro2005tvd] this is done in a fully–discrete framework. In the context of the present work concerned with semidiscrete methods, it is important to remark that a broadly adopted practice consists in employing high order space discretizations along with lower order time discretizations. See for example [evstigneev2016construction, Evstigneev2016OnTC, gerolymos2009very, balsara2000monotonicity, shi2003resolution, hermes2012linear, gao2020seventh], in which very high order spatial reconstructions are considered but the order of accuracy of the selected time discretizations never exceeds 4. Many published works consider very high order space discretizations in combination with strong stability preserving (SSP) [shu1988total, shu1988efficient] or linearly strong stability preserving (\ell\text{SSP}) [gottlieb2001strong] Runge–Kutta (RK) in time. The accuracy barrier for such ODEs solver, if non-negative RK coefficients are to be preserved [shu1988efficient], is order 4 [ruuth2002two] on nonlinear problems. Actually, \ell\text{SSP} RK methods can be arbitrarily high order accurate but only on linear problems. With the main goal of preventing loss in accuracy due to the mismatch between temporal and spatial order, in some works, a well–tuned reduction of the time step is performed. The main benefit of this strategy is to formally make the accuracy of the scheme equal to the one of the space discretization, however, the severely reduced time step causes excessive numerical diffusion and a huge increase in computational cost, making the scheme unsuitable for practical applications. In many other works, no adaptation of the time step is considered and the formal order of the scheme is limited by the one of the lower order time discretization. Such a practice is based on the questionable assumption that the spatial error always dominates the time error. Preliminary investigations performed by the authors seem to contradict this expectation, though a thorough study of this issue is left for future works. In fact, as already stated, the WENO–DeC framework adopted in this paper allows for the construction of arbitrarily high order schemes, which are distinguished by the fact that the temporal order of accuracy matches the one of the WENO spatial reconstruction, that is to say 2r-1, where r-1 is the degree of component ENO polynomials making up the WENO polynomial. Related works on arbitrarily high order frameworks are available in the literature; see for example [veiga2024improving, velasco2023spectral, abgrall2023extensions, Decremi, micalizzi2024novel, abgrall2019high, bacigaluppi2023posteriori, abgrall2020high]. Some of them [veiga2024improving, velasco2023spectral, abgrall2023extensions] are obtained through a simple method of lines approach, adopting a spatial discretization of the PDE and solving in time the resulting ODEs system with a sufficiently accurate time integration method. In the other mentioned references, more involved space–time discretizations are considered, such as the continuous Galerkin–DeC framework described in [Decremi, micalizzi2024novel, abgrall2019high, bacigaluppi2023posteriori, abgrall2020high]. An alternative approach to construct schemes of arbitrary space–time accuracy is the fully–discrete ADER methodology, first communicated in the early works [toro2001towards, grptoro, titarev2002ader, schwartzkopff2002ader]. In the fully–discrete ADER approach, the discretizations in space and time are inextricably coupled via the solution of the Generalized Riemann problem [grptoro], GRP_{m}, at cell interfaces, in which the initial conditions consist of nonlinear reconstructed polynomials of arbitrary degree m, leading to a method of order of accuracy equal to m+1 in space and time. The ADER method is a one–step scheme in which the nonlinear reconstruction is performed only once per time step. The ADER methodology has been developed in both the FV and DG frameworks. Further developments of the ADER methodology can be found for example in [dumbser2005ader, dumbser2006arbitrary, dumbser2006building, dumbser2008unified, ADERNSE, dumbser2009very, boscheri2019high, popov2024space, toro2024ader, micalizzi2023efficient]. Elementary introductions to ADER can be found in [ToroBook, Chapters 19 and 20] and in [toro2024computational, Chapter 14]. The rest of the paper is structured as follows. In Section 2 we recall the governing equations and the FV method in the semidiscrete setting. In Section 3 we describe the space discretization, namely, the WENO reconstruction in Section 3.1 and the numerical fluxes under investigation in Section 3.2, while, an outline of the DeC time discretization is given in Section 4. Numerical results are reported in Section 5. Conclusions and future perspectives are found in Section 6."
https://arxiv.org/html/2411.07136v1,Trap Identification in Molecular Charge Transport Networks,"This paper introduces a method to identify traps in molecular charge transport networks as obtained by multiscale modeling of organic semiconductors. Depending on the materials, traps can be defect-like single molecules or clusters of several neighboring ones, and can have a significant impact on the dynamics of charge carriers. Our proposed method builds on the random walk model of charge dynamics on a directed, weighted graph, the molecular transport network. It comprises an effective heuristic to determine the number of traps or trap clusters based on the eigenvalues and eigenvectors of the random walk Laplacian matrix and uses subsequent spectral clustering techniques to identify these traps. In contrast to currently available methods, ours enables identification of trap molecules in organic semiconductors without having to explicitly simulate the charge dynamics. As a prototypical system we study an amorphous morphology of bathocuproine, a material with known high energetic disorder and charge trapping. Based on a first-principle multiscale model, we first obtain a reference charge transport network and then modify its properties to represent different trap characteristics. In contrast to currently available methods, our approach successfully identifies both single trap, multiple distributed traps, and a combination of a single-molecule trap and trap regions on an equal footing.","Organic semiconductors (OSCs) are materials composed of organic molecules that are often organized in a disordered, amorphous structure, and exhibit semiconducting properties. Unlike traditional inorganic semiconductors, OSCs are flexible and allow for much easier tuning of charge mobility, so they have found applications in sensing devices [1], high-performance computing [2], organic light-emitting diodes [3], and organic photovoltaic cells [4]. The functionality and controllable charge mobility of OSC are to a large extent credited to so-called traps, which at the microscopic level are the molecules that can be occupied by charge carriers resulting in a significant change of charge mobility [5, 6, 7, 8]. Those traps are usually single molecules, or a region consisting of very few molecules. Charge carriers can easily occupy those trapping molecules, while altering external conditions likely results in the release of carriers from the traps. Such behaviors lead to sensitive and controllable charge mobility of OSC. A wide range of OSC applications [9, 10, 11, 12, 13] revealed that by tuning the number of charge carriers one can achieve controllable charge mobility. In trap dominated materials where the carrier number is greater than that of traps, only a portion of the carriers is captured by the traps and the remaining carriers can experience fast transport. For example, [14, 15] show that in the Gaussian disorder models used for the theoretical study of charge transport in OSC, when the carrier number is increased by two times, the mobility can increase by approximately 100 times. Zooming into the molecular resolution, charge transport in OSCs is a sequence of transition events between the localized states [16, 17] and is modeled as a continuous time random walk (CTRW) process [18]. The transition rates of carriers depends on all the individual molecules’ geometries and relative orientation, which affect electronic structure properties such as the energy levels, electronic coupling elements between the molecules, and reorganization energies. Those quantities can be calculated from a first-principle multiscale model detailed in Section 2. On a macroscopic level, traps are often considered in the literature in terms of the energy density of state (DOS) p(E), typically assumed to be Gaussian curves or exponential. In equilibrium, the mean energy of a charge carrier in the DOS is E_{\infty}=\frac{\int_{-\infty}^{\infty}Eg(E)p(E)dE}{\int_{-\infty}^{\infty}g(% E)p(E)dE}. (1) Here g(E)=[\exp(\frac{E-E_{F}}{k_{B}T})+1]^{-1} is the Fermi-Dirac distribution with the Fermi energy E_{F} determined by \int_{-\infty}^{\infty}g(E)p(E)dE=N_{c}, with N_{c} being the number of charge carriers. Molecules with energies much lower than E_{\infty} are then considered as (deep) traps. However, such a qualitative criterion is insufficient to identify traps in a molecular charge transport network for several reasons: First, the estimate of E_{\infty} is based on a chosen model DOS which has some assumed continuous distribution. A realistic material, even on the scale 100 nm, will, however, not exhibit such a continuous DOS. Second, a discrete version of Eq. (1) depends on the number of molecules in the system, and the equilibrium energy in such a discrete DOS is dependent on system size [19]. Third, focusing on the DOS alone ignores other contributing factors to the charge dynamics, or the features of the transport network, such as electronic coupling elements between pairs of neighboring molecules, structural details of the material and or spatial correlations. These details are connected to the variety of physical sources for taps, e.g., interfacial effects, defects in molecular packing, or chemical impurities. This makes it difficult to provide a quantitative definition of traps that can be used for identification. At present, there are no methods for the identification of traps in molecular charge transport networks that perform reliably for all different trap types. Few attempts have been reported in identifying trap regions, or clusters, based on analyzing the actual simulated dynamics, e.g., via kinetic Monte Carlo (KMC) [20]. Qualitatively, once entered into such a trap region, the random walk (representing the charge dynamic of a single carrier) transitions mostly within it and escaping it is a rare event, making such KMC simulations very time-consuming. Two methods to accelerate KMC simulations which indirectly involve trap identification have previously been discussed. One is based on the (stochastic) watershed algorithm filling regions (”basins”) in the spatially resolved energy distribution [21]. This purely energy-based criterion does, however, not consider additional details of the factors influencing the molecular charge transport network. The second method [22] is based on a graph-theoretic decomposition (GD) and makes use of the fact that in the presence of trapping regions the Markov chain on the molecular charge transport network is nearly completely decomposable [23], allowing the associated graph to be partitioned into subgraphs. While this method takes the full information of the hopping-type dynamics into account, it is sensitive to the choice of parameters (related to, e.g., graph connectivity properties or transition rate ratios) and is not successful in identifying single trap nodes in the graph (as we will also discuss in Section 4.1.1). In this paper, we propose a new method that builds upon the idea of graph partitioning by using spectral clustering based on a specific type of Laplacian matrix of the graph [24, 25]. The aim of this method is to separate the graph into partitions, by minimizing a normalized cut cost function, such that the random walk processes rarely transitions between different partitions. While obtaining a minimized normalized cut is a NP-hard problem, a relaxed solution of this discrete optimization problem can be obtained from the eigenvectors of the random walk Laplacian matrix which will be introduced in Section 3. Our proposed method includes an effective heuristic for the determination of the number of traps or trap clusters based on these eigenvalues and eigenvectors of this random walk Laplacian and subsequent performing spectral clustering (using K-means clustering) to identify the traps. The former depends on a single threshold parameter for which we find an optimal choice neatly independent of the specific system. Using the charge transport network resulting from a multiscale model of an amorphous morphology of bathocuproine (BCP) [26], a molecular material with known high energetic disorder [27] and complex charge trapping behavior, we demonstrate that our approach successfully identifies both a single trap, multiple distributed traps, and a combination of a single-molecule trap and trap regions on an equal footing. We also find a strong relation between the cost function associated with the normalized cut and the charge-carrier dynamics simulated in a time-of-flight setup [28, 29], as well as the physical characteristics of the trap (regions). In what follows, Section 2 will introduce the elements of the first-principle multiscale model used to obtain the molecular charge transport network of BCP based on a combination of classical molecular dynamics (MD) with quantum electronic structure theory on the level of density-functional theory (DFT), and the calculation of the time-of-flight to assess charge-carrier dynamics from the model. In Section 3 we give the details of the spectral-clustering based trap identification method we propose in this work, including the determination of the cluster number and K-means clustering. The results of the application of this method to the BCP system and its modifications to cover different trap types is presented and discussed in Section 4. A brief conclusion and discussion concludes the paper."
https://arxiv.org/html/2411.07194v1,Re-anchoring Quantum Monte Carlo with Tensor-Train Sketching,"We propose a novel algorithm for calculating the ground-state energy of quantum many-body systems by combining auxiliary-field quantum Monte Carlo (AFQMC) with tensor-train sketching. In AFQMC, having a good trial wavefunction to guide the random walk is crucial for avoiding sign problems. Typically, this trial wavefunction is fixed throughout the simulation. Our proposed method iterates between determining a new trial wavefunction in the form of a tensor train, derived from the current walkers, and using this updated trial wavefunction to anchor the next phase of AFQMC. Numerical results demonstrate that our algorithm is highly accurate for large spin systems, achieving a relative error of 10^{-5} in estimating ground-state energies. Additionally, the overlap between our estimated trial wavefunction and the ground-state wavefunction achieves a high-fidelity. We provide a convergence proof, highlighting how an effective trial wavefunction can reduce the variance in the AFQMC energy estimate.","The quantum many-body problem appears in a wide range of fields, including condensed matter physics, high energy and nuclear physics, quantum chemistry, and material science. One of the most challenging parts of this problem is that the computational cost grows exponentially with the size of the system. Quantum Monte Carlo (QMC) [1, 2, 3, 4] is a class of algorithms that can efficiently deal with high-dimensional problems by reducing the computational cost to a polynomial scale with the system size. It guarantees that in expectation, one can obtain the exact ground-state energy. However, with a finite sample size, QMC generally suffers from sign problem [5, 6], meaning the variance grows exponentially in time. To fix this issue, the constrained-path auxiliary-field quantum Monte Carlo (cp-AFQMC) [7] and phaseless auxiliary-field quantum Monte Carlo (ph-AFQMC) [8] were developed to avoid the sign problem, and have been successfully applied to both lattice models [9, 10, 11, 12, 13, 14, 15, 16, 17, 18] and realistic materials [19, 20, 21, 22, 23, 24, 25, 26, 27, 28]. In cp-AFQMC, the wavefunction is represented as a sum of an ensemble of statistical independent random walkers. The sign problem is avoided by introducing a trial wavefunction to bias the random walkers such that they point in the same direction as the trial wavefunction, with a price of a potential systematic bias. This bias depends on the quality of the trial wavefunction [29]. Ideally, the bias will be removed if the trial wavefunction is exactly the ground-state wavefunction of the system [30]. Based on this result, self-consistent approaches for successively improving the trial wavefunction have been developed [31, 32, 33, 34] to reduce the systematic error. On the other hand, a different type of approach towards the quantum many-body problem is to directly solve for the wavefunction through approximating it by some low-complexity ansatz. One of the most popular ansatz is the matrix product state (MPS) [35, 36], also known as the tensor train (TT) [37]. A huge advantage of TT representation is that its storage complexity and computational complexity can be made near linear (instead of exponential) in the dimension d. However, for complicated systems, the approximation error can be large, which leads to inaccurate energy estimates. Recently, an algorithm that directly combines AFQMC and TT representation has been proposed by one of the authors [38]. In this method, one projects the random walkers into a TT by a TT-sketching technique [39] at every step. It presents a potentially cheaper strategy to update a TT, using AFQMC to simulate a matrix-vector multiplication. While this method avoids the expensive compression of the Hamiltonian operator, it still suffers from the limitation that sometimes, one cannot approximate the ground-state wavefunction as a low-rank TT. In this paper, we provide improvements to AFQMC and tensor methods by combining the best of both worlds. A TT representation of the wavefunction is estimated periodically using TT-sketching. Then the estimated tensor-train wavefunction is used as the trial wavefunction to “anchor” the next episode of AFQMC. We then alternate between estimating a new trial wavefunction, and running the next episode of AFQMC. While this can potentially remove the energy bias by successively improving the trial wavefunction, we show that this strategy also significantly reduces the energy variance as well. On the other hand, our approach in estimating the tensor-train wavefunction is made cheap by the fact that it does not have to be highly accurate, but just have to be sufficiently accurate to guide the walkers. This is unlike other tensor-train approaches, including [38], where it is imperative to obtain an accurate approximation to the ground state since the tensor-train is used to obtain to final energy estimate. We also motivate our procedure by theoretically showing that an improved trial wavefunction can significantly lower the variance of AFQMC’s energy. The rest of this paper is organized as follows. In Section 2, we introduce the procedure of cp-AFQMC for calculating the ground state of spin models, and also some preliminaries for TT and TT-sketching. Our proposed algorithm is then presented in Section 3. In Section 4, we provide a theoretical analysis for the convergence of cp-AFQMC, showing that the procedure can give an accurate energy estimate, despite the fact that the wavefunction itself has a large variance. The results of numerical experiments are shown in Section 5."
https://arxiv.org/html/2411.07151v1,Slice sampling tensor completion for model order reduction of parametric dynamical systems,"The paper addresses the problem of finding a low-rank approximation of a multi-dimensional tensor, \boldsymbol{\Phi}, using a subset of its entries. A distinctive aspect of the tensor completion problem explored here is that entries of the d-dimensional tensor \boldsymbol{\Phi} are reconstructed via C-dimensional slices, where C<d-1. This setup is motivated by, and applied to, the reduced-order modeling of parametric dynamical systems. In such applications, parametric solutions are often reconstructed from space-time slices through sparse sampling over the parameter domain. To address this non-standard completion problem, we introduce a novel low-rank tensor format called the hybrid tensor train. Completion in this format is then incorporated into a Galerkin reduced order model (ROM), specifically an interpolatory tensor-based ROM. We demonstrate the performance of both the completion method and the ROM on several examples of dynamical systems derived from finite element discretizations of parabolic partial differential equations with parameter-dependent coefficients or boundary conditions.","In this paper, we focus on reduced-order modeling for a multiparameter dynamical system using sparse sampling of the parameter domain. Our dimension reduction technique is based on a tensor completion method in a hybrid tensor train (HTT) low-rank format. This new format is particularly well-suited for slice sampling, where the observed entries of a tensor are available as slices. Such slice sampling is relevant in the context of dynamical systems that depend on multiple parameters, where the parametric solution manifold can be effectively learned through space-time slices. Building on the success of solving low-rank matrix completion problems [11, 8, 9, 37, 21, 30], there has been significant progress in developing numerical algorithms for completing multi-dimensional arrays (tensors) in various low-rank tensor formats; see, e.g., [41, 23, 6, 26, 7]. A typical low rank tensor completion problem can be formulated as finding a minimal rank tensor \widetilde{\boldsymbol{\Phi}}\in\mathbb{R}^{N_{1}\times\dots\times N_{d}} that fits an unknown tensor \boldsymbol{\Phi}\in\mathbb{R}^{N_{1}\times\dots\times N_{d}} for a subset of its observed entries: \widetilde{\boldsymbol{\Phi}}=\underset{\boldsymbol{\Psi}\in\mathbb{R}^{N_{1}% \times\dots\times N_{d}}}{\operatorname{argmin}}\mbox{rank}(\boldsymbol{\Psi})% ,\quad\text{s.t.}~{}\boldsymbol{\Psi}|_{\Omega}=\boldsymbol{\Phi}|_{\Omega}, (1) where \boldsymbol{\Psi}|_{\Omega} is a restriction of the tensor on the set of indexes \Omega of observed entries. The exact fitting can be relaxed to approximate one, yielding the inexact completion problem, \widetilde{\boldsymbol{\Phi}}=\underset{\boldsymbol{\Psi}\in\mathbb{R}^{N_{1}% \times\dots\times N_{d}}}{\operatorname{argmin}}\mbox{rank}(\boldsymbol{\Psi})% ,\quad\text{s.t.}~{}\|\boldsymbol{\Psi}|_{\Omega}-\boldsymbol{\Phi}|_{\Omega}% \|\leq\varepsilon, (2) with a given \varepsilon\geq 0. The inexact completion (2) is a common problem setup for the case of noisy data [10] and this is a formulation we are interested here. The definition of the tensor rank in (1)–(2) is not unique and depends on the choice of a rank revealing tensor decomposition, with CANDECOMP/PARAFAC (CP), Tuker or Tensor Train (TT) among most popular choices. For CP, Tucker, and TT ranks, the completion problems (1) and (2) are NP-hard. One popular approach is to relax it into a convex optimization problem [35, 17, 6], following the ideas from [11, 9]. Other approaches to tensor completion include ALS methods [38, 20], Riemannian optimization [23, 36], Bayesian methods [39, 40], and projection methods [33], with applications ranging from video recovery to seismic data reconstruction [6, 25]. The application of tensor completion methods for reduced-order modeling of parametric ODEs or PDEs is rare, and this study aims to explore this direction. In general, the use of tensor methods for solving parametric PDEs is not new. Several studies have developed sparse tensorized solvers for certain high-dimensional and stochastic PDEs [34, 22, 13, 18, 31, 15, 14]. The reconstruction of scalar output quantities of parametric solutions in tensor form from incomplete observations was addressed in [4, 19]. In [4], the authors employed a tensor cross approximation, while [19] applied TT-completion via Riemannian optimization to recover an option pricing statistic from solutions of parametrized Heston and multi-dimensional Black-Scholes models. Additionally, a comparison of TT-cross interpolation and TT-completion for a parameterized diffusion equation in [36] demonstrated that TT-completion requires fewer PDE solver executions to find a low-rank approximation of a particular solution functional. While the works [4, 36, 19] focused on recovering scalar solution statistics in tensor format, here we aim to approximate a tensor of solution snapshots for subsequent use in building Galerkin ROMs for the parametrized dynamical system of interest. Similar to the previous studies, we seek to find a low-rank approximation of the tensor from a small subset of its entries. However, due to the relatively high separation ranks and large sizes of the space and time modes in the snapshot tensor, applying existing completion algorithms in standard low-rank formats is computationally prohibitive. Moreover, the sampling of the unknown tensor occurs not elementwise, but slicewise. Thst is, for a given (e.g., randomly chosen) parameter value, an entire slice of the unknown tensor is obtained by solving the dynamical system for all space and time degrees of freedom. To leverage slice sampling and improve the computational efficiency of the completion, we introduce a new low-rank format, which we call HTT. Completion in the HTT format involves the projection of a sparsely sampled tensor onto reduced orthogonal bases along the spatial and temporal modes (similar to HOSVD), followed by multiple completions of smaller elementwise sampled tensors in TT format, which can be processed in parallel using available completion algorithms. This new format is particularly well-suited for the subsequent construction of a Galerkin ROM known in the literature as the low rank tensor decomposition (LRTD) ROM [27, 28, 29]. The LRTD–ROM is a natural extension of the POD ROM for parametric dynamical systems. The remainder of the paper is organized as follows: Section 2 provides a more detailed explanation of slice sampling and introduces the completion method. We deviate from the traditional approach of defining tensor rank before formulating the completion problem, as we find it more instructive to first explain the method of obtaining a fitting tensor. The resulting rank-revealing format becomes more intuitive afterward. Section 3 describes the Galerkin ROM for the dynamical system. This ROM utilizes HTT as the dimension reduction technique (in place of the standard POD), and we refer to it as HTT-ROM. Section 4 presents the results of numerical experiments."
https://arxiv.org/html/2411.06712v1,A non-Hermitian quantum mechanics approach for extracting and emulating continuum physics based on bound-state-like calculations: technical details,"This work applies a reduced basis method to study the continuum physics of a finite quantum system—either few or many-body. Specifically, we develop reduced-order models, or emulators, for the underlying inhomogeneous Schrödinger equation and train the emulators against the equation’s bound-state-like solutions at complex energies. The emulators can quickly and accurately interpolate and extrapolate the matrix elements of the Hamiltonian resolvent operator (Green’s function) in a parameter space that encompasses the complex energy plane as well as other real-valued parameters in the Schrödinger equation. The spectra, discretized and compressed as the result of emulation, and the associated resolvent matrix elements (or amplitudes), have the defining characteristics of non-Hermitian quantum mechanics calculations, featuring complex eigenenergies with negative imaginary parts and resolvent matrix elements with branch cuts moved below the real axis in the complex energy plane. Therefore, we now have a method that extracts continuum physics from bound-state-like calculations and emulates them in the input parameter space. Some significant results have previously been presented in a short report (arXiv:2408.03309). Here, we provide details of the study and an in-depth analysis, including how this method could be coupled with existing approaches to compute and emulate bound and continuum states.","In a recent paper Zhang (2024), we reported on a new application of the reduced basis method (RBM) Duguet et al. (2024); Drischler et al. (2023); Melendez et al. (2022) in the study of continuum physics of finite quantum systems. Technical details of the study are provided in this paper. Readers are advised to read the short report first, which could facilitate reading the current paper. By continuum physics, we mean the part of the spectrum of a Hamiltonian operator where the system can break up into subsystems (i.e., above the system’s lowest threshold) and the states and observables associated with that sector of the spectrum. To be quantitative, let H(\bm{\theta}) be the Hamiltonian operator. An important operator is H’s resolvent or Green’s function111We are working with time-independent Schrödinger equations in this work.. Its matrix element between two source states |{S}(\bm{\theta})\rangle and |\tilde{{S}}(\bm{\theta})\rangle is named amplitude \mathcal{A}, with \displaystyle\mathcal{A}(E,\bm{\theta})\equiv\left\langle\tilde{{S}}(\bm{% \theta})\left|\frac{1}{E-H(\bm{\theta})}\right|{S}(\bm{\theta})\right\rangle\,. (2) The vector \bm{\theta} collects the parameters of H, {S}, and \tilde{{S}}. \mathcal{A} represents an array of continuum physics observables, such as response functions and scattering amplitudes, depending on the construction of the source states. See Sec. II.1 for the details. The analytical properties of \mathcal{A} in the complex E plane, such as isolated poles and branch cuts (BCTs), are directly connected to the basic features of H’s spectrum Newton (1982). This informs an interesting numeric computational framework, called non-Hermitian quantum mechanics (NHQM) Reinhardt (1982); Moiseyev (2011), as discussed in Sec. II.2. Since the approximations of \mathcal{A} produced in a broad class of calculations (including NHQM) could be viewed as rational approximations Trefethen (2023) in terms of the variable E, a recent and relevant development on this subject is mentioned in Sec. II.3. In principle, we can solve the inhomogeneous Schrödinger equation, (E-H)|\Psi\rangle=|S\rangle\,, (3) or \langle\tilde{\Psi}|(E-H)=\langle\tilde{{S}}|\,, (4) and compute \displaystyle\mathcal{A}=\langle\tilde{{S}}|\Psi\rangle=\langle\tilde{\Psi}|{S% }\rangle\,. (5) To simplify our notation, we assume the E and \bm{\theta} dependence is implicit unless otherwise stated. These equations and \mathcal{A} are the main targets of this study. We aim to develop RBM-based reduced-order models (ROMs), or emulators, for the solutions of Eqs. (3) and (4) and \mathcal{A} so that we can extrapolate them in the complex E plane and interpolate them in the space of \bm{\theta}. Extrapolation in E is a crucial capability, from which \mathcal{A} at real Es can be inferred from the solutions at complex Es. Solving these equations directly at real Es poses severe numeric challenges, but at complex Es, it becomes a more feasible bound-state-like calculation. The emulation in \bm{\theta} enables rapid explorations of the continuum physics calculations in the parameter space, another helpful functionality. As a model-order-reduction (MOR) tool for a parameterized equation system Hesthaven et al. (2015); Quarteroni et al. (2016); Benner et al. (2017a, b, 2015), the RBM first constructs a subspace spanned by the equation’s full (or high-fidelity) solution at a sample of parameter sets, called snapshots, during the so-called offline training stage. Afterward, the equation system is projected into the subspace to form a ROM, which can be used to emulate the solutions and the associated observables in the parameter space. The dimension of the subspace is typically low and scales mildly with the number of parameters Duguet et al. (2024). Consequently, the computing cost for running emulators at the online emulation stage is dramatically reduced compared to simply repeating high-fidelity calculations, e.g., when exploring the parameter space of the calculations. The basic principle of the RBM was recently rediscovered in nuclear theory as the eigenvector continuation method Frame et al. (2018), where the focus was solving the eigenvalue problem. The RBM-based emulators have gained much attention and further development, including for nuclear-bound states Sarkar and Lee (2021a, 2022); König et al. (2020); Demol et al. (2020); Ekström and Hagen (2019); Demol et al. (2021); Yoshida and Shimizu (2022); Anderson et al. (2022); Giuliani et al. (2023); Yapa et al. (2023), resonant states Yapa et al. (2023, 2024) and general continuum scattering states Furnstahl et al. (2020); Drischler et al. (2021); Melendez et al. (2021); Zhang and Furnstahl (2022); Bai and Ren (2021); Drischler and Zhang (2022); Melendez et al. (2022); Drischler et al. (2023); Bai (2022); Garcia et al. (2023); Odell et al. (2024). One difference in our work’s RBM aspect is emulating inhomogeneous linear equations with continuous spectra222Ref. Melendez et al. (2022) surveyed different ROMs, including for the inhomogeneous equations. However, the continuous spectrum aspect was not illuminated.. In contrast, most previous quantum physics-related studies have considered emulating a specific eigenstate of a Hamiltonian operator, either a bound, resonance, or scattering state at a real energy E. More significantly, this is the first time to consider the complex E plane as part of the parameter space, in addition to the other model input parameters, such as \bm{\theta} in Eqs. (3) and (4). Our RBM formalism is discussed in detail in Sec. III. If we only consider the E variable, our complex-E emulation (CEE) is superficially similar to the rational Krylov methods Antoulas (2005); Van Beeumen et al. (2017); Peng et al. (2019) applied in studying finite linear equation systems. However, the CEE generalizes the Krylov methods to studying linear systems with continuous spectra. We also gain insights about a potential connection, mentioned throughout this paper, between the CEE and the (near)-optimal rational approximations Trefethen (2023) of a univariate function with branch points. Such a connection does not exist in the case of a linear system with only a discrete spectrum. When including emulation in other real-valued parameters, our study further extends the rational Krylov methods to the case with higher-dimensional parameter spaces; it also generalizes the univariate rational approximation to a multivariate one. We call it CERPE, an abbreviation for “complex-energy real-parameter emulator.” Further discussions on the related works can be found in Sec. III.5 On the physics front, our CEE is a new NHQM method for computing continuum states and observables. This is demonstrated numerically with two and three-body systems in Secs. IV and V. Some analytic understanding of the NHQM aspect of our CEE and the existing NHQM methods, including integration-contour deformation Glöckle (1983), complex scaling of different variants Reinhardt (1982); Moiseyev (2011); Myo et al. (2014); Lazauskas and Carbonell (2011); Lazauskas (2012); Papadimitriou and Vary (2015); Lazauskas (2015); Lazauskas and Carbonell (2020), and Berggren-basis based methods Michel and Płoszajczak (2021); Li et al. (2021); Berggren (1968); Michel et al. (2002, 2003); Id Betan et al. (2003); Hagen et al. (2006); Rotureau et al. (2006); Fossez et al. (2017); Hu et al. (2020); Michel et al. (2022), are presented in Sec. II.2. The generic strategies behind these methods are elaborated in that section using a simple model. However, our method differs significantly from existing NHQM approaches. The fundamental distinction is in constructing a finite-dimensional non-Hermitian Hamiltonian H matrix—a step we call “non-Hermitization.”333This can also be viewed as breaking time-reversal symmetry in the basis. This difference and its implications are discussed in Sec. III.3. The CERPE component of this study is also useful for continuum physics studies. Both Hamiltonian spectra and \mathcal{A} can be interpolated, extrapolated, or emulated in the space of \bm{\theta} in the inhomogeneous Schrödinger equations. The functionality of CERPE follows the same argument of existing emulators: they provide efficient interfaces for the users to access computationally expensive calculations with dramatically reduced computing costs Zhang and Furnstahl (2022). For example, with this emulator technology, model calibration and uncertainty quantification, particularly those based on Bayesian statistics, would become feasible for complex models and expensive calculations. In short, with CEE, continuum physics can potentially be extracted from bound-state-like calculations—an advantage of the NHQM methods. The CERPE further expands the functionality of such continuum physics calculations by reaching more users. Another new physics insight is concerned with complex-energy (CE) Schlessinger and Schwartz (1966); Schlessinger (1968a, b); McDonald and Nuttall (1969); Uzu et al. (2003); Deltuva and Fonseca (2012, 2013a, 2013b, 2014) and Lorentz integral transformation (LIT) methods Efros (1985a); Efros et al. (1994, 2007); Orlandini et al. (2014); Sobczyk et al. (2021, 2024); Bonaiti et al. (2024). The inhomogeneous Schrödinger equations are also solved at complex energies in the LIT calculations and effectively in the CE calculations.444In the existing implementation of the CE method Uzu et al. (2003); Deltuva and Fonseca (2012, 2013a, 2013b, 2014), the Lippmann-Schwinger and the Faddeev equations are solved for the on- and off-shell scattering amplitudes. However, the wave functions can be computed with those amplitudes and vice versa Schlessinger and Schwartz (1966). Using their procedures, these methods connect the complex-E results to the real-E ones. Our results suggest that these existing calculations can be viewed from the lens of the general NHQM framework. Perhaps more importantly, the CERPE developed here can be applied directly to emulate these existing calculations. The general procedures for achieving this can be found in Sec. VI. One counterintuitive understanding of computing continuum physics is worth a brief mention. The results shown in this work suggest that the CEE, which approximates \mathcal{A} using a small non-Hermitian H-matrix in Eq. (2), produces better results at real energies than the high-fidelity calculations based on a large Hermitian H-matrix. What is puzzling is that the two agree numerically in the instances with complex energies, i.e., the emulator is trained by these high-fidelity calculations. In contrast, in the existing emulator studies, emulators are supposed to reproduce high-fidelity calculations, including during extrapolations. We inevitably conclude that the CEE (and thus CERPE) as an extrapolant for E is biased to the physical continuum physics instead of the high-fidelity results based on a discrete spectrum. The non-Hermitization of H plays a key role here. This is further discussed in Sec. VII. We emphasize that although numerical results are only presented for simple two- and three-body systems with short-range interactions, both CEE and CERPE should work for general finite systems, as the applicability of the RBM method and the working of non-Hermitization are general, without specific reference to the size and the interaction nature of the system. The RBM method requires smooth dependence of the solution on the input parameters, which have been found to hold up in few and many-body studies. Meanwhile, as explained later, the non-Hermitization depends on the spectrum’s continuous nature and the training points’ setup in the complex E plane. However, we also need to point out that all the numeric calculations here are performed with high accuracy, with relative errors on the order of 10^{-12} in the training calculations. Our understanding of the methods developed here is based on such calculations. In practice, the training calculations, although attainable using bound-state methods, can have more significant errors. How the errors impact the performance of the emulator’s extrapolation and ways to stabilize the extrapolation need to be studied in the future. A summary of the organization of the rest of the paper is as follows. In Sec. II, a general discussion about H’s spectrum, its resolvent operator, and their connections to the continuum observables are provided. Recent developments in rational approximation studies are mentioned in light of their relevance in this work. Section III discusses the RBM framework used in this study. Numerical experiments of the CEE and CERPE in both two and three-body systems are discussed and analyzed carefully in Secs. IV and V. In Sec. VI, we discuss the potential couplings between our methods and other calculation methods. In Sec. VII, a summary is provided. The appendices collect some detailed information needed to reproduce the numerical calculations in this work."
https://arxiv.org/html/2411.06536v1,Quantum Calculations of Hydrogen Absorption and Diffusivity in Bulk \mathrm{CeO_{2}},"CeO2 (ceria) is an attractive material for heterogeneous catalysis applications involving hydrogen due to its favorable redox activity combined with its relative impermeability to hydrogen ions and molecules. However, to date, many bulk ceria/hydrogen properties remain unresolved in part due to a scarcity of experimental data combined with quantum calculation results that vary according to the approach used. In this regard, we have conducted a series of Density Functional Theory (DFT) calculations utilizing generalized gradient (GGA), meta-GGA, and hybrid functionals as well as several corrections for electronic correlations, applied to a number of properties regarding hydrogen in bulk stoichiometic \mathrm{CeO_{2}}. Our calculations place reasonable bounds on the lattice constants, band gaps, hydrogen absorption energies, and O-H bond vibrational frequencies that can be determined by DFT. In addition, our results indicate that the activation energy barriers for hydrogen bulk diffusion are uniformly low (<0.15\ \mathrm{eV}) for the calculation parameters probed here and that, in general, the effect of hydrogen tunneling is small at ambient temperatures. Our study provides a recipe to determine fundamental physical chemical properties of Ce-O-H interactions while also determining realistic ranges for diffusion kinetics. This can facilitate the determination of future coarse-grained models that will be able to guide and elucidate experimental efforts in this area.","In the last several decades, ceria (\mathrm{CeO_{2}}) has been the subject of numerous experimental and theoretical studies because of its potential in an array of applications predominantly in heterogeneous catalysis, including water-gas shift reactions[1, 2], water splitting[3], and semi-hydrogenation of alkynes[4]. However, the mechanism for hydrogen absorption and transport in ceria remains an open question. Chemisorption of hydrogen ions resulting from H2 dissociation or H2O splitting is exothermic and forms hydroxyl species on the surface, which could act as a first step in the formation of cerium hydrides.[5, 6, 7, 8] In contrast, hydrogen can require relatively high temperatures and pressures to diffuse into the bulk of sub-stoichiometric surfaces, where it can form hydroxyl groups and hydride ions near oxygen vacancies.[5, 9] Such studies would benefit from atomistic calculations that could help determine initial hydriding reaction steps as well as the ensuing chemical rate parameters, which would help elucidate the importance of competing chemical mechanisms. In this regard, quantum calculations with Kohn-Sham Density Functional Theory (DFT) remains a popular choice for determining the breaking and forming of bonds in f-electron systems (e.g., Refs. 10, 11, 12). DFT calculations regarding ceria surface chemistry, though, largely remain quantitatively inconsistent. There has been reported a wide range of energetic barriers for diffusion into the bulk from \mathrm{CeO_{2}}(111) surfaces, with values between 1.15\ \mathrm{eV} and 1.67\ \mathrm{eV} (Refs. 13 and 14, respectively). In addition, there exist similar discrepancies in DFT data regarding hydrogen bulk diffusion in stoichimetric CeO2, with results ranging from 0.18\ \mathrm{eV} in Ref. 13 to 0.52\ \mathrm{eV} in Ref. 14. This is in sharp contrast to one set of experimental results[9] utilizing nuclear reaction analysis (NRA) on sub-stoichiometric \mathrm{CeO_{1.69}} films exposed to \mathrm{H_{2}} to estimate a diffusion activation barrier of 1.69\ \mathrm{eV}. The range of DFT results as well as the seeming disagreement with experimental diffusion barriers is likely in part due to the sensitivity of H-Ce-O interactions to different calculation parameters, including choice of exchange-correlation functional and level of theory (discussed below) and use of specific Hubbard parameters (e.g., DFT+U) to better account for electron correlations. For example, while the Hubbard U correction is generally only applied to the Ce 4f orbitals, some data on a number of metal oxides (including ceria) indicate advantages to including additional Hubbard corrections to the O 2p orbitals to improve the description of defect states.[15, 16, 17, 18, 19] Thus, to the best of our knowledge, there does not exist a systematic determination of how hydrogen properties within CeO2 depend on the various options available here, and how different choices might affect the interpretation of results and possible comparison to experiments. In this work, we address these issues by employing a range of DFT calculations at various levels of theory to better elucidate \mathrm{CeO_{2}}/H interactions. We report on calculations with the generalized gradient (GGA), meta-GGA, and screened hybrid functionals, using a Hubbard U correction on Ce 4f orbitals as well as a wide range of values for the additional correction to O 2p. We then report on bulk stoichiometric \mathrm{CeO_{2}} properties, interstitial hydrogen formation energies, and diffusion activation energy barriers. We include Arrhenius prefactor values and O–H bond vibrational frequencies, where applicable. Finally, we use the characteristic crossover temperatures to estimate the significance of quantum nuclear tunneling effects vs. classical, over-the-barrier hopping mechanisms. We believe our results place accurate bounds on hydrogen diffusion within bulk \mathrm{CeO_{2}} that can help elucidate experimental results while also providing guidance for future DFT studies on similar systems."
https://arxiv.org/html/2411.06447v1,Multi-Parameter Molecular MRI Quantification using Physics-Informed Self-Supervised Learning,"Biophysical model fitting plays a key role in obtaining quantitative parameters from physiological signals and images. However, the model complexity for molecular magnetic resonance imaging (MRI) often translates into excessive computation time, which makes clinical use impractical. Here, we present a generic computational approach for solving the parameter extraction inverse problem posed by ordinary differential equation (ODE) modeling coupled with experimental measurement of the system dynamics. This is achieved by formulating a numerical ODE solver to function as a step-wise analytical one, thereby making it compatible with automatic differentiation-based optimization. This enables efficient gradient-based model fitting, and provides a new approach to parameter quantification based on self-supervised learning from a single data observation. The neural-network-based train-by-fit pipeline was used to quantify semisolid magnetization transfer (MT) and chemical exchange saturation transfer (CEST) amide proton exchange parameters in the human brain, in an in-vivo molecular MRI study (n=4). The entire pipeline of the first whole brain quantification was completed in 18.3\pm8.3 minutes, which is an order-of-magnitude faster than comparable alternatives. Reusing the single-subject-trained network for inference in new subjects took 1.0\pm0.2 s, to provide results in agreement with literature values and scan-specific fit results (Pearson’s r>0.98, p<0.0001).","Magnetic resonance imaging (MRI) plays a central role in clinical diagnosis and neuroscience. This modality is highly versatile and can be selectively programmed to generate a large number of image contrasts[1], each sensitive to certain biophysical parameters of the tissue. In recent years, there has been extensive research into developing quantitative MRI (qMRI) methods that can provide reproducible measurements of magnetic tissue properties (such as: T1, T2, and T{}_{2}^{*}), while being agnostic to the scan site and the exact acquisition protocol used[2]. Classical qMRI quantifies each biophysical property separately[3], using repeated acquisition and gradual variation of a single acquisition parameter under steady state conditions. This is followed by fitting the model to an analytical solution of magnetization vector dynamics[4]. The exceedingly long acquisition times associated with the classical quantification pipeline have motivated the development of magnetic resonance fingerprinting (MRF)[5], which is an alternative paradigm for the joint extraction of multiple tissue parameter maps from a single pseudorandom pulse sequence. Since MRF data are acquired under non-steady state conditions[6], the corresponding magnetization vector can only be resolved numerically. This comes at the expense of the complexity of the inverse problem, namely finding tissue parameters that best reconstruct the signal according to the forward model of spin dynamics. Since model fitting under these conditions takes an impractically long time[7], MRF is commonly solved by dictionary matching, where a large number of simulated signal trajectories are compared to experimentally measured data[8]. Unfortunately, the size of the dictionary scales exponentially with the number of parameters (the ""curse of dimensionality""[9]), which rapidly escalates the compute and memory demands of both generation and subsequent use of the dictionary for pattern matching-based inference. Recently, various deep learning (DL)-based methods have been developed for replacing the lengthy dictionary matching with neural-network (NN)-based inference[10, 11, 12, 13]. While this approach greatly reduces the parameter quantification time, networks still need to be trained using a comprehensive dictionary of synthetic signals. Since dictionary generation may take days[12], it constitutes an obvious bottleneck for routine use of MRF, and reduces the possibilities for addressing a wide variety of clinical scenarios. The complexity and time constraints associated with the MRF pipeline are drastically exacerbated for molecular imaging applications that involve a plurality of proton pools, such as chemical exchange saturation transfer (CEST) MRI[14]. While CEST has demonstrated great potential for dozens of biomedical applications[15, 16, 17, 18, 19, 20, 21], some on the verge of entering clinical practice[22], the inherently large number of tissue properties greatly complicate analysis[23]. This has prompted considerable efforts to transition from CEST-weighted imaging to fully quantitative mapping of proton exchange parameters [24, 25, 26, 27, 28]. Early CEST quantification used the fitting of the classical numerical model (based on the underlying Bloch-McConnell equations) after irradiation at various saturation pulse powers (B_{1})[27]. However, applying this approach in a pixelwise manner in-vivo is unrealistic because both the acquisition and reconstruction steps may require several hours. Later, faster approaches, such as quantification of the exchange by varying saturation power/time and Omega-plots [29, 30, 31, 32] still rely on steady-state (or close to steady state) conditions, and approximate analytical expressions of the signal as a function of the tissue parameters[33, 34]. Unfortunately, a closed-form analytical solution does not exist for most practical clinical CEST protocols, which utilize a train of off-resonant radiofrequency (RF) pulses saturating multiple interacting proton pools. Similarly to the quantification of water T1 and T2, incorporating the concepts of MRF into CEST studies provided new quantification capabilities[28, 35, 36, 37, 38] and subsequent biological insights, for example, in the detection of apoptosis after oncolytic virotherapy[12]. However, in order to make CEST MRF a realistic, robust, and attractive candidate for clinical use, the long dictionary generation time associated with each new application needs to be replaced by a rapid and flexible approach that adequately models multiple proton pools under saturation pulse trains. Here, we describe a physics-based deep learning framework for rapid model fitting of the human brain tissue proton spin properties. While this approach is applicable for quantifying a variety of MRI parameters, we focus on a challenging CEST imaging scenario, involving multiple proton pools, a saturation pulse train, and non-steady-state MRF acquisition. The computational pipeline (Fig. 1) combines a spin physics simulator and a NN-based quantitative parameter reconstructor in a fully auto-differentiable manner[39]. Our system effectively solves and inverts the Bloch-McConnell ordinary differential equations (ODEs), which govern the multi-pool exchange, saturation, and relaxation dynamics of molecular MRI. Hence, we refer to this approach as ""neural Bloch McConnell fitting"" (NBMF). Importantly, the network can be be trained in a self-supervised manner, directly on the single-subject data of interest (inspired by related work on test-time-[40], internal-[41], and zero-shot-[42, 41, 43] learning). This circumvents the need for prior curation of a large training dataset, which is often inaccessible, especially for molecular MRI. Figure 1: Schematic representation of the core neural Bloch McConnell fitting (NBMF) pipeline. (Top) A quantitative parameter reconstructor parameterized as a multi-layer perceptron (MLP) and a differentiable Bloch-McConnell simulator are serially connected into a single computational graph. Single-subject MRF data serves both as the input and as the regression target for the reconstructor-simulator circuit. The network convergence provides the fitted exchange parameter maps for the examined subject as well as a trained NN reconstructor; the latter can be used to extract parameter maps for new subjects within seconds (Bottom). The simulator can be realized using the exact numerical Bloch McConnell ODE solver or using analytical approximations when available (e.g., for 2-pool semisolid-MT quantification [34]). While not shown in the diagram, auxiliary per-voxel data such as T1, T2, B0, and B1 maps can be added as input to the neural reconstructor and the simulator. Furthermore, the pipeline main block can be serially repeated so that estimated semisolid MT volume fraction (fss) and proton exchange rate (kssw) maps inferred at the first stage are joined to the raw data used in a second reconstructor aimed to quantify the amide proton exchange parameters (fs, ksw)."
https://arxiv.org/html/2411.06152v1,On the convection boundedness of numerical schemes across discontinuities,"This short note introduces a novel diagnostic tool for evaluating the convection boundedness properties of numerical schemes across discontinuities. The proposed method is based on the convection boundedness criterion and the normalised variable diagram. By utilising this tool, we can determine the CFL conditions for numerical schemes to satisfy the convection boundedness criterion, identify the locations of over- and under-shoots, optimize the free parameters in the schemes, and develop strategies to prevent numerical oscillations across the discontinuity. We apply the diagnostic tool to assess representative discontinuity-capturing schemes, including THINC, fifth-order WENO, and fifth-order TENO, and validate the conclusions drawn through numerical tests. We further demonstrate the application of the proposed method by formulating a new THINC scheme with less stringent CFL conditions.","Numerical simulations of complex flow systems present significant challenges due to the presence of discontinuities, such as material interfaces in multiphase flows, reaction fronts in combustion, and shock waves in supersonic flows. Numerical methods for resolving discontinuities are generally categorized into tracking and capturing schemes. Discontinuity-capturing schemes are widely used due to their flexibility and ability to extend to higher-order accuracy. However, designing high-resolution discontinuity-capturing schemes is challenging, as Godunov’s theorem states that no linear scheme of higher than second order can maintain monotonicity. Over the decades, significant efforts have been made to overcome Godunov’s barrier and develop non-linear, high-resolution discontinuity-capturing schemes. High-order shock-capturing schemes, such as WENO (Weighted Essentially Non-Oscillatory) [1, 2, 3], CWENO (Central WENO) [4, 5], and TENO (Targeted Essentially Non-Oscillatory) [6, 7], have been successfully developed. Recent advancements [8, 9, 10, 11] have further improved the resolution and robustness of these schemes. Interface-capturing schemes, such as THINC (Tangent Hyperbola for Interface Capturing) [12], have been applied to compressible flows to enhance the resolution of discontinuous flow structures, such as contact discontinuities, through the BVD (Boundary Variation Diminishing) algorithm [13, 14, 15] or discontinuity-detecting criterion [16]. Recent work [17, 18] has demonstrated that existing three-cell-based non-linear schemes can be unified into a single framework, from which a new high-resolution scheme, named ROUND (Reconstruction Operator on Unified Normalized-variable Diagram), has been proposed. Significant efforts have also been made to understand and optimize the numerical properties of non-linear discontinuity-capturing schemes, such as their spectral properties [19] and stability [20]. A general framework based on a quantitative error metric for evaluating shock-capturing schemes has also been developed [21]. To quantify the overshoot error as a function of the CFL number, recent work [22] introduced error metrics for non-linear shock-capturing schemes. However, limited research has addressed the convection boundedness properties of non-linear schemes across discontinuities. Therefore, this study proposes a diagnostic tool to evaluate and improve the convection boundedness of numerical schemes across discontinuities. This work is organised as follows. In Section 2, the convection boundedness criterion across discontinuities and the proposed diagnostic method based on the normalised variable diagram are given. In Section 3, we apply the proposed method to evaluate the representative schemes and validate the conclusions drawn through numerical tests. In Section 4, we demonstrate the application of the proposed method by formulating a new THINC scheme with less stringent CFL conditions. Finally, a brief concluding remark is given in Section 5."
https://arxiv.org/html/2410.21772v1,Organic compounds in metallic hydrogen.,"Metallic hydrogen[1] is the most common condensed material in the universe, comprising the centre of gas giant planets[2, 3, 4] However, experimental studies are extremely challenging[5, 6, 7, 8], and most of our understanding of this material has been led by theory. Chemistry in this environment has not been probed experimentally, so here we examine hydrocarbon chemistry in metallic hydrogen using density functional theory calculations[9, 10]. We find that carbon and oxygen react with metallic hydrogen to produce an entirely new type of hydrocarbon chemistry based on sixfold coordinated carbon with organic-style molecules CH6, C2H8, C3H10 OH3 NH4 and CH4OH. These are charged molecules stabilised by the metallic environment. Their associated electric fields are screened, giving oscillation in the surrounding electron and proton densities. In view of the excess hydrogen we refer to them as hypermethane, hyperethane etc. The relationship to traditional chemistry is that the metallic background acts as an electron donor and stabilizes negatively charged ions. This enables the formation of six covalent bonds per carbon atom, or three per oxygen atom. This demonstrates that organic chemistry may take place in very different environments from those found on earth, and may be common throughout the universe.","Metallic hydrogen is believed to be the most common condensed phase of matter in the universe, comprising the cores of gas-giant planets, and giving rise to their enormous magnetic fields. However, it is exceptionally challenging to make metallic hydrogen on earth. Consequently, most of our understanding of this material comes from theory. Modern planetary models depict layers separated by the weight of elements, e.g. gas giants feature an outer molecular hydrogen and helium envelope and a core of metallic hydrogen depleted of helium [11, 12, 13, 4] in which is predicted to be insoluble below its own metallization conditions [14, 15, 16, 17, 18, 19, 20, 21]. Understanding of material properties allows us to infer the composition and structure of exoplanets from their mass-radius relation[22, 14, 2, 23, 24, 3, 25, 26]. Chemical bonding is different at high pressure. For example, on earth, the major components of the mantle exhibit sixfold coordinated silicon[27, 28, 29, 30, 31], in contrast to the fourfold sp3 bonding found in normal conditions. The unconventional formation of sixfold coordinated silicon is well described by density functional theory calculations as being enabled by the electrons donated from the Mg ions, and stabilised at pressure thanks to the increased density[32, 33, 34, 35, 36, 37, 38]. If chemistry can change so radically just few thousand kilometres beneath our feet, how different might it be elsewhere in the solar system? While helium in hydrogen, and high pressure hydrogen-rich metals are very well studied, particularly with potential applications to superconductivity, less attention has been paid to the issue of solubility of heavier elements in metallic hydrogen, and the implications different chemical bonding within giant planets[39, 21, 40, 41]. Theoretical study of metallic hydrogen began in 1935, when Wigner and Huntington [1] used free electron theory to estimate the density of metallic hydrogen, obtaining a value remarkably close to current estimates. This implied that hydrogen molecules would transition into atomic metallic hydrogen when subjected to sufficiently high pressure - unfortunately their estimate of 25GPa was more than an order of magnitude too low. In 1968, Ashcroft made a remarkable prediction that metallic hydrogen would be a room temperature superconductor. Predictions of the crystal structure of metallic hydrogen came even later[9, 42, 43, 44], through ab initio random-structure exploration. Theoretical assessments [42, 9, 45, 46, 47, 48, 49] indicate that at low temperature hydrogen remains molecular to about 500GPa. Surprisingly, the first atomic and metallic phase is now believed to be a complex, open structure, rather than the dense-packed structures assumed by Wigner, Huntingdon and Ashcroft. This type of open structure is typical of the high pressure Group I electride materials, having I4_{1}/amd symmetry, isostructural with cesium IV[50, 51, 52, 53]. 500GPa is at the limit of current experiments which have seen signs of bandgap closure and reflectivity.[5, 6, 7]. This I4_{1}/amd structure persists until 2.5 TPa, beyond which more densely packed structures are favoured[54, 42]. Fluid metallic hydrogen has been detected at much lower pressures, but higher temperatures, in both static and dynamic compression[8, 55], and calculation[24, 56, 57, 58]. In experiments[59], synthesis of solid metallic hydrogen has been claimed at pressures exceeding 420 GPa using infrared absorption measurements [5], and at an even higher pressure of 495 GPa as evidenced by reflectivity measurements [6]. Liquid metallic hydrogen has been reported at much lower pressures both in experiment [60, 8, 55] and simulation[61, 56, 62, 57]. Carbon is particularly important, being the fourth most abundant element and the building block of organic chemistry. The solubility of hydrocarbons in metallic hydrogen remains the preserve of theorists. Hydrocarbons have recently been studied with planetary interior conditions, i.e. high pressure and high temperature [63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 41, 74]. These studies are useful for understanding the interior of giant planets, such as Neptune and Uranus [66, 71, 72, 73] , and Jupiter and Satern [74]. Some special compounds, such as CH4(H2)2 [68, 69], C2H6 and C4H10 [73], can be formed under these extreme conditions. In addition, there were a number of studies on the hydrocarbon [41] and helium [74] in the hydrogen environment. Studies on giant planets suggest that hydrocarbons likely exist within the middle layer of their structure [72, 75, 73]. Methane (CH4) has been identified as the most abundant hydrocarbon at pressures of up to several hundred GPa and forms hydrogen-rich compounds with H2 up to 160GPa[76, 68]. At higher pressures, simulations suggested that methane decomposes into hydrogen and diamond [72, 75, 41]. Due to its density, the latter subsequently gravitationally sinks deeper into the planet in a phenomenon known as diamond rain.[72, 75, 77, 78, 79, 80, 81, 82, 83]. This predicted demixing contrasts with the observation of high pressure reaction between diamond and hydrogen[84] - an issue which has caused significant practical challenges to synthesizing metallic hydrogen in diamond anvil cells[59]. One challenge for theory is the richness of hydrocarbon chemistry. The demonstration that methane is unstable to decomposition does not preclude other stable hydrocarbons. Moreover, given the high temperatures and excess of hydrogen over carbon in gas giant planets, even a low solubility limit could result in much of the carbon remaining in solution in the metallic hydrogen layers. Here, we use density functional theory calculations to consider what form of carbon will exist in a metallic hydrogen environment. We start by examining the free energy in the well-characterized case of solid solution carbon in crystalline I4_{1}/amd metallic hydrogen. Then we demonstrate the equivalence of the molecular dynamics approach as a reliable estimator of thermodynamic properties, and apply molecular dynamics to investigate the planetary-relevant fluid metallic hydrogen. We predict the existence of a new hydrocarbon chemistry, based around a basic sixfold coordination of carbon and threefold coordination of oxygen. For example we observe CH6, C2H8, C3H10, OH3 and CH4OH."
