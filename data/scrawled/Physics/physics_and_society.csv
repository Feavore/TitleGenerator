URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10294v1,Static network structure cannot stabilize cooperation among Large Language Model agents,"Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner’s Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design—to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.","Over the last few years, the advances in artificial intelligence have ignited hopes of new methods for the behavioral and social sciences[1, 2]. In particular, chatbots powered by so-called large language models (LLM)[3] are believed to be able to emulate humans in conversations well enough to eventually replace them in experiments[4]. Even though experiments with human participants are ultimately needed to advance the behavioral and social sciences, there are expectations AI agents could aid in experimental design and provide experimental agents with programmable behavior (so-called “digital twins”) [1, 2]. LLMs are trained on extensive datasets of human-generated text and semantic knowledge from various societies [1, 4]. These models operate as conditional probability distributions, where altering the context or narrative can steer them toward more desirable outcomes by influencing the likelihood of specific responses while reducing others [4]. Consequently, LLMs are highly skilled at following instructions and embodying assigned personalities [5]. When given personalities, LLMs can display traits that resemble human nature, almost as if they possess their mind [6, 7]. This training process can also improve LLMs’ understanding and reasoning regarding cooperation, defection, and balancing individual and collective interests [8]. Recent behavioral experiments have shown that LLM agents can effectively substitute human participants in certain contexts, particularly within Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies [4]. However, this substitution does not extend to other cultural contexts [9]. Beyond cultural differences, within the same cultures, individuals’ social and strategic preferences depend on the network they are part of [10, 11, 12, 13]. Individuals can infer underlying network structures and adapt their social behavior, even without a complete overview of the network. Through interactions with neighbors and social learning, individuals discern the network they are part of, and this structure significantly influences their social behavior. A fascinating question arises: can large language models discern these network structures and adjust their behavior similarly to humans? This inquiry becomes especially intriguing when we consider their potential to help solve societal challenges, like promoting cooperation in social dilemmas. For this vision to come to life, it is crucial that AI agents consistently demonstrate behavior akin to that of humans. We have some understanding of LLMs’ capabilities in repeated prisoners’ dilemma games in well-mixed populations [5, 14, 8, 15, 16, 17], but we lack insight into how they perform in network settings. There, individuals consider not only strategies but also their interactions with specific network members [18, 19]. Previous research has shown that individuals can establish cooperation when they have the opportunity to adjust their social ties based on their experiences with neighbors [19]. Interestingly, it has been found that in a prisoner’s dilemma game, individuals can achieve stable cooperation within specific static network structures when the benefit-to-cost ratio exceeds the number of connections. However, the same parameter settings do not foster cooperation in a well-mixed population. The findings suggest that when individuals are aware of their neighborhood and the consequences of their cooperative actions, they can adapt their strategies based on the success of their neighbors, leading to stable cooperation even in networks with a few defectors [13]. In this study, we investigate whether LLMs can reliably adjust their social behavior in response to the network structure they operate within, as humans do. We study the capabilities of LLM agents, as of 2024, in the context of networked social dilemmas [18, 20, 5]. These stylized situations are designed to explore how humans prioritize between short-sighted egoistic and long-term prosocial choices. Such dilemmas have a wide range of social science applications, from behavioral economics, which explores questions like how to best incentivize people to follow policies, to anthropology and evolution, which search for the unique behavioral mechanisms behind human cooperation among known lifeforms."
https://arxiv.org/html/2411.10227v1,Entropy and type-token ratio in gigaword corpora,"Lexical diversity measures the vocabulary variation in texts. While its utility is evident for analyses in language change and applied linguistics, it is not yet clear how to operationalize this concept in a unique way. We here investigate entropy and text-token ratio, two widely employed metrics for lexical diversities, in six massive linguistic datasets in English, Spanish, and Turkish, consisting of books, news articles, and tweets. These gigaword corpora correspond to languages with distinct morphological features and differ in registers and genres, thus constituting a diverse testbed for a quantitative approach to lexical diversity. Strikingly, we find a functional relation between entropy and text-token ratio that holds across the corpora under consideration. Further, in the limit of large vocabularies we find an analytical expression that sheds light on the origin of this relation and its connection with both Zipf and Heaps laws. Our results then contribute to the theoretical understanding of text structure and offer practical implications for fields like natural language processing.","Entropy and type-token ratio (TTR) are useful metrics for evaluating the richness of lexical variation in writing. The TTR simply measures the proportion of unique words (types) to the total number of words (tokens) in a given text [1, 2], serving as a rough indicator of lexical diversity [3, 4, 5, 6]. The TTR has been originally leveraged in the study of children vocabulary [2, 7] or illnesses which affect linguistic capabilities [8]. However, the notion of TTR is problematic given its dependence on text length. As a consequence, its usefulness reduces when comparing two samples with differing token numbers. Efforts have been made to propose alternatives that show text length independence and are valid for both written and spoken corpora of different registers [9]. In contrast, entropy takes into account the distribution of these words in terms of their occurrence frequencies, capturing both the diversity of lexical items and also how uniformly they are used. As such, understanding the word entropy of texts [10] offers valuable insights into the complexity and unpredictability of language [11]. Originally proposed within information theory [12, 13], in the context of language higher entropy suggests more unpredictability in word choices, which can reveal underlying structures of language use and cognitive processing. Texts with high entropy exhibit greater diversity in word usage, reflecting more complex linguistic structures. Specifically, this measure and related operationalizations have been applied in a broad range of linguistic studies, including differentiation between translated and original documents [14], the study of ambiguity in different legal systems [15], quantification of language change [16], identification of stopwords [17], or observation of change in literary preference [18], among others. Hence, entropy and TTR would address lexical diversity from two different, seemingly independent perspectives. Nonetheless, the production of natural language texts are constrained by statistical laws [19]. These empirical rules highlight patterns that occur in word frequency and vocabulary growth across languages. On the one hand, Zipf law [20] states that a few words are used very frequently, while most are rare. On the other hand, Heaps law [21, 22] dictates that the number of types in a text increases more slowly (sublinearly) than its total number of tokens. These laws, among others [23], help linguists and computational scientists model language, offering insights into communication efficiency and the structure of human language systems. Below, we provide evidence that there exists a functional relation between entropy and type-token ratio. It follows that these diversity measures are not fully independent from each other, and we understand their dependence making use of Zipf and Heaps laws. Quite generally, Zipf and Heaps laws are closely related to the concept of diversity in text analysis. Zipf law, by highlighting the unequal distribution of word frequencies, influences the predictability of words in a text. As mentioned above, this distribution plays a key role in calculating the entropy of natural language texts, where frequent words reduce unpredictability and rarer words increase it. Similarly, Heaps law, which describes vocabulary growth, directly affects the lexical diversity of a text as calculated from the TTR definition. Our work discusses two main findings. First, we uncover an empirical relation between the entropy and the type-token ratio based on our analysis of six gigaword corpora. This contribution is significantly noteworthy since the TTR has typically been computed in texts containing around 10^{5} tokens [4, 6], much smaller than our datasets. While understaning the interplay between the two diversity measures remains a complicated task, the aforementioned relation is consistently found across the studied massive corpora. Second, to gain insight about this correlated behaviour of the entropy and the type-token ratio, we fit an analytical expression for the word entropy to the corpus data. This expression is based on the validity of Zipf law and holds for large vocabularies. Using Heaps law, we express the word entropy in terms of the type-token ratio and fit it to the six massive corpora, finding an excellent agreement with the numerical results. The agreement is observed for languages of different morphological types and sources of distinct genres and registers or styles. This is particularly relevant since it suggests that the entopy-TTR relation is robust against how language users vary their word choices, at least in large corpora. Our findings would then be valuable in optimizing performance and efficiency across various tasks and applications of natural language processing (NLP) and machine learning including quantifying information content, evaluating large language models and enhancing data compression. The remaining of the manuscript is structured as follows. In Section II, we describe the corpora used and analyze their corresponding Zipf and Heaps laws. In Section III we derive an analytical expression for the entropy of large vocabularies and fit this formula to the different datasets, obtaining good results. In Section IV we investigate the type-token ratio in the various corpora using the Heaps law. We combine both diversity metrics in Section V, where we discuss the functional relation that connects H and TTR and its agreement with the empirical results. Finally, Section VI contains our conclusions and suggestions for further work."
https://arxiv.org/html/2411.09520v1,Accounting for carbon capture solvent cost and energy demand in the energy system,"Technical carbon dioxide removal through bioenergy with carbon capture or direct air capture plays a role in virtually all climate mitigation scenarios. Both of these technologies rely on the use of chemical solvents or sorbents in order to capture CO2. Lately, concerns have surfaced about the cost and energy implications of producing solvents and sorbents at scale. Here, we show that the production of chemical sorbents could have significant implications on system cost, energy use and material use depending on how much they are consumed. Among the three chemical sorbents investigated, namely monoethanolamine (MEA) for post-combustion carbon capture, potassium hydroxide for liquid direct air capture and polyethylenimine-silica (PEI) for solid sorbent direct air capture, we found that the production of the compound for solid sorbent direct air capture represent the highest uncertainties for the system. At the high range of solid sorbent consumption, total energy system cost increased by up to 6.5%, while effects for other options were small to negligible. Scale-up of material production capacities was also substantial for MEA and PEI. Implications of sorbent consumption for carbon capture technologies should be considered more thoroughly in scenarios relying on direct air capture using a solid sorbent.","Carbon capture plays a key role in virtually all climate mitigation scenarios limiting the global mean temperature increase to 1.5oC and 2oC compared to pre-industrial levels [1]. Captured carbon can be sequestered to avoid fossil emissions to the atmosphere, or to provide carbon dioxide removal (CDR) if the carbon stems from biomass (BECCS) or direct air capture (DACCS). Alternatively, utilisation of captured carbon (CCU) provides raw material to production of fuels and chemicals, and contributes to increasing carbon efficiency of scarce biogenic carbon [2]. In the IPCC Illustrative Mitigation Pathways that limit warming to 2oC or lower between 168–763 GtCO2 and 0–339 GtCO2 are sequestered in this century through BECCS and DACCS, respectively [1], corresponding to an annual average of 6-26% and 0-11% of 2024 global CO2 emissions [3]. Studies with sector-coupled energy system models also include CCU and arrive at 1-6 Gt annual carbon capture by mid-century globally [4, 5], or 390-800 Mt for Europe [6, 7, 8]. However, carbon capture technology has yet to be proven at scale. 51 Mt carbon capture capacity is in place in 2024, mainly in fossil applications for enhanced oil recovery [9]. 30 Mt bioenergy with carbon capture (BECC) [10] and 65 Mt DAC [11] is being planned world-wide. Besides expansion challenges [12], high cost and energy demand and resource limitations [2], carbon capture requires the use of solvents or sorbents which capture CO2 from either syngas (pre-combustion), flue gases (post-combustion), or from the atmosphere (direct air capture, DAC). While solvents for post-combustion carbon capture have been proven at commercial scale, solvents and sorbents for DAC have been less researched and deployed. Uncertainties regarding costs [13, 14] as well as energy and material requirements [15] related to solvents and sorbents are substantial. For example, Chatterjee & Huang [15] estimated that the energy demand of material production and solvent regeneration alone may amount to 12-20% and 34-51% of global energy demand, respectively, signaling that these aspects deserve a closer analysis. Despite the large role played by carbon capture in modeled mitigation scenarios, uncertainty regarding solvents has thus far received little attention, and costs and energy use of solvent production has not explicitly been taken into account in such studies. This study aims to quantify the effect of cost and energy use uncertainties of solvents and sorbents for carbon capture on the cost-competitiveness of carbon capture technologies and on energy system cost."
https://arxiv.org/html/2411.08066v1,q-Index Degree Distribution in Random Networks via Superstatistics,"In this study, we employ a superstatistical approach to construct q-exponential and q-Maxwell-Boltzmann complex networks, generalizing the concept of scale-free networks. By adjusting the crossover parameter \lambda, we control the degree of the q-exponential plateau at low node degrees, allowing a smooth transition to pure power-law degree distributions. Similarly, the parameter b modulates the q-Maxwell-Boltzmann curvature, facilitating a shift toward pure power-law networks. This framework introduces a novel perspective for constructing and analyzing scale-free networks. Our results show that these additional degrees of freedom significantly enhance the flexibility of both network types in terms of topological and transport properties, including clustering coefficients, small-world characteristics, and resilience to attacks. Future research will focus on exploring the dynamic properties of these networks, offering promising directions for further investigation.","In nonequilibrium statistical mechanics, superstatistical models serve as a robust framework for analyzing complex systems subjected to significant environmental changes and temperature fluctuationsBeck and Cohen (2003) A superstatistical complex system is mathematically characterized by the integration of multiple statistical distributionsAlbert and Barabási (2002), one representing equilibrium statistical mechanics and the other reflecting a gradually varying system parameter. Central to this approach is the requirement for a significant separation of timescales: the local relaxation time of the system should be substantially shorter than the typical timescale of changeBeck et al. (2005). The superstatistical framework has found applications across various complex systems, including hydrodynamic turbulence Beck (2007),frequency fluctuations in power gridsSchäfer et al. (2018),Application to the SYM-H geomagnetic indexSánchez (2024), analyze complex network formation from random graph fluctuationsAbe and Thurner (2006). and air pollution statistics Williams et al. (2020). In past research, the analysis of complex networks has primarily focused on network growth models with preferential attachment, often overlooking those without preferential attachment mechanisms. These models tend to result in different degree distribution forms and network characteristicsSampaio Filho et al. (2023). While these studies have made significant progress in certain areas, they have failed to fully explain the diversity and heterogeneity of real-world complex networks, especially in the context of nonequilibrium dynamics.To address this gap, we propose a model based on superstatistics, drawing inspiration from the theory of Brownian motion in nonequilibrium physics Carro et al. (2016); Thurner and Tsallis (2005); Wedemann et al. (2009). The goal is to reveal the statistical features of complex network structures across different scalesAlbert and Barabási (2002). Specifically, our study explores how concepts such as the q-exponential distribution, the q-Maxwell–Boltzmann distribution, and power-law distributions intertwine and jointly shape the evolution of networks at both local and global scales. Our work not only fills a critical gap in network theory but also provides new insights into understanding self-organization and nonlinear phenomena within complex systems. This approach offers a powerful theoretical tool for future tasks in network optimization and dynamic prediction"
https://arxiv.org/html/2411.08052v1,Mobility-based Traffic Forecasting in a Multimodal Transport System,"We study the analysis of all the movements of the population on the basis of their mobility from one node to another, to observe, measure, and predict the impact of traffic according to this mobility. The frequency of congestion on roads directly or indirectly impacts our economic or social welfare. Our work focuses on exploring some machine learning methods to predict (with a certain probability) traffic in a multimodal transportation network from population mobility data. We analyze the observation of the influence of people’s movements on the transportation network and make a likely prediction of congestion on the network based on this observation (historical basis). Keywords: Optimization, Modeling, Transportation, Mobility, Road Safety, Machine Learning.","The government of Senegal had expressed the importance to transport sector and urban mobility. The vision in transport sector explains the importance of the investments planned in the emerging senegalese plan. In particular, the priority action plan, in relation to urban mobility: the etablishing of the Regional Express Train (TER), the Bus Rapid Transit (BRT), and the reinforcement of the Dakar Dem Dikk fleet (400 city buses out of 475 vehicles) (CETUD, 2024). In addition, we have the continued renewal of the urban fleet of fast buses and ndiaga ndiaye (more than 60%), together with the professionalization of informal or artisanal actors. Urban transport malfunctions (congestion, pollution, and accidents) are recorded in some African countries at about 4% of GDP. The movement of an individual in a network is simply a succession of nodes and arcs. For a given individual, let us assume that we only know his path to a certain node. Many urban areas in the world, especially in developing countries, are facing a rapid increase in population density, that generates a transport demand that cannot be supported by transport infrastructures. Between 1976 and 2022, the population of the Dakar region increased approximately 67 times while at the same time the transport network and urban planning were not sufficiently adapted to this development (ANSD, 2024). This leads to congestion problems and a reduction in urban accessibility defined as the ability to access certain given resources or activities, within a given time frame (Gueye et al., 2015). Is it possible, by analyzing all movements, to predict (with a certain probability) the next node where this individual will go? The scientific fields associated with this question are related to pattern recognition and machine learning. These two fields make extensive use of statistical and operations research techniques. The underlying application is traffic forecasting. This new research direction is based on the idea that there are recurrent trajectory patterns hidden in CDR files, the knowledge of which allows to reduce considerably the amount of files needed. And these patterns can be discovered by Machine Learning on the data corpus (and others). Being able to make such predictions gives a definite advantage on the operational management. This work is positioned on resolution approaches. There are currently few methods, especially exact, to solve this type of problem by considering the system in its globality. Knowing where individuals start from (origins), where they go (destinations), what they do (activities), which paths they follow, and by which means of transport (mode) is essential information whose real-time knowledge can inform urban management policies. We aim at, is a system that exploits in real-time this kind of files in. However, such an objective poses many computational and mathematical challenges. - Challenge 1: Exploration of data mining methods. First of all, the data analysis proposed by Gueye in (Gueye et al., 2015), Baldé in (Baldé et al., 2021; Baldé and Ndiaye, 2016), Kone in (Koné et al., 2019; 2020) consisted of a series of algorithms based on simple and improvable heuristic rules. However, the extraction of knowledge from data is precisely the domain of data mining and multi-agent systems, using statistical techniques. We have not exploited all the possibilities. One of this research directions will be to explore this path. - Challenge 2: Traffic prediction by machine learning. Secondly, having real-time information based on the exploitation of data files must take into account the momentary absence of these files. It is indeed difficult to envisage, because of the construction delays and their sizes, to have a system regularly fed, on very short time steps by data. And capable, on equally short time steps, of extracting the relevant information. We have also experimentally observed, in the challenge, that not all trajectories are relevant to study. Indeed, some of them contain too many information gaps to be exploitable because the individuals are only detected if they receive or send a call/sms or by Google if they have turned on the Location History setting, Google Mobility Data (Aktay et al., 2020). And the trajectories can be grouped into clusters (of trajectories) sharing approximately the same characteristics: almost the same origins, same destinations, same activities, same modes. Only one trajectory among each cluster is then needed to report on all of them. For example, let us assimilate the transportation network to a graph whose arcs represent the roads and nodes their intersections. The movement of an individual in this network is just a succession of nodes and arcs taken. For a given individual given, let us admit that we only know his path to a certain node. Is it possible, by analyzing all the trips, to predict (with a certain probability) the next node where this individual will go? The article is organized as follows. In Section 2, we present the mathematical model, error measurement and some advantages for the Prophet forecasting. In Section 3, we numerically determine the Dakar traffic forecast by analyzing the mobility time series and improving the model prediction. Finally, in Section 4, we present conclusions and perspectives."
https://arxiv.org/html/2411.08046v1,Mathematical representation of bias and nudges centered on intangible goods using quantum information theory,"The purpose of this study is to explore whether the relationship between bias and nudges can be mathematically expressed in terms of quantum information theory, particularly by means of individually customized nudges. Based on the value function of customer satisfaction, which is subject to uncertainty due to the subjectivity of customer evaluations, a model of bias and nudges is proposed that takes into account the environment for intangible goods. Then, by defining an index of nudges from the mathematical properties of the value function obtained from this economic model, a model could be expressed in this study that has a mathematical structure of the same nature as nudges expressed in standard economics, where welfare is impaired by bias from the mathematical structure of the gross social surplus derived as a social welfare function. Moreover, the mathematical structure of the gross social surplus can be made larger than that in standard economics, adding knowledge about the mathematical design of nudges as individually customized customer experiences. This increases the feasibility of the economic model based on quantum information theory and the mathematical design of customized nudges.","Based on quantum information theory, this study examines the subjective level of customer satisfaction with intangible goods intended to provide positive customer experiences and thereby suggests solutions to issues regarding the possibility of constructing a mathematical economic model of bias and nudges and its application to intangible goods, which account for a growing share of the economy. According to Thaler and Sunstein (2008), a nudge is an intervention ""that alters people’s behavior in a predictable way without forbidding any options or significantly changing their economic incentives."" In the framework of nudges by Jimenez-Gomez (2018), individuals experience internalities (costs that might be imposed on the future self), and their choices and welfare depend on the environment, which nudges can alter. He showed that individual preferences do not need to be fully recovered in social planning, which would be difficult to do because of internalities. In heterogeneous populations, the optimal nudge balances the tradeoff between correcting the internalities of biased individuals and the psychological costs of nudges to all individuals (Jimenz-Gomez, 2018).Nudges have been applied in various domains, including nutrition (List and Samek, 2015), retirement savings (Bern Heim et al., 2011), and energy conservation (Allcott and Taubinsky, 2015). Although nudges have been studied and implemented in practical settings, theoretical study of nudges by formal economic analysis remains insufficient. One challenge is that formal analyses typically assume infallible and entirely rational agents, making nudges unnecessary. However, if agents are assumed to be fallible, it becomes possible to analyze settings where nudges can increase welfare by utilizing the concepts of experienced utility and decision utility. The present study uses a standard economics framework to define the difference between decision utility and experienced utility as an internality and mathematically analyzes nudges under several assumptions. The result suggests that for optimal nudges that improve welfare, there is a trade-off between correcting biased individuals’ internalities and the psychological costs of nudges to all individuals, as suggested by Jimenz-Gomez (2018). There is skepticism about nudges, and reasons why their external validity has not been established include that nudges are context-dependent, and that bottlenecks vary across individuals due to their heterogeneity. In social experiments on nudges, evaluations of effectiveness are needed that include measurement of the quantity or proportion of people who took certain actions due to the interventions (Kawasaki and Tadano 2023). Quantum information theory, which lays the foundation for quantum computers, is based on the problem that observation of an object changes its state (Nielsen and Chuang 2000). Quantum decision theory is an application of quantum information theory to social science. A mathematical expression of a person’s irrational decisions has been proposed by Tversky and Kahneman (1992) and studied in the field of behavioral economics (Cheon and Takahashi 2010, Takahashi 2013, Yukalov and Sornette 2017, Fukuda 2022, Fukuda 2023). Fukuda (2022) set up and analyzed an economic model of intangible goods based on the theory of observation of quantum information. That study derived the value function in prospect theory and found its correspondence to principles of behavioral economics, which violates the independence axiom. Thus, it is now increasingly plausible that this approach based on quantum information can potentially be used as an economic model for the theory of designing intangible goods and as a mathematical model for designing customer experiences. However, to the author’s knowledge, no economic analyses of bias and nudges have been carried out that specify the rate of behavioral improvement with a mathematical behavioral economic model of nudges that draws on quantum information theory and enables mathematical representation of context and heterogeneity. In terms of research methodology, this study follows previous work that drew on quantum information theory to quantify customer satisfaction. Here the relationship between bias and nudges are explored and the possibility of mathematically representing individually customized nudges is examined. In light of the unanswered questions in previous studies, this study treats the subjective experience of intangible goods as a rotation in the Hilbert space, which can mathematically represent context and heterogeneity. In doing this, the aim is to provide new insights into the mathematical structure of nudges by specifying the behavioral improvement rate based on a mathematical model of bias and nudges associated with intangible goods that draws on quantum information theory. Given that the share of intangible goods in the economy has rapidly increased in recent years, there is now an urgent need to develop scientific approaches to intangible goods rather than relying on experience and intuition, and attempts such as this study have become increasingly necessary. The findings of this study are as follows. For intangible goods, when the relevant environment is considered, the peak-end rule from behavioral economics emerges when customers’ perception of intangible goods is mathematically entangled with their perception of the relevant environment. The repeat rate is instrumental in understanding bias and nudges, and can serve as an indicator of the rate of behavioral improvement due to nudges. The model analyzed in this study and nudges considered in standard economics share a common mathematical structure. Welfare loss due to bias can be expressed based on the mathematical structure of gross social surplus derived as the social welfare function. Gross social surplus can be larger here than in the case of standard economics. Designing customized customer experiences is possible by mathematically expressing a default effect of nudges. The new findings of this study regarding the mathematical relationships of nudges are based on the relationship between bias and nudges linked to intangible goods (which, in turn, is based on quantum information theory and reflects the relevant environment) and can increase the feasibility of mathematical design of customer experiences by enabling prediction of the rate of behavioral improvement due to nudges. This may contribute to further development of today’s economy, which has seen an increasing share of intangible goods."
https://arxiv.org/html/2411.08608v1,Comparative study of random walks with one-step memory on complex networks,"We investigate searching efficiency of different kinds of random walk on complex networks which rely on local information and one-step memory. For the studied navigation strategies we obtained theoretical and numerical values for the graph mean first passage times as an indicator for the searching efficiency. The experiments with generated and real networks show that biasing based on inverse degree, persistence and local two-hop paths can lead to smaller searching times. Moreover, these biasing approaches can be combined to achieve a more robust random search strategy. Our findings can be applied in the modeling and solution of various real-world problems.","Random walk is a ubiquitous concept that describes wandering in certain space in which the location where the walker will be in the next moment is chosen randomly. In complex networks it can applied for modeling diverse phenomena like searching through information networks [1], diffusion of information, ideas and viruses in social networks, stock market fluctuations, and solving various problems such as page ranking in the web [19], semi-supervised graph labeling [29, 10], link prediction in graphs [2], and graph representation learning [13, 17]. Since the onset of interest in complex networks, various models of random walk on top of them have been proposed. The standard uniform random walk is based on randomly choosing the next node in the walk with equal probability from all neighbors of the node where the walker currently is. By applying master equation approach [18] or Markov chain theory [12] one can obtain theoretical results for a key quantity in the random walk – the mean first passage time (MFPT), that represents the expected number of steps needed for the walker to reach randomly chosen target for the first time. Using the same formalism, various modifications of the uniform random walk have been applied that exploit the local properties of the network, aimed at improving the search time. One approach is based on the degrees of the neighbors [9], particularly when biasing proportionally to the inverse degree of the next node [6, 4]. Some authors have considered local neighborhood exploration by random walks using marking as well as biasing based on neighbors degrees[5]. In another approach memory is applied where the probability to jump to some next node depends on the current, but also on the previously visited one [3, 4, 7]. Other problems that have recently received attention are random walk on networks with resetting [21], multiple simultaneous random walks [20], and random walk on hypergraphs [8]. The theoretical expressions for calculating MFPT in random walks with one-step memory presented in [4] provide a useful testbed that can be employed for comparing various biasing strategies in relatively small networks. Nevertheless, the findings can be then applied to networks with arbitrary sizes. In this work, we aim to study and combine different approaches with local information in order to see whether further improvement is possible. We study five types of random walks with one-step memory: simple forward going, inverse degree biased, two-hop paths based, persistent, and we introduce a combination of persistent and inverse degree biased. For comparison in our study we also include two standard random walks without memory: uniform and inverse degree biased. Our findings can be applied for potential improvements in the study of a wide range of problems mentioned at the beginning of this introduction. In Section II we describe the theoretical expressions for calculating MFPTs in random walks with one-step memory on complex networks represented as graphs. Several graph searching strategies using such random walks are described in Section III. In Section IV we present the results obtained with the theoretical expressions and numerical simulations on several synthetic and real complex networks, while in Section V we give some general conclusions."
https://arxiv.org/html/2411.08387v1,Steady-State and Dynamical Behavior of a PDE Model of Multilevel Selection with Pairwise Group-Level Competition,"Evolutionary competition often occurs simultaneously at multiple levels of organization, in which traits or behaviors that are costly for an individual can provide collective benefits to groups to which the individual belongs. Building off of recent work that has used ideas from game theory to study evolutionary competition within and among groups, we study a PDE model for multilevel selection that considers group-level evolutionary dynamics through a pairwise conflict depending on the strategic composition of the competing groups. This model allows for incorporation of group-level frequency dependence, facilitating the exploration for how the form of probabilities for victory in a group-level conflict can impact the long-time support for cooperation via multilevel selection. We characterize well-posedness properties for measure-valued solutions of our PDE model and apply these properties to show that the population will converge to a delta-function at the all-defector equilibrium when between-group selection is sufficiently weak. We further provide necessary conditions for the existence of bounded steady state densities for the multilevel dynamics of Prisoners’ Dilemma and Hawk-Dove scenarios, using a mix of analytical and numerical techniques to characterize the relative strength of between-group selection required to ensure the long-time survival of cooperation via multilevel selection. We also see that the average payoff at steady state appears to be limited by the average payoff of the all-cooperator group, even for games in which groups achieve maximal average payoff at intermediate levels of cooperation, generalizing behavior that has previously been observed in PDE models of multilevel selection with frequency-indepdent group-level competition.","In various natural and social systems, evolutionary dynamics driven by natural selection or cultural transmission can operate across multiple levels of organization, creating tensions between the evolutionary incentives of individuals and the collectives to which the individuals belong. These tensions between levels of selection arise on scales ranging from genetic conflict within cells [1, 2, 3] to the evolution of cooperative behavior in complex animal societies [4, 5, 6]. Natural selection operating on aggregates of individuals can help to facilitate major evolutionary transitions like the emergence of protocells [7, 8, 9] the origin of chromosomes [10, 11, 12] and the evolution of multicellularity [13, 14, 15, 16], while cultural group selection has been attributed as a mechanism for the promotion of cooperative social norms that facilitate the formation of large human societies [6, 17]. To understand such varied natural phenomena that arise from cross-scale evolutionary competition, it can be helpful to use mathematical modeling to formulate and analyze the tug-of-war between traits or behavior favored at different levels of biological organization. Evolutionary game theory provides a mathematical framework that can be helpful to analyze the conflict between an individual incentive to cheat and a collective incentive to achieve cooperation within a group of individuals. Modeling game-theoretic interactions in group-structured populations provides examples of misalignment between individual-level and group-level interests, as social dilemmas may arise in which individual payoff is maximized by a cheating strategy but the average payoff of group members is maximized when at least some members of a group cooperate. By considering two-player, two-strategy games with a range of payoff matrices, it is possible to formulate a variety of social dilemmas in which cooperation is socially beneficial, but in which individual-level replicator dynamics can favor dominance of defection, coexistence between cooperators and defectors, or bistability of all-defector and all-cooperator states [18, 19]. Depending on the payoff structure of underlying games, it is also possible to explore scenarios in which the average payoff of group members is maximized by a group composed only of cooperators, as well as games for which an intermediate level of cooperation can maximize the collective payoff for a group [20]. A range of mathematical frameworks have been introduced to describe the dynamics of multilevel selection, incorporating different ways to describe group-structured populations and how the evolution of cooperation can be achieved through group-level competition. These frameworks include trait-group models in which collective replication of transiently-formed groups can help to promote altruistic behaviors [21, 22, 23], models with fixed group structure featuring group-level fission or fusion events [24, 25, 26, 27, 28, 29, 30, 31], and spatially explicit models in which group formation and group-level competition emerges via spatial pattern formation [32, 7, 33, 34, 35] or other aggregation processes [16, 36]. The mathematical approaches used to describe cross-scale evolutionary dynamics range from individual-based stochastic models used to describe the fixation or persistence of cooperation due to group-level competition [24, 25, 37, 38, 39] to a variety of PDE models describing multilevel selection that incorporate individual-level evolutionary forces like migration, mutation, and genetic drift [40, 41, 42] or which incorporate detailed group-level events including fission, fusion, and collective extinction of groups [26, 27, 28, 29, 31, 30, 43].Similar nested stochastic models have been further explored to study a range of biological phenomena including host-pathogen coevolution [44, 45], the evolution of cooperative or complementary genetic replicators in protocells [46, 47, 48, 49, 50], and the origin of chromosomes [10]. Luo and couathors recently introduced a stochastic framework for describing evolutionary dynamics featuring individual and collective birth-death competition in group-structured populations [51, 52, 53], modeling finite population dynamics through a nested Moran process and deriving a PDE describing the dynamics of multilevel selection in the limit of large population size. Luo and Mattingly considered the case of two types of individuals in which one type had a fixed advantage under individual-level replication and the other type conferred a collective advantage to their group, showing that beneficial group-level outcomes could be achieved in the long-time behavior a PDE model of multilevel selection when competition among groups was sufficiently strong [52]. Subsequent extensions of these two-level birth-models have explored fixation probabilities in finite populations [38], the existence of quasi-stationary distributions in a diffusive PDE scaling limit of the two-level stochastic process [54, 42], and the formulation of individual-level and group-level replication rates based on two-player, two-strategy social dilemma games played within each group [20, 55]. The resulting hyperbolic PDE models for multilevel selection have been further generalized to study multilevel dynamics with individual-level and group-level replication rates described by arbitrary functions of the fraction of cooperators within each group [56], and results for these generalized models have been applied to explore synergistic effects of group-level competition and within-group mechanisms for promoting the evolution of cooperation [57, 58] and to study models of protocell evolution and the origin of chromosomes [59]. For these two-level replicator equation models, it was possible to use the method of characteristics to determine the long-time behavior for these models of multilevel selection. A particularly interesting feature of the two-level replicator model was a phenomenon described as a “shadow of lower-level selection”, in which the long-time group-level replication rate could not exceed the replication rate of the all-cooperator group, even for scenarios in which the replication rate of groups was maximized by intermediate levels of cooperation [20, 55, 56]. One question of interest for this paper is whether this behavior is limited to the case of previously studied two-level replicator equations, or whether this long shadow cast by lower-level selection can hold for a broader class of PDE models of multilevel selection that incorporate frequency-dependent competition at the group level. While existing work on generalizations of the Luo-Mattingly PDE model of multilevel selection typically assume that group-level replication events occur at rates that depend only on the strategic composition of the replicating group, it is also possible to incorporate frequency-dependent competition at the level of groups by assuming that group-level selection occurs through interactions between competing groups. Questions related to multilevel selection with group-level interactions have often been considered in the evolutionary anthropology literature, arising in models of cultural evolution of behaviors that spread through both individual-level transmission and pairwise conflict between groups. Simulation studies of cultural group selection have been used to study the coevolution of cooperative behaviors and within-group social norms for punishment of defectors or rewarding of cooperators [6, 60, 61]. The simulation model introduced by Boyd and coauthors described competition between groups through a series of pairwise conflicts between groups, in which the probability of group-level victory depends on the strategic composition of the competing groups, with the victorious group producing a copy of itself and replacing the group that lost the pairwise conflict. This form of pairwise between-group competition differs from the models based on the two-level birth-death process proposed by Luo and coauthors, in which the replication rate of groups depended only on the strategic composition of the replicating group. By allowing pairwise conflicts to determine group-level replication events, we are now incorporating frequency dependence in our model of group-level competition, allowing for a more general description of the evolutionary dynamics of group-structured competition featuring selection within and between groups. Such models of pairwise group-level conflict have also recently been applied to study nested models of multilevel selection with density-dependent within-group dynamics [43], and prior theoretical work on intergroup conflict in animal populations [62] suggests substantial room for formulating multilevel selection models with intergroup competition for resources in the presence of individual-level competition within groups. Our goal in this paper is to understand how pairwise group-level competition impacts the dynamics of multilevel selection for various scenarios arising from evolutionary games. We look to explore how the incorporation of group-level frequency dependence impacts both the qualitative behavior and mathematical details of PDE models of multilevel selection relative to existing work on two-level replicator equations that feature group-level replication rates depending only on the strategic composition of the replicating group. This analysis allows us to explore how the tug-of-war between individual level and group incentives plays out in the case of pairwise group conflicts, broadening the scope of analytically tractable models of multilevel selection and highlighting which previously studied behaviors of two-level evolutionary dynamics may be robust to the specific assumptions made by mathematical modelers when formulating a stochastic or PDE model of multilevel selection. In this paper, we study the dynamical and steady-state behavior of a PDE model of multilevel selection that incorporates pairwise between-group competition, expanding on the recent class of PDE models that have assumed frequency-independent group-level replication rates. We provide a measure-valued formulation for the PDE model, using the method of characteristics and a contraction mapping argument to show well-posedness of measure-valued solutions and obtain an implicit representation formula to solutions of the multilevel dynamics. As a first application of this measure-valued representation of solutions, we study the infimum and supremum Hölder exponent of solutions near the all-cooperator equilibrium, which are two properties of the measure-valued solution that can be helpful to characterize the tail-behavior of the strategic distribution of the group-structured population [56]. We are able to show that the infimum and supremum Hölder exponents near the all-cooperator composition is preserved in time for our model, suggesting that these quantities may play a similar role for multilevel dynamics with pairwise group conflicts as they have been shown to do in the case of two-level replicator equations [56]. We then explore the possible long-time outcomes for the multilevel dynamics for the cases of within-group and group-level competition based on generalizations of the Prisoners’ Dilemma (PD) and Hawk-Dove games (HD). For the PD game, we use the representation formula for measure-valued solutions to show that the population will converge to a delta-function at the all-defector equilibrium when between-group selection is sufficiently weak relative to competition within groups. We also explore the possibility of existence of density steady states for the multilevel dynamics for the PD and HD scenarios, deriving necessary conditions for the existence of steady states with given behavior near the all-cooperator equilibrium. These necessary conditions provide us with an expression for the average group-level victory probability such a steady-state population would achieve in pairwise competition with the all-cooperator group, and motivate a conjectured formula for the threshold strength of between-group competition required to achieve a steady state supporting positive levels of cooperation under the dynamics of multilevel selection with pairwise group-level competition. We further explore numerical simulations for our PDE model, observing good agreement between the behavior of numerical solutions and the conjectured expressions for the threshold selection strength and collective success of possible steady-state populations. Notably, we see that the numerical solutions and conjectured analytical formulas suggest that the population may be limited by the collective success of the all-cooperator group even in the limit of infinitely strong between-group competition, suggesting that the shadow of lower-level selection seen in two-level replicator models may also generalize to our PDE models for multilevel selection featuring frequency-dependent group-level competition. We also consider a class of individual and group-level replication rates that generalize the multilevel dynamics for Stag-Hunt (SH) games, with individual-level selection featuring bistability of the all-defector and all-cooperator groups and in which group-level selection most favors the all-cooperator composition. We show that, in the presence of any pairwise group-level competition, the population will converge upon a delta-function at the all-cooperator outcome, so cooperation will achieve long-time fixation in the population when an all-cooperator group is locally stable under individual-level dynamics and is favored under group-level competition. This result provides an analogue to the result of Boyd and Richerson on group selection between alternative stable within-group equilibria [63], which was initially proposed in the context of finite population dynamics with a separation of time-scales between within-group and group-level competition. This result suggests that achieving local stability of a cooperative equilibrium will be sufficient to achieve full-cooperation under our model of multilevel selection with pairwise group conflict, which can be useful for future work exploring synergistic effects between pairwise group-level competition and within-group mechanisms that help to promote and stabilize cooperation with groups. In Section 2, we introduce our PDE model for multilevel selection with pairwise group-level competition and we discuss the game-theoretical background used to generate assumptions on the individual and group replication rates. In Section 3, we present a measure-valued formulation of our PDE model for multilevel selection, providing a characterization of well-posedness and preservation of the tail behavior for measure-valued solutions. We then study dynamical and steady state properties of our model for the case of generalized PD games in Section 4, and provide similar analysis for the generalizations of the HD and SH games in Section 5. We then study numerical solutions to the multilevel dynamics in Section 6, studying dynamical behavior for an example group-level victory probability based on the Fermi update rule and providing evidence for results and conjectures provided in the previous two sections. We present a discussion of our results and an outlook for future work in Section 7, and we provide additional proofs of analytical results and information on our numerical simulations in the appendix."
https://arxiv.org/html/2411.07323v1,Mean-field analysis for cognitively-grounded opinion dynamics with confirmation bias,"Understanding how individuals’ beliefs and attitudes evolve within a population is crucial for explaining social phenomena such as polarization and consensus formation. We explore a persuasive arguments model incorporating confirmation bias, where individuals preferentially accept information aligning with their existing beliefs. By employing a mean-field approach, widely used in statistical physics, we simplify complex processes of argument exchange within the population. Our analysis proceeds by projecting the model onto continuous opinion dynamics and further reducing it through mean-field reasoning. The findings highlight the robustness of mean-field predictions and their compatibility with agent-based simulations, capturing the transition from consensus to polarization induced by confirmation bias.","Understanding how individuals’ beliefs and attitudes evolve within a population is crucial for explaining social phenomena such as polarization, consensus formation, and the spread of misinformation. Opinion dynamics models have been instrumental in providing insights into these processes by simulating the interactions and influences among individuals [1, 2, 3, 4, 5]. Our research explores a persuasive arguments model [6] where agents exchange pro and con arguments, thereby shaping their opinions through social interactions. A key element of the model is the incorporation of confirmation bias, a cognitive mechanism where individuals preferentially accept information that aligns with their existing beliefs while discounting contradictory evidence. This bias is known to play a significant role in real-world opinion dynamics [7, 8, 9, 10], leading to the reinforcement of existing beliefs and the potential for increased polarization within a population [11, 12, 6, 13]. To analyze the complex socio-cognitive interactions within the population, we employ a mean-field approach. Mean-field theory, widely used in physics to simplify complex systems, has been applied in various domains such as epidemics [14] and neural networks [15]. The use of mean-field theory in social dynamics has been extensively reviewed by Castellano et al. [16], highlighting its effectiveness in capturing the macroscopic behavior of social systems from microscopic interactions. Persuasive argument models [17, 18, 19, 6] explicitly model a cognitive layer of arguments. Therefore, model reduction proceeds in two main steps. First, we project the original model [6] onto the space of continuous opinion dynamics [4, 20], deriving an influence response function (IRF, [21, 22]) that governs the expected opinion change. This projection enables a comparison between the reduced and the original model through simulations. Second, we further reduce the model by a mean-field reasoning, dividing the population into two compartments. Using dynamical systems tools, we provide a comprehensive examination of the critical points and transitions in this idealized compartment model. The study demonstrates that mean-field treatment can effectively simplify the analysis of cognitively-grounded opinion dynamics, capturing essential features such as critical points and phase transitions to a high degree of accuracy."
https://arxiv.org/html/2411.07907v1,When Randomness Beats Redundancy: Insights into the Diffusion of Complex Contagions,"How does social network structure amplify or stifle behavior diffusion? Existing theory suggests that when social reinforcement makes the adoption of behavior more likely, it should spread more—both farther and faster—on clustered networks with redundant ties. Conversely, if adoption does not benefit from social reinforcement, then it should spread more on random networks without such redundancies. We develop a novel model of behavior diffusion with tunable probabilistic adoption and social reinforcement parameters to systematically evaluate the conditions under which clustered networks better spread a behavior compared to random networks. Using both simulations and analytical techniques we find precise boundaries in the parameter space where either network type outperforms the other or performs equally. We find that in most cases, random networks spread a behavior equally as far or farther compared to clustered networks despite strong social reinforcement. While there are regions in which clustered networks better diffuse contagions with social reinforcement, this only holds when the diffusion process approaches that of a deterministic threshold model and does not hold for all socially reinforced behaviors more generally. At best, clustered networks only outperform random networks by at least a five percent margin in 18% of the parameter space, and when social reinforcement is large relative to the baseline probability of adoption.","Introduction How does social network structure amplify or stifle behavior diffusion? Existing theory suggests this relationship between structure and diffusion depends on the micro-foundations of how a behavior is adopted from peer to peer in the process of social influence [centola2007complex, guilbeault2018complex]. For some behaviors, the chance of adoption increases as individuals are exposed to multiple influential neighbors who serve as socially reinforcing sources. For other behaviors, the chance of adoption remains constant regardless of the number socially reinforcing neighbors they are exposed to. When the socially reinforced adoption rate is greater than the non-socially reinforced adoption rate, such that a behavior “benefits” from social reinforcement, the behavior is called a complex contagion. Existing theory suggests it spreads more—both faster and farther—on clustered networks [centola2007complex]. On the other hand, if the likelihood of adopting a behavior does not “benefit” from social reinforcement we speak of a simple contagion. In this case, existing theory suggests the behavior will spread more on random networks [centola2007complex, granovetter1973strength, watts1998collective, hebert2010propagation]. These stylized results that form the basis of current understanding describe individuals as deterministic [centola2007complex], subject to changing their behavior based on fixed rules such as adopting a behavior when a threshold of neighbors have adopted the behavior.222The paper that develops the original theory [centola2007complex] does incorporate some probabilistic features including stochastic thresholds as robustness checks but treats individual-level adoption as strictly deterministic. While the idea that complex contagions spread faster and further on clustered networks holds with such additions, it is unclear whether this pattern persists in cases of probabilistic adoption, which is our focus here. However, humans are not deterministic rule followers; they are probabilistic decision-makers. This is supported by many studies of peer influence where non-socially reinforced and socially reinforced adoption are probabilistic [bakshy2012role, bakshy2012social, centola2010spread, romero2011differences, leskovec2007dynamics, lee2022complex, fink2016investigating]. It remains unclear whether this dichotomy between random networks better diffusing simple contagions and clustered networks better diffusing complex contagions generalizes to the probabilistic nature of real human behavior. That is, do these results hold when we move from the notion that adoption depends on social reinforcement (i.e., does not happen without it) to a less restrictive version where adoption is simply more likely when it is socially reinforced?333An additional benefit of probabilistic models besides better aligning with the probabilistic nature of human behavior is that they can account for noise or mistakes in behavioral data (e.g., “trembling hand”). As a result, such probabilistic models can be fit to empirical data that contains observations that have a probability of zero in a deterministic model. In the deterministic case there is a clear advantage to random networks better spreading simple contagions and clustered networks better spreading complex contagions. Random networks are characterized by short path lengths and a lack of clustering. This allows the diffusing behavior to reach a greater number of unique individuals without “wasting” redundant social ties on encouraging the same individual to adopt [watts1998collective, granovetter1973strength]. Conversely, the redundant ties in clustered networks enable repeated exposure to multiple influential neighbors at the expense of reaching fewer individuals [centola2007complex]. Fundamentally, this presents a trade-off. The very lack of clustering in random networks that enables more unique individuals to be exposed is also the clustering that enables redundant exposures. In a deterministic setting, simple and complex contagions fall cleanly on either side of this trade-off. Deterministic simple contagions are equally likely to be adopted from exposure to one as opposed to multiple influential neighbors, so there is no benefit from socially reinforcing, redundant ties. They spread faster on random networks that avoid such redundant ties. Deterministic complex contagions cannot be adopted with exposure to only one adopting neighbor so the ability to reach many unique individuals without redundant exposure is not beneficial [centola2007complex, guilbeault2021topological]. Instead, they can only spread on clustered networks and not random networks [centola2007complex, guilbeault2021topological]. When the decision to adopt a behavior is probabilistic, however, the spread of simple and complex contagions both benefit from the ability to reach more unique individuals through short path lengths and the redundant exposure to influential neighbors [dodds2005generalized, lee2022complex]. This is because, by nature of basic probability, the cumulative probability of adopting a behavior increases with repeated exposures even if the chance of adoption remains constant. Hence, it is unclear whether random or clustered networks are more advantageous to the diffusion of behaviors with probabilistic adoption. Already, a growing body of work introducing probabilistic elements to the canonical deterministic complex contagion model have found instances where the original theory does not hold [eckles2024long, sassine2023does, keating2022multitype, lu2011small, cui2014message, zheng2013spreading, de2009role]. Two recent papers are particularly relevant [eckles2024long, sassine2023does]. Both papers make important contributions by introducing some probabilistic non-socially reinforced adoption in their models, and find that such additions can lead to either faster [eckles2024long] or farther [sassine2023does] spread on random networks in contrast to results by Centola & Macy [centola2007complex]. However, they do not systematically vary probabilistic adoption for both socially reinforced and non-socially reinforced adoption together. Without providing a systematic investigation of the interplay of non-socially reinforced and socially reinforced adoption rates, neither quantifies the conditions under which random networks always spread faster and farther compared to clustered networks. As both papers [sassine2023does, eckles2024long] and other existing studies [de2009role, o2015mathematical, centola2010spread] are choosing exemplar points within the parameter space of stochastic contagions, it also remains unclear how representative certain diffusion patterns are in characterizing complex contagions more generally. Additionally, neither paper includes variable threshold dynamics, or examines analytically how far a behavior spreads based on variable levels of non-socially reinforced and reinforced adoption. To address this, we introduce a novel conceptual model of a contagion process with both tunable probabilistic adoption rates, and social reinforcement parameters. Our model relaxes the deterministic assumption of the original theory [centola2007complex, centola2010spread] and opens up a parameter space of non-socially reinforced and socially reinforced adoption probabilities that describe both stochastic and deterministic simple and complex contagions. Such a model has only been partially explored in past work [lu2011small, cui2014message, zheng2013spreading, de2009role, eckles2024long, sassine2023does, keating2022multitype]. We compare the diffusion of different contagion types, parameterized by the model, on clustered ring lattice networks [watts1998collective] to that of regular random networks constructed by rewiring clustered networks [maslov2002specificity], while holding network size and node degree (number of neighbors each individual in the network has) constant across network types. Using both agent-based modeling and analytical techniques, we are able to identify precise thresholds (or lower bounds of thresholds for certain cases) of adoption and social reinforcement, demarcating regions in which behavior on random networks spreads faster, further, or equally compared to clustered networks. We find that by introducing probabilistic non-socially reinforced and socially reinforced adoption, most instances of complex contagion spread equally or more on random networks even though the behavior exhibits positive social reinforcement. The key mechanism driving this result is that the gains in diffusion from reaching a greater number of unique individuals through the short paths and non-redundant ties of random networks outweighs the gains repeated exposure enabled socially reinforcing, redundant ties of clustered networks. The canonical result by Centola & Macy [centola2007complex] of greater spread of complex contagions on clustered networks only occurs among a small subset of complex contagions, namely those that approach a deterministic spreading process, which are unrepresentative of empirical social contagions [lee2022complex]. This subset shrinks further when individuals have more connections, when an individual needs proportionally more exposure to influential neighbors to themselves adopt, or when an individual remains influential for longer periods of time after adopting a behavior. In summary, that complex contagions spread faster and farther on clustered networks only holds true for specific, highly deterministic, regions of the behavioral parameter space. In most other areas, random networks spread a behavior equally or better. This suggests that greater diffusion on clustered networks is not a defining feature of complex contagions. Past experimental work [centola2010spread] that confirms the original theory, while contributing important and valid insights, may not be entirely representative of complex contagion more broadly when the assumption of deterministic behavior is relaxed. By developing a framework that systematically varies non-socially reinforced as well as socially reinforced adoption probabilities we can clearly demarcate this region of greater spread. This allows us to fully characterize model behavior as a function of other attributes of the network structure and behavior, thus building on other modeling work in the area [eckles2024long, sassine2023does, keating2022multitype, lu2011small, cui2014message, zheng2013spreading, de2009role]. Establishing Micro-foundations of Social Influence: A Model of Stochastic Contagion We introduce a model that describes the micro-level process of social influence, formalizing the differences between simple and complex contagions. All individuals in the network begin having not adopted a behavior (they are “susceptible”), except for several randomly chosen “seed” individuals who have already adopted and can influence their immediate neighbors to adopt (the seeds are “infected” individuals; Figure 1A). Those who have adopted a behavior remain influential towards their neighbors for a set time length (T) after which they can no longer influence others (they are “recovered”). This mirrors the Susceptible-Infective-Recovered (SIR) model from epidemiology [anderson1991infectious, barrat2008dynamical]. For each time step, all susceptible individuals are simultaneously exposed to any neighboring individuals who are currently influential. With every exposure to an influential neighbor, an individual may adopt the behavior with a certain “per-exposure” probability. This per-exposure probability of adoption is defined by p(c), where c indexes the number of different influential neighbors an individual has been in exposed to from the start of the simulation. p(c)=\begin{cases}0,&\text{if }c=0\\ p_{1},&\text{if }1\leq c<i\\ p_{2},&\text{if }c\geq i.\end{cases} All individuals follow this adoption rule identically and there is no heterogeneity among individuals except for network position. When an individual does not have contact with any influential neighbors, they cannot adopt the behavior. If an individual has been exposed to less than c different influential neighbors, they will adopt with a non-socially reinforced probability of p_{1}, which we call the below threshold adoption probability. If the number of different adopting neighbors an individual is in contact with equals or exceeds i, which we call the social reinforcement threshold, an individual adopts the behavior with a socially reinforced probability of p_{2}, which we call the above threshold adoption probability. In practice, even if an individual is exposed to multiple neighbors within one time step, the number of exposures is still counted serially. For instance, if i=2 and an unexposed individual is exposed to three influential neighbors for the first time within one time step, one neighbor “transmits” the behavior with the below threshold probability of p_{1} while the other two transmit the behavior with the above threshold probability of p_{2}. The difference between p_{1} and p_{2} quantifies the amount of social reinforcement the adoption of a behavior is sensitive to, the idea being that multiple exposures reinforce the likelihood of adoption beyond that of the baseline, below threshold adoption rate p_{1}. Setting different values of p_{1} and p_{2} can parameterize behaviors with different levels of below and above threshold adoption rates. This allows us to recover well studied forms of complex and simple contagions, while at the same time allows us to examine overlooked regions of the space (Figure 1C). When p_{1}=p_{2}, the threshold parameter i has no effect and p(c) remains constant across all additional contacts c. Increasing the number of influential neighbors an individual is exposed to does not increase an individual’s per-exposure probability of adoption, so the behavior is considered a simple contagion. The behavior is a deterministic simple contagion when p_{1}=p_{2}=1, and a stochastic simple contagion when 0<p_{1}=p_{2}<1. When p_{1}\neq p_{2}, the behavior is a complex contagion and is sensitive to social reinforcement. Social reinforcement can be positive when the per-exposure adoption probability increases with exposure to multiple influential neighbors, p_{1}<p_{2} (as theorized in complex contagion about costly behaviors such as attending a protest) or negative if exposure to additional influential neighbors somehow dampen each other, p_{1}>p_{2} (e.g., spreading a rumor may become less satisfying if many people already know it). Under both positive and negative social reinforcement, the complex contagion can be deterministic (p_{1}=0,p_{2}=1 in the positive case; p_{1}=1,p_{2}=0 in the negative case) or stochastic (0\leq p_{1}<p_{2}\leq 1 but not including p_{1}=0,p_{2}=1 or p_{1}=1,p_{2}=0). We focus on simple contagions and complex contagions with positive social reinforcement that are either deterministic or stochastic, where p_{1}\leq p_{2}. Among complex contagions with positive social reinforcement, the social reinforcement threshold i parameterizes how many different neighbors an individual must be in contact with in order to adopt at p_{2} instead of p_{1}, “activating” this positive reinforcement effect. Holding constant the total number of neighbors an individual has (formalized as the individual’s degree k), while increasing i increases the costliness of adopting a behavior, in the sense that contact with more socially reinforcing neighbors relative to the total number of neighbors is required to adopt at the higher, above threshold adoption probability. This is not unlike various existing threshold models [schelling1969models, granovetter1978threshold, valente1996social] where individuals adopt a behavior based on whether a certain threshold of neighbors adopts. However, rather than governing deterministic adoption, surpassing i only increases the likelihood of adoption from p_{1} to p_{2}. As we are interested in providing a minimal model that systematically varies adoption and social reinforcement, we model adoption in probabilistic terms while retaining a homogeneous social reinforcement threshold i that serves as a model parameter. The length of time an individual remains influential for after adopting, or what we call the “time of influence” T, models a distinction between behaviors that remain transmissible for longer periods of time as opposed to shorter periods of time. For instance, behaviors that remain highly visible, salient, or relevant over time (such as changing a highly visible profile picture on social media) may exhibit longer times of influence compared to behaviors where visibility quickly diminishes with time (such as changing a highly visible profile picture on social media).444This is similar to incorporating memory parameters into a contagion [dodds2005generalized, cui2014message, sassine2023does]. At the extreme, such a distinction between diffusion processes with longer or shorter times of influence is analogous to the differences between the canonical Susceptible-Infective (SI) model, where the time of influence is infinite, and Susceptible-Infective-Recovered (SIR) model, where the time of influence is some finite value. Research from epidemiology has shown divergent diffusion patterns from SI and SIR models, giving reason to believe that varying time of influence may have a significant role in how a behavior spreads [anderson1991infectious, barrat2008dynamical, dorogovtsev2008critical]. Core to understanding the difference between simple and complex contagions is making a distinction between gains in diffusion from social reinforcement on the one hand, and gains from receiving repeated exposures to influential neighbors on the other. The former, benefiting from social reinforcement, refers to an increase in the per-exposure probability of adoption of a behavior as exposure to the number of influential neighbors increases (adopting at p_{2} instead of p_{1}). This is characteristic of complex contagions with positive reinforcement studied here. The latter, benefiting from repeated exposures to influential neighbors, refers to the extent to which the cumulative probability increases with more exposures, simply from the nature of probability (the chance of observing at least one coin toss to come up heads is higher when we flip two coins than when flipping just one (i.e., p(\texttt{at least one head})=1-(1-0.5)^{2}=0.75)). While benefiting from social reinforcement is only possible when the number of different influential neighbors exceeds the threshold i, benefiting from redundant exposures occur with every exposure, regardless of whether they are from the same neighbor or different neighbors. Non-socially reinforced stochastic simple contagions benefit only from increasing exposure to influential neighbors, but complex contagions with positive social reinforcement benefit from both redundant exposure to influential neighbors and the socially amplified adoption probability p_{2} (when exposures exceed the threshold i). This difference can be formalized by the cumulative probability F(c) of the per-exposure probability of adoption p(c), where F_{C}(c)=P(C\leq c) (Figure 1B). The cumulative probability of adopting a simple contagion can be expressed as, F(c)=1-(1-\beta)^{c} where p_{1}=p_{2}=\beta. When the behavior is a deterministic simple contagion, p_{1}=p_{2}=1, F(c)=1, and the likelihood of adopting the behavior does not increase with additional exposures after the first exposure. However, when 0<\beta<1 and the behavior is a stochastic simple contagion, F(c) increases with additional exposures to influential neighbors, similarly to that of complex contagions, even though the behavior is not more likely to be adopted with socially reinforcement. In the case of stochastic complex contagions though, F(c) increases at a faster rate compared to stochastic simple contagions with the same below threshold probability p_{1}. This is visible in the bottom right panel of Figure 1B: while the simple stochastic contagion experiences increasing cumulative adoption probability from exposure to more influential neighbors (albeit with diminishing returns), the increase for the stochastic complex contagion is higher. Given that both simple and complex probabilistic contagions benefit from repeated exposures through redundant ties, but can also be transmitted along non-redundant ties, it becomes theoretically ambiguous as to whether the presence of clustering and redundant ties would be beneficial for spread in either case. Stochastic simple contagions benefit from redundant exposures, while stochastic complex contagions with non-zero below threshold adoption probabilities can benefit from reaching more unique individuals even through non-redundant ties (Figure 1B). This stands in contrast to the clean cut deterministic case where complex contagions spread better on clustered networks because they only benefit from redundant ties, and simple contagions spread better on random networks because they only benefit from non-redundant ties. The relative strengths of these two effects, gains from redundant ties as opposed to gains from non-redundant ties, will determine which network spreads behavior “better”. By exploring this model, we will show that socially reinforced complex contagions spread farther and faster on clustered networks only in a small area of the p_{1}\leq p_{2} parameter space whereas in the majority of the parameter space the random network either performs equally or better. We additionally test the effects of differing degree (k), social reinforcement threshold (i), and time of influence (T; see Methods). Figure 1: Micro and Macro Views of Diffusion on Clustered and Random Networks. A. The seeding structure and early time diffusion in random and clustered networks, with example parameters k=6 and i=3. Random networks are able to reach more individuals (ki-i+1 in the first time step), but all adopt at the lower, below threshold adoption rate p_{1}. Clustered networks reach less individuals (k in the first time step) but more individuals receive reinforcing signals and may adopt at the above threshold, p_{2}. This illuminates a fundamental trade-off of having more or less redundant ties. B. Per-exposure and cumulative adoption probabilities for stochastic and deterministic simple and complex contagions. Deterministic simple contagions do not benefit from social reinforcement, but deterministic complex contagions, as well as stochastic simple and complex contagions do. C. The space of all possible p_{1}\leq p_{2} values that uniquely define a behavior, or adoption trajectory (p(c),F(c))."
https://arxiv.org/html/2411.07771v1,Efficiency of energy-consuming random walkers: Variability in energy helps,"Energy considerations can significantly affect the behavior of a population of energy-consuming agents with limited energy budgets, for instance, in the movement process of people in a city. We consider a population of interacting agents with an initial energy budget walking on a graph according to an exploration and return (to home) strategy that is based on the current energy of the person. Each move reduces the available energy depending on the flow of movements and the strength of interactions, and the movement ends when an agent returns home with a negative energy. We observe that a uniform distribution of initial energy budgets results in a larger number of visited sites per consumed energy (efficiency) compared to case that all agents have the same initial energy if return to home is relevant from the beginning of the process. The uniform energy distribution also reduces the amount of uncertainties in the total travel times (entropy production) which is more pronounced when the strength of interactions and exploration play the relevant role in the movement process. That is variability in the energies can help to increase the efficiency and reduce the entropy production specially in presence of strong interactions.","Human everyday activities (e.g., commuting between home and workplace, or shopping) are specific kinds of recurrent diffusion process in which agents travel from one place to another and then return to their starting point after following a trajectory with a number of steps of various lengths Gonzalez ; Scafetta2011 ; Barbosa . An exploration and return (ER) strategy is usually used to model such movements in social and ecological systems song-2010 ; Majumdar2010 ; Benichou2011 ; Pappalardo ; wang2022 . This basic principle has been generalized in many studies to better describe for instance the social, economical, and geometrical aspects of these systems gonzalez-2015 ; vazifeh2021 . In the past few years, the emergence of location tracking devices (e.g., GPS navigator and smart phones), and location-based services (e.g., Foursquare, Yelp checkin and Google places) provides good opportunities to study human mobility patterns at very different spatial and temporal scales gallotti-2012 ; barthelemy-2019 ; bettencourt-2021 ; batty-2021 ; gonzalez-2022 : from mobility of individuals inside a city to mobility and transportation in an entire country Chowel ; Brockmann ; Vespignani ; gonzalez-2023 . As a consequence, important progresses have been made from reconstruction of population density, mobility patterns and flows Phithakkitnukoon ; Kitamura ; Peng , traffic forecasting and urban planning Nagel ; Wang ; Rozenfeld , marketing campaign and prediction of epidemics Fibich ; Pastor-Satorras , to designing of mobile network protocols Chaintreau . Efficiency of structure and dynamical processes is essential to maintain a sustainable system like a city horner-2002 ; newman-2006 ; dong-2016 ; latora-2018 ; indaco-2019 ; barthelemy-2022 . However, the majority of research on this subject have focused on simulating the statistical characteristics of human mobility, such as displacement and gyration radius. We know that energy, used by human body or vehicle, plays a key role in transportation and other forms of social activities Kolbl ; boyer-2009 ; Wang22 . It has been observed that the average travel times for different transportation modes (e.g. walking, cycling, bus, or car travel) are inversely proportional to the (physiological) energy consumption rates measured for the respective physical activities Kolbl . Interestingly, when daily travel-time distributions of different transport modes are appropriately scaled, they turn out to have a universal functional relationship Kolbl ; kolbl-2021 . In this work we are going to investigate the effects of energy consumption and limited energy budgets on a measure of efficiency and entropy production in a movement process which is based on the ER strategy. We see how constraints on energy budgets can influence the movements of these interacting and energy consuming agents, for instance resulting in a subdiffusion regime. In particular, we are interested in the efficiency of such a movement process and its relation to a measure of entropy or uncertainty production in such a process indaco-2020 ; indaco-2021 . We study the number of distinct locations visited by a person per the consumed energy. A greater number of demands are expected to be fulfilled during an urban exploration when a larger number of distinct sites are visited. A reasonable definition of efficiency should also take into account the energy consumed by the agent in this process. Additionally, we are interested in the uncertainty in the travel times which is generated by interactions between the agents. Smaller uncertainties in the travel times are expected to result in a better planning and therefore smaller dissipation. The paper is organized as follows. We start in Sec. II.1 with a definition of the model and the main parameters. In Sec. II.2 we study an effective one-dimensional model and write a master equation in the continuum limit which can easily be generalized to higher dimensions. A naive mean-field approximation to the equation is presented and the results are compared with the exact solutions of the master equation in one dimension. Section II.3 is devoted to the numerical simulations of the model in two dimensions. Concluding remarks are given in Sec. III."
https://arxiv.org/html/2411.06282v1,Two scholarly publishing cultures? Open access drives a divergence in European academic publishing practices,"The current system of scholarly publishing is often criticized for being slow, expensive, and not transparent. The rise of open access publishing as part of open science tenets, promoting transparency and collaboration, together with calls for research assesment reforms are the results of these criticisms. The emergence of new open access publishers presents a unique opportunity to empirically test how universities and countries respond to shifts in the academic publishing landscape. These new actors challenge traditional publishing models, offering faster review times and broader accessibility, which could influence strategic publishing decisions.Our findings reveal a clear division in European publishing practices, with countries clustering into two groups distinguished by the ratio of publications in new open access journals with accelerated review times versus legacy journals. This divide underscores a broader shift in academic culture, highlighting new open access publishing venues as a strategic factor influencing national and institutional publishing practices, with significant implications for research accessibility and collaboration across Europe.","One of the mainstays of evaluating the performance of universities is their performance in research, and a major plank of that evaluation is constituted by publication in academic journals. Likewise, the metrics-based evaluation of individual academics follows similar processes shaped by pressures to publish in high-impact journals. Researchers may choose open access venues to increase visibility and compliance with open science mandates, while universities and national science systems might adapt their evaluation criteria, balancing prestige with the growing importance of transparency and public access. This dynamic provides fertile ground for studying how institutions adjust their incentives and how these shifts affect researchers publishing strategies. Researchers often prioritize journal prestige and citation counts, sometimes at the cost of research quality and broader societal impact. Institutional policies and national funding systems frequently reward publication volume and impact factor, reinforcing this trend. Recent studies show a growing disconnect between researchers’ values, which may favor openness and integrity, and institutional incentives focused on metrics. Performance-based funding models and evolving open science practices reveal how current systems reshape research behavior, raising concerns about the sustainability and ethics of research evaluation. This paper examines the impact of these forces on publishing practices and researcher behavior. We have previously discussed the controversies involved in using publication metrics in academic journals to evaluate individual academics [1] and many of those issues applying to the use of publication in academic journals apply to the evaluation of universities. Building on the pressures associated with metrics-based evaluations, recent studies suggest that the dominance of journal impact factors and the resulting “publish or perish” culture may further undermine research quality. Bohorquez et al. [2] found that pressure to publish in prestigious journals can lead researchers to adjust findings to fit publication standards, often at the expense of comprehensive evidence. Researchers under significant pressure may prioritize journal prestige and rapid publication timelines over open-access and transparency considerations, as Johann et al. [3] describe, opting for instrumental rather than normative publication strategies. Furthermore, Ross-Hellauer et al. [4] examined the phenomenon of “value dissonance,” where researchers’ commitment to open and responsible research increasingly conflicts with institutional demands for high-impact publications, favoring citation metrics over collaborative and ethical practices. Additionally, Baccini et al. [5] showed how bibliometric-driven evaluations encourage behaviors like self-citations and strategic citation practices, ultimately fostering a citation-centric approach that may detract from genuine scientific impact. These findings underscore the need to reassess research assessment policies, highlighting the growing gap between institutional metrics and researchers’ values, as well as the importance of aligning evaluation criteria with the principles of open science and research integrity. The interplay between country-level research reforms and researchers’ publishing choices was highlighted in studies by Cernat [6] and Dagienė et al. [7], revealing how policy-driven metrics reshape academic behavior. Cernat’s analysis of Romania’s 2016 reforms, which imposed strict publication criteria amidst funding cuts, led to a focus on high-impact journals at the expense of conference proceedings, ultimately reducing overall research productivity. This case exemplifies the misalignment between top-down policy intentions and researchers’ capabilities under constrained resources. Similarly, Dagienė et al.’s study on Lithuania’s performance-based funding system shows how the push for indexed journal publications has spurred strategic publishing behaviors, emphasizing quantity over quality. These cases illustrate the influence of diverse stakeholders—scientific elites, policymakers, universities, and researchers—in shaping research assessment policies, raising concerns about the sustainability and genuine innovation fostered by metrics-driven funding. Here, we investigate the evolution of academic publishing practices in Europe by analyzing the publishing data on universitiy and country level in the European Union, focusing on the ratio between publications in new, open access journals (MDPI in our case), and those in traditional, legacy journals (here, The Big Five). By examining the distributions of this ratio, we identify two distinct groups of universities and countries with different scholarly publishing cultures. We show how publishing choice correlates with innovation potential and corruption perception at the country level, revealing the broader socio-economic context that shapes academic publishing practices. Our findings reveal significant insights into the current state and evolving trends of scholarly publishing practices among universities and EU countries."
https://arxiv.org/html/2411.06011v1,Exploring the impact of reflexivity theory and cognitive social structures on the dynamics of doctor-patient social system,"Conventional economic and socio-behavioural models assume perfect symmetric access to information and rational behaviour among interacting agents in a social system. However, real-world events and observations appear to contradict such assumptions, leading to the possibility of other, more complex interaction rules existing between such agents. We investigate this possibility by creating two different models for a doctor-patient system. One retains the established assumptions, while the other incorporates principles of reflexivity theory and cognitive social structures. In addition, we utilize a microbial genetic algorithm to optimize the behaviour of the physician and patient agents in both models. The differences in results for the two models suggest that social systems may not always exhibit the behaviour or even accomplish the purpose for which they were designed and that modelling the social and cognitive influences in a social system may capture various ways a social agent balances complementary and competing information signals in making choices.","Conventional economic and behavioural studies have long assumed that an agent has complete access to information and adheres to rational behaviour to model interactions between social agents [KorobkinUlen2000, Soros2013]. However, evidence challenging these assumptions has accumulated, pointing to other, more plausible theories [BromileyPapenhausen2003, Crotty2017]. This paper presents a novel model of social system design and analysis. Our model incorporates ideas from the theory of reflexivity [Soros2013] and cognitive social structures [Krackhardt1987a] offering a socio-cognitive approach to understanding the behaviours of individual social agents and emergent social phenomena. Reflexivity theory, considered formally proposed for economic analysis by George Soros [Soros2013],[Umpleby2018], is similar to the concepts found in the studies and literature of second-order cybernetics [Scott2004]. The theory, in a nutshell, suggests that social agents and the environment, which constitute a single system, are involved in a feedback loop with not just negative feedback, as usually considered to be the case in classical economic analysis of markets and social phenomena, but also positive feedback to and from the agents and the environment driving the overall system towards a specific, attractor state [Davis2020]. The critical insight from the theory that we have utilized in our model is that feedback loops between the agents and their environments change both the agents and the environments; once the agents act, the environments that they are in change as well, which influences the following actions the agents take and so it continues. Furthermore, the social agents can only access their own subjective realities of these changes, which are different from an objective reality (See Figure 1). These subjective realities can often drive many social phenomena, including business cycles and stock market bubbles [Beinhocker2013],[Soros2013], despite their possible deviations from the objective reality. Figure 1: Outline of Reflexivity Theory [Soros2013] Cognitive social structures, an extension of the theory of social structures, was instrumental in understanding that social agents are situated in a social network with relationship dynamics and that the position of social agents in a network influences the perceptions, decisions and actions of each agent in that network [Brands2013],[Krackhardt1987a]. Once again, similar to what we observe in the theory of reflexivity, the social agents do not have direct access to objective reality according to this framework. Instead, social agents perceive their interactions with others through a subjective lens, forming the basis for their actions and decisions. However, the critical difference between the two theories is that while the theory of reflexivity explores the dynamics of the interactions of social agents in a system, cognitive social structures uncover the matrix of relationships that influence the subjective perceptions of each agent [Frank2015]. Our primary motivation behind pursuing this research is the intuition that socially embedded, reflexive agents have different sets of behaviours that lead to the emergence of social phenomena which are closer to what we observe in our social systems compared to what we observe from classical economic and sociological models where rationality and perfect information assumptions are made. It is essential to realize and recognize that perceptions of individual agents are real in their consequences, even if there is no direct, one-to-one relationship between observed behaviours and objective reality. To flesh out our intuition, we compare and contrast observations from the simulation of two types of models, which we call the ”classical” and ”cognitive social system”, respectively. In particular, we explore the behaviours of doctors and patients in a primary healthcare network. We have chosen this social system partly because of the abundance of literature regarding the choices and behaviours of patients and doctors [Djulbegovic.etal2014],[Harris2003],[Kozikowski.etal2022] and also because of its utilitarian nature; better comprehension of this system would lead to more robust and precise healthcare services that will help in saving lives and improving standards of living [Cabrera.etal2011],[Comis.etal2021]. The classical model employs perfect information and rational behaviour assumptions, while the cognitive social system integrates the theory of reflexivity and cognitive social structures. We want to find out whether the best doctors, as determined by certain traits in the models, such as their credentials and abilities to conduct research, among others, get the most patients. In classical models, the best doctors would ideally have the best reputation because patients would know the best doctors and choose them accordingly to receive treatments. However, in the cognitive social system, the best doctors may not have the most favourable reputations because of social ties, which modulate the perception of their abilities and, thus, alter the judgements of patients choosing them. We implement a microbial genetic algorithm to optimize the agents in both models [Harvey2011]. We use this variation of the genetic algorithm because both of our models are relatively small in scale, and using a complete, conventional genetic algorithm may detract from the focus of our analysis. While a microbial genetic algorithm is more skeletal than a conventional one, it is more than adequate for our purposes in this paper. Ultimately, we are convinced that cognitive social systems can be generalized to understand other social systems, such as those found in areas such as education and defense, among others, and even design new ones. The potential of cognitive social system modelling to capture the dynamics of agents’ interactions embedded in social systems more accurately than conventional models is immense, as long as the modeller can rely on sound literature and verify the models with well-founded data."
