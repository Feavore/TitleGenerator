URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10428v1,BICEP/ XIX: Extremely Thin Composite Polymer Vacuum Windows for BICEP and Other High Throughput Millimeter Wave Telescopes,"Millimeter-wave refracting telescopes targeting the degree-scale structure of the cosmic microwave background (CMB) have recently grown to diffraction-limited apertures of over 0.5 meters. These instruments are entirely housed in vacuum cryostats to support their sub-kelvin bolometric detectors and to minimize radiative loading from thermal emission due to absorption loss in their transmissive optical elements. The large vacuum window is the only optical element in the system at ambient temperature, and therefore minimizing loss in the window is crucial for maximizing detector sensitivity. This motivates the use of low-loss polymer materials and a window as thin as practicable. However, the window must simultaneously meet the requirement to keep sufficient vacuum, and therefore must limit gas permeation and remain mechanically robust against catastrophic failure under pressure. We report on the development of extremely thin composite polyethylene window technology that meets these goals. Two windows have been deployed for two full observing seasons on the BICEP3 and BA150 CMB telescopes at the South Pole. On BICEP3, the window has demonstrated a 6% improvement in detector sensitivity.","The cosmic microwave background (CMB) is the afterglow from the Big Bang, and as such, is an excellent tool to explore cosmological models (Penzias & Wilson, 1965; Planck Collaboration et al., 2020a). CMB photons—emitted as the opaque primordial plasma cooled into transparent neutral gas—carry information about the conditions of the early universe. The CMB has a near-perfect black body spectrum, ruling out a steady state expanding universe, and the temperature deviations on that spectrum provide strong evidence for the standard model of cosmology, \LambdaCDM (Wright et al., 1994; Spergel et al., 2003; Planck Collaboration et al., 2020a). There remain unresolved questions about the early universe (such as the horizon and flatness problems) that paradigms like Inflation attempt to explain. Currently, several experiments are aiming to constrain the parameters of Inflation by mapping the CMB. A very small excess of primordial “B modes” (curl in the polarization field of the CMB) at degree scales is an expected prediction from Inflation in the early universe (Kamionkowski & Kovetz, 2016). As constraints on cosmological parameters advance, there is a need to improve sensitivity of the telescopes making these measurements. There are broadly two ways to accomplish this: increase detector counts while keeping the per-detector sensitivity the same or decrease the optical loading thus increasing the per-detector sensitivity. These two goals lead to conflicting physical designs. Increasing the detector count requires a larger aperture size, which leads to thicker optical components. Since the optical emission from a component increases with its thickness, larger apertures tend to degrade the per-detector sensitivity. The BICEP/Keck series of experiments are small-aperture on-axis refracting telescopes, designed to optimize high optical throughput at degree scales. This strategy has been successful in placing leading constraints on the millimeter polarized signal at degree scales for the past twenty years (BICEP/Keck Collaboration et al., 2021). Each generation of BICEP/Keck receivers has increased the number of detectors by roughly an order of magnitude: BICEP1 had a 250 mm aperture for 49 detector pairs at 95, 150, and 220 GHz (Takahashi et al., 2010); BICEP2 (150 GHz) and the Keck Array of five receivers (150 GHz, later adding 95, 220 and 270 GHz) had 256 detector pairs each for their 264 mm apertures (BICEP2 Collaboration et al., 2015; Kernasovskiy et al., 2012; Staniszewski et al., 2012); the currently operating BICEP3 (95 GHz) has 1200 detector pairs in a 520 mm aperture (BICEP/Keck Collaboration et al., 2022). A single BICEP Array receiver—the set of receivers currently being deployed which range in frequency from 30 to 300 GHz—has between 100 and 4000 detector pairs (depending on the designed frequency) for a 560 mm aperture (Moncelsi et al., 2020; Schillaci et al., 2023; The BICEP/Keck Collaboration et al., 2024). To accommodate the order of magnitude increase in detector count between the recent stages of BICEP receivers the aperture size of the receivers has doubled from 264 mm to between 520–560 mm (BICEP2 and Keck Array Collaborations et al., 2015; Moncelsi et al., 2020). The readout electronics and transition edge detectors are carefully designed so that the receivers are photon-noise limited. Therefore reducing in-band optical load can have a profound impact on a given BICEP receiver’s per-detector sensitivity. Figure 1: [Left] Modeled optical loading from a window with thicknesses of 1.4 mm, 25.4 mm, and 31.8 mm, at band centers from 30 to 270 GHz (fractional bandwidth of 0.25) through a BICEP-style instrument. Black points are measured optical loading on BICEP3 from a 31.8 mm (1.25”) UHMWPE spare window and the 1.4 mm laminate window deployed on BICEP3 for the 2023 season. Shaded bands are uncertainty on the loss properties of the window, with \tan\delta between 0.6\times 10^{-4} to 2.4\times 10^{-4}. [Right] Modeled relative noise equivalent temperature per detector (NET{}_{\text{det}}) and relative survey time increase compared to an instrument without a window, for each of the BICEP Array observing band center frequencies. The in-band radiative load emitted by the optical elements is the product of their absorptive loss and physical temperature. The absorption loss of a material is directly related to the complex permittivity (\varepsilon=\varepsilon^{\prime}-i\varepsilon^{\prime\prime}) of a material. Absorption is discussed numerically throughout this paper as a material’s loss tangent, which is the ratio of the real part over the complex part of the permittivity, denoted as \tan\delta. The permittivity of a material varies with temperature, and the \tan\delta at millimeter wavelengths of many polymers decreases with cryogenic cooling, reducing the absorption and emission of a polymer optic in-band (Frank et al., 1977; Schnabel, 2014). However, the reduction in optical emission at lower temperature is driven primarily by the lower physical temperature rather than the lower loss (Lamb, 1996). The entire instrument is placed within a vacuum cryostat to cool both the optics and the sub-kelvin detectors. However, the cryostat obviously requires a transmissive window to observe external signals, which must remain in contact with the ambient environment. Vacuum windows must have low transmission loss in band and be strong enough to withstand the force of atmospheric pressure. Previously, millimeter-wave vacuum windows have been made of ceramics [such as fused-silica (Datta et al., 2021)], plastic foams [including various formulations of Zotefoam (BICEP2 and Keck Array Collaborations et al., 2015)], or bulk plastic materials [primarily high density polyethylene (HDPE) or ultra high molecular weight polyethylene (UHMWPE) (BICEP/Keck Collaboration et al., 2022; D’Alessandro et al., 2018)]. A more detailed history of the development of millimeter vacuum windows can be found in Barkats et al. (2018). The larger aperture sizes of modern CMB receivers, however, limit the materials available for use. BICEP3 and BICEP Array have nominal clear apertures at the window of 730 mm, while the full outer diameter of the window is 900 mm. Current plans for the CMB-Stage 4 Small Aperture Telescopes (SATs) and Large Aperture Telescope (LATs) include similarly sized windows (Abazajian et al., 2019). Ceramics become prohibitively expensive at those scales. Plastic foams would become impracticably thick and lossy: the primary advantage of the foam windows are their low index of refraction and low transmission loss, but they have the disadvantage of being very weak. A suitably thick foam window would include many laminated layers, which would make their loss properties significantly worse. For example, the 150 GHz Keck receiver windows were 120 mm thick and estimated to cause \lesssim2% transmission loss (BICEP2 and Keck Array Collaborations et al., 2015); the doubled aperture size for new BICEP receivers would require a foam window four times thicker. A window with such high transmission loss (\lesssim8%) would add approximately 3 pW optical load to a 150 GHz detector, swamping out all other sources of optical power at that frequency. Bulk plastics, being relatively weak though still stronger than plastic foams, must also become significantly thicker to maintain a suitable safety factor for a vacuum vessel (discussed further in Section 2.2). For BICEP3, a 95 GHz receiver, we previously estimated that over half of the instrument loading originated from the 31.8 mm thick HDPE slab window (BICEP/Keck Collaboration et al., 2022). That window has since been replaced by the thin window presented in this paper. In Figure 1 we model the optical loading on a large aperture BICEP-style receiver for six observed bands and different window thicknesses. We also show the relative change in per-detector white noise and survey time for different window thicknesses. For the lowest frequency band, at 30–40 GHz, decreasing the window thickness only yields marginal improvements in white noise and mapping speed. For bands above 40 GHz, however, a substantial reduction in window thickness from the nominal could potentially decrease noise by tens of percent. At the high frequencies in the 270 GHz band, such a window thickness reduction could potentially decrease the survey time by as much as 50%. The window is the only element in the optical chain where improvement would produce such a significant return, primarily due to the window’s relatively high temperature. We have achieved an order of magnitude reduction in window thickness using a special form of polyethylene we call high modulus polyethylene (HMPE, commercial name Dyneema (Dyneema, 2008) or Spectra) laminated with low density polyethylene (LDPE) to generated a polyethylene composite material. The high strength of the HMPE allows for thicknesses of around a millimeter, in the same range as the wavelength of light we are observing, which will become relevant for the optical properties of the window. The LDPE melts at lower temperature than the HMPE, allowing for the laminating material to fully surround the HMPE fibers without compromising the strength carrier’s integrity (Vasile, 2005; Eiben et al., 2022). Because the composite is entirely polyethylene, we expect the window to appear homogeneous to millimeter light, and therefore have similar optical properties—such as index of refraction, loss and scattering—as standard bulk polyethylene. As of 2024, we have deployed two thin laminate windows on BICEP/Keck receivers; one as a retrofit on the 95 GHz BICEP3 and one on the new 150 GHz BICEP Array receiver (hereafter BA150). Section 2 of this paper discusses the design considerations and mechanical tests for these windows. Section 3 explores in-lab validation of optical properties such as polarization and anti-reflection. Section 4 reviews optical characterizations performed on the deployed window before and after installation on BICEP3. We then conclude with final thoughts and next steps."
https://arxiv.org/html/2411.09790v1,The Last Arecibo Message,"The Arecibo Message was a brief binary-encoded communication transmitted into space from the Arecibo Observatory on November 16, 1974, intended to demonstrate human technological prowess. In late 2018, to commemorate the 45th anniversary of this message, the Arecibo Observatory initiated the New Arecibo Message competition. Following a series of challenges, our Boriken Voyagers team was recognized as the winner of the competition in August 2020. Although the primary objective of the competition was to conceptualize rather than transmit a message, the collapse of the Arecibo Telescope in December 2020 precluded any subsequent transmission efforts. Therefore, to commemorate the 50th anniversary of the Arecibo Message, this paper presents the Last Arecibo Message, as originally developed for the Arecibo Telescope. If the original message says we are a form of life reaching out to connect, our message says we are ready to explore the universe together. The prospect of transmitting this or a similar message remains an open question.","On November 16th, 1974, Frank Drake and the staff at the Arecibo Observatory sent the most powerful broadcast ever pointed into deep space at that time (Staff at the National Astronomy & Ionosphere Center, 1975). This secured their legacy not solely in historical records and scholarly articles, but also in the consciousness of everyone seeking solutions to humanity’s most pressing inquiries. Since the original message was broadcast, technology has progressed in multiple ways. Today, the world is more connected than ever. We constantly use radio frequencies to communicate with others worldwide, making them a cornerstone of our modern society. For other civilizations in the galaxy, radio transmissions might be just as important. However, we have not detected any sign of extraterrestrial intelligence. Maybe, in order to find extraterrestrials, we have to establish contact first. Any message meant to establish contact with extraterrestrial civilizations must undergo some scrutiny to overcome the challenges of interstellar communication. The content, the possibility of being detected, and the risk associated with sending a message must be addressed before being transmitted. This seemingly simple concept makes this concept challenging, as the receiving side must be able to decode the information we send them. Some assumptions must also be taken to model the message in a way that is universal and easier to translate and detect. Our Boriken111Boriken is the indigenous given name to Puerto Rico by the aboriginal Taino/Arawakan people. Voyagers team proposed a message capable of being decoded based on the constraints and assumptions required for the New Arecibo Message challenge in 2018. In this paper, we describe our thought process for the content of the message, as well as a target visible from the Arecibo Telescope, another of the requirements of the competition. Since the telescope collapsed in December 2020, this was the last message developed by the Arecibo Observatory."
https://arxiv.org/html/2411.09729v1,The SDSS-V Local Volume Mapper (LVM): Data Analysis Pipeline,"We introduce the Data Analysis Pipeline (DAP) for the Sloan Digital Sky Survey V (SDSS-V) Local Volume Mapper (LVM) project, referred to as the LVM-DAP. We outline our methods for recovering both stellar and emission line components from the optical integral field spectroscopy, highlighting the developments and changes implemented to address specific challenges of the data set. The observations from the LVM project are unique because they cover a wide range of physical resolutions, from approximately 0.05 pc to 100 pc, depending on the distance to the targets. This, along with the varying number of stars sampled in each aperture (ranging from zero, just one of a few, to thousands), presents challenges in using previous spectral synthesis methods and interpreting the spectral fits. We provide a detailed explanation of how we model the stellar content and separate it from the ionized gas emission lines. To assess the accuracy of our results, we compare them with both idealized and more realistic simulations, highlighting the limitations of our methods. We find that the DAP robustly correct for stellar continuum features and recover emission line parameters (e.g. flux, equivalent width, systemtic velocity and velocity dispersion) with a precision and accuracy that fulfill the requirements of the primary goal of the analysis. In addition, the recovered stellar parameters are reliable for single stars, the recovery of integrated populations is less precise. We conclude with a description of the data products we provide, instructions for downloading and using our software, and a showcase illustrating the quality of the data and the analysis on a deep exposure taken on the Huygens region at the center of the Orion Nebula.","The Local Volume Mapper (LVM, Drory et al., 2024) is one of the mappers that comprises the Sloan Digital Sky Survey V (SDSS-V, Kollmeier et al., 2017, Kollmeier in prep.). The core science goal for the LVM is to quantify stellar feedback physics at the energy injection scale (<10 pc), connecting ionizing stars with ionized gas diagnostics of the physical conditions in the interstellar medium (ISM). To achieve this goal, it is currently conducting a unique Integral Field Spectroscopy (IFS) survey of the Milky Way, the Large and Small Magellan Clouds, and nearby galaxies in the Local Volume using a new facility comprising 4 alt-alt mounted celiostats, 16 cm refractive telescopes, a lenslet-coupled fiber-optic system, and an array of spectrographs covering 3600-9800 Å at R\sim 4000. The ultra-wide field science Integral Field Unit (IFU) has a diameter of 0.5 degrees with 1801 35.3″ clear apertures in a hexagonal arrangement of 25 rings. The celiostat design ensures stationary optics and a stable fiber system, mitigating issues in traditional fiber systems (e.g. Law et al., 2021). This innovative instrument will provide over 55\times 10^{6} spectra, covering the Milky Way at spatial resolutions of 0.05 to 1 pc, the Magellanic Clouds at 10 pc resolution, and very nearby galaxies of large apparent diameter at 100 pc. Survey operations began in November 2023 and will continue through May 2027. Following the precedent of previous SDSS surveys, all raw and reduced data, and the data products from dedicated analyses, as well as the codes, will be made publicly available, starting with SDSS DR20 in late 2025. Early science data have demonstrated the LVM’s capability to deliver high-quality spectral mapping of Galactic nebulae (Kreckel et al., 2024), crucial for addressing its core science goals and providing new insights into the energy injection scales within Local Group galaxies. To achieve these goals and digest the amount of data produced in these unique observations, it is required to create dedicated pipelines to reduce and analyze the data. In this article we present the first version of the LVM data analysis pipeline (DAP, version 1.0.0). The main goal of this tool is to automatically analyze the reduced, extracted and calibrated spectra provided by the Data Reduction Pipeline (Mejía-Narváez et al., in prep), decoupling the stellar and ionized gas emission line components, and providing observational and physical parameters of both components. There are several tools and pipelines already available in the literature that perform similar analyses (e.g., PPxF, MaNGA-DAP, PyCASSO, Fit3D, pyPipe3D, Cappellari & Emsellem, 2004; de Amorim et al., 2017; Westfall et al., 2019; Lacerda et al., 2022, for citing just a few). However, none of them has been developed to explore spectra that sample targets at the small physical scales of the LVM. At those scales it cannot be guaranteed that the number of stars within a single aperture is large enough to apply stellar population spectral synthesis, the basic physical reasoning behind all these tools (Walcher et al., 2011; Conroy, 2013). We have implemented in the DAP a new approach that we consider valid when the requirement to sample a fully populated IMF is not fulfilled. In this approach, instead of modelling the stellar component using single stellar populations (SSP), we adopt combinations of representative stellar spectra valid for these resolved stellar populations (RSP). Our primary goal with the DAP is to robustly recover emission line properties (flux, equivalent width, systematic velocity, velocity dispersion), accurately correcting for underlying stellar continuum features. Our secondary goal is to recover stellar parameters associated with the stellar continuum (Teff, log(g), [Fe/H], [\alpha/Fe]), flexibly allowing for either small or large combinations of individual stars. In this article we aim to: (i) present the new methodology required to model the stellar spectrum when the number of stars in the considered aperture is not sufficiently large to apply the stellar synthesis method; (ii) present the procedure adopted to create the RSP template library propagating the probability distribution functions (PDFs) of the physical parameters that are represented by each template; (iii) describe and distribute the first operational version of the DAP, that is able to recover the ionized emission line properties (its primarily goal) and to obtain a preliminary model for the stellar component; (iv) present the proposed format with which we will distribute the dataproducts, distributing some example codes of how to use them; (v) characterize the quality of the recovered parameters based on simulated and real data, highlighting the current limitations and suggesting possible future improvements; and (vi) show the implementation on real LVM data. The code we present here is working end-to-end, however we acknowledge that it will require additional updates throughout the course of the survey, as it is usually the case in this kind of project. For example, a refinement in the input parameters (e.g., RSP templates) can flexibly be implemented in future applications of the DAP. The structure of this article is as follows: a summary of the data to be analyzed is presented in Sec. 2, with the implemented methodology being described in Sec. 3, including an overview of the analysis sequence in Section 3.1, details on the adopted stellar component analysis and how the stellar templates are created in Sec. 3.3, and a description of the delivered data products in Sec. 3.4; the reliability of the methodology is tested with extensive simulations described in Sec. 4, including both physically motivated (Sec. 4.1) and purely empirical (Sec. 4.2) simulations; the accuracy and prevision in the recovery of the emission lines and the stellar population properties are described in Sec. 4.3, 4.4, and 4.3; an example of the use of the DAP is presented in Sec. 5.1, in which we analyze a deep exposure centred in the Huygens region of the Orion nebula, including a description of this particular dataset in Sec. 5.2, with a summary of the performed analysis results in Sec. 5.3, a detailed exploration of the generated deep integrated spectrum in Sec. 5.4, the list of detected emission lines in Sec. 5.5, the physical properties derived from those emission lines in Sec. 5.6, and the spatial distribution of the emission line fluxes presented in Sec. 5.7; how to download the code is presented in Sec. 6, and finally a summary of the main results of this study is presented in Sec. 7."
https://arxiv.org/html/2411.10409v1,Design of Dedicated Tilt-to-Length Calibration Maneuvers for LISA,"Tilts of certain elements within a laser interferometer can undesirably couple into measurements as a form of noise, known as Tilt-To-Length (TTL) coupling. This TTL coupling is anticipated to be one of the primary noise sources in the Laser Interferometer Space Antenna (LISA) mission, after Time Delay Interferometry (TDI) is applied. Despite the careful interferometer design and calibration on the ground, TTL is likely to require in-flight mitigation through post-processing subtraction to achieve the necessary sensitivity. Past research has demonstrated TTL subtraction in simulations through the estimation of 24 linear coupling coefficients using a noise minimization approach. This paper investigates an approach based on performing rotation maneuvers for estimating coupling coefficients with low uncertainties. In this study, we evaluate the feasibility and optimal configurations of such maneuvers to identify the most efficient solutions. We assess the efficacy of TTL calibration maneuvers by modulating either the spacecraft (SC) attitude or the Moving Optical Sub-Assembly (MOSA) yaw angle. We found that sinusoidal signals with amplitudes of around 30\text{\,}\mathrm{nrad}\text{/} and frequencies near 43\text{\,}\mathrm{mHz}\text{/} are practical and nearly optimal choices for such modulations. Employing different frequencies generates uncorrelated signals, allowing for multiple maneuvers to be executed simultaneously. Our simulations enable us to estimate the TTL coefficients with precision below 15\text{\,}\mathrm{\SIUnitSymbolMicro m}\text{/}\mathrm{rad} (1-\sigma, in free space) after a total maneuver time of 20 minutes. The results are compared to the estimation uncertainties that can be achieved without using maneuvers.","LISA is a space-based Gravitational Wave (GW) detector that will operate within a measurement band ranging from approximately 0.1\text{\,}\mathrm{mHz}\text{/} to 1\text{\,}\mathrm{Hz}\text{/} [1]. The detector is composed of three SC, arranged in an almost equilateral triangle with 2.5 million \mathrm{km}\text{/} arm length, that will trail behind the Earth in a heliocentric orbit. The LISA mission is led by the European Space Agency (ESA) and was recently adopted for an expected launch in the 2030s. Each SC will host two MOSA, each comprising a telescope, an Optical Bench (OB) and a Test Mass (TM). Each MOSA will be pointed towards one of the two remote SC. The MOSA will be designated using the notation shown in Fig. 1, where MOSA ij refers to the assembly on SC i facing SC j. The lengths of the three arms of the LISA constellation will vary with time, unlike ground-based GW detectors. This leads to the interferometric measurements being highly affected by laser frequency noise. To mitigate this noise, the TDI [2] algorithm will be applied to generate TDI output variables, which mimic three virtual equal-arm interferometers. In this study we examine the second generation Michelson X,Y,Z combinations. These variables will be affected by TTL coupling, i.e. they contain error terms which depend on the MOSA tilt angles with respect to (w.r.t.) the incident beam. This TTL should be estimated and subtracted from the measurements in post-processing. In this paper we investigate the possibility of using rotation maneuvers to estimate the TTL coefficients, i.e. the parameters of the TTL model. By injecting a modulation signal into the angles that cause TTL, the signal for the fit is enhanced. This can reduce the uncertainty in the TTL coefficient estimation. This option could be used if TTL is less well separable from other noise terms or GW signals than anticipated. In such a case, the maneuvers could serve as a beneficial backup plan. It may also be decided to perform such maneuvers once within the commissioning phase. TTL maneuvers have already been performed in the LISA Pathfinder (LPF) mission, cf. [3, 4]. A comparable approach has successfully been used in the GRACE Follow-On (GFO) mission and is considered for future geodesy missions as well [wegener_phd, wegener_2020]. Other sources addressing TTL in LISA include [5, 6, 7, 8, 9, 10]. Wanner et al. [5] provide a comprehensive analytical description of TTL in the individual interferometers of LISA as well as in the TDI Michelson variables. In [6] and [7], it is described how the TTL error can be estimated through noise minimization and subtracted from the TDI variables, utilizing pointing angles measured by Differential Wavefront Sensing (DWS) [11]. George et al. [8] apply a Fisher information matrix analysis to derive lower bounds for the uncertainty with which the TTL coefficients can be estimated and use these to analyze the residual TTL noise after post-processing subtraction. Figure 1: LISA SC constellation, MOSA index notation. Image credit: [6]. In [9], the observability of TTL in the TDI Michelson variables is shown by propagating the TTL contributions through the TDI algorithm. The two options of estimating the TTL coefficients with or without rotation maneuvers are discussed. Periodic maneuvers at frequencies outside the LISA measurement band are considered, in order not to degrade the science measurements. Thus, large amplitudes are required, however, the feasibility of such maneuvers is not discussed. This study is extended in [10] by additionally considering GW signals in the measurements and introducing a separation of TDI variables that allows performing TTL maneuvers without disturbing science operations. The maneuvers discussed in [10] are stochastically generated, instead of periodic stimuli. A quantitative analysis of the estimation error is performed, however, a rather long integration time of 15 hours was assumed. This paper follows a different approach of designing dedicated TTL maneuvers, focussing on sinusoidal stimuli at frequencies within the LISA measurement band. We investigate what angular amplitudes are achievable when implemented via SC or MOSA rotations. A detailed analysis of the estimation uncertainty shows a strong dependency on the maneuver frequency, which can be optimized subsequently. In order to maximize the efficiency, we develop a plan to perform several maneuvers simultaneously. This facilitates very good estimation of the TTL coefficients after an integration time of merely 20 minutes. With simulations we quantify the improvement that such maneuvers provide over the noise minimization approach. The notation and the TTL model are defined in Sec. II. In Sec. III, the simulator settings are specified. The parameter estimation method is briefly described in Sec. IV. Section V on the maneuver design is the main part of this paper. In particular, we discuss the optimal maneuver frequency, and how multiple maneuvers can be performed simultaneously. The simulation results are reported in Sec. VI."
https://arxiv.org/html/2411.10140v1,Early Detection of Multiwavelength Blazar Variability,"Blazars are a subclass of active galactic nuclei (AGNs) with relativistic jets pointing toward the observer. They are notable for their flux variability at all observed wavelengths and timescales. Together with simultaneous measurements at lower energies, the very-high-energy (VHE) emission observed during blazar flares may be used to probe the population of accelerated particles. However, optimally triggering observations of blazar high states can be challenging. Notable examples include identifying a flaring episode in real time and predicting VHE flaring activity based on lower energy observables. For this purpose, we have developed a novel deep learning analysis framework, based on data-driven anomaly detection techniques. It is capable of detecting various types of anomalies in real-world, multiwavelength light curves, ranging from clear high states to subtle correlations across bands. Based on unsupervised anomaly detection and clustering methods, we differentiate source variability from noisy background activity, without the need for a labeled training dataset of flaring states. The framework incorporates measurement uncertainties and is robust given data quality challenges, such as varying cadences and observational gaps. We evaluate our approach using both historical data and simulations of blazar light curves in two energy bands, corresponding to sources observable with the Fermi Large Area Telescope, and the upcoming Cherenkov Telescope Array Observatory (CTAO). In a statistical analysis, we show that our framework can reliably detect known historical flares.","Observations of transient phenomena are key to disentangling the physical processes at play in astrophysical systems. In the past years, notable transient events include the spatial correlation of an astrophysical neutrino with a flaring gamma-ray source (Aartsen et al., 2018), the observation of gamma rays and gravitational waves produced by a kilonova (Abbott et al., 2017), and the discovery of new classes of very-high-energy (VHE; >100 GeV) gamma-ray emitters such as gamma-ray bursts and novae (MAGIC Collaboration et al., 2019; H. E. S. S. Collaboration et al., 2019; Aharonian et al., 2022) via detection of transient emission. Measuring or constraining the very-high-energy (VHE) gamma-ray emission from transient events is of particular interest, as these gamma rays track the most extreme acceleration processes. Imaging atmospheric Cherenkov telescopes (IACTs), such as VERITAS, MAGIC, H.E.S.S., and the next generation Cherenkov Telescope Array Observatory, are the most sensitive instruments for measuring VHE gamma-ray emission (Park, 2015; Aleksić et al., 2016; Aharonian et al., 2006; Acharya et al., 2013). However, they have fields of view of less than 10\degree, which limits the chance of serendipitous transient detection. Source variability detected at other wavelengths is therefore used to trigger IACT observations, increasing the probability of observing transient events with IACTs. The deep learning approach presented here collates multiwavelength (MWL) information on the activity state of known gamma-ray sources and uses this information to anticipate periods of unusual emission (e.g., flares) in the VHE band. In developing this method, we focus on blazars, a gamma-ray source class which shows strong variability on all observed timescales and wavelengths, and for which a large archive of VHE and MWL observations exists. 1.1 Blazars Blazars are a class of active galactic nuclei with a relativistic jet oriented towards the observer, resulting in strongly Doppler-boosted emission. They are the most numerous source class detected in high energy (HE; 100 MeV–100 GeV) and VHE gamma rays. However, the mechanisms driving particle acceleration in the jets and the observed gamma-ray emission are not well-understood, and models of varying complexity abound (for discussion of acceleration see e.g., Kirk et al. (2000); Sironi et al. (2015); for emission see Cerruti (2020) and references therein). Gaining a better understanding of the underlying acceleration and emission mechanisms in blazars is relevant for a number of topics. For example, blazars are plausible sources of ultra-high-energy cosmic rays, should protons be accelerated in their jets (e.g., Murase et al. (2012); Rodrigues et al. (2018)). As extragalactic sources, located at cosmological distances from Earth, blazar observations can be used to probe the photon and magnetic fields traversed en route to Earth, and to test for signatures of Lorentz invariance violation and photon coupling to axion-like particles, effects that are expected to grow with gamma-ray propagation distance (see Biteau & Meyer (2022) and references therein). For such scientific goals, precise measurements of the broadband spectral energy distributions of sources (SEDs) is critical, being the main handle for interpretation of the emission mechanisms. The photon emission from all VHE-detected blazars is characterized as variable at some or all observed wavelengths, from radio to VHE, on timescales from minutes to years. Bright states are of particular interest, but it is challenging to simultaneously detect a source and measure the SED over multiple wavelengths. This is due in part to limitations on collection areas of instruments in different wavelengths, as well as to the intrinsic variation in flux as a function of energy. Sources that are typically too dim to be detected in the HE and VHE bands without long integration periods (weeks to months) can conversely be detected during flaring episodes within minutes or hours. The SED of a variable blazar is modelled most robustly using simultaneous MWL observations, in order to ensure that different time-dependent flux states are not conflated. Bright states are key to probing the emission mechanism of blazars, and to their physical interpretation. Particularly important is the VHE band, where the photon flux is comparatively low. Flare detection is therefore a cornerstone of the scientific programs of the IACT community (see Aharonian et al. (2007); Albert et al. (2008); Acciari et al. (2009); Abeysekara et al. (2015) for selected examples). Identifying correlated variation at different wavelengths is also valuable, as different models of blazar emission predict correlations (or lack thereof) between different bands. However, consistent definition and identification of flares remains a challenge in the community, as e.g., discussed by Zimmerman et al. (2024). 1.2 Deep Learning and Anomaly Detection Machine learning and, in particular, deep learning, are widely used in astronomy. Many applications are based on supervised learning. This involves input data which are labeled, where the algorithm is trained to encode the mapping between inputs and labels. Examples include classification, such as \gamma/hadron separation for IACT experiments (see e.g., Feng & Lin, 2016; Nieto Castaño et al., 2017), as well as regression tasks, e.g., the evaluation of the redshift of a galaxy (Sadeh et al., 2016). Conversely, for unsupervised learning, specific labels are not known a priori and the objective is to find patterns in the data. Applications are commonly based on clustering (Min et al., 2018) and/or outlier detection (Reis et al., 2018; Malanchev et al., 2021). In the following, we expand on the work of Sadeh (2020) (hereafter SA20), who utilized a recurrent neural network (RNN) for outlier detection. An RNN is a form of a directed graph, representing a sequence of steps in time. Outputs from each time step are fed as input to the next, in addition to the respective temporal data. Such models are useful for characterization of complex data on different timescales. SA20 illustrated the use of their method for the detection of various types of astrophysical transients, such as gamma-ray bursts and neutrino emission by candidate neutrino point sources. The general concept is to use an RNN to characterize the background to a potential transient. This is done by providing the network with existing/past data, taken before the emergence of the putative transient event. For instance, in the case of the search for an astrophysical neutrino transient, the background represents the continuously observed signals of atmospheric neutrinos. The purpose of the RNN is to predict the background within the near future (upcoming time steps of the network), based on observations of the near past. Potential transients are then detectable as deviations of new observations from the expected background predictions. For their chosen datasets, SA20 could construct RNN inputs that increase with the intensity of putative signals. For example, they used the reconstructed gamma-ray flux within a region of interest from an IACT experiment, which increases when a new gamma-ray source appears. As such, transients would always manifest themselves as upward fluctuations from the background predictions of the network. Considering observables that by construction always scale up with the strength of the signal enables the definition of a simple outlier score, acting as a test statistic (TS) for detection. The latter is defined by SA20 as the integrated difference between the predictions of the RNN and the stream of real data. In the current work, we focus on blazar flares as the target transient phenomena. However, we set out to identify any unexpected activity from the source, such as notable downward deviations or correlated changes of the emission on different timescales. For this purpose, the simple test statistic used by SA20 is not appropriate, given that it is designed exclusively for upward deviations from the background. In order to construct a generalized test statistic for arbitrary types of flares, we expand the architecture of the network used by SA20, as discussed in the following. This paper is organized as follows. In Sect. 2, we introduce our machine-learning based framework and in Sect. 3 we describe how it is trained. Sect. 4 contains a simulation study, in which we evaluate the sensitivity of our method. In Sect. 5 we demonstrate the utility of our method in an analysis of real data from BL Lacertae, before concluding in Sect. 6."
https://arxiv.org/html/2411.10124v1,Modeling beam chromaticity for high-resolution CMB analyses,"We investigate the impact of beam chromaticity, i.e., the frequency dependence of the beam window function, on cosmological and astrophysical parameter constraints from CMB power spectrum observations. We show that for future high-resolution CMB measurements it is necessary to include a color-corrected beam for each sky component with a distinct spectral energy distribution. We introduce a formalism able to easily implement the beam chromaticity in CMB power spectrum likelihood analyses and run a case study using a Simons Observatory (SO) Large Aperture Telescope-like experimental setup and within the public SO software stack. To quantify the impact, we assume that beam chromaticity is present in simulated spectra but omitted in the likelihood analysis. We find that, for passbands of fractional width \Delta\nu/\nu\sim 0.2, neglecting this effect leads to significant biases, with astrophysical foreground parameters shifting by more than 2\sigma and cosmological parameters by significant fractions of the error.","The increase in precision in Cosmic Microwave Background (CMB) observations which we have witnessed over the last three decades [see e.g., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] has demanded a large effort in developing data analysis pipelines able to account for precise characterization of the instruments and of the sky emission. This work is essential to make sure that the very tight constraints set on cosmological models are robust and unbiased. For example, the subsequent releases of the Planck mission data (from the initial early survey results to the latest NPIPE products) have seen the deployment of many techniques to reduce systematics arising from e.g., uncertain instrument performance or the scanning strategy, and contamination from Galactic emission [11, 12, 13, 14, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]. While the Planck satellite focused on getting the most robust large scale modes (\ell\lesssim 1500-2000), from the ground, experiments like the Atacama Cosmology Telescope (ACT) and the South Pole Telescope (SPT) have refined intermediate and small angular scales (1000\lesssim\ell\lesssim 10000). The requirement for those scales is to tackle in particular the astrophysical foreground emission from extra-galactic sources and other unresolved signals [25, 6, 26, 27], and the frequency-dependent systematics effects which couple with them. If ignored, these systematics can lead to incorrect estimates of foreground parameters – preventing their astrophysical and cosmological interpretation [28, 29, 30, 31, 32, 33, 34]– and potentially introduce biases in the estimation of cosmological parameters. Next-generation ground-based experiments like the Simons Observatory (SO) [35] and CMB-S4 [36] are actively developing these aspects of the analysis pipeline [37, 38, 39, 40, 41]. A crucial ingredient for a correct estimate of CMB and foreground emission is the accurate knowledge of the beam, which represents the optical response of the instrument. Its width encodes the resolution of the instrument and its azimuthal and polar profile depends on the entire optical chain of the instrument. It is customary, in particular for high-resolution analyses, to use the assumption of azimuthally-symmetric beams [42], which holds well for the main beam component and simplifies the beam to its radial profile b(\theta), function of the polar angle \theta. This works in the case of a redundant scanning strategy, where each pixel is observed from many angles and the averaged beam gets symmetrized. Usually beam profiles are estimated employing observations of planets or known sources [43, 44] and, for broad passbands, the resulting beams can strongly depend on source spectral type. Thus, the effective beam profile may vary for different sources of emissions. However, in many previous cosmological analyses this effect has often been neglected [22, 45] or not included in the baseline analysis because too small to significantly impact cosmological parameter inference111The ACT DR4 analysis computed an approximate color-correction to adjust the beam estimated from Uranus observations to the beam appropriate for the CMB and used those in the baseline likelihood. Color-corrections for the other sky components were neglected. Tests assessing the impact of beam chromaticity were done as a robustness test of the cosmology results [6]; finding negligible impact on cosmological parameters and O(\lesssim 1\sigma) in astrophysical parameters, the extended beam modeling was not used in the baseline likelihood. [5, 6], particularly given the sensitivity or the multipole range of the experiment. Only some recent component separation studies [46, 38] included this effect for the first time in the baseline analysis settings. As we show in this paper, with the increasing sensitivity of upcoming experiments, it will be essential to accurately measure and incorporate the frequency dependence of beam profiles into the analysis pipeline. This will allow proper modeling of the observed power spectra and ensure an unbiased recovery of both cosmological and foreground parameters. In this work, we lay out the formalism for the integration of beam chromaticity in the power spectrum and likelihood analysis of a CMB experiment, and show the potential impact of this effect on the recovery of parameters from an SO Large Aperture Telescope (LAT)-like experiment. We derive the mathematics needed for the integration of this term in the calculation of the foreground Spectral Energy Distributions (SEDs), and we implement the modeling of the chromatic beams in the public SO power spectrum likelihood code LAT_MFLike222LAT_MFLike, version 1.0.0 and its foreground spectrum library fgspectra333fgspectra, version 1.3.0. The paper is organized as follows. We describe our formalism in Section II, then present its implementation and results on how the chromatic beam effect can bias cosmological and foreground parameters in Section III. We then draw conclusions in Section IV."
https://arxiv.org/html/2411.09733v1,Fuzzy Gasoline: Cosmological hydrodynamical simulations of dwarf galaxy formation with Fuzzy Dark Matter,"We present the first set of high-resolution, hydrodynamical cosmological simulations of galaxy formation in a Fuzzy Dark Matter (FDM) framework. These simulations were performed with a new version of the gasoline2 code, known as fuzzy-gasoline, which can simulate quantum FDM effects alongside a comprehensive baryonic model that includes metal cooling, star formation, supernova feedback, and black hole physics, previously used in the NIHAO simulation suite. Using thirty zoom-in simulations of galaxies with halo masses in the range 10^{9}\lesssim M_{\text{halo}}/M_{\odot}\lesssim 10^{11}, we explore how the interplay between FDM’s quantum potential and baryonic processes influences dark matter distributions and observable galaxy properties. Our findings indicate that both baryons and low-mass FDM contribute to core formation within dark matter profiles, though through distinct mechanisms: FDM-induced cores emerge in all haloes, particularly within low-mass systems at high redshift, while baryon-driven cores form within a specific mass range and at low redshift. Despite these significant differences in dark matter structure, key stellar observables such as star formation histories and velocity dispersion profiles remain remarkably similar to predictions from the Cold Dark Matter (CDM) model, making it challenging to distinguish between CDM and FDM solely through stellar observations.","The Cold Dark Matter (CDM) model, characterized by its cold, dark, and collisionless nature, has been considered the leading framework for explaining the dark matter component in cosmic structure formation over the past few decades (see e.g. Mo et al., 2010, for a comprehensive review on the subject). Nonetheless, unresolved tensions at small scales, combined with the ongoing failure to detect Weakly Interacting Massive Particles (WIMPs) — the leading particle candidate of the CDM model — have continued to raise doubts about the model’s viability. Motivated by the elusiveness of WIMPs in predominant direct and indirect detection methods, several alternative dark matter models have come to the forefront, investigating the lower mass regimes for dark matter particles (Jungman et al., 1996). Moving away from the GeV/c2 mass range associated with WIMPs, these efforts explored and proposed several lighter dark matter particle candidates, one being the axion particle, which is theorized to arise from the CP-symmetry breaking in quantum chromo-dynamics (QCD) theories (Peccei & Quinn, 1977). In a cosmological context, a pseudo-scalar bosonic particle can be generalized from the QCD axion model, motivating a comprehensive class of axion-like particles (ALPs) acting as dark matter candidates. These ALPs span a broad range of masses, encompassing over 24 orders of magnitude from 10^{-24} to 1 eV/c2 (see e.g. Hui et al., 2017; Ferreira, 2021, for reviews on FDM models). Dark matter models related to ALP particle masses in the mass range (10^{-24} to 10^{-19} eV/c2) are known as Fuzzy Dark Matter (FDM) models, whose identifying boson mass m_{\chi} is typically represented in terms of m_{22}=m_{\chi}/(10^{-22}\text{ eV}/c^{2}). The mass range of FDM corresponds to de Broglie wavelengths on scales of \mathcal{O}(1\text{ kpc}), exhibiting wave-like behavior at sub-galactic scales (Hu et al., 2000). The quantum wave-like nature of FDM results in a net repulsive force that, on one hand, modifies the matter power spectrum of cold dark matter (CDM) during matter-radiation equality and smooths out density perturbations at small scales, ultimately leading to fewer collapsed structures (Hu et al., 2000; Marsh & Silk, 2014). On the other hand, it induces a resistance to gravitational collapse resulting in decreased dark matter (DM) distribution in the central region of FDM haloes. This effectively translates to FDM haloes featuring cored inner DM density profiles (\rho(r)\sim constant) contrasted with CDM’s cuspy inner DM density profiles (\rho(r)\sim r^{-1}) for dwarf galaxy systems (Hu et al., 2000). While the CDM model has been successful in modeling large-scale cosmological structures (Springel et al., 2005; Tegmark et al., 2006; Alam et al., 2017), several challenges have arisen on smaller, non-linear scales. These include well-known issues such as the cusp-core problem (Flores & Primack, 1994; Moore, 1994) and the missing satellites problem (Klypin et al., 1999; Moore et al., 1999) [see Bullock & Boylan-Kolchin (2017) for a detailed review]. Verifying the model’s validity at these non-linear scales has proven to be particularly challenging. In response, numerous studies have defended the CDM model, pointing out that earlier works overstated the severity of these problems due to theoretical and observational limitations. These studies emphasize the growing importance of baryonic physics in structure formation on smaller scales (Brooks & Zolotov, 2014; Macciò et al., 2020; Waterval et al., 2022), as well as the inefficiency of star formation in dwarf galaxies, which complicates their observational detection (Yang et al., 2003; Fitts et al., 2017; Frings et al., 2017). Previous studies investigating the role of baryonic feedback processes in dwarf galaxies have found that baryons are able to produce significant cores (\sim 1 kpc) in their dark matter distribution (Governato et al., 2010; Macciò et al., 2011; Benítez-Llambay et al., 2019). The most-widely accepted mechanism explaining this phenomenon is the sub-dynamical time-scale changes in the central (\sim\bigO(\text{kpc})) potential of the halo. These rapid changes in the central potential, caused by stellar and black hole feedback, are tied to strong gas outflows that irreversibly alter the central potential by transferring energy to collisionless DM particles (Pontzen & Governato, 2012). However, these baryonic effects help alleviate these small scale tensions only up to a certain mass scale (M_{halo} \sim 10{}^{10}M_{\odot}). Since these mechanisms are out of play in lowest mass, gas-deficient dark-matter dominated dwarf galaxies, the central DM distribution of the halo reverts back to the cuspy profiles (e.g. Tollet et al., 2016). The addition of FDM interaction to baryonic effects might help alleviate these tensions at lower halo masses while maintaining CDM large scale features. Numerical simulations of structure formation within FDM models have been initially performed by means of highly numerically intensive Adaptive Mesh Refinement (AMR) algorithms able to solve the Schrödinger-Poisson equations over a grid (see e.g. Schive et al., 2010, 2018; Mocz et al., 2017), leading to impressive and very detailed results on the properties of individual FDM collapsed objects (see e.g. Woo & Chiueh, 2009; Schive et al., 2014; Veltmaat et al., 2018). However, the computational cost of such approach hindered the possibility to extend the investigation of late time structure formation to large cosmological volumes. To address this issue, N-Body codes were employed, initially only including the (linear) suppression in the initial conditions but neglecting the integrated effect of the FDM interaction during the subsequent dynamical evolution (see e.g. Schive et al., 2016; Iršič et al., 2017; Armengaud et al., 2017) – i.e. basically treating FDM as standard dark matter with a suppressed primordial power spectrum. The inclusion of the typical FDM interaction in N-body codes was achieved with ax-gadget Nori & Baldi (2018), a modified version of the cosmological hydrodynamical code p-gadget3 that implemented the general scheme suggested by Mocz & Succi (2015) and Marsh (2015). The code ax-gadget allowed the investigation of FDM in larger cosmological volumes with a vast number of systems (Nori et al., 2019) as well as a variety of complex galactic systems with many evolving substructures (Nori & Baldi, 2020; Nori et al., 2023; Elgamal et al., 2024) hardly obtainable with other simulation strategies. Nonetheless, previous studies on FDM cosmologies with ax-gadget have all relied on dark-matter-only (DMO) simulations. To further investigate FDM models in a proper physical context and examine their impact on galaxy formation, this work expands on what has been done with ax-gadget since Nori & Baldi (2018) by incorporating baryonic effects in a cosmological hydrodynamical N-body code, which are essential for a correct description of structure formation. While effective in modeling FDM behavior, ax-gadget is limited in simulating baryonic processes like gas cooling, star formation, and black hole feedback. Conversely, gasoline2– another cosmological hydrodynamical code with a compatible N-body structure – has been constantly developed and integrated with new routines related to baryonic process, and has been shown to be very effective in these areas in the past years (e.g. Stinson et al., 2006; Brooks et al., 2013; Wang et al., 2015). By integrating the FDM routines from ax-gadget into gasoline2 (Wadsley et al., 2017), we have developed a new version of the gasoline2 code, fuzzy-gasoline, capable of running hydrodynamic FDM simulations with baryons through to the present day (z=0) of large and complex systems at a reasonable computational cost. To the authors knowledge, this is the first code of its kind capable to do so. In this work, we leverage the fuzzy-gasoline code to create novel hydrodynamical simulations of dwarf galaxy systems with halo masses in the range of 10^{9}\lesssim M_{\text{halo}}/M_{\odot}\lesssim 10^{11}. We detail their properties, including dark matter, gas and star density and velocity profiles, as well as star formation histories, and compare them with those of their cold dark matter (CDM) NIHAO counterparts. Our goal is to explore two key aspects: first, what is the combined effect of baryons and FDM on galactic properties, and second, whether it is possible to disentangle the degeneracy of the two individual contributions. The remainder of this paper is organized as follows: in Sec. 2 we provide an overview of the theoretical background of FDM models; in Sec. 3 we detail the numerical methodology implemented in this work, specifically related to FDM dynamics and simulations; in Sec. 4 we present the main results, focusing on DM density profiles, differentiating between its two driving factors: FDM’s quantum pressure and baryonic feedback processes, and their impact on the observable properties of the explored systems; finally, we summarize our findings in Sec. 5."
https://arxiv.org/html/2411.09185v1,Progress towards a megapixel linear-mode avalanche photodiode array for ultra-low background shortwave infrared astronomy,"Spectroscopy of Earth-like exoplanets and ultra-faint galaxies are priority science cases for the coming decades. Here, broadband source flux rates are measured in photons per square meter per hour, imposing extreme demands on detector performance, including dark currents lower than 1 e-/pixel/kilosecond, read noise less than 1 e-/pixel/frame, and large formats. There are currently no infrared detectors that meet these requirements. The University of Hawai’i and industrial partners are developing one promising technology, linear mode avalanche photodiodes (LmAPDs), which is on track to meet the above-mentioned requirements.We present progress towards developing a science-grade, megapixel format linear-mode avalanche photodiode array for low background shortwave (1 - 2.4 um) infrared astronomy. Our latest results show outstanding performance, with dark current <1e-4 electrons/pixel/second and read noise reducing by 30% per volt of bias, reaching less than 1e-/pixel/frame in correlated double-sampling, and able to average down to \sim0.3 e-/pixel/frame when using multiple non-destructive reads. We present some on-sky data as well as comment on prospects for photon number resolving capability.","111portions of this introductory text adapted from [1] The high quantum efficiency (QE), low dark current (DC), and tunable cut-off wavelength of mercury cadmium telluride (HgCdTe) makes it the leading material for astronomical infrared detectors. Superb large format arrays such as the HAWAII family, manufactured by Teledyne imaging systems, are in regular use at observatories around the world, and comprise fifteen of the eighteen detectors on the James Webb Space Telescope. These arrays have dark current well below 1 e-/pixel/kilosecond when operated at temperatures below \sim60 K [2], and read noise of \sim10-15 e-/pix/frame, which may be reduced to \sim5 e-/pix/frame by frame averaging [3]. However, for photon starved science such as exoplanet imaging or faint galaxy spectroscopy, such arrays are too noisy[4, 5]. In particular, the read noise imposes a severe barrier, and has not been improved significantly in the three decades, as it is a fundamental limitation to the MOSFET-based source follower used in each readout pixel node [6]. There is no imminent path to overcoming this. To put the noise in context, one may compare the relative contribution of dark current to read noise over reasonable frame times. For a total exposure time of t and frame time t_{fr}, the variance of dark current is DC\cdot t and that of read noise is RN^{2}\cdot t/t_{fr}. Assuming 5 e- of read noise and assuming 1 e-/pixel/kilosecond of dark current, the variance of read noise is 25 or 250 times higher for frame times of 1000 and 100 seconds. There is a pressing need for extremely low noise infrared detectors, as the latest astronomical decadal survey[7] identified a 6-meter space telescope operating from UV-IR the highest priority mission, with exoplanet imaging and spectroscopy of Earth-like exoplanets as the primary science driver. For this science, typical flux rates are about 1 photon per square meter per hour in V-band, and it is well known that detector noise is the most serious obstacle for such missions [8, 9, 10], leading to a need for dark currents <0.001 e-/pix/s and read noise <0.3 rms e-/pix/frame. In the optical, EMCCDs can meet these noise requirements, but in the infrared—where most of the deep biomarker spectral signatures exist—no current sensors are suitable. The benefits of such a sensor would also extend to ground-based astronomy. Modern high-resolution infrared spectrographs are designed with resolutions of 30,000 to 100,000, necessary to resolve stellar spectral or planetary lines. However, for spectral resolutions above \sim5000, all observations are read noise limited in the interline continuum [11]. As such, reducing the read noise is the most straightforward method of allowing such instruments to reach their full potential. 1.1 Linear-mode avalanche photodiodes Linear-mode avalanche photodiodes (LmAPDs) offer one potential path to overcoming this read noise barrier. In these devices, large electric fields cause signal amplification through electron avalanching, so electrons are multiplied before the read noise penalty. This leads to an “effective” read noise, which is simply the base read noise divided by the multiplication gain222For example, operated at low bias voltage, before avalanching sets in, a received signal of 13 photoelectrons will encounter an underlying read noise of \sim13 e-, giving a signal-to-noise ratio (SNR) of \sim1. At moderate bias voltage, with an avalanche gain of \sim10, the signal of 13 electrons will be multiplied to \sim130 e-, so the SNR will be 10. This is equivalent to a 13 e- signal seeing an effective read noise of \sim13e-/10=1.3 e-.. The price of this is a reduced full well, which is mostly irrelevant for the low flux rates in question. HgCdTe LmAPDs, developed by Leonardo corporation (formerly Selex) in partnership with ESO and the University of Hawai’i, have found wide use as high speed wavefront sensors, such as in the SAPHIRA detectors.[12] The current generation of SAPHIRA arrays have demonstrated sensitivity from 0.8 to 2.5 \mum with high QE (>80%), fast pixel response, and an avalanche gain (aka APD gain) of >500, offering an unmatched combination of sub-electron effective read noise (as low as 0.1 rms e-) at 1kHz frame rate at convenient operating temperatures of 90-100K[13, 14]. These detectors are now in regular use at major observatories around the world. While SAPHIRA detectors can be used as science focal plane detectors, they are unsuitable for the low-flux science cases discussed above. First, their pixel format (320 x 256) is too small for integral field unit spectroscopy, which is better matched to a 4 megapixel (eg, 2k x 2k) sensor. Second, the lack of reference pixels puts high demands on voltage and temperature stability for long exposures, where drifts in the voltage level manifest as an extra noise source. Finally, when operated at low APD gain, SAPHIRAs deliver a dark current close to the level required, and still likely glow limited[15]. However, when operated at high APD gain, to lower the effective read noise, the large voltages applied results in trap-assisted tunneling of electrons across the p-n junction, which causes the effective dark current to exponentially increase to unacceptable levels. In practice, this means that SAPHIRAs cannot deliver low dark current and low read noise simultaneously. Fig. 1 shows theoretical performance curves of the SAPHIRA bandgap as a dashed blue line. The effective dark current stays very low, and as the bias voltage increases (top axis, unlabeled), the read noise reduces up until \sim3 e-. At that point, tunneling current begins to dominate the dark current budget, and by the time the read noise reaches 1 e-, it has increased by four orders of magnitude. Figure 1: Theoretical performance curves of LmAPD bandgap designs. Graded-bandgap LmAPD designs can deliver the simultaneously low read noise and dark current needed for the next generation of NASA space missions (green shaded region). The bias voltage (upper x axis) controls the read noise (lower x axis, decreasing to the right). The standard bandgap design has tunneling current at too low bias voltage (or too high read noise), after which the dark current increases exponentially. At low read noise, a graded bandgap design has dark current 1000-10,000x lower than a standard bandgap design. These theoretical curves assume an operating temperature of 60K. Motivated by the SAPHIRA results, the University of Hawai’i has partnered with Leonardo corporation, Markury Scientific, and Hawaii Aerospace to develop an LmAPD device suitable for ultra-low background infrared astronomy, with the goals of a dark current <0.001 e-/pix/s and an effective read noise <1.0 rms e-/pix/frame. The main differences from the SAPHIRA will be a larger format (1k x 1k with 15 \mum pixels), a design including reference pixels to improve overall stability, and careful bandgap engineering to move the onset of tunneling current to higher voltage, so low read noise and dark current can be simultaneously achieved (see Fig. 1). Our development plan includes three stages. The first is fabricating and testing engineering-grade megapixel sensors with the same bandgaps as the SAPHIRAs, but with smaller pixels and reference pixels. These first devices are mainly aimed at proving the capabilities of making larger-format devices, but are not expected to push the state-of-the-art in sensitivity. Second, fabricating and testing larger science-grade sensors with a modified, graded bandgap, that should give low dark current and read noise simultaneously. Finally, radiation testing of the science-grade devices to relevant space-like levels. The first stage, fabrication and testing of the engineering-grade sensors, took place in 2020-2021. Test results were quite encouraging, showing a dark current below 1e-/1000s at 50 K, a device glow of about one electron every 12 frames, and read noise in correlated double-sampling decreasing by 30% per volt following theoretical predictions, and reaching 2e- at 8 volts of bias. This work is reported in Ref. 1. This paper presents the results obtained from the science grade versions of this new 1kx1k detector design. We describe the detector design, readout chain, and laboratory test environment. We present measured dark current, glow, and read noise, as well as a first attempt to demonstrate photon number resolving capability. We also discuss persistence and non-uniformity, and present on-sky data."
https://arxiv.org/html/2411.09049v1,FAINT WHITE DWARF FLUX STANDARDS: DATA AND MODELS,Fainter standard stars are essential for the calibration of larger telescopes. This work adds to the CALSPEC (calibration spectra) database 19 faint white dwarfs (WDs) with all-sky coverage and V magnitudes between 16.5 and 18.7. Included for these stars is new UV (ultraviolet) HST (Hubble Space Telescope) STIS (Space Telescope Imaging Spectrometer) spectrophotometry between 1150 and 3000 Å with a resolution of \sim500. Pure hydrogen WD models are fit to these UV spectra and to six-band HST/WFC3 (Wide Field Camera 3) photometry at 0.28 to 1.6 µm to construct predicted model SEDs (spectral energy distributions) covering wavelengths from 900 Å to the JWST (James Webb Space Telescope) limit of 30 µm using well-established CALSPEC procedures for producing flux standards with the goal of 1% accuracy.,"Bohlin et al. (2020) and references therein discuss the CALSPEC111http://www.stsci.edu/hst/instrumentation/reference-data-for-calibration-and-tools/astronomical-catalogs/calspec database of SEDs for stellar flux standards. The CALSPEC SEDs are based on HST/STIS flux calibrated spectra, sometimes supplemented by HST/WFC3 and HST NICMOS (Near Infrared Camera and Multi-Object Spectrometer) spectrophotometry. Model atmosphere grids of stellar spectra are fit to these measured flux distributions to estimate the flux at longer wavelengths than covered by the HST observations. For this work, the Hubeny grid version 207 is used to fit the STIS SEDs. Bohlin et al. (2020) introduced this grid222DOI10.17909/t9-7myn-4y46, which contains 132 models with effective temperature (T_{\mathrm{eff}}) in the range 20,000-95,000 K and surface gravity (\log g) between 7.0 and 9.5, with six steps of 0.5, where g has units of cm s-2. The steps in T_{\mathrm{eff}} are 2,000 K between 20,000 and 40,000 K and 5,000 K between 40,000 and 95,000 K. The normal CALSPEC SED includes a mix of the observed spectrophotometry with model extrapolations to longer wavelengths. The fitted models provide standard star flux distributions for calibration of IR (infrared) instrumentation. Most of the current JWST CALSPEC standards are too bright (V <16) for many of the standard detector modes and provide only indirect flux calibrations via subarray data. However, our new fainter stars (V=16 to 19) can be observed in the standard science modes, thus avoiding any uncertainty associated with the small detector subarray modes used for bright stars. The JWST flux calibration plan (Gordon et al., 2022) utilizes three categories of CALSPEC standards: hot, A type, and G type stars, while WDs fall in the hot star category. Our long term project to establish a network of faint WD stars with complete sky coverage provides an ideal set of fainter IR flux standards for JWST. These faint star SEDs were based on HST/WFC3 photometry in six filters for 35 WDs from the A. Saha programs 12967, 13711, and 15113. Narayan et al. (2019) published preliminary results, while Calamida et al. (2022) present a variability analysis and finding charts. Details of the WFC3 photometric data reduction are in Calamida et al. (2019), which includes a description of the ILAPH photometry. Axelrod et al. (2023) fit this six-filter WFC3 photometry to sub-percent precision with pure-hydrogen model atmosphere SEDs using a hierarchical Bayesian analysis process similar to the preliminary analysis of Narayan et al. (2019). The Axelrod et al. (2023) paper examines the internal consistency of the SEDs of all 35 DA white dwarfs that span a wide range of temperatures and surface gravities and includes the three primary WDs that are used to define the flux scale for CALSPEC. This analysis compares their measured fluxes from the near-UV to the near-IR against predictions from the Hubeny & Lanz (1995) NLTE (non-local thermodynamic equilibrium) models of DA white dwarf atmospheres and results in a rigid lattice of SEDs on a physical basis and in relative apparent brightness for the set of stars that is self-consistent to a few milli-mag rms within this wavelength domain. The method of analysis is independent of CALSPEC, except for effectively borrowing the zero-point used in the 2014 version of the CALSPEC (CALSPEC14) flux scale for the absolute flux of Vega at 5556 Å (air) to achromatically tie this ’lattice’ to an absolute flux scale. While CALSPEC14 and Axelrod et al. (2023) both used the same three primary DA white dwarf model SEDs, the current implementation of CALSPEC differs in two respects: a) it is based on newer models, which extend the wavelength coverage to the JWST limit of 30 µm; and b) its absolute fluxes reconcile absolute flux measures of Sirius in the midIR with the Vega 5556 Å flux to determine the achromatic flux zero-point. The analysis in this paper is tied to the current Bohlin et al. (2020) CALSPEC flux scale and, thus, can differ systematically from that derived in Axelrod et al. (2023). In addition, the fitted data sets and methods of analyses differ. To supplement the WFC3 photometric constraints on the model fits, the HST program 16764 (G. Narayan PI) obtained shorter wavelength coverage with the STIS G140L and G230L gratings for 19 of the 32 stars in Axelrod et al. (2023) that are bright enough to obtain STIS UV spectrophotometry between 1150 and 3000 Å. Our goal is to produce CALSPEC SEDs extending to the JWST limit of 30 µm by fitting both the original WFC3 photometry along with the new STIS spectrophotometry. Consequent to the statements in the above paragraph, minor updates bring the Axelrod photometry onto the current CALSPEC flux scale system; and more recent NLTE models extend the wavelength coverage to 30 µm. An analysis with the standard \chi^{2} technique of Bohlin et al. (2020) used for all the current CALSPEC models provides consistent SEDs for on-going JWST flux calibrations with fainter standards than are currently available. JWST results will provide feedback to our continuing analyses that update these preliminary results. The definition of flux standards is never finalized but is a continually evolving process of improvement as the data set and analysis techniques mature. Section 2 describes the HST data, while Section 3 explains the procedure of finding the models that best fit the data. Section 4 summarizes the results and the future plans for improvements."
https://arxiv.org/html/2411.09012v1,AstroMLab 3: Achieving GPT-4o Level Performance in Astronomy with a Specialized 8B-Parameter Large Language Model,"AstroSage-Llama-3.1-8B is a domain-specialized natural-language AI assistant tailored for research in astronomy, astrophysics, and cosmology. Trained on the complete collection of astronomy-related arXiv papers from 2007-2024 along with millions of synthetically-generated question-answer pairs and other astronomical literature, AstroSage-Llama-3.1-8B demonstrates remarkable proficiency on a wide range of questions. AstroSage-Llama-3.1-8B scores 80.9\% on the AstroMLab-1 benchmark, greatly outperforming all models—proprietary and open-weight—in the 8-billion parameter class, and performing on par with GPT-4o. This achievement demonstrates the potential of domain specialization in AI, suggesting that focused training can yield capabilities exceeding those of much larger, general-purpose models. AstroSage-Llama-3.1-8B is freely available, enabling widespread access to advanced AI capabilities for astronomical education and research.","Large-language model (LLM) assistants are rapidly gaining traction across all sectors of knowledge work worldwide. In astronomy, these models are used for providing factual information, as programming assistants, for brainstorming ideas, and for providing explanations tailored to the level of understanding or preferred style of the user. LLMs exhibit a remarkable robustness, often delivering useful outputs even when the input is malformed, lacks context, or contains inaccuracies. Despite their potential, the development of specialized LLMs has been limited due to their recent emergence and the substantial resources required for training. Previous studies [1, 2, 3, 4] have shown that models narrowly tailored to a specific domain can perform on par with, or even exceed, much larger general-purpose models. This suggests that a large, highly domain-specific model could achieve state-of-the-art performance. In astronomy, however, high-performing specialized language models have not yet been achieved. While models like AstroLLaMA [5, 6] have gained attention, they lack comprehensive benchmarking of their astronomical knowledge recall capabilities. Recent studies [7] have shown that many of these models, due to limited specialized training data and fine-tuning for instruction-following, suffer from either catastrophic forgetting or an inability to follow precise question-answering instructions, often performing worse than their baseline models (in this case, the Llama models). Building on the previous efforts of cosmosage [8] and AstroLLaMA, we have developed AstroSage-Llama-3.1-8B, a natural language assistant specialized in astronomy, astrophysics, cosmology, and astronomical instrumentation. For the remainder of this paper, we will refer to these subdomains collectively as “astronomy”. Through the use of a substantially more extensive and well-curated training dataset, we demonstrate for the first time that our specialized language model significantly outperforms baseline models in downstream tasks, particularly in astronomical knowledge recall. In the long term, we envision an agentic research assistant capable of autonomously conducting literature reviews, identifying relevant hypotheses, carrying out data analysis, and even formulating new research questions. The development of such scientific agents (LLMs capable of solving scientific problems end-to-end) is already a rapidly growing field in astronomy. Recent studies have shown promising results in automating research tasks, such as analyzing James Webb Space Telescope data through multi-agent collaboration and self-play reinforcement learning [9]. However, these studies have been largely constrained by the substantial API costs associated with proprietary models. Realizing this level of agency will require extensive experimentation and careful optimization. Given the substantial compute costs and data requirements inherent in large-scale model training, keeping the model size manageable while maintaining high performance is crucial. Our approach, demonstrated through astronomy knowledge recall, shows that specialized models can achieve state-of-the-art performance in specific domains. This not only makes the development of advanced research assistants more feasible but also ensures their accessibility to a wider range of institutions and researchers, potentially transforming the landscape of astronomical research and education."
https://arxiv.org/html/2411.09673v1,FANSIC: a Fast ANalog SiPM Integrated Circuit for the readout of large silicon photomultipliers,"Silicon photo-multipliers (SiPM) have been replacing traditional photomultiplier tubes in most light sensing applications. However, when large detection surface coverage is needed, photomultipliers (PMTs) are still the preferred choice. The main reasons are the sensor thermal noise and the duration of the fast component of its signal, both increasing with the sensor surface. In this work we propose an application specific integrated circuit (ASIC), called Fast ANalog SiPM Integrated Circuit (FANSIC), for the readout of large SiPMs addressing these limitations. The ASIC has an active summation stage, which allows to divide a large detection surface into smaller ones offering faster response both in single ended and differential outputs. The high input bandwith allows to reach full-width-half-maximum (FWHM) signals or the order of 3–5 ns which limits the impact of internal and external uncorrelated noise. The results of the first implementation of FANSIC, designed in CMOS 65 nm technology, is described in this paper.","FANSIC is a fully analog front-end ASIC with 32 input channels designed to readout and process the signals of large SiPM sensors with single-photon resolution, 1 ns peaking time, and dynamic range in hundreds of photo-electrons. The main features of the chip are reported in Table 1. The chip is a prototype intended to confirm the performance achievable on a standard CMOS 65 nm technology node. The aim is to guarantee a short output signal and low noise levels while being compatible with input capacitance values up to 1 nF. This is a key requirement for many applications that handle signals with GHz rate and duration in the nanosecond range. Moreover, processing the signal of large arrays of sensors (bigger than 6\times 6 mm2) is a common need for many experiments that require covering large surfaces. The application target of FANSIC is in cameras of gamma-ray telescopes, like Imaging Atmospheric Cherenkov Telescopes (IACTs) which detect the Cherenkov light produced by atmospheric showers triggered by interaction of atmospheric nuclei with gamma and cosmic rays. This light is produced in flashes with a typical duration shorter than 5-10 ns and most of this signal light is emitted in wavelengths between 320 nm and 550 nm. IACTs employ cameras with hundreds to several thousands of photo-sensors clustered into pixels and located in their focal plane. Most cameras still adopt photomultipliers (PMTs) because their larger size compared to SiPMs allows to cover square-meter surfaces with less complexity and number of components. Recently, the ETH in Zürich and the University of Geneva have pioneered the adoption of SiPMs in IACTs cameras through the construction and operation of the FACT [1] and SST-1M [2]. The SST-1M camera has 1286 pixels and a linear dimension of about 1 m and is now in operation at the Ondřejov site in Czeck Republic [3]. FANSIC has been designed to be compatible with the requirements of a further evolution to larger dimension cameras with diameters of about 2 m. These Advanced Cameras are intended for the upgrade of the Large-Sized Telescope (LST) of the Cherenkov Telescope Array Observatory (CTAO) [4] during the 30 years of operation foreseen for this new generation of gamma-ray observatory [5]. Technology CMOS 65 nm Dimensions 12.845 mm2 (3.584\times 3.584 mm) Inputs 32 current inputs Outputs 8 voltage outputs (each sum of four inputs) Power supply 1.2 V Power consumption 23 mW (per pixel, i.e. 4 input channels) Programmable settings Amplifier bias currents, and buffer AC coupling capacitors Package None (bare dies) Table 1: Main characteristics of the FANSIC chip"
https://arxiv.org/html/2411.09319v1,Solar flares in the Solar Orbiter Era: Short exposure EUI/FSI observations of STIX flares,"Aims. This paper aims to demonstrate the importance of short-exposure extreme ultraviolet (EUV) observations of solar flares in the study of particle acceleration, heating and energy partition in flares. This work highlights the observations now available from the Extreme Ultraviolet Imager (EUI) instrument suite onboard Solar Orbiter while operating in short exposure mode.Methods. A selection of noteworthy flares observed simultaneously by the Spectrometer Telescope for Imaging X-rays (STIX) and the Full Sun Imager of EUI (EUI/FSI) are detailed. New insights are highlighted and potential avenues of investigation are demonstrated, including forward modelling the atmospheric response to a non-thermal beam of electrons using the RADYN 1D hydrodynamic code, in order to compare the predicted and observed EUV emission.Results. The examples given in this work demonstrate that short exposure EUI/FSI observations are providing important diagnostics during flares. A dataset of more than 9000 flares observed by STIX (from November 2022 until December 2023) with at least one short exposure EUI/FSI 174 Å image is currently available. The observations reveal that the brightest parts of short-exposure observations consist of substructure in flaring ribbons which spatially overlap with the hard X-ray emission observed by STIX in the majority of cases. We show that these observations provide an opportunity to further constrain the electron energy flux required for flare modelling, among other potential applications.","During solar flares, the emission at ultraviolet (UV) and extreme ultraviolet (EUV) wavelengths can become locally enhanced by several orders of magnitude on a timescale of seconds to minutes. Even moderately sized C-class flares typically saturate current EUV and UV imagers as well as soft X-ray imagers. This means that spatial information in the saturated pixels is lost. Further, depending on the type of detectors used (CMOS vs. CCD), the total flux may or may not be conserved and neighbouring pixels might be affected (so called “bleeding”). Saturation during enhanced solar flare emission in such instruments is due to the fact that they are not exclusively designed to study flares. It is challenging to design an instrument with a sufficiently large dynamic range that can observe both the faintest solar emission and the largest flares for a given exposure time. The saturation effect leads to challenges in many flare studies, particularly for relating coronal and chromospheric UV/EUV emission to observations from hard X-ray emission during the impulsive phase of flares. One way to address the saturation issue of EUV imagers is to shorten the exposure time used. Some EUV/UV imagers, such as Atmospheric Imaging Assembly (AIA) onboard the Solar Dynamics Observatory, use automatic exposure control, in which flight software determines the exposure time based on count flux (Lemen et al., 2012). However, with variations in this exposure time over the course of a flare, it can become a challenge to study given that the response of the detectors may be non-linear in exposure time which can introduce artificial variations linked to the exposure time changes. Regardless, despite the variable exposure time setting, the exposure time is often too long and AIA observations of flares are frequently still saturated. Efforts have been made to overcome saturation issues through a reconstruction approach which utilises the knowledge of the AIA diffraction pattern (Raftery et al., 2011; Krucker et al., 2011; Schwartz et al., 2014; Torre et al., 2015; Guastavino et al., 2019; Krucker et al., 2021). However, there are still challenges with total flux conservation and dealing with pixel bleeding. Kazachenko et al. (2017) also tackled this issue by linearly interpolating the flux prior to and post saturation in order to infer spatial information about flare ribbons. An approach of this kind, is, however, not suitable for photometric analysis. In the era of multi-messenger observations, the availability of EUV imagers on the far side of the Sun from Earth has become increasingly important for event studies since we cannot rely on Earth-based assets like AIA. For example, the Extreme Ultraviolet Imager (EUI) (Rochus et al., 2020; Berghmans et al., 2023) onboard Solar Orbiter (Müller et al., 2020) provides EUV images for observations taken by the Solar Orbiter instrument suite as the spacecraft sweeps out an elliptical orbit spending half of the mission duration on the far side of the Sun from Earth. EUI consists of three telescopes, the Full Sun Imager (EUI/FSI) and two High Resolution Imagers, which observe in Lyman \alpha, 174 Å and 304 Å passbands. The focus of this work is on EUI/FSI observations. To combat the aforementioned saturation issues, EUI/FSI has been regularly taking short exposure observations alongside normal exposure frames in both the 174 Å and 304 Å channels since the end of 2022. This paper demonstrates the value of short exposure EUV observations in flare studies and details the invaluable new datasets provided by EUI/FSI as well as recent observations with EUI’s High Resolution Imager, EUI/HRIEUV . In this work, a subset of interesting flares co-observed by the Spectrometer Telescope for Imaging X-rays (STIX) are presented and new insights enabled by this observation mode are discussed."
https://arxiv.org/html/2411.09311v1,Compression Method for Solar Polarization Spectra Collected from Hinode SOT/SP Observations,"The complex structure and extensive details of solar spectral data, combined with a recent surge in volume, present significant processing challenges. To address this, we propose a deep learning-based compression technique using deep autoencoder (DAE) and 1D-convolutional autoencoder (CAE) models developed with Hinode SOT/SP data. We focused on compressing Stokes I and V polarization spectra from the quiet Sun, as well as from active regions, providing a novel insight into comprehensive spectral analysis by incorporating spectra from extreme magnetic fields. The results indicate that the CAE model outperforms the DAE model in reconstructing Stokes profiles, demonstrating greater robustness and achieving reconstruction errors around the observational noise level. The proposed method has proven effective in compressing Stokes I and V spectra from both the quiet Sun and active regions, highlighting its potential for impactful applications in solar spectral analysis, such as detection of unusual spectral signals.","Observational spectral data encapsulates important and varied physical information with a multi-dimensional structure about astronomical bodies, necessitating thorough investigation and analysis for a comprehensive understanding of space. The increase in the number of observatory instruments in recent years has led to a substantial growth in the volume of astronomical data. This surge not only emphasizes the significance of studying such data but also opens up promising opportunities for leveraging deep learning techniques in the processing and analysis of these vast datasets in the big data era. One approach to handling such intricate data is the feature extraction technique, which takes the high-dimensional raw data as input, compresses it, and reconstructs it to the original size. The most important features of the original high-dimensional data are extracted in the compressed part, enabling them to serve as representatives of the original complex dataset in subsequent studies. Autoencoders (LeCun, 1987; Kramer, 1991; Goodfellow et al., 2016) play a powerful role in deep learning-based dimensionality reduction. Through this compression approach, further studies such as anomaly detection (Chen et al., 2018; Ryu et al., 2023) and classification (Gogoi and Begum, 2017; Yeom et al., 2021) of data can also be accomplished. In the latter part of the 2010s, several studies aimed to develop and apply compression methods for spectral data, particularly in the context of galaxy observations. Portillo et al. (2020) utilized variational autoencoder models to compress galaxy spectra by reducing it to six parameters, offering more accurate reconstructions than principal component analysis (PCA). Melchior et al. (2023) introduced an architecture to represent and generate restframe galaxy spectra from 6 to 10 latent parameters, resulting in accurate reconstructions with superresolution and reduced noise. When compared to data in other fields of space science, solar spectral data stand out in terms of their increased precision and complexity in higher dimensionality, encompassing details about light polarization, temperature, and the magnetic field on the solar surface. Therefore, processing this type of observational data poses a significant challenge. Previous works referring to the representational dimension of solar polarimetric spectra include, Asensio Ramos (2006)’s two-part minimum description length principle for approximation model selection, which suggests the optimal eigenvector dimension for denoising PCA, and Asensio Ramos et al. (2007)’s intrinsic dimensionality estimation method for spectropolarimetry data. López Ariste and Casini (2002) implemented a PCA inversion technique using 10 eigenprofiles for a single Stokes profile. A feature extraction technique by Socas-Navarro (2005) for simulated solar profiles, based on a multi-layer perceptron, represents one of the first uses of a neural network for solar spectra, achieving higher accuracy than previous methods such as PCA but requiring significant computational expense. Studies conducted on inversion techniques using deep learning, include Gafeira et al. (2021)’s convolutional neural network-based inversion method for Stokes profiles using Hinode (Kosugi et al., 2007) data. Additionally, Asensio Ramos and Díaz Baso (2019) introduced convolutional neural networks that output thermodynamic and magnetic properties from synthetic Stokes profiles, and achieved a precision comparable to the standard technique. Regarding deep learning-based solar spectral compression, Sadykov et al. (2021) used a fully connected autoencoder to reduce one-dimensional quiet Sun spectra, collected by NASA’s IRIS (De Pontieu et al., 2014) satellite, from 110 to 4 in size, achieving an average reconstruction error comparable to the variations in the line continuum. Upon reviewing previous works, it becomes apparent that compression techniques for observational solar spectra have primarily been developed for one-dimensional spectra related to spatial positions in the quiet Sun. However, active regions cannot be disregarded, as they are associated with a variety of significant solar phenomena—such as solar flares, solar jets, and coronal mass ejections—necessitating thorough study as important regions of interest. This motivates our proposal to develop an efficient compression method for solar polarization spectra, applicable to both the quiet Sun and active regions, by utilizing two-dimensional key polarimetric parameters. We conduct our study using observational solar spectra from Hinode SOT/SP (Tsuneta et al., 2008; Suematsu et al., 2008; Lites et al., 2013), a collaborative mission of JAXA, NASA, and ESA. This mission has been collecting solar spectro-polarimetric data since 2006, constituting an extensive solar spectral database suitable for our work. Our study introduces the compression of solar spectra through the development of two distinct models: a deep autoencoder (DAE) and a 1D-convolutional autoencoder (CAE). Considering the intricate nature of Stokes profiles characterized by high noise levels, we exclusively focus on Stokes I (total intensity) and Stokes V (circular polarization) selected from the set of four parameters. In Section 2, we provide a description of the Hinode data, followed in Section 3 by a comprehensive explanation of the methods applied in our study. Sections 4 and 5 present the results and discussion, respectively. In Section 6, we conclude the paper with a brief summary."
https://arxiv.org/html/2411.09248v1,Constraint on Lorentz Invariance Violation for spectral lag transition in GRB 160625B using profile likelihood,"We reanalyze the spectral lag data for of GRB 160625B using frequentist inference to constrain the energy scale (E_{QG}) of Lorentz Invariance Violation (LIV). For this purpose, we use profile likelihood to deal with the astrophysical nuisance parameters. This is in contrast to Bayesian inference implemented in previous works, where marginalization was carried out over the nuisance parameters. We show that with profile likelihood, we do not find a global minimum for \chi^{2} as a function of E_{QG} below the Planck scale for both the linear and quadratic models of LIV, whereas bounded credible intervals were obtained using Bayesian inference. Therefore, we can set lower limits in a straightforward manner. We find that E_{QG}\geq 3.7\times 10^{16} GeV and E_{QG}\geq 2.6\times 10^{7} GeV at 68% c.l., for linear and quadratic LIV, respectively. Therefore, this is the first proof of principles application of profile likelihood method to the analysis of GRB spectral lag data to constrain LIV.","The spectral lags of Gamma-ray Bursts (GRBs) have been widely used Desai (2024); Yu et al. (2022); Wei and Wu (2022) as a probe of Lorentz invariance Violation (LIV) ever since this was first proposed more than two decades ago Amelino-Camelia et al. (1998). The spectral lag is defined as the time difference between the arrival of high energy and low energy photons, and is positive if the high energy photons precede the low energy ones. In case of LIV caused by an energy-dependent speed of light, one expects a turnover in the spectral lag data at high energies. Among the plethora of searches for LIV using GRBs, the first work which found a turnover in the spectral lag data was by Wei et al. (2017) (W17, hereafter). This analysis found evidence for a transition from positive to negative time lag in the spectral lag data for GRB 160625B, by using the data from Fermi-LAT and Fermi-GBM. By modeling the time lag as sum of intrinsic astrophysical time-lag and an energy-dependent speed of light, which kicks in at high energies, they argued that this observation constitutes a robust evidence for a turnover in the spectral lag data. Statistical significance of this turnover was then calculated using frequentist, information theory and Bayesian model selection techniques Ganguly and Desai (2017); Gunapati et al. (2022). Using Bayesian inference, lower limits on the quantum gravity energy scale was set at 0.5\times 10^{16} GeV and 1.4\times 10^{7} GeV for linear and quadratic LIV, respectively Wei et al. (2017). These limits were obtained by marginalizing over the astrophysical nuisance parameters. All other analyses searching for LIV using GRB spectral lags have always used Bayesian inference. These include some of our own past works Agrawal et al. (2021); Desai et al. (2023); Pasumarti and Desai (2023). In this work we redo the analysis in Wei et al. (2017) using frequentist inference, where we deal with the nuisance astrophysical parameters using profile likelihood. While the profile likelihood is a “bread and butter” tool in experimental high energy Physics Particle Data Group et al. (2020), until recently it has seldom been used in Astrophysics, where Bayesian inference is commonly used. Recently, however there has been a renaissance in the use of Profile likelihood in the field of Cosmology (see Herold et al. (2022); Campeti and Komatsu (2022); Colgáin et al. (2024); Karwal et al. (2024); Herold et al. (2024) for an incomplete list). In particular it was shown that one reach opposite conclusions for the fraction of Early Dark energy using Profile Likelihood as compared to Bayesian inference Herold et al. (2022). The outline of this manuscript is as follows. We review the basic data analysis done in W17 to search for LIV in Section II. We compare the contrast Bayesian and frequentist parameter estimation highlighting how these methods handle nuisance parameters in Sect. III. Our results and conclusions can be found in Sect. IV and Sect. V respectively."
https://arxiv.org/html/2411.09096v1,"KMT-2021-BLG-0284, KMT-2022-BLG-2480, and KMT-2024-BLG-0412: Three
microlensing events involving two lens masses and two source stars","Aims. We carried out a project involving the systematic analysis of microlensing data from the Korea Microlensing Telescope Network survey. The aim of this project is to identify lensing events with complex anomaly features that are difficult to explain using standard binary-lens or binary-source models.Methods. Our investigation reveals that the light curves of microlensing events KMT-2021-BLG-0284, KMT-2022-BLG-2480, and KMT-2024-BLG-0412 display highly complex patterns with three or more anomaly features. These features cannot be adequately explained by a binary-lens (2L1S) model alone. However, the 2L1S model can effectively describe certain segments of the light curve. By incorporating an additional source into the modeling, we identified a comprehensive model that accounts for all the observed anomaly features.Results. Bayesian analysis, based on constraints provided by lensing observables, indicates that the lenses of KMT-2021-BLG-0284 and KMT-2024-BLG-0412 are binary systems composed of M dwarfs. For KMT-2022-BLG-2480, the primary lens is an early K-type main-sequence star with an M dwarf companion. The lenses of KMT-2021-BLG-0284 and KMT-2024-BLG-0412 are likely located in the bulge, whereas the lens of KMT-2022-BLG-2480 is more likely situated in the disk. In all events, the binary stars of the sources have similar magnitudes due to a detection bias favoring binary source events with a relatively bright secondary source star, which increases detection efficiency.","Since the mid-2010s, the Korea Microlensing Telescope Network (KMTNet) team has been conducting gravitational microlensing experiments using a network of three wide-field telescopes deployed in the Southern Hemisphere (Kim et al., 2016). The data collected from these observations are transmitted almost in real-time to the headquarters of the Korea Astronomy and Space Science Institute (KASI) for processing. A self-developed algorithm (Kim et al., 2018) is employed to identify lensing events, and the light curves of these detected events are meticulously examined for any discontinuous anomalies. These anomalies undergo careful analysis by multiple researchers to determine their origins and confirm whether they are caused by planetary companions to the lens, which is the primary objective of the experiment. Currently, the KMTNet experiment detects over 3,000 gravitational lensing events annually, with about 10% of these events exhibiting anomalies of various origins (Zang et al., 2021b, 2022). Of these anomalies, approximately 10% are confirmed to be of planetary origin (Gould et al., 2022). Anomalies in lensing light curves can arise from various factors. The most common cause is the binarity of the lens (Mao & Paczyński, 1991). In these binary-lens single-source (2L1S) events, caustics form on the source plane, and the source’s passage through these caustics results in light curves that differ from those of single-lens single-source (1L1S) events (Schneider & Weiss, 1986). These caustics exhibit complex patterns depending on the separation and mass ratio between the lens components and, along with various source trajectories, produce a wide range of anomaly patterns (Erdl & Schneider, 1993; Han, 2006; Cassan, 2008; Gaudi, 2012). Another important cause of anomalies is the binarity of the source. In these single-lens binary-source (1L2S) cases, the event’s light curve is the superposition of the lensing events occurring for each individual source, which leads to deviations in the lensing light curves (Griest & Hu, 1992; Di Stefano & Esin, 1995; Han & Gould, 1997; Dominik, 1998; Han & Jeong, 1998). In some rare cases, the observed anomalies cannot be explained by a three-body lensing model (lens plus source) and require a four-body lensing model, which includes an additional lens or source component. To date, 14 events have been identified as 3L1S events, in which the lens consists of three masses. Of these, six events correspond to planetary systems consisting of a host star and two planets, seven involve binary systems that include a planet, and the remaining event has been identified as a triple stellar system composed of three stars. Another type of four-body event occurs when both the lens and the source are binaries, known as a 2L2S event. So far, ten such events have been identified, four of which involve binary lenses that include planetary companions. In Table 1, we provide a summary of some of the known four-body lensing events, along with a brief description of the corresponding lens systems. Table 2: Coordinates, extinction, and baseline magnitude. Event (RA, Dec)J2000 (l,b) A_{I} I_{\rm base} Other ID KMT-2021-BLG-0284 (17:58:03.64, -32:18:10.30) (-1^{\circ}\hskip-2.0pt.5363,-4^{\circ}\hskip-2.0pt.0171) 1.48 19.49 MOA-2021-BLG-072 KMT-2022-BLG-2480 (17:35:06.77, -29:54:43.27) (-2^{\circ}\hskip-2.0pt.0414,1^{\circ}\hskip-2.0pt.4213) 2.71 19.68 KMT-2024-BLG-0412 (17:55:48.89, -29:54:40.79) (0^{\circ}\hskip-2.0pt.2984,-2^{\circ}\hskip-2.0pt.4062) 1.54 18.79 OGLE-2024-BLG-0496 We conducted a project in which KMTNet data were systematically analyzed to reveal the nature of events with complex anomalous features that are challenging to explain. In the initial phase, we examined lensing events from the KMTNet survey that exhibited anomalies in their light curves, which were then independently analyzed by multiple modelers. Most of these anomalies were successfully explained using either 2L1S or 1L2S models. However, for a small subset of events in which these three-body models could not account for the anomalies, we carried out more detailed analyses using advanced models. These analyses revealed that the difficulty in explaining the anomalies in some events was due to significant higher-order effects, such as lens orbital motion, as seen in events like OGLE-2018-BLG-0971, MOA-2023-BLG-065, and OGLE-2023-BLG-0136 (Han et al., 2024a). In other cases, the anomalies were caused by the presence of an additional source or lens component, as shown in the 2L2S and 3L1S events summarized in Table 1. It is important to note that the list of four-body lensing events in the table is not complete, as the nature of the anomalies in some events remains unclear, and more advanced models are currently being tested to interpret them. In this work, we present analyses of three lensing events with complex anomalies in their light curves that were successfully interpreted using 2L2S models. This paper is organized as follows. In Sect. 2 we detail the observations conducted to collect the data used for our analyses, including a brief description of the instrumentation. We also outline the procedures for data reduction and photometry. Section 3 begins with the definition of the lensing parameters used in our modeling across different interpretations of lensing events. We then describe the procedure of light curve modeling employed to determine these parameters. Subsequent subsections present the analysis process and results for each individual event. Each subsection discusses specific anomalies observed in the event’s light curve, provides model parameters derived from the analysis, and outlines the configuration of the lens system. In Sect. 4 we identify the source stars associated with each event and estimate the angular Einstein radius based on the derived information on the source star. Section 5 outlines the physical quantities of the lens determined based on the observables of individual events. Lastly, Sect. 6 summarizes our findings and presents the conclusions drawn from the study."
https://arxiv.org/html/2411.08842v1,AstroM: A self-supervised multimodal model for astronomy,"While machine-learned models are now routinely employed to facilitate astronomical inquiry, model inputs tend to be limited to a primary data source (namely images or time series) and, in the more advanced approaches, some metadata. Yet with the growing use of wide-field, multiplexed observational resources, individual sources of interest often have a broad range of observational modes available. Here we construct an astronomical multimodal dataset and propose AstroM3, a self-supervised pre-training approach that enables a model to learn from multiple modalities simultaneously. Specifically, we extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodal setting, allowing the integration of time-series photometry data, spectra, and astrophysical metadata. In a fine-tuning supervised setting, our results demonstrate that CLIP pre-training improves classification performance for time-series photometry, where accuracy increases from 84.6% to 91.5%. Furthermore, CLIP boosts classification accuracy by up to 12.6% when the availability of labeled data is limited, showing the effectiveness of leveraging larger corpora of unlabeled data. In addition to fine-tuned classification, we can use the trained model in other downstream tasks that are not explicitly contemplated during the construction of the self-supervised model. In particular we show the efficacy of using the learned embeddings for misclassifications identification, similarity search, and anomaly detection. One surprising highlight is the ""rediscovery"" of Mira subtypes and two Rotational variable subclasses using manifold learning and dimension reduction algorithm. To our knowledge this is the first construction of an n>2 mode model in astronomy. Extensions to n>3 modes is naturally anticipated with this approach.","Despite the vast volumes of publicly available raw astronomical data, with a few notable subfield exceptions, the application of machine learning to discovery and inference has yet to broadly permeate the field. One impediment stems from the challenge of fusing data across heterogeneous modes of collection. Off-the-shelf architectures do not easily accommodate a mixture of irregularly sampled multi-spectral multi-scale heteroskedatic time-series data, images, spectra, and metadata. Another issue, arising in the classification context, is that very few ground-truth labels exist. This “small label” problem arose, for example, in Richards et al. (2012), who sought to probabilistically classify 50,124 variable stars using only 810 labels over 28 classes. Last, models learned on a dataset from one survey do not easily transfer to other data collected on the same objects from different surveys (e.g., Long et al. 2012; Kim et al. 2021). Our self-supervised multimodal architecture addresses the first two challenges, establishing methods and milestones for a more generalized foundation model applicable to inference tasks on unseen survey data. Our work builds upon the Contrastive Language-Image Pretraining (CLIP) framework, originally introduced by Radford et al. (2021); CLIP demonstrated the power of contrastive learning on large-scale image and text datasets to learn joint representations. Since its introduction, CLIP has been extensively researched and improved in various ways. For example, Li et al. (2021) enhanced data efficiency through supervision, while Yao et al. (2021) focused on improving semantic alignment. Cherti et al. (2023) introduced scaling laws, and Sun et al. (2023) optimized the model for faster training. Additionally, CLIP has been combined with other pretraining objectives: Mu et al. (2022) incorporated image self-supervision, and Singh et al. (2022) along with Li et al. (2022) added masked multimodal, image, and language modeling. Furthermore, CLIP has been extended to other modalities: audio-text (Wu et al., 2023), video-text (Luo et al., 2021; Xu et al., 2021; Ma et al., 2022), and point cloud-text (Zhang et al., 2022). In the astronomical context, Parker et al. (2024) used dual-mode CLIP on static-sky galaxy images and spectra. Closest to the approach of our work outside of astronomy, Guzhov et al. (2022) adapted CLIP for use with three modalities: audio, image, and text. Given the proven versatility and success of CLIP in different domains, we build upon it herein. We extend CLIP to work on three modalities: time-series photometry, spectra, and metadata (see Figure 1). Our work, and a recent preprint from Zhang et al. (2024), are the first efforts to incorporate time-series data with CLIP, and our three-mode model represents a critical step towards the development of a foundational multimodal model for time-domain astronomy."
https://arxiv.org/html/2411.08747v1,"Regression for Astronomical Data with Realistic Distributions, Errors and Non-linearity","We have developed a new regression technique, the maximum likelihood (ML)-based method and its variant, the KS-test based method, designed to obtain unbiased regression results from typical astronomical data. A normalizing flow model is employed to automatically estimate the unobservable intrinsic distribution of the independent variable as well as the unobservable correlation between uncertainty level and intrinsic value of both independent and dependent variables from the observed data points in a variational inference based empirical Bayes approach. By incorporating these estimated distributions, our method comprehensively accounts for the uncertainties associated with both independent and dependent variables. Our test on both mock data and real astronomical data from PHANGS-ALMA and PHANGS-JWST demonstrates that both the ML based method and the KS-test based method significantly outperform the existing widely-used methods, particularly in cases of low signal-to-noise ratios. The KS-test based method exhibits remarkable robustness against deviations from underlying assumptions, complex intrinsic distributions, varying correlations between uncertainty levels and intrinsic values, inaccuracies in uncertainty estimations, outliers, and saturation effects. We recommend the KS-test based method as the preferred choice for general applications, while the ML based method is suggested for small samples with sizes of N<100. A GPU-compatible Python implementation of our methods, nicknamed “raddest”, will be made publicly available upon acceptance of this paper.","Linear and log-linear regression analyses are extensively applied in astronomical research. As a generalization of the conventional Ordinary Least Squares (OLS) estimator, the Weighted Least Squares (WLS) method incorporates the unequal variance of data points (heteroscedasticity) into the regression. Both OLS and WLS are derived with assumption that the independent variables have no observational uncertainties. However, this assumption is generally invalid for astronomical data. The presence of uncertainties in both dependent and independent variables can lead to a bias towards zero in the estimated slope of the (log-)linear relation (e.g. Fuller, 2009). Therefore, Orthogonal Distance Regression (ODR) and similar techniques have been introduced to mitigate the bias (e.g. Isobe et al., 1990; Hogg et al., 2010; Robotham & Obreschkow, 2015). While ODR yields satisfactory results when its explicit and implicit assumptions are met, substantial accuracy is not guaranteed in some complex, yet not uncommon cases. In addition, OLS and ODR often produce significantly different results (e.g. Ellison et al., 2021), making the selection of regression techniques a non-trivial issue. In the past three decades, considerable efforts have been dedicated to develop regression techniques for astronomical data (e.g. Press et al., 1992; Akritas & Bershady, 1996; Kelly, 2007; Hogg et al., 2010; Cappellari et al., 2013; Robotham & Obreschkow, 2015; Feldmann, 2019; Bartlett & Desmond, 2023). Although providing significantly improved regression results in many cases, these methods have various limitations and still require further improvement in certain cases. For instance, when deal with log-linear Regression, the widely-applied LINMIX method (Kelly, 2007) relies on the “delta method” to convert uncertainties in variables to uncertainties in their logarithms, an approximation method that is accurate only for data of high signal-to-noise ratios (S/N). In order to address the uncertainties in an independent variable, one needs to model the intrinsic distribution of the variable in the first place. This is done by employing Gaussian Mixture Models (GMMs) in both LINMIX and ROXY, a recent JAX-based implementation of a similar technique with support for non-linear cases (Bartlett & Desmond, 2023). While GMMs offer a workable solution, they become inefficient when the intrinsic distribution of the variable is complex. Furthermore, GMMs introduce an additional hyperparameter, the number of Gaussian components, which needs to be determined. Bartlett & Desmond (2023) show analytically that a single Gaussian is sufficient, regardless of the actual intrinsic distribution of the independent variable, given an infinite sample size and constant uncertainties. However, these assumptions are not always valid for astronomical datasets, which often exhibit heteroscedasticity and more complex noise behaviors than assumed. The Gaussian distribution decays rapidly away from its mean, leading to a significant gradient vanishing problem which makes optimization difficult for gradient-based algorithms. In addition, these methods rely on hierarchical Bayesian approaches, using Markov Chain Monte Carlo (MCMC, in LINMIX) or Hamiltonian Monte Carlo (HMC, in ROXY) to simultaneously sample the parameters of the GMM and the linear regression parameters, thereby providing a complete posterior distribution. While offering a comprehensive description of the results, this approach significantly increases computational costs. Additionally, these methods neglect the potential correlation between the uncertainty level of a variable and its intrinsic value (the value with no observational error), which could arise due to the Poisson nature of photon counting and observational strategies that aim for fixed S/N. Another widely-applied method, Leopy (Feldmann, 2019) addresses arbitrary correlation forms, dependence between data points, and the presence of censored or missing data. In this method, however, the intrinsic distribution of the independent variable is not modelled natively, but is required to be provided as user input. This presents a challenge in real-world applications where the intrinsic distribution is often unknown. To address these limitations, we propose a new regression technique that combines the approaches of Kelly (2007), Feldmann (2019) and Bartlett & Desmond (2023), the advanced machine learning technique of Normalizing Flows (NFs) (e.g., Dinh et al., 2014; Jimenez Rezende & Mohamed, 2015), and variational inference-based empirical Bayes analysis. This new technique employs NF to model both the intrinsic distribution of the independent variable and the correlation between uncertainty level and intrinsic value. These distributions are estimated directly from the observed data points. Incorporating these estimated distributions allows for a comprehensive consideration of uncertainties in both independent and dependent variables. Our method can be applied to problems exhibiting intrinsic scatter and non-linear relationships. Furthermore, to enhance robustness, we introduce a variant version that replaces the likelihood (or posterior) with a distribution distance as the optimization objective. We evaluate our proposed methods and compare them with other commonly used techniques in the context of log-linear regression (distinct from linear correlation, as we will demonstrate). Our analysis utilizes both simulated data exhibiting log-linear relationships and real astronomical data from PHANGS-ALMA (Leroy et al., 2021a, b) and PHANGS-JWST (Lee et al., 2023; Williams et al., 2024), specifically focusing on the log-linear correlation between the CO(2-1) emission line flux and the JWST mid-infrared fluxes observed with the F770W, F1000W, F1130W, and F2100W filters. Tests on both simulated and real data demonstrate that our proposed method, particularly the distribution distance based variant, exhibits superior overall performance and enhanced robustness. While the experiments on simulated and real data focus on one-dimensional regression problems, our method is derived within the framework of multidimensional regression (as detailed in subsection 2.1). Given NF’s effectiveness in modeling high-dimensional distributions, it would be straightforward to generalize our method for regression of higher dimensions. This paper is organized as follows. In section 2 we describe both the proposed method and other methods from previous studies. section 3 showcases the test in log-linear case on mock data. Next, section 4 presents the application of the method to real PHANGS-ALMA and PHANGS-JWST data. Finally, we discuss and summarize our results in section 5 and section 6."
https://arxiv.org/html/2411.08647v1,"The Galactica database: an open, generic and versatile tool for the dissemination of simulation data in astrophysics","The Galactica simulation database is a platform designed to assist computational astrophysicists with their open science approach based on FAIR (Findable, Accessible, Interoperable, Reusable) principles. It offers the means to publish their numerical simulation projects, whatever their field of application or research theme and provides access to reduced datasets and object catalogs online. The application implements the Simulation Datamodel IVOA standard.To provide the scientific community indirect access to raw simulation data, Galactica can generate, on an ”on-demand” basis, custom high-level data products to meet specific user requirements. These data products, accessible through online WebServices, are produced remotely from the raw simulation datasets. To that end, the Galactica central web application communicates with a high-scalability ecosystem of data-processing servers called Terminus by means of an industry-proven asynchronous task management system. Each Terminus node, hosted in a research institute, a regional or national supercomputing facility, contributes to the ecosystem by providing both the storage and the computational resources required to store the massive simulation datasets and post-process them to create the data products requested on Galactica, hence guaranteeing fine-grained sovereignty over data and resources.This distributed architecture is very versatile, it can be interfaced with any kind of data-processing software, written in any language, handling raw data produced by every type of simulation code used in the field of computational astrophysics. Its generality and versatility, together with its excellent scalability makes it a powerful tool for the scientific community to disseminate numerical models in astrophysics in the exascale era.","The Amsterdam call for Open Science in 2016 started to promote public access to both the scientific publications and the data obtained with public funds. This call has been implemented at national level all over the world in recent years in the form of Open Science plans and programs to encourage the effective sharing of publications and research data. The first international framework on open science, the UNESCO Recommendation on Open Science (UNESCO, 2021), was adopted by 193 countries attending UNESCO’s General Conference in 2021. In recent years, data publication and reuse has become a key requirement demanded by both funding agencies and resource (computation or observation time) allocation committees. In the field of astrophysics, a number of initiatives have been taken to disseminate scientific data, mostly in astronomy and to a lesser extent, for numerical models in computational astrophysics. Online science platforms with astronomical data have been made available in the past few decades, e.g. the Sloan Digital Sky Survey (York et al., 2000), the Dark Energy Survey Science Portal (The Dark Energy Survey Collaboration, 2005; Fausti Neto et al., 2018) to provide the scientific community with access to theses surveys. In the same manner, similar dedicated science portals are currently being developed for observational data that will be produced by the Euclid space telescope (Laureijs et al., 2011), the Square Kilometer Array (SKA) telescope (Lazio, 2009) or the Vera Rubin Observatory (Ivezić et al., 2019). If the dissemination of astronomical data has been greatly facilitated by the standardization of file format, for example the FITS format (Wells et al., 1981), or the MeasurementSet (Kemball & Wieringa, 2000) standard in radio astronomy, computational astrophysicists lack a standardized data model to enable them to exchange their numerical simulation data in a uniform way. Nevertheless, a few simulation projects have released their data to the community, many of them in the cosmology field: e.g. the MultiDark (Prada et al., 2012) and Bolshoi (Klypin et al., 2011) simulations hosted on the CosmoSim database, the galaxy cluster merger catalog (ZuHone et al., 2018) hosted on the yt Hub, the Illustris project (Vogelsberger et al., 2014; Nelson et al., 2019), the web portal for cosmological hydrodynamical simulations (Ragagnin et al., 2017), CosmoHub (Carretero et al., 2017; Tallada et al., 2020) or the Theoretical Astrophysical Observatory (Bernyk et al., 2016). Some projects even attempted to combine infrastructures in the field of turbulence studies, e.g. the Johns Hopkins Turbulence Database (Li et al., 2008), the Catalogue for Astrophysical Turbulence Simulations (Burkhart et al., 2020) or in the field of star formation (the StarFormat database) or interstellar medium (ISMDB). One of the major collaborative efforts led to the release of the generic Django-Daiquiri framework (Galkin et al., 2020, and references therin), a web application developed with the aim of being deployed for each project to host the data produced by the numerical simulations conducted in the project. It greatly lowered the technical barrier individual research groups have to overcome to publish their data on a web application, but unfortunately the required technical expertise and maintenance expenses are still out of reach for a majority of small individual projects led by computational astrophysicists."
https://arxiv.org/html/2411.08519v1,Detection and classification of radio sources with deep learning,"In this paper we present three different applications, based on deep learning methodologies, that we are developing to support the scientific analysis conducted within the ASKAP-EMU and MeerKAT radio surveys. One employs instance segmentation frameworks to detect compact and extended radio sources and imaging artefacts from radio continuum images. Another application uses gradient boosting decision trees and convolutional neural networks to classify compact sources into different astronomical classes using combined radio and infrared multi-band images. Finally, we discuss how self-supervised learning can be used to obtain valuable radio data representations for source detection, and classification studies.","A new era in radio astronomy has begun as the SKA precursor telescopes started their planned survey programs. Among them, the Evolutionary Map of the Universe (EMU) (Norris et al. 2011) of the Australian SKA Pathfinder (ASKAP, Hotan et al. 2021) started in 2022 to survey \sim75% of the sky at 940 MHz with a target noise rms of \sim15 \muJy/beam and an angular resolution of \sim10"". The EMU source cataloguing process will require an unprecedented degree of automation and knowledge extraction, as the expected number of detectable sources is \sim50 millions. So will be for other precursors and future SKA observations. In this context, machine learning (ML) can be a powerful resource for several data post-processing tasks, such as source detection, classification and anomaly discovery, shortening the time to deliver scientific results, and potentially enabling groundbreaking discoveries. In this paper we present three different application of deep learning methodologies on radio continuum data produced by SKA precursors: radio source finding with object detection frameworks (Section 2), compact source type classification with multi-wavelength data (Section 3), and self-supervised learning of radio data for source detection and classification (Section 4)."
https://arxiv.org/html/2411.08118v1,Coupled-mode theory for astrophotonics,"Coupled-mode theory (CMT) is a powerful tool for simulating near-harmonic systems. In telecommunications, variations of the theory have been used extensively to study waveguides, both analytically and through numerical modelling. Analogous mathematical techniques to the CMT are also widely used in quantum mechanics. The purpose of this work is to collect different formulations of the CMT and their underlying connections to quantum mechanical techniques, and to showcase their utility in modelling slowly varying waveguides including directional couplers and photonic lanterns. My choice of example waveguides is motivated by the astronomical applications of such devices in starlight nulling, wavefront sensing, and high-resolution spectroscopy. I first provide a brief review of the standard form of the CMT, applicable for waveguides with fixed eigenmodes. Next, I show that the CMT also applies for slowly varying waveguides, and demonstrate the close relation between the CMT and several well-known approximation methods from quantum mechanics, as well as concepts like geometric phase. Finally, I present a verification of my analysis, in the form of the numerical package cbeam.","1 Background 1.1 Astrophotonics The cross-disciplinary field of astrophotonics applies photonic devices to the technical challenges of astronomy (Jovanovic et al., 2023), and has since led to novel demonstrations of photonic nullers (Martinod et al., 2021; Xin et al., 2022), spectrometers (Gatkine et al., 2019; Lin et al., 2021), wavefront sensors (Norris et al., 2020; Lin et al., 2023), and wavefront correctors (Diab et al., 2024). Such devices offer new ways to process the light collected by astronomical telescopes, in a form factor much smaller than bulk-optical components. Figure 1 (panels a & b) show two examples of astrophotonic devices: the directional coupler (Agrawal, 2021), which acts as a photonic beamsplitter, and the photonic lantern (PL; Leon-Saval et al. 2005; Birks et al. 2015), which converts multi-moded light into single-moded light. With recent improvements in the fabrication capability for such devices, the astrophotonics community is now poised to consider how to tune designs for better performance in astronomical applications. This might include optimizing directional couplers or tricouplers (Klinner-Teo et al., 2022) and phase shifters for broadband nulling, or designing PLs which simultaneously maximize wavefront sensing and spectroscopy capability. To do so requires a physical understanding of how light propagates through astrophotonic devices. At least within the astrophotonics community, this understanding is often heavily reliant on numerical modelling techniques such as the finite-difference beam propagation method, which are accurate but opaque and difficult to intuit. At the same time, design optimizations for photonic devices would benefit from simulation methods that are faster than the standard methods used today. The goal of this paper is to introduce a mathematical tool that can be used to model and understand the propagation of light through a “slowly-varying” photonic device, through the lens of astronomical instrumentation. The precise meaning of the slowly-varying constraint will be developed later in this work, but for now it suffices to say that this constraint applies to the vast majority of astrophotonic devices, including PLs, photonic integrated circuits, arrayed waveguide gratings, directional couplers, multimode interferometers, fiber Bragg gratings, and Kerr combs; and that photonic devices not satisfying the slowly-varying constraint should probably be redesigned, if possible, to satisfy this constraint and simplify the modelling process. The mathematical tools reviewed in this work all fall under the broad category of “coupled-mode theory” (CMT). 1.2 Introduction to coupled-mode theory Figure 1: a. A 2\times 2 directional coupler, formed from two single-moded waveguide channels which brought close enough to transfer power and then separated again. b. A photonic lantern: an optical waveguide which looks like a few-moded optical fiber on one end and splits off into multiple single-moded outputs at the other. The propagation of light through this device also can be modeled as coupled harmonic oscillators. c. A mechanical system of coupled harmonic oscillators composed of masses and springs, constrained to move in one dimension along a frictionless surface. The CMT is a well-known and well-used mathematical tool. The origins of the CMT are often attributed to studies of radio transmission lines and electron beams by Miller and Pierce, respectively (Miller, 1954; Pierce, 1954); but the underlying ideas appear even earlier in quantum mechanics, for instance in studies of molecular dynamics by Born and Oppenheimer (Born & Oppenheimer, 1927). In telecommunications, the CMT has been widely used to study the propagation of light through few-moded optical waveguides, both analytically and numerically (Okamoto, 2006; Agrawal, 2021; Haus et al., 1987). To motivate the CMT, we will first consider a mechanical analog. Suppose we have a system of N masses connected by springs, as shown in Figure 1c; our goal is to determine the dynamics of the system given an arbitrary initial condition. We first consider the dynamics of a single mass-spring system (where the other end of the spring is fixed), which is a harmonic oscillator obeying \dfrac{d^{2}x}{dt^{2}}=-\dfrac{k}{m}x (1) where k is the spring constant, m the mass, and x denotes position. The corresponding solution for x(t) is the complex exponential. If we chain many mass-spring systems together, the dynamics of the system is that of a many harmonic oscillators, which can also transfer energy between each other. Mathematically, the equation of motion is \dfrac{d^{2}\bm{x}}{dt^{2}}=-A\bm{x} (2) where \bm{x}=[x_{1},x_{2},...,x_{N}]^{T} are the positions of each mass and A is an N\times N matrix whose off-diagonal terms correspond to the linkage between the individual mass-spring systems. To solve this equation, we assume the solution is harmonic in time, i.e. we propose the ansatz \bm{x}\propto\bm{u}e^{\i\omega t}. This reduces equation 2 to the eigenvalue problem \omega^{2}\bm{u}=A\bm{u} (3) where \bm{u} and \omega^{2} are identified as an eigenvector-eigenvalue pair. The motion of all the masses in the system may therefore be expanded in terms of the eigenvectors, whose amplitudes oscillate harmonically in time; this is “normal mode analysis”. So far, our analysis has been exact. We now consider the case where the the matrix A is allowed to vary with time, for instance through a gradual weakening of the springs or application of an external force. This is the problem that coupled-mode theory (CMT) attempts to solve. The principle of the CMT is as follows: suppose there is a system which supports a basis of harmonic oscillator eigenmodes. When a slowly varying perturbation is added, we assume that perturbed solutions will be “almost” harmonic, and attempt to find solutions formed from a linear combination of the unperturbed eigenmodes. To account for the perturbation, the modes are allowed to cross-couple through varying mode coefficients — hence the name “coupled-mode” theory. Existing treatments in the context of optical waveguides typically provide the “fixed-basis” formulation of the theory, suitable for optical fibers or composite waveguides composed of individual channels, such as directional couplers (Agrawal, 2021), but not for slowly varying waveguides whose eigenmodes change incrementally with longitudinal coordinate z. A motivating example for such a waveguide is the PL, which in recent years has been the subject of growing interest for astronomical applications such as telluric line suppression (Trinh et al., 2013), high-resolution spectroscopy (Jovanovic, N. et al., 2017; Lin et al., 2021), starlight nulling (Martinod et al., 2021; Xin et al., 2022), wavefront sensing (Lin et al., 2023), and spectroastrometry (Kim et al., 2024). The main purpose of this paper, comprising §2-3, is to provide an overview for the fixed-basis CMT as well as for the more broadly applicable “varying-basis” formulation (Snyder & Love, 1983), in some instances called “coupled local mode theory” (CLMT) (Jin et al., 2010; Chen et al., 2020), and to demonstrate its applicability to slowly varying waveguides. Unlike some previous implementations (Chen et al., 2020; Sunder & Sharma, 2020) which assume non-degenerate modes, the formulation presented here is applicable even in the presence of degeneracy, and accounts for non-adiabatic cross-coupling. The secondary purpose, comprising §4, is to show how many of the ideas in the varying-basis CMT have well-studied counterparts in quantum mechanics: concepts including the WKB approximation, perturbation theory, the adiabatic approximation, and geometric phase. While individual connections have been often drawn in previous studies, this work aims at establishing a broader connection, hopefully introducing the reader to techniques and ideas that have normally not been associated with coupled-mode theory and which could be useful in future developments of astrophotonic devices. Throughout, derivations are written using mathematical notation borrowed from quantum mechanics; I briefly review this notation for readers who may be less familiar with it. Finally, in §5 I verify my analysis with cbeam, a simple numerical implementation of the varying-basis CMT, and use it to simulate a two-channel directional coupler and a 6-port PL."
https://arxiv.org/html/2411.08079v1,Application of Machine Learning Methods for Detecting Atypical Structures in Astronomical Maps,The paper explores the use of various machine learning methods to search for heterogeneous or atypical structures on astronomical maps. The study was conducted on the maps of the cosmic microwave background radiation from the Planck mission obtained at various frequencies. The algorithm used found a number of atypical anomalous structures in the actual maps of the Planck mission. This paper details the machine learning model used and the algorithm for detecting anomalous structures. A map of the position of such objects has been compiled. The results were compared with known astrophysical processes or objects. Future research involves expanding the dataset and applying various algorithms to improve the detection and classification of outliers.,"Significant progress in astronomy has been made in recent years due to the introduction of machine learning methods for analyzing observational data. The enormous amount of data generated by telescopes and other instruments complicates the manual search and identification of both interesting and unusual structures. Machine learning algorithms can effectively analyze large datasets in high-dimensional parameter spaces and identify patterns that would be difficult for humans to detect 1 . The Planck Space Observatory, launched by the European Space Agency and NASA in 2009, was designed to study the cosmic microwave background (CMB) radiation, the oldest light in the universe. CMB maps contain a wealth of valuable information about the early universe and its large-scale structure, making the mission extremely important for cosmology. The mission aimed to map the Big Bang’s CMB with improved sensitivity and resolution and to test theories about the universe’s origin and evolution. However, extracting and analyzing this information is an extremely difficult task. The task is to identify structures on the raw CMB maps that deviate from the expected pattern in the standard cosmological model 4 . It is necessary to develop and implement an approach that allows to identify various heterogeneous structures from space maps. We called them foreground outliers because these structures can be caused by various astrophysical phenomena, such as galaxy clusters, supernovae, or other celestial objects, whose intensity exceeds that of the CMB. Examples of such objects include galaxies or galaxy clusters, bright stars, supernovae, dust contaminations, and other celestial objects. The study of foreground outliers on CMB maps is challenging due to their volume and complexity, requiring the development of methods and technologies to automate and enhance their detection and subsequent analysis. This makes it difficult to find them all manually. To address this issue, it is proposed to use machine learning methods, particularly neural networks and clustering algorithms. Neural networks are intended to extract features from CMB maps. Feature extraction involves identifying and extracting important features or patterns in a dataset relevant to the problem. In this case, neural networks will be trained to identify patterns on CMB maps indicating the presence of foreground outliers. Clustering algorithms will be used to group similar structures in the feature space created by the neural network. These methods can enhance the efficiency and accuracy of foreground outliers detection on CMB maps. In machine learning, foreground outliers detection can be classified as the task of detecting anomalous images in a sample consisting of a set of images that belong to some general population. Anomaly detection in arbitrary data can be classified as a binary or multi-cluster clustering task, depending on the purpose (detection or classification). Anomalies are data that do not conform to the established concept of normal behavior. It is important to distinguish anomalies from noise in the data. So, we define foreground outliers as statistically rare patterns on the map regardless of their origin. This means that as a result, among foreground outliers we can observe both point-like objects of known or unknown astrophysical origin, and might be CMB regions that do not correspond to the Lambda-CDM model. Anomaly detection is characterized by unique features that do not allow typical clustering approaches to be applied without modification. These features depend on the task statement and the subject area, while the data structure also has an impact. An overview of the main methods for anomaly detection is provided in 6 . The main features of the anomaly detection task include: 1. Class imbalance, where anomalous objects constitute a small fraction of the total data (usually less than 1%). 2. The potential absence of anomalies in the training sample, while their appearance in real data necessitates effective model detection. 3. Difficulties in defining a universal measure of similarity between data samples. 4. The challenge of distinguishing noise from anomalies in data, leading to poor anomaly detection quality in noisy datasets. In this paper, the problem of detecting anomalies in images is considered. There are several methods for solving this problem. The work 7 provides an overview of the main anomaly detection methods. Here are some of them: 1. Statistical methods. One approach involves constructing a sample distribution function and identifying points that fall outside this distribution. 2. Feature extraction-based methods. Machine learning algorithms, such as neural networks, can identify patterns in data 8 , 9 . By analyzing these patterns, indicators of anomalies can be found. This approach is particularly effective for high-dimensional data. 3. Reconstruction-based methods. Autoencoders can be used to reconstruct images, and reconstruction errors can indicate anomalies. 4. Clustering-based methods. Clustering algorithms can group similar data, with any image that does not fit the clusters being considered an anomaly. The choice of method depends on the specific task and characteristics of the data, so first you need to analyze the available data."
https://arxiv.org/html/2411.08801v1,Identifying Spicules in Mg II: Statistics and Comparisons with H\alpha,"The Sun’s chromosphere is a critical region to understand when considering energy and mass deposition into the transition region and corona, but many of the smaller, faster events which transport a portion of this mass and energy are still difficult to observe, identify and model. Solar Spicules are small, spike-like events in the solar chromosphere that have the potential to transfer energy and mass to the transition region, but whose energetic origins are still being researched. Chromospheric spicule activity on-disk can be identified by observing temporary excursions in the red and blue wings of chromospheric emission lines. Researchers have demonstrated this in Hydrogen Alpha (H\alpha, 6563 Å), Ca ii (8542 Å, k 3934 Å), Mg ii (h 2803 Å, k 2796 Å), and Si iv (1394 Å, 1405 Å) spectral observations, with the vast majority of identification efforts focused on lower chromospheric observations of H\alpha and Ca ii. Because any spicules which deposit mass and energy into the transition region must necessarily pass through the upper chromosphere, observations from this region such as Mg ii or Hydrogen Lyman Alpha (Ly\alpha 1216 Å) in enough quantity to perform proper statistics will be critical to fully characterizing spicules’ impact on mass and energy transfer in the Sun. This research proposes a definition with numerical limits for how spicules appear in Mg ii wavelengths, tunes an algorithm for automatically detecting spicules in Mg ii spectral observations, and uses K Means Clustering to identify and display the full range of spicule spectrum shapes. This work will help allow statistical studies on spicules in the upper chromosphere to be as thorough as those of the lower chromosphere, allowing researchers to better understand the physical nature of spicules and their role in energy transfer and deposition in the solar atmosphere.","The Sun’s chromosphere is a region which is highly dynamic on all scales and has historically been difficult to study. This region changes from being plasma dominated (\beta>1) with strong flows and convection re-arranging the magnetic field, to being magnetic field dominated (\beta<1) where magnetic fields define structures, plasma volumes, and movements. In addition, all energy which heats the inexplicably-hot corona passes through this region to be deposited at its upper boundary in the transition region, where the majority of the temperature change takes place. Modeling the magnetic field is critical to understanding and predicting the trajectory for CMEs and the risks of SEPs and solar flares, while modeling plasma and energy movement are critical to understanding the source of coronal heating. One current issue facing researchers is the inability to fully recreate solar spicules in modeling efforts. Spicules are fine, hair-like features on the Sun extending from the top of the photosphere up into the chromosphere, sometimes reaching as far as the solar transition region (TR). They are spikes of hot plasma enclosed within magnetic flux tubes which carry mass and energy up from the photosphere. We know that some spicules reach the TR to deposit their mass and energy while others do not, but we do not know how often this happens or how much mass and energy are deposited. A number of models have investigated spicule formation energy sources including Alfvén waves Cranmer & Woolsey (2015); Iijima & Yokoyama (2017), shocks and wave action Sterling et al. (2010); De Pontieu et al. (2004a), magnetic tension Martínez-Sykora et al. (2017), and magnetic reconnection Ding et al. (2011); Shelyag et al. (2018). Several groups have been studying spicule-like features in magnetohydrodynamic (MHD) models Martínez-Sykora et al. (2017); Chintzoglou et al. (2018); González-Avilés et al. (2020); Dover et al. (2020); Srivastava et al. (2023) This includes two groups that have managed to simulate solar spicules in MHD models, but the drivers behind those efforts differ. Martínez-Sykora et al. (2017) found that including ambipolar diffusion created long, finger-like structures in the model results which seemed to agree with Ly\alpha observations by Chintzoglou et al. (2018). On the other hand, Iijima & Yokoyama (2017) formed spicule-like structures in their model, concluding that Lorentz forces as opposed to heating drove spicule formation. Rapid time cadence upper chromospheric observations have the potential to differentiate between the models. There have been many studies of on-disk solar spicules, but the vast majority focused on observations in the lower chromosphere with more than 45,000 identifications in H\alpha (6563 Å) and Ca ii (8542 Å) (Hansteen et al., 2006; De Pontieu et al., 2007; Rouppe van der Voort et al., 2007; Langangen et al., 2008; Vourlidas et al., 2016; Rouppe van der Voort et al., 2009; Sekse et al., 2012, 2013b; Yurchyshyn et al., 2013; Rouppe van der Voort & de la Cruz Rodriguez, 2013; Sekse et al., 2013a; Yurchyshyn et al., 2014; Rouppe van der Voort et al., 2015; Henriques et al., 2016; Shetye et al., 2016; Pereira et al., 2016; Lipartito et al., 2014; Skogsrud et al., 2016; Bose et al., 2019; Samanta et al., 2019; De Pontieu et al., 2009; Rutten et al., 2019; De Pontieu et al., 2011; Yurchyshyn et al., 2020; Sterling et al., 2020; Dover et al., 2020; Nived et al., 2022; Vilangot Nhalil et al., 2022; Chaurasiya et al., 2024; Bose et al., 2021a, 2019), while a few hundred spicules had been identified in upper chromospheric Mg ii (2796.34 Å, 2803.52 Å) and Ly\alpha (1215.67 Å) or TR Si iv (1403 Å) observations (Tian et al., 2014; Rouppe van der Voort et al., 2015; Skogsrud et al., 2016; Henriques et al., 2016; Bose et al., 2019; Narang et al., 2016; Chintzoglou et al., 2018, 2021; Vilangot Nhalil et al., 2022; Chaurasiya et al., 2024; Bose et al., 2023). This has resulted in a relative lack of spicule examples in the upper chromosphere, a region which is critical to our understanding of how energy and mass are deposited into the TR. However, Herde et al. (2023) (hereafter referred to as Paper I) demonstrated the feasibility of a new untuned method for using Mg ii observations from the Interface Region Imaging Spectrograph [IRIS] De Pontieu et al. (2014a) to identify a large number of on-disk upper-chromospheric spicule identifications for the purpose of statistical studies. This research builds on that paper by setting out the limits for how spicules appear in Mg ii spectra, tuning the method, and applying it to a greater range of datasets. With access to large-scale Mg ii spicule identifications, researchers can use this to understand how spicules evolve as they fade out of traditional H\alpha and Ca ii observations and into hotter, higher emissions. This information would help close the observational gap between the lower chromosphere and the transition region, allowing for a more full understanding of solar spicules’ energetic interactions with that boundary layer. 1.1 Spicules Spicules are small, ubiquitous, jet-like features in the solar chromosphere which, when observed on the solar limb (edge), look like slender threads sticking out from the Sun’s surface Roberts (1945). Physically, they are upflowing or downflowing streams of plasma constrained within small magnetic flux tubes above the Sun’s surface. They were first identified on the solar limb by Italian priest and astronomer Angelo Secchi in 1871 Secchi (1871) and have since been matched to their disk-counterparts called Rapid Blueshift/Redshift Excursions (RBEs/RREs) Langangen et al. (2008); Rouppe van der Voort et al. (2009); Sekse et al. (2012). RREs and RBEs are so named because they appear to redshift or blueshift the spectral line wing for a short period of time. Their importance in impacting the dynamics of the solar atmosphere has long been recognized (Beckers, 1968, 1972) and they have formed a part of numerous reviews such as Beckers (1968, 1972), Roberts (1945), and more recently Tsiropoula et al. (2012). Spicules are found in both active and quiet regions on the Sun(Athay & Holzer, 1982; De Pontieu et al., 2011), and estimates suggest that there are more than a million spicules on the Sun at any given time (Beckers, 1972). Over the past 20 years, a great deal of detail has emerged on their spectral and general characteristics (e.g. De Pontieu et al., 2004a; De Pontieu et al., 2007; Rouppe van der Voort et al., 2009; Sekse et al., 2012; Yurchyshyn et al., 2013; Bose et al., 2019, 2021b; Bate et al., 2022; Bose et al., 2023), thanks primarily to the development of high resolution ground-based and space-based telescopes that achieve a spatial resolution better than 0.1′′. The interest in their study was massively revived after seeing-free, high-cadence observations were made possible by the Hinode spacecraft Tsuneta et al. (2008). The original spicules observed (now called Type I, also referred to as dynamic fibrils or mottles) are longer in duration (3-5 min) and less energetic than their later-discovered counterparts. Dynamic fibrils appear primarily in active regions Pereira et al. (2012) while mottles are their quiet sun counterparts Rouppe van der Voort et al. (2007). Type I spicules receive energy from the leakage of photospheric oscillations (p-modes) which steepen into shocks in the chromosphere De Pontieu et al. (2004b); Hansteen et al. (2006). Later, in 2007, De Pontieu et al. (2007) discovered a new class of spicules called Type II. These spicules are shorter-lived (1-3 min), highly dynamic, and appear more often in regions of quiet Sun and coronal holes De Pontieu et al. (2007); Pereira et al. (2012). Unlike the Type-Is, the origin of Type II spicules is still debated, with magnetic reconnection Ding et al. (2011), non-linear propagation of Alfvénic waves Matsumoto & Shibata (2010) and release of amplified magnetic tension through ambipolar diffusion Martínez-Sykora et al. (2017) being the prime candidates. As spicules evolve, their changing temperature shifts them into and out of different elemental spectral emission lines. H\alpha and Ca ii emissions form lower in the chromosphere and at cooler temperatures while Mg ii h 2803.52 Å and k 2796.34 Å emissions and Ly\alpha 1215.67 Å form higher in the chromosphere and at higher temperatures Leenaarts et al. (2013). This means if a spicule is heated, it is first detectable in H\alpha and Ca ii wavelengths, and may then switch into emitting at Mg ii, Ly\alpha, or even transition region Si iv wavelengths at high enough temperatures. This was discovered when Type II spicules were first identified and researchers noticed that they would appear to “fade” during their evolution. This rapid fading was correctly perceived as a sign of heating and/or opacity changes during their evolution, and was confirmed after the launch of the Interface Region Imaging Spectrograph (IRIS; De Pontieu et al., 2014b) mission, where the fading Ca ii H spicules subsequently appeared in the hotter Mg ii ultraviolet and sometimes Si iv passbands Pereira et al. (2014). Over the course of these studies, Type II spicules were sometimes found to be heated to TR Pereira et al. (2014); Rouppe van der Voort et al. (2015) and even coronal temperatures De Pontieu et al. (2011); Henriques et al. (2016); Samanta et al. (2019) and a study by (Samanta et al., 2019) found that most spicules observed in a quiet sun region appeared to transfer hot plasma into the corona, supporting similar observations of active regions De Pontieu et al. (2011); Ji et al. (2012). These properties make them a potential candidate that can contribute to the energy balance of the Sun’s outer atmosphere. Understanding spicule activity in the upper chromosphere, including how often lower-chromospheric spicules reach all the way through to the TR, is critical in being able to fully describe how spicules contribute mass and energy to the Sun’s upper atmosphere and to modeling their associated magnetic field behavior. With the tools to easily identify thousands of spicules across multiple datasets, researchers will be able to better study upper-chromospheric activity for comparison with much better understood lower-chromospheric activity."
https://arxiv.org/html/2411.08604v1,Production of GEM-like structures using laser-cutting techniques,"It has previously been proposed to enhance the ionization yield of liquid argon time projection chambers (LArTPC) used in dark matter and neutrino experiments by loading LAr with dopants with ionization energies below the energy of LAr scintillation photons. While dual-phase LArTPCs have excellent sensitivity to single ionization electrons, granting some sub-keV thresholds, their compatibility with photosensitive dopants is hindered by gas-phase electroluminescence photons re-ionizing the dopants, creating a positive feedback loop. This challenge can be addressed by optically decoupling the gaseous and liquid phases with a barrier that transmits electrons. One potential way to do this uses a pair of structures based on Gaseous Electron Multipliers (GEMs) with mis-aligned holes. Rather than amplifying electron signals in gas pockets within their holes, their holes will be filled with LAr and a lower bias voltage will be applied, so that incident drifting electrons are drawn into the holes but not amplified. Instead, amplification will occur in the gas phase above the structures, as typical for dual-phase TPCs. Its core element is a GEM-like structure machined from polyethylene naphthalate (PEN). Since PEN scintillates in the visible spectrum, the risk of increased radioactivity due to the larger mass compared to traditional wire grids is negated by the potential to veto its own radioactivity. As such, these structures may also be a useful alternative to wire grids. In this work, we report the newest developments on the production of GEM-like structures using laser-based techniques, namely the manufacture of the first batch PEN and PMMA-based GEM-like structures. This process allows low-cost, reproducible fabrication of a high volume of such structures. In addition to being a low radioactive technique, we expect that it will allow the scaling up of the production of these structures at a reduced cost. First tests indicate good electrical stability, while the performance assessment is still ongoing.","WIMPs can be directly detected by dual-phase Ar-based TPCs, such as those developed by the DarkSide collaboration [1]. Future detectors will search in the lower energy region and focus on increasing the sensitivity, by relying on S2 signals and by improving S2 amplification. A possible improvement, currently being studied at the University of California, Riverside is the use of dopants with very low ionization energies, which may enhance the medium’s ionization yield. “Photosensitive dopants” with ionization energies below the energy of scintillation photons can also convert S1 into additional S2. However, S2 photons may leak back into the LAr and cause additional ionization, resulting in a positive feedback loop. To prevent this feedback, one possibility is to construct a structure similar to a gaseous electron multiplier (GEM) [2] that transmits electrons but blocks VUV photons. This can be done with two stacked GEMs with offset patterns, operated fully in the liquid phase, beneath a gas pocket in traditional dual-phase LArTPCs, with applied voltage too small to induce electroluminescence. To self-veto radiogenic backgrounds from the GEMs, 125 \mum-thick polyethylene naphthalate (PEN), which scintillates, was used. The first batch used a hexagonal hole pattern, which optimizes surface coverage (higher hole density and hence better transparency) with 200 \mum-diameter holes, and 1000 \mum pitch. A PEDOT:PSS (Clevios™) transparent conductive coating produced the electrodes, improving light collection of the readout plane while blocking VUV photons that may induce feedback. To improve the gain and minimize re-ionization, two such structures can be used. By shifting the holes’ positions (misalignment), these structures prevent electroluminescence photons from the gas pocket above them from entering the LAr. Nevertheless, cost-effective, radiopure, and scalable production of these solutions poses additional challenges. Despite recent advances observed in the development of MPGD technology, in particular of GEM-like structures, the methods of fabricating GEMs are limited [2, 3]. GEM production can be divided into two main approaches: 1) micro-hole production via photolithography and chemical etching or 2) mechanical hole drilling [3]. While the former may exhibit problems due to displacement of the holes in the dielectric and electrode layers due to alignment inaccuracies, the latter may result in metal burrs during drilling, which may cause electronic discharge [3]. An alternative approach was developed at the University of Tokyo [4], using doped photosensitive etchings (e.g. PEG and PEG3C), and a process similar to the lift-off method in the metal deposition step. Since the hole drilling relies entirely on photolithography, this approach displays two major challenges: First, uneven doping leads to rough hole walls, which manifest as tip structures in the holes, resulting in a greater discharge probability. Second, etching processes that rely on the photosensitive properties of special doped materials (such as Ag) cannot be applied to other glass materials with lower radioactive background and high-dielectric-strength. In addition, processes such as lift-off will cause curling, difficult to eliminate, and which can increase the electrical discharge probability. In this work, the production of PEN-based GEM structures using laser-cutting techniques is presented. A novel GEM fabrication method is proposed to solve the technical limitations of current approaches, namely in what concerns the producing of conical (or bi-conical) GEM-holes in thick structures, that will allow to improve the electron transparency while minimizing possible problems from charge-buildup in the GEM-like holes. Due to its characteristics, this technique is expected to allow the development of large area solutions, and easy production scaling, being adequate for the production requirements of dark matter experiments as an alternative radiopure technique."
https://arxiv.org/html/2411.08551v1,Estimating Stellar Atmospheric Parameters and [\alpha/Fe] for LAMOST O-M type Stars Using a Spectral Emulator,"In this paper, we developed a spectral emulator based on the Mapping Nearby Galaxies at Apache Point Observatory Stellar Library (MaStar) and a grouping optimization strategy to estimate effective temperature (T_{\text{eff}}), surface gravity (log g), metallicity ([Fe/H]) and the abundance of alpha elements with respect to iron ([\alpha/Fe]) for O-M-type stars within the Large Sky Area Multi-Object Fiber Spectroscopic Telescope (LAMOST) low-resolution spectra. The primary aim is to use a rapid spectral-fitting method, specifically the spectral emulator with the grouping optimization strategy, to create a comprehensive catalog for stars of all types within LAMOST, addressing the shortcomings in parameter estimations for both cold and hot stars present in the official LAMOST AFGKM-type catalog. This effort is part of our series of studies dedicated to establishing an empirical spectral library for LAMOST. Experimental results demonstrate that our method is effectively applicable to parameter prediction for LAMOST, with the single-machine processing time within 70 hr. We observed that the internal error dispersions for T_{\text{eff}}, log g, [Fe/H], and [\alpha/Fe] across different spectral types lie within the ranges of 15-594 K, 0.03-0.27 dex, 0.02-0.10 dex, and 0.01-0.04 dex, respectively, indicating a good consistency. A comparative analysis with external data highlighted deficiencies in the official LAMOST catalog and issues with MaStar parameters, as well as potential limitations of our method in processing spectra with strong emission lines and bad pixels. The derived atmospheric parameters as a part of this work are available at https://nadc.china-vo.org/res/r101402/.","A main task of modern astrophysics is to understand when and how galaxies formed and evolved. Our own galaxy, the Milky Way, offers a unique opportunity to study galaxies in considerable detail by measuring and analyzing the properties of stars (Jurić et al., 2008; Ivezić et al., 2008). The principal properties of stars, such as effective temperature (T_{\text{eff}}), surface gravity (log g), metallicity ([Fe/H]) and the abundance of alpha elements with respect to iron ([\alpha/Fe]), can be measured from spectra. At present, more and more large surveys, such as the Radial Velocity Experiment (Steinmetz et al. 2006), the Sloan Extension for Galactic Understanding and Exploration (SEGUE; Yanny et al. 2009), the Large Sky Area Multi-object Fiber Spectroscopic Telescope (LAMOST; Zhao et al. 2012; Cui et al. 2012; Luo et al. 2015), the Galactic Archaeology with HERMES (De Silva et al. 2015), the Apache Point Observatory Galactic Evolution Experiment (APOGEE; Majewski et al. 2017), the Sloan Digital Sky Survey V (SDSS; Almeida et al. 2023), the Dark Energy Spectroscopic Instrument (DESI; DESI Collaboration et al. 2016a, 2016b; Abareshi et al. 2022), Gaia Radial Velocity Spectrometer (Gaia Collaboration et al. 2023), the 4.2-m William Herschel Telescope Enhanced Area Velocity Explorer (Jin et al. 2023b), the Multi-Object Optical and Near-infrared Spectrograph (Cirasuolo et al. 2020), the upcoming the 4-metre Multi-Object Spectroscopic Telescope (de Jong et al. 2022), the Chinese Space Station Telescope (CSST; Gong et al. 2019), provide a large amount of spectra to help us understand the evolution and chemical formation of the Milky Way and even the Universe. These large surveys have been specially designed to yield full sets of atmospheric parameters for a wide variety of stars, and the methods for constructing spectral emulators 111Spectral Emulator: A method using machine learning to map stellar parameters to spectra (Czekala et al., 2015; Tabernero et al., 2022). based on theoretical and empirical spectral libraries are the most widely used spectroscopic techniques for determining stellar atmospheric parameters. Their widespread adoption is evidenced by a variety of publicly available implementations within the community. These methods are foundational to several analytical tools, including the SEGUE atmospheric parameter Pipeline (Lee et al. 2011), My God It’s Full Of Stars (Sbordone et al. 2014), the LAMOST atmospheric parameter Pipeline (LASP; Luo et al. 2015), the APOGEE atmospheric parameter and Chemical Abundance Pipeline (Pérez et al. 2016), Self-consistent ab initio Fitting of Stellar Spectra (The Payne; Ting et al. 2019), Spectrophotometric Modeling of Stars in the Gaia Era (MINESweeper; Cargile et al. 2020), the LAMOST atmospheric parameter Pipeline for M-type stars (LASPM; Du et al. 2021), and a Bayesian code to infer stellar atmospheric parameters using spectral emulators (STEPARSYN; Tabernero et al. 2022). However, the aforementioned methods have certain limitations. For instance, the spectral emulators for atmospheric parameter determination often involves \chi^{2} optimization, a process that has always been extremely time consuming, especially for millions of spectra. Furthermore, while these methods perform well in predicting parameters for FGK-type stars, catalogues have generally struggled to provide reliable parameters for stars outside of the FGK regime, such as OBA and M stars. For the LAMOST survey, such a vast data set also includes O-M-type stars, but currently, LAMOST has not officially provided atmospheric parameters for O- and B-type stars, and issues still exist in the parameters of A- and M-type stars. Considering the above problems, we used a spectral emulator based on the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) Stellar Library (MaStar; Yan et al. 2019; Abdurro’uf et al. 2022) and the new proposed strategy of grouping optimization to estimate T_{\text{eff}}, log g, [Fe/H], and [\alpha/Fe] for O-M-type stars within the LAMOST low-resolution spectra. Our spectral emulator, integrating a principal component analysis (PCA; Abdi & Williams 2010; Jolliffe & Cadima 2016) and Gaussian process regression (GPR; Rasmussen & Williams 2005), produces spectra for specified T_{\text{eff}}, log g, [Fe/H], and [\alpha/Fe]. The grouping optimization strategy was employed to achieve a balance between efficiency and accuracy in solving for the minimum \chi^{2}. Experimental results show that the combination of the spectral emulator and grouping optimization strategy yields reliable atmospheric parameter predictions while enhancing the efficiency of the spectral-fitting method, making it suitable for large-scale surveys. The measurements of LAMOST low-resolution atmospheric parameters using this approach are viable for the subsequent establishment of an empirical spectral library for LAMOST. The remainder of this paper is organized as follows. We introduce methodology in Section 2, including motivation and workflow. Then, we describe the data sets used in this work in Section 3. We discuss the results and compare them to those in previous works in Section 4. Finally, the conclusions are presented in Section 5."
https://arxiv.org/html/2411.08170v2,Simulation of Solar Wind Charged Particle Energy Deposited and Particle Identification by \DeltaE-E Discrimination in the SNAPPY Cubesat Detector,The Solar Neutrino and Astro-Particle PhYsics (SNAPPY) Cubesat is expected to launch in 2025 and it will carry into a polar orbit a prototype test detector for solar neutrino background studies while over the Earth’s poles for the neutrino Solar Orbiting Laboratory future project (\nuSOL). During this flight it is possible to do other science measurements. One of these is an improved study of the solar wind particles through better particle identification and high resolution energy measurements. This study aimed to understand how well could the solar wind particles be identified using the planned detector but instead of using the veto array as an anti-coincidence it would be used as a \DeltaE energy sampling of a phoswich particle ID system.,"As a NASA Jump Start undergraduate at Wichita State University, double majoring in Aerospace Engineering and Physics, I am conducting research on the particle discrimination capabilities of the CubeSat detector using the \DeltaE-E method and measuring the energy spectrum. The focus of the \textnuSOL project is the detection and study of neutrinos, primarily from the Sun. The research goals of the \textnuSOL project is to utilize the 1/r^{2} neutrino flux with the Sun, determining the neutrino gravitational focus to the Sun, and observing dark matter candidates [1]. The CubeSat detector design features two different detector materials: Polyvinyltoluene (the Veto) and gadolinium aluminum gallium garnet (called GAGG). Both detector materials are scintillators. The whole detector includes four GAGG crystals that are enclosed within the Veto. The purpose of the GAGG is to measure the neutrino interaction on 71Ga that is characterized by a double pulse [1]. The purpose of the Veto is to act as a filter for the GAGG. Neutrinos likely will not interact with the Veto’s material, and in principle, any particles detected by the Veto should not be neutrinos. Therefore, any double pulse detected in the GAGG that occurs simultaneously with a pulse from the Veto is not considered as a neutrino interaction. The current plan is for the CubSsat detector prototype to go into low altitude Polar Earth orbit, with an orbital period of 90 minutes, attached to a CubeSat nanosatellite. The detector will be turned on in fifteen minute intervals while the CubeSat is oriented over Earth’s north and south poles. Eventually \textnuSOL plans to go into orbit around the Sun, within a distance of seven solar radii at perihelion. The purpose of the solar orbit is to take advantage of the higher flux of neutrinos at closer distances to the Sun. The overall goal of the \textnuSOL project is to present a new and innovative way of studying the Sun’s fusion core and the particle physics of neutrinos. The CubeSat is a test of the detector, but we can use it to study solar wind at Earth and near the Sun in final mission."
https://arxiv.org/html/2411.07507v1,Physics of radio antennas,"Radio antennas are widely used in the field of particle astrophysics in searches for ultra-high energy cosmic rays (UHECR) and neutrinos (UHEN). It is therefore necessary to properly describe the physics of their response. In this article, we summarize the mathematics underlying parameterizations of radio antennas. As a paradigm, we focus on a half-wave dipole and also discuss measurements of characteristics, performed in an electromagnetic (EM) anechoic chamber.","Radio neutrino and cosmic ray experiments such as the Askaryan Radio Array (ARA) [1], Radio Neutrino Observatory-Greenland (RNO-G) [2], Radar Echo Telescope (RET) [3] and others deploy radio antennas as the front-end sensor for their detectors. Those antennas are often sensitive over a wide bandwidth, starting from 50 MHz and extending to a few GHz, making them ideal for UHEN and UHECR observation through Askaryan radiation, geomagnetic radiation, and radar echoes. These techniques are based on the detection of coherently superposed photons, each with meter-scale wavelengths (frequency \sim\mathcal{O}(100) MHz) rather than detection of incoherently summed photons or high frequency optical single photons. Additionally, radio antennas are robust and relatively simple and cheap to fabricate. Radio antennas can be used either as a transmitter (Tx), in the case when EM signal arising from electrical current in its conductor is radiated, or a receiver (Rx) when it receives EM radiation and converts it into electrical current. Before discussing the primary antenna parameters, we review some basic electromagnetic theory. We start with the free-space Maxwell’s equations in a source-free region (e.g., a vacuum). The fields (function of position, \vec{r} and time, t) in a region far away from the current or charges (sources) that created them can be written in differential equation form as \vec{\nabla}\cdot\vec{E}=0,~{}~{}\vec{\nabla}\times\vec{E}=-\frac{\partial\vec% {B}}{\partial t},~{}~{}\vec{\nabla}\cdot\vec{B}=0,~{}~{}\vec{\nabla}\times\vec% {B}=\mu_{0}\varepsilon_{0}\frac{\partial\vec{E}}{\partial t}. (1) Following the standard derivation, and taking the curl of Faraday’s law (second Eqn. of (1)), we obtain the vector wave equation for the electric field (in Volts/m) as \displaystyle\vec{\nabla}\times(\vec{\nabla}\times\vec{E})=-\frac{\partial(% \vec{\nabla}\times\vec{B})}{\partial t}~{}~{}~{}~{} \displaystyle\Longrightarrow~{}~{}~{}~{}\vec{\nabla}(\vec{\nabla}\cdot\vec{E})% -\nabla^{2}\vec{E}=-\mu_{0}\varepsilon_{0}\frac{\partial^{2}\vec{E}}{\partial t% ^{2}} \displaystyle\Longrightarrow~{}~{}~{}~{}\nabla^{2}\vec{E}-\mu_{0}\varepsilon_{% 0}\frac{\partial^{2}\vec{E}}{\partial t^{2}}=0. (2) The electric field cannot be any arbitrary function of \vec{r} and t. Instead, it is physically possible only if it satisfies the differential Eqn. (2). The simplest solution is the plane-wave solution where the electric field varies with time in a sinusoidal manner with an angular frequency of \omega radians/sec. Note that this field is a function of spatial coordinate z, but the direction of the electric field is orthogonal to the z-axis. \displaystyle\vec{E}=({E_{x}}\hat{x}+{E_{y}}\hat{y})e^{j\phi}=({E_{x}}\hat{x}+% {E_{y}}\hat{y})e^{j\omega(t-z\sqrt{\mu_{0}\varepsilon_{0}})} (3) How fast does the wave move? For a constant phase \phi, \displaystyle\frac{d\phi}{dt}=0~{}~{}~{}~{}\Longrightarrow~{}~{}~{}~{}\frac{dz% }{dt}=v_{p}=\frac{1}{\sqrt{\mu_{0}\varepsilon_{0}}}=c. (4) Therefore, E-field moves with a phase velocity, v_{p}, equal to the speed of light, c. Similarly, the magnetic field also moves with c. From Eqns. (3) and (4), \vec{E}=({E_{x}}\hat{x}+{E_{y}}\hat{y})e^{j\omega t}e^{-j(\omega/c)z}, (5) where (\omega/c)=k_{0}=2\pi/\lambda is called the ‘spatial frequency’. The wave vector along the propagation direction is \vec{k}=k_{0}\hat{z}. There are many other solutions besides the plane-wave solution. The most relevant of these, perhaps, is the spherical wave, \vec{E}=({E_{\theta}}\hat{\theta}+{E_{\phi}}\hat{\phi})\frac{e^{-jk_{0}r}}{r}e% ^{j\omega t}. (6) Similar to the electric field wave Eqn. (2), the magnetic field (in Webers/m2) wave equation can be written as \nabla^{2}\vec{B}-\mu_{0}\varepsilon_{0}\frac{\partial^{2}\vec{B}}{\partial t^% {2}}=0. (7) The \vec{E}, \vec{B}, and \vec{k} are mutually perpendicular and comprise an orthogonal set of vectors; hence, the ‘transverse’ nature of EM waves. From the known E-field, we can write the B-field as \vec{B}=({B_{x}}\hat{x}+{B_{y}}\hat{y})e^{j\omega t}e^{-jk_{0}z}=\frac{k_{0}}{% \omega}(-{E_{y}}\hat{x}+{E_{x}}\hat{y})e^{j\omega t}e^{-jk_{0}z}. (8) In addition, we define the magnetizing field (in Amperes/m) as \displaystyle{\vec{H}=\frac{\vec{B}}{\mu_{0}}=\frac{k_{0}}{\mu_{0}\omega}(-{E_% {y}}\hat{x}+{E_{x}}\hat{y})e^{j\omega t}e^{-jk_{0}z}}. (9) Now we can derive two important ratios. \frac{|\vec{E}|}{|\vec{B}|}=\frac{\omega}{k_{0}}\frac{\sqrt{|E_{x}|^{2}+|E_{y}% |^{2}}}{\sqrt{|E_{y}|^{2}+|E_{x}|^{2}}}=\frac{k_{0}c}{k_{0}}=c, (10) and \frac{|\vec{E}|}{|\vec{H}|}=\frac{\mu_{0}\omega}{k_{0}}\frac{\sqrt{|E_{x}|^{2}% +|E_{y}|^{2}}}{\sqrt{|E_{y}|^{2}+|E_{x}|^{2}}}=\mu_{0}c=\frac{\mu_{0}}{\sqrt{% \mu_{0}\varepsilon_{0}}}=\sqrt{\frac{\mu_{0}}{\varepsilon_{0}}}=\eta_{0}. (11) The first ratio motivates why the radio neutrino and cosmic ray experiments measure the electric, and not magnetic field. The second ratio is called the free space wave impedance, Z_{f}=\eta_{0} which is equal to 120\pi=377\Omega."
https://arxiv.org/html/2411.07321v1,Accelerating radio astronomy imaging with RICK,"This paper presents an implementation of radio astronomy imaging algorithms on modern High Performance Computing (HPC) infrastructures, exploiting distributed memory parallelism and acceleration throughout multiple GPUs. Our code, called RICK (Radio Imaging Code Kernels), is capable of performing the major steps of the w-stacking algorithm presented in Offringa et al. (2014) both inter- and intra-node, and in particular has the possibility to run entirely on the GPU memory, minimising the number of data transfers between CPU and GPU. This feature, especially among multiple GPUs, is critical given the huge sizes of radio datasets involved.After a detailed description of the new implementations of the code with respect to the first version presented in Gheller et al. (2023), we analyse the performances of the code for each step involved in its execution. We also discuss the pros and cons related to an accelerated approach to this problem and its impact on the overall behaviour of the code. Such approach to the problem results in a significant improvement in terms of runtime with respect to the CPU version of the code, as long as the amount of computational resources does not exceed the one requested by the size of the problem: the code, in fact, is now limited by the communication costs, with the computation that gets heavily reduced by the capabilities of the accelerators.","Radio astronomy is currently witnessing a rapid increase in the volume of data that are being collected by radio-interferometers like the LOw Frequency ARray (LOFAR, van Haarlem et al., 2013), MeerKAT (Jonas and MeerKAT Team, 2016), the Murchison Widefield Array (MWA, Mitchell et al., 2010), the Australian Square Kilometre Array Pathfinder (ASKAP, Johnston et al., 2007). These instruments can produce petabytes of data every year, precursors of what will be delivered by the Square Kilometre Array (SKA111https://www.skatelescope.org/), expected to generate hundreds of petabytes of data each year. In Gheller et al. (2023) (hereafter Paper I) we have introduced a novel approach for implementing the w-stacking algorithm (Offringa et al., 2014) for imaging on state-of-the-art High Performance Computing (HPC) systems, effectively exploiting heterogeneous architectures, consisting of thousands of multi-core CPUs equipped with accelerators like GPUs to enhance computational performance while minimising power consumption. Imaging is a computationally intensive step in the data processing pipeline (for an extensive introduction we refer to Taylor et al., 1999, and references therein), requiring a significant amount of memory and computing time. This is due to operations such as gridding, which involves resampling the observed data on a computational mesh, and fast Fourier transform (FFT), which converts between Fourier and real space. The computational demands increase when dealing with observations that have large fields of view, especially in current radio interferometers and at low frequencies. This is because curvature effects cannot be ignored, making the problem fully three-dimensional. In such cases, a solution is to introduce a “w-term” correction (see Sec. 2, Cornwell et al., 2008; Offringa et al., 2014). The gridding, FFT, and w-correction steps are integrated into the so-called w-stacking gridder algorithm. These steps are suitable to distributed memory parallelism, exploiting parallel FFT solutions and relying on a Cartesian 3D computational mesh, that can be effectively distributed and efficiently managed across different processing units, resulting in good scalability on large HPC architectures. In modern HPC systems, besides distributed processing, performance can be achieved through multi-core and accelerated (many-core) computing based on GPUs. Current trends suggest that some form of heterogeneous computing will be prevalent in emerging architectures (e.g., Keckler et al., 2011). Therefore, the ability to fully exploit new heterogeneous and many-core solutions is of paramount importance towards achieving optimal performance. In Paper I we have described the enabling of the gridding and w-correction algorithms to multi/many-core parallelism. We have demonstrated how utilising GPUs can significantly decrease the time to solution thanks to their outstanding performance. In addition, we have pointed out how multi/many-threads solutions allow using a smaller number of Message Passing Interface (MPI) tasks compared to a pure MPI set-up, mitigating communication-related problems. However, to fully harness power of GPU acceleration, it is critical to enable the full code for GPUs, including MPI communication. This is essential to: 1. speed-up all algorithmic components, in particular the FFT step. The performance of the code is hindered by non-accelerated parts, which become bottlenecks; 2. avoid unnecessary data movements between the host and the device, required, in the code presented in Paper I, to implement data communication among CPUs and for the FFT transform. Efficient code performance relies heavily on minimising data movements; 3. exploit high performance interconnect available among GPUs on the same computing node. In this paper, we discuss how we have tackled these challenges by exploiting compilers and libraries provided by the NVIDIA HPC Software Development Kit (SDK222https://docs.nvidia.com/hpc-sdk/index.html). These libraries enable us to perform accelerated FFT calculations and to optimise inter-GPU MPI communication. This leads to a code that, once ones has loaded the input visibilities from the file system, can fully run on GPUs; the CPUs are used once more only at the end of the calculation, solely for the purpose of saving the final image to a file. In addition, we present a solution based on FFTW that allows the utilisation of hybrid multi-core plus MPI calculation of the Fourier transform on CPUs. This is crucial to reduce the communication between MPI tasks while also providing an efficient and portable solution that is not dependent on the NVIDIA SDK. The code, called Radio Imaging Code Kernel (RICK), is developed using the C programming language standard (with extensions to C++ only to support GPUs through CUDA). The code is publicly available333The code is publicly available here: https://github.com/ICSC-Spoke3/RICK, with all the details on the compilation and execution of the code for different architectures; in future releases, we plan to build a specific container for RICK, for the sake of reproducibility of our results. Alongside CUDA, in Paper I we have introduced the Open Multi-Processing (OpenMP) support for offloading gridding and w-stacking operations to accelerators, providing a portable solution even for GPU implementation. The code is compatible with various computing platforms, although optimal performance requires the availability of suitable hardware and software solutions. Parallelism has been exploited supporting multi-core processors (via OpenMP) and distributed HPC architectures (via MPI). Throughout the paper, we refer to computing or processing unit as the computing entity addressing some parts of the work. In the case of parallel work based on MPI, a computing unit is a single core (mapping to an MPI task). In the case of multi-threaded OpenMP implementation, it is a multi-core CPU. In the case of accelerated computing, it is a GPU. We present the results obtained on a state-of-the-art supercomputing platform, namely the Leonardo pre-exascale system operated by CINECA, the Italian National Supercomputing Centre, ranked as seventh in the TOP500 list444https://www.top500.org of June 2024. CPU and GPU tests have been performed using the same code base switching between different building options by selecting the proper flags and macros in the Makefile and exploiting the NVIDIA SDK available on the system. All tests have been performed using LOFAR datasets, representative of current SKA-pathfinder radio-observations. The paper is organised as follows. The methods used for performing the w-stacking is described in Sec. 2, together with a summary of the code presented in Paper I. In Sec. 3 we introduce the solutions adopted for the full GPU enabling of RICK. In Sec. 4 the results of the performance and scalability tests are presented and discussed. Sec. 5 is devoted to the communication and to its impact on the code. Conclusions are drawn in Sec. 6."
https://arxiv.org/html/2411.07318v1,The Simons Observatory: Laboratory Beam Characterization for the first Small Aperture Telescope,"The Simons Observatory is a ground-based telescope array located at an elevation of 5200 meters, in the Atacama Desert in Chile, designed to measure the temperature and polarization of the cosmic microwave background. It comprises four telescopes: three 0.42-meter small aperture telescopes (SATs), focused on searching for primordial gravitational waves, and one 6-meter large aperture telescope, focused on studying small-scale perturbations. Each of the SATs will field over 12,000 TES bolometers, with two SATs sensitive to both 90 and 150 GHz frequency bands (SAT-MF1, and SAT-MF2), while the third SAT is sensitive to 220 and 280 GHz frequency bands. Prior to its deployment in 2023, the optical properties of SAT-MF1 were characterized in the laboratory. We report here on measurements of beam maps acquired using a thermal source on SAT-MF1, along with measurements of near-field beam maps using a holographic method that enables characterization of both the amplitude and phase of the beam response, yielding an estimate of the far-field radiation pattern received by the telescope. We find that the near-field half-width-half-maximum (HWHM) requirements are met across the focal plane array for the 90 GHz frequency band, and through most of the focal plane array for the 150 GHz frequency band. Namely, the mean of the bandpass averaged HWHM of the edge-detector universal focal plane modules match the simulated HWHM to 10.4 \%, with the discrepancy caused by fringing in the simulation. The measured radial profile of the beams matches simulations to within 2 dB from the beam center to at least the -10 dB level. Holography estimates of the far-field 90 GHz beams match the full-width-half-maximum from simulation within 1\%, and the beam radial profiles deviate by less than 2 dB inside the central lobe. The success of the holography and thermal beam map experiments confirmed the optical performance were sufficient to meet the science requirements. SAT-MF1 was deployed to Chile in June, 2023. On-site observations are currently underway.","1 INTRODUCTION The Simons Observatory (SO) is a ground-based telescope array dedicated to creating high-resolution and high-sensitivity temperature and polarization maps of the cosmic microwave background (CMB). It is currently deployed to the Atacama Desert and is composed of three small aperture telescopes (SATs) and one large aperture telescope (LAT). These telescopes work in tandem to observe the CMB over 6 frequency bands and over a large range of angular scales, using a combined \approx 60,000 transition-edge sensor (TES) detectors [1]. The six observed frequency bands are: the low frequency (LF) bands centered at 30 and 40 GHz, the middle frequency (MF) bands centered at 90 and 150 GHz, and the ultra high frequency (UHF) bands centered at 220 and 280 GHz. The LF bands are used to observe low-frequency synchotron emission, the UHF bands are used to characterize galactic dust and foreground signal, while the MF bands are used to analyze the peak CMB signal. The LAT has a 6m aperture and an 8^{\circ} field of view, allowing for the LAT to resolve the CMB sky at arcminute angular resolution [2]. The LAT’s science goals focus on small-scale anisotropies. The SATs have a 42 cm aperture and an 35^{\circ} field of view. The SATs have an angular resolution of a half-degree, and will be used to study large-scale B-modes [3]. Each SAT holds one optics tube and has \approx 12,000 detectors. Two of them will be sensitive to MF (SAT-MF1 and SAT-MF2), and one will be sensitive to UHF (SAT-UFH). One of the primary science goals of SO is to observe or constrain the tensor-to-scalar ratio, r, on the order of r=0.01 with uncertainty of \sigma(r)<0.003 [4]. SO requires accurate measurement of its beam shape and precise characterization of systematic errors throughout the optical system to observe the CMB sky at the precision needed to constrain r. Intensity-only (thermal) beam maps are used in cosmology as confirmation and calibration of telescope optics, testing the angular response of the telescope as a thermal source scans across the field of view of the focal plane. The full-width-half-max (FWHM), ellipticity, and side-lobes of thermal beam maps provide an accurate characterization of the telescope beam shape [5], allowing for the deconvolution of the telescope beam from the observed CMB signal [6] [7]. It is ideal to create beam maps in the far-field of the telescope before deploying the telescope to the site. However, observing far-field calibration sources from the laboratory is challenging. Near-field thermal beam maps act as a practical, first-order test of the telescope optics. Systematic errors in the observed near-field beam can be studied, allowing for problems to be rectified in the laboratory. Collaborators at the University of Chicago in 2021 applied the holographic method to measure the near-field beam intensity and polarization of the LAT-test receiver optics tube, providing an estimate of the far-field beam response of the telescope [8], [9]. From late 2022 to early 2023, we performed near-field holography and thermal beam map experiments to study the telescope optics of SAT-MF1 at the University of California San Diego. In this proceedings, we validate the near-field beam shape of the Simons Observatory SAT-MF1 via the near-field thermal beam maps, and estimate its far-field beam shape via the holography experiment. Section 2 details the experimental setup for the holography and thermal beam map experiments, and outlines the analysis pipeline used to compare experimental results to simulations. Section 3 introduces the validation metrics of the optics, and discusses the results of the experiments compared against these metrics."
https://arxiv.org/html/2411.08022v1,Commissioning of the 2.6 m tall two-phase xenon time projection chamber of Xenoscope,"Xenoscope is a demonstrator for a next-generation xenon-based observatory for astroparticle physics, as proposed by the XLZD (XENON-LUX-ZEPLIN-DARWIN) collaboration. It houses a \qty2.6m tall, two-phase xenon time projection chamber (TPC), in a cryostat filled with \qty∼360kg of liquid xenon. The main goals of the facility are to demonstrate electron drift in liquid xenon over this distance, to measure the electron cloud transversal and longitudinal diffusion, as well as the optical properties of the medium. In this work, we describe in detail the construction and commissioning of the TPC and report on the observation of light and charge signals with cosmic muons.","Two-phase (liquid and gas) xenon time projection chambers (TPCs) are powerful detectors in the fields of astroparticle physics and rare-event searches [1, 2]. Ongoing experiments based on this technology have reached the multi-tonne scale and ultra-low background rates [3, 4, 5]. Next-generation detectors plan to further increase the xenon target mass and thus the linear dimensions of the TPCs, in order to house several tens of tonnes of liquid xenon (LXe) [6, 7, 8, 9]. At the same time, the backgrounds will be reduced such that interaction rates will be dominated by cosmic and solar neutrinos [10, 11, 12] and by second-order weak decays [13]. The XENON-LUX-ZEPLIN-DARWIN (XLZD) experiment will operate a TPC with an active mass of \qtyrange6080t [8, 9]. This scale-up imposes a series of technological challenges to be addressed with large-scale demonstrators, such as PANCAKE [14] and Xenoscope [15]. The PANCAKE facility can house a TPC with a \qty2.6m diameter, while Xenoscope was constructed to operate a \qty2.6m tall TPC. The design and construction of Xenoscope, without the TPC, are detailed in ref. [15], and first results with a \qty53cm tall purity monitor were published in ref. [16]. In this work, the focus is on the new systems, in particular the \qty2.6m tall TPC, and on its commissioning phases with gaseous xenon (GXe) and LXe. We also show first scintillation and electroluminescence signals acquired from cosmic muons. The paper is organised as follows: section 2 gives an overview of the entire facility, section 3 details the \qty2.6m tall TPC and the systems designed and installed for its operation, section 4 describes the commissioning of the different systems, the first run of Xenoscope with the dual- phase TPC and the data acquired with cosmic muons. section 5 provides a summary of the results and an outlook towards the next steps of the project. Additionally, a new gas-extraction line was added and commissioned during the run and is reported in Appendix A."
https://arxiv.org/html/2411.07988v1,"Lecture notes for the 52 (March 2023) Saas-Fee Advanced School, Switzerland.","These are exciting times for studies of galaxy formation and the growth of structures. New observatories and advanced simulations are revolutionising our understanding of the cycling of matter into, through, and out of galaxies. This chapter first describes why baryons are essential for galaxy evolution, providing a key test of \Lambda-Cold Dark Matter cosmological model. In particular, we describe a basic framework to convert measurements of the gas properties observed in absorption spectra into global estimates of the condensed (stars and cold gas) matter mass densities. We then review our current understanding of the cycling of baryons from global to galactic scales, in the so-called circumgalactic medium. The final sections are dedicated to future prospects, identifying new techniques and up-coming facilities as well as key open questions. This chapter is complemented with a series of hands-on exercises which provide a practical guide to using publicly available hydrodynamical cosmological simulations. Beyond providing a direct connection between new observations and advanced simulations, these exercises give the reader the necessary tools to make use of these theoretical models to address their own science questions. Ultimately, our increasingly accurate description of the circumgalactic medium reveals its crucial role in transforming the pristine early Universe into the rich and diverse Universe of the present day.","1 The Baryon Census While tremendously successful, the \Lambda-Cold Dark Matter (\Lambda-CDM) model of cosmology remains incomplete. Indeed, 95% of the expected matter-energy content is in a form which is currently unknown. The \Lambda-CDM model posits the need of an elusive matter component, coined dark matter, which together with the dark energy dominate the Universe’s matter-energy budget. Figure 1 provides a visual representation of this. Figure 1: A visual representation of the Universe’s matter-energy budget. Dark energy is an unknown form of energy which affects the largest scales of the Universe. The \Lambda-CDM model postulates the existence of an elusive matter component, dubbed dark matter, which together with the dark energy dominate the Universe’s component budget. Baryonic matter refers to the ordinary, non-dark matter, material. Importantly, only a minority of the baryonic matter can be probed by the observations of the light from stars within galaxies. A central tenet is that the vast majority (90%) of the normal matter is in the form of intergalactic gas. Another challenge comes from the formation of structure in the Universe which is based on a well accepted cosmological framework. In a nutshell, matter and radiation initially coupled in the hot, post big-bang plasma where matter was distributed evenly throughout the cosmos. Small primordial density fluctuations at the epoch of the Cosmic Microwave Background (CMB) are amplified with cosmic time through gravitational instability. We currently have limited empirical information about this epoch dubbed ‘dark ages’. This is the period, however, at which the framework for the large-scale structures of the Universe begins to assemble. Dark matter produced the first cosmic web structures through collapse, while ordinary baryonic matter is able to gravitationally follow this structure. As fluctuations grow, they create a network of filaments of dark matter and an overdense gaseous medium. With only hydrogen and helium in atomic form and no metals yet, this epoch of structure formation gives rise to the first (Population III) stars which, to this date, remain challenging to observe. These stars are however important both because they likely produce the first ionising photons that would then reionise the Universe, but also because they pollute the pristine interstellar media with metals Welsh23 . Figure 2: Observations of light element abundances. These measurements, together with Big Bang nucleosynthesis, provide an estimate of baryonic matter density of \Omega_{\rm baryons}\approx 0.0490, in agreement with both Cosmic Microwave Background (CMB) anisotropies planck2016 and dispersion measures in Fast Radio Bursts (FRBs) estimates Macquart20 . The total amount of baryons in the Universe is thus well-constrained Cooke24 . Galaxy formation continues through Dark Matter halos growth and merging; baryonic matter accretes through cosmic web filaments, and fuels star formation and galaxy formation to make the first objects already observable at z\sim10 Carniani24 . The gas is of prime interest since it will flow along the filaments and feed the formation of galaxies, groups and clusters. However, several physical processes affect the dynamics, thermodynamics and composition of the gas. First of all, filaments are heated by gravitational contraction and shocks Kang05 . Second, the ultraviolet background from young stars and quasars photoionises this gas and heats it Gnedin10 ; Hoeft06 . At the same time, cooling effects in the densest regions, radiate the thermal energy of the gas. Finally, supernovae-driven winds enrich and heat this gas in the neighbourhood of star-forming galaxies Cen06 . Both hydrodynamical cosmological simulations and observations indicate that the rate of star formation is to first order predicted by the amount of cold, dense gas available at any cosmic epoch (see Section 3.1). These results help us understand how efficiently (in terms of conversion of gas into stars) and where exactly stars form. 1.1 Why is Baryon Physics key? Figure 3: Left: Recently, estimates of Dispersion Measures (DM) of Fast Radio Bursts (FRBs) with known redshifts has resulted in a third independent estimate of \Omega_{\rm baryons} Macquart20 . Right: Such studies will flourish as more observations are accumulating fast with upcoming facilities including CHIME which leads to tens of FRBs detections a day Abdalla22 . Baryonic matter refers to the ordinary, non-dark matter, material. The total amount of baryons in the Universe has been quantified by three independent sets of observations, all of which converge to similar value of \Omega_{\rm baryons}\approx 0.0490. On one hand, Cosmic Microwave Background (CMB) anisotropies planck2016 provides a measure of \Omega_{\rm baryons}. Additionally, observations light element abundances together with Big Bang nucleosynthesis cooke2018 ; Mossa20 ; Cooke24 lead to a measure of the baryon density (Figure 2). Indeed, the first synthesis of light elements (such as deuterium, helium and lithium) happened in the early Universe while heavier elements have been produced through stellar nucleosynthesis. Observations can be used to determine the primordial abundances of elements formed in the Big Bang, which provides a unique measure of the baryonic density of the Universe, \Omega_{\rm baryons}. Recently, Macquart20 estimated Dispersion Measures (DM) of Fast Radio Bursts (FRBs) with known redshifts resulting in a third independent estimate of \Omega_{\rm baryons} (Figure 3 left panel). Such studies will flourish as more observations are accumulating fast with upcoming facilities including CHIME which will lead to tens of FRB detections a day (Figure 3 right panel, Abdalla22 ). Importantly, only a minority of the baryonic matter can be probed by the observations of light in galaxies. The vast majority (90%) of the normal matter is in the form of gas as predicted by simulations in a typical phase diagram (Figure 4). This gas, notably the cold gas, provides the reservoir of fuel for forming stars and ultimately planets. A large reservoir of this gas still remains elusive and up to 20-50% of the baryons are thought to be hidden into the so-called Warm-Hot Intergalactic Medium (WHIM) often referred to as the “missing baryons” Cen99 ; Dave01 ; Driver21 as illustrated in 5. Accompanying hands-on #1 provides a starter guide to access similar particle properties in the TNG simulations Nelson13 . It also proposes a number of exercises to naviguate the interface to the simulations for the reader to get familiar with the working framework. Figure 4: Simulated phase-diagram at redshift z=0. The figure displays the density distribution of simulated particles in a temperature-density parameter space. Star formation is labeled ”cond” for condensed material, the cold diffuse gas as ”diffuse”, the Warm-Hot Intergalactic Medium is refered to as WHIM, while hot gas in groups of galaxies and galaxy clusters are labeled ”hot”. The fractions of gas-mass within each region, whose boundaries are indicated with black lines, is provided on the plot. This figure evidences the large contribution of the so-called WHIM to the matter budget torrey19 . Figure 5: Current census of baryons. Low-redshift budget of the baryonic matter probed by observations. About 20% of baryons have not yet been located or most importantly, their physical properties have not been characterised. These are referred to as the “missing baryons” DeGraaff19 . The current \Lambda-CDM paradigm successfully describes cosmological evolution of the large-scale structures of the Universe, organised in galaxies, clusters, sheets, and filaments separated by voids. However, \Lambda-CDM over-predicts the amount of small-scale structures, including the number of dwarf galaxies, a problem referred to as the “missing satellites problem” Bullock10 ; Perivolaropoulos22 . The lack of observed small-scale structures may imply the existence of physical mechanisms which would suppress small-scale fluctuations in particular. The missing dwarf galaxies in cosmological simulations Bullock10 relates to the poorly resolved baryonic processes on the small-scales, including gas inflows and outflows. Furthermore, constraining the matter distribution in the Universe and its evolution with cosmic time from the next generation of galaxy surveys will rely on various type of measurements. Weak gravitational lensing of galaxies in particular offers a promising avenue by measuring per-cent-level distortions of galaxies’ ellipticities which are caused by bending of the path of photons due to the effects of gravity Mandelbaum18 ; Mellier24 . These distortions map the distribution of matter in the Universe at various epochs and thus set new constraints on the physical properties of dark energy, theories of gravity, as well as the nature of dark matter. VanDaalen11 showed that the distribution of matter, which an inherent input to weak lensing analyses, is significantly affected by baryonic effects. The major impact takes place at Mpc-scales, where the power is suppressed due to active galactic nuclei-driven gas ejection. Cosmological hydrodynamical simulations make predictions of these physical processes at the relevant scales and their impact on the total matter power spectrum. Predictions vary vastly from one model to another, in part because the numerical methods differ but also because of the various implementation of baryonic (‘sub-grid’) processes (Figure 6). These effects have been major topic of research in recent years Semboloni11 ; Chisari18 ; Foreman20 ; Chisari18 ; Schneider15 ; Schneider19 ; Huang19 ; Debackere20 ; vanDaalen20 ; Amon22 ; Salcido23 ; Arico23 . Indeed, baryons are known to have effect on haloes: they compress the mass near the center, but suppress the density beyond the core which results in a reduction of the halo mass (and hence mass function) that becomes weaker towards higher masses Velliscig14 ; Cui14 ; Cusworth14 ; Bocquet16 ; HernandezAguayo23 . Currently, there is relatively little quantitative agreement among simulations regarding a robust description of the properties of gas flows, and specifically the implementation of feedback processes. Only detailed observations of star-forming and active galactic nuclei-driven winds (leading to measurements of velocity, gas phase, mass loading factor) will better constrain these complex physical processes. Observations of the baryonic matter are thus a unique means of tackling this degeneracy and providing new constraints on feedback models. Therefore, the modelled baryonic physics impact the properties of warm dark matter and thus the inference of cosmological parameters from weak lensing measurements from KIDS Janis19 , DES Abbott22 and the next generation of surveys such as LSST Ivezic19 , Euclid Amendola18 and Roman/WFIRST Spergel15 . Figure 6: Dark Matter power spectrum. The figure presents the ratio of the predictions from models including baryonic physics (labelled ”hydro”) to Dark Matter Only (labelled ”DMO”). The lines display various state-of-the-art cosmological hydrodynamical simulations. Both the sign and amplitude of the difference between these predictions is important, especially at large k-values, corresponding to small physical scales where baryonic physics dominate Huang19 . {trailer} Hands-on to analyzing cosmological galaxy formation simulations These hands-on sections walk through a “getting started” guide for analyzing cosmological hydrodynamical simulations of galaxy formation – like IllustrisTNG. Additional documentation and further information is available at www.tng-project.org/data. The content is split into four sections, with a brief setup first. 1. A first plot from the Group Catalog 2. Galaxy population relations and integral properties 3. Observables: predicting gas absorption/emission 4. The baryon cycle: measuring mass flow rates [0] Setup First, we import the helper scripts for loading the simulation data: import illustris_python as il import matplotlib.pyplot as plt import numpy as np [Hands-on #1] A first plot from the Group Catalogs Define the path for the data, i.e. choose the simulation we want to work with. Always start with a low resolution simulation for testing, as the size of data (i.e. time needed to load and plot) is smaller. Once you have a piece of code working, you can always change the path to a higher resolution simulation. basePath = ’sims.TNG/TNG100-3/output/’ We can then load fields from the z=0 group catalog. We specify the redshift by the snapshot number: snap_number = 99 We can load any field which is available (i.e. pre-computed) in the group catalogs. In particular, plot the relationship between star formation rate and stellar mass for galaxies. To do so, we will load both quantities, for all subhalos: • masses ”by type” (SubhaloMassInRadType), • star formation rates (SubhaloSFRinRad). Note: the ”InRad” suffix indicates that both measurements are considering only particles/cells within twice the stellar half mass radius, which is an OK first definition for the size of a galaxy. Other measurements of these same quantities, following other definitions, are also available. fields = [’SubhaloMassInRadType’,’SubhaloSFRinRad’] subhalos = il.groupcat.loadSubhalos(basePath, snap_number, fields=fields) Then we can inspect the result: subhalos.keys() dict_keys([’count’, ’SubhaloMassInRadType’, ’SubhaloSFRinRad’]) subhalos[’count’] 118820 subhalos[’SubhaloMassInRadType’].shape (118820, 6) Inspecting the return, we see it is a normal dictionary. The ”count” indicates that there are about 100k total subhalos (in TNG100-3, versus 4.4 million in TNG100-1). Each requested field is returned as a numpy array. We can now make a plot of star formation rate versus stellar mass. To do so, we need to select the values of interest from the catalog arrays, and be careful of physical unit conventions. (Always check the units and definition of fields before use). # select which particle type we are interested in mass_stars_code = subhalos[’SubhaloMassInRadType’][:,4] # careful of units! mass_stars_msun = mass_stars_code * 1e10 / 0.6774 sfr_msun_per_yr = subhalos[’SubhaloSFRinRad’] # plot fig, ax = plt.subplots() ax.plot(mass_stars_msun, sfr_msun_per_yr, ’.’, ms=1.5) ax.set_xscale(’log’) ax.set_yscale(’log’) ax.set_xlabel(’Galaxy Stellar Mass [M$_\odot$]’) ax.set_ylabel(’Star Formation Rate [M$_\odot / yr$]’); Figure 7: Hands-on plot: the relationship between galaxy star formation rate and galaxy stellar mass, for the TNG100-3 simulation at z=0. Exercise * 1. Change the simulation above (e.g. to a higher resolution, or to a different TNG box), and remake the plot. Do you see any different features? What is your interpretation? Caution: are there any subhalos with ‘SFR == 0‘? They would have gotten lost due to the y-log axis. Find them, and include them on the plot. What is your interpretation of these subhalos? * 2. Quenched galaxies have largely stopped forming new stars. There are many definitions of quenched: one is that the specific star formation rate \rm{sSFR}=\rm{SFR}/M_{\star} is below a constant threshold value of 10^{-11}\rm{yr}^{-1} (at z=0). Compute and plot sSFR versus stellar mass for all galaxies. Add a horizontal line for this threshold. Then, measure the fraction of quenched galaxies as a function of mass (i.e. in N bins of stellar mass) and plot quenched fraction versus stellar mass. Does it make sense? 1.2 The Baryon Cycle Galaxies are not isolated islands. Their interactions with their environment profoundly influence their evolution. They form as gas cools and condenses at the centres of a population of massive halos growing by gravitational amplification of fluctuations in an initially near-uniform distribution of pre-existing dark matter. The canonical picture has galaxy growth being fed by inflows of gas from the intergalactic medium, IGM Dekel09 ; vandeVoort11 . Gas acquired through mergers or through accretion of IGM replenishes the fuel needed for star formation. These baryons from the cosmic web cool into a dense atomic then a molecular phase, which fuels star formation. These processes are collectively described as a self-regulation, where part of the gas is consumed in star formation while outflows eject some other quantity of gas from the system. Molecular gas is formed by complex small-scale physics including cloud collisions, dynamical/orbital features, or turbulence making the clouds become self-gravitating, and initiating star formation Gronke2017 ; gronke18 ; Gronke2022 . Another important component of these processes is the metal enrichment of the interstellar medium which in turns will affects the efficiency of star formation since not only do metals act as coolants but they also are fundamental to dust production by shielding molecules from dissociating radiation. Figure 8: The expected (black line) and observed (red line) galaxy luminosity function. The discrepancies in the low- and high-mass ends is related to Supernovae and Active Galactic Nuclei feedback, respectively Silk12 . The \Lambda-CDM cosmology also makes strong predictions on the halo mass function. Departures from these expectations with respect to the observed stellar mass function of galaxies are apparent at both the low and the high mass ends Silk12 . These deviations indicate a lower star formation efficiency in these two different mass regimes (Figure 8). To reconcile predictions from theory with observations, current simulations invoke feedback from various astrophysical processes at both mass scales. In low-mass galaxies, models include supernovae feedback to reproduce the observations while at high masses, feedback from an accreting super-massive black holes is required. The canonical picture is thus that once stars are formed, galaxies enrich the intergalactic medium with ionising photons and heavy elements formed in stars and supernovae, by driving galactic and active galactic nuclei-driven winds into the surrounding Pettini03 , some of which will fall back onto the galaxies in so-called galactic fountains Fraternali17 ; Bish19 . Gas flowing out of the galaxy (winds driven by supernovae or a super-massive black hole in the galaxy center) quenches star formation. Yet another important open question for galaxy formation models is to reproduce the low star formation efficiency observed in galaxies and dark matter halos. Figure 9: Illustration of intergalactic transfer. Hydrodynamical FIRE simulations predict the amount of baryonic material acquired through fresh accretion (purple), wind recycling (blue) or transfer from another haloe (green) as a function of redshift anglesalcazar17 . From a theoretical point of view, the difficulty is related to the various scales involved: simulating the large cosmological scales together with the pc-scale typical of interstellar physics poses an major dynamical scale computing challenge Crain23 . Recently, advances in numerical methods and computing capabilities have enabled extraordinary progress in the simulation of structure formation. Accompanying chapter by Jeremy Blaizot covers most of the fundamental processes implemented in current state-of-the-art simulations. It is a remarkable achievement that the overall physical properties (density and temperature) of the gas (specially HI) in these models reproduce observations of the absorbers column densities (i.e. the number of atoms along the line-of-sight) and line widths Fumagalli2011 ; Rahmati2014 ; gaikwad2017 . However, simulating the multiphase interstellar medium still remain challenging even in “zoom-in” simulations even with modern powerful computers. To overcome this challenge, it is necessary to make use of sub-grid modules to model unresolved physical processes, such as winds from dying stars, and supernovae teyssier2019 ; Maio22 ; Butsky24 . Only by implementing the most realistic physics will simulations be able to interpret contemporary observations. Important questions still to answer include which objects and media contribute to the global quantities. The question remains of the relation between the intergalactic gas relation and the interstellar medium of galaxies, including small-scale cold clumps and eventually molecular gas in the ISM of galaxies. Reaching a full understanding of the cycling of baryons between HI, H2 and the ionised phase of the gas requires even more advanced simulations. Hands-on #2 provides an opportunity to look at the gas properties in TNG simulations. A description of galaxy evolution thus requires to depict fully the ‘baryon cycle’, i.e., how material cycles through different phases (hot, warm and cold) and locations from outside galaxies into the interstellar medium and back Tumlinson17 ; FaucherGiguere23 . This baryon cycle is instrumental to the fueling and the regulation of star formation through and accretion and outflowing processes. The availability of the gas reservoir drives the instantaneous star formation rate and therefore directly governs the growth of galaxies and super massive black holes. More globally, the temporal and spatial evolution of material describes these processes of motion and transformation of the baryons Walter2020 ; Tacconi2020 ; PerouxHowk20 . The inflows and outflows exchange gas, metals, and angular momentum between the galaxy and its atmosphere. A detailed probe of baryons exchanges is of paramount importance for understanding these processes (Figures 9). Since gas, stars, and metals are intimately connected, gas flows affect the history of star formation and chemical enrichment in galaxies. Understanding this cosmic baryon cycle is crucial for understanding galaxy evolution. A comprehensive model of structure formation must thus consider our Universe as one large, complex ecosystem in which galaxies, stars and ultimately planets can form on different scales. 1.3 The Circumgalactic Medium In this context, the enriched circumgalactic gas, or CGM, surrounding galaxies provides the most direct probe of inflows and winds, which are driven by either stars, supernovae, active galactic nuclei and/or cosmic rays. Ultimately, we would like to measure the mass moves in these processes and how this evolves with time as well as obtain a description on how material enriched in metals, dust and energy eventually cycles back onto the galaxy. New observations and modern simulations reveal that the large, diffuse gas reservoir of the CGM is both multi-scale and multi-phase FaucherGiguere23 . Recently, it has been suggested the cloudlets of cold gas could be entrained in hotter medium those providing a theoretical framework for the recent observational results Gronke2022 . Cold CGM gas also plays a role in modulating halo cooling, act as a reservoir for star formation, and thus affects the galactic feedback processes. The CGM therefore has become a central component of studies of galaxy evolution and there structure formation. Over the past decade, new observations have revolutionised the study of the circumgalactic gas surrounding galaxies Tumlinson11 ; Borthakur13 ; Werk14 ; Heckman17 ; Lehner18 ; Muzahid18 ; Prochaska19 ; Chen2020 ; Peroux2020 . These studies show that the CGM is multiphase (with cold, warm, and hot gas) and makes up a large fraction of the baryon and metal budget. More recently, some works focused on the material closer to the galaxy (i.e. <0.5 Rvir), referring to it as the inner CGM. The CGM is increasingly recognised for its significant role in driving the evolution of galaxies. Hydrodynamical simulations (e.g., IllustrisTNG naiman18 ; pillepich18b ; nelson18a ; marinacci18 ; springel18 , EAGLE crain15 ; schaye2015 ; mcalpine2016 , SIMBA dave16 , FIRE FaucherGiguere16 ) also show the complexity of the CGM. A large fraction of the CGM seems to be bound to the galaxy and may have circulated multiple times through the galaxy (Figures 9). Recently, several groups have implemented different solutions to improve the spatial resolution in the lower density CGM of hydrodynamical simulations and to systematically vary the properties of simulated halos. An increasing number of physical processes, such as galactic winds and jets are being incorporated, essential to increase the realism of the simulations. Cosmic rays, turbulence, and magnetic fields likely also impact CGM physics significantly pakmor17 ; vandeVoort2021 ; Ramesh2023 . Full radiation hydrodynamics simulations in large boxes are also now becoming feasible, providing insight into the role that radiation and thermal pressure play in regulating the hydrogen and helium reionization processes and the structure of the Lyman-\alpha forest. Finally, efficient emulators that utilise GPU acceleration or machine learning techniques are now being developed for sampling the large parameter space relevant for cosmological analyses VillaescusaNavarro21 ; Appleby23 ; Gebek23 . {trailer} Hands-on to analyzing cosmological galaxy formation simulations [Hands-on #2] Galaxy population relations and integral properties When a physical quantity that you are interested in is already in the group catalogs (or can be computed from quantities in the catalogs), this is a good starting point. We consider a ”galaxy gas fraction” defined as f_{\rm gas}=M_{\rm gas}/(M_{\rm gas}+M_{\star}). For a quick look, we load the first 5 subhalos of TNG100-1 at z=0, and print this gas fraction. basePath = ’sims.TNG/TNG100-1/output/’ snap = 99 for i in range(5): sub = il.groupcat.loadSingle(basePath, snap, subhaloID=i) gas_mass = sub[’SubhaloMassInHalfRadType’][0] # units? stars_mass = sub[’SubhaloMassInHalfRadType’][4] # units? fgas = gas_mass / (gas_mass + stars_mass) print(i, fgas) 0 0.097318634 1 0.0105384765 2 0.014834739 3 0.0027430148 4 0.0047753155 Q: Within what radius (or ”aperture”) is this gas fraction computed? Q: Why is the first f_{\rm gas}\sim 10\%, but the other four are \lesssim 1\%? Exercise Make a scatterplot of the gas fraction of galaxies as a function of galaxy stellar mass: use ‘loadSubhalos()‘. Overplot a line representing a running mean (or median). What is your interpretation of the typical behavior with mass? Exercise What if we were interested in the ”halo gas fraction” f_{\rm gas}=(M_{\rm gas}/M_{\rm total}) instead? This would tell us about the gaseous content of the CGM, rather than of the galaxy itself. What aperture/definition would you want to use? What field from the group catalogs would be most appropriate? Make a scatterplot of halo gas fraction as a function of (i) galaxy stellar mass, (ii) total halo mass. Exercise Until now, we have been plotting all subhalos in the group catalog – that is, both ”centrals” and ”satellites”, where a satellite is another haloe within the Virial radius of the central. It is always a good idea to consider centrals and satellites separately. Look at the list of halo fields, and subhalo fields, in the group catalog. Which fields provide the links between the two? (There are ”links” in both directions). 1. Create a three lists of subhalo indices (or ”IDs”): (a) all subhalos, (b) central subhalos only, and (c) satellite subhalos only. 2. Print out the list of central subhalo IDs. What do the gaps represent? 3. Plot the ”satellite fraction” of subhalos as a function of mass. 4. Repeat the gas fraction plot from above, (over)plotting centrals and satellites separately. What is your interpretation? Exercise (Challenge) We have not yet seen the “merger trees”. We can explore how we follow a subhalo through time, and see how it evolves. We will use the ‘SubLink‘ merger tree. 1. Look at the documentation for the il.sublink.loadTree() function. 2. Consider TNG100-1 at z=0. Select all central subhalos whose **parent halo** has 11.5<M_{\rm 200c}/\rm{M}_{\odot}<11.6. How many are there? 3. For the first N=10 of these, load the ”main progenitor branch” (MPB) of each. (You should do this in a loop.) 4. Make a plot of gas fraction versus time (i.e. snapshot number, or redshift if possible), overplotting all N halos. Label each, in the legend, with the halo ID. 5. Add the median relation, taking the median across the N halos, as a solid black line. What is your interpretation?"
https://arxiv.org/html/2411.07970v2,MUltiplexed Survey Telescope: Perspectives for Large-Scale Structure Cosmology in the Era of Stage-V Spectroscopic Survey,"The MUltiplexed Survey Telescope (MUST) is a 6.5-meter telescope under development. Dedicated to highly-multiplexed, wide-field spectroscopic surveys, MUST observes over 20,000 targets simultaneously using 6.2-mm pitch positioning robots within a \sim 5\,{\rm deg}^{2} field of view. MUST aims to carry out the first Stage-V spectroscopic survey in the 2030s to map the 3D Universe with over 100 million galaxies and quasars, spanning from the nearby Universe to redshift z\sim 5.5, corresponding to around 1 billion years after the Big Bang. To cover this extensive redshift range, we present an initial conceptual target selection algorithm for different types of galaxies, from local bright galaxies, luminous red galaxies, and emission line galaxies to high-redshift (2<z<5.5) Lyman-break galaxies. Using Fisher forecasts, we demonstrate that MUST can address fundamental questions in cosmology, including the nature of dark energy, test of gravity theories, and investigations into primordial physics. This is the first paper in the series of science white papers for MUST, with subsequent developments focusing on additional scientific cases such as galaxy and quasar evolution, Milky Way physics, and dynamic phenomena in the time-domain Universe.","Over the past four decades, beginning with the “Stick Man” from the CfA Redshift Survey in 1982 [1, 2], spectroscopic mapping of large-scale structures (LSS) has accumulated more than 30 million redshifts of nearby and distant galaxies (see Figure 2). This monumental achievement has contributed significantly to the establishment of the current cosmological model \LambdaCDM, along with other cosmological probes, i.e., cosmic microwave background (CMB; [3, 4]), Type-Ia supernovae [5, 6, 7], or measurements of weak lensing (e.g., [8, 9]). For two decades (2000–2020), major experiments such as the Sloan Digital Sky Survey (SDSS; [10]), followed by the ongoing (2021–2026) survey from the Dark Energy Spectroscopic Instrument111https://www.desi.lbl.gov/the-desi-survey/ (DESI; [11]), have mapped the 3D universe at low and intermediate redshift (z\lesssim 3). Clustering measurements from spectroscopic surveys of galaxies and quasars have become a key probe of cosmology. They provide precise measurements on the baryon acoustic oscillations (BAO) scale [12] and the linear growth rate of structure f\sigma_{8} through redshift space distortions (RSD) [13]. Recent results from the DESI collaboration [14] suggest a potential deviation from the cosmological constant to time-varying dark energy. By the end of this decade, we expect to have sub-percent-level constraints on dark energy and gravity from galaxy clustering up to redshift z\sim 2. When the DESI project wraps up, we have finished the spectroscopic survey component of the four stages envisioned by the Dark Energy Task Force (DETF) report [15]. Yet, many fundamental questions remain unanswered, calling for a new era of cosmological experiments in the 2030s. Going one step further, 3D maps of the universe at high redshift (z>2) will enable the observation of linear modes in the primordial universe, significantly enhancing our ability to constrain dark energy and inflation [16]. In the next decade, a series of ground- and space-based photometric surveys CSST [17], Euclid [18], Nancy Grace Roman Space Telescope [19], LSST [20] will provide deep and high-quality images for future galaxy spectroscopic surveys allowing to target galaxies such as Lyman Break Galaxy (LBG) or Lyman-\alpha emitter (LAE) [21, 22] as tracer of matter at high redshift 2<z<5. Large-volume high-redshift redshift surveys using these new tracers have the unprecedented potential to help us test primordial non-Gaussianity, probe dynamic dark energy, and reveal possible new features in the primordial power spectrum, uncovering tantalizing hints for new physics. At the same time, a high-density spectroscopic survey of low-redshift (z<1.5) can provide a high-fidelity 3D map of the cosmic web and trace the matter distributions into the non-linear regimes, opening doors to unexplored scientific opportunities. Such a multi-purpose dataset can also enhance the scientific performance of other cosmological probes, such as calibrating the photometric redshift and intrinsic alignment models for weak gravitational lensing surveys (e.g., [23]; [24, 25]), providing spectroscopic follow-up and host galaxy properties for supernova surveys (e.g., [26, 27]), or exploring new approaches to map the low-redshift large-scale structures (LSS) such as a peculiar velocities survey (e.g., [28, 29]). More importantly, it will help maximize the potential for synergies between spectroscopic surveys and other cosmological probes, such as CMB (e.g., [30]), weak lensing (e.g., [31, 32, 33, 34]), and intensity mapping (IM, e.g., [35]) experiments. Motivated by these two promising directions, the cosmological & high-energy physics community has recently coined the concept for a Stage-V spectroscopic experiment (e.g., [36, 37]) to fulfill these high expectations. By definition, a Stage-V spectroscopic facility should utilize a telescope with high etendue value (A\Omega; A and \Omega are the collecting area and the field-of-view of the telescope) to ensure a high survey speed. More importantly, the facility should have significantly improved multiplexed capability (number of targets that can be observed simultaneously) compared to the Stage-IV survey (e.g., 5,000 fibers for DESI). Conceptually, a Stage-V facility demands a minimum of 10,000 fibers that could be easily reconfigured to target different objects. This is the primary technical challenge now for such an ambitious vision. At the same time, a Stage-V facility should also have excellent optical performance, high-performance multi-object spectrographs that at least cover the whole optical wavelength range, and a site with good observing conditions. Building on these requirements, multiple ground-based concepts have been proposed, including the 6.5 m MegaMapper telescope [38, 39], the dual-6 m & dual-hemisphere Spec-S5 project222https://www.spec-s5.org/ [40], the 11 m Maunakea Spectroscopic Explorer333https://mse.cfht.hawaii.edu/ (MSE; [41]), the 12 m Wide-field Spectroscopic Telescope444https://www.wstelescope.com/ (WST; [42]), and the \sim12 m Extremely Large Spectroscopic Survey Telescope (ESST; [43]). The MUltiplexed Survey Telescope555https://must.astro.tsinghua.edu.cn/en (MUST) is a 6.5-meter telescope [44] under active development. MUST will be located at the 4358 m Peak A of Saishiteng Mountain in Qinghai, China. Equipped with over 20,000 fibers over a \sim 5~{}\rm{deg}^{2} field of view (FoV), it features three-channel spectrographs covering wavelengths from 370-960 nm, with spectral resolution between R=2,000 and 4,500. MUST is designed to conduct an ambitious Stage-V cosmological spectroscopic survey, aiming to precisely measure key cosmological parameters and improve our understanding of dark energy and cosmic evolution. With the first light scheduled for 2031, MUST expect to conduct the first Stage-V spectroscopic survey, targeting Lyman Break Galaxy (LBG) and Lyman-\alpha emitter (LAE) at high redshift across \sim 13,000 deg2 of the northern sky. Clustering analysis of these tracers will provide sub-percent precision measurements on BAO parameters – D_{A}(z)/r_{d} and H(z)r_{d} – and the linear growth rate of structure, f\sigma_{8}, at redshift 2<z<5, a region not yet covered by current spectroscopic galaxy surveys. Additionally, MUST will provide stringent constraints on primordial non-Gaussianities (PNG, local type) through the parameter f_{\mathrm{NL}}^{\mathrm{local}} with a precision of \sigma(f_{\mathrm{NL}}^{\mathrm{local}})\sim 1. This will enable stringent testing of a wide range of inflationary models. MUST is expected to provide sufficient precision (\sim 0.03\,{\rm eV} when combined with CMB data) on the sum of neutrino masses to constrain a nonzero neutrino mass with \sim 2\sigma significance, assuming normal hierarchy. The inverted mass hierarchy can be tested with a significance \sim 1.3\sigma. Finally, from power spectrum measurements of the Lyman-\alpha forest, MUST will yield the most precise constraint on warm dark matter mass to date m_{X}>10.5 keV at 95% confidence level (assuming 14,000 deg2 and k_{\rm max}=0.67 h Mpc-1). Besides the unique potential of the MUST project, synergies with other cosmological surveys, i.e., future imaging surveys (e.g. CSST, Euclid, LSST), CMB experiments (e.g., Simon Observatory [45], CMB-S4 [46], LiteBird [47]) or radio surveys (e.g., SKAO [48]) will enhance the constraining power of MUST and will result in a better understanding of our Universe. As a dedicated spectroscopic survey facility, MUST can also carry out spectroscopic surveys supporting a wide range of scientific topics outside of LSS cosmology, such as the study of galaxy evolution, super-massive black hole (SMBH), the structure of the Milky Way, and time-domain astrophysics. This paper describes the MUST instrument and the scientific objectives of the cosmological survey that will be conducted over 5 years of observation. Section 2 provides an overview of the MUST project, including the current status of the whole project, the design of the telescope (Section 2.1), the focal plane system & the spectrograph (Section 2.2), the site & observing condition (Section 2.3), and the overall scientific capabilities (Section 2.4). Section 3 describes the key scientific motivations of MUST for the Stage-V cosmological surveys. The primary scientific goals are covered in detail while briefly summarizing the potential for new probes and the potential synergies with other cosmological surveys. Section 4 presents the current target selection strategy and provides the redshift distribution and target density estimations for the cosmological forecast. We will also introduce the conceptual survey design for MUST. Finally, Section 5 describes the method to forecast the cosmological potential of MUST theoretically and summarizes the forecasted results on dark energy, structure growth, primordial non-Gaussianity, neutrino mass, and warm dark matter constraints. Discussions and main conclusions of this project and future directions are described in Section 6. Throughout this work, we adopt as fiducial baseline \LambdaCDM cosmology with parameters H_{0}=67.6 km s-1 Mpc-1, \Omega_{b}=0.046 and \Omega_{m}=0.31. All magnitudes in this work are defined in the AB magnitude system [49]. Figure 1: Overview of the MUST project. The top right panel shows the location of the MUST site currently selected in Qinghai province of China. The top left panel is a picture of the Saishiteng mountain near Lenghu. We highlight the peaks that have been or are being developed for astronomy. The highest peak – Peak A – with an altitude of 4358 m, was selected as the site for MUST. From left to right, the bottom panels illustrate the preliminary design of the dome of MUST and the telescope, a sketch of the conceptual design of the focal plane of MUST, fiber, and spectrograph systems, and the fiducial design of the modular focal plane of MUST. By current design, MUST will host 21,168 robotic fiber positioners using 336 triangular modules."
https://arxiv.org/html/2411.07394v1,The Mega-MUSCLES Treasury Survey: X-ray to infrared Spectral Energy Distributions of a representative sample of M dwarfs,"We present 5–1\times 10^{7} Å spectral energy distributions (SEDs) for twelve M dwarf stars covering spectral types M0–M8. Our SEDs are provided for community use as a sequel to the Measurements of the Ultraviolet Spectral Characteristics of Low-mass Exoplanetary Systems (MUSCLES) survey. The twelve stars include eight known exoplanet hosts and four stars chosen to fill out key parameter space in spectral type and rotation period. The SEDs are constructed from Hubble Space Telescope ultraviolet spectroscopy and XMM Newton, Chandra and/or Swift X-ray observations and completed with various model data, including \mathrm{Lyman}\,\alpha reconstructions, PHOENIX optical models, APEC coronal models and Differential Emission Measure models in the currently-unobservable Extreme Ultraviolet. We provide a complete overview of the Mega-MUSCLES program, including a description of the observations, models, and SED construction. The SEDs are available as MAST High-Level Science Products and we describe the various data products here. We also present ensemble measurements from our sample that are of particular relevance to exoplanet science, including the high-energy fluxes in the habitable zone and the FUV/NUV ratio. Combined with MUSCLES, Mega-MUSCLES provides SEDs covering a wide range of M dwarf spectral types and ages such that suitable proxies for any M dwarf planet host of interest may be found in our sample. However, we find that ultraviolet and X-ray fluxes can vary even between stars with similar parameters, such that observations of each exoplanet host star will remain the gold standard for interpreting exoplanet atmosphere observations.","M dwarf stars, with masses \approx 0.1-0.5 \mathrm{M}_{\odot}, have emerged as the premier targets for exoplanet discovery and characterization over the past two decades (Tarter et al., 2007), particularly for rocky planets with masses and radii comparable to that of Earth in orbits within the habitable zone around the host star. The focus on M dwarfs is partly due to their great abundance, representing over 70 % of stars (Henry et al., 2006), but mostly for observational considerations. The sensitivity of both of the dominant techniques for exoplanet discovery, the transit and radial velocity methods, scale dramatically and favourably with decreased stellar mass, such that planets with similar mass and radius to the Earth are much easier to find around M dwarfs than around larger stars. Additionally, the habitable zones of M dwarfs are much closer in compared to the habitable zones around larger stars due to their lower luminosity, so the orbital periods of planets there are of order days to weeks rather than years, requiring less time-on-target to detect and confirm. Short orbital periods, and a relatively small planet-to-star size ratio, also makes M dwarf planets favourable targets for atmospheric characterisation via transit spectroscopy. The importance of M dwarfs to exoplanet science has been highlighted by some of the most remarkable discoveries, such as the habitable zone planet orbiting Proxima Centauri (Anglada-Escudé et al., 2016) and the seven Earth-sized planets around TRAPPIST-1 (Gillon et al., 2016, 2017). The launch of JWST has enabled an increasing number of atmospheric observations of planets orbiting M dwarfs, ruling out thick atmospheres around several of the most accessible targets (Greene et al., 2023; Zieba et al., 2023; Lincowski et al., 2023; Moran et al., 2023). With the advent of the Rocky Worlds program (Redfield et al., 2024), continued JWST observations of planets around M dwarfs are guaranteed for years to come. When attempting to assess the characteristics and potential habitability of such planets, we must be cautious when comparing them with the Solar system (Rugheimer et al., 2015a; Rugheimer & Kaltenegger, 2018). The differences between M dwarfs and Sun-like stars are many. Their lower temperature means that the spectrum is shifted into the red, and the average M dwarf is much more active than more massive stars, both in terms of flare frequency and maximum flare strength relative to its luminosity (Loyd et al., 2018b; Froning et al., 2019), with the potential effects on their planets exacerbated by the close-in habitable zones (Buccino et al., 2007; Vida et al., 2017). Perhaps the starkest difference between M dwarfs and Sun-like stars are their high energy spectral energy distributions (SEDs). Multiple observations have demonstrated that the X-ray fluxes of M dwarfs relative to their bolometric luminosities can be thousands of times higher than for Sun-like stars (Wheatley et al., 2017; Wright et al., 2018; Brown et al., 2023). Conversely, the red-ward shift of the photospheric spectrum results in relative near-ultraviolet flux ratios being lower than Sun-like stars by a similar factor. The X-ray and ultraviolet fluxes are key drivers of exoplanet atmospheric structure and chemistry. Photochemistry in the upper atmospheres of exoplanets (i.e., the layers that we can most readily observe) is governed by the incident ultraviolet spectrum, and in particular the strength of the \mathrm{Lyman}\,\alpha line and the ratio of FUV to NUV fluxes (Segura et al., 2005; Moses et al., 2013; Miguel et al., 2015; JWST Transiting Exoplanet Community Early Release Science Team et al., 2023). X-ray and EUV radiation drives atmospheric escape, such that planets in extreme XUV environments may not retain atmospheres at all (Watson et al., 1981; Poppenhaeger et al., 2024; Van Looveren et al., 2024). A number of surveys have been pursued over the last several years to use HST and other facilities to obtain energetic spectra of M dwarf stars. Programs like Living with a Red Dwarf, HAZMAT, MUSCLES, and FUMES, plus observations focused on individual targets of interest, have greatly expanded the sample of M stars with observed ultraviolet spectra (Guinan et al., 2016; Loyd et al., 2021, 2018a; France et al., 2016; Youngblood et al., 2016; Loyd et al., 2016, 2018b; Bourrier et al., 2017; MacGregor et al., 2021; Waalkes et al., 2019; Diamond-Lowe et al., 2022; Pineda et al., 2021a; Feinstein et al., 2022; Rockcliffe et al., 2021). Ideally, a full X-ray through optical SED should be obtained for any M dwarf of interest, but such observations are often impractical for multiple reasons. Whilst the high energy emission from M dwarfs is a higher fraction of the bolometic flux relative to a larger star, it is still intrinsically faint, requiring large investments of limited space telescope observing time for even the closest M dwarfs. Accordingly, the MUSCLES Treasury Survey was conceived to provide complete SEDs for a representative sample of low mass stars, while also providing scaling relations for other surveys with more targets but less wavelength coverage (Youngblood et al., 2017). MUSCLES used HST in combination with X-ray facilities and optical photospheric model atmospheres to obtain 5 Å – 5.5 \mum spectral energy distributions (SEDs) for seven M dwarf and four K dwarf stars. High-level data products were delivered to MAST as a resource for the exoplanet modeling community as well as stellar astrophysicists to provide a uniform ultraviolet survey for the study of M dwarfs that includes reliable measurements of Ly\alpha and resolved stellar emission lines. The MUSCLES SEDs High Level Science Product archive111https://archive.stsci.edu/prepds/muscles/ has been used in a number of theoretical studies to model the atmospheric properties of exoplanet atmospheres (Spake et al., 2018; Wunderlich et al., 2020; Kawashima et al., 2019), predict observable atmospheric signatures that can be detected with the JWST (Morley et al., 2017), and investigate the effects of stellar flares on planetary atmospheres and potential life (Miguel et al., 2015; Louca et al., 2023) . The original MUSCLES survey focused on stars with known exoplanets. Given the known nearby systems at the time, largely found by optical radial velocity searches, this resulted in a sample that was biased toward the higher mass end of the M dwarf sequence. Only threee MUSCLES targets, GJ 1214, GJ 1061 and Proxima Centauri, had masses below <0.3 \mathrm{M}_{\odot}. With the launch of JWST, the search for “Earth-like” planets is moving toward the low mass end of the stellar Main Sequence. Surveys including MEarth, TRAPPIST/SPECULOOS, and TESS are finding Earth-sized planets around stars with masses of \sim0.3 \mathrm{M}_{\odot} or below (Berta-Thompson et al., 2015; Dittmann et al., 2017; Winters et al., 2019; Pidhorodetska et al., 2021). It is targets like these, around low mass stars, that will be the subject of atmospheric characterization of rocky planets in the next decade or more (Kempton et al., 2018; Fortenbach & Dressing, 2020; Redfield et al., 2024). It was with this in mind that the Mega-MUSCLES Treasury Survey was conceived as the successor to MUSCLES. Mega-MUSCLES observed thirteen M dwarfs across a range of mass, age, and activity levels (Figure 1), with a particular focus on low mass M stars that are the likely first targets for atmospheric characterization of Earth-like planets. The sample list included observations for six more of the closest low-mass exoplanet host stars; for slow-rotating stars to 0.14 \mathrm{M}_{\odot} (to serve as proxies for future habitable planet hosts); for stars like GJ 1132 but with faster rotation periods (to track XUV evolution with age); for the 2nd (GJ 699 = Barnard’s Star) and 7th (GJ 729) closest star systems; and for the multiple rocky planet system, TRAPPIST-1. The target list included stars with no known exoplanets that filled out the phase space of M dwarf mass and rotation/activity. When combined with the existing observations, the complete Mega-MUSCLES library spans a range of stellar masses (0.14–0.8 \mathrm{M}_{\odot}), high to low X-ray luminosity fraction (an indicator of activity level), and planetary systems ranging from Jupiters to super-Neptunes to super-Earths. In this paper, we present an overview of the Mega-MUSCLES Treasury Survey. Several papers have already been published focusing on individual targets of interest (Froning et al., 2019; France et al., 2020; Wilson et al., 2021), describing the X-ray observations for both MUSCLES and Mega-MUSCLES in detail (Brown et al., 2023), using the observations to probe proxies of stellar coronal and chromospheric behavior (Linsky et al., 2020), and developing optical proxies for high energy emission (Melbourne et al., 2020). This paper will focus on summarizing the survey observing methods, SED generation, and analysis of the data as a population. Figure 1: Left panel: Conservative (green) and optimistic (orange) Habitable Zones from Kopparapu et al. (2014) as a function of stellar effective temperature for the Mega-MUSCLES sample, along with their planetary systems (Table 1). Note that GJ 849 b & c are off the x-axis scale at 2.409 au and 4.974 au respectively. Right panel: Stellar masses, temperatures and rotation periods for the Mega-MUSCLES sample. Stars from Newton et al. (2018) are shown in grey for comparison."
https://arxiv.org/html/2411.07313v1,Quantum Metrology for Gravitational Wave Astronomy,"Einstein’s General Theory of Relativity predicts that accelerating mass distributions produce gravitational radiation, analogous to electromagnetic radiation from accelerating charges. These gravitational waves have not been directly detected to date, but are expected to open a new window to the Universe in the near future. Suitable telescopes are kilometre-scale laser interferometers measuring the distance between quasi free-falling mirrors. Recent advances in quantum metrology may now provide the required sensitivity boost. So-called squeezed light is able to quantum entangle the high-power laser fields in the interferometer arms, and could play a key role in the realization of gravitational wave astronomy.","I Gravitational waves Gravitational waves are ripples in space-time, i.e. dynamic changes in space curvature that propagate at the speed of light. According to GR they are transverse and quadrupolar in nature, have two polarization states, and are extremely weak. GWs of detectable amplitude cannot be generated on Earth, but a variety of known astrophysical and cosmological sources are predicted to emit gravitational radiation that should reach the Earth with a strength within reach SSc09 ; GWICweb . Whilst GWs have not yet been directly observed their existence is beyond doubt. A binary system of compact objects, such as neutron stars (as depicted in Figure 1) or black holes, emit GWs at twice their orbital frequency . The energy carried away by the GWs leads to a precisely predictable decay in the orbital period of the binary. Hulse and Taylor verified this mechanism for orbital decay to exquisite precision with observations of the binary pulsar system PSR1913+16, WTa05 . Their discovery is regarded as unequivocal, albeit indirect, proof of the existence of GWs that led to the 1993 Nobel Prize in Physics. GWs from complex astrophysical sources carry a plethora of information that will have a major impact on gravitational physics, astrophysics and cosmology. GW signals are typically distinguished in one of four broad and often overlapping classes SSc09 ; GWICweb , based on expected waveforms, and hence optimal search techniques. They are: binary inspirals and mergers, burst sources, periodic sources, and stochastic sources. In the following we briefly review the physics and astrophysics that can be extracted from the observation of GWs emitted by these sources. Binary inspirals and mergers The final stages of life of neutron star binaries will provide the richest signals, see Fig. 1. As the binary loses energy, the orbital period decreases and enters the human audio frequency band. After another \approx 100 cycles the stars merge in a catastrophic explosion providing a GW burst signal of a few hundred Hertz up to a kiloHertz. The merger is expected to produce a black hole surrounded by a torus which will release a giant burst of gamma rays. Simultaneous observation of GWs and gamma rays would confirm that the merger of neutron stars is the engine of many of the observed short, hard gamma ray bursts AbbottETAL08a . Recent advances in numerical relativity now make it possible to make predictions of the waveforms generated around the merger BGR08 . Comparison with observed waveforms will provide accurate tests of GR in the hitherto untested strong-field regime. The imprint of tidal distortions on the GW waveform from a binary system with at least one neutron star will constrain the equation of state of the nuclear matter making up the star. Independent of the nature of the binary, the final state of the merger will be a perturbed black hole, whose oscillation modes will decay in time producing more gravitational radiation. Such observations offer a striking confirmation of the existence of black holes. The famous “no-hair” theorem says that black holes are completely characterized by their mass and angular momentum Chandrasekhar98 . Measuring the GWs emitted by black hole binary systems where the mass ratio of the components is large, the “no-hair” theorem can be tested. Direct observation of the gravitational waveforms from inspiralling black holes and neutron stars can also provide the luminosity distance to the source without any complex calibrations SSc09 . If, in addition, the redshift can be measured (via the identification of electro-magnetic counterparts), the Hubble parameter Sch86 , the dark energy and dark matter content of the Universe and the dark energy equation of state can be determined. Figure 2: Gravitational waves GWs are dynamical deformations of space-time perpendicular to the direction of wave propagation. As a result, distances between free-falling test masses in a transverse plane will change with a strain h=\Delta L/L with quantities defined as shown. For a black hole or neutron star binary system with orbital frequency f_{\rm{BS}} distances will oscillate at twice that frequency f=f_{\rm{BS}}. The wavelength of this oscillation is given by \lambda=c/f with c the speed of light. Burst sources Burst sources refer to short-lived GW transients, the main known candidates being core-collapse supernovae and collapses to black holes DFM01 ; BRe06 . Observation of GWs will open a way to extract information about the dynamics occurring in the core of the supernova, and should complement and enhance the understanding gained from electromagnetic observations. Periodic sources Spinning compact objects will generate periodic GW signals depending on the degree of non-axisymmetric deformations OGu69 (departure from rotational symmetry is a necessary ingredient for generation of quadrupolar moments). Detection of GWs from such sources will confirm models of the underlying physics which might allow the growth of a “mountain” on a neutron star. The lack of observation of GWs from the Crab Pulsar at the sensitivity of current ground based detectors has already constrained its deviation from rotational symmetry AbbottETAL08a . The distribution of neutron stars in the Galaxy could be mapped out using GW observations. Spinning neutron stars currently invisible on Earth could be detected via their GW emission Abb09 . Stochastic sources Stochastic sources have both astrophysical and cosmological origins Peebles93 ; Mag00 . The “holy grail” is the Big Bang itself. In principle, we should be able to observe a relic background of GWs from the very early Universe, some time between 10-18 seconds and 10-9 seconds after the Big Bang, when light did not even exist. The electromagnetic analogue of this radiation is the cosmic microwave background, which gives information about conditions in the Universe 385,000 years after the Big Bang BenETAL03 ; SpeETAL03 . Gravitational radiation is the only way to observe the conditions in a much earlier epoch. Absence of a detectable stochastic background signal in current GW detectors has constrained certain models of the early Universe based on cosmic superstring population LIGO-Nat09 . Of course the most tantalizing sources are those we do not yet know exist. The opening of every major new electro-magnetic window to the Universe has revealed major surprises that have revolutionized our understanding of the Universe. Observing the Universe with an entirely new messenger will very likely continue this tradition. Frequencies of GWs GW astronomy targets phenomena that involve astronomically large masses in acceleration. This, in turn, leads to the expectation that GW emission frequencies will be low, typically below a few tens of kiloHertz. A black hole binary system, for example, has to have an orbital period of just 0.02 s in order to produce GWs at f=100 Hz (Fig. 2). Supernova explosions are expected to have a broad spectral emission, with components that may reach kiloHertz frequencies. However, the strongest detectable GWs are expected at lower frequencies, all the way down to the millihertz or even nanohertz regime. Strength of GWs Gravitational waves that reach the Earth are extremely weak. For example, the merger of two neutron stars at the other end of our galaxy (D\approx 50,000 light years away) would produce a GW strain amplitude of about h\approx 10^{-19} SSc09 . The same source at the distance of about 60 million light years, where the Virgo cluster which comprises up to 2000 Galaxies are located, would result in a corresponding strain amplitude of only h\approx 10^{-22}. With the sophisticated technology now available, such tiny strains of space-time can be detected and it is very probable that there will be numerous direct detections in the coming decade."
https://arxiv.org/html/2411.07305v2,: Image-based photometric redshifts for AGN,"Context. Computing reliable photometric redshifts (photo-z) for active galactic nuclei (AGN) is a challenging task, primarily due to the complex interplay between the unresolved relative emissions associated with the supermassive black hole and its host galaxy. Spectral energy distribution (SED) fitting methods, while effective for galaxies and AGN in pencil-beam surveys, face limitations in wide or all-sky surveys with fewer bands available, lacking the ability to accurately capture the AGN contribution to the SED, hindering reliable redshift estimation. This limitation is affecting the many 10s of millions of AGN detected in existing datasets, e.g., those AGN clearly singled out and identified by SRG/eROSITA.Aims. Our goal is to enhance photometric redshift performance for AGN in all-sky surveys while simultaneously simplifying the approach by avoiding the need to merge multiple data sets. Instead, we employ readily available data products from the 10th Data Release of the Imaging Legacy Survey for the Dark Energy Spectroscopic Instrument, which covers ¿ 20,000 deg2 of extragalactic sky with deep imaging and catalog-based photometry in the grizW1-W4 bands. We fully utilize the spatial flux distribution in the vicinity of each source to produce reliable photo-z.Methods. We introduce PICZL, a machine-learning algorithm leveraging an ensemble of convolutional neural networks. Utilizing a cross-channel approach, the algorithm integrates distinct SED features from images with those obtained from catalog-level data. Full probability distributions are achieved via the integration of Gaussian mixture models.Results. On a validation sample of 8098 AGN, PICZL achieves an accuracy \sigma_{\mathrm{NMAD}} of 4.5% with an outlier fraction \eta of 5.6%. These results significantly outperform previous attempts to compute accurate photo-z for AGN using machine learning. We highlight that the model’s performance depends on many variables, predominantly the depth of the data and associated photometric error. A thorough evaluation of these dependencies is presented in the paper.Conclusions. Our streamlined methodology maintains consistent performance across the entire survey area, when accounting for differing data quality. The same approach can be adopted for future deep photometric surveys such as LSST and Euclid, showcasing its potential for wide-scale realization. With this paper, we release updated photo-z (including errors) for the XMM-SERVS W-CDF-S, ELAIS-S1 and LSS fields.","In recent decades, our understanding of Active Galactic Nuclei (AGN) and their role in galaxy and cosmic evolution has significantly advanced. These luminous celestial powerhouses are thought to be fueled by the accretion of matter onto supermassive black holes (SMBHs) located at the centers of galaxies, exerting intense energetic radiation across the entire electromagnetic spectrum, ranging from radio to \gamma-rays (Padovani et al., 2017). The close correlation observed between the mass of the central SMBH, whether active or inactive and the properties of its host galaxy’s bulge — such as the galaxy’s mass and velocity dispersion (e.g., Gebhardt et al., 2000; Ferrarese & Merritt, 2000) — suggests a co-evolutionary relationship between galaxies and their central engines (Kormendy & Ho, 2013; Heckman & Best, 2014). Ongoing research focuses on understanding scaling relations, the evolution of SMBHs within galaxies, and the interconnected rates of star formation (SFR) and black hole accretion (BHAR) over cosmic time (e.g., Madau & Dickinson, 2014). To further explore and address these unresolved topics requires diverse AGN samples with reliable redshifts to determine BH demographics and constrain models of galaxy evolution. For all these studies, redshift is an indispensable quantity, with spectroscopic redshifts (spec-z) remaining the preferred estimates for determining precise cosmic distances (Hoyle, 2016). However, while multi-objects spectrographs, such as the Sloan Digital Sky Survey (SDSS-V; York et al., 2000; Kollmeier et al., 2019), the Dark Energy Spectroscopic Instrument (DESI; DESI-Collaboration et al., 2016), the Subaru Prime Focus Spectrograph (PFS; Tamura et al., 2016) or the 4-metre Multi-Object Spectroscopic Telescope (4MOST; De Jong et al., 2019), are set to provide a drastic rise in the number of observed sources over the next several years, we are currently in the situation in which millions of AGN have been detected all-sky by various surveys (e.g., by the Wide-field Infrared Survey Explorer mission (WISE; Wright et al., 2010), and the extended Roentgen Survey with an Imaging Telescope Array (eROSITA; Merloni et al., 2012; Predehl et al., 2021), with only the brightest sources having been observed spectroscopically (Dahlen et al., 2013). The growing disparity between photometric and spectroscopic observations will only widen with upcoming surveys such as the Legacy Survey of Space and Time (LSST; Ivezic et al., 2019) and Euclid (Collaboration et al., 2024), covering unprecedented areas and depths (Newman & Gruen, 2022). Thus for the bulk of AGN, we must make use of multiband photometry and rely on photometric redshifts (photo-z). First implemented by Baum (1957) for inactive galaxies, these low-precision redshift estimates utilize photometric observations to effectively obtain a sparsely sampled spectral energy distribution (SED), trading precision for scalability. They encompass an array of techniques assuming color-redshift evolution (Connolly et al., 1995; Steidel et al., 1996; Illingworth, 1999; Bell et al., 2004), including template-based approaches (e.g., Bolzonella et al., 2000; Ilbert et al., 2006; Salvato et al., 2008; Beck et al., 2017), where redshifted models built on theoretical or empirical SEDs are fitted to observed multi-band photometry. Although a limited number of available bands can introduce uncertainties (see review by Salvato et al., 2018), photo-z methods offer an efficient way to estimate distances for all sources in an imaging survey, yielding highly accurate estimates with as few as three bands for passive galaxies (Benitez, 2000; Abdalla et al., 2011; Arnouts & Ilbert, 2011; Brescia et al., 2014; Desprez et al., 2020). By contrast, reliable photo-z for AGN have historically required highly homogenized photometry across ¿20 filters, which was only achievable in pencil-beam surveys (Salvato et al., 2011). As such, this level of detail continues to be unfeasible for wide-area surveys. However, with the 10th data release of the DESI Legacy Imaging Surveys (LS10, Dey et al., 2019), we now have a broad-sky survey that, while lacking NIR coverage, includes a few optical bands supplemented by mid-IR WISE data. This allows us to explore the possibility of generating reliable photo-z for AGN over the full sky, despite having fewer filters compared to the densely sampled pencil-beam surveys. SED fitting applied to a broad population of AGN remains particularly challenging due to the uncertainty and difficulty of disentangling the relative contributions of the nucleus and respective host to a given band (e.g., Luo et al., 2010; Salvato et al., 2011; Brescia et al., 2019). Since the accretion properties of SMBHs, often characterized as the bolometric luminosity divided by the Eddington limit, or the Eddington ratio, significantly influence the SED of AGN, the intense power-law continuum radiation can either partly (host-dominated) or entirely (quasar-dominated), outshine the respective host, hiding key spectral features that lead to redshift degeneracies (Pierce et al., 2010; Pović et al., 2012; Bettoni et al., 2015). Consequently, selecting a limited number of templates can be insufficient for correct redshift determination, while increasing the number of templates raises the degeneracy (see discussion in Salvato et al., 2011; Ananna et al., 2017). In this regime of accounting for AGN contributions to galaxy photo-z, one potential approach involves modeling objects as a combination of quasar and galaxy templates (eg., Cardamone et al., 2010), performed with EAZY (Brammer et al., 2008). In addition, surveys typically estimate fluxes with models that do not account for a mixed contribution from AGN and host galaxy. Ultimately, AGN are also intrinsically variable sources on the timescales explored by the previously mentioned surveys leading to incongruent photometry acquired across different epochs. In contrast to template-fitting methods, more recent approaches have shifted towards the use of empirical Machine Learning (ML) models, performing regression or classification, to tackle photo-z applied predominantly to inactive galaxies (Collister & Lahav, 2004; Laurino et al., 2011; Zhang et al., 2013; Hoyle, 2016; D’Isanto & Polsterer, 2018; Brescia et al., 2019; Eriksen et al., 2020; Li et al., 2021). Provided with a very large and complete spec-z sample, ML architectures manipulate photometric input features to minimize the divergence between spectroscopic and ML-derived redshifts. Over the years, a plethora of ML architectures, including decision trees (Breiman, 2001; Carliles et al., 2010; Li et al., 2022), Gaussian processes (Almosallam et al., 2016) and K-nearest neighbours (Zhang et al., 2013; Luken et al., 2019) have been employed, yielding accurate point predictions and, more interestingly, full probability density functions (PDFs) (Kind & Brunner, 2013; Cavuoti et al., 2016; Rau et al., 2015; Sadeh et al., 2016). The latter grants access to the prediction uncertainty, as otherwise naturally provided by template-fitting approaches, relevant for studies dealing with, e.g. luminosity functions (Aird et al., 2010; Buchner et al., 2015; Georgakakis et al., 2015). However, the limited availability of a sizable training sample of AGN has resulted in only a few attempts to compute photo-z for mostly nucleus-dominated objects with ML-based methods (Mountrichas et al., 2017; Fotopoulou & Paltani, 2018; Ruiz et al., 2018; Meshcheryakov et al., 2018; Brescia et al., 2019; Nishizawa et al., 2020). More recently, the conventional approach of manually selecting photometric features for ML has been replaced by bright, well-resolved galaxies at low redshift (Hoyle, 2016; Pasquet et al., 2018; Campagne, 2020; Hayat et al., 2021). In this regime, integrating galaxy images into deep neural networks inherently captures essential details like flux, morphology, and other features that would typically be extracted from catalogs based on predefined assumptions, leading to a more comprehensive redshift estimation process. This approach is particularly advantageous for addressing current limitations faced by photo-z methods for AGN, as it leverages model-independent fluxes and redshift indicative features, including surface brightness profiles (Stabenau et al., 2008; Jones & Singal, 2017; Gomes et al., 2017; Zhou et al., 2021, 2023). Unlike creating a single SED from total flux measurements, projects employing images with independent pixel-by-pixel SEDs at identical redshift have demonstrated increased photo-z constraining power, alleviating previous empirical approaches by decreasing the fraction of outliers (Henghes et al., 2022; Schuldt et al., 2021; Lin et al., 2022; Dey et al., 2022a; Newman & Gruen, 2022). Here, we introduce PICZL (Photometrically Inferred CNN redshift(Z) Likelihoods), an enhanced approach to photo-z estimation that builds upon (CircleZ by Saxena et al., 2024). While the authors demonstrated that redshift degeneracies encountered for AGN, typical in cases of limited photometry, can be broken by integrating aperture photometry alongside traditional total/model fluxes and colors, PICZL instead computes photo-z PDFs for AGN directly from homogenized flux band cutouts by leveraging the more detailed spatial light profile. All inputs are obtained utilizing LS10 exclusively. Similar to Saxena et al. (2024), PICZL can produce reliable photo-z PDFs for all Legacy-detected sources associated with an AGN. However, the model can, in principle, be applied to other extragalactic sources (e.g, inactive galaxies, Götzenberger et al. in prep.) granted that a dedicated training sample is used. We employ an ensemble of the same ML algorithm, notably convolutional neural networks (CNNs), known for their proficiency in learning intricate patterns, as outlined by (Lecun et al., 1998a). Specifically designed for image analysis, CNNs excel at identifying and extracting relevant predictive features directly from images, thereby reducing computational overhead compared to fully connected architectures. Harnessing this more extensive pool of information, these models surpass alternative models based on condensed feature-based input sets. The paper is structured as follows: Sect. 2 introduces the AGN training sample down-selection. Sect. 3 focuses on the photometric data products available within LS10. Sect. 4 details the photometric data preprocessing, followed by Sect. 5, which outlines the model pipeline. Sect. 6 presents and quantifies the redshift results, while Sect. 7 evaluates the photo-z released for the XMM-SERVS (Chen et al., 2018; Ni et al., 2021) fields. Sect. 8 outlines current limitations and discusses how we can achieve further improvements. Sect. 9 explores implications for future surveys, concluding with a summary. In this paper, unless stated differently, we express magnitudes in the AB system and adopt a \LambdaCDM cosmology with H_{0}=69.8 km s-1 Mpc-1, \Omega_{\text{m}}=0.28 and \Lambda=0.72."
https://arxiv.org/html/2411.07303v1,Peering into the black box: forward-modeling the uncertainty budget of high-resolution spectroscopy of exoplanet atmospheres,"Ground-based high-resolution cross-correlation spectroscopy (HRCCS; R\gtrsim 15{,}000) is a powerful complement to space-based studies of exoplanet atmospheres. By resolving individual spectral lines, HRCCS can precisely measure chemical abundance ratios, directly constrain atmospheric dynamics, and robustly probe multidimensional physics. But the subtleties of HRCCS datasets—e.g., the lack of exoplanetary spectra visible by eye and the statistically complex process of telluric removal—can make interpreting them difficult. In this work, we seek to clarify the uncertainty budget of HRCCS with a forward-modeling approach. We present a HRCCS observation simulator, scope,111https://github.com/arjunsavel/scope that incorporates spectral contributions from the exoplanet, star, tellurics, and instrument. This tool allows us to control the underlying dataset, enabling controlled experimentation with complex HRCCS methods. Simulating a fiducial hot Jupiter dataset (WASP-77Ab emission with IGRINS), we first confirm via multiple tests that the commonly used principal components analysis does not bias the planetary signal when few components are used. Furthermore, we demonstrate that mildly varying tellurics and moderate wavelength solution errors induce only mild decreases in HRCCS detection significance. However, limiting-case, strongly varying tellurics can bias the retrieved velocities and gas abundances. Additionally, in the low-SNR limit, constraints on gas abundances become highly non-Gaussian. Our investigation of the uncertainties and potential biases inherent in HRCCS data analysis enables greater confidence in scientific results from this maturing method.","Exoplanet atmospheres offer insight into extreme regimes. When studying the atmospheres accessible with current instruments, we must confront the fundamental properties of gas in high-temperature, low-pressure environments (Showman & Guillot, 2002). Doing so pushes our understanding of atmospheric dynamics, microphysical processes, and gas-phase chemistry into new territory (e.g., Mbarek & Kempton, 2016; Powell et al., 2018; Tan & Komacek, 2019; Beltz et al., 2021; Gao et al., 2021). In the past decade, ground-based high-resolution cross-correlation spectroscopy (HRCCS; R\gtrsim 15,000; Birkby, 2018) has carved out a compelling niche as a probe of exoplanet atmospheres. This technique is unique in its ability to directly constrain exoplanetary winds (e.g., Snellen et al., 2010; Kempton & Rauscher, 2012; Showman et al., 2013), and by resolving tens of thousands of individual spectral lines, it has enabled precise constraints on chemical composition (e.g., Line et al., 2021; Gandhi et al., 2022; Maguire et al., 2022; Boucher et al., 2023). With clear evidence of three-dimensional physics and chemistry (Flowers et al., 2019; Beltz et al., 2020; Ehrenreich et al., 2020; Wardenier et al., 2021; Savel et al., 2021; Gandhi et al., 2022; Beltz et al., 2023; Nortmann et al., 2024) and the potential to unveil dozens of chemical species in the optical and near-infrared (e.g., Carleo et al., 2022; Kesseli et al., 2022; Prinoth et al., 2022), HRCCS datasets promise a strong complement to their lower-resolution counterparts, one that will endure even in the era of JWST atmospheric studies (Gardner et al., 2006; Greene et al., 2016; Brogi & Line, 2019). Complicating the high scientific potential of HRCCS are the subtleties in its analysis. Unlike in low-resolution studies, exoplanet spectra in general are not detected in each observed spectral channel with HRCCS, but are rather buried in the noise and not visible “by eye.” Therefore, the presence of the planetary signal in HRCCS must be inferred from cross-correlation functions (e.g., Snellen et al., 2010). This feature of HRCCS is accompanied by “black box”-like data processing. While the steps to this analysis are fundamentally deterministic and can be understood with forward modeling, the complexity of the underlying physical processes (namely tellurics) in turn requires complex statistical treatments. Further complicating matters is the axis of researcher choice. Previous studies have sought to shed light on the impact of these processing steps, comprehensively exploring the sensitivity of HRCCS results to analysis choices (de Kok et al., 2013; Cabot et al., 2019; Langeveld et al., 2021; Gully-Santiago & Morley, 2022; Meech et al., 2022; Brogi et al., 2023; Cheverall et al., 2023). Furthermore, studies often “inject” additional signal into data to check the responsivity of their dataset to true signal (e.g,. Gibson et al., 2022). Even so, it is difficult to exactly isolate in which circumstances the data, the analysis techniques, or some combination drives the behavior of the results. Taken together, these aspects have produced an opaque uncertainty budget for HRCCS. In this work, we seek to provide intuition for the less intuitive aspects of HRCCS data processing and analysis to better understand the HRCCS uncertainty budget. We do so with a forward-modeling approach, simulating HRCCS emission spectroscopy datasets and analyzing them as one would analyze real data. Our focus is threefold: on variable tellurics and their removal, on wavelength solution stability, and on the level of photon noise. All of these uncertainty sources are unexplored in HRCCS from the modeling perspective, and they are hypothesized to explain outstanding questions in HRCCS (e.g., the sub-unity scaling of the planetary spectrum found by van Sluijs et al., 2022). For this work, our model aims to mimic the HRCCS dataset of WASP-77Ab from Line et al. (2021) observed with the IGRINS spectrograph (Yuk et al., 2010; Park et al., 2014; Lee et al., 2017; Mace et al., 2018). We focus on this dataset because of its high quality, the availability of many telluric standard stars at this observatory, and the validation of this dataset’s analysis by recent JWST results (August et al., 2023). Though our work focuses on simulating emission spectra, our results have direct implications for transmission spectroscopy studies as well, and we will comment on them in detail. Section 2 describes the methods of this work, including our HRCCS data simulation pipeline—which we name scope—and our reduction of computational burden. Section 3 contains the results of our forward-modeling experiments. Section 4 discusses our results, including a list of known caveats. Finally, we conclude this work in Section 5."
https://arxiv.org/html/2411.06144v1,First Use of GPS Satellites for Beam Calibration of Radio Telescopes,"We present results from the first application of the Global Navigation Satellite System (GNSS; GPS is one example of a collection of satellites in GNSS) for radio beam calibration using a commercial GNSS receiver with the Deep Dish Development Array (D3A) at the Dominion Radio Astrophysical Observatory (DRAO). Several GNSS satellites pass through the main and side lobes of the beam each day, enabling efficient mapping of the 2D beam structure. Due to the high SNR and abundance of GNSS satellites, we find evidence that GNSS can probe several side lobes of the beam through repeatable measurements of the beam over several days. Over three days of measurements, we find a measured difference reaching a minimum of 0.56 db-Hz in the main lobe of the primary beam. These results show promise for the use of GNSS in beam mapping for the Canadian Hydrogen Observatory and Radio-transient Detector (CHORD) and other future “large-N” radio interferometers. They also motivate future development of the technique within radio astronomy.","Detection of 21cm emission beyond the local universe requires an extremely accurate understanding of both telescope systematics and astrophysical foregrounds. Precise beam calibration and complementary foreground removal present some of the most critical challenges in future precision 21cm line intensity mapping experiments. The Canadian Hydrogen Observatory and Radio transient Detector (CHORD) is a new radio telescope with 21cm line intensity mapping amongst its primary science goals. CHORD will be constructed at the Dominion Radio Astrophysical Observatory (DRAO) near Penticton in British Columbia (CHORD_white_paper). Although each dish will be adjustable in declination, CHORD will operate primarily as a drift-scan instrument and will contain 512 dishes. It will build on the success of the Canadian Hydrogen Intensity Mapping Experiment (CHIME) by extending sensitivity to local universe 21cm emission and localizing fast radio bursts (FRBs) to milli-arcsecond precision while implementing several instrumental innovations. CHIME (also a drift-scan telescope) has recently cited beam calibration as one of its ongoing challenges in detecting Baryon Acoustic Oscillations (BAO) at low redshift (chime_paper_latest). CHORD has a frequency range of 300-1500 MHz spanning three times the frequency range of CHIME (400-800 MHz). A new technique using the Global Navigation Satellite System (GNSS) is a viable addition to beam calibration for CHORD. Notably, the frequency range of CHORD spans nearly the full range of GNSS frequencies. The Deep Dish Development Array (D3A) is a CHORD prototype experiment which consists of three six-meter dishes that function as an interferometer with a frequency range between 300 and 1600 MHz. The D3A is being built to refine CHORD’s new technologies. The key calibration scheme for CHORD requires consistent repeated measurements of the same interferometric modes, i.e., redundant calibration (red_cal). Precise knowledge of the differences between the beam of each interferometric element is needed by ensuring both precise metrological and beam measurements. The dish has a surface precision design goal of one thousandth of a wavelength at 300 MHz (1mm). Advances in dish metrology (metrology) have shown that current composite manufacturing processes can produce dishes that meet or exceed this requirement. This leaves the bottleneck for accurate redundant calibration to precision beam mapping. Traditional beam mapping techniques use well known and radio bright astrophysical sources which drift-scan over the beam. This produces a measurement of the beam convolved with the radio source. Additionally, holographic methods have been explored to increase precision of beam mapping with astrophysical sources with the CHIME pathfinder. Berger_2016 and amiri2024holographicbeammeasurementscanadian used measurements with the DRAO John A. Galt telescope which lies adjacent to the CHIME and future CHORD site. This allowed for measurements of the East to West beam structure to be made, but they only sparsely sample the beam in declination resulting in gaps in the North-South direction. Long integration times were also required for low signal-to-noise ratio (SNR) sources. CHIME has also relied on drift-scan only measurements of the beam with the Sun (beam_map), but this relies heavily on solar variability being at a minimum. In the case of radio bright astrophysical sources, few are sufficiently bright to map the beams of individual sources into the side lobes, and their fixed trajectories span a small solid angle. The design of ”large-N”, ”small-D” interferometers allows for a large primary field of view (due to the small diameter and large number of elements). Because the primary field of view is designed to be much larger, beam mapping techniques with large solid angle coverage are especially useful. In recent years, beam calibration methods have extended from using celestial radio sources (astro_cals), to employing artificial calibration sources such as drones (e.g., Chang_2015; bolli_2016; 2024arXiv240700856H). Using drones allows for precision control of the calibration. A drone can emit frequency dependent emission and tailor its flight path to the instrument covering the entirety of the beam. In 2024arXiv240704848T, the first drone-based beam measurement of a cylindrical radio interferometer was made with CHIME. The holographic (latest measurements in amiri2024holographicbeammeasurementscanadian), solar, and drone beams were similar, especially in indicating the location of the primary beam and side lobes. Measuring the far field beam pattern is challenging with most drones, especially at higher frequencies or with larger dishes (far field > than 300m for a D3A dish observing the 21cm line). In addition to drones, satellites present another prime artificial calibration source and except in the case of many Very Long Baseline Interferometry (VLBI) experiments always lie in the far field. For example, Orbcomm satellites were used to map the Murchison Widefield Array which operates at extremely lower frequencies around 100 MHz (Line_2018; Chokshi_2021). There is also promise in using GNSS as a phase calibrator in VLBI to measure total electron content of the ionosphere at different sites, especially for localization of FRBs in real time. This is limited by the precision in comparing GNSS arrival times to those expected based on the satellite distance. FRBs are transients that go off at highly uncertain times, and there is usually not a readily available classic VLBI calibrator to use at the time of the burst. However, characterization of a GNSS receiver to the precision such that the use of GNSS for phase calibration could be useful requires an extremely thorough understanding of the receiver itself (coster_accuracy). Leading ionospheric mapping efforts are pushing down to a precision that is useful in VLBI fringe fitting, i.e., a relative total electron content (TEC) measurement at the 0.01 TECu level. However, utilizing the carrier phase to obtain extremely precise phase measurements may be possible because the GNSS measurements will be made on the same precision clocks as the VLBI instrument. The National Institute of Standards and Technology (NIST) has shown that frequency comparisons with a precision of 10^{-15} have been made over 24 hours of averaging (precision_GNSS_carrier). This is ongoing research by NIST and Common View GPS Time Transfer. GNSS shows some promise for use as a prior in initial VLBI phase calibration. Because there does not exist one perfect beam mapping technique for drift-scan instruments, multiple complementary techniques are often needed. GNSS satellites are a potential addition to the currently available beam mapping techniques. They overlap with low-z 21cm measurements with frequencies spanning between approximately 1100-1600 MHz. They also lie in the far field of the D3A with orbits at an altitude of approximately 20,000 km. GNSS is an umbrella term for the many satellite constellations used in localization, timing, and atmospheric observations and contains greater than 100 high SNR far field satellites worldwide. The abundance of GNSS satellites could lead to more complete coverage of the CHORD beam at GNSS frequencies. We present an overview of the first beam map measured using GNSS. We use an off-the-shelf commercially available GNSS receiver to provide an initial characterization of the D3A beam. In Section 2, we discuss the experimental setup, provide a brief overview of the GNSS mapping technique, and the commericial GNSS receiver. The first repeatable measurements of a radio telescope beam with GNSS are presented in Section 3. We conclude in Section 4 with a brief discussion of the potential for a radiometric measurement of the beam with GNSS in the future and the preliminary promise of GNSS beam calibration."
https://arxiv.org/html/2411.05889v1,"qCMOS detectors and the case of hypothetical primordial black holes in the solar system, near earth objects, transients, and other high cadence observations","Recent progress with CMOS detector development has opened new parameter space for high cadence time resolved imaging of transients and fast proper motion solar system objects. Using computer simulations for a ground-based 1.23 m telescope, this research note illustrates the gain of a new generation of fast readout low noise qCMOS sensors over CCDs and makes the case for high precision monitoring of asteroid orbits that can potentially shed light on the hypothetical existence of low mass primordial black holes, as well as for other applications requiring high speed imaging.","Over the past decade, the improvement of CMOS image sensors has made this technology gradually more attractive for astronomical instrumentation. Although currently parameters like pixel size, overall sensor area, and quantum efficiency at longer wavelengths still favor conventional CCDs over CMOS, the development of low noise and fast readout qCMOS sensors (Ma et al., 2022) holds the promise of a paradigm change. Roberts et al. (2024) have evaluated the commercially available Hamamatsu ORCA-Quest qCMOS camera for measurements of quantum spatial correlations and come to conclusions that encourage the use of such a camera also for astronomical imaging. This research note motivates the application of fast cadence qCMOS imaging, remembering that time domain is a priority science area of the Decadal Survey Astro2020, for observations requiring a time resolution of order 1 s, e.g., exoplanet transits of M stars that are affected by flares (Tovar Mendoza et al., 2022), near Earth objects, space debris, precision tracking of asteroids in the search for hypothetical primordial black holes within the solar system (Tran et al., 2024; Thoss & Burkert, 2024), or fast transients of any kind. The advance of qCMOS over conventional CCD technology is illustrated with simulations of short exposure time images of a star observed with a 1 m class ground based telescope."
https://arxiv.org/html/2411.06960v1,Collective modes in relativistic cold asymmetric nuclear matter within the covariant Vlasov approach,"A covariant relativistic approach based on the Vlasov equation is applied to the study of infinite asymmetric nuclear matter. We use several Walecka-type hadronic models and obtain the dispersion relations for the longitudinal modes. The isovector and isoscalar collective modes are determined for a wide range of densities as a function of isospin asymmetry and momentum transfer within a set of eleven relativistic mean field models with different nuclear matter properties. Special attention is given to beta-equilibrium matter. It is shown that the possible propagation of isoscalar and isovector-like modes depends directly on the density dependence of the symmetric nuclear matter equation of state and of the symmetry energy, with a stiff equation of state favouring the propagation of isoscalar like collective modes at high densities, and a stiff symmetry energy defining the behavior of the isovector like modes which propagate for densities below two times saturation density. The coupling of the nuclear modes to the electron plasmon is also discussed.","A multidisciplinary theoretical effort involving astrophysics, nuclear and particle physics, and statistical physics is required to understand compact stars, supernova cores, and neutron stars. From low densities up to several times nuclear saturation, these stellar objects are essentially composed of neutrons, protons, electrons, muons above a density close to saturation density, and possibly, if their mean free path is short enough, neutrinos. The electrons and muons neutralize the proton charge and thus suppress the divergent Coulomb energy contribution. In addition to understanding the equation of state of the stellar matter, a good description of the mean free path of the neutrinos in the medium is required. Neutrino opacity has been shown to be influenced by nucleon-nucleon interactions through coherent scattering of density fluctuations Horowitz et al. (2004a, b). Both the single particle contribution and the collective contribution must be considered. It is therefore important to understand the collective modes in asymmetric nuclear matter in order to predict the behavior of neutrinos. Low-density stellar matter consists of neutron-rich nuclei immersed in a gas of neutrons, a consequence of the liquid-gas phase transition that characterizes nuclear matter in this density range. This matter plays an important role in the collapse of supernovae into neutron stars, see for instance Oertel et al. (2017); Hempel et al. (2012); Fischer et al. (2014). At very low densities, the competition between long-range Coulomb repulsion and short-range nuclear attraction defines the properties of the inhomogeneous matter formed, which is known as nuclear pasta Ravenhall et al. (1983). It can occur in different structures and its properties have important consequences in the crust of neutron stars and in the core collapse of supernovae Chamel and Haensel (2008). It was shown that the isovector channel of nuclear matter is important to define the properties of the transition to the neutron star core Ducoin et al. (2011). In fact, the curst-core transition may be determined studying the response of nuclear matter to small perturbations, since the appearance of collective unstable modes is intrinsically linked to the existence of a non-homogeneous phase. In the present work, we are interested in studying the longitudinal nuclear collective stable modes arising from small oscillations around an equilibrium state in nuclear matter using the covariant Vlasov approach Zhuang and Heinz (1996); Avancini et al. (2018). We will analyze the behavior of several nuclear models used to describe neutron star matter. The collective modes of nuclear matter have already been studied in Ref. Matsui (1981) within a Landau-Fermi liquid formalism considering a relativistic mean field description. In this study both symmetric matter and neutron matter isoscalar modes were discussed. Later, in Ref. Lim and Horowitz (1989), a relativistic Hartree calculation was carried out and the zero sound, longitudinal and transverse modes were obtained for symmetric matter. A similar study was undertaken within a semiclassical approach in Nielsen et al. (1991), also for symmetric matter. Subsequently, in Refs. Greco et al. (2003); Avancini et al. (2005), both isoscalar and isovector collective modes were discussed also within an RMF description. One of the main goals of this work is to obtain dispersion relations for neutron-proton-electron (NPE) matter within the formalism of the covariant Wigner function, which allows us to calculate the propagation of density modes. The dispersion relations are used to calculate the longitudinal collective modes, both stable and unstable. The unstable modes determine the dynamical spinodal zones that define the boundaries of the inhomogeneous region of the neutron star crust. This formalism is of particular interest because it can be easily generalized to calculate the electrical and thermal conductivity in the magnetized/non-magnetized matter, which is fundamental to the study of neutron star cooling. In a previous work Avancini et al. (2018), we have studied, in the context of the covariant Vlasov approach, neutron-proton-electron matter under a strong magnetic field. In this work, the dispersion relations for the longitudinal and transverse modes were calculated within a Walecka-type hadronic model and the instability regions for longitudinal and transverse modes were studied. In particular, the crust-core transition of a magnetized neutron star was discussed in detail. We investigate the role of isospin and the presence of the Coulomb field and electrons on the longitudinal collective nuclear modes. It has been shown Greco et al. (2003); Avancini et al. (2005) that at lower densities an isovector-like collective mode exists. The onset density of the mode depends on the isospin asymmetry. At higher densities, two to three times the saturation density, this mode changes to an isoscalar-like mode and the authors of Ref. Greco et al. (2003) have even suggested that the experimental observation of the neutron wave would identify the transition density. We expect the presence of electrons to affect the properties of these modes, namely the excitations with large wavelengths when protons and electrons must move together. We restrict ourselves to the zero-temperature case. In the following, we will refer to neutral matter composed of protons, neutrons, and electrons as npe matter, and to charged matter composed only of protons and neutrons as np matter. In Sec. II we present the covariant Vlasov equation formalism for nuclear matter, including electrons and the electromagnetic field already discussed in Refs. Providência et al. (2006a); Brito et al. (2006). A brief discussion of the plasmon modes predicted within the present formalism is also included. In Sec. III the numerical results are presented and discussed. Finally, in the last section, the main conclusions are drawn."
https://arxiv.org/html/2411.06830v1,"Towards Model-free Temperature Diagnostics of Warm Dense Matter
from Multiple Scattering Angles","Warm dense matter (WDM) plays an important role in astrophysical objects and technological applications, but the rigorous diagnostics of corresponding experiments is notoriously difficult. In this work, we present a model-free analysis of x-ray Thomson scattering (XRTS) measurements at multiple scattering angles. Specifically, we analyze scattering data that have been collected for isochorically heated graphite at the Linac Coherent Light Source (LCLS). Overall, we find good consistency in the extracted temperature between small and large scattering angles, whereas possible signatures of non-equilibrium may be hidden by the source function, and by the available dynamic spectral range. The present proof-of-principle study directly points to improved experimental set-ups for equation-of-state measurements and for the model-free study of relaxation times.","Acknowledgments This work was partially supported by the Center for Advanced Systems Understanding (CASUS) which is financed by Germany’s Federal Ministry of Education and Research (BMBF) and by the Saxon state government out of the State budget approved by the Saxon State Parliament. This work has received funding from the European Union’s Just Transition Fund (JTF) within the project Röntgenlaser-Optimierung der Laserfusion (ROLF), contract number 5086999001, co-financed by the Saxon state government out of the State budget approved by the Saxon State Parliament. This work has received funding from the European Research Council (ERC) under the European Union’s Horizon 2022 research and innovation programme (Grant agreement No. 101076233, ”PREXTREME”). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. The work of H.M.B., B.B., M.P.B. Ti.D., M.J.M. was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344 and supported by Laboratory Directed Research and Development (LDRD) Grants No. 24-ERD-044 and 25-ERD-047."
https://arxiv.org/html/2411.06452v1,"Detecting Secular Perturbations in  Planetary Systems Using 
Simultaneous Impact Parameter Variation Analysis (SIPVA)","Recovering impact parameter variations in multi-planet systems is an effective approach for detecting non-transiting planets and refining planetary mass estimates. Traditionally, two methodologies have been employed: the Individual Fit, which fits each transit independently to analyze impact parameter changes, and the Dynamical Fit, which simulates planetary dynamics to match transit light curves. We introduce a new fitting method, Simultaneous Impact Parameter Variation Analysis (SIPVA), which outperforms the Individual Fit and is computationally more efficient than the Dynamical Fit. SIPVA directly integrates a linear time-dependent model for impact parameters into the Monte Carlo Markov Chain (MCMC) algorithm by fitting all transits simultaneously. We evaluate SIPVA and the Individual Fit on artificial systems with varying LLRs and find that SIPVA consistently outperforms the Individual Fit in recovery rates and accuracy. When applied to selected Kepler planetary candidates exhibiting significant transit duration variations (TDVs), SIPVA identifies significant impact parameter trends in 10 out of 16 planets. In contrast, the Individual Fit does so in only 4. We also employ probabilistic modeling to calculate the theoretical distribution of planets with significant impact parameter variations across all observed Kepler systems and compare the distribution of recovered candidates by the Individual Fit and Dynamical Fit from previous work with our theoretical distribution. Our findings offer an alternative framework for analyzing planetary transits, relying solely on Bayesian inference without requiring prior assumptions about the planetary system’s dynamical architecture.","Accurately estimating the mass and eccentricity of exoplanets remains a significant challenge in detecting and characterizing Kepler multi-planet systems. One common approach involves reproducing observational data through N-body simulations to determine planetary masses (Wisdom & Holman, 1991; Chambers, 1999; Doyle et al., 2011; Carter et al., 2012), which has contributed multiple measurements for Kepler systems. Despite advancements in numerical integrators (Rein & Liu, 2012; Rein & Spiegel, 2014; Rein & Tamayo, 2015; Rein et al., 2019a, b; Javaheri et al., 2023) and increased computational capabilities, this method is limited by the Kepler mission’s approximately four-year observational window, which is insufficient to capture the frequent yet subtle orbital parameter changes necessary for precise mass determination using N-body techniques. Transit Timing Variation (TTV) analysis is an effective method for estimating exoplanet masses (Agol et al., 2005; Grimm et al., 2018; Dai et al., 2023; Masuda et al., 2024). Several tools, including ttvim (Nesvorný & Beaugé, 2010), TTVFast (Deck et al., 2014), TTVFaster (Agol & Deck, 2016a), PhoDyMM (Jones et al., 2022), and PyDynamicaLC (Yoffe et al., 2021), facilitate efficient TTV analyses. In additional to mass, studies focusing on eccentricities have examined the fundamental TTV modes to higher orders (Lithwick et al., 2012; Agol & Deck, 2016b; Hadden & Lithwick, 2017). However, TTV analysis is challenged by mass and eccentricity degeneracies (Lithwick et al., 2012), complicating the precise determination of each individual parameter. Moreover, the limited number of transit observations for planets with shorter orbital periods reduces temporal resolution, making the detection and analysis of subtle TTV signals more difficult. Transit Duration Variation (TDV) analysis, especially when combined with TTV, can help overcome the mass-eccentricity degeneracy (Agol & Fabrycky, 2018). Recent studies have actively explored using TDVs to constrain exoplanetary parameters (Shahaf et al., 2021; Judkovsky et al., 2020). Physical evidence for secular changes in TDVs driven by variations in impact parameters resulting from mutual inclinations has been presented by Millholland et al. (2021). Consequently, analyzing impact parameter variations (TbVs) has emerged as an alternative approach. Using TbVs, Judkovsky et al. (2022a) analyzed 54 systems with 140 planets, achieving mass detections of more than 3\sigma for 102 planets, including 43 lighter than 5,M_{\oplus}. Their subsequent research, Judkovsky et al. (2024), estimates orbital parameters for 101 planets across 23 systems, with mass significances better than 3\sigma for 95 planets, including 46 without prior constraints. Recovering and precisely estimating the magnitude and direction of TDVs or TbVs is crucial before using them to constrain planetary parameters. Traditionally, two methodologies have been employed: the Individual Fit, which fits each transit independently to analyze TDVs (Holczer et al., 2016), and the Dynamical Fit, which simulates planetary dynamics to match transit light curves (Judkovsky et al., 2022a, b, 2024; Langford & Agol, 2024). However, the Dynamical Fit requires strong prior assumptions about planetary orbital configurations, including the number of planets, their masses, and gravitational interactions. This research presents a Bayesian inference approach with less assumptions, fitting each planet’s transits individually and independently of neighboring planets. Our method, the Simultaneous Impact Parameter Variation Analysis (SIPVA), directly incorporates a linear time-dependent model for impact parameters into the Markov Chain Monte Carlo (MCMC) algorithm by fitting all transits simultaneously. SIPVA avoids modeling the full gravitational dynamics, reducing computational complexity and dependence on prior knowledge of the system’s architecture. We demonstrate that SIPVA outperforms the Individual Fit in recovery rate and accuracy, providing an alternative approach for analyzing planetary transits. Table 1: Notation Table Notation Description Notation Description t Time T Period a Semi-major axis p Planet-star radius ratio b Impact parameter u_{1} Limb darkening parameter 1 R_{*} Radius of the star u_{2} Limb darkening parameter 2 M_{*} Mass of the star \rho_{*} Stellar density (g/cm3) m Mass of the planet \omega Orbital frequency \delta_{t} Time interval of short-cadence of Kepler n Number of data points t_{e} Transit epoch N Number of planets in the system \dot{b} Change in impact parameter over time N_{T} Number of transits \dot{b}_{\text{ind}} Estimate of \dot{b} from Individual Fit \dot{b}_{\text{grp}} Estimate of \dot{b} from SIPVA t_{\dot{b},\text{ind}} t-value of \dot{b} from Individual Fit t_{\dot{b},\text{grp}} t-value of \dot{b} from SIPVA z Normalized separation between centers \bar{F} Raw flux from Kepler \hat{F} Model flux from \vec{\Theta} F Actual flux from Kepler \sigma Standard deviation for error in F and \hat{F} \sigma_{I} Standard deviation for inclination secw \sqrt{e}\cos\omega sesw \sqrt{e}\sin\omega q1_{\text{Kepler}} Limb darkening coefficient 1 (Kepler) q2_{\text{Kepler}} Limb darkening coefficient 2 (Kepler) \log_{10}(\sigma_{\text{wn}}) Log white noise standard deviation Note. — This table summarizes the notation and variables used throughout this paper. This paper is organized as follows. In Section 2, we present a theoretical calculation of the Log Likelihood Ratio (LLR) of TbV transit lightcurves between the Null Model (\mathcal{M}_{0}), which assumes no change in the impact parameter over time, and the Alternative Model (\mathcal{M}_{1}), which incorporates a changing impact parameter. In Section 3, we introduce our MCMC methodology for recovering impact parameter changes through both Individual Fits (Sections 3.2 and 3.3) and SIPVA after transit folding (Section 3.4). We also evaluate the performance of Individual Fits and SIPVA on various artificial systems with different LLRs (Section 3.5). Section 4 applies our fitting algorithms to the selected Kepler planetary system candidates from Millholland et al. (2021) that demonstrate significant TDV. Lastly, in Section 5, we develop a theoretical model to estimate the relative frequency of planets exhibiting significant impact parameter changes across all Kepler observations and compare our estimates with observations from Holczer et al. (2016) and Judkovsky et al. (2022a, 2024). Finally, in Section 6, we summarize our findings. For clarity, we have provided a notation table (Table 1) summarizing the variables used throughout this paper."
https://arxiv.org/html/2411.06157v1,The COBREX archival survey: improved constraints on the occurrence rate of wide-orbit substellar companions,"Context. Direct imaging (DI) campaigns are uniquely suited to probing the outer regions around young stars and looking for giant exoplanet and brown dwarf companions, hence providing key complementary information to radial velocity (RV) and transit searches for the purpose of demographic studies. However, the critical 5-20 au region, where most giant planets are thought to form, remains poorly explored, lying in-between RV and DI capabilities.Aims. Significant gains in detection performances can be attained at no instrumental cost by means of advanced post-processing techniques. In the context of the COBREX project, we have assembled the largest collection of archival DI observations to date in order to undertake a large and uniform re-analysis. In particular, this paper details the re-analysis of 400 stars from the GPIES survey operated at GPI@Gemini South.Methods. Following the pre-reduction of raw frames, GPI data cubes were processed by means of the PACO algorithm. Candidates were identified and vetted based on multi-epoch proper motion analysis – whenever possible – and by means of a suitable color-magnitude diagram. The conversion of detection limits into detectability maps allowed for an estimate of unbiased occurrence frequencies of giant planets and brown dwarfs.Results. Deeper detection limits were derived compared to the literature, with up to a twofold gain in minimum detectable mass compared to previous GPI-based publications. Although no new substellar companion was confirmed, we identified two interesting planet candidates awaiting follow-up observations. We derive an occurrence rate of 1.7_{-0.7}^{+0.9}\% for 5 M_{\textrm{Jup}} <m<13 M_{\textrm{Jup}} planets in 10\leavevmode\nobreak\ \text{au}<a<100\leavevmode\nobreak\ \text{au}, that raises to 2.2_{-0.8}^{+1.0}\% when including substellar objects up to 80 M_{\textrm{Jup}}. Our results are in line with the literature, but come with lower uncertainties thanks to the enhanced detection sensitivity. We confirm the finding, hinted at by previous studies, of a larger occurrence of giant planets around BA hosts compared to FGK stars; moreover, we tentatively observe a smaller occurrence of brown dwarf companions around BA stars, although larger samples are needed to shed light on this point.Conclusions. While waiting for the wealth of data expected from future instrument and facilities, valuable information can still be extracted from existing data. In this regard, a complete re-analysis of SPHERE and GPI data is expected to provide the most precise demographic constraints ever provided by imaging.","Bolstered by more than 5000 confirmed detections to date, the exoplanet field has become mature enough to accompany the still thriving detection-oriented endeavor with follow-up studies aimed at shedding light on key questions related to the origin, the prevalence, and the architecture of planetary systems. By unveiling statistical trends in the measured physical, orbital and star-related properties of the exoplanet population, exoplanet demographics seeks to connect theory and observation, in order to fully understanding the physical processes underlying planet formation (Biazzo et al. 2022). The census of known exoplanets currently spans about four magnitudes in mass and in semi-major axis111Empirically estimated based on the Extrasolar Planet Encyclopaedia: http://www.exoplanet.eu/.. No single detection method is adequate to probe such a large extent of the parameter space: it is through the combination of the different methods, each optimized for detection inside a specific niche, that the large-scale picture can be unveiled and reconstructed (see, e.g., Gratton et al. 2023, 2024). However, obtaining a complete and unbiased blend from heterogeneous ingredients is hindered by factors such as inconsistent detection criteria, completeness and false positive assessment, uncertainty quantification, neglect of underlying selection or observational biases (Gaudi et al. 2021). Whenever two different methods can be simultaneously employed, their complementarity allows better characterizing individual objects (see, e.g., Gandolfi et al. 2017; Bonnefoy et al. 2018; Bourrier et al. 2018; Lacedelli et al. 2021; Kuzuhara et al. 2022; Philipot et al. 2023, Lagrange et al. under review) and strengthening the statistical trends emerging in each of the methods (Rogers 2015; Santerne et al. 2016). In the cases where different techniques probe instead different separations within the same system, the joint analysis opens up exquisite dynamical and formation studies (see, e.g., Covino et al. 2013; Bryan et al. 2016; Zhu & Wu 2018). Radial velocity (RV) surveys have provided invaluable constraints on the physical and orbital properties of giant planets up to \sim 5 au (Wolthoff et al. 2022; Rosenthal et al. 2024). Yet, the reliability of RV trends for larger separations has been questioned (Lagrange et al. 2023), and the predicted yields for direct imaging (DI) surveys based on extrapolations of RV results have been shown to be too optimistic (see, e.g., Cumming et al. 2008; Dulz et al. 2020). On the other hand, direct imaging (DI) is mostly sensitive to young giant planets in wide (a\gtrsim 20 au) orbits, providing access to the scarcely studied outskirts of planetary systems. Starting from 2004 (Chauvin et al. 2004), direct imaging has discovered \sim 30 planets (M<13M_{\textrm{Jup}}) (Zurlo 2024), including iconic systems like the disk-enshrouded PDS 70 (Keppler et al. 2018), the \sim 20-Myr-old \beta Pictoris (Lagrange et al. 2009), 51 Eridani (Macintosh et al. 2015), AF Leporis (Mesa et al. 2023; De Rosa et al. 2023; Franson et al. 2023), and the four-planet HR 8799 (Marois et al. 2008). These detections are the main outcome of large blind surveys targeting tens (e.g. MASSIVE, Lannier et al. 2016; SEEDS, Uyama et al. 2017; LEECH, Stone et al. 2018) or hundreds of stars (e.g. NICI-PCF, Liu et al. 2010; IDPS, Galicher et al. 2016; ISPY-NACO, Launhardt et al. 2020). The forefront of DI surveys, enabled by the exquisite performances of imagers and integral field spectrographs, coupled with extreme AO systems mounted on 8-m-class telescopes, is currently represented by the 400-star SHINE (Chauvin et al. 2017) and the 600-star GPIES (Nielsen et al. 2019) surveys. By constraining the overall frequency and the properties of wide-separation giant planets, DI studies are expected to enable a thorough comparison with concurrent formation models (see, e.g., Bowler 2016; Vigan et al. 2021); orbital properties, for instance, shed light upon their formation and dynamical evolution (Bowler et al. 2020); the dependence of frequency on stellar mass provides clues about the initial state of the disk and the formation mechanisms at play (Nielsen et al. 2019; Janson et al. 2021). However, despite years of extensive searches, it is still not clear whether the main formation channel for the observed wide-orbit population be core accretion (CA; Pollack et al. 1996; Mordasini et al. 2009), the bottom-up process responsible for the formation of planets in the Solar System, or rather a top-down star-like scenario like gravitational instability (GI; Boss 1997; Vorobyov 2013). While an interplay between the two scenarios is deemed to be favored by empirical parametric models (Reggiani et al. 2016; Vigan et al. 2021) and direct comparison with synthetic planet populations (Vigan et al. 2021) alike, understanding in an unambiguous way how each known companion was formed is still beyond reach. The large uncertainties still existing in the interpretation of the observed picture can be attributed, at least partially, to the fact that the critical 5-20 au region, where most giant planets are thought to form, remains poorly explored being exactly in-between current RV and DI capabilities. Under given observing conditions, the final performances attainable by a high-contrast imaging observation are dictated both by instrumental (e.g., the telescope, the science instrument, the performance of adaptive optics and coronagraphs) and post-processing components (the algorithms applied to science images to decrease the level of systematic and random noise) (Galicher & Mazoyer 2024). Depending on observing conditions, stellar brightness and angular separation, state-of-the-art instruments such as the Spectro-Polarimetric High-Contrast Exoplanet Research (SPHERE; Beuzit et al. 2019) and the Gemini Planet Imager (GPI; Macintosh et al. 2014) typically achieve raw planet-to-star contrasts as low as 10^{-3}-10^{-5} (Poyneer et al. 2016; Courtney-Barrer et al. 2023). On the instrumental side, 30-m-class telescopes and space-borne coronagraphic instruments are expected to bring about a major leap forward for the field in the next decade (see, e.g., Kasdin et al. 2020; Kasper et al. 2021), whereas upgrades of existing instruments such as SPHERE+ (Boccaletti et al. 2022) and GPI 2.0 (Chilcote et al. 2018) are going to represent the forefront in the medium term; on the reduction side, advanced post-processing algorithms have been already shown to increase the contrast by as much as two orders of magnitudes compared to pre-reduced data. Therefore, the developments of more powerful reduction techniques can greatly increase detection capabilities working on observations that already exist (see, e.g., Currie et al. 2023). In the framework of the COupling data and techniques for BReakthroughs in EXoplanetary systems exploration (COBREX) project, we collected more than a thousand archival SPHERE and GPI observations, assembling the largest exoplanetary direct imaging survey to date, with the aim of re-reducing them in a uniform and self-consistent way. The results of the full re-reduction of the SHINE survey are illustrated in Chomez et al. (2024). In this work, we present the re-reduction of 400 stars coming from GPIES. Despite being the largest DI observational campaign to date, just two new substellar objects were discovered during the survey: one planet (51 Eri b, Macintosh et al. 2015) and one brown dwarf (HR 2562 B, Konopacky et al. 2016). A statistical analysis of the first 300 stars was performed by Nielsen et al. (2019, hereafter N19). By combining the two surveys, it will be possible to obtain the tightest constraints to date on the occurrence of wide-orbit giant planets, hence providing an ideal test-bed to scrutinize planet formation models. This paper is organized as follows: after laying out the selection criteria for the sample and the corresponding observations (Section 2.2), and uniformly deriving the stellar parameters of interest (Section 2.3), we describe in detail the process of data reduction (Section 2.4). Section 3 presents the results of the analysis, namely companion candidates and completeness maps. The derived occurrence rates are presented and discussed in Section 3.4. A thorough comparison with the literature is the subject of Section 4. Finally, in Section 5, we summarize the results of this work."
https://arxiv.org/html/2411.05905v1,Robustness of Neural Ratio and Posterior Estimators to Distributional Shifts for Population-Level Dark Matter Analysis in Strong Gravitational Lensing,"We investigate the robustness of Neural Ratio Estimators (NREs) and Neural Posterior Estimators (NPEs) to distributional shifts in the context of measuring the abundance of dark matter subhalos using strong gravitational lensing data. While these data-driven inference frameworks can be accurate on test data from the same distribution as the training sets, in real applications, it is expected that simulated training data and true observational data will differ in their distributions. We explore the behavior of a trained NRE and trained sequential NPEs to estimate the population-level parameters of dark matter subhalos from a large sample of images of strongly lensed galaxies with test data presenting distributional shifts within and beyond the bounds of the training distribution in the nuisance parameters (e.g., the background source morphology). While our results show that NREs and NPEs perform well when tested perfectly in distribution, they exhibit significant biases when confronted with slight deviations from the examples seen in the training distribution. This indicates the necessity for caution when applying NREs and NPEs to real astrophysical data, where high-dimensional underlying distributions are not perfectly known.","One of the most striking open problems in modern astrophysics is that the nature of \sim 80\% of the matter content of the universe is remains unknown (e.g., Davis et al., 1985; Blumenthal et al., 1984; White et al., 1987; Bertone et al., 2005). This form of matter, called dark matter, is believed to consist of new, invisible particles that do not interact with regular matter electromagnetically. Shedding light on the nature of the dark matter particle is one of the main goals of modern cosmology and particle astrophysics (e.g., Drlica-Wagner et al., 2019). It has been well understood that different dark matter particle properties can result in different spatial distributions of dark matter structures on various scales (e.g., Kuhlen et al., 2012). Thus, by measuring the spatial distribution of dark matter it is possible to discriminate between different dark matter models. The most commonly accepted dark matter model, Cold Dark Matter (CDM) has been able to explain the observations of the large scale structure (e.g., the Cosmic Microwave Background, the Baryon Acoustic Oscillations, weak lensing, etc.) with great precision and accuracy (e.g., Hinshaw et al., 2013; Planck Collaboration et al., 2020). However, on subgalactic scales a number of discrepancies between the predictions of CDM and the observations of the dwarf satellites of the Milky Way have given rise to the possibility of alternative dark matter models (e.g., Kravtsov & Borgani, 2012). Strong gravitational lensing, the formation of multiple images of distant light sources due to the deflection of their light rays by the gravity of intervening structures, is a powerful probe of the subgalactic distribution of matter in the lensing galaxies and along the line-of-sight to the background sources due to its purely gravitational nature. Since different spatial distributions for the projected matter density can result in different distortions in the images, the analysis of lensed images allows the inference of these projected densities, including the abundance and distribution of subhalos. These can then be related to population-level parameters such as the halo mass function on these subgalactic scales, which are directly linked to predictions of dark matter models. However, measuring the effect of small halos on lensed images is a challenging, nonlinear inverse problem. The signal is weak and suffers from multiple degeneracies with other nuisance parameters, such as the morphology of the background source. Furthermore, the properties of a population of dark matter subhalos correspond to a high-dimensional space (e.g., the positions and masses of a large number of subhalos), making the inference of the abundance and distribution of subhalos a difficult problem for traditional, explicit likelihood modeling methods. Past works have introduced a number of approximations in an attempt to make the problem tractable, for example by assuming Gaussian priors, linearizing the lensing model, limiting the analysis to only modeling the effect of the most massive subhalos, or performing a power-spectrum analysis (e.g., Vegetti et al., 2010; Hezaveh et al., 2016b, a; Cyr-Racine et al., 2016; Birrer et al., 2017; Brennan et al., 2019; Despali et al., 2020). Despite these simplifications, these methods are still generally computationally costly, which limits the possibility of extensive testing them for potential biases. Neural network inference frameworks, such as Neural Ratio Estimators (NREs) and Neural Posterior Estimators (NPEs), have recently emerged as a promising solution to these problems since they can be trained to approximate the intractable likelihood or the posterior of parameter distributions directly from high-dimensional input data (e.g., Cranmer et al., 2015; Baldi et al., 2016; Papamakarios & Murray, 2016; He et al., 2016; Brehmer et al., 2018, 2018, 2020). In principle, both analysis frameworks can marginalize over large numbers of nuisance parameters and return an optimal, unbiased likelihood or posterior for the parameters of interest. Within the context of subhalo studies with strong gravitational lensing, this means directly inferring parameters describing the population-level distribution of subhalos (e.g., the subhalo mass function) from a collection of strong lensing systems, while marginalizing over nuisance parameters, including the source galaxy morphologies, the macro-lens parameters, and the parameters of an a priori unknown number of individual subhalos. Recent work has demonstrated the promise of such approaches to circumvent approximations of the intractable likelihood in strong gravitational lensing (e.g., Brehmer et al., 2019; Coogan et al., 2022; Mishra-Sharma, 2022; Zhang et al., 2022; Karchev et al., 2023; Wagner-Carena et al., 2023, 2024; Zhang et al., 2024). Despite their potential, these methods face challenges when applied to real observational data. While these methods have been shown to work well when tested on data coming from the same distribution as the training data, their performance on out-of-distribution (OOD) data is not guaranteed. These subhalo inference frameworks rely on producing large volumes of labeled, simulated data for training, requiring a detailed match between the distributions of real and simulated data for unbiased inference. However, even using the most sophisticated simulation pipelines, it is to be expected that there will always remain some level of mismatch between synthetic and real data distributions. Given the weakness of the expected signal of interest, this leaves NPEs and NREs vulnerable to biased inference. In this work, we investigate the performance of NREs and NPEs for the inference of the subhalo mass function in the presence of realistic distributional shifts between the training and test datasets. We produce several simulated datasets with minor variations in background source morphologies, the distribution of lens macro model parameters, subhalo profiles, and observational noise statistics and show that when the training and test datasets are distributionally shifted, the performance of the NRE and NPE can be dramatically affected. Note that these experiments are not meant to explore an exhaustive list of possible distributional shifts, but rather to establish the vulnerability of this class of methods to such shifts in strong lensing analysis. In Section 2 we describe the simulations and the inference frameworks. In Section 3 we describe the data generation process for the inference frameworks. In Section 4, we present the details of the distributional shifts considered. In Section 5 we present the performance of the inference frameworks on OOD tests and present our conclusions in Section 6. In what follows, we adopt a flat \LambdaCDM cosmology with parameters from Planck Collaboration et al. (2020)."
