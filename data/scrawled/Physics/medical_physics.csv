URL,Title,Abstract,Introduction
https://arxiv.org/html/2411.10025v1,Prediction of cerebral blood volume change after resuscitation from hypoxic-ischemic insult for newborn piglets,"Neonatal hypoxic-ischemic encephalopathy (HIE) is a significant cause of neonatal mortality and developmental disabilities. It has been revealed that the temporal behavior of the cerebral blood volume (CBV) carries information on the degree of hypoxia-ischemia. CBV can be estimated by means of near-infrared spectroscopy. The change of CBV after the insult is related to the change of CBV during the insult. In this paper, we consider a mathematical model which governs the time evolution of CBV after the insult. We show that the temporal behavior of CBV can be predicted with the Kalman filter which is based on the mathematical model.","Neonatal hypoxic-ischemic encephalopathy (HIE) causes of death and developmental disabilities in newborns. Therapeutic hypothermia is often not effective. Thus it is necessary to understand the degree of hypoxia-ischemia (HI) to make treatment plans [1, 2, 3]. Treatments must be delivered within six hours of birth. Hence early recognition of cerebral hemodynamic changes is necessary. Since near-infrared light is particularly absorbed by hemoglobin, cerebral blood volume (CBV) can be estimated with near-infrared spectroscopy (NIRS) using the time-resolved spectroscopy (TRS, Hamamatsu Photonics K.K.). For the piglet experiment which will be considered in this paper, two optical fibers (one is for emission and the other is for detection) were attached to the head of each piglet with the source-detector distance 30\,{\rm mm}. In TRS, the time-correlated single-photon counting technique is used to detect photons. By photon detection with three wavelengths, the oxyHb and deoxyHb concentrations can be estimated, which can then be converted to CBV [4, 5]. Compared with X-ray, light is free from radiation exposure. This means that NIRS measurements can be performed repeatedly. Thus a time series of NIRS imaging is available. The use of NIRS for monitoring oxygenation of the brain has provided useful insights for the management of newborns [6, 7]. An asphyxia piglet model has been established, which exhibits a uniform degree of histopathological brain injury, and the change in CBV was investigated for the piglets by means of near-infrared imaging [8, 9, 10, 11]. In piglets subjected to the HI insult, CBV initially rises to a peak and then keeps decreasing until resuscitation. The CBV reduction from the peak during the insult is related to the severity of brain injuries caused by autoregulatory impairment. Impaired cerebral autoregulation results in adverse neurological outcomes [12, 13]. The time evolution of CBV reflects the degree of hypoxia-ischemia. In particular, the dependence of the temporal change y after the insult on the decay amount x of CBV during the HI insult was investigated [14]. To predict the time evolution of CBV after the insult, it is desirable to find a differential equation which governs the phenomenon. The change of CBV in time takes place as a consequence of extremely complicated blood flows in the body, which in principle can be described by the fluid dynamics. Without touching detailed blood flows in the head, we seek a differential equation which is able to reproduce the time evolution of CBV. This is in some sense related to seeking laws of thermodynamics without statistical mechanics. In this paper, we propose a differential equation which governs the time evolution of CBV for the piglets. To validate the mathematical model, we predict the time dependence of CBV using the observed data from the piglet experiments. The Kalman filter was used for the prediction. The Kalman filter, including the extended Kalman filter, has been used for a pulmonary blood flow estimator [15], cerebral blood flow autoregulation [16, 17], and the heart rate detection with reflected light [18], microvessel imaging for microvessel density maps and blood flow speed maps [19], and the hemodynamic responses for functional near-infrared spectroscopy [20]. See a review [21] for the blood flow for the cardiovascular system. Since it is not easy to obtain the time dependence of CBV for neonates and even newborn piglets, the Kalman filter has not been used in the context of neonatal HIE. Moreover, to the best of our knowledge, any mathematical model has not been proposed for the time evolution of CBV for the neonatal HIE."
https://arxiv.org/html/2411.10308v1,A Realistic Collimated X-Ray Image Simulation Pipeline,"Collimator detection remains a challenging task in X-ray systems with unreliable or non-available information about the detectors position relative to the source. This paper presents a physically motivated image processing pipeline for simulating the characteristics of collimator shadows in X-ray images. By generating randomized labels for collimator shapes and locations, incorporating scattered radiation simulation, and including Poisson noise, the pipeline enables the expansion of limited datasets for training deep neural networks. We validate the proposed pipeline by a qualitative and quantitative comparison against real collimator shadows. Furthermore, it is demonstrated that utilizing simulated data within our deep learning framework not only serves as a suitable substitute for actual collimators but also enhances the generalization performance when applied to real-world data.","In digital radiography, the detection of collimator-covered areas is essential to present diagnostically relevant regions to radiologists. Geometric alignment algorithms, as described in [9], can be employed in X-ray systems with known extrinsic projection parameters. However, despite their availability, these often suffer from inaccuracies sabotaging effectiveness in practice. Due to the inherent geometrical variability in conventional X-ray systems, particularly with mobile flat panel detectors, precise information of the relative position to the detector is unavailable. Moreover, imprecise collimator movement further complicates the detection process, necessitating analysis within image domain. Contrary to a simplistic threshold-based approach, the identification of relevant areas is challenging due to the presence of physical effects like edge-blurring, noise, and scattered radiation. Even human visual perception faces difficulties due to these complexities, as depicted in Fig. 1. (a) Full contrast (b) Contrast adjusted (c) Collimator mask (d) Lineplot Figure 1: Illustrative case for collimator detection depicted in two contrast settings. (a) Contrast adjusted to full image. (b) Contrast adjusted to the orange box. The collimated area (c) is shown as a binary mask. In (d), the intensity profile along the dashed line is compared to the collimated area to visualize the complexity of image-based collimator detection. Deep neural networks (DNNs) show promise for collimator detection, but the limited availability of pre-processed raw data poses a challenge for training robust networks in medical applications. So far, machine learning approaches for collimator detection have not significantly outperformed classic analytical methods in the literature. For instance, comparing the plane detection Hough transform proposed by Kawashita et al. [6] with Mao et al.’s [11] approach that combines random forest learning with a landmark detector in a multi-view learning approach, both methods demonstrate similar performance on unseen data. According to Mao et al. [11], each classifier was trained using only 200 training images. To enhance the performance of machine learning algorithms, it is reasonable to assume that the implementation of robust data augmentation techniques is beneficial. These techniques aim to increase the quantity and variety of datasets. In this context, suitable augmentation techniques can be categorized into deep learning-based methods, such as generative adversarial networks (GANs) [5], and physically motivated approaches. Although GANs have shown promising potential for post-processed X-ray image augmentation (without collimators) in studies like Bowles et al. [1], Madani et al. [10], Kora et al. [7], and Ng et al. [12], they require sophisticated techniques and lack comprehensibility when aiming to serve as reliable training data. Unlike this concept, physically motivated approaches offer a robust alternative for augmentation. These methods leverage an understanding of the underlying physics involved in imaging processes. By incorporating physical principles, these approaches ensure reproducibility and reliability, as demonstrated by Eckert et al. [4] and Xu et al. [15]. In this paper a physically motivated image processing pipeline is presented that simulates the characteristics of real collimators enabling the expansion of limited datasets of X-ray images without collimators. The data augmentation method enables the generation of unlimited pre-processed image data e.g. for training DNNs."
https://arxiv.org/html/2411.10116v1,Two-step registration method boosts sensitivity in longitudinal fixel-based analyses,"Longitudinal analyses are increasingly used in clinical studies as they allow the study of subtle changes over time within the same subjects. In most of these studies, it is necessary to align all the images studied to a common reference by registering them to a template. In the study of white matter using the recently developed fixel-based analysis (FBA) method, this registration is important, in particular because the fiber bundle cross-section metric is a direct measure of this registration. In the vast majority of longitudinal FBA studies described in the literature, sessions acquired for a same subject are directly independently registered to the template. However, it has been shown in T1-based morphometry that a 2-step registration through an intra-subject average can be advantageous in longitudinal analyses. In this work, we propose an implementation of this 2-step registration method in a typical longitudinal FBA aimed at investigating the evolution of white matter changes in Alzheimer’s disease (AD). We compared at the fixel level the mean absolute effect and standard deviation yielded by this registration method and by a direct registration, as well as the results obtained with each registration method for the study of AD in both fixelwise and tract-based analyses. We found that the 2-step method reduced the variability of the measurements and thus enhanced statistical power in both types of analyses.","Longitudinal analyses are powerful experimental designs that limit inter-subject variability compared to cross-sectional analyses, allowing the study of more subtle effects. Such analyses often require all acquired images to be align to a common reference by registering them to a template. In the study of brain white matter changes using diffusion magnetic resonance imaging (MRI), the recently developed fixel-based analysis (FBA) method [1] allows to overcome the major limitation of classical diffusion MRI models in the presence of the very frequent fiber crossing situations, by defining a new basic element, called fixel, which represents one fiber population within a voxel, and by performing analyses fixelwise instead of voxelwise. This method allows the study of white matter at both micro- and macroscopic scales through two metrics: the fiber density (FD), which describes the microstructure, and the fiber bundle cross-section (FC), which describes the macrostructure. This second metric is derived from the Jacobian of the deformation fields between the native images and a study-specific population template [1]. A typical FBA pipeline therefore requires the construction of such a template and a registration step of all acquired images to this template. In the context of a longitudinal FBA, to the best of our knowledge, in the vast majority of the studies described in the literature, images acquired for a same subject are directly independently registered to the population template, as it is done in the usual FBA pipeline, and the longitudinal design is only addressed in the construction of an unbiased population template and in statistical analyses [2]. However, in longitudinal studies conducted with other imaging modalities, such as T1-based morphometry, it has been shown that also adapting the registration to the template step by performing a 2-step registration method can offer advantages, such as reducing the variability of the effects that are measured [3]. This 2-step registration method consists of, first co-registering the images obtained for a same subject and computing an intra-subject average, and then registering this average to the population template. In this study, we hypothesized that this 2-step registration method could also be advantageous in the context of longitudinal FBA. We therefore propose an implementation of this method in a typical longitudinal FBA aimed at investigating the evolution of white matter changes in Alzheimer’s disease (AD) which includes AD patients and healthy controls. To quantitatively compare this method with the usual direct registration method, we implemented two FBA pipelines that were identical except for the registration to the population template step, and we compared the mean rates of change of the fixel-based metrics and the standard deviation of these rates of change yielded by each registration method at the fixel level. We then compared the results obtained with each method when testing if the fixel-based metrics decrease over time in AD, in both fixelwise and tract-based analyses."
https://arxiv.org/html/2411.09618v1,MICCAI-CDMRI 2023 QuantConn Challenge Findings on Achieving Robust Quantitative Connectivity through Harmonized Preprocessing of Diffusion MRI,"White matter alterations are increasingly implicated in neurological diseases and their progression. International-scale studies use diffusion-weighted magnetic resonance imaging (DW-MRI) to qualitatively identify changes in white matter microstructure and connectivity. Yet, quantitative analysis of DW-MRI data is hindered by inconsistencies stemming from varying acquisition protocols. Specifically, there is a pressing need to harmonize the preprocessing of DW-MRI datasets to ensure the derivation of robust quantitative diffusion metrics across acquisitions. In the MICCAI-CDMRI 2023 QuantConn challenge, participants were provided raw data from the same individuals collected on the same scanner but with two different acquisitions and tasked with preprocessing the DW-MRI to minimize acquisition differences while retaining biological variation. Harmonized submissions are evaluated on the reproducibility and comparability of cross-acquisition bundle-wise microstructure measures, bundle shape features, and connectomics. The key innovations of the QuantConn challenge are that (1) we assess bundles and tractography in the context of harmonization for the first time, (2) we assess connectomics in the context of harmonization for the first time, and (3) we have 10x additional subjects over prior harmonization challenge, MUSHAC and 100x over SuperMUDI. We find that bundle surface area, fractional anisotropy, connectome assortativity, betweenness centrality, edge count, modularity, nodal strength, and participation coefficient measures are most biased by acquisition and that machine learning voxel-wise correction, RISH mapping, and NeSH methods effectively reduce these biases. In addition, microstructure measures AD, MD, RD, bundle length, connectome density, efficiency, and path length are least biased by these acquisition differences. A machine learning approach that learned voxel-wise cross-acquisition relationships was the most effective at harmonizing connectomic, microstructure, and macrostructure features, but requires the same subject be scanned at each site co-registered. NeSH, a spatial and angular resampling method, was also effective and has generalizable framework not reliant co-registration. Our code is available at https://github.com/nancynewlin-masi/QuantConn/.","Diffusion-weighted magnetic resonance imaging (DW-MRI) of the brain enables in-vivo characterization of white matter microstructure and supports structural brain connectivity mapping (Pierpaoli et al., 1996). DW-MRI acquisition involves varying magnetic field strength with pulsed gradients to sensitize to the movement of water molecules, following the Stejskal and Tanner method (Stejskal and Tanner, 1965). There are a growing number of multi-site DW-MRI studies that encompass varying scanner manufacturers and acquisition protocols. An imaging “site” refers to the acquisition protocol parameters and scanner specifications used to collect an image. Initiatives such as the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (Jack et al., 2008), the National Alzheimer’s Coordinating Center (NACC) (NAC, ), the Open Access Series of Imaging Studies (OASIS3) (LaMontagne et al., 2019), and the Baltimore Longitudinal Study of Aging (BLSA) (Ferrucci, 2008) incorporate data from diverse scanner vendors and protocols. Diffusion imaging inherits site-effects of conventional MRI caused by magnetic field inhomogeneities, field strength, voxel size, and vendor differences. Additionally, it has unique challenges related to diffusion sensitization and processing, including the number of directions the gradient field is applied, the timing of applied gradients, and the DW-MRI reconstruction algorithm (Nencka et al., 2018; Ni et al., 2006). Perturbations to these acquisition decisions within and across datasets can introduce significant confounding site-dependent differences in DW-MRI and subsequent connectomic and bundle analyses. Vollmar et al. demonstrated confounding site differences in the analysis of whole brain, region of interest, and tract-defined microstructure in a cohort of traveling subjects scanned with various protocols and scanner vendors (Vollmar et al., 2010). Similar findings emerged from studies involving multiple vendors, models, and protocols, leading to substantial sources of variation (Karayumak et al., 2019). This variability extends to derived metrics, such as tractography bundles (Schilling et al., 2021) and complex network measures (Newlin et al., 2023; Onicas et al., 2022). Schilling et al. indicated that fiber bundle shape and microstructure analysis were influenced by scanner manufacturer, acquisition protocol, diffusion sampling scheme, diffusion sensitization, and overall bundle processing workflow (Schilling et al., 2021). Joint datasets from retrospective studies by Newlin et al. and Onicas et al. revealed significant differences in complex network measures, such as modularity, global efficiency, clustering coefficient, density, characteristic path length, small-worldness, and average betweenness centrality, attributed to variations in protocol and scanner vendor (Newlin et al., 2023; Onicas et al., 2022). Consequently, there is a clear imperative to address these site-effects in connectivity and structure analyses through a process commonly known as ”harmonization”. Diffusion image harmonization refers to methods, using preprocessing, machine learning, resampling, etc., that reduce bias associ ated with data collection and storage while preserving biological variation (Pinto et al., 2020). The Quantitative Connectivity through Harmonized Preprocessing of Diffusion MRI (QuantConn) challenge is intended to detail and evaluate current image acquisition harmonization techniques. Our evaluation is focused on reproducibility of downstream tasks (tractography, connectomics, tractometry) and their features (bundle macrostructure and microstructure, complex network measures of the connectome)."
https://arxiv.org/html/2411.09599v1,Effect of Parametric Variation of Chordae Tendineae Structure on Simulated Atrioventricular Valve Closure,"Purpose: Many approaches have been used to model chordae tendineae geometries in finite element simulations of atrioventricular heart valves. Unfortunately, current “functional” chordae tendineae geometries lack fidelity (e.g., branching) that would be helpful when informing clinical decisions. The objectives of this work are (i) to improve synthetic chordae tendineae geometry fidelity to consider branching and (ii) to define how the chordae tendineae geometry affects finite element simulations of valve closure.Methods: In this work, we develop an open-source method to construct synthetic chordae tendineae geometries in the SlicerHeart Extension of 3D Slicer. The generated geometries are then used in FEBio finite element simulations of atrioventricular valve function to evaluate how variations in chordae tendineae geometry influence valve behavior. Effects are evaluated using functional and mechanical metrics.Results: Our findings demonstrated that altering the chordae tendineae geometry of a stereotypical mitral valve led to changes in clinically relevant valve metrics (regurgitant orifice area, contact area, and billowing volume) and valve mechanics (first principal strains). Specifically, cross sectional area had the most influence over valve closure metrics, followed by chordae tendineae density, length, radius and branches. We then used this information to showcase the flexibility of our new workflow by altering the chordae tendineae geometry of two additional geometries (mitral valve with annular dilation and tricuspid valve) to improve finite element predictions.Conclusion: This study presents a flexible, open-source method for generating synthetic chordae tendineae with realistic branching structures. Further, we establish relationships between the chordae tendineae geometry and valve functional/mechanical metrics. This research contribution helps enrich our open-source workflow and brings the finite element simulations closer to use in a patient-specific clinical setting.","Atrioventricular valve (AVV) disease is associated with significant morbidity and mortality in both the developing and developed world. Mitral valve (MVR) and tricuspid valve regurgitation (TVR) are thought to affect 5 million [1] and 1.6 million [2] people in the U.S., respectively. A healthy MV and TV will adequately control the flow of blood between the atria and ventricles on the left and right sides of the heart, respectively. Consequently, failure of these AVVs can result in inefficient blood circulation leading to end-organ dysfunction (e.g., liver, lung, kidney), or death. MVR is associated with an increased risk of congestive heart failure and stroke and is the most prevalent form of AVV failure [3, 4]. The TV, while previously described as the “forgotten valve”, has increasingly been the subject of important investigations [5] as part of an effort to better treat TV dysfunction. AVV failure is similarly devastating to both children and adults with congenital heart disease, and in particular to single ventricle patients [6, 7]. Figure 1: Unloaded mitral valve with key anatomical features. The core anatomy of an AVV consists of leaflets, a ring-shaped annulus attaching the leaflets to the heart wall, and chordae tendineae (CDT) that anchor the leaflets to the papillary muscles (PM) in the ventricle wall (Fig. 1). Morphological changes or failure of any of these components can lead to an incompetent valve that allows blood to flow backward from the ventricle to the atrium during ventricular contraction, leading to atrioventricular valve regurgitation (AVVR) [8, 9]. Based on etiology, the factors of AVVR have been categorized as either primary (e.g., direct) or secondary (e.g., indirect) in terms of its effects on the AVV [4, 3, 10]. Myxomatous valve degeneration is an example of primary AVVR that is characterized by elongated CDT and prolapsing leaflets stemming from decreased stiffness of both CDT and leaflet tissue [11, 12]. In contrast, ischemic cardiomyopathy is an example of secondary AVVR related to left ventricular dilation and remodeling [3, 13, 14]. In both primary and secondary AVVR cases, impaired CDT can contribute to valve dysfunction and failure given the essential role they play in normal AVV function [15, 16]. In primary AVVR, CDT rupture can result in leaflet prolapse. While in secondary cases, elevated chordal tension in the setting of ventricular dilation, or ischemia-related alterations to the ventricle, can lead to CDT restriction of leaflet motion and reduction in leaflet contact (coaptation). Congenitally abnormal CDT structures in children, such as those seen in parachute or arcade AVV [17, 18], play a deleterious role in valve function. Consequently, chordal tension and direction plays a significant part in the successful repair of complex congenital heart disease with valvular abnormalities such as complete atrioventricular canal and hypoplastic left heart syndrome [19, 15, 6, 20, 21, 22]. Given the essential role of CDT in valve function, significant effort has been made to develop therapies directed at replacing or altering chordal geometry to achieve improved AVV function. These interventions are dependent on CDT mechanics, especially as related to their physical classification. Specifically, CDT classified as “primary” are inserted within leaflet free edge and prevent leaflet prolapse during ventricular contraction. Secondary CDT, embedded in leaflet body, support the mechanical interaction between AVV and ventricle, with additional stability within the leaflet basal area provided by tertiary CDT [23, 24, 25, 26]. Thus, precise assessment of insertion location, and optimal chordal length, is necessary to understand valve function and to achieve desired therapeutic outcomes. This includes surgical interventions such as: neochordae reconstruction where artificial primary CDT are created to restore function to a prolapsing valve [16]; and PM relocation where CDT-to-leaflet insertion vectors are modified to improve valve function [27, 21, 22, 28, 29]). In this context, realistic representation and fidelity of CDT topology is also critical to emerging computational simulations of AVV function [30]. Notably, finite element (FE) and other multi-physics simulations of AVV function are uniquely positioned to investigate mechanisms of valve dysfunction. In addition, when combined with patient-specific three-dimensional (3D) images (e.g., 3D echocardiography, computed tomography, cardiac magnetic resonance imaging), these computational techniques have the potential to inform iterative preoperative optimization of valve repair “in silico” [30, 31, 32, 33, 34]. However, while advances in 3D imaging provide a reliable and non-invasive means of determining valve leaflet structure [35, 36, 37, 38, 15, 39], modern clinical imaging cannot yet reliably resolve CDT geometry at their individual chordal insertions with the accuracy and fidelity of “ground truth” geometries achieved from ex vivo and pathological specimens [30, 40]. As a result, significant effort has gone into creating image-informed [41, 42, 27, 43] or functionally equivalent [40] CDT topology for the computational simulation of valve function. Most relevant to the present work, Khalighi et al. [40] demonstrated that CDT geometries could be simplified to random insertions of CDT-like structures without branching. These functionally equivalent CDT topologies provided similar predictions as CDT geometries created from high-fidelity ex vivo images of ovine hearts. Although the simplified CDT geometries could predict valve behavior, they do not have realistic features (i.e., branching) necessary to model CDT-related pathology or interventions (e.g., chordal rupture or neochordae). As such, others have created FE models with various implementations of branching CDT geometries [34, 33, 44]. However, the sensitivity of FE simulations of valve function to variations in chordal branching patterns and distributions remains unknown. Furthermore, there is not an existing procedural method for the precise parametric and reproducible creation of specific chordal branching patterns, geometries and density. The objectives of this work are thus to (i) define how CDT geometry affects FE predictions of AVV closure and (ii) to compare FE simulation results of the more realistic branched CDT geometries to simpler non-branching CDT geometries previously used by ourselves [45, 32] and others [40]. In order to achieve this analysis we created a novel open-source module in SlicerHeart to procedurally generate chordal branching geometries [39]. We then employed this framework to quantify how variation of five CDT parameters influenced clinically relevant metrics of valve closure metrics in three realistic atrioventricular valve models."
https://arxiv.org/html/2411.09204v1,RibCageImp: A Deep Learning Framework for 3D Ribcage Implant Generation,"The recovery of damaged or resected ribcage structures requires precise, custom-designed implants to restore the integrity and functionality of the thoracic cavity. Traditional implant design methods rely mainly on manual processes, making them time-consuming and susceptible to variability. In this work, we explore the feasibility of automated ribcage implant generation using deep learning. We present a framework based on 3D U-Net architecture that processes CT scans to generate patient-specific implant designs. To the best of our knowledge, this is the first investigation into automated thoracic implant generation using deep learning approaches. Our preliminary results, while moderate, highlight both the potential and the significant challenges in this complex domain. These findings establish a foundation for future research in automated ribcage reconstruction and identify key technical challenges that need to be addressed for practical implementation.","The reconstruction of compromised thoracic structures, such as ribcage, requires high-precision engineering in implant design. Accurate dimensional specifications are crucial for restoring anatomical alignment and biomechanical functional integrity of the chest cavity [1]. While modern medical imaging and advanced technologies have enabled the development of many prosthetic structures, ribcage reconstruction presents unique challenges [2]. The complex 3D geometry of the thoracic cavity and patient-specific anatomical variations make the design of custom ribcage implants particularly complex. The conventional design workflows remain primarily manual and require extensive specialized expertise [3]. It requires extensive time for clinicians to analyze patient-specific thoracic architecture through manual measurements and Computer-Aided Design (CAD) interpretations, translating complex anatomical data into fabrication-ready implant specifications. These technical limitations and the critical need for precise anatomical alignment create significant barriers to delivering optimal patient-specific thoracic implants. The primary challenge lies in achieving high accuracy and efficiency in ribcage implant design while minimizing manual interventions. This necessitates the development of automated design approaches that can maintain thoracic geometric precision, reduce processing time, and ensure consistent quality across different patients. Additionally, such automation must account for critical anatomical landmarks, biomechanical constraints, and surgical considerations that traditionally rely on clinical expertise. Fig. 1: Illustration of defective ribcage R_{d}, ground truth implant I_{g}, and complete ribcage with predicted ground truth R_{d}+I_{g}. The emergence of deep learning methods such as Convolutional Neural Networks (3D CNNs) [4], especially the 3D U-Net [5], has demonstrated remarkable success in medical image analysis. These networks have shown capabilities in processing volumetric medical data for complex tasks, including brain tumor segmentation [6], early disease detection and diagnosis [7], and surgical planning [8]. While 3D U-Net-based models have been successfully implemented for cranial implant design [9], the ribcage presents significantly more complex challenges. Cranial implants primarily deal with relatively uniform, curved surfaces focused mainly on structural protection and aesthetic outcomes. In contrast, ribcage implants must address multiple curved bones with varying cross-sections, intricate joint connections, and complex spatial relationships. Further, it also ensures the functionality of proper respiratory mechanics, vital organ protection, natural chest wall movement, and seamless integration with the surrounding musculoskeletal structure. Ribcage reconstruction becomes important in various medical situations. These include the removal of chest tumors (both primary and spreading cancers), severe chest injuries from accidents, and congenital disabilities affecting chest wall structure. Each patient’s case is unique and requires a customized implant solution that perfectly matches their specific anatomy [10]. Developing automated, learning-based approaches for ribcage implant design promises significant benefits. This advancement reduces design time, improves implant quality, and enhances surgical outcomes and patient recovery. This is particularly significant given the time-critical nature of many cases, especially in emergency trauma scenarios requiring rapid implant design and efficient surgical planning in oncological cases. An extensive literature review reveals a complete absence of learning-based approaches for automated ribcage implant design, with current methods relying solely on manual CAD processes, statistical shape analysis, template-based modifications, and expert-guided design iterations [11]. CT scans offer high-resolution, volumetric views of thoracic anatomy, ideal for developing learning-based methods for direct 3D implant generation. Leveraging this rich anatomical information—such as spatial relationships and bone structure—into a 3D U-Net model for implant design could seamlessly integrate into standard clinical workflows. Our key contributions to the paper are: • We propose the first deep-learning approach for automated ribcage implant generation directly from CT scans. This establishes a new paradigm for thoracic reconstruction, moving beyond traditional CAD-based methods towards automated, data-driven solutions. • We adapt and optimize the 3D U-Net architecture to demonstrate the feasibility of ribcage reconstruction. • We provide comprehensive evaluation and analysis through detailed validation of the proposed approach on diverse patient cases. The current results demonstrate the viability of the deep model and highlight opportunities for further refinement and optimization. This work provides a preliminary solution and opens new research directions for advancing automated ribcage implant generation. It serves as a foundational reference for future researchers aiming to contribute to this domain."
https://arxiv.org/html/2411.08237v2,Quantitative Imaging of Co and F-Labeled Tracers in a Single “Multiplexed” PET Imaging Session,"In this study, we explore the use of Co-55 as a radioisotope for multiplexed PET (mPET) by utilizing its emission of a prompt gamma-ray in cascade with a positron during decay. We leverage the prompt-gamma signal to generate triple coincidences for a Co-55-labeled tracer, allowing us to distinguish it from a tracer labeled with a pure positron emitter, such as F-18. By employing triple versus double coincidence detection and signal processing methodology, we successfully separate the Co-55 signal from that of F-18. Phantom studies were conducted to establish the correlation between Co-55 double and triple coincidence counts and Co-55 activity. Additionally, we demonstrate the potential for quantifying hot spots within a warm background produced by both Co-55 and F-18 signals in a single PET scan. Finally, we showcase the ability to simultaneously image two tracers in vivo in a single PET session with mouse models of cancer.","Positron emission tomography (PET) is a vital and versatile tool for observing and quantifying biological pathways in vivo [1]. Small animal PET imaging is used to guide the development of new therapeutics for diseases such as cancer and neurological disorders by tracking how a radiotracer (a radioactively-tagged molecule of interest) distributes in the body [2]. Currently, conventional PET is limited to imaging one tracer at a time. However, simultaneous multi-tracer or multiplexed PET (mPET) would be highly useful due to its ability to visualize and quantify two (and potentially more) biological pathways in one scan session [3]. One of the obstacles preventing the realization of mPET is the conventional signal processing used in PET. PET radiotracers are tagged with radioactive isotopes that emit positrons (\beta^{+}) during the decay process. These positrons then annihilate with electrons around them to produce two oppositely-directed 511 keV photons. These pairs of photons are detected by the PET scanner and used to reconstruct the 3-D distribution of the radiotracer [4]. Due to the uniform energy of annihilation photons, reliance on their detection alone is insufficient for differentiating between various radiotracers. One way to achieve mPET is by imaging one radiotracer tagged with a pure positron-emitting (\beta^{+}) isotope and another tagged with a prompt-gamma+positron-emitting (\beta^{+}-\gamma) isotope [5, 6]. In this approach, the \beta^{+}-\gamma isotope releases an additional photon in cascade with positron emission, resulting in a triple coincidence (three photons) that is distinguishable from the conventional double coincidence (two photons) from a pure positron emitter. In this paper, we further explore the concept of multiplexed PET (mPET), demonstrating for the first time quantitative imaging of Co-55 simultaneously with F-18 (a \beta^{+} isotope) in both phantom and mice experiments. Co-55 is a \beta^{+}-\gamma isotope that emits a prompt-gamma of 931 keV in 75% of positron emissions [7]. Further, Co-55 is a long-lived radionuclide that we believe will be useful for labeling and tracking an antibody that targets important biomarkers for cancer immunotherapy. 1.1 Previous Work The first descriptions of using the triple coincidence produced from a \beta^{+}-\gamma emitter to distinguish from two photon coincidences from a pure \beta^{+} emitter were presented in [5, 8, 9, 10]. Fukuchi et. al was the first study to image simultaneous dual-tracer PET in animals by imaging FDG (a pure \beta^{+} emitter) and Na-22 (a \beta^{+}-\gamma emitter) [11]. More recently, Pratt et. al showed dual-tracer PET with various combinations of pure \beta^{+} emitting isotope labeled tracers imaged with \beta^{+}-\gamma emitting isotope labeled tracers such as Zr-89 with I-124, F-18 with I-124, and Zr-89 with Y-86 [3]."
https://arxiv.org/html/2411.08819v1,Bilateral Signal Warping for Left Ventricular Hypertrophy Diagnosis,"Left Ventricular Hypertrophy (LVH) is a major cardiovascular risk factor, linked to heart failure, arrhythmia, and sudden cardiac death, often resulting from chronic stress like hypertension. Electrocardiography (ECG), while varying in sensitivity, is widely accessible and cost-effective for detecting LVH-related morphological changes. This work introduces a bilateral signal warping (BSW) approach to improve ECG-based LVH diagnosis. Our method creates a library of heartbeat prototypes from patients with consistent ECG patterns. After preprocessing to eliminate baseline wander and detect R peaks, we apply BSW to cluster heartbeats, generating prototypes for both normal and LVH classes. We compare each new record to these references to support diagnosis. Experimental results show promising potential for practical application in clinical settings.","Left Ventricular Hypertrophy (LVH) is a critical marker of cardiovascular health, characterized by an increase in the mass of the left ventricle in response to chronic stressors. This heart condition is prevalent in people with risk factors such as obesity and diabetes, and also commonly occurs in many patients with untreated hypertension [kannel1970electrocardiographic, jothiramalingam2021machine, tang2024optimized]. As LVH serves as a strong predictor of cardiovascular diseases, including heart failure, arrhythmia, and sudden cardiac death, early screening and management are essential to control morbidity and mortality [brady2020electrocardiogram, liu2023left]. Electrocardiography (ECG) is a widely accessible and cost-effective screening tool for LVH, despite its varying sensitivity and specificity [liu2023left, bacharova2014role]. Recently, advances in artificial intelligence (AI) and healthcare data digitization have led to automated LVH diagnostic algorithms using ECG with accuracy on par with expert physicians [liu2023left, AI_lvh, AI_lvh2]. However, these methods are often criticized as “black-box” systems, often lacking interpretability and reliability in clinical settings [AI1, AI2]. The difficulty in understanding which features drive neural network decisions complicates validation, limits transparency, and poses potential risks to patient safety. This motivates the development of computer-aided methods that can “mimic” physician diagnostics and therefore provide supporting evidence for each decision [interp_ai, interp_ai2], which has been successfully applied to electroencephalograms[eeg]. Inspired by Sir William Osler’s words, “The good physician treats the disease; the great physician treats the patient who has the disease”, we recognize that doctors often compare patient cases to reach diagnostic conclusions. This raises the question: which patients should serve as prototypes for such comparisons? To solve this, we introduce a bilateral signal warping (BSW) approach in this work to enhance explainable ECG-based LVH diagnosis. Our method constructs a library of heartbeat prototypes by first extracting representative signal patterns for each selected patient candidate. BSW is then applied to group and warp these signals to generate distinct prototypes for normal and LVH cases. These normal and abnormal prototypes act as references to which new ECG records can be compared, thus enabling explainable LVH diagnosis based on both quantitative and visual patient comparison."
https://arxiv.org/html/2411.07353v1,A Discrete Fiber Dispersion Model with Octahedral Symmetry Quadrature for Applications in Skin Mechanics,"Advanced simulations of the mechanical behavior of soft tissues frequently rely on structure-based constitutive models, including smeared descriptions of collagen fibers. Among them, the so-called Discrete Fiber Dispersion (DFD) model is based on a discrete integration of the fiber-strain energy over all the fiber directions. In this paper, we recall the theoretical framework of the DFD model, including a derivation of the stress and stiffness tensors required for the finite element implementation. Specifically, their expressions for incompressible plane stress problems are obtained. The use of a Lebedev quadrature, built exploiting the octahedral symmetry, is then proposed, illustrating the particular choice adopted for the orientation of the integration points. Next, the convergence of this quadrature scheme is assessed by means of three numerical benchmark tests, highlighting the advantages with respect to other angular integration methods available in the literature. Finally, we propose as applicative example a simulation of Z-plasty, a technique commonly used in reconstructive skin surgery, considering multiple geometrical configurations and orientations of the fibers. Results are provided in terms of key mechanical quantities relevant for the surgical practice.","In recent years, the development of structure-based mechanical models has gained significant attention in computational mechanics, particularly for materials with a complex internal microstructure such as soft collagenous tissues. These models aim to capture the non-linear and anisotropic material behavior more accurately with respect to purely phenomenological formulations, by accounting for the non-uniform dispersion of the fibers within the material [1]. Tissues such as arterial walls, heart valves, gastric muscle, and skin are mainly constituted by a dispersion of wavy collagen fibers embedded in a ground isotropic substance [2, 3, 4, 5]. The orientation distribution of the fibers in the three-dimensional space is given by a Probability Density Function (PDF), which can be included in the constitutive model according to two main approaches: the Generalized Structure Tensor (GST) and the Angular Integration (AI) approaches [6]. In the GST approach, the PDF is used to calculate a structure tensor representative of the fiber distribution, which is then used in the strain-energy function to determine a pseudo-invariant accounting for the anisotropy. Based on a specific shape of the PDF, the structure tensor can be computed in a closed form, without requiring further integration during the analysis. For this reason, models based on this approach, such as those proposed by Merodio and Ogden [7], Gasser et al. [8], Holzapfel et al. [9] to name a few, became popular due to their computational efficiency [1]. A downside of these models is that they cannot exclude the contribution of compressed fibers along individual directions without compromising the computational efficiency, since all the fibers directions are a priori included. Latorre and Montáns [10] showed that the criterion originally chosen for excluding the compressed fibers, which is based on the mean direction [8], may produce nonphysical discontinuities in the stress–strain behavior. They proposed a new pre-integrated method, although limited to axially symmetric fiber distributions. Melnik et al. [11], Li et al. [12] introduced a GST-based model in which the structure tensor is integrated accounting for the stretched fibers only, but since the computation is required at each deformation the efficiency of the original model is compromised. On the other hand, in the AI approach, also referred to as Continuous Fiber Dispersion (CFD) [13], the PDF is used to weigh the integration of the single fiber strain-energy over all the directions, allowing for a simple fiber exclusion criterion during the computation, and independent of the shape of the PDF. However, this comes with a compromise on the computational efficiency, since the integration must be performed multiple times during the analysis. In an effort to increase the computational performance while preserving the advantages of the AI approach, an hybrid approach named Discrete Fiber Dispersion (DFD) model was introduced, based on a discrete integration over the spherical domain of the fibers [13]. In this method, the choice of the integration scheme plays a fundamental role and can significantly affect the mechanical response. Li et al. [13] originally proposed an integration method based on the discretization of the unit sphere with uniform triangular areas, each associated to a discrete fiber orientation with a weight computed by exactly integrating the PDF over the relative area. The method has been optimized by Rolf-Pissarczyk et al. [14] who developed an adaptive meshing to locally discretize the unit sphere surface with finer elements where the values of the PDF are concentrated. However, this method tends to increase the number of integration points for fiber distribution approaching the perfect alignment, while in such case only few representative directions would be sufficient. Moreover, there is little control on the number of integration points, since the refinement bisects spherical triangular elements until all the weights attain a value lower than a specified threshold. In the light of the above, alternative quadrature schemes appear a valid choice for the integration due to their versatility for selecting the level of precision and the number of points [15]. Notable examples include methods proposed by Bažant and Oh [16], Fliege and Maier [17], Heo and Xu [18]. There are mathematical requirements that need to be satisfied. To ensure poly-convexity of the strain-energy function [19], integration points should not be associated to negative coefficients [20]. Moreover, the distribution of the points over the sphere should be as uniform as possible in order to maximize the integration efficiency, as well as to attain the invariance of the integral under general rotations of the integration points [20, 21]. The number of points also plays a crucial role, since schemes with reduced points may induce anisotropic behaviors for isotropic materials, as pointed out by Ehret et al. [20]. Lastly, since stress, strain, and related kinematic variables have identical values at diametrically opposite points on the sphere, centrally symmetric integration schemes are favored. They indeed allow the integral to be computed over only half of the sphere, effectively reducing the number of integrand evaluations by half [22]. Recently, Britt and Ehret [23] proposed an approach based on the transformation of the spherical integral over the unit sphere into a linear integral over the distribution of the stretch of the fibers, which can be easily computed using a n-point Gauss quadrature. According to the findings of Ehret et al. [20], who compared the performance of different integration schemes, quadrature schemes based on the construction of a spherical grid with polyhedral symmetry showed great potential in terms of computational efficiency and accuracy. In particular, the Lebedev rule, built exploiting octahedral symmetry [24, 25], demonstrated to perform the best out of the nine methods tested by Skacel and Bursa [21]. However, only elementary loading cases were considered, and further analyses including complex loading configurations are required to investigate the convergence rate, as well as the ideal number of integration points for efficient Finite Element (FE) simulations. To this aim, in the present work, we address the numerical performance of the quadrature scheme with both elementary and advanced benchmark tests in large deformations. Specifically, we analyze the homogeneous uniaxial tension and simple shear tests, and the isotropic annular disk subjected to torsion of the internal boundary. The tests conducted confirm that the Lebedev quadrature exhibits rapid convergence to the exact solution, validating its suitability for high-fidelity mechanical simulations. The analyses were performed in the commercial FE software Abaqus [26]. As an applicative example of the DFD model considered, we show a Z-plasty simulation, which has great relevance in skin mechanics. Z-plasty involves making Z-shaped incisions to release tension and improve the mobility of the skin, particularly in areas of scarring or contracture [27]. While the procedure is widely used in clinical practice, its application is largely guided by empirical experience rather than by a quantitative understanding of the underlying mechanics. If not correctly executed, these surgeries could potentially worsen the existing conditions, with the consequent need of further and more risky interventions. Therefore, a preoperative inspection is extremely important, and surgeons must assess key factors such as skin quality, underlying bone structure, vascular supply, extension of the defect, and direction of the natural skin tension. By combining this information the surgeon can determine the appropriate technique and plan the operation, as well as inform the patient about risks and possible outcomes [28]. In this regard, several surgical guidelines can be found in the literature [29, 30, 31], some of which providing an algorithmic approach to determine the most suitable technique depending on factors such as the defect length, skin availability or skin tension [32]. However, these guidelines all lack quantitative mechanical evidences. Recent works of Buganza Tepole et al. [33] and Stowers et al. [34] extensively analyzed similar surgical procedures, providing valuable mechanical insights that could help refine surgical practices, shifting from empirically driven decisions to more data-informed approaches. Following the same approach, in this work the DFD model is applied to simulate Z-plasty for various incision configurations and fiber orientations, providing novel insights into the mechanical behavior of the skin after the procedure. The simulation results can provide support for surgical practice, especially in terms of optimizing incision placement and orientation relative to the collagen fiber architecture of the skin. For example, the findings suggest that certain incision angles with respect to the mean collagen direction lead to reduced post-operative strain, thus improving healing outcomes. These results not only enhance the understanding of Z-plasty mechanics but also pave the way for more personalized and optimized surgical strategies. The paper is organized as follows. In Section 2 the theoretical framework of the DFD model is presented, including the derivation of the stress and stiffness tensors required for the FE implementation, and their specific formulation for incompressible plane stress problems. The Lebedev quadrature scheme is then introduced, illustrating the particular choice for the orientation of the integration points. Next, the convergence of the quadrature is assessed by means of numerical benchmark tests. In Section 4 the Z-plasty is simulated for multiple geometrical configurations and orientations of the fibers. Results are provided in terms of key mechanical quantities relevant for the surgical practice. In Section 5 some considerations about the implemented model and the Z-plasty results are addressed, while in Section 6 the conclusions are drawn."
https://arxiv.org/html/2411.07918v1,Isometric Transformations for Image Augmentation in Mueller Matrix Polarimetry,"Mueller matrix polarimetry captures essential information about polarized light interactions with a sample, presenting unique challenges for data augmentation in deep learning due to its distinct structure. While augmentations are an effective and affordable way to enhance dataset diversity and reduce overfitting, standard transformations like rotations and flips do not preserve the polarization properties in Mueller matrix images. To this end, we introduce a versatile simulation framework that applies physically consistent rotations and flips to Mueller matrices, tailored to maintain polarization fidelity. Our experimental results across multiple datasets reveal that conventional augmentations can lead to misleading results when applied to polarimetric data, underscoring the necessity of our physics-based approach. In our experiments, we first compare our polarization-specific augmentations against real-world captures to validate their physical consistency. We then apply these augmentations in a semantic segmentation task, achieving substantial improvements in model generalization and performance. This study underscores the necessity of physics-informed data augmentation for polarimetric imaging in deep learning (DL), paving the way for broader adoption and more robust applications across diverse research in the field. In particular, our framework unlocks the potential of DL models for polarimetric datasets with limited sample sizes. Our code implementation is available at github.com/hahnec/polar_augment.","Image data augmentation plays a crucial role in training deep neural networks by enhancing dataset diversity and mitigating the risk of overfitting [1]. Without sufficient data variety, larger deep learning models may memorize training examples rather than learning generalizable features, which results in high training accuracy but reduced performance on unseen data. This makes data augmentation essential for achieving robust, generalizable models [2]. In scenarios with limited training data, such as in polarimetric imaging, data augmentation becomes even more essential. For example, medical datasets are often constrained by the difficulty of acquiring large, annotated samples, making robust augmentation strategies a cornerstone for improving DL model generalization and performance [2]. Typical image augmentations involve pseudo-random modifications of image data including cropping [1], intensity and color variations [3], additive noise or geometric transformations [2, 4, 5]. Geometric augmentation techniques, such as rotations and image flips, are extensively used in natural images but face limitations when applied to polarimetric images. This is because polarimetric imaging captures not only the spectral intensity but also the polarization state of scattered light, which is defined by the orientation of the electric field’s oscillation plane [6, 7, 8]. As a result, standard color image rotation algorithms fail to account for changes in polarization properties. In polarimetric imaging, these properties are represented using the Mueller matrix, a 4\times4 real-valued transfer matrix that relates the Stokes vectors of the input and output light beams after interaction with a sample [9, 10, 11]. While the Mueller matrix captures full polarimetric information, conventional augmentations ignore its unique structure, highlighting the need for tailored transformation methods to prevent misleading results and support generalization. Our research addresses these limitations by developing novel augmentation techniques uniquely designed to consider the structural intricacies of the Mueller matrix. This framework introduces transformations that are both broadly applicable across polarimetric learning scenarios and rigorously grounded in physical principles. By directly accommodating the specific properties of Mueller matrix data, our method provides a crucial resource for training in applications such as semantic segmentation [12, 13], denoising models [14, 15], polarimetric demosaicking [16, 17, 18], or Mueller matrix data recovery [19]. Our proposed augmentation techniques are particularly valuable in fields constrained by limited and diverse polarimetric datasets, enabling more precise and resilient model training in data-limited environments [20, 21]. Tailoring physics-informed data augmentation specific to imaging modalities has been explored in other fields such as ultrasound [22], magnetic resonance imaging (MRI)[23], and light-fields [24]. Existing augmentation techniques for polarimetric data are predominantly tailored to radar systems in astronomy and remote sensing, where physical modeling is mainly limited to rotation and mirror transformations [25, 26, 27]. However, these methods fail to account for the unique structure of the Mueller matrix, making them unsuitable for optical polarimetric imaging applications. Previous studies on rotation-invariant parameters of the Mueller matrix and photon coordinate transformations in backscattering polarimetry provide a valuable groundwork for incorporating physical properties into augmentation methods [28, 29, 30]. Building on this foundation, our work translates physical polarimetry models directly into deep learning-based augmentations, paving the way for more representative and effective training datasets—crucial for advancing optical polarimetric imaging applications that are constrained by data limitations. Although prior research has acknowledged the challenges of augmenting optical polarimetric images [31, 32], it lacks a rigorous mathematical framework for handling Mueller matrix transformations. Additionally, evaluations in existing work rely primarily on segmentation scores without quantitative comparisons to measured data, underscoring the need for validated augmentation methods that fully account for the unique structure of Mueller matrix images. To this end, we present a simulation framework that extends traditional spatial transformations to polarimetric imaging by systematic modeling of changes in the Mueller matrix elements. Our approach ensures that augmentations follow the physical laws of polarization, closely simulating how light would interact with materials and surfaces in real-world conditions. Specifically, our findings reveal that rotating Mueller matrix images without modifying the polarization states results in erroneous polarization characteristics. By accounting for both spatial and polarization-specific variations, we can improve the generalization and accuracy of DL models in polarimetric imaging tasks. The paper is structured as follows: the Methods section reviews standard spatial transformations and introduces isometric polarimetric mappings. The Results section presents key polarimetry features, metrics, and experimental validations of our framework. We then demonstrate the effectiveness of the proposed augmentations in training a segmentation network and conclude with a discussion."
https://arxiv.org/html/2411.07877v1,A fast plastic scintillator for low intensity proton beam monitoring,"In the context of particle therapy monitoring, we are developing a gamma-ray detector to determine the ion range in vivo from the measurement of particle time-of-flight. For this application, a beam monitor capable to tag in time the incident ion with a time resolution below 235 ps FWHM (100 ps rms) is required to provide a start signal for the acquisition. We have therefore developed a dedicated detector based on a fast organic scintillator (EJ-204) of 25 \times25 \times1 mm3 coupled to four SiPM strips that allow measuring the particle incident position by scintillation light sharing. The prototype was characterised with single protons of energies between 63 and 225 MeV at the MEDICYC and ProteusONE facilities of the Antoine Lacassagne proton therapy centre in Nice. We obtained a time resolution of 120 ps FWHM at 63 MeV, and a spatial resolution of \sim2 mm rms for single particles. Two identical detectors also allowed to measure the MEDICYC proton energy with 0.3% accuracy.","Compared to conventional X-ray radiotherapy, Particle Therapy (PT) provides a high ballistic precision with limited irradiation of healthy tissue, thanks to a sharp maximum of the dose deposition at the end of the particle range (Bragg peak). However, PT accuracy may be limited by numerous sources of uncertainties such as patient mispositioning, transient modifications of the anatomy and errors in the determination of tissue stopping powers from X-ray images. In order to reduce the safety margins imposed by these uncertainties and improve the technique specificity, particle range has to be measured on-line with millimetric precision[1]. Several groups worldwide are developing different techniques for PT monitoring, taking advantage of the spatial and/or temporal correlation between the dose and the emission vertices of secondary particles like Prompt Gammas (PGs)[2]. In this context, we are developing a PG detection system that exploits the PG Timing (PGT) technique to determine the ion range from an exclusive measurement of particles Time-Of-Flight (TOF)[3]. The system consists of a multi-channel PG detectors array (TIARA for Tof Imaging ARrRay) arranged around the target/patient to measure the PG time of arrival t_{stop}, which are read-out in coincidence with a fast beam monitor that tags in time the incident ions t_{start}. The overall TOF measured (TOF=t_{stop}-t_{start}) is equal to the ion transit time in the patient up to the PG vertex, plus the PG TOF from the vertex to the gamma detector. The PG vertex coordinates and subsequently the ion range in the patient can be retrieved from direct measurements of the TOFs as a solution of a non-trivial inverse problem [4]. For the PGT technique, the higher the system time resolution, the higher the accuracy achieved on the determination of the particle range. We have already demonstrated that a Coincidence Time Resolution (CTR) between the beam monitor and TIARA of 235 ps FWHM allows a proton range accuracy of a few millimeters with a limited statistics of \sim107 protons, equivalent to an average irradiation spot in PT treatments[4, 5]. Here we describe the development and characterisation of the beam monitor we have conceived for this application. The monitor must have a time resolution well below the 235 ps FWHM targeted for the whole system CTR for single ions; a detection surface large enough to cover the section of a typical pencil beam (more than 1 cm FWHM at lower proton energies[6]); a nearly perfect detection efficiency; a good energy resolution to count incident protons (at least at low intensities). Moreover it must be capable of measuring the beam incident position with an accuracy of the order of the millimeter. Several proton beam monitors of this kind are under development. They are based on Ultra Fast Silicon Detectors (UFSD) (1.2 \times 1.2 mm2 and 50 µm thick) [7], small plastic scintillators (3 \times 3 \times 3 mm3) coupled to Silicon Photomultipliers (SiPM) by optical fibres [8] and diamonds (9 \times 9 \times 0.5 mm3) [9, 10], but their limited size is not adapted to clinical beams. A large-area, high time resolution counter based on a plastic scintillator (60 \times 30 \times 5 mm3) read-out by SiPMs was proposed, instead by Cattaneo et al.[11]: they achieved a time resolution of 42 \pm 2 ps rms with electrons, which approximately deposit 1 MeV in the detector. The detector we have developed is also based on a fast plastic scintillator coupled to SiPMs and is position-sensitive. Its design is described in section II. It was recently tested on the two accelerators of the Centre Antoine Lacassagne (CAL) in Nice: the MEDICYC cyclotron provides 63 MeV protons within a bunch of 4 ns width delivered every 40 ns [12] and the IBA synchrocyclotron S2C2 (ProteusONE) provides protons with an energy ranging from 100 to 225 MeV within bunches of 16 ns period and 50% duty cycle [6]. Given the pulsed time structure of CAL accelerators, the overall CTR is a convolution of the intrinsic system CTR and the proton bunch width. Actually, at clinical intensities, where thousands of protons may be delivered within each bunch, it is impossible to identify the true proton-gamma coincidences and only the proton bunch can be tagged in time. Therefore, in order to fully exploit the potential of our detector, we propose to reduce the beam current to \sim1 proton/bunch during the first irradiation spot(s) [13]. This approach will allow to verify patient positioning as well as to assess the absence of anatomical modifications occurring before the treatment session with the highest accuracy. Preliminary experiments carried out at cyclotron, synchrotron and synchro-cyclotron facilities have shown that the implementation of this regime presents no technical barrier, but the use of a dedicated, high sensitivity beam detector as the one proposed is required to monitor the delivered dose and cope with beam intensity fluctuations at low fluxes. Notwithstanding, the TOF measurement of single protons of 100-200 MeV are especially challenging because of their limited ionisation capabilities. In this work, the beam monitor performance is thus evaluated at low beam intensities in terms of Detector Time Resolution (DTR), detection efficiency and spatial resolution (section III). Two identical prototypes were also used to determine the beam energy from particle TOF[14] (section IV)."
https://arxiv.org/html/2411.07495v1,Towards Seamless Integration of Magnetic Tracking into Fluoroscopy-guided Interventions,"The 2D projective nature of X-ray radiography presents significant limitations in fluoroscopy-guided interventions, particularly the loss of depth perception and prolonged radiation exposure. Integrating magnetic trackers into these workflows is promising; however, it remains challenging and under-explored in current research and practice. To address this, we employed a radiolucent magnetic field generator (FG) prototype as a foundational step towards seamless magnetic tracking (MT) integration. A two-layer FG mounting frame was designed for compatibility with various C-arm X-ray systems, ensuring smooth installation and optimal tracking accuracy. To overcome technical challenges, including accurate C-arm pose estimation, robust fluoro-CT registration, and 3D navigation, we proposed the incorporation of external aluminum fiducials without disrupting conventional workflows. Experimental evaluation showed no clinically significant impact of the aluminum fiducials and the C-arm on MT accuracy. Our fluoro-CT registration demonstrated high accuracy (mean projection distance \approx 0.7\text{\,}\mathrm{mm}), robustness (wide capture range), and generalizability across local and public datasets. In a phantom targeting experiment, needle insertion error was between 2\text{\,}\mathrm{mm} and 3\text{\,}\mathrm{mm}, with real-time guidance using enhanced 2D and 3D navigation. Overall, our results demonstrated the efficacy and clinical applicability of the MT-assisted approach. To the best of our knowledge, this is the first study to integrate a radiolucent FG into a fluoroscopy-guided workflow.","\IEEEPARstart X-ray fluoroscopy continues to be the predominant modality for intra-operative image guidance, ubiquitously employed across various domains including cardiovascular, endovascular, orthopedic, and neuro-interventions, as well as in pain management and biopsies[1, 2, 3]. Fluoroscopy provides high-contrast visualization of osseous structures, contrast-enhanced regions, and metallic objects, thereby enabling interventionists to monitor and safeguard the real-time advancement of surgical instruments. To appreciate the instrument depth, the C-arm must be oriented perpendicular to the instrument’s trajectory and, after evaluating the depth, reverted to its original pose to monitor the instrument’s orientation. Additionally, the C-arm requires efficient positioning to achieve desired radiographic views for optimizing instrument placement, a process that often necessitates repeated image acquisitions, known as “fluoro-hunting”. The requirement to reposition the C-arm and perform repeated image acquisitions stems from the 2D projective nature of X-ray radiography. This nature contributes to the main limitations of fluoroscopy-guided interventions, notably the loss of depth perception and prolonged radiation exposure for both patients and medical personnel. To mitigate these limitations, one promising strategy is to integrate spatial trackers into fluoroscopy-guided surgical workflows. Specifically, pose sensors are attached to surgical instruments, and optionally to the patient’s anatomy and the C-arm, each of which is spatially calibrated and co-registered with one another. The co-registration of all tracked objects into a common coordinate system enables the development of surgical navigation systems, as well as serving as the technological foundation for image fusion and 3D visualization. Merloz et al. [1] incorporated an optical tracker into fluoroscopy-guided pedicle screw insertion, marking one of the earliest clinical evaluations of computer-assisted spine surgeries. Their study demonstrated that the navigated approach reduced the rate of cortex penetration and radiation exposure time, but at the expense of a longer operative time. Recently, Gao et al. [4] developed a fluoroscopy-guided robotic system designed for transforaminal lumbar epidural injections, incorporating an optical tracker to track both an injection device and a surgical robot. Similarly, Bakhtiarinejad et al. [5] proposed a fluoroscopy guidance system for osteoporotic hip augmentation, further expanding the application of the optical tracking-based solution. However, due to their reliance on line-of-sight, optical trackers pose practical challenges for optimizing their spatial configuration within the operating room to ensure simultaneous tracking of surgical instruments, patient anatomy, the C-arm, as well as some potential external hardware such as a surgical robot. Furthermore, optical trackers are incapable of tracking flexible surgical instruments, such as catheters, guidewires, and flexible endoscopes seated subcutaneously [6]. Consequently, the utilization of optical trackers is often restricted to neurosurgery [7] and orthopedics, where the surgical target is rigid and exposed. Magnetic tracking (MT) has gained considerable attention due to its capability to track flexible instruments placed subcutaneously without the constraints of line-of-sight. Recently, Ramadani et al. [6] conducted a thorough review of tracking techniques for curvilinear catheters in interventional radiology and endovascular interventions, highlighting the efficacy of MT in these contexts. Nonetheless, it is well-recognized that MT is susceptible to interference from ferromagnetic materials. Over two decades ago, Hummel et al. [8] demonstrated that X-ray fluoroscopy settings could cause distortion of magnetic fields, thereby affecting the tracking robustness of MT. More recently, advancements in MT technology have been explored. Yaniv et al. [9] investigated the effect of bi-plane fluoroscopes on the accuracy of the Aurora MT system (Northern Digital Inc., Ontario, Canada). Their findings indicated that the presence of bi-plane fluoroscopes in a clinical setup has a negligible effect on the tracking accuracy, with a maximal error of 1.4\text{\,}\mathrm{mm}. Similarly, Lugez et al. [10] demonstrated that MT has the potential to provide effective assistance during surgical and interventional procedures. Additionally, Xu et al. [11] employed a magnetic tracker for pedicle screw insertion and compared it with a robot-assisted approach, demonstrating the efficacy of MT-tracked over the robot-assisted approaches in terms of faster instrument placement and reduced X-ray exposure. It is important to note that standard magnetic field generators (FGs) are constructed with radiopaque materials, which introduce metal-induced imaging artifacts in radiographs, potentially limiting their clinical utility. To mitigate this issue, Yoo et al. [12] incorporated a window FG, in which the FG coils are configured around the periphery of an open aperture, into the surgical table. While the FG window design effectively eliminated imaging artifacts induced by metal coils, it resulted in diminished tracking accuracy and restricted the use of fluoroscopy to small-oblique views. Recent advancements include radiolucent FGs that minimize metal-induced imaging artifacts under X-ray fluoroscopy [13, 14], as well as error compensation techniques developed to mitigate the adverse effects of ferromagnetic interference, such as distorting the magnetic tracking field and limiting the effective tracking range. However, a solution that seamlessly integrates MT into fluoroscopy-guided surgical workflows remains challenging, particularly with respect to the latest radiolucent FGs. In this study, we focus on addressing the following unmet clinical needs to improve the fluoroscopy-guided procedure: 1. The ability to enable robust 3D navigation. Recent studies [15] demonstrate that 3D navigation is more effective and clinically preferred compared to conventional 2D fluoroscopy guidance. It offers the advantages of 2D and 3D reformations in CT-like quality, thereby addressing the limitation of depth perception inherent in 2D fluoroscopy, along with 3D tracking of surgical instruments. One critical technical component for achieving 3D navigation is to effectively register fluoroscopic images with pre-operative CTs [16]. Despite fluoro-CT registration methods having been extensively investigated over the past decades, this technique has not yet become a standard component in fluoroscopy-guided interventions [17]. The widespread adoption of this approach has been restrained by challenges, including limited capture range and sensitivity to the initial pose estimate [18, 19]. Therefore, an easy-to-use and robust fluoro-CT registration solution remains a clinical need to benefit 3D navigation. 2. Compatibility with standard surgical workflows. To ensure optimal MT accuracy, the FG must be placed in proximity to the surgical or therapeutic site [20]. There are several configurations for placing the FG, including above the patient [20], on the surgical table alongside the patient [10], or adjacent to the patient but off the surgical table [9]. Additionally, the FG setup must not obstruct the fluoroscopic view and should minimize disruption to the standard surgical workflow. However, these current configurations pose challenges for seamless integration into various fluoroscopy-guided procedures. In addition, in the context of fluoroscopy-guided procedures, two types of C-arm imaging systems are generally used: manually driven and mechanically driven. The more commonly used former approach requires manual maneuvering to achieve desired views. Typically, its intrinsic parameters, such as pixel spacing, image dimension and source-to-detector (SID) distance, remain fixed. Mechanically driven systems are often employed for complex procedures, such as vascular, cardiac and neurosurgical procedures. Their movements, including rotations, panning and height adjustments, are motorized and tracked, enabling rapid repositioning during procedures. However, some C-arm intrinsic parameters, such as SID distance, may vary according to clinical requirements, thereby dynamically affecting the 3D navigation functionality. Therefore, for MT integration to be widely applicable in fluoroscopy-guided procedures, it is essential to accommodate both manually and mechanically driven C-arm systems. In this study, we employ a tabletop radiolucent FG prototype [13] developed by Northern Digital Inc.. To the best of our knowledge, this is the first study to integrate a radiolucent FG into a fluoroscopy-guided workflow. To achieve this, we first design a two-layer radiolucent FG mounting frame. This hardware configuration is seamlessly integrated into the surgical table and ensures optimal MT accuracy. Subsequently, we propose a strategy of incorporating external fiducials onto the FG mounting frame to optimize the workflow and accommodate various C-arm systems. Specifically, external fiducials are employed to accurately estimate the C-arm pose and facilitate the registration of fluoroscopic images with pre-operative CTs, which further enables the development of 3D surgical navigation. In addition, via a video decomposition step, our framework significantly reduces imaging artifacts caused by metallic fiducials and the radiolucent FG, providing an unobstructed radiographic view under real-time fluoroscopy guidance."
https://arxiv.org/html/2411.07376v1,Ensemble Learning for Microbubble Localization in Super-Resolution Ultrasound,"Super-resolution ultrasound (SR-US) is a powerful imaging technique for capturing microvasculature and blood flow at high spatial resolution. However, accurate microbubble (MB) localization remains a key challenge, as errors in localization can propagate through subsequent stages of the super-resolution process, affecting overall performance. In this paper, we explore the potential of ensemble learning techniques to enhance MB localization by increasing detection sensitivity and reducing false positives. Our study evaluates the effectiveness of ensemble methods on both in vivo and simulated outputs of a Deformable DEtection TRansformer (Deformable DETR) network. As a result of our study, we are able to demonstrate the advantages of these ensemble approaches by showing improved precision and recall in MB detection and offering insights into their application in SR-US.","Traditional ultrasound techniques encounter an inherent compromise between image clarity and penetration. Enhancing resolution through increasing frequency of the propagated waves, comes at the cost of increased tissue absorption, limiting the depth of effective imaging. Conversely, lower frequencies allow for deeper penetration but yield less detailed images [1]. Following the break-throughs in optical super-resolution imaging [2], Ultrasound Localization Microscopy (ULM) has been proposed, which employing the strong backscatter echo properties of ultrasound contrast agent microbubbles (MBs) can, theoretically, provide a ten-fold improvement in ultrasound blood flow imaging [3]. This advancement has significantly enhanced our understanding of disease states and progression, particularly in functional brain imaging [4], cancer [5] and diabetes [6]. To create super-resolution ultrasound (SR-US) images of the vascular maps, MBs injected into the bloodstream are identified, localized, and frequently monitored. The MBs, confined to blood vessels, enable detailed visualization of vascular structures [7]. The precision of MB localization in SR-US faces several challenges, such as Point Spread Functions (PSFs) distortion caused by MB vicinity, near-field imaging effects, variations in tissue speed of sound and attenuation [8, 9]. These factors collectively contribute to alterations in PSF shape, complicating accurate localization. On the other hand, while higher MB concentrations can reduce acquisition time, they may also lead to reduced localization precision as the MB signals coincide [10]. Addressing these issues requires careful consideration of imaging parameters, processing techniques, and potentially the development of novel algorithms that can account for these various factors to maintain localization accuracy across diverse imaging conditions. In the past few years, several methods of MB localization have been developed and evaluated. Conventional localization methods rely on the assumption of isolated scatterers within the region of interest [11]. However, this assumption breaks down as MB trajectories intersect. To tackle the problem of overlapping PSFs while allowing for higher MB concentrations and shorter acquisition times, various Deep Learning (DL) networks were introduced which rely on learning complex patterns from simulations and incorporating temporal context into the localization process. A common framework employed in many DL methods is the Convolutional Neural Network (CNN) with an encoder-decoder architecture with variations in the upsampling techniques and the specific building blocks employed for feature encoding [12, 13, 14]. Shin et al. [15] introduced LOCA-ULM, which enhances localization accuracy by incorporating temporal context from adjacent frames employing two U-nets in sequence. Building on this concept, Pustovalov et al. [16] recently proposed an architecture inspired by DECODE [17]. Their method utilizes 3D CNNs to incorporate temporal context, potentially allowing for the integration of information from a larger number of frames. Transformer-based architectures have gained popularity in ULM due to their effectiveness in capturing long-range relationships in images. The ULM-TransUnet framework [18] combines the strengths of U-Net architectures and Transformer models, enhancing performance through multi-scale feature integration and long-term contextual dependencies. Previously, we employed a DEformable DEtection TRansformer (DEDETR) solution for MB detection to better handle PSF deformations and variations in appearance, resulting in improved detection performance [19, 20]. Nevertheless, the effectiveness of any model in real-world applications is heavily based on meeting specific criteria related to data quality and hyper-parameter optimization. In these situations, ensemble learning can offer a valuable solution by combining multiple weaker models to achieve desired results [21, 22]. By leveraging the strengths of multiple models, ensembles can often achieve performance levels that surpass those of individual models while manifesting increased robustness and generalization capabilities. Beyond traditional ensemble methods, researchers have developed sophisticated techniques to refine the outputs of detection models by eliminating redundant bounding boxes. These approaches include Non-Maximum Suppression (NMS), Soft-NMS [23], NMW (Non-Maximum Weighting) [24], and Weighted Box Fusion (WBF) [25]. Each of these methods offers unique strategies for consolidating overlapping detections and improving overall detection accuracy. While combining multiple machine learning models is a widespread technique for enhancing overall performance across various applications, its implementation for ULM applications presents unique challenges. For one, most of the aforementioned techniques do not detect MBs as bounding boxes and rather focus on finding the centroid of the MBs. Secondly, choosing models that while capturing a variety of different shapes and sizes of MBs in different densities, also don’t overlap and repeat themselves. In this paper, we: 1. Propose a versatile ensemble framework for MB detection based on a transformer-based solution, i.e. DEDETR. This approach integrates multiple detectors to leverage their complementary strengths and to optimize the ensemble’s decision-making process, enhancing both the accuracy and robustness of our MB detection results. 2. Perform extensive empirical evaluation of the proposed ensemble framework and its associated ensemble strategies. Through experimentation across both simulation and in vivo datasets, we demonstrate notable improvement in the details of the generated super-resolution maps, including improved precision, recall, and RMSE scores. 3. Analyze the framework’s ability to mitigate individual detector weaknesses and adapt to diverse object scales and appearances, showcasing its effectiveness, generalizability, and robustness. (a) NMS (b) NMSW (c) Soft NMS (d) WBF (e) DEDETR (f) GT Fig. 1: Full-view SR maps of the simulation test dataset for different methods. (A) (B) (C) DEDETR (Patch 4) WBF GT Fig. 2: Zoomed-in boxes from different simulation SR maps, showing the results from each method (indicated by columns) for each of the (A), (B), and (C) boxes."
https://arxiv.org/html/2411.06958v1,Data-driven discovery of mechanical models directly from MRI spectral data,"Finding interpretable biomechanical models can provide insight into the functionality of organs with regard to physiology and disease. However, identifying broadly applicable dynamical models for in vivo tissue remains challenging. In this proof of concept study we propose a reconstruction framework for data-driven discovery of dynamical models from experimentally obtained undersampled MRI spectral data. The method makes use of the previously developed spectro-dynamic framework which allows for reconstruction of displacement fields at high spatial and temporal resolution required for model identification. The proposed framework combines this method with data-driven discovery of interpretable models using Sparse Identification of Non-linear Dynamics (SINDy). The design of the reconstruction algorithm is such that a symbiotic relation between the reconstruction of the displacement fields and the model identification is created. Our method does not rely on periodicity of the motion. It is successfully validated using spectral data of a dynamic phantom gathered on a clinical MRI scanner. The dynamic phantom is programmed to perform motion adhering to 5 different (non-linear) ordinary differential equations. The proposed framework performed better than a 2-step approach where the displacement fields were first reconstructed from the undersampled data without any information on the model, followed by data-driven discovery of the model using the reconstructed displacement fields. This study serves as a first step in the direction of data-driven discovery of in vivo models.","Deriving models from first principles is generally prohibitively challenging for non-linear biomechanical systems. Data-driven discovery (DDD) has recently emerged as an attractive method for identifying interpretable closed form models from spatiotemporal measurements of complex dynamic systems[1, 2, 3]. The seminal work by Brunton et al.[4, 5, 6] introduces a new approach to DDD using sparse regression to determine the underlying non-linear dynamical models directly from time-series data. This method for Sparse Identification of Non-linear Dynamics (SINDy) leverages the fact that most physical systems can be described with only a few terms making the governing equations sparse in a high-dimensional non-linear function space. Although the robustness of the method has been extensively researched using simulated data with added Gaussian noise[7, 8], its effectiveness using real measurement data is less well explored [9]. Our goal is to use SINDy to identify dynamical models directly from experimentally obtained undersampled spectral data from the MRI scanner. MRI scanners, that naturally gather data in the spectral domain (k-space), provide excellent soft tissue contrast without the use of harmful radiation. However, this modality generally lacks temporal resolution at reasonable spatial resolutions required for in vivo imaging. This severely complicates the dynamical analysis, restricting the motion analysis on physiological time scales[10]. The spectral motion model, which has been presented as part of the recently developed Spectro-dynamic MRI framework[11], is leveraged to overcome these problems. This framework enables us to distill displacement fields with a high spatiotemporal resolution directly from k-space data. By working directly in k-space, the intermediate step of reconstructing time series images (the spatial domain approach) is circumvented. This allows for the use of undersampled k-space data, which makes real-time imaging with high temporal resolution possible. Unlike gated approaches to dynamic MRI, where the motion is binned into different states[12], this approach does not rely on periodicity of the dynamics. Coupling the recently introduced spectral motion model with SINDy allows for data-driven discovery of non-linear models with high temporal resolution, directly from undersampled k-space in an iterative manner. Using this approach to identify reduced order dynamical models for in vivo soft tissue holds great promise. Reduced order models are often applied to describe complex biophysical processes as they significantly reduce the system complexity[13]. Future applications of this technique for in vivo tissue include the identification of reduced order models that describe cardiac motion dynamics which are essential for understanding the functionality of the cardiac system or stomach motility for understanding an important component of the digestive process. The contribution of this proof of concept study is that we identify the governing equations directly from experimentally obtained undersampled k-space data from a dynamic phantom, acquired using a clinical MRI system. This controlled environment allows us to test and thoroughly validate the method using a series of dynamics of increasing complexity. These models include forced non-linear systems and non-linear Van der Pol oscillators previously employed to describe cardiac time series data[14]. The remainder of the paper is organized as follows. In section II the theory behind our approach is presented, followed by an outline of the methods in section III in which the set-up of the MRI experiments is discussed together with the numerical implementation of the inverse problem. The main results are presented in section IV and include a comparison with a more direct 2-step approach. Open problems and possible next steps are discussed in section V, ending with a conclusion in section VI."
https://arxiv.org/html/2411.06447v1,Multi-Parameter Molecular MRI Quantification using Physics-Informed Self-Supervised Learning,"Biophysical model fitting plays a key role in obtaining quantitative parameters from physiological signals and images. However, the model complexity for molecular magnetic resonance imaging (MRI) often translates into excessive computation time, which makes clinical use impractical. Here, we present a generic computational approach for solving the parameter extraction inverse problem posed by ordinary differential equation (ODE) modeling coupled with experimental measurement of the system dynamics. This is achieved by formulating a numerical ODE solver to function as a step-wise analytical one, thereby making it compatible with automatic differentiation-based optimization. This enables efficient gradient-based model fitting, and provides a new approach to parameter quantification based on self-supervised learning from a single data observation. The neural-network-based train-by-fit pipeline was used to quantify semisolid magnetization transfer (MT) and chemical exchange saturation transfer (CEST) amide proton exchange parameters in the human brain, in an in-vivo molecular MRI study (n=4). The entire pipeline of the first whole brain quantification was completed in 18.3\pm8.3 minutes, which is an order-of-magnitude faster than comparable alternatives. Reusing the single-subject-trained network for inference in new subjects took 1.0\pm0.2 s, to provide results in agreement with literature values and scan-specific fit results (Pearson’s r>0.98, p<0.0001).","Magnetic resonance imaging (MRI) plays a central role in clinical diagnosis and neuroscience. This modality is highly versatile and can be selectively programmed to generate a large number of image contrasts[1], each sensitive to certain biophysical parameters of the tissue. In recent years, there has been extensive research into developing quantitative MRI (qMRI) methods that can provide reproducible measurements of magnetic tissue properties (such as: T1, T2, and T{}_{2}^{*}), while being agnostic to the scan site and the exact acquisition protocol used[2]. Classical qMRI quantifies each biophysical property separately[3], using repeated acquisition and gradual variation of a single acquisition parameter under steady state conditions. This is followed by fitting the model to an analytical solution of magnetization vector dynamics[4]. The exceedingly long acquisition times associated with the classical quantification pipeline have motivated the development of magnetic resonance fingerprinting (MRF)[5], which is an alternative paradigm for the joint extraction of multiple tissue parameter maps from a single pseudorandom pulse sequence. Since MRF data are acquired under non-steady state conditions[6], the corresponding magnetization vector can only be resolved numerically. This comes at the expense of the complexity of the inverse problem, namely finding tissue parameters that best reconstruct the signal according to the forward model of spin dynamics. Since model fitting under these conditions takes an impractically long time[7], MRF is commonly solved by dictionary matching, where a large number of simulated signal trajectories are compared to experimentally measured data[8]. Unfortunately, the size of the dictionary scales exponentially with the number of parameters (the ""curse of dimensionality""[9]), which rapidly escalates the compute and memory demands of both generation and subsequent use of the dictionary for pattern matching-based inference. Recently, various deep learning (DL)-based methods have been developed for replacing the lengthy dictionary matching with neural-network (NN)-based inference[10, 11, 12, 13]. While this approach greatly reduces the parameter quantification time, networks still need to be trained using a comprehensive dictionary of synthetic signals. Since dictionary generation may take days[12], it constitutes an obvious bottleneck for routine use of MRF, and reduces the possibilities for addressing a wide variety of clinical scenarios. The complexity and time constraints associated with the MRF pipeline are drastically exacerbated for molecular imaging applications that involve a plurality of proton pools, such as chemical exchange saturation transfer (CEST) MRI[14]. While CEST has demonstrated great potential for dozens of biomedical applications[15, 16, 17, 18, 19, 20, 21], some on the verge of entering clinical practice[22], the inherently large number of tissue properties greatly complicate analysis[23]. This has prompted considerable efforts to transition from CEST-weighted imaging to fully quantitative mapping of proton exchange parameters [24, 25, 26, 27, 28]. Early CEST quantification used the fitting of the classical numerical model (based on the underlying Bloch-McConnell equations) after irradiation at various saturation pulse powers (B_{1})[27]. However, applying this approach in a pixelwise manner in-vivo is unrealistic because both the acquisition and reconstruction steps may require several hours. Later, faster approaches, such as quantification of the exchange by varying saturation power/time and Omega-plots [29, 30, 31, 32] still rely on steady-state (or close to steady state) conditions, and approximate analytical expressions of the signal as a function of the tissue parameters[33, 34]. Unfortunately, a closed-form analytical solution does not exist for most practical clinical CEST protocols, which utilize a train of off-resonant radiofrequency (RF) pulses saturating multiple interacting proton pools. Similarly to the quantification of water T1 and T2, incorporating the concepts of MRF into CEST studies provided new quantification capabilities[28, 35, 36, 37, 38] and subsequent biological insights, for example, in the detection of apoptosis after oncolytic virotherapy[12]. However, in order to make CEST MRF a realistic, robust, and attractive candidate for clinical use, the long dictionary generation time associated with each new application needs to be replaced by a rapid and flexible approach that adequately models multiple proton pools under saturation pulse trains. Here, we describe a physics-based deep learning framework for rapid model fitting of the human brain tissue proton spin properties. While this approach is applicable for quantifying a variety of MRI parameters, we focus on a challenging CEST imaging scenario, involving multiple proton pools, a saturation pulse train, and non-steady-state MRF acquisition. The computational pipeline (Fig. 1) combines a spin physics simulator and a NN-based quantitative parameter reconstructor in a fully auto-differentiable manner[39]. Our system effectively solves and inverts the Bloch-McConnell ordinary differential equations (ODEs), which govern the multi-pool exchange, saturation, and relaxation dynamics of molecular MRI. Hence, we refer to this approach as ""neural Bloch McConnell fitting"" (NBMF). Importantly, the network can be be trained in a self-supervised manner, directly on the single-subject data of interest (inspired by related work on test-time-[40], internal-[41], and zero-shot-[42, 41, 43] learning). This circumvents the need for prior curation of a large training dataset, which is often inaccessible, especially for molecular MRI. Figure 1: Schematic representation of the core neural Bloch McConnell fitting (NBMF) pipeline. (Top) A quantitative parameter reconstructor parameterized as a multi-layer perceptron (MLP) and a differentiable Bloch-McConnell simulator are serially connected into a single computational graph. Single-subject MRF data serves both as the input and as the regression target for the reconstructor-simulator circuit. The network convergence provides the fitted exchange parameter maps for the examined subject as well as a trained NN reconstructor; the latter can be used to extract parameter maps for new subjects within seconds (Bottom). The simulator can be realized using the exact numerical Bloch McConnell ODE solver or using analytical approximations when available (e.g., for 2-pool semisolid-MT quantification [34]). While not shown in the diagram, auxiliary per-voxel data such as T1, T2, B0, and B1 maps can be added as input to the neural reconstructor and the simulator. Furthermore, the pipeline main block can be serially repeated so that estimated semisolid MT volume fraction (fss) and proton exchange rate (kssw) maps inferred at the first stage are joined to the raw data used in a second reconstructor aimed to quantify the amide proton exchange parameters (fs, ksw)."
https://arxiv.org/html/2411.06252v1,A deep learning model for inter-fraction head and neck anatomical changes,"Objective: To assess the performance of a probabilistic deep learning based algorithm for predicting inter-fraction anatomical changes in head and neck patients.Approach: A probabilistic daily anatomy model for head and neck patients (DAMHN) is built on the variational autoencoder architecture. The model approximates the generative joint conditional probability distribution of the repeat computed tomography (rCT) images and their corresponding masks on the planning CT images (pCT) and their masks. The model outputs deformation vector fields, which are used to produce possible rCTs and associated masks. The dataset is composed of 93 patients (i.e., 367 pCT - rCT pairs), 9 (i.e., 37 pairs) of which were set aside for final testing. The performance of the model is assessed based on the reconstruction accuracy and the generative performance for the set aside patients.Main results: The model achieves a DICE score of 0.92 and an image similarity score of 0.65 on the test set. The generated parotid glands volume change distributions and center of mass shift distributions were also assessed. For both, the medians of the distributions are close to the true ones, and the distributions are broad enough to encompass the real observed changes. Moreover, the generated images display anatomical changes in line with the literature reported ones, such as the medial shifts of the parotids glands.Significance: DAMHN is capable of generating realistic anatomies observed during the course of the treatment and has applications in anatomical robust optimization, treatment planning based on plan library approaches and robustness evaluation against inter-fractional changes.","1 Synthetic CT uses in Proton Therapy Proton Therapy (PT) has desirable dose characteristics, such as similar target coverage and lower organs at risk (OAR) doses, when compared to traditional photon based radiotherapy (RT) (Chen et al., 2023). However, the increased dose conformality implies an increased susceptibility to dose degradation by uncertainties such as setup errors, range errors and anatomical changes over the course of the typically month long treatment duration (van Kranen et al., 2009). To diminish the dose degradation, robust optimization and evaluation (Unkelbach & Paganetti, 2018) with isotropic setup and range settings (Liu et al., 2013) and offline adaptive replanning (Deiter et al., 2020) is performed in clinical practice. This results in a high dose region that surrounds the target, which in the case of the H&N region where OARs are in close proximity to the target, could result in high chances of side effects. Moreover, there are certain anatomical changes that are not effectively accounted for by robust optimization taking setup and range errors into account. One proposed option (van de Water et al., 2018) is the inclusion of additional (synthetic) computed tomography (CT) images in the (anatomical) robust optimization process. While this provided increased target coverage and lower OAR doses for the specific H&N patients in the cohort, compared to conventional robust optimization, it still created a high dose region surrounding the target. To reduce this region to its minimum and counter long and short-term inter-fraction occurring anatomical variations, online adaptive proton therapy (OAPT) has been proposed. In this workflow, a new CT is acquired for each fraction and within a short time a new fully re-optimized plan is generated. The resulting plan would only need minimal robustness settings to counter the effects of range uncertainties, machine related setup uncertainties and remaining intra-fraction uncertainties. The short time available and the limited computational resources imply that fully robust reoptimization in the online setting is still a topic of research (Oud et al., 2024). The plan library (PL) approach was proposed as an intermediate solution (Oud et al., 2022; van de Schoot et al., 2016). This approach used the planning CT image to generate multiple plans with varying robustness settings. On the given day, it administers an appropriately chosen plan, therefore resulting in NTCP reductions or sometimes in increased robustness that ensures adequate target coverage. In this approach, synthethic CT images can be used to expand the pre-compiled library of plans, by generating optimal plans for the future patient anatomies predicted by the model. Assuming that in the limited time in which the patient is on the treatment table, the original treatment plan is adapted or refined, synethic CT images can prove useful for plan QA. Specifically, several CT images with associated truly optimal plans, could be generated a priori. On the given day, a fast dosimetric check can be performed between the adapted and refined plan and the truly optimal pre-generated plan. Thus, models of inter-fractional anatomical changes have applications in several PT related workflows such as anatomical robust optimization, plan quality assurance in OAPT or expanding the plan library approach. Multiple approaches to synthethic CT generation have been employed, such as principal component analysis (PCA) or deep learning. An overview of the different possible approaches is given by the work of (Smolders et al., 2024). Deep learning models have been shown to outperform PCA based ones in the case of prostate anatomies (Pastor-Serrano et al., 2023) and denoising diffusion probabilistic models (Smolders et al., 2024) were successfully applied for artificial CT generation for the H&N site where they were additionally shown to increase robustness to anatomical changes. This work builds upon the previous publication of (Pastor-Serrano et al., 2023) on a generative deep learning daily anatomy model (DAM) for prostate inter-fractional anatomical changes. The model architecture and the data processing pipeline are changed and thereafter applied to a H&N radiotherapy cohort. The model is referred to from here on as DAMHN. Section 2 details the probabilistic framework of the model. Section 3 provides details on the dataset generation and the specific architecture configuration used for training. Section 4 contains the results and their discussion. The performance of the model was assessed via several tests. The results of a reconstruction accuracy test are shown in Subsection 4.1. The generative performance was assessed in terms of the model’s capability to predict realistic anatomical changes. To this end, an overview of the typical changes in head and neck patients reported by literature studies is given in Subsection 4.2. The anatomical changes present on the training set are discussed in Subsection 4.3. Subsection 4.4 presents and discusses the anatomical changes predicted by the model. Subsection 4.5 compares these anatomical changes with the ones presented in the recently published denoising diffusion probabilistic models DiffuseRT model (Smolders et al., 2024). Lastly, a latent space analysis is presented in Subsection 4.6. Section 5 concludes this work and discusses some improvement points."
https://arxiv.org/html/2411.06340v1,"Deep Learning in Classical X-ray Ghost Imaging for
Dose Reduction","Ghost imaging (GI) is an unconventional technique that combines information from two correlated patterned light fields to compute an image of the object of interest. A standard pixelated camera records the structure of one light field (that does not interact with the object), and a bucket detector (or single-pixel camera) measures the total intensity of the second light field that is transmitted or scattered by the object. GI can be performed with visible light as well as penetrating radiation such as x-rays, electrons, etc. Penetrating radiation is usually ionizing and damages biological specimens; therefore, minimising the dose of this radiation in a medical or biological imaging context is important. GI has been proposed as a potential way to achieve this. With prior knowledge of the object of interest, such as sparsity in a specific basis (e.g., Fourier basis) or access to a large dataset for neural network training, it is possible to reconstruct an image of the object with a limited number of measurements. However, low sampling does not inherently equate to low dose. Here, we specifically explore the scenario where reduced sampling corresponds to low-dose conditions. In this simulation-based paper, we examine how deep learning (DL) techniques could reduce dose in classical x-ray GI. Since GI is based on illumination patterns, we start by exploring optimal sets of patterns that allow us to reconstruct the image with the fewest measurements, or lowest sampling rate, possible. We then propose a DL neural network that can directly reconstruct images from GI measurements even when the sampling rate is extremely low. We demonstrate that our deep learning-based GI (DLGI) approach has potential in image reconstruction, with results comparable to direct imaging (DI) at the same dose. However, given the same prior knowledge and detector quantum efficiency, it is very challenging for DLGI to outperform DI under low-dose conditions. We discuss how it may be achievable due to the higher sensitivity of bucket detectors over pixel detectors.","X-ray technology is widely used to produce 2D and 3D images of structures inside the human body. Nevertheless, since x-rays are a form of ionizing radiation, overdoses of medical x-ray imaging may result in higher cancer incidence rates [1]. Consequently, there must be a trade-off between gaining more information or achieving higher image quality and minimising x-ray dose. In excised samples, an excessive dose can damage the structure, thus distorting the resulting images. So, “how can we reduce the dose while still obtaining a useful image?” becomes an important question. It has been proposed that ghost imaging has the potential to achieve this [2]. Ghost imaging (GI) is a new paradigm in imaging that was first realised using entangled photon pairs in the field of quantum optics late last century (see [3]). Classical variants were later realised using thermal light with structured/patterned illumination since only the position correlation property of the entangled photons was required [3]. The GI measurement process is outlined in Sec. II.1 and the image computation is detailed in Sec. III.1. Classical GI has now been extended from visible light to penetrating radiation such as x-rays [4, 5], electrons [6], neutrons [7], etc. The x-ray ghost imaging technology has been utilised in two-dimensions by Zhang et al. [8] and Pelliccia et al. [5], as well as in three-dimensions by Kingston et al. [9]. Without prior knowledge of the specimen, it is challenging to recover high quality images using classical GI. Kingston et al. showed that direct imaging or a scanning probe will always produce better results than classical GI under the same photon-shot noise [2]. However, classical GI can become advantageous when we have prior knowledge of the imaging object. It becomes theoretically possible to reconstruct the object with fewer measurements, thus resulting in reduced radiation dose, preventing the object from being damaged in biological-imaging settings. One way to achieve this is through compressed sensing (CS), in which an image is assumed to be sparse in a particular transform space or domain, (e.g., the Fourier space), allowing images to be reconstructed under a low sampling rate [10]. However, this requires an extremely sparse object image in some particular basis, and this is rarely satisfied. Another method is through artificial intelligence techniques such as deep learning [11], using a large dataset that encapsulates the prior knowledge to train a neural network. For example, Wang et al. have developed a neural network with a structure based on eHoloNet [12] that outperforms other ghost imaging reconstruction methods even when the sampling rate is low [13]. The purpose of this paper is to explore the possibility of dose reduction in x-ray ghost imaging through deep learning. There have been many studies investigating GI with deep learning (e.g. [13, 14, 15]), even in the context of x-ray imaging [16] and neutron imaging [17]. Lyu et al. [15] showed that their deep learning ghost imaging approach outperforms classical compressive sensing techniques at extremely low sampling rates. Inspired by this, we aim to further explore its application in low-dose conditions. While a low sampling rate is often associated with a low-dose condition, it is not a direct correlation; it is possible to have a low sampling rate but a high dose, i.e., a significant amount of x-ray radiation is used. Hence, this work explores the application of deep learning ghost imaging (DLGI) in extremely low-dose conditions. We first assume that fewer measurements lead to a lower dose under the same conditions. We then attempt to find optimal illumination patterns and develop a neural network that minimises the number of measurements while still obtaining sufficient image information with a reasonable computation time. Following that, we compare our DLGI recovered images with images produced by conventional direct imaging subjected to the same total dose, to see if dose reduction can be achieved through this DLGI approach to classical x-ray ghost imaging. Note that in our simulations, we assume the same detector quantum efficiency in the comparison, where DLGI will be at an inherent disadvantage compared to direct imaging. In practice, we would anticipate an increase in measured flux in the GI data while maintaining the same dose on the object (see Sec. V.3)."
